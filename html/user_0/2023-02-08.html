<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03494">http://arxiv.org/abs/2302.03494</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03494] A Categorical Archive of ChatGPT Failures](http://arxiv.org/abs/2302.03494) #security</code></li>
<li>Summary: <p>Large language models have been demonstrated to be valuable in different
fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of
data and simulates human conversation by comprehending context and generating
appropriate responses. It has garnered significant attention due to its ability
to effectively answer a broad range of human inquiries, with fluent and
comprehensive answers surpassing prior public chatbots in both security and
usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,
which is the focus of this study. Ten categories of failures, including
reasoning, factual errors, math, coding, and bias, are presented and discussed.
The risks, limitations, and societal implications of ChatGPT are also
highlighted. The goal of this study is to assist researchers and developers in
enhancing future language models and chatbots.
</p></li>
</ul>

<h3>Title: From Emulation to Mathematical: A More General Traffic Obfuscation Approach To Encounter Feature based Mobile App traffic Classification. (arXiv:2302.03118v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03118">http://arxiv.org/abs/2302.03118</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03118] From Emulation to Mathematical: A More General Traffic Obfuscation Approach To Encounter Feature based Mobile App traffic Classification](http://arxiv.org/abs/2302.03118) #security</code></li>
<li>Summary: <p>The usage of the mobile app is unassailable in this digital era. While tons
of data are generated daily, user privacy security concerns become an important
issue. Nowadays, tons of techniques, such as machine learning and deep learning
traffic classifiers, have been applied to analyze users app traffic. These
techniques allow the monitor to get the fingerprints of using apps while the
user traffic is still encrypted, which raises a severe privacy issue. In order
to fight against this type of data analysis, people have been researching
obfuscation algorithms to confuse feature-based machine learning classifiers
with data camouflage by modification on packet length distribution. The
existing works achieve this goal by remapping traffic packet length
distribution from the source app to the fake camouflage app. However, this
solution suffers from its lack of scalability and flexibility in practical
application since the method needs to pre-sample the target fake apps traffic
before the use of traffic camouflage. In this paper, we proposed a practical
solution by using a mathematical model to calculate the target distribution
while maintaining at least 50 percent accuracy drops on the performance of the
AppScanner mobile traffic classifier and roughly 20 percent overhead created
during packet modification.
</p></li>
</ul>

<h3>Title: Homomorphic Hashing Based on Elliptic Curve Cryptography. (arXiv:2302.03290v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03290">http://arxiv.org/abs/2302.03290</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03290] Homomorphic Hashing Based on Elliptic Curve Cryptography](http://arxiv.org/abs/2302.03290) #security</code></li>
<li>Summary: <p>For avoiding the exposure of plaintexts in cloud environments, some
homomorphic hashing algorithms have been proposed to generate the hash value of
each plaintext, and cloud environments only store the hash values and calculate
the hash values for future needs. However, longer hash value generation time
and longer have value summary time may be required by these homomorphic hashing
algorithms with higher security strengths. Therefore, this study proposes a
homomorphic hashing based on elliptic curve cryptography (ECC) to provide a
homomorphic hashing function in accordance with the characteristics of ECC.
Furthermore, mathematical models and practical cases have been given to prove
the proposed method. In experiments, the results show that the proposed method
have higher efficiency with different security strengths.
</p></li>
</ul>

<h3>Title: Unsupervised Deep Learning for IoT Time Series. (arXiv:2302.03284v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03284">http://arxiv.org/abs/2302.03284</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03284] Unsupervised Deep Learning for IoT Time Series](http://arxiv.org/abs/2302.03284) #security</code></li>
<li>Summary: <p>IoT time series analysis has found numerous applications in a wide variety of
areas, ranging from health informatics to network security. Nevertheless, the
complex spatial temporal dynamics and high dimensionality of IoT time series
make the analysis increasingly challenging. In recent years, the powerful
feature extraction and representation learning capabilities of deep learning
(DL) have provided an effective means for IoT time series analysis. However,
few existing surveys on time series have systematically discussed unsupervised
DL-based methods. To fill this void, we investigate unsupervised deep learning
for IoT time series, i.e., unsupervised anomaly detection and clustering, under
a unified framework. We also discuss the application scenarios, public
datasets, existing challenges, and future research directions in this area.
</p></li>
</ul>

<h3>Title: Towards Meaningful Anomaly Detection: The Effect of Counterfactual Explanations on the Investigation of Anomalies in Multivariate Time Series. (arXiv:2302.03302v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03302">http://arxiv.org/abs/2302.03302</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03302] Towards Meaningful Anomaly Detection: The Effect of Counterfactual Explanations on the Investigation of Anomalies in Multivariate Time Series](http://arxiv.org/abs/2302.03302) #security</code></li>
<li>Summary: <p>Detecting rare events is essential in various fields, e.g., in cyber security
or maintenance. Often, human experts are supported by anomaly detection systems
as continuously monitoring the data is an error-prone and tedious task.
However, among the anomalies detected may be events that are rare, e.g., a
planned shutdown of a machine, but are not the actual event of interest, e.g.,
breakdowns of a machine. Therefore, human experts are needed to validate
whether the detected anomalies are relevant. We propose to support this anomaly
investigation by providing explanations of anomaly detection. Related work only
focuses on the technical implementation of explainable anomaly detection and
neglects the subsequent human anomaly investigation. To address this research
gap, we conduct a behavioral experiment using records of taxi rides in New York
City as a testbed. Participants are asked to differentiate extreme weather
events from other anomalous events such as holidays or sporting events. Our
results show that providing counterfactual explanations do improve the
investigation of anomalies, indicating potential for explainable anomaly
detection in general.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Ethical Considerations for Collecting Human-Centric Image Datasets. (arXiv:2302.03629v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03629">http://arxiv.org/abs/2302.03629</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03629] Ethical Considerations for Collecting Human-Centric Image Datasets](http://arxiv.org/abs/2302.03629) #privacy</code></li>
<li>Summary: <p>Human-centric image datasets are critical to the development of computer
vision technologies. However, recent investigations have foregrounded
significant ethical issues related to privacy and bias, which have resulted in
the complete retraction, or modification, of several prominent datasets. Recent
works have tried to reverse this trend, for example, by proposing analytical
frameworks for ethically evaluating datasets, the standardization of dataset
documentation and curation practices, privacy preservation methodologies, as
well as tools for surfacing and mitigating representational biases. Little
attention, however, has been paid to the realities of operationalizing ethical
data collection. To fill this gap, we present a set of key ethical
considerations and practical recommendations for collecting more
ethically-minded human-centric image data. Our research directly addresses
issues of privacy and bias by contributing to the research community best
practices for ethical data collection, covering purpose, privacy and consent,
as well as diversity. We motivate each consideration by drawing on lessons from
current practices, dataset withdrawals and audits, and analytical ethical
frameworks. Our research is intended to augment recent scholarship,
representing an important step toward more responsible data curation practices.
</p></li>
</ul>

<h3>Title: PLACES: Prompting Language Models for Social Conversation Synthesis. (arXiv:2302.03269v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03269">http://arxiv.org/abs/2302.03269</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03269] PLACES: Prompting Language Models for Social Conversation Synthesis](http://arxiv.org/abs/2302.03269) #privacy</code></li>
<li>Summary: <p>Collecting high quality conversational data can be very expensive for most
applications and infeasible for others due to privacy, ethical, or similar
concerns. A promising direction to tackle this problem is to generate synthetic
dialogues by prompting large language models. In this work, we use a small set
of expert-written conversations as in-context examples to synthesize a social
conversation dataset using prompting. We perform several thorough evaluations
of our synthetic conversations compared to human-collected conversations. This
includes various dimensions of conversation quality with human evaluation
directly on the synthesized conversations, and interactive human evaluation of
chatbots fine-tuned on the synthetically generated dataset. We additionally
demonstrate that this prompting approach is generalizable to multi-party
conversations, providing potential to create new synthetic data for multi-party
tasks. Our synthetic multi-party conversations were rated more favorably across
all measured dimensions compared to conversation excerpts sampled from a
human-collected multi-party dataset.
</p></li>
</ul>

<h3>Title: One-shot Empirical Privacy Estimation for Federated Learning. (arXiv:2302.03098v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03098">http://arxiv.org/abs/2302.03098</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03098] One-shot Empirical Privacy Estimation for Federated Learning](http://arxiv.org/abs/2302.03098) #privacy</code></li>
<li>Summary: <p>Privacy auditing techniques for differentially private (DP) algorithms are
useful for estimating the privacy loss to compare against analytical bounds, or
empirically measure privacy in settings where known analytical bounds on the DP
loss are not tight. However, existing privacy auditing techniques usually make
strong assumptions on the adversary (e.g., knowledge of intermediate model
iterates or the training data distribution), are tailored to specific tasks and
model architectures, and require retraining the model many times (typically on
the order of thousands). These shortcomings make deploying such techniques at
scale difficult in practice, especially in federated settings where model
training can take days or weeks. In this work, we present a novel "one-shot"
approach that can systematically address these challenges, allowing efficient
auditing or estimation of the privacy loss of a model during the same, single
training run used to fit model parameters. Our privacy auditing method for
federated learning does not require a priori knowledge about the model
architecture or task. We show that our method provides provably correct
estimates for privacy loss under the Gaussian mechanism, and we demonstrate its
performance on a well-established FL benchmark dataset under several
adversarial models.
</p></li>
</ul>

<h3>Title: Differential Privacy with Higher Utility through Non-identical Additive Noise. (arXiv:2302.03511v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03511">http://arxiv.org/abs/2302.03511</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03511] Differential Privacy with Higher Utility through Non-identical Additive Noise](http://arxiv.org/abs/2302.03511) #privacy</code></li>
<li>Summary: <p>Differential privacy is typically ensured by perturbation with additive noise
that is sampled from a known distribution. Conventionally, independent and
identically distributed (i.i.d.) noise samples are added to each coordinate. In
this work, propose to add noise which is independent, but not identically
distributed (i.n.i.d.) across the coordinates. In particular, we study the
i.n.i.d. Gaussian and Laplace mechanisms and obtain the conditions under which
these mechanisms guarantee privacy. The optimal choice of parameters that
ensure these conditions are derived theoretically. Theoretical analyses and
numerical simulations show that the i.n.i.d. mechanisms achieve higher utility
for the given privacy requirements compared to their i.i.d. counterparts.
</p></li>
</ul>

<h3>Title: Towards a User Privacy-Aware Mobile Gaming App Installation Prediction Model. (arXiv:2302.03332v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03332">http://arxiv.org/abs/2302.03332</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03332] Towards a User Privacy-Aware Mobile Gaming App Installation Prediction Model](http://arxiv.org/abs/2302.03332) #privacy</code></li>
<li>Summary: <p>Over the past decade, programmatic advertising has received a great deal of
attention in the online advertising industry. A real-time bidding (RTB) system
is rapidly becoming the most popular method to buy and sell online advertising
impressions. Within the RTB system, demand-side platforms (DSP) aim to spend
advertisers' campaign budgets efficiently while maximizing profit, seeking
impressions that result in high user responses, such as clicks or installs. In
the current study, we investigate the process of predicting a mobile gaming app
installation from the point of view of a particular DSP, while paying attention
to user privacy, and exploring the trade-off between privacy preservation and
model performance. There are multiple levels of potential threats to user
privacy, depending on the privacy leaks associated with the data-sharing
process, such as data transformation or de-anonymization. To address these
concerns, privacy-preserving techniques were proposed, such as cryptographic
approaches, for training privacy-aware machine-learning models. However, the
ability to train a mobile gaming app installation prediction model without
using user-level data, can prevent these threats and protect the users'
privacy, even though the model's ability to predict may be impaired.
Additionally, current laws might force companies to declare that they are
collecting data, and might even give the user the option to opt out of such
data collection, which might threaten companies' business models in digital
advertising, which are dependent on the collection and use of user-level data.
We conclude that privacy-aware models might still preserve significant
capabilities, enabling companies to make better decisions, dependent on the
privacy-efficacy trade-off utility function of each case.
</p></li>
</ul>

<h3>Title: A Privacy-Preserving Hybrid Federated Learning Framework for Financial Crime Detection. (arXiv:2302.03654v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03654">http://arxiv.org/abs/2302.03654</a></li>
<li>Code URL: <a href="https://github.com/illidanlab/hyfl">https://github.com/illidanlab/hyfl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03654] A Privacy-Preserving Hybrid Federated Learning Framework for Financial Crime Detection](http://arxiv.org/abs/2302.03654) #privacy</code></li>
<li>Summary: <p>The recent decade witnessed a surge of increase in financial crimes across
the public and private sectors, with an average cost of scams of \$102m to
financial institutions in 2022. Developing a mechanism for battling financial
crimes is an impending task that requires in-depth collaboration from multiple
institutions, and yet such collaboration imposed significant technical
challenges due to the privacy and security requirements of distributed
financial data. For example, consider the Society for Worldwide Interbank
Financial Telecommunications (SWIFT) system, which generates 42 million
transactions per day across its 11,000 global institutions. Training a
detection model of fraudulent transactions requires not only secured SWIFT
transactions but also the private account activities of those involved in each
transaction from corresponding bank systems. The distributed nature of both
samples and features prevents most existing learning systems from being
directly adopted to handle the data mining task. In this paper, we collectively
address these challenges by proposing a hybrid federated learning system that
offers secure and privacy-aware learning and inference for financial crime
detection. We conduct extensive empirical studies to evaluate the proposed
framework's detection performance and privacy-protection capability, evaluating
its robustness against common malicious attacks of collaborative learning. We
release our source code at https://github.com/illidanlab/HyFL .
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Protecting Language Generation Models via Invisible Watermarking. (arXiv:2302.03162v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03162">http://arxiv.org/abs/2302.03162</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03162] Protecting Language Generation Models via Invisible Watermarking](http://arxiv.org/abs/2302.03162) #protect</code></li>
<li>Summary: <p>Language generation models have been an increasingly powerful enabler for
many applications. Many such models offer free or affordable API access, which
makes them potentially vulnerable to model extraction attacks through
distillation. To protect intellectual property (IP) and ensure fair use of
these models, various techniques such as lexical watermarking and synonym
replacement have been proposed. However, these methods can be nullified by
obvious countermeasures such as "synonym randomization". To address this issue,
we propose GINSEW, a novel method to protect text generation models from being
stolen through distillation. The key idea of our method is to inject secret
signals into the probability vector of the decoding steps for each target
token. We can then detect the secret message by probing a suspect model to tell
if it is distilled from the protected one. Experimental results show that
GINSEW can effectively identify instances of IP infringement with minimal
impact on the generation quality of protected APIs. Our method demonstrates an
absolute improvement of 19 to 29 points on mean average precision (mAP) in
detecting suspects compared to previous methods against watermark removal
attacks.
</p></li>
</ul>

<h3>Title: Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning. (arXiv:2302.03281v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03281">http://arxiv.org/abs/2302.03281</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03281] Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning](http://arxiv.org/abs/2302.03281) #protect</code></li>
<li>Summary: <p>Modern representation learning methods may fail to adapt quickly under
non-stationarity since they suffer from the problem of catastrophic forgetting
and decaying plasticity. Such problems prevent learners from fast adaptation to
changes since they result in increasing numbers of saturated features and
forgetting useful features when presented with new experiences. Hence, these
methods are rendered ineffective for continual learning. This paper proposes
Utility-based Perturbed Gradient Descent (UPGD), an online
representation-learning algorithm well-suited for continual learning agents
with no knowledge about task boundaries. UPGD protects useful weights or
features from forgetting and perturbs less useful ones based on their
utilities. Our empirical results show that UPGD alleviates catastrophic
forgetting and decaying plasticity, enabling modern representation learning
methods to work in the continual learning setting.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency. (arXiv:2302.03251v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03251">http://arxiv.org/abs/2302.03251</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03251] SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency](http://arxiv.org/abs/2302.03251) #defense</code></li>
<li>Summary: <p>Deep neural networks (DNNs) are vulnerable to backdoor attacks, where
adversaries embed a hidden backdoor trigger during the training process for
malicious prediction manipulation. These attacks pose great threats to the
applications of DNNs under the real-world machine learning as a service (MLaaS)
setting, where the deployed model is fully black-box while the users can only
query and obtain its predictions. Currently, there are many existing defenses
to reduce backdoor threats. However, almost all of them cannot be adopted in
MLaaS scenarios since they require getting access to or even modifying the
suspicious models. In this paper, we propose a simple yet effective black-box
input-level backdoor detection, called SCALE-UP, which requires only the
predicted labels to alleviate this problem. Specifically, we identify and
filter malicious testing samples by analyzing their prediction consistency
during the pixel-wise amplification process. Our defense is motivated by an
intriguing observation (dubbed scaled prediction consistency) that the
predictions of poisoned samples are significantly more consistent compared to
those of benign ones when amplifying all pixel values. Besides, we also provide
theoretical foundations to explain this phenomenon. Extensive experiments are
conducted on benchmark datasets, verifying the effectiveness and efficiency of
our defense and its resistance to potential adaptive attacks. Our codes are
available at https://github.com/JunfengGo/SCALE-UP.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Membership Inference Attacks against Diffusion Models. (arXiv:2302.03262v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03262">http://arxiv.org/abs/2302.03262</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03262] Membership Inference Attacks against Diffusion Models](http://arxiv.org/abs/2302.03262) #attack</code></li>
<li>Summary: <p>Diffusion models have attracted attention in recent years as innovative
generative models. In this paper, we investigate whether a diffusion model is
resistant to a membership inference attack, which evaluates the privacy leakage
of a machine learning model. We primarily discuss the diffusion model from the
standpoints of comparison with a generative adversarial network (GAN) as
conventional models and hyperparameters unique to the diffusion model, i.e.,
time steps, sampling steps, and sampling variances. We conduct extensive
experiments with DDIM as a diffusion model and DCGAN as a GAN on the CelebA and
CIFAR-10 datasets in both white-box and black-box settings and then confirm if
the diffusion model is comparably resistant to a membership inference attack as
GAN. Next, we demonstrate that the impact of time steps is significant and
intermediate steps in a noise schedule are the most vulnerable to the attack.
We also found two key insights through further analysis. First, we identify
that DDIM is vulnerable to the attack for small sample sizes instead of
achieving a lower FID. Second, sampling steps in hyperparameters are important
for resistance to the attack, whereas the impact of sampling variances is quite
limited.
</p></li>
</ul>

<h3>Title: Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority Influence. (arXiv:2302.03322v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03322">http://arxiv.org/abs/2302.03322</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03322] Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority Influence](http://arxiv.org/abs/2302.03322) #attack</code></li>
<li>Summary: <p>Cooperative multi-agent reinforcement learning (c-MARL) offers a general
paradigm for a group of agents to achieve a shared goal by taking individual
decisions, yet is found to be vulnerable to adversarial attacks. Though
harmful, adversarial attacks also play a critical role in evaluating the
robustness and finding blind spots of c-MARL algorithms. However, existing
attacks are not sufficiently strong and practical, which is mainly due to the
ignorance of complex influence between agents and cooperative nature of victims
in c-MARL.
</p></li>
</ul>

<p>In this paper, we propose adversarial minority influence (AMI), the first
practical attack against c-MARL by introducing an adversarial agent. AMI
addresses the aforementioned problems by unilaterally influencing other
cooperative victims to a targeted worst-case cooperation. Technically, to
maximally deviate victim policy under complex agent-wise influence, our
unilateral attack characterize and maximize the influence from adversary to
victims. This is done by adapting a unilateral agent-wise relation metric
derived from mutual information, which filters out the detrimental influence
from victims to adversary. To fool victims into a jointly worst-case failure,
our targeted attack influence victims to a long-term, cooperatively worst case
by distracting each victim to a specific target. Such target is learned by a
reinforcement learning agent in a trial-and-error process. Extensive
experiments in simulation environments, including discrete control (SMAC),
continuous control (MAMujoco) and real-world robot swarm control demonstrate
the superiority of our AMI approach. Our codes are available in
https://anonymous.4open.science/r/AMI.
</p>

<h2>robust</h2>
<h3>Title: High-Resolution GAN Inversion for Degraded Images in Large Diverse Datasets. (arXiv:2302.03406v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03406">http://arxiv.org/abs/2302.03406</a></li>
<li>Code URL: <a href="https://github.com/booooooooooo/cri">https://github.com/booooooooooo/cri</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03406] High-Resolution GAN Inversion for Degraded Images in Large Diverse Datasets](http://arxiv.org/abs/2302.03406) #robust</code></li>
<li>Summary: <p>The last decades are marked by massive and diverse image data, which shows
increasingly high resolution and quality. However, some images we obtained may
be corrupted, affecting the perception and the application of downstream tasks.
A generic method for generating a high-quality image from the degraded one is
in demand. In this paper, we present a novel GAN inversion framework that
utilizes the powerful generative ability of StyleGAN-XL for this problem. To
ease the inversion challenge with StyleGAN-XL, Clustering \&amp; Regularize
Inversion (CRI) is proposed. Specifically, the latent space is firstly divided
into finer-grained sub-spaces by clustering. Instead of initializing the
inversion with the average latent vector, we approximate a centroid latent
vector from the clusters, which generates an image close to the input image.
Then, an offset with a regularization term is introduced to keep the inverted
latent vector within a certain range. We validate our CRI scheme on multiple
restoration tasks (i.e., inpainting, colorization, and super-resolution) of
complex natural images, and show preferable quantitative and qualitative
results. We further demonstrate our technique is robust in terms of data and
different GAN models. To our best knowledge, we are the first to adopt
StyleGAN-XL for generating high-quality natural images from diverse degraded
inputs. Code is available at https://github.com/Booooooooooo/CRI.
</p></li>
</ul>

<h3>Title: SimCon Loss with Multiple Views for Text Supervised Semantic Segmentation. (arXiv:2302.03432v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03432">http://arxiv.org/abs/2302.03432</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03432] SimCon Loss with Multiple Views for Text Supervised Semantic Segmentation](http://arxiv.org/abs/2302.03432) #robust</code></li>
<li>Summary: <p>Learning to segment images purely by relying on the image-text alignment from
web data can lead to sub-optimal performance due to noise in the data. The
noise comes from the samples where the associated text does not correlate with
the image's visual content. Instead of purely relying on the alignment from the
noisy data, this paper proposes a novel loss function termed SimCon, which
accounts for intra-modal similarities to determine the appropriate set of
positive samples to align. Further, using multiple views of the image (created
synthetically) for training and combining the SimCon loss with it makes the
training more robust. This version of the loss is termed MV-SimCon. The
empirical results demonstrate that using the proposed loss function leads to
consistent improvements on zero-shot, text supervised semantic segmentation and
outperforms state-of-the-art by $+3.0\%$, $+3.3\%$ and $+6.9\%$ on PASCAL VOC,
PASCAL Context and MSCOCO, respectively. With test time augmentations, we set a
new record by improving these results further to $58.7\%$, $26.6\%$, and
$33.3\%$ on PASCAL VOC, PASCAL Context, and MSCOCO, respectively. In addition,
using the proposed loss function leads to robust training and faster
convergence.
</p></li>
</ul>

<h3>Title: Sparse Mixture Once-for-all Adversarial Training for Efficient In-Situ Trade-Off Between Accuracy and Robustness of DNNs. (arXiv:2302.03523v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03523">http://arxiv.org/abs/2302.03523</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03523] Sparse Mixture Once-for-all Adversarial Training for Efficient In-Situ Trade-Off Between Accuracy and Robustness of DNNs](http://arxiv.org/abs/2302.03523) #robust</code></li>
<li>Summary: <p>Existing deep neural networks (DNNs) that achieve state-of-the-art (SOTA)
performance on both clean and adversarially-perturbed images rely on either
activation or weight conditioned convolution operations. However, such
conditional learning costs additional multiply-accumulate (MAC) or addition
operations, increasing inference memory and compute costs. To that end, we
present a sparse mixture once for all adversarial training (SMART), that allows
a model to train once and then in-situ trade-off between accuracy and
robustness, that too at a reduced compute and parameter overhead. In
particular, SMART develops two expert paths, for clean and adversarial images,
respectively, that are then conditionally trained via respective dedicated sets
of binary sparsity masks. Extensive evaluations on multiple image
classification datasets across different models show SMART to have up to 2.72x
fewer non-zero parameters costing proportional reduction in compute overhead,
while yielding SOTA accuracy-robustness trade-off. Additionally, we present
insightful observations in designing sparse masks to successfully condition on
both clean and perturbed images.
</p></li>
</ul>

<h3>Title: Capturing Topic Framing via Masked Language Modeling. (arXiv:2302.03183v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03183">http://arxiv.org/abs/2302.03183</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03183] Capturing Topic Framing via Masked Language Modeling](http://arxiv.org/abs/2302.03183) #robust</code></li>
<li>Summary: <p>Differential framing of issues can lead to divergent world views on important
issues. This is especially true in domains where the information presented can
reach a large audience, such as traditional and social media. Scalable and
reliable measurement of such differential framing is an important first step in
addressing them. In this work, based on the intuition that framing affects the
tone and word choices in written language, we propose a framework for modeling
the differential framing of issues through masked token prediction via
large-scale fine-tuned language models (LMs). Specifically, we explore three
key factors for our framework: 1) prompt generation methods for the masked
token prediction; 2) methods for normalizing the output of fine-tuned LMs; 3)
robustness to the choice of pre-trained LMs used for fine-tuning. Through
experiments on a dataset of articles from traditional media outlets covering
five diverse and politically polarized topics, we show that our framework can
capture differential framing of these topics with high reliability.
</p></li>
</ul>

<h3>Title: APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning. (arXiv:2302.03488v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03488">http://arxiv.org/abs/2302.03488</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03488] APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning](http://arxiv.org/abs/2302.03488) #robust</code></li>
<li>Summary: <p>Practical natural language processing (NLP) tasks are commonly long-tailed
with noisy labels. Those problems challenge the generalization and robustness
of complex models such as Deep Neural Networks (DNNs). Some commonly used
resampling techniques, such as oversampling or undersampling, could easily lead
to overfitting. It is growing popular to learn the data weights leveraging a
small amount of metadata. Besides, recent studies have shown the advantages of
self-supervised pre-training, particularly to the under-represented data. In
this work, we propose a general framework to handle the problem of both
long-tail and noisy labels. The model is adapted to the domain of problems in a
contrastive learning manner. The re-weighting module is a feed-forward network
that learns explicit weighting functions and adapts weights according to
metadata. The framework further adapts weights of terms in the loss function
through a combination of the polynomial expansion of cross-entropy loss and
focal loss. Our extensive experiments show that the proposed framework
consistently outperforms baseline methods. Lastly, our sensitive analysis
emphasizes the capability of the proposed framework to handle the long-tailed
problem and mitigate the negative impact of noisy labels.
</p></li>
</ul>

<h3>Title: Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery. (arXiv:2302.03668v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03668">http://arxiv.org/abs/2302.03668</a></li>
<li>Code URL: <a href="https://github.com/YuxinWenRick/hard-prompts-made-easy">https://github.com/YuxinWenRick/hard-prompts-made-easy</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03668] Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery](http://arxiv.org/abs/2302.03668) #robust</code></li>
<li>Summary: <p>The strength of modern generative models lies in their ability to be
controlled through text-based prompts. Typical "hard" prompts are made from
interpretable words and tokens, and must be hand-crafted by humans. There are
also "soft" prompts, which consist of continuous feature vectors. These can be
discovered using powerful optimization methods, but they cannot be easily
interpreted, re-used across models, or plugged into a text-based interface.
</p></li>
</ul>

<p>We describe an approach to robustly optimize hard text prompts through
efficient gradient-based optimization. Our approach automatically generates
hard text-based prompts for both text-to-image and text-to-text applications.
In the text-to-image setting, the method creates hard prompts for diffusion
models, allowing API users to easily generate, discover, and mix and match
image concepts without prior knowledge on how to prompt the model. In the
text-to-text setting, we show that hard prompts can be automatically discovered
that are effective in tuning LMs for classification.
</p>

<h3>Title: Temporal Robustness against Data Poisoning. (arXiv:2302.03684v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03684">http://arxiv.org/abs/2302.03684</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03684] Temporal Robustness against Data Poisoning](http://arxiv.org/abs/2302.03684) #robust</code></li>
<li>Summary: <p>Data poisoning considers cases when an adversary maliciously inserts and
removes training data to manipulate the behavior of machine learning
algorithms. Traditional threat models of data poisoning center around a single
metric, the number of poisoned samples. In consequence, existing defenses are
essentially vulnerable in practice when poisoning more samples remains a
feasible option for attackers. To address this issue, we leverage timestamps
denoting the birth dates of data, which are often available but neglected in
the past. Benefiting from these timestamps, we propose a temporal threat model
of data poisoning and derive two novel metrics, earliness and duration, which
respectively measure how long an attack started in advance and how long an
attack lasted. With these metrics, we define the notions of temporal robustness
against data poisoning, providing a meaningful sense of protection even with
unbounded amounts of poisoned samples. We present a benchmark with an
evaluation protocol simulating continuous data collection and periodic
deployments of updated models, thus enabling empirical evaluation of temporal
robustness. Lastly, we develop and also empirically verify a baseline defense,
namely temporal aggregation, offering provable temporal robustness and
highlighting the potential of our temporal modeling of data poisoning.
</p></li>
</ul>

<h3>Title: Autodecompose: A generative self-supervised model for semantic decomposition. (arXiv:2302.03124v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03124">http://arxiv.org/abs/2302.03124</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03124] Autodecompose: A generative self-supervised model for semantic decomposition](http://arxiv.org/abs/2302.03124) #robust</code></li>
<li>Summary: <p>We introduce Autodecompose, a novel self-supervised generative model that
decomposes data into two semantically independent properties: the desired
property, which captures a specific aspect of the data (e.g. the voice in an
audio signal), and the context property, which aggregates all other information
(e.g. the content of the audio signal), without any labels given. Autodecompose
uses two complementary augmentations, one that manipulates the context while
preserving the desired property and the other that manipulates the desired
property while preserving the context. The augmented variants of the data are
encoded by two encoders and reconstructed by a decoder. We prove that one of
the encoders embeds the desired property while the other embeds the context
property. We apply Autodecompose to audio signals to encode sound source (human
voice) and content. We pre-trained the model on YouTube and LibriSpeech
datasets and fine-tuned in a self-supervised manner without exposing the
labels. Our results showed that, using the sound source encoder of pre-trained
Autodecompose, a linear classifier achieves F1 score of 97.6\% in recognizing
the voice of 30 speakers using only 10 seconds of labeled samples, compared to
95.7\% for supervised models. Additionally, our experiments showed that
Autodecompose is robust against overfitting even when a large model is
pre-trained on a small dataset. A large Autodecompose model was pre-trained
from scratch on 60 seconds of audio from 3 speakers achieved over 98.5\% F1
score in recognizing those three speakers in other unseen utterances. We
finally show that the context encoder embeds information about the content of
the speech and ignores the sound source information.
</p></li>
</ul>

<p>Our sample code for training the model, as well as examples for using the
pre-trained models are available here:
\url{https://github.com/rezabonyadi/autodecompose}
</p>

<h3>Title: Optimization using Parallel Gradient Evaluations on Multiple Parameters. (arXiv:2302.03161v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03161">http://arxiv.org/abs/2302.03161</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03161] Optimization using Parallel Gradient Evaluations on Multiple Parameters](http://arxiv.org/abs/2302.03161) #robust</code></li>
<li>Summary: <p>We propose a first-order method for convex optimization, where instead of
being restricted to the gradient from a single parameter, gradients from
multiple parameters can be used during each step of gradient descent. This
setup is particularly useful when a few processors are available that can be
used in parallel for optimization. Our method uses gradients from multiple
parameters in synergy to update these parameters together towards the optima.
While doing so, it is ensured that the computational and memory complexity is
of the same order as that of gradient descent. Empirical results demonstrate
that even using gradients from as low as \textit{two} parameters, our method
can often obtain significant acceleration and provide robustness to
hyper-parameter settings. We remark that the primary goal of this work is less
theoretical, and is instead aimed at exploring the understudied case of using
multiple gradients during each step of optimization.
</p></li>
</ul>

<h3>Title: Deep-OSG: A deep learning approach for approximating a family of operators in semigroup to model unknown autonomous systems. (arXiv:2302.03358v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03358">http://arxiv.org/abs/2302.03358</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03358] Deep-OSG: A deep learning approach for approximating a family of operators in semigroup to model unknown autonomous systems](http://arxiv.org/abs/2302.03358) #robust</code></li>
<li>Summary: <p>This paper proposes a novel deep learning approach for approximating
evolution operators and modeling unknown autonomous dynamical systems using
time series data collected at varied time lags. It is a sequel to the previous
works [T. Qin, K. Wu, and D. Xiu, J. Comput. Phys., 395:620--635, 2019], [K. Wu
and D. Xiu, J. Comput. Phys., 408:109307, 2020], and [Z. Chen, V. Churchill, K.
Wu, and D. Xiu, J. Comput. Phys., 449:110782, 2022], which focused on learning
single evolution operator with a fixed time step. This paper aims to learn a
family of evolution operators with variable time steps, which constitute a
semigroup for an autonomous system. The semigroup property is very crucial and
links the system's evolutionary behaviors across varying time scales, but it
was not considered in the previous works. We propose for the first time a
framework of embedding the semigroup property into the data-driven learning
process, through a novel neural network architecture and new loss functions.
The framework is very feasible, can be combined with any suitable neural
networks, and is applicable to learning general autonomous ODEs and PDEs. We
present the rigorous error estimates and variance analysis to understand the
prediction accuracy and robustness of our approach, showing the remarkable
advantages of semigroup awareness in our model. Moreover, our approach allows
one to arbitrarily choose the time steps for prediction and ensures that the
predicted results are well self-matched and consistent. Extensive numerical
experiments demonstrate that embedding the semigroup property notably reduces
the data dependency of deep learning models and greatly improves the accuracy,
robustness, and stability for long-time prediction.
</p></li>
</ul>

<h3>Title: Robustness Implies Fairness in Casual Algorithmic Recourse. (arXiv:2302.03465v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03465">http://arxiv.org/abs/2302.03465</a></li>
<li>Code URL: <a href="https://github.com/ehyaei/robustness-implies-fairness">https://github.com/ehyaei/robustness-implies-fairness</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03465] Robustness Implies Fairness in Casual Algorithmic Recourse](http://arxiv.org/abs/2302.03465) #robust</code></li>
<li>Summary: <p>Algorithmic recourse aims to disclose the inner workings of the black-box
decision process in situations where decisions have significant consequences,
by providing recommendations to empower beneficiaries to achieve a more
favorable outcome. To ensure an effective remedy, suggested interventions must
not only be low-cost but also robust and fair. This goal is accomplished by
providing similar explanations to individuals who are alike. This study
explores the concept of individual fairness and adversarial robustness in
causal algorithmic recourse and addresses the challenge of achieving both. To
resolve the challenges, we propose a new framework for defining adversarially
robust recourse. The new setting views the protected feature as a pseudometric
and demonstrates that individual fairness is a special case of adversarial
robustness. Finally, we introduce the fair robust recourse problem to achieve
both desirable properties and show how it can be satisfied both theoretically
and empirically.
</p></li>
</ul>

<h3>Title: Towards Robust Inductive Graph Incremental Learning via Experience Replay. (arXiv:2302.03534v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03534">http://arxiv.org/abs/2302.03534</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03534] Towards Robust Inductive Graph Incremental Learning via Experience Replay](http://arxiv.org/abs/2302.03534) #robust</code></li>
<li>Summary: <p>Inductive node-wise graph incremental learning is a challenging task due to
the dynamic nature of evolving graphs and the dependencies between nodes. In
this paper, we propose a novel experience replay framework, called
Structure-Evolution-Aware Experience Replay (SEA-ER), that addresses these
challenges by leveraging the topological awareness of GNNs and importance
reweighting technique. Our framework effectively addresses the data dependency
of node prediction problems in evolving graphs, with a theoretical guarantee
that supports its effectiveness. Through empirical evaluation, we demonstrate
that our proposed framework outperforms the current state-of-the-art GNN
experience replay methods on several benchmark datasets, as measured by metrics
such as accuracy and forgetting.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Toward Face Biometric De-identification using Adversarial Examples. (arXiv:2302.03657v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03657">http://arxiv.org/abs/2302.03657</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03657] Toward Face Biometric De-identification using Adversarial Examples](http://arxiv.org/abs/2302.03657) #biometric</code></li>
<li>Summary: <p>The remarkable success of face recognition (FR) has endangered the privacy of
internet users particularly in social media. Recently, researchers turned to
use adversarial examples as a countermeasure. In this paper, we assess the
effectiveness of using two widely known adversarial methods (BIM and ILLC) for
de-identifying personal images. We discovered, unlike previous claims in the
literature, that it is not easy to get a high protection success rate
(suppressing identification rate) with imperceptible adversarial perturbation
to the human visual system. Finally, we found out that the transferability of
adversarial examples is highly affected by the training parameters of the
network with which they are generated.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Natural Language Processing for Policymaking. (arXiv:2302.03490v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03490">http://arxiv.org/abs/2302.03490</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03490] Natural Language Processing for Policymaking](http://arxiv.org/abs/2302.03490) #extraction</code></li>
<li>Summary: <p>Language is the medium for many political activities, from campaigns to news
reports. Natural language processing (NLP) uses computational tools to parse
text into key information that is needed for policymaking. In this chapter, we
introduce common methods of NLP, including text classification, topic modeling,
event extraction, and text scaling. We then overview how these methods can be
used for policymaking through four major applications including data collection
for evidence-based policymaking, interpretation of political decisions, policy
communication, and investigation of policy effects. Finally, we highlight some
potential limitations and ethical concerns when using NLP for policymaking.
</p></li>
</ul>

<p>This text is from Chapter 7 (pages 141-162) of the Handbook of Computational
Social Science for Policy (2023). Open Access on Springer:
https://doi.org/10.1007/978-3-031-16624-2
</p>

<h3>Title: A Survey on Arabic Named Entity Recognition: Past, Recent Advances, and Future Trends. (arXiv:2302.03512v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03512">http://arxiv.org/abs/2302.03512</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03512] A Survey on Arabic Named Entity Recognition: Past, Recent Advances, and Future Trends](http://arxiv.org/abs/2302.03512) #extraction</code></li>
<li>Summary: <p>As more and more Arabic texts emerged on the Internet, extracting important
information from these Arabic texts is especially useful. As a fundamental
technology, Named entity recognition (NER) serves as the core component in
information extraction technology, while also playing a critical role in many
other Natural Language Processing (NLP) systems, such as question answering and
knowledge graph building. In this paper, we provide a comprehensive review of
the development of Arabic NER, especially the recent advances in deep learning
and pre-trained language model. Specifically, we first introduce the background
of Arabic NER, including the characteristics of Arabic and existing resources
for Arabic NER. Then, we systematically review the development of Arabic NER
methods. Traditional Arabic NER systems focus on feature engineering and
designing domain-specific rules. In recent years, deep learning methods achieve
significant progress by representing texts via continuous vector
representations. With the growth of pre-trained language model, Arabic NER
yields better performance. Finally, we conclude the method gap between Arabic
NER and NER methods from other languages, which helps outline future directions
for Arabic NER.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: On the Convergence of Federated Averaging with Cyclic Client Participation. (arXiv:2302.03109v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03109">http://arxiv.org/abs/2302.03109</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03109] On the Convergence of Federated Averaging with Cyclic Client Participation](http://arxiv.org/abs/2302.03109) #federate</code></li>
<li>Summary: <p>Federated Averaging (FedAvg) and its variants are the most popular
optimization algorithms in federated learning (FL). Previous convergence
analyses of FedAvg either assume full client participation or partial client
participation where the clients can be uniformly sampled. However, in practical
cross-device FL systems, only a subset of clients that satisfy local criteria
such as battery status, network connectivity, and maximum participation
frequency requirements (to ensure privacy) are available for training at a
given time. As a result, client availability follows a natural cyclic pattern.
We provide (to our knowledge) the first theoretical framework to analyze the
convergence of FedAvg with cyclic client participation with several different
client optimizers such as GD, SGD, and shuffled SGD. Our analysis discovers
that cyclic client participation can achieve a faster asymptotic convergence
rate than vanilla FedAvg with uniform client participation under suitable
conditions, providing valuable insights into the design of client sampling
protocols.
</p></li>
</ul>

<h3>Title: Federated Learning with Regularized Client Participation. (arXiv:2302.03662v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03662">http://arxiv.org/abs/2302.03662</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03662] Federated Learning with Regularized Client Participation](http://arxiv.org/abs/2302.03662) #federate</code></li>
<li>Summary: <p>Federated Learning (FL) is a distributed machine learning approach where
multiple clients work together to solve a machine learning task. One of the key
challenges in FL is the issue of partial participation, which occurs when a
large number of clients are involved in the training process. The traditional
method to address this problem is randomly selecting a subset of clients at
each communication round. In our research, we propose a new technique and
design a novel regularized client participation scheme. Under this scheme, each
client joins the learning process every $R$ communication rounds, which we
refer to as a meta epoch. We have found that this participation scheme leads to
a reduction in the variance caused by client sampling. Combined with the
popular FedAvg algorithm (McMahan et al., 2017), it results in superior rates
under standard assumptions. For instance, the optimization term in our main
convergence bound decreases linearly with the product of the number of
communication rounds and the size of the local dataset of each client, and the
statistical term scales with step size quadratically instead of linearly (the
case for client sampling with replacement), leading to better convergence rate
$\mathcal{O}\left(\frac{1}{T^2}\right)$ compared to
$\mathcal{O}\left(\frac{1}{T}\right)$, where $T$ is the total number of
communication rounds. Furthermore, our results permit arbitrary client
availability as long as each client is available for training once per each
meta epoch.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Deep Class-Incremental Learning: A Survey. (arXiv:2302.03648v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03648">http://arxiv.org/abs/2302.03648</a></li>
<li>Code URL: <a href="https://github.com/zhoudw-zdw/cil_survey">https://github.com/zhoudw-zdw/cil_survey</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03648] Deep Class-Incremental Learning: A Survey](http://arxiv.org/abs/2302.03648) #fair</code></li>
<li>Summary: <p>Deep models, e.g., CNNs and Vision Transformers, have achieved impressive
achievements in many vision tasks in the closed world. However, novel classes
emerge from time to time in our ever-changing world, requiring a learning
system to acquire new knowledge continually. For example, a robot needs to
understand new instructions, and an opinion monitoring system should analyze
emerging topics every day. Class-Incremental Learning (CIL) enables the learner
to incorporate the knowledge of new classes incrementally and build a universal
classifier among all seen classes. Correspondingly, when directly training the
model with new class instances, a fatal problem occurs -- the model tends to
catastrophically forget the characteristics of former ones, and its performance
drastically degrades. There have been numerous efforts to tackle catastrophic
forgetting in the machine learning community. In this paper, we survey
comprehensively recent advances in deep class-incremental learning and
summarize these methods from three aspects, i.e., data-centric, model-centric,
and algorithm-centric. We also provide a rigorous and unified evaluation of 16
methods in benchmark image classification tasks to find out the characteristics
of different algorithms empirically. Furthermore, we notice that the current
comparison protocol ignores the influence of memory budget in model storage,
which may result in unfair comparison and biased results. Hence, we advocate
fair comparison by aligning the memory budget in evaluation, as well as several
memory-agnostic performance measures. The source code to reproduce these
evaluations is available at https://github.com/zhoudw-zdw/CIL_Survey/
</p></li>
</ul>

<h3>Title: Memory-Based Meta-Learning on Non-Stationary Distributions. (arXiv:2302.03067v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03067">http://arxiv.org/abs/2302.03067</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03067] Memory-Based Meta-Learning on Non-Stationary Distributions](http://arxiv.org/abs/2302.03067) #fair</code></li>
<li>Summary: <p>Memory-based meta-learning is a technique for approximating Bayes-optimal
predictors. Under fairly general conditions, minimizing sequential prediction
error, measured by the log loss, leads to implicit meta-learning. The goal of
this work is to investigate how far this interpretation can be realized by
current sequence prediction models and training regimes. The focus is on
piecewise stationary sources with unobserved switching-points, which arguably
capture an important characteristic of natural language and action-observation
sequences in partially observable environments. We show that various types of
memory-based neural models, including Transformers, LSTMs, and RNNs can learn
to accurately approximate known Bayes-optimal algorithms and behave as if
performing Bayesian inference over the latent switching-points and the latent
parameters governing the data distribution within each segment.
</p></li>
</ul>

<h3>Title: Fair Minimum Representation Clustering. (arXiv:2302.03151v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03151">http://arxiv.org/abs/2302.03151</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03151] Fair Minimum Representation Clustering](http://arxiv.org/abs/2302.03151) #fair</code></li>
<li>Summary: <p>Clustering is an unsupervised learning task that aims to partition data into
a set of clusters. In many applications, these clusters correspond to
real-world constructs (e.g. electoral districts) whose benefit can only be
attained by groups when they reach a minimum level of representation (e.g. 50\%
to elect their desired candidate). This paper considers the problem of
performing k-means clustering while ensuring groups (e.g. demographic groups)
have that minimum level of representation in a specified number of clusters. We
show that the popular $k$-means algorithm, Lloyd's algorithm, can result in
unfair outcomes where certain groups lack sufficient representation past the
minimum threshold in a proportional number of clusters. We formulate the
problem through a mixed-integer optimization framework and present a variant of
Lloyd's algorithm, called MiniReL, that directly incorporates the fairness
constraints. We show that incorporating the fairness criteria leads to a
NP-Hard sub-problem within Lloyd's algorithm, but we provide computational
approaches that make the problem tractable for even large datasets. Numerical
results show that the approach is able to create fairer clusters with
practically no increase in the k-means clustering cost across standard
benchmark datasets.
</p></li>
</ul>

<h3>Title: A conceptual model for leaving the data-centric approach in machine learning. (arXiv:2302.03361v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03361">http://arxiv.org/abs/2302.03361</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03361] A conceptual model for leaving the data-centric approach in machine learning](http://arxiv.org/abs/2302.03361) #fair</code></li>
<li>Summary: <p>For a long time, machine learning (ML) has been seen as the abstract problem
of learning relationships from data independent of the surrounding settings.
This has recently been challenged, and methods have been proposed to include
external constraints in the machine learning models. These methods usually come
from application-specific fields, such as de-biasing algorithms in the field of
fairness in ML or physical constraints in the fields of physics and
engineering. In this paper, we present and discuss a conceptual high-level
model that unifies these approaches in a common language. We hope that this
will enable and foster exchange between the different fields and their
different methods for including external constraints into ML models, and thus
leaving purely data-centric approaches.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Explainable Action Prediction through Self-Supervision on Scene Graphs. (arXiv:2302.03477v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03477">http://arxiv.org/abs/2302.03477</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03477] Explainable Action Prediction through Self-Supervision on Scene Graphs](http://arxiv.org/abs/2302.03477) #interpretability</code></li>
<li>Summary: <p>This work explores scene graphs as a distilled representation of high-level
information for autonomous driving, applied to future driver-action prediction.
Given the scarcity and strong imbalance of data samples, we propose a
self-supervision pipeline to infer representative and well-separated
embeddings. Key aspects are interpretability and explainability; as such, we
embed in our architecture attention mechanisms that can create spatial and
temporal heatmaps on the scene graphs. We evaluate our system on the ROAD
dataset against a fully-supervised approach, showing the superiority of our
training regime.
</p></li>
</ul>

<h3>Title: Structured Generative Models for Scene Understanding. (arXiv:2302.03531v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03531">http://arxiv.org/abs/2302.03531</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03531] Structured Generative Models for Scene Understanding](http://arxiv.org/abs/2302.03531) #interpretability</code></li>
<li>Summary: <p>This position paper argues for the use of \emph{structured generative models}
(SGMs) for scene understanding. This requires the reconstruction of a 3D scene
from an input image, whereby the contents of the image are causally explained
in terms of models of instantiated objects, each with their own type, shape,
appearance and pose, along with global variables like scene lighting and camera
parameters. This approach also requires scene models which account for the
co-occurrences and inter-relationships of objects in a scene. The SGM approach
has the merits that it is compositional and generative, which lead to
interpretability.
</p></li>
</ul>

<p>To pursue the SGM agenda, we need models for objects and scenes, and
approaches to carry out inference. We first review models for objects, which
include ``things'' (object categories that have a well defined shape), and
``stuff'' (categories which have amorphous spatial extent). We then move on to
review \emph{scene models} which describe the inter-relationships of objects.
Perhaps the most challenging problem for SGMs is \emph{inference} of the
objects, lighting and camera parameters, and scene inter-relationships from
input consisting of a single or multiple images. We conclude with a discussion
of issues that need addressing to advance the SGM agenda.
</p>

<h3>Title: Scalable Gaussian process regression enables accurate prediction of protein and small molecule properties with uncertainty quantitation. (arXiv:2302.03294v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03294">http://arxiv.org/abs/2302.03294</a></li>
<li>Code URL: <a href="https://github.com/jlparkI/benchmarking_xGPR">https://github.com/jlparkI/benchmarking_xGPR</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03294] Scalable Gaussian process regression enables accurate prediction of protein and small molecule properties with uncertainty quantitation](http://arxiv.org/abs/2302.03294) #interpretability</code></li>
<li>Summary: <p>Gaussian process (GP) is a Bayesian model which provides several advantages
for regression tasks in machine learning such as reliable quantitation of
uncertainty and improved interpretability. Their adoption has been precluded by
their excessive computational cost and by the difficulty in adapting them for
analyzing sequences (e.g. amino acid and nucleotide sequences) and graphs (e.g.
ones representing small molecules). In this study, we develop efficient and
scalable approaches for fitting GP models as well as fast convolution kernels
which scale linearly with graph or sequence size. We implement these
improvements by building an open-source Python library called xGPR. We compare
the performance of xGPR with the reported performance of various deep learning
models on 20 benchmarks, including small molecule, protein sequence and tabular
data. We show that xGRP achieves highly competitive performance with much
shorter training time. Furthermore, we also develop new kernels for sequence
and graph data and show that xGPR generally outperforms convolutional neural
networks on predicting key properties of proteins and small molecules.
Importantly, xGPR provides uncertainty information not available from typical
deep learning models. Additionally, xGPR provides a representation of the input
data that can be used for clustering and data visualization. These results
demonstrate that xGPR provides a powerful and generic tool that can be broadly
useful in protein engineering and drug discovery.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Five policy uses of algorithmic explainability. (arXiv:2302.03080v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03080">http://arxiv.org/abs/2302.03080</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03080] Five policy uses of algorithmic explainability](http://arxiv.org/abs/2302.03080) #explainability</code></li>
<li>Summary: <p>The notion that algorithmic systems should be "explainable" is common in the
many statements of consensus principles developed by governments, companies,
and advocacy organizations. But what exactly do these policy and legal actors
want from explainability, and how do their desiderata compare with the
explainability techniques developed in the machine learning literature? We
explore this question in hopes of better connecting the policy and technical
communities. We outline five settings in which policymakers seek to use
explainability: complying with specific requirements for explanation; helping
to obtain regulatory approval in highly regulated settings; enabling or
interfacing with liability; flexibly managing risk as part of a self-regulatory
process; and providing model and data transparency. We illustrate each setting
with an in-depth case study contextualizing the purpose and role of
explanation. Drawing on these case studies, we discuss common factors limiting
policymakers' use of explanation and promising ways in which explanation can be
used in policy. We conclude with recommendations for researchers and
policymakers.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Spatial Functa: Scaling Functa to ImageNet Classification and Generation. (arXiv:2302.03130v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03130">http://arxiv.org/abs/2302.03130</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03130] Spatial Functa: Scaling Functa to ImageNet Classification and Generation](http://arxiv.org/abs/2302.03130) #diffusion</code></li>
<li>Summary: <p>Neural fields, also known as implicit neural representations, have emerged as
a powerful means to represent complex signals of various modalities. Based on
this Dupont et al. (2022) introduce a framework that views neural fields as
data, termed <em>functa</em>, and proposes to do deep learning directly on this
dataset of neural fields. In this work, we show that the proposed framework
faces limitations when scaling up to even moderately complex datasets such as
CIFAR-10. We then propose <em>spatial functa</em>, which overcome these limitations by
using spatially arranged latent representations of neural fields, thereby
allowing us to scale up the approach to ImageNet-1k at 256x256 resolution. We
demonstrate competitive performance to Vision Transformers (Steiner et al.,
2022) on classification and Latent Diffusion (Rombach et al., 2022) on image
generation respectively.
</p></li>
</ul>

<h3>Title: Boosting Zero-shot Classification with Synthetic Data Diversity via Stable Diffusion. (arXiv:2302.03298v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03298">http://arxiv.org/abs/2302.03298</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03298] Boosting Zero-shot Classification with Synthetic Data Diversity via Stable Diffusion](http://arxiv.org/abs/2302.03298) #diffusion</code></li>
<li>Summary: <p>Recent research has shown it is possible to perform zero-shot classification
tasks by training a classifier with synthetic data generated by a diffusion
model. However, the performance of this approach is still inferior to that of
recent vision-language models. It has been suggested that the reason for this
is a domain gap between the synthetic and real data. In our work, we show that
this domain gap is not the main issue, and that diversity in the synthetic
dataset is more important. We propose a \textit{bag of tricks} to improve
diversity and are able to achieve performance on par with one of the
vision-language models, CLIP. More importantly, this insight allows us to endow
zero-shot classification capabilities on any classification model.
</p></li>
</ul>

<h3>Title: HumanMAC: Masked Motion Completion for Human Motion Prediction. (arXiv:2302.03665v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03665">http://arxiv.org/abs/2302.03665</a></li>
<li>Code URL: <a href="https://github.com/linghaochan/humanmac">https://github.com/linghaochan/humanmac</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03665] HumanMAC: Masked Motion Completion for Human Motion Prediction](http://arxiv.org/abs/2302.03665) #diffusion</code></li>
<li>Summary: <p>Human motion prediction is a classical problem in computer vision and
computer graphics, which has a wide range of practical applications. Previous
effects achieve great empirical performance based on an encoding-decoding
fashion. The methods of this fashion work by first encoding previous motions to
latent representations and then decoding the latent representations into
predicted motions. However, in practice, they are still unsatisfactory due to
several issues, including complicated loss constraints, cumbersome training
processes, and scarce switch of different categories of motions in prediction.
In this paper, to address the above issues, we jump out of the foregoing
fashion and propose a novel framework from a new perspective. Specifically, our
framework works in a denoising diffusion style. In the training stage, we learn
a motion diffusion model that generates motions from random noise. In the
inference stage, with a denoising procedure, we make motion prediction
conditioning on observed motions to output more continuous and controllable
predictions. The proposed framework enjoys promising algorithmic properties,
which only needs one loss in optimization and is trained in an end-to-end
manner. Additionally, it accomplishes the switch of different categories of
motions effectively, which is significant in realistic tasks, \textit{e.g.},
the animation task. Comprehensive experiments on benchmarks confirm the
superiority of the proposed framework. The project page is available at
\url{https://lhchen.top/Human-MAC}.
</p></li>
</ul>

<h3>Title: Machine learning benchmarks for the classification of equivalent circuit models from solid-state electrochemical impedance spectra. (arXiv:2302.03362v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03362">http://arxiv.org/abs/2302.03362</a></li>
<li>Code URL: <a href="https://github.com/batterydev/autoecm">https://github.com/batterydev/autoecm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03362] Machine learning benchmarks for the classification of equivalent circuit models from solid-state electrochemical impedance spectra](http://arxiv.org/abs/2302.03362) #diffusion</code></li>
<li>Summary: <p>Analysis of Electrochemical Impedance Spectroscopy (EIS) data for
electrochemical systems often consists of defining an Equivalent Circuit Model
(ECM) using expert knowledge and then optimizing the model parameters to
deconvolute various resistance, capacitive, inductive, or diffusion responses.
For small data sets, this procedure can be conducted manually; however, it is
not feasible to manually define a proper ECM for extensive data sets with a
wide range of EIS responses. Automatic identification of an ECM would
substantially accelerate the analysis of large sets of EIS data. Here, we
showcase machine learning methods developed during the BatteryDEV hackathon to
classify the ECMs of 9,300 EIS measurements provided by QuantumScape. The
best-performing approach is a gradient-boosted tree model utilizing a library
to automatically generate features, followed by a random forest model using the
raw spectral data. A convolutional neural network using boolean images of
Nyquist representations is presented as an alternative, although it achieves a
lower accuracy. We publish the data and open source the associated code. The
approaches described in this article can serve as benchmarks for further
studies. A key remaining challenge is that the labels contain uncertainty and
human bias, underlined by the performance of the trained models.
</p></li>
</ul>

<h3>Title: Graph Generation with Destination-Driven Diffusion Mixture. (arXiv:2302.03596v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03596">http://arxiv.org/abs/2302.03596</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03596] Graph Generation with Destination-Driven Diffusion Mixture](http://arxiv.org/abs/2302.03596) #diffusion</code></li>
<li>Summary: <p>Generation of graphs is a major challenge for real-world tasks that require
understanding the complex nature of their non-Euclidean structures. Although
diffusion models have achieved notable success in graph generation recently,
they are ill-suited for modeling the structural information of graphs since
learning to denoise the noisy samples does not explicitly capture the graph
topology. To tackle this limitation, we propose a novel generative process that
models the topology of graphs by predicting the destination of the process.
Specifically, we design the generative process as a mixture of diffusion
processes conditioned on the endpoint in the data distribution, which drives
the process toward the probable destination. Further, we introduce new training
objectives for learning to predict the destination, and discuss the advantages
of our generative framework that can explicitly model the graph topology and
exploit the inductive bias of the data. Through extensive experimental
validation on general graph and 2D/3D molecular graph generation tasks, we show
that our method outperforms previous generative models, generating graphs with
correct topology with both continuous and discrete features.
</p></li>
</ul>

<h3>Title: Long Horizon Temperature Scaling. (arXiv:2302.03686v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.03686">http://arxiv.org/abs/2302.03686</a></li>
<li>Code URL: <a href="https://github.com/andyshih12/longhorizontemperaturescaling">https://github.com/andyshih12/longhorizontemperaturescaling</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.03686] Long Horizon Temperature Scaling](http://arxiv.org/abs/2302.03686) #diffusion</code></li>
<li>Summary: <p>Temperature scaling is a popular technique for tuning the sharpness of a
model distribution. It is used extensively for sampling likely generations and
calibrating model uncertainty, and even features as a controllable parameter to
many large language models in deployment. However, autoregressive models rely
on myopic temperature scaling that greedily optimizes the next token. To
address this, we propose Long Horizon Temperature Scaling (LHTS), a novel
approach for sampling from temperature-scaled joint distributions. LHTS is
compatible with all likelihood-based models, and optimizes for the long-horizon
likelihood of samples. We derive a temperature-dependent LHTS objective, and
show that fine-tuning a model on a range of temperatures produces a single
model capable of generation with a controllable long-horizon temperature
parameter. We experiment with LHTS on image diffusion models and
character/language autoregressive models, demonstrating advantages over myopic
temperature scaling in likelihood and sample quality, and showing improvements
in accuracy on a multiple choice analogy task by $10\%$.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
