<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-25</h1>
<h3>Title: Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction</h3>
<ul>
<li><strong>Authors: </strong>Yujia Tong, Jingling Yuan, Chuang Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17768">https://arxiv.org/abs/2507.17768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17768">https://arxiv.org/pdf/2507.17768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17768]] Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction(https://arxiv.org/abs/2507.17768)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>With the development of mobile and edge computing, the demand for low-bit quantized models on edge devices is increasing to achieve efficient deployment. To enhance the performance, it is often necessary to retrain the quantized models using edge data. However, due to privacy concerns, certain sensitive data can only be processed on edge devices. Therefore, employing Quantization-Aware Training (QAT) on edge devices has become an effective solution. Nevertheless, traditional QAT relies on the complete dataset for training, which incurs a huge computational cost. Coreset selection techniques can mitigate this issue by training on the most representative subsets. However, existing methods struggle to eliminate quantization errors in the model when using small-scale datasets (e.g., only 10% of the data), leading to significant performance degradation. To address these issues, we propose QuaRC, a QAT framework with coresets on edge devices, which consists of two main phases: In the coreset selection phase, QuaRC introduces the ``Relative Entropy Score" to identify the subsets that most effectively capture the model's quantization errors. During the training phase, QuaRC employs the Cascaded Layer Correction strategy to align the intermediate layer outputs of the quantized model with those of the full-precision model, thereby effectively reducing the quantization errors in the intermediate layers. Experimental results demonstrate the effectiveness of our approach. For instance, when quantizing ResNet-18 to 2-bit using a 1% data subset, QuaRC achieves a 5.72% improvement in Top-1 accuracy on the ImageNet-1K dataset compared to state-of-the-art techniques.</li>
</ul>

<h3>Title: Knowledge Abstraction for Knowledge-based Semantic Communication: A Generative Causality Invariant Approach</h3>
<ul>
<li><strong>Authors: </strong>Minh-Duong Nguyen, Quoc-Viet Pham, Nguyen H. Tran, Hoang-Khoi Do, Duy T. Ngo, Won-Joo Hwang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17784">https://arxiv.org/abs/2507.17784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17784">https://arxiv.org/pdf/2507.17784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17784]] Knowledge Abstraction for Knowledge-based Semantic Communication: A Generative Causality Invariant Approach(https://arxiv.org/abs/2507.17784)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>In this study, we design a low-complexity and generalized AI model that can capture common knowledge to improve data reconstruction of the channel decoder for semantic communication. Specifically, we propose a generative adversarial network that leverages causality-invariant learning to extract causal and non-causal representations from the data. Causal representations are invariant and encompass crucial information to identify the data's label. They can encapsulate semantic knowledge and facilitate effective data reconstruction at the receiver. Moreover, the causal mechanism ensures that learned representations remain consistent across different domains, making the system reliable even with users collecting data from diverse domains. As user-collected data evolves over time causing knowledge divergence among users, we design sparse update protocols to improve the invariant properties of the knowledge while minimizing communication overheads. Three key observations were drawn from our empirical evaluations. Firstly, causality-invariant knowledge ensures consistency across different devices despite the diverse training data. Secondly, invariant knowledge has promising performance in classification tasks, which is pivotal for goal-oriented semantic communications. Thirdly, our knowledge-based data reconstruction highlights the robustness of our decoder, which surpasses other state-of-the-art data reconstruction and semantic compression methods in terms of Peak Signal-to-Noise Ratio (PSNR).</li>
</ul>

<h3>Title: Hyperbolic Deep Learning for Foundation Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Neil He, Hiren Madhu, Ngoc Bui, Menglin Yang, Rex Ying</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17787">https://arxiv.org/abs/2507.17787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17787">https://arxiv.org/pdf/2507.17787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17787]] Hyperbolic Deep Learning for Foundation Models: A Survey(https://arxiv.org/abs/2507.17787)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Foundation models pre-trained on massive datasets, including large language models (LLMs), vision-language models (VLMs), and large multimodal models, have demonstrated remarkable success in diverse downstream tasks. However, recent studies have shown fundamental limitations of these models: (1) limited representational capacity, (2) lower adaptability, and (3) diminishing scalability. These shortcomings raise a critical question: is Euclidean geometry truly the optimal inductive bias for all foundation models, or could incorporating alternative geometric spaces enable models to better align with the intrinsic structure of real-world data and improve reasoning processes? Hyperbolic spaces, a class of non-Euclidean manifolds characterized by exponential volume growth with respect to distance, offer a mathematically grounded solution. These spaces enable low-distortion embeddings of hierarchical structures (e.g., trees, taxonomies) and power-law distributions with substantially fewer dimensions compared to Euclidean counterparts. Recent advances have leveraged these properties to enhance foundation models, including improving LLMs' complex reasoning ability, VLMs' zero-shot generalization, and cross-modal semantic alignment, while maintaining parameter efficiency. This paper provides a comprehensive review of hyperbolic neural networks and their recent development for foundation models. We further outline key challenges and research directions to advance the field.</li>
</ul>

<h3>Title: Helix 1.0: An Open-Source Framework for Reproducible and Interpretable Machine Learning on Tabular Scientific Data</h3>
<ul>
<li><strong>Authors: </strong>Eduardo Aguilar-Bejarano, Daniel Lea, Karthikeyan Sivakumar, Jimiama M. Mase, Reza Omidvar, Ruizhe Li, Troy Kettle, James Mitchell-White, Morgan R Alexander, David A Winkler, Grazziela Figueredo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17791">https://arxiv.org/abs/2507.17791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17791">https://arxiv.org/pdf/2507.17791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17791]] Helix 1.0: An Open-Source Framework for Reproducible and Interpretable Machine Learning on Tabular Scientific Data(https://arxiv.org/abs/2507.17791)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Helix is an open-source, extensible, Python-based software framework to facilitate reproducible and interpretable machine learning workflows for tabular data. It addresses the growing need for transparent experimental data analytics provenance, ensuring that the entire analytical process -- including decisions around data transformation and methodological choices -- is documented, accessible, reproducible, and comprehensible to relevant stakeholders. The platform comprises modules for standardised data preprocessing, visualisation, machine learning model training, evaluation, interpretation, results inspection, and model prediction for unseen data. To further empower researchers without formal training in data science to derive meaningful and actionable insights, Helix features a user-friendly interface that enables the design of computational experiments, inspection of outcomes, including a novel interpretation approach to machine learning decisions using linguistic terms all within an integrated environment. Released under the MIT licence, Helix is accessible via GitHub and PyPI, supporting community-driven development and promoting adherence to the FAIR principles.</li>
</ul>

<h3>Title: LSDM: LLM-Enhanced Spatio-temporal Diffusion Model for Service-Level Mobile Traffic Prediction</h3>
<ul>
<li><strong>Authors: </strong>Shiyuan Zhang, Tong Li, Zhu Xiao, Hongyang Du, Kaibin Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17795">https://arxiv.org/abs/2507.17795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17795">https://arxiv.org/pdf/2507.17795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17795]] LSDM: LLM-Enhanced Spatio-temporal Diffusion Model for Service-Level Mobile Traffic Prediction(https://arxiv.org/abs/2507.17795)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Service-level mobile traffic prediction for individual users is essential for network efficiency and quality of service enhancement. However, current prediction methods are limited in their adaptability across different urban environments and produce inaccurate results due to the high uncertainty in personal traffic patterns, the lack of detailed environmental context, and the complex dependencies among different network services. These challenges demand advanced modeling techniques that can capture dynamic traffic distributions and rich environmental features. Inspired by the recent success of diffusion models in distribution modeling and Large Language Models (LLMs) in contextual understanding, we propose an LLM-Enhanced Spatio-temporal Diffusion Model (LSDM). LSDM integrates the generative power of diffusion models with the adaptive learning capabilities of transformers, augmented by the ability to capture multimodal environmental information for modeling service-level patterns and dynamics. Extensive evaluations on real-world service-level datasets demonstrate that the model excels in traffic usage predictions, showing outstanding generalization and adaptability. After incorporating contextual information via LLM, the performance improves by at least 2.83% in terms of the coefficient of determination. Compared to models of a similar type, such as CSDI, the root mean squared error can be reduced by at least 8.29%. The code and dataset will be available at: this https URL.</li>
</ul>

<h3>Title: CoCAI: Copula-based Conformal Anomaly Identification for Multivariate Time-Series</h3>
<ul>
<li><strong>Authors: </strong>Nicholas A. Pearson, Francesca Zanello, Davide Russo, Luca Bortolussi, Francesca Cairoli</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17796">https://arxiv.org/abs/2507.17796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17796">https://arxiv.org/pdf/2507.17796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17796]] CoCAI: Copula-based Conformal Anomaly Identification for Multivariate Time-Series(https://arxiv.org/abs/2507.17796)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>We propose a novel framework that harnesses the power of generative artificial intelligence and copula-based modeling to address two critical challenges in multivariate time-series analysis: delivering accurate predictions and enabling robust anomaly detection. Our method, Copula-based Conformal Anomaly Identification for Multivariate Time-Series (CoCAI), leverages a diffusion-based model to capture complex dependencies within the data, enabling high quality forecasting. The model's outputs are further calibrated using a conformal prediction technique, yielding predictive regions which are statistically valid, i.e., cover the true target values with a desired confidence level. Starting from these calibrated forecasts, robust outlier detection is performed by combining dimensionality reduction techniques with copula-based modeling, providing a statistically grounded anomaly score. CoCAI benefits from an offline calibration phase that allows for minimal overhead during deployment and delivers actionable results rooted in established theoretical foundations. Empirical tests conducted on real operational data derived from water distribution and sewerage systems confirm CoCAI's effectiveness in accurately forecasting target sequences of data and in identifying anomalous segments within them.</li>
</ul>

<h3>Title: GenSelect: A Generative Approach to Best-of-N</h3>
<ul>
<li><strong>Authors: </strong>Shubham Toshniwal, Ivan Sorokin, Aleksander Ficek, Ivan Moshkov, Igor Gitman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17797">https://arxiv.org/abs/2507.17797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17797">https://arxiv.org/pdf/2507.17797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17797]] GenSelect: A Generative Approach to Best-of-N(https://arxiv.org/abs/2507.17797)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative reward models with parallel sampling have enabled effective test-time scaling for reasoning tasks. Current approaches employ pointwise scoring of individual solutions or pairwise comparisons. However, pointwise methods underutilize LLMs' comparative abilities, while pairwise methods scale inefficiently with larger sampling budgets. We introduce GenSelect, where the LLM uses long reasoning to select the best solution among N candidates. This leverages LLMs' comparative strengths while scaling efficiently across parallel sampling budgets. For math reasoning, we demonstrate that reasoning models, such as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing scoring approaches with simple prompting.</li>
</ul>

<h3>Title: Wasserstein GAN-Based Precipitation Downscaling with Optimal Transport for Enhancing Perceptual Realism</h3>
<ul>
<li><strong>Authors: </strong>Kenta Shiraishi, Yuka Muto, Atsushi Okazaki, Shunji Kotsuki</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17798">https://arxiv.org/abs/2507.17798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17798">https://arxiv.org/pdf/2507.17798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17798]] Wasserstein GAN-Based Precipitation Downscaling with Optimal Transport for Enhancing Perceptual Realism(https://arxiv.org/abs/2507.17798)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>High-resolution (HR) precipitation prediction is essential for reducing damage from stationary and localized heavy rainfall; however, HR precipitation forecasts using process-driven numerical weather prediction models remains challenging. This study proposes using Wasserstein Generative Adversarial Network (WGAN) to perform precipitation downscaling with an optimal transport cost. In contrast to a conventional neural network trained with mean squared error, the WGAN generated visually realistic precipitation fields with fine-scale structures even though the WGAN exhibited slightly lower performance on conventional evaluation metrics. The learned critic of WGAN correlated well with human perceptual realism. Case-based analysis revealed that large discrepancies in critic scores can help identify both unrealistic WGAN outputs and potential artifacts in the reference data. These findings suggest that the WGAN framework not only improves perceptual realism in precipitation downscaling but also offers a new perspective for evaluating and quality-controlling precipitation datasets.</li>
</ul>

<h3>Title: Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yi Xin, Juncheng Yan, Qi Qin, Zhen Li, Dongyang Liu, Shicheng Li, Victor Shea-Jay Huang, Yupeng Zhou, Renrui Zhang, Le Zhuo, Tiancheng Han, Xiaoqing Sun, Siqi Luo, Mengmeng Wang, Bin Fu, Yuewen Cao, Hongsheng Li, Guangtao Zhai, Xiaohong Liu, Yu Qiao, Peng Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17801">https://arxiv.org/abs/2507.17801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17801">https://arxiv.org/pdf/2507.17801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17801]] Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling(https://arxiv.org/abs/2507.17801)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model that revisits and revitalizes the autoregressive paradigm for high-quality image generation and beyond. Unlike existing approaches that rely on pretrained components or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from scratch, enabling unrestricted architectural design and licensing freedom. It achieves generation quality on par with state-of-the-art diffusion models such as DALL-E 3 and SANA, while preserving the inherent flexibility and compositionality of autoregressive modeling. Our unified tokenization scheme allows the model to seamlessly handle a wide spectrum of tasks-including subject-driven generation, image editing, controllable synthesis, and dense prediction-within a single generative framework. To further boost usability, we incorporate efficient decoding strategies like inference-time scaling and speculative Jacobi sampling to improve quality and speed, respectively. Extensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG) demonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses diffusion-based models. Moreover, we confirm its multi-task capabilities on the Graph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally well. These results position Lumina-mGPT 2.0 as a strong, flexible foundation model for unified multimodal generation. We have released our training details, code, and models at this https URL.</li>
</ul>

<h3>Title: Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yimeng Zhang, Tian Wang, Jiri Gesi, Ziyi Wang, Yuxuan Lu, Jiacheng Lin, Sinong Zhan, Vianne Gao, Ruochen Jiao, Junze Liu, Kun Qian, Yuxin Tang, Ran Xue, Houyu Zhang, Qingjun Cui, Yufan Guo, Dakuo Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17842">https://arxiv.org/abs/2507.17842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17842">https://arxiv.org/pdf/2507.17842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17842]] Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning(https://arxiv.org/abs/2507.17842)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently demonstrated strong potential in generating 'believable human-like' behavior in web environments. Prior work has explored augmenting training data with LLM-synthesized rationales and applying supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can improve downstream action prediction. However, the performance of such approaches remains inherently bounded by the reasoning capabilities of the model used to generate the rationales. In this paper, we introduce Shop-R1, a novel reinforcement learning (RL) framework aimed at enhancing the reasoning ability of LLMs for simulation of real human behavior in online shopping environments Specifically, Shop-R1 decomposes the human behavior simulation task into two stages: rationale generation and action prediction, each guided by distinct reward signals. For rationale generation, we leverage internal model signals (e.g., logit distributions) to guide the reasoning process in a self-supervised manner. For action prediction, we propose a hierarchical reward structure with difficulty-aware scaling to prevent reward hacking and enable fine-grained reward assignment. This design evaluates both high-level action types and the correctness of fine-grained sub-action details (attributes and values), rewarding outputs proportionally to their difficulty. Experimental results show that our method achieves a relative improvement of over 65% compared to the baseline.</li>
</ul>

<h3>Title: SV3.3B: A Sports Video Understanding Model for Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Sai Varun Kodathala, Yashwanth Reddy Vutukoori, Rakesh Vunnam</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17844">https://arxiv.org/abs/2507.17844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17844">https://arxiv.org/pdf/2507.17844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17844]] SV3.3B: A Sports Video Understanding Model for Action Recognition(https://arxiv.org/abs/2507.17844)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This paper addresses the challenge of automated sports video analysis, which has traditionally been limited by computationally intensive models requiring server-side processing and lacking fine-grained understanding of athletic movements. Current approaches struggle to capture the nuanced biomechanical transitions essential for meaningful sports analysis, often missing critical phases like preparation, execution, and follow-through that occur within seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B parameter video understanding model that combines novel temporal motion difference sampling with self-supervised learning for efficient on-device deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction mechanism that intelligently identifies the 16 most representative frames from sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through mask-denoising objectives and an LLM decoder fine-tuned for sports action description generation. Evaluated on a subset of the NSVA basketball dataset, SV3.3B achieves superior performance across both traditional text generation metrics and sports-specific evaluation criteria, outperforming larger closed-source models including GPT-4o variants while maintaining significantly lower computational requirements. Our model demonstrates exceptional capability in generating technically detailed and analytically rich sports descriptions, achieving 29.2% improvement over GPT-4o in ground truth validation metrics, with substantial improvements in information density, action complexity, and measurement precision metrics essential for comprehensive athletic analysis. Model Available at this https URL.</li>
</ul>

<h3>Title: Explainable Graph Neural Networks via Structural Externalities</h3>
<ul>
<li><strong>Authors: </strong>Lijun Wu, Dong Hao, Zhiyi Fan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GT, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17848">https://arxiv.org/abs/2507.17848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17848">https://arxiv.org/pdf/2507.17848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17848]] Explainable Graph Neural Networks via Structural Externalities(https://arxiv.org/abs/2507.17848)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have achieved outstanding performance across a wide range of graph-related tasks. However, their "black-box" nature poses significant challenges to their explainability, and existing methods often fail to effectively capture the intricate interaction patterns among nodes within the network. In this work, we propose a novel explainability framework, GraphEXT, which leverages cooperative game theory and the concept of social externalities. GraphEXT partitions graph nodes into coalitions, decomposing the original graph into independent subgraphs. By integrating graph structure as an externality and incorporating the Shapley value under externalities, GraphEXT quantifies node importance through their marginal contributions to GNN predictions as the nodes transition between coalitions. Unlike traditional Shapley value-based methods that primarily focus on node attributes, our GraphEXT places greater emphasis on the interactions among nodes and the impact of structural changes on GNN predictions. Experimental studies on both synthetic and real-world datasets show that GraphEXT outperforms existing baseline methods in terms of fidelity across diverse GNN architectures , significantly enhancing the explainability of GNN models.</li>
</ul>

<h3>Title: Dynamic and Generalizable Process Reward Modeling</h3>
<ul>
<li><strong>Authors: </strong>Zhangyue Yin, Qiushi Sun, Zhiyuan Zeng, Qinyuan Cheng, Xipeng Qiu, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17849">https://arxiv.org/abs/2507.17849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17849">https://arxiv.org/pdf/2507.17849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17849]] Dynamic and Generalizable Process Reward Modeling(https://arxiv.org/abs/2507.17849)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Process Reward Models (PRMs) are crucial for guiding Large Language Models (LLMs) in complex scenarios by providing dense reward signals. However, existing PRMs primarily rely on heuristic approaches, which struggle with cross-domain generalization. While LLM-as-judge has been proposed to provide generalized rewards, current research has focused mainly on feedback results, overlooking the meaningful guidance embedded within the text. Additionally, static and coarse-grained evaluation criteria struggle to adapt to complex process supervision. To tackle these challenges, we propose Dynamic and Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to capture and store fine-grained, multi-dimensional reward criteria. DG-PRM dynamically selects reward signals for step-wise reward scoring. To handle multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation to identify discriminative positive and negative pairs. Experimental results show that DG-PRM achieves stunning performance on prevailing benchmarks, significantly boosting model performance across tasks with dense rewards. Further analysis reveals that DG-PRM adapts well to out-of-distribution scenarios, demonstrating exceptional generalizability.</li>
</ul>

<h3>Title: Performance Evaluation and Threat Mitigation in Large-scale 5G Core Deployment</h3>
<ul>
<li><strong>Authors: </strong>Rodrigo Moreira, Larissa F. Rodrigues Moreira, Flávio de Oliveira Silva</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17850">https://arxiv.org/abs/2507.17850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17850">https://arxiv.org/pdf/2507.17850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17850]] Performance Evaluation and Threat Mitigation in Large-scale 5G Core Deployment(https://arxiv.org/abs/2507.17850)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense</a></li>
<li><strong>Abstract: </strong>The deployment of large-scale software-based 5G core functions presents significant challenges due to their reliance on optimized and intelligent resource provisioning for their services. Many studies have focused on analyzing the impact of resource allocation for complex deployments using mathematical models, queue theories, or even Artificial Intelligence (AI). This paper elucidates the effects of chaotic workloads, generated by Distributed Denial of Service (DDoS) on different Network Functions (NFs) on User Equipment registration performance. Our findings highlight the necessity of diverse resource profiles to ensure Service-Level Agreement (SLA) compliance in large-scale 5G core deployments. Additionally, our analysis of packet capture approaches demonstrates the potential of kernel-based monitoring for scalable security threat defense. Finally, our empirical evaluation provides insights into the effective deployment of 5G NFs in complex scenarios.</li>
</ul>

<h3>Title: Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Lifeng Chen, Jiner Wang, Zihao Pan, Beier Zhu, Xiaofeng Yang, Chi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17853">https://arxiv.org/abs/2507.17853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17853">https://arxiv.org/pdf/2507.17853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17853]] Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models(https://arxiv.org/abs/2507.17853)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-image (T2I) generation have led to impressive visual results. However, these models still face significant challenges when handling complex prompt, particularly those involving multiple subjects with distinct attributes. Inspired by the human drawing process, which first outlines the composition and then incrementally adds details, we propose Detail++, a training-free framework that introduces a novel Progressive Detail Injection (PDI) strategy to address this limitation. Specifically, we decompose a complex prompt into a sequence of simplified sub-prompts, guiding the generation process in stages. This staged generation leverages the inherent layout-controlling capacity of self-attention to first ensure global composition, followed by precise refinement. To achieve accurate binding between attributes and corresponding subjects, we exploit cross-attention mechanisms and further introduce a Centroid Alignment Loss at test time to reduce binding noise and enhance attribute consistency. Extensive experiments on T2I-CompBench and a newly constructed style composition benchmark demonstrate that Detail++ significantly outperforms existing methods, particularly in scenarios involving multiple objects and complex stylistic conditions.</li>
</ul>

<h3>Title: FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains</h3>
<ul>
<li><strong>Authors: </strong>Muayad Abujabal, Lyes Saad Saoud, Irfan Hussain</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17859">https://arxiv.org/abs/2507.17859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17859">https://arxiv.org/pdf/2507.17859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17859]] FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains(https://arxiv.org/abs/2507.17859)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate fish detection in underwater imagery is essential for ecological monitoring, aquaculture automation, and robotic perception. However, practical deployment remains limited by fragmented datasets, heterogeneous imaging conditions, and inconsistent evaluation protocols. To address these gaps, we present \textit{FishDet-M}, the largest unified benchmark for fish detection, comprising 13 publicly available datasets spanning diverse aquatic environments including marine, brackish, occluded, and aquarium scenes. All data are harmonized using COCO-style annotations with both bounding boxes and segmentation masks, enabling consistent and scalable cross-domain evaluation. We systematically benchmark 28 contemporary object detection models, covering the YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models. Evaluations are conducted using standard metrics including mAP, mAP@50, and mAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and inference profiling in terms of latency and parameter count. The results highlight the varying detection performance across models trained on FishDet-M, as well as the trade-off between accuracy and efficiency across models of different architectures. To support adaptive deployment, we introduce a CLIP-based model selection framework that leverages vision-language alignment to dynamically identify the most semantically appropriate detector for each input image. This zero-shot selection strategy achieves high performance without requiring ensemble computation, offering a scalable solution for real-time applications. FishDet-M establishes a standardized and reproducible platform for evaluating object detection in complex aquatic scenes. All datasets, pretrained models, and evaluation tools are publicly available to facilitate future research in underwater computer vision and intelligent marine systems.</li>
</ul>

<h3>Title: Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17860">https://arxiv.org/abs/2507.17860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17860">https://arxiv.org/pdf/2507.17860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17860]] Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis(https://arxiv.org/abs/2507.17860)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in Deep Learning and its application on the edge hold great potential for the revolution of routine screenings for skin cancers like Melanoma. Along with the anticipated benefits of this technology, potential dangers arise from unforseen and inherent biases. Thus, assessing and improving the fairness of such systems is of utmost importance. A key challenge in fairness assessment is to ensure that the evaluation dataset is sufficiently representative of different Personal Identifiable Information (PII) (sex, age, and race) and other minority groups. Against the backdrop of this challenge, this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT model to assess the fairness of publicly available melanoma classifiers. The results suggest that fairness assessment using highly realistic synthetic data is a promising direction. Yet, our findings indicate that verifying fairness becomes difficult when the melanoma-detection model used for evaluation is trained on data that differ from the dataset underpinning the synthetic images. Nonetheless, we propose that our approach offers a valuable new avenue for employing synthetic data to gauge and enhance fairness in medical-imaging GenAI systems.</li>
</ul>

<h3>Title: Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic</h3>
<ul>
<li><strong>Authors: </strong>Rıza Özçelik, Sarah de Ruiter, Francesca Grisoni</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17876">https://arxiv.org/abs/2507.17876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17876">https://arxiv.org/pdf/2507.17876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17876]] Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic(https://arxiv.org/abs/2507.17876)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The scarcity of molecules with desirable properties (i.e., 'positive' molecules) is an inherent bottleneck for generative molecule design. To sidestep such obstacle, here we propose molecular task arithmetic: training a model on diverse and abundant negative examples to learn 'property directions' $--$ without accessing any positively labeled data $--$ and moving models in the opposite property directions to generate positive molecules. When analyzed on 20 zero-shot design experiments, molecular task arithmetic generated more diverse and successful designs than models trained on positive molecules. Moreover, we employed molecular task arithmetic in dual-objective and few-shot design tasks. We find that molecular task arithmetic can consistently increase the diversity of designs while maintaining desirable design properties. With its simplicity, data efficiency, and performance, molecular task arithmetic bears the potential to become the $\textit{de-facto}$ transfer learning strategy for de novo molecule design.</li>
</ul>

<h3>Title: Learning to Locate: GNN-Powered Vulnerability Path Discovery in Open Source Code</h3>
<ul>
<li><strong>Authors: </strong>Nima Atashin, Behrouz Tork Ladani, Mohammadreza Sharbaf</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17888">https://arxiv.org/abs/2507.17888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17888">https://arxiv.org/pdf/2507.17888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17888]] Learning to Locate: GNN-Powered Vulnerability Path Discovery in Open Source Code(https://arxiv.org/abs/2507.17888)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, explainability</a></li>
<li><strong>Abstract: </strong>Detecting security vulnerabilities in open-source software is a critical task that is highly regarded in the related research communities. Several approaches have been proposed in the literature for detecting vulnerable codes and identifying the classes of vulnerabilities. However, there is still room to work in explaining the root causes of detected vulnerabilities through locating vulnerable statements and the discovery of paths leading to the activation of the vulnerability. While frameworks like SliceLocator offer explanations by identifying vulnerable paths, they rely on rule-based sink identification that limits their generalization. In this paper, we introduce VulPathFinder, an explainable vulnerability path discovery framework that enhances SliceLocator's methodology by utilizing a novel Graph Neural Network (GNN) model for detecting sink statements, rather than relying on predefined rules. The proposed GNN captures semantic and syntactic dependencies to find potential sink points (PSPs), which are candidate statements where vulnerable paths end. After detecting PSPs, program slicing can be used to extract potentially vulnerable paths, which are then ranked by feeding them back into the target graph-based detector. Ultimately, the most probable path is returned, explaining the root cause of the detected vulnerability. We demonstrated the effectiveness of the proposed approach by performing evaluations on a benchmark of the buffer overflow CWEs from the SARD dataset, providing explanations for the corresponding detected vulnerabilities. The results show that VulPathFinder outperforms both original SliceLocator and GNNExplainer (as a general GNN explainability tool) in discovery of vulnerability paths to identified PSPs.</li>
</ul>

<h3>Title: DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Hanzhou Liu, Binghan Li, Chengkai Liu, Mi Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17892">https://arxiv.org/abs/2507.17892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17892">https://arxiv.org/pdf/2507.17892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17892]] DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration(https://arxiv.org/abs/2507.17892)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers, with their self-attention mechanisms for modeling long-range dependencies, have become a dominant paradigm in image restoration tasks. However, the high computational cost of self-attention limits scalability to high-resolution images, making efficiency-quality trade-offs a key research focus. To address this, Restormer employs channel-wise self-attention, which computes attention across channels instead of spatial dimensions. While effective, this approach may overlook localized artifacts that are crucial for high-quality image restoration. To bridge this gap, we explore Dilated Neighborhood Attention (DiNA) as a promising alternative, inspired by its success in high-level vision tasks. DiNA balances global context and local precision by integrating sliding-window attention with mixed dilation factors, effectively expanding the receptive field without excessive overhead. However, our preliminary experiments indicate that directly applying this global-local design to the classic deblurring task hinders accurate visual restoration, primarily due to the constrained global context understanding within local attention. To address this, we introduce a channel-aware module that complements local attention, effectively integrating global context without sacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based architecture specifically designed for image restoration, achieves competitive results across multiple benchmarks, offering a high-quality solution for diverse low-level computer vision problems.</li>
</ul>

<h3>Title: Federated Learning for Large-Scale Cloud Robotic Manipulation: Opportunities and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Obaidullah Zaland, Chanh Nguyen, Florian T. Pokorny, Monowar Bhuyan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17903">https://arxiv.org/abs/2507.17903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17903">https://arxiv.org/pdf/2507.17903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17903]] Federated Learning for Large-Scale Cloud Robotic Manipulation: Opportunities and Challenges(https://arxiv.org/abs/2507.17903)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is an emerging distributed machine learning paradigm, where the collaborative training of a model involves dynamic participation of devices to achieve broad objectives. In contrast, classical machine learning (ML) typically requires data to be located on-premises for training, whereas FL leverages numerous user devices to train a shared global model without the need to share private data. Current robotic manipulation tasks are constrained by the individual capabilities and speed of robots due to limited low-latency computing resources. Consequently, the concept of cloud robotics has emerged, allowing robotic applications to harness the flexibility and reliability of computing resources, effectively alleviating their computational demands across the cloud-edge continuum. Undoubtedly, within this distributed computing context, as exemplified in cloud robotic manipulation scenarios, FL offers manifold advantages while also presenting several challenges and opportunities. In this paper, we present fundamental concepts of FL and their connection to cloud robotic manipulation. Additionally, we envision the opportunities and challenges associated with realizing efficient and reliable cloud robotic manipulation at scale through FL, where researchers adopt to design and verify FL models in either centralized or decentralized settings.</li>
</ul>

<h3>Title: Deep learning-aided inverse design of porous metamaterials</h3>
<ul>
<li><strong>Authors: </strong>Phu Thien Nguyen, Yousef Heider, Dennis M. Kochmann, Fadi Aldakheel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17907">https://arxiv.org/abs/2507.17907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17907">https://arxiv.org/pdf/2507.17907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17907]] Deep learning-aided inverse design of porous metamaterials(https://arxiv.org/abs/2507.17907)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The ultimate aim of the study is to explore the inverse design of porous metamaterials using a deep learning-based generative framework. Specifically, we develop a property-variational autoencoder (pVAE), a variational autoencoder (VAE) augmented with a regressor, to generate structured metamaterials with tailored hydraulic properties, such as porosity and permeability. While this work uses the lattice Boltzmann method (LBM) to generate intrinsic permeability tensor data for limited porous microstructures, a convolutional neural network (CNN) is trained using a bottom-up approach to predict effective hydraulic properties. This significantly reduces the computational cost compared to direct LBM simulations. The pVAE framework is trained on two datasets: a synthetic dataset of artificial porous microstructures and CT-scan images of volume elements from real open-cell foams. The encoder-decoder architecture of the VAE captures key microstructural features, mapping them into a compact and interpretable latent space for efficient structure-property exploration. The study provides a detailed analysis and interpretation of the latent space, demonstrating its role in structure-property mapping, interpolation, and inverse design. This approach facilitates the generation of new metamaterials with desired properties. The datasets and codes used in this study will be made open-access to support further research.</li>
</ul>

<h3>Title: From Seed to Harvest: Augmenting Human Creativity with AI for Red-teaming Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Jessica Quaye, Charvi Rastogi, Alicia Parrish, Oana Inel, Minsuk Kahng, Lora Aroyo, Vijay Janapa Reddi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17922">https://arxiv.org/abs/2507.17922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17922">https://arxiv.org/pdf/2507.17922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17922]] From Seed to Harvest: Augmenting Human Creativity with AI for Red-teaming Text-to-Image Models(https://arxiv.org/abs/2507.17922)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) models have become prevalent across numerous applications, making their robust evaluation against adversarial attacks a critical priority. Continuous access to new and challenging adversarial prompts across diverse domains is essential for stress-testing these models for resilience against novel attacks from multiple vectors. Current techniques for generating such prompts are either entirely authored by humans or synthetically generated. On the one hand, datasets of human-crafted adversarial prompts are often too small in size and imbalanced in their cultural and contextual representation. On the other hand, datasets of synthetically-generated prompts achieve scale, but typically lack the realistic nuances and creative adversarial strategies found in human-crafted prompts. To combine the strengths of both human and machine approaches, we propose Seed2Harvest, a hybrid red-teaming method for guided expansion of culturally diverse, human-crafted adversarial prompt seeds. The resulting prompts preserve the characteristics and attack patterns of human prompts while maintaining comparable average attack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded dataset achieves substantially higher diversity with 535 unique geographic locations and a Shannon entropy of 7.48, compared to 58 locations and 5.28 entropy in the original dataset. Our work demonstrates the importance of human-machine collaboration in leveraging human creativity and machine computational capacity to achieve comprehensive, scalable red-teaming for continuous T2I model safety evaluation.</li>
</ul>

<h3>Title: UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction</h3>
<ul>
<li><strong>Authors: </strong>Hongrong Yang, Markus Schlaepfer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17924">https://arxiv.org/abs/2507.17924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17924">https://arxiv.org/pdf/2507.17924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17924]] UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction(https://arxiv.org/abs/2507.17924)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Accurate population flow prediction is essential for urban planning, transportation management, and public health. Yet existing methods face key limitations: traditional models rely on static spatial assumptions, deep learning models struggle with cross-city generalization, and Large Language Models (LLMs) incur high computational costs while failing to capture spatial structure. Moreover, many approaches sacrifice resolution by clustering Points of Interest (POIs) or restricting coverage to subregions, limiting their utility for city-wide analytics. We introduce UrbanPulse, a scalable deep learning framework that delivers ultra-fine-grained, city-wide OD flow predictions by treating each POI as an individual node. It combines a temporal graph convolutional encoder with a transformer-based decoder to model multi-scale spatiotemporal dependencies. To ensure robust generalization across urban contexts, UrbanPulse employs a three-stage transfer learning strategy: pretraining on large-scale urban graphs, cold-start adaptation, and reinforcement learning this http URL on over 103 million cleaned GPS records from three metropolitan areas in California, UrbanPulse achieves state-of-the-art accuracy and scalability. Through efficient transfer learning, UrbanPulse takes a key step toward making high-resolution, AI-powered urban forecasting deployable in practice across diverse cities.</li>
</ul>

<h3>Title: Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text</h3>
<ul>
<li><strong>Authors: </strong>Hulayyil Alshammari, Praveen Rao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17944">https://arxiv.org/abs/2507.17944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17944">https://arxiv.org/pdf/2507.17944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17944]] Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text(https://arxiv.org/abs/2507.17944)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have rapidly transformed the creation of written materials. LLMs have led to questions about writing integrity, thereby driving the creation of artificial intelligence (AI) detection technologies. Adversarial attacks, such as standard and humanized paraphrasing, inhibit detectors' ability to detect machine-generated text. Previous studies have mainly focused on ChatGPT and other well-known LLMs and have shown varying accuracy across detectors. However, there is a clear gap in the literature about DeepSeek, a recently published LLM. Therefore, in this work, we investigate whether six generally accessible AI detection tools -- AI Text Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can consistently recognize text generated by DeepSeek. The detectors were exposed to the aforementioned adversarial attacks. We also considered DeepSeek as a detector by performing few-shot prompting and chain-of-thought reasoning (CoT) for classifying AI and human-written text. We collected 49 human-authored question-answer pairs from before the LLM era and generated matching responses using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied adversarial techniques such as paraphrasing and humanizing to add 196 more samples. These were used to challenge detector robustness and assess accuracy impact. While QuillBot and Copyleaks showed near-perfect performance on original and paraphrased DeepSeek text, others -- particularly AI Text Classifier and GPT-2 -- showed inconsistent results. The most effective attack was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and 52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best five-shot result misclassifying only one of 49 samples (AI recall 96%, human recall 100%).</li>
</ul>

<h3>Title: Clo-HDnn: A 4.66 TFLOPS/W and 3.78 TOPS/W Continual On-Device Learning Accelerator with Energy-efficient Hyperdimensional Computing via Progressive Search</h3>
<ul>
<li><strong>Authors: </strong>Chang Eun Song, Weihong Xu, Keming Fan, Soumil Jain, Gopabandhu Hota, Haichao Yang, Leo Liu, Kerem Akarvardar, Meng-Fan Chang, Carlos H. Diaz, Gert Cauwenberghs, Tajana Rosing, Mingu Kang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17953">https://arxiv.org/abs/2507.17953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17953">https://arxiv.org/pdf/2507.17953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17953]] Clo-HDnn: A 4.66 TFLOPS/W and 3.78 TOPS/W Continual On-Device Learning Accelerator with Energy-efficient Hyperdimensional Computing via Progressive Search(https://arxiv.org/abs/2507.17953)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Clo-HDnn is an on-device learning (ODL) accelerator designed for emerging continual learning (CL) tasks. Clo-HDnn integrates hyperdimensional computing (HDC) along with low-cost Kronecker HD Encoder and weight clustering feature extraction (WCFE) to optimize accuracy and efficiency. Clo-HDnn adopts gradient-free CL to efficiently update and store the learned knowledge in the form of class hypervectors. Its dual-mode operation enables bypassing costly feature extraction for simpler datasets, while progressive search reduces complexity by up to 61% by encoding and comparing only partial query hypervectors. Achieving 4.66 TFLOPS/W (FE) and 3.78 TOPS/W (classifier), Clo-HDnn delivers 7.77x and 4.85x higher energy efficiency compared to SOTA ODL accelerators.</li>
</ul>

<h3>Title: AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Md. Al-Masrur Khan, Durgakant Pushp, Lantao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17957">https://arxiv.org/abs/2507.17957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17957">https://arxiv.org/pdf/2507.17957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17957]] AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation(https://arxiv.org/abs/2507.17957)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is trained on labeled source domain data (e.g., synthetic images) and adapted to an unlabeled target domain (e.g., real-world images) without access to target annotations. Existing UDA-SS methods often struggle to balance fine-grained local details with global contextual information, leading to segmentation errors in complex regions. To address this, we introduce the Adaptive Feature Refinement (AFR) module, which enhances segmentation accuracy by refining highresolution features using semantic priors from low-resolution logits. AFR also integrates high-frequency components, which capture fine-grained structures and provide crucial boundary information, improving object delineation. Additionally, AFR adaptively balances local and global information through uncertaintydriven attention, reducing misclassifications. Its lightweight design allows seamless integration into HRDA-based UDA methods, leading to state-of-the-art segmentation performance. Our approach improves existing UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on Synthia-->Cityscapes. The implementation of our framework is available at: this https URL</li>
</ul>

<h3>Title: VIBE: Video-Input Brain Encoder for fMRI Response Modeling</h3>
<ul>
<li><strong>Authors: </strong>Daniel Carlstrom Schad, Shrey Dixit, Janis Keck, Viktor Studenyak, Aleksandr Shpilevoi, Andrej Bicanski</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17958">https://arxiv.org/abs/2507.17958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17958">https://arxiv.org/pdf/2507.17958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17958]] VIBE: Video-Input Brain Encoder for fMRI Response Modeling(https://arxiv.org/abs/2507.17958)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present VIBE, a two-stage Transformer that fuses multi-modal video, audio, and text features to predict fMRI activity. Representations from open-source models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a modality-fusion transformer and temporally decoded by a prediction transformer with rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod dataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson correlations of 32.25 on in-distribution Friends S07 and 21.25 on six out-of-distribution films. An earlier iteration of the same architecture obtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second overall in the Algonauts 2025 Challenge.</li>
</ul>

<h3>Title: OPEN: A Benchmark Dataset and Baseline for Older Adult Patient Engagement Recognition in Virtual Rehabilitation Learning Environments</h3>
<ul>
<li><strong>Authors: </strong>Ali Abedi, Sadaf Safa, Tracey J.F. Colella, Shehroz S. Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17959">https://arxiv.org/abs/2507.17959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17959">https://arxiv.org/pdf/2507.17959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17959]] OPEN: A Benchmark Dataset and Baseline for Older Adult Patient Engagement Recognition in Virtual Rehabilitation Learning Environments(https://arxiv.org/abs/2507.17959)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Engagement in virtual learning is essential for participant satisfaction, performance, and adherence, particularly in online education and virtual rehabilitation, where interactive communication plays a key role. Yet, accurately measuring engagement in virtual group settings remains a challenge. There is increasing interest in using artificial intelligence (AI) for large-scale, real-world, automated engagement recognition. While engagement has been widely studied in younger academic populations, research and datasets focused on older adults in virtual and telehealth learning settings remain limited. Existing methods often neglect contextual relevance and the longitudinal nature of engagement across sessions. This paper introduces OPEN (Older adult Patient ENgagement), a novel dataset supporting AI-driven engagement recognition. It was collected from eleven older adults participating in weekly virtual group learning sessions over six weeks as part of cardiac rehabilitation, producing over 35 hours of data, making it the largest dataset of its kind. To protect privacy, raw video is withheld; instead, the released data include facial, hand, and body joint landmarks, along with affective and behavioral features extracted from video. Annotations include binary engagement states, affective and behavioral labels, and context-type indicators, such as whether the instructor addressed the group or an individual. The dataset offers versions with 5-, 10-, 30-second, and variable-length samples. To demonstrate utility, multiple machine learning and deep learning models were trained, achieving engagement recognition accuracy of up to 81 percent. OPEN provides a scalable foundation for personalized engagement modeling in aging populations and contributes to broader engagement recognition research.</li>
</ul>

<h3>Title: TimelyHLS: LLM-Based Timing-Aware and Architecture-Specific FPGA HLS Optimization</h3>
<ul>
<li><strong>Authors: </strong>Nowfel Mashnoor, Mohammad Akyash, Hadi Kamali, Kimia Azar</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17962">https://arxiv.org/abs/2507.17962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17962">https://arxiv.org/pdf/2507.17962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17962]] TimelyHLS: LLM-Based Timing-Aware and Architecture-Specific FPGA HLS Optimization(https://arxiv.org/abs/2507.17962)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Achieving timing closure and design-specific optimizations in FPGA-targeted High-Level Synthesis (HLS) remains a significant challenge due to the complex interaction between architectural constraints, resource utilization, and the absence of automated support for platform-specific pragmas. In this work, we propose TimelyHLS, a novel framework integrating Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to automatically generate and iteratively refine HLS code optimized for FPGA-specific timing and performance requirements. TimelyHLS is driven by a structured architectural knowledge base containing FPGA-specific features, synthesis directives, and pragma templates. Given a kernel, TimelyHLS generates HLS code annotated with both timing-critical and design-specific pragmas. The synthesized RTL is then evaluated using commercial toolchains, and simulation correctness is verified against reference outputs via custom testbenches. TimelyHLS iteratively incorporates synthesis logs and performance reports into the LLM engine for refinement in the presence of functional discrepancies. Experimental results across 10 FPGA architectures and diverse benchmarks show that TimelyHLS reduces the need for manual tuning by up to 70%, while achieving up to 4x latency speedup (e.g., 3.85x for Matrix Multiplication, 3.7x for Bitonic Sort) and over 50% area savings in certain cases (e.g., 57% FF reduction in Viterbi). TimelyHLS consistently achieves timing closure and functional correctness across platforms, highlighting the effectiveness of LLM-driven, architecture-aware synthesis in automating FPGA design.</li>
</ul>

<h3>Title: Improving the Computational Efficiency and Explainability of GeoAggregator</h3>
<ul>
<li><strong>Authors: </strong>Rui Deng, Ziqi Li, Mingshu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17977">https://arxiv.org/abs/2507.17977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17977">https://arxiv.org/pdf/2507.17977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17977]] Improving the Computational Efficiency and Explainability of GeoAggregator(https://arxiv.org/abs/2507.17977)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer</a></li>
<li><strong>Abstract: </strong>Accurate modeling and explaining geospatial tabular data (GTD) are critical for understanding geospatial phenomena and their underlying processes. Recent work has proposed a novel transformer-based deep learning model named GeoAggregator (GA) for this purpose, and has demonstrated that it outperforms other statistical and machine learning approaches. In this short paper, we further improve GA by 1) developing an optimized pipeline that accelerates the dataloading process and streamlines the forward pass of GA to achieve better computational efficiency; and 2) incorporating a model ensembling strategy and a post-hoc model explanation function based on the GeoShapley framework to enhance model explainability. We validate the functionality and efficiency of the proposed strategies by applying the improved GA model to synthetic datasets. Experimental results show that our implementation improves the prediction accuracy and inference speed of GA compared to the original implementation. Moreover, explanation experiments indicate that GA can effectively captures the inherent spatial effects in the designed synthetic dataset. The complete pipeline has been made publicly available for community use (this https URL).</li>
</ul>

<h3>Title: MeAJOR Corpus: A Multi-Source Dataset for Phishing Email Detection</h3>
<ul>
<li><strong>Authors: </strong>Paulo Mendes (1), Eva Maia (1), Isabel Praça (1) ((1) GECAD, ISEP, Polytechnic of Porto, Portugal)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17978">https://arxiv.org/abs/2507.17978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17978">https://arxiv.org/pdf/2507.17978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17978]] MeAJOR Corpus: A Multi-Source Dataset for Phishing Email Detection(https://arxiv.org/abs/2507.17978)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Phishing emails continue to pose a significant threat to cybersecurity by exploiting human vulnerabilities through deceptive content and malicious payloads. While Machine Learning (ML) models are effective at detecting phishing threats, their performance largely relies on the quality and diversity of the training data. This paper presents MeAJOR (Merged email Assets from Joint Open-source Repositories) Corpus, a novel, multi-source phishing email dataset designed to overcome critical limitations in existing resources. It integrates 135894 samples representing a broad number of phishing tactics and legitimate emails, with a wide spectrum of engineered features. We evaluated the dataset's utility for phishing detection research through systematic experiments with four classification models (RF, XGB, MLP, and CNN) across multiple feature configurations. Results highlight the dataset's effectiveness, achieving 98.34% F1 with XGB. By integrating broad features from multiple categories, our dataset provides a reusable and consistent resource, while addressing common challenges like class imbalance, generalisability and reproducibility.</li>
</ul>

<h3>Title: SIFOTL: A Principled, Statistically-Informed Fidelity-Optimization Method for Tabular Learning</h3>
<ul>
<li><strong>Authors: </strong>Shubham Mohole, Sainyam Galhotra</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17979">https://arxiv.org/abs/2507.17979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17979">https://arxiv.org/pdf/2507.17979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17979]] SIFOTL: A Principled, Statistically-Informed Fidelity-Optimization Method for Tabular Learning(https://arxiv.org/abs/2507.17979)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Identifying the factors driving data shifts in tabular datasets is a significant challenge for analysis and decision support systems, especially those focusing on healthcare. Privacy rules restrict data access, and noise from complex processes hinders analysis. To address this challenge, we propose SIFOTL (Statistically-Informed Fidelity-Optimization Method for Tabular Learning) that (i) extracts privacy-compliant data summary statistics, (ii) employs twin XGBoost models to disentangle intervention signals from noise with assistance from LLMs, and (iii) merges XGBoost outputs via a Pareto-weighted decision tree to identify interpretable segments responsible for the shift. Unlike existing analyses which may ignore noise or require full data access for LLM-based analysis, SIFOTL addresses both challenges using only privacy-safe summary statistics. Demonstrating its real-world efficacy, for a MEPS panel dataset mimicking a new Medicare drug subsidy, SIFOTL achieves an F1 score of 0.85, substantially outperforming BigQuery Contribution Analysis (F1=0.46) and statistical tests (F1=0.20) in identifying the segment receiving the subsidy. Furthermore, across 18 diverse EHR datasets generated based on Synthea ABM, SIFOTL sustains F1 scores of 0.86-0.96 without noise and >= 0.75 even with injected observational noise, whereas baseline average F1 scores range from 0.19-0.67 under the same tests. SIFOTL, therefore, provides an interpretable, privacy-conscious workflow that is empirically robust to observational noise.</li>
</ul>

<h3>Title: Machine Unlearning of Traffic State Estimation and Prediction</h3>
<ul>
<li><strong>Authors: </strong>Xin Wang, R. Tyrrell Rockafellar, Xuegang (Jeff)Ban</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17984">https://arxiv.org/abs/2507.17984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17984">https://arxiv.org/pdf/2507.17984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17984]] Machine Unlearning of Traffic State Estimation and Prediction(https://arxiv.org/abs/2507.17984)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Data-driven traffic state estimation and prediction (TSEP) relies heavily on data sources that contain sensitive information. While the abundance of data has fueled significant breakthroughs, particularly in machine learning-based methods, it also raises concerns regarding privacy, cybersecurity, and data freshness. These issues can erode public trust in intelligent transportation systems. Recently, regulations have introduced the "right to be forgotten", allowing users to request the removal of their private data from models. As machine learning models can remember old data, simply removing it from back-end databases is insufficient in such systems. To address these challenges, this study introduces a novel learning paradigm for TSEP-Machine Unlearning TSEP-which enables a trained TSEP model to selectively forget privacy-sensitive, poisoned, or outdated data. By empowering models to "unlearn," we aim to enhance the trustworthiness and reliability of data-driven traffic TSEP.</li>
</ul>

<h3>Title: AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based Visible-Infrared Person Re-ID</h3>
<ul>
<li><strong>Authors: </strong>Huy Nguyen, Kien Nguyen, Akila Pemasiri, Akmal Jahan, Clinton Fookes, Sridha Sridharan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17995">https://arxiv.org/abs/2507.17995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17995">https://arxiv.org/pdf/2507.17995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17995]] AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based Visible-Infrared Person Re-ID(https://arxiv.org/abs/2507.17995)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Person re-identification (Re-ID) across visible and infrared modalities is crucial for 24-hour surveillance systems, but existing datasets primarily focus on ground-level perspectives. While ground-based IR systems offer nighttime capabilities, they suffer from occlusions, limited coverage, and vulnerability to obstructions--problems that aerial perspectives uniquely solve. To address these limitations, we introduce this http URL, the first aerial-ground cross-modality video-based person Re-ID dataset. This dataset captures 1,837 identities across 4,861 tracklets (124,855 frames) using both UAV-mounted and fixed CCTV cameras in RGB and infrared modalities. this http URL presents unique challenges including cross-viewpoint variations, modality discrepancies, and temporal dynamics. Additionally, we propose TCC-VPReID, a novel three-stream architecture designed to address the joint challenges of cross-platform and cross-modality person Re-ID. Our approach bridges the domain gaps between aerial-ground perspectives and RGB-IR modalities, through style-robust feature learning, memory-based cross-view adaptation, and intermediary-guided temporal modeling. Experiments show that this http URL presents distinctive challenges compared to existing datasets, with our TCC-VPReID framework achieving significant performance gains across multiple evaluation protocols. Dataset and code are available at this https URL.</li>
</ul>

<h3>Title: Exploring the interplay of label bias with subgroup size and separability: A case study in mammographic density classification</h3>
<ul>
<li><strong>Authors: </strong>Emma A.M. Stanley, Raghav Mehta, Mélanie Roschewitz, Nils D. Forkert, Ben Glocker</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17996">https://arxiv.org/abs/2507.17996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17996">https://arxiv.org/pdf/2507.17996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17996]] Exploring the interplay of label bias with subgroup size and separability: A case study in mammographic density classification(https://arxiv.org/abs/2507.17996)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Systematic mislabelling affecting specific subgroups (i.e., label bias) in medical imaging datasets represents an understudied issue concerning the fairness of medical AI systems. In this work, we investigated how size and separability of subgroups affected by label bias influence the learned features and performance of a deep learning model. Therefore, we trained deep learning models for binary tissue density classification using the EMory BrEast imaging Dataset (EMBED), where label bias affected separable subgroups (based on imaging manufacturer) or non-separable "pseudo-subgroups". We found that simulated subgroup label bias led to prominent shifts in the learned feature representations of the models. Importantly, these shifts within the feature space were dependent on both the relative size and the separability of the subgroup affected by label bias. We also observed notable differences in subgroup performance depending on whether a validation set with clean labels was used to define the classification threshold for the model. For instance, with label bias affecting the majority separable subgroup, the true positive rate for that subgroup fell from 0.898, when the validation set had clean labels, to 0.518, when the validation set had biased labels. Our work represents a key contribution toward understanding the consequences of label bias on subgroup fairness in medical imaging AI.</li>
</ul>

<h3>Title: GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures</h3>
<ul>
<li><strong>Authors: </strong>Jake R. Patock, Nicole Catherine Lewis, Kevin McCoy, Christina Gomez, Canling Chen, Lorenzo Luzi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18009">https://arxiv.org/abs/2507.18009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18009">https://arxiv.org/pdf/2507.18009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18009]] GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures(https://arxiv.org/abs/2507.18009)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>State-of-the-art (SOTA) image and text generation models are multimodal models that have many similarities to large language models (LLMs). Despite achieving strong performances, leading foundational multimodal model architectures frequently lag behind the architectural sophistication of contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner (CoCa) model that incorporates Gaussian error gated linear units, root mean squared normalization, and rotary positional embedding into the textual decoders and the vision transformer (ViT) encoder. Each architectural modification has been shown to improve model performance in LLMs, but has yet to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model with the same modified textual decoders but with CoCa's original ViT encoder. We used standard pretraining and fine-tuning workflows to benchmark the models on contrastive and generative tasks. Our GRR-CoCa significantly outperformed Baseline CoCa on the pretraining dataset and three diverse fine-tuning datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were 13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We show that GRR-CoCa's modified architecture improves performance and generalization across vision-language domains.</li>
</ul>

<h3>Title: Technical Report of TeleChat2, TeleChat2.5 and T1</h3>
<ul>
<li><strong>Authors: </strong>Zihan Wang, Xinzhang Liu, Yitong Yao, Chao Wang, Yu Zhao, Zhihao Yang, Wenmin Deng, Kaipeng Jia, Jiaxin Peng, Yuyao Huang, Sishi Xiong, Zhuo Jiang, Kaidong Yu, Xiaohui Hu, Fubei Yao, Ruiyu Fang, Zhuoru Jiang, Ruiting Song, Qiyi Xie, Rui Xue, Xuewei He, Yanlei Xue, Zhu Yuan, Zhaoxi Zhang, Zilu Huang, Shiquan Wang, Xin Wang, Hanming Wu, Mingyuan Wang, Xufeng Zhan, Yuhan Sun, Zhaohu Xing, Yuhao Jiang, Bingkai Yang, Shuangyong Song, Yongxiang Li, Zhongjiang He, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18013">https://arxiv.org/abs/2507.18013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18013">https://arxiv.org/pdf/2507.18013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18013]] Technical Report of TeleChat2, TeleChat2.5 and T1(https://arxiv.org/abs/2507.18013)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce the latest series of TeleChat models: \textbf{TeleChat2}, \textbf{TeleChat2.5}, and \textbf{T1}, offering a significant upgrade over their predecessor, TeleChat. Despite minimal changes to the model architecture, the new series achieves substantial performance gains through enhanced training strategies in both pre-training and post-training stages. The series begins with \textbf{TeleChat2}, which undergoes pretraining on 10 trillion high-quality and diverse tokens. This is followed by Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to further enhance its capabilities. \textbf{TeleChat2.5} and \textbf{T1} expand the pipeline by incorporating a continual pretraining phase with domain-specific datasets, combined with reinforcement learning (RL) to improve performance in code generation and mathematical reasoning tasks. The \textbf{T1} variant is designed for complex reasoning, supporting long Chain-of-Thought (CoT) reasoning and demonstrating substantial improvements in mathematics and coding. In contrast, \textbf{TeleChat2.5} prioritizes speed, delivering rapid inference. Both flagship models of \textbf{T1} and \textbf{TeleChat2.5} are dense Transformer-based architectures with 115B parameters, showcasing significant advancements in reasoning and general task performance compared to the original TeleChat. Notably, \textbf{T1-115B} outperform proprietary models such as OpenAI's o1-mini and GPT-4o. We publicly release \textbf{TeleChat2}, \textbf{TeleChat2.5} and \textbf{T1}, including post-trained versions with 35B and 115B parameters, to empower developers and researchers with state-of-the-art language models tailored for diverse applications.</li>
</ul>

<h3>Title: Predictive Scaling Laws for Efficient GRPO Training of Large Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Datta Nimmaturi, Vaishnavi Bhargava, Rajat Ghosh, Johnu George, Debojyoti Dutta</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18014">https://arxiv.org/abs/2507.18014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18014">https://arxiv.org/pdf/2507.18014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18014]] Predictive Scaling Laws for Efficient GRPO Training of Large Reasoning Models(https://arxiv.org/abs/2507.18014)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) for reasoning tasks using reinforcement learning methods like Group Relative Policy Optimization (GRPO) is computationally expensive. To address this, we propose a predictive framework that models training dynamics and helps optimize resource usage. Through experiments on Llama and Qwen models (3B 8B), we derive an empirical scaling law based on model size, initial performance, and training progress. This law predicts reward trajectories and identifies three consistent training phases: slow start, rapid improvement, and plateau. We find that training beyond certain number of an epoch offers little gain, suggesting earlier stopping can significantly reduce compute without sacrificing performance. Our approach generalizes across model types, providing a practical guide for efficient GRPO-based fine-tuning.</li>
</ul>

<h3>Title: Emotion Recognition from Skeleton Data: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Haifeng Lu, Jiuyi Chen, Zhen Zhang, Ruida Liu, Runhao Zeng, Xiping Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18026">https://arxiv.org/abs/2507.18026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18026">https://arxiv.org/pdf/2507.18026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18026]] Emotion Recognition from Skeleton Data: A Comprehensive Survey(https://arxiv.org/abs/2507.18026)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Emotion recognition through body movements has emerged as a compelling and privacy-preserving alternative to traditional methods that rely on facial expressions or physiological signals. Recent advancements in 3D skeleton acquisition technologies and pose estimation algorithms have significantly enhanced the feasibility of emotion recognition based on full-body motion. This survey provides a comprehensive and systematic review of skeleton-based emotion recognition techniques. First, we introduce psychological models of emotion and examine the relationship between bodily movements and emotional expression. Next, we summarize publicly available datasets, highlighting the differences in data acquisition methods and emotion labeling strategies. We then categorize existing methods into posture-based and gait-based approaches, analyzing them from both data-driven and technical perspectives. In particular, we propose a unified taxonomy that encompasses four primary technical paradigms: Traditional approaches, Feat2Net, FeatFusionNet, and End2EndNet. Representative works within each category are reviewed and compared, with benchmarking results across commonly used datasets. Finally, we explore the extended applications of emotion recognition in mental health assessment, such as detecting depression and autism, and discuss the open challenges and future research directions in this rapidly evolving field.</li>
</ul>

<h3>Title: NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database</h3>
<ul>
<li><strong>Authors: </strong>Weizhi Fei, Hao Shi, Jing Xu, Jingchen Peng, Jiazheng Li, Jingzhao Zhang, Bo Bai, Wei Han, Zhenyuan Chen, Xueyan Niu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18028">https://arxiv.org/abs/2507.18028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18028">https://arxiv.org/pdf/2507.18028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18028]] NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database(https://arxiv.org/abs/2507.18028)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Efficiently editing knowledge stored in large language models (LLMs) enables model updates without large-scale training. One possible solution is Locate-and-Edit (L\&E), allowing simultaneous modifications of a massive number of facts. However, such editing may compromise the general abilities of LLMs and even result in forgetting edited facts when scaling up to thousands of edits. In this paper, we model existing linear L\&E methods as querying a Key-Value (KV) database. From this perspective, we then propose NeuralDB, an editing framework that explicitly represents the edited facts as a neural KV database equipped with a non-linear gated retrieval module, % In particular, our gated module only operates when inference involves the edited facts, effectively preserving the general abilities of LLMs. Comprehensive experiments involving the editing of 10,000 facts were conducted on the ZsRE and CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results demonstrate that NeuralDB not only excels in editing efficacy, generalization, specificity, fluency, and consistency, but also preserves overall performance across six representative text understanding and generation tasks. Further experiments indicate that NeuralDB maintains its effectiveness even when scaled to 100,000 facts (\textbf{50x} more than in prior work).</li>
</ul>

<h3>Title: ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Ahmad ALBarqawi, Mahmoud Nazzal, Issa Khalil, Abdallah Khreishah, NhatHai Phan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18031">https://arxiv.org/abs/2507.18031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18031">https://arxiv.org/pdf/2507.18031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18031]] ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks(https://arxiv.org/abs/2507.18031)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, extraction, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>The rapid rise of deepfake technology, which produces realistic but fraudulent digital content, threatens the authenticity of media. Traditional deepfake detection approaches often struggle with sophisticated, customized deepfakes, especially in terms of generalization and robustness against malicious attacks. This paper introduces ViGText, a novel approach that integrates images with Vision Large Language Model (VLLM) Text explanations within a Graph-based framework to improve deepfake detection. The novelty of ViGText lies in its integration of detailed explanations with visual data, as it provides a more context-aware analysis than captions, which often lack specificity and fail to reveal subtle inconsistencies. ViGText systematically divides images into patches, constructs image and text graphs, and integrates them for analysis using Graph Neural Networks (GNNs) to identify deepfakes. Through the use of multi-level feature extraction across spatial and frequency domains, ViGText captures details that enhance its robustness and accuracy to detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText significantly enhances generalization and achieves a notable performance boost when it detects user-customized deepfakes. Specifically, average F1 scores rise from 72.45% to 98.32% under generalization evaluation, and reflects the model's superior ability to generalize to unseen, fine-tuned variations of stable diffusion models. As for robustness, ViGText achieves an increase of 11.1% in recall compared to other deepfake detection approaches. When facing targeted attacks that exploit its graph-based architecture, ViGText limits classification performance degradation to less than 4%. ViGText uses detailed visual and textual analysis to set a new standard for detecting deepfakes, helping ensure media authenticity and information integrity.</li>
</ul>

<h3>Title: Removing Box-Free Watermarks for Image-to-Image Models via Query-Based Reverse Engineering</h3>
<ul>
<li><strong>Authors: </strong>Haonan An, Guang Hua, Hangcheng Cao, Zhengru Fang, Guowen Xu, Susanto Rahardja, Yuguang Fang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18034">https://arxiv.org/abs/2507.18034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18034">https://arxiv.org/pdf/2507.18034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18034]] Removing Box-Free Watermarks for Image-to-Image Models via Query-Based Reverse Engineering(https://arxiv.org/abs/2507.18034)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect, attack, robust, watermark, generative</a></li>
<li><strong>Abstract: </strong>The intellectual property of deep generative networks (GNets) can be protected using a cascaded hiding network (HNet) which embeds watermarks (or marks) into GNet outputs, known as box-free watermarking. Although both GNet and HNet are encapsulated in a black box (called operation network, or ONet), with only the generated and marked outputs from HNet being released to end users and deemed secure, in this paper, we reveal an overlooked vulnerability in such systems. Specifically, we show that the hidden GNet outputs can still be reliably estimated via query-based reverse engineering, leaking the generated and unmarked images, despite the attacker's limited knowledge of the system. Our first attempt is to reverse-engineer an inverse model for HNet under the stringent black-box condition, for which we propose to exploit the query process with specially curated input images. While effective, this method yields unsatisfactory image quality. To improve this, we subsequently propose an alternative method leveraging the equivalent additive property of box-free model watermarking and reverse-engineering a forward surrogate model of HNet, with better image quality preservation. Extensive experimental results on image processing and image generation tasks demonstrate that both attacks achieve impressive watermark removal success rates (100%) while also maintaining excellent image quality (reaching the highest PSNR of 34.69 dB), substantially outperforming existing attacks, highlighting the urgent need for robust defensive strategies to mitigate the identified vulnerability in box-free model watermarking.</li>
</ul>

<h3>Title: NWaaS: Nonintrusive Watermarking as a Service for X-to-Image DNN</h3>
<ul>
<li><strong>Authors: </strong>Haonan An, Guang Hua, Yu Guo, Hangcheng Cao, Susanto Rahardja, Yuguang Fang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18036">https://arxiv.org/abs/2507.18036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18036">https://arxiv.org/pdf/2507.18036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18036]] NWaaS: Nonintrusive Watermarking as a Service for X-to-Image DNN(https://arxiv.org/abs/2507.18036)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust, watermark</a></li>
<li><strong>Abstract: </strong>The intellectual property of deep neural network (DNN) models can be protected with DNN watermarking, which embeds copyright watermarks into model parameters (white-box), model behavior (black-box), or model outputs (box-free), and the watermarks can be subsequently extracted to verify model ownership or detect model theft. Despite recent advances, these existing methods are inherently intrusive, as they either modify the model parameters or alter the structure. This natural intrusiveness raises concerns about watermarking-induced shifts in model behavior and the additional cost of fine-tuning, further exacerbated by the rapidly growing model size. As a result, model owners are often reluctant to adopt DNN watermarking in practice, which limits the development of practical Watermarking as a Service (WaaS) systems. To address this issue, we introduce Nonintrusive Watermarking as a Service (NWaaS), a novel trustless paradigm designed for X-to-Image models, in which we hypothesize that with the model untouched, an owner-defined watermark can still be extracted from model outputs. Building on this concept, we propose ShadowMark, a concrete implementation of NWaaS which addresses critical deployment challenges by establishing a robust and nonintrusive side channel in the protected model's black-box API, leveraging a key encoder and a watermark decoder. It is significantly distinctive from existing solutions by attaining the so-called absolute fidelity and being applicable to different DNN architectures, while being also robust against existing attacks, eliminating the fidelity-robustness trade-off. Extensive experiments on image-to-image, noise-to-image, noise-and-text-to-image, and text-to-image models, demonstrate the efficacy and practicality of ShadowMark for real-world deployment of nonintrusive DNN watermarking.</li>
</ul>

<h3>Title: GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs</h3>
<ul>
<li><strong>Authors: </strong>Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18043">https://arxiv.org/abs/2507.18043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18043">https://arxiv.org/pdf/2507.18043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18043]] GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs(https://arxiv.org/abs/2507.18043)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Inference-time steering methods offer a lightweight alternative to fine-tuning large language models (LLMs) and vision-language models (VLMs) by modifying internal activations at test time without updating model weights. However, most existing approaches rely on fixed, global intervention vectors, overlook the causal influence of individual input tokens, and fail to leverage informative gradients from the model's logits, particularly in multimodal settings where visual and textual inputs contribute unevenly. To address these limitations, we introduce GrAInS, an inference-time steering approach that operates across both language-only and vision-language models and tasks. GrAInS uses contrastive, gradient-based attribution via Integrated Gradients to identify the top-k most influential tokens, both positively and negatively attributed based on their contribution to preferred versus dispreferred outputs. These tokens are then used to construct directional steering vectors that capture semantic shifts from undesirable to desirable behavior. During inference, GrAInS adjusts hidden activations at transformer layers guided by token-level attribution signals, and normalizes activations to preserve representational scale. This enables fine-grained, interpretable, and modular control over model behavior, without retraining or auxiliary supervision. Empirically, GrAInS consistently outperforms both fine-tuning and existing steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514 with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all while preserving the model's fluency and general capabilities.</li>
</ul>

<h3>Title: Synthetic Data Generation for Phrase Break Prediction with Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Hoyeon Lee, Sejung Son, Ye-Eun Kang, Jong-Hwan Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18044">https://arxiv.org/abs/2507.18044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18044">https://arxiv.org/pdf/2507.18044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18044]] Synthetic Data Generation for Phrase Break Prediction with Large Language Model(https://arxiv.org/abs/2507.18044)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current approaches to phrase break prediction address crucial prosodic aspects of text-to-speech systems but heavily rely on vast human annotations from audio or text, incurring significant manual effort and cost. Inherent variability in the speech domain, driven by phonetic factors, further complicates acquiring consistent, high-quality data. Recently, large language models (LLMs) have shown success in addressing data challenges in NLP by generating tailored synthetic data while reducing manual annotation needs. Motivated by this, we explore leveraging LLM to generate synthetic phrase break annotations, addressing the challenges of both manual annotation and speech-related tasks by comparing with traditional annotations and assessing effectiveness across multiple languages. Our findings suggest that LLM-based synthetic data generation effectively mitigates data challenges in phrase break prediction and highlights the potential of LLMs as a viable solution for the speech domain.</li>
</ul>

<h3>Title: RECALLED: An Unbounded Resource Consumption Attack on Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoran Gao, Yuanhe Zhang, Zhenhong Zhou, Lei Jiang, Fanyu Meng, Yujia Xiao, Kun Wang, Yang Liu, Junlan Feng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18053">https://arxiv.org/abs/2507.18053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18053">https://arxiv.org/pdf/2507.18053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18053]] RECALLED: An Unbounded Resource Consumption Attack on Large Vision-Language Models(https://arxiv.org/abs/2507.18053)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Resource Consumption Attacks (RCAs) have emerged as a significant threat to the deployment of Large Language Models (LLMs). With the integration of vision modalities, additional attack vectors exacerbate the risk of RCAs in large vision-language models (LVLMs). However, existing red-teaming studies have largely overlooked visual inputs as a potential attack surface, resulting in insufficient mitigation strategies against RCAs in LVLMs. To address this gap, we propose RECALLED (\textbf{RE}source \textbf{C}onsumption \textbf{A}ttack on \textbf{L}arge Vision-\textbf{L}anguag\textbf{E} Mo\textbf{D}els), the first approach for exploiting visual modalities to trigger unbounded RCAs red-teaming. First, we present \textit{Vision Guided Optimization}, a fine-grained pixel-level optimization, to obtain \textit{Output Recall} adversarial perturbations, which can induce repeating output. Then, we inject the perturbations into visual inputs, triggering unbounded generations to achieve the goal of RCAs. Additionally, we introduce \textit{Multi-Objective Parallel Losses} to generate universal attack templates and resolve optimization conflicts when intending to implement parallel attacks. Empirical results demonstrate that RECALLED increases service response latency by over 26 $\uparrow$, resulting in an additional 20\% increase in GPU utilization and memory consumption. Our study exposes security vulnerabilities in LVLMs and establishes a red-teaming framework that can facilitate future defense development against RCAs.</li>
</ul>

<h3>Title: Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tevin Atwal, Chan Nam Tieu, Yefeng Yuan, Zhan Shi, Yuhong Liu, Liang Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18055">https://arxiv.org/abs/2507.18055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18055">https://arxiv.org/pdf/2507.18055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18055]] Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs(https://arxiv.org/abs/2507.18055)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>The increasing use of synthetic data generated by Large Language Models (LLMs) presents both opportunities and challenges in data-driven applications. While synthetic data provides a cost-effective, scalable alternative to real-world data to facilitate model training, its diversity and privacy risks remain underexplored. Focusing on text-based synthetic data, we propose a comprehensive set of metrics to quantitatively assess the diversity (i.e., linguistic expression, sentiment, and user perspective), and privacy (i.e., re-identification risk and stylistic outliers) of synthetic datasets generated by several state-of-the-art LLMs. Experiment results reveal significant limitations in LLMs' capabilities in generating diverse and privacy-preserving synthetic data. Guided by the evaluation results, a prompt-based approach is proposed to enhance the diversity of synthetic reviews while preserving reviewer privacy.</li>
</ul>

<h3>Title: BokehDiff: Neural Lens Blur with One-Step Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Chengxuan Zhu, Qingnan Fan, Qi Zhang, Jinwei Chen, Huaqi Zhang, Chao Xu, Boxin Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18060">https://arxiv.org/abs/2507.18060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18060">https://arxiv.org/pdf/2507.18060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18060]] BokehDiff: Neural Lens Blur with One-Step Diffusion(https://arxiv.org/abs/2507.18060)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce BokehDiff, a novel lens blur rendering method that achieves physically accurate and visually appealing outcomes, with the help of generative diffusion prior. Previous methods are bounded by the accuracy of depth estimation, generating artifacts in depth discontinuities. Our method employs a physics-inspired self-attention module that aligns with the image formation process, incorporating depth-dependent circle of confusion constraint and self-occlusion effects. We adapt the diffusion model to the one-step inference scheme without introducing additional noise, and achieve results of high quality and fidelity. To address the lack of scalable paired data, we propose to synthesize photorealistic foregrounds with transparency with diffusion models, balancing authenticity and scene diversity.</li>
</ul>

<h3>Title: TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Zehan Li, Hongjie Chen, Yuxin Zhang, Jing Zhou, Xuening Wang, Hang Lv, Mengjie Du, Yaodong Song, Jie Lian, Jian Kang, Jie Li, Yongxiang Li, Zhongjiang He, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18061">https://arxiv.org/abs/2507.18061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18061">https://arxiv.org/pdf/2507.18061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18061]] TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios(https://arxiv.org/abs/2507.18061)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Spoken language models (SLMs) have seen rapid progress in recent years, along with the development of numerous benchmarks for evaluating their performance. However, most existing benchmarks primarily focus on evaluating whether SLMs can perform complex tasks comparable to those tackled by large language models (LLMs), often failing to align with how users naturally interact in real-world conversational scenarios. In this paper, we propose TELEVAL, a dynamic benchmark specifically designed to evaluate SLMs' effectiveness as conversational agents in realistic Chinese interactive settings. TELEVAL defines three evaluation dimensions: Explicit Semantics, Paralinguistic and Implicit Semantics, and System Abilities. It adopts a dialogue format consistent with real-world usage and evaluates text and audio outputs separately. TELEVAL particularly focuses on the model's ability to extract implicit cues from user speech and respond appropriately without additional instructions. Our experiments demonstrate that despite recent progress, existing SLMs still have considerable room for improvement in natural conversational tasks. We hope that TELEVAL can serve as a user-centered evaluation framework that directly reflects the user experience and contributes to the development of more capable dialogue-oriented SLMs.</li>
</ul>

<h3>Title: Adapting Large VLMs with Iterative and Manual Instructions for Generative Low-light Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Xiaoran Sun, Liyan Wang, Cong Wang, Yeying Jin, Kin-man Lam, Zhixun Su, Yang Yang, Jinshan Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18064">https://arxiv.org/abs/2507.18064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18064">https://arxiv.org/pdf/2507.18064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18064]] Adapting Large VLMs with Iterative and Manual Instructions for Generative Low-light Enhancement(https://arxiv.org/abs/2507.18064)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Most existing low-light image enhancement (LLIE) methods rely on pre-trained model priors, low-light inputs, or both, while neglecting the semantic guidance available from normal-light images. This limitation hinders their effectiveness in complex lighting conditions. In this paper, we propose VLM-IMI, a novel framework that leverages large vision-language models (VLMs) with iterative and manual instructions (IMIs) for LLIE. VLM-IMI incorporates textual descriptions of the desired normal-light content as enhancement cues, enabling semantically informed restoration. To effectively integrate cross-modal priors, we introduce an instruction prior fusion module, which dynamically aligns and fuses image and text features, promoting the generation of detailed and semantically coherent outputs. During inference, we adopt an iterative and manual instruction strategy to refine textual instructions, progressively improving visual quality. This refinement enhances structural fidelity, semantic alignment, and the recovery of fine details under extremely low-light conditions. Extensive experiments across diverse scenarios demonstrate that VLM-IMI outperforms state-of-the-art methods in both quantitative metrics and perceptual quality. The source code is available at this https URL.</li>
</ul>

<h3>Title: Group Sequence Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, Junyang Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18071">https://arxiv.org/abs/2507.18071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18071">https://arxiv.org/pdf/2507.18071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18071]] Group Sequence Policy Optimization(https://arxiv.org/abs/2507.18071)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.</li>
</ul>

<h3>Title: C-AAE: Compressively Anonymizing Autoencoders for Privacy-Preserving Activity Recognition in Healthcare Sensor Streams</h3>
<ul>
<li><strong>Authors: </strong>Ryusei Fujimoto, Yugo Nakamura, Yutaka Arakawa</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18072">https://arxiv.org/abs/2507.18072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18072">https://arxiv.org/pdf/2507.18072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18072]] C-AAE: Compressively Anonymizing Autoencoders for Privacy-Preserving Activity Recognition in Healthcare Sensor Streams(https://arxiv.org/abs/2507.18072)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Wearable accelerometers and gyroscopes encode fine-grained behavioural signatures that can be exploited to re-identify users, making privacy protection essential for healthcare applications. We introduce C-AAE, a compressive anonymizing autoencoder that marries an Anonymizing AutoEncoder (AAE) with Adaptive Differential Pulse-Code Modulation (ADPCM). The AAE first projects raw sensor windows into a latent space that retains activity-relevant features while suppressing identity cues. ADPCM then differentially encodes this latent stream, further masking residual identity information and shrinking the bitrate. Experiments on the MotionSense and PAMAP2 datasets show that C-AAE cuts user re-identification F1 scores by 10-15 percentage points relative to AAE alone, while keeping activity-recognition F1 within 5 percentage points of the unprotected baseline. ADPCM also reduces data volume by roughly 75 %, easing transmission and storage overheads. These results demonstrate that C-AAE offers a practical route to balancing privacy and utility in continuous, sensor-based activity recognition for healthcare.</li>
</ul>

<h3>Title: Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged Mixed-Precision Quantization Method</h3>
<ul>
<li><strong>Authors: </strong>Qingcheng Zhu, Yangyang Ren, Linlin Yang, Mingbao Lin, Yanjing Li, Sheng Xu, Zichao Feng, Haodong Zhu, Yuguang Yang, Juan Zhang, Runqi Wang, Baochang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18073">https://arxiv.org/abs/2507.18073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18073">https://arxiv.org/pdf/2507.18073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18073]] Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged Mixed-Precision Quantization Method(https://arxiv.org/abs/2507.18073)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Deploying large language models (LLMs) is challenging due to their massive parameters and high computational costs. Ultra low-bit quantization can significantly reduce storage and accelerate inference, but extreme compression (i.e., mean bit-width <= 2) often leads to severe performance degradation. To address this, we propose Squeeze10-LLM, effectively "squeezing" 16-bit LLMs' weights by 10 times. Specifically, Squeeze10-LLM is a staged mixed-precision post-training quantization (PTQ) framework and achieves an average of 1.6 bits per weight by quantizing 80% of the weights to 1 bit and 20% to 4 bits. We introduce Squeeze10LLM with two key innovations: Post-Binarization Activation Robustness (PBAR) and Full Information Activation Supervision (FIAS). PBAR is a refined weight significance metric that accounts for the impact of quantization on activations, improving accuracy in low-bit settings. FIAS is a strategy that preserves full activation information during quantization to mitigate cumulative error propagation across layers. Experiments on LLaMA and LLaMA2 show that Squeeze10-LLM achieves state-of-the-art performance for sub-2bit weight-only quantization, improving average accuracy from 43% to 56% on six zero-shot classification tasks--a significant boost over existing PTQ methods. Our code will be released upon publication.</li>
</ul>

<h3>Title: PyPitfall: Dependency Chaos and Software Supply Chain Vulnerabilities in Python</h3>
<ul>
<li><strong>Authors: </strong>Jacob Mahon, Chenxi Hou, Zhihao Yao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18075">https://arxiv.org/abs/2507.18075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18075">https://arxiv.org/pdf/2507.18075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18075]] PyPitfall: Dependency Chaos and Software Supply Chain Vulnerabilities in Python(https://arxiv.org/abs/2507.18075)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Python software development heavily relies on third-party packages. Direct and transitive dependencies create a labyrinth of software supply chains. While it is convenient to reuse code, vulnerabilities within these dependency chains can propagate through dependencies, potentially affecting down-stream packages and applications. PyPI, the official Python package repository, hosts many packages and lacks a comprehensive analysis of the prevalence of vulnerable dependencies. This paper introduces PyPitfall, a quantitative analysis of vulnerable dependencies across the PyPI ecosystem. We analyzed the dependency structures of 378,573 PyPI packages and identified 4,655 packages that explicitly require at least one known-vulnerable version and 141,044 packages that permit vulnerable versions within specified ranges. By characterizing the ecosystem-wide dependency landscape and the security impact of transitive dependencies, we aim to raise awareness of Python software supply chain security.</li>
</ul>

<h3>Title: Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints</h3>
<ul>
<li><strong>Authors: </strong>Haomin Qi, Zihan Dai, Chengbo Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18076">https://arxiv.org/abs/2507.18076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18076">https://arxiv.org/pdf/2507.18076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18076]] Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints(https://arxiv.org/abs/2507.18076)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) remains a computational bottleneck due to their scale and memory demands. This paper presents a comprehensive evaluation of parameter-efficient fine-tuning (PEFT) techniques, including LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that dynamically integrates BOFT's orthogonal stability with LoRA-GA's gradient-aligned rapid convergence. By computing per-layer adaptive updates guided by gradient norms, the hybrid method achieves superior convergence efficiency and generalization across diverse tasks. We also explore, for the first time, the adaptation of unitary RNN (uRNN) principles to transformer-based LLMs, enhancing gradient stability through structured unitary constraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench, and HumanEval -- using models ranging from 7B to 405B parameters demonstrate that our hybrid method consistently outperforms individual PEFT baselines, approaching full fine-tuning accuracy while reducing resource consumption by up to 2.1 times in training time and 50 percent in memory usage. These findings establish the hybrid approach as a practical and scalable fine-tuning solution for real-world deployment of LLMs under resource constraints.</li>
</ul>

<h3>Title: TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound</h3>
<ul>
<li><strong>Authors: </strong>Pascal Spiegler, Taha Koleilat, Arash Harirpoush, Corey S. Miller, Hassan Rivaz, Marta Kersten-Oertel, Yiming Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18082">https://arxiv.org/abs/2507.18082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18082">https://arxiv.org/pdf/2507.18082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18082]] TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound(https://arxiv.org/abs/2507.18082)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Pancreatic cancer carries a poor prognosis and relies on endoscopic ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle noise, low contrast, and unintuitive appearance of EUS make segmentation of pancreatic tumors with fully supervised deep learning (DL) models both error-prone and dependent on large, expert-curated annotation datasets. To address these challenges, we present TextSAM-EUS, a novel, lightweight, text-driven adaptation of the Segment Anything Model (SAM) that requires no manual geometric prompts at inference. Our approach leverages text prompt learning (context optimization) through the BiomedCLIP text encoder in conjunction with a LoRA-based adaptation of SAM's architecture to enable automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total parameters. On the public Endoscopic Ultrasound Database of the Pancreas, TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised DL models and foundation models (e.g., SAM and its variants). As the first attempt to incorporate prompt learning in SAM-based medical image segmentation, TextSAM-EUS offers a practical option for efficient and robust automatic EUS segmentation. Our code will be publicly available upon acceptance.</li>
</ul>

<h3>Title: Comparison of Segmentation Methods in Remote Sensing for Land Use Land Cover</h3>
<ul>
<li><strong>Authors: </strong>Naman Srivastava, Joel D Joy, Yash Dixit, Swarup E, Rakshit Ramesh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18099">https://arxiv.org/abs/2507.18099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18099">https://arxiv.org/pdf/2507.18099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18099]] Comparison of Segmentation Methods in Remote Sensing for Land Use Land Cover(https://arxiv.org/abs/2507.18099)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Land Use Land Cover (LULC) mapping is essential for urban and resource planning, and is one of the key elements in developing smart and sustainable this http URL study evaluates advanced LULC mapping techniques, focusing on Look-Up Table (LUT)-based Atmospheric Correction applied to Cartosat Multispectral (MX) sensor images, followed by supervised and semi-supervised learning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo Supervision (CPS). The CPS model is further refined with dynamic weighting, enhancing pseudo-label reliability during training. This comprehensive approach analyses the accuracy and utility of LULC mapping techniques for various urban planning applications. A case study of Hyderabad, India, illustrates significant land use changes due to rapid urbanization. By analyzing Cartosat MX images over time, we highlight shifts such as urban sprawl, shrinking green spaces, and expanding industrial areas. This demonstrates the practical utility of these techniques for urban planners and policymakers.</li>
</ul>

<h3>Title: Datasets and Recipes for Video Temporal Grounding via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ruizhe Chen, Zhiting Fan, Tianze Luo, Heqing Zou, Zhaopeng Feng, Guiyang Xie, Hansheng Zhang, Zhuochen Wang, Zuozhu Liu, Huaijian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18100">https://arxiv.org/abs/2507.18100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18100">https://arxiv.org/pdf/2507.18100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18100]] Datasets and Recipes for Video Temporal Grounding via Reinforcement Learning(https://arxiv.org/abs/2507.18100)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Video Temporal Grounding (VTG) aims to localize relevant temporal segments in videos given natural language queries. Despite recent progress with large vision-language models (LVLMs) and instruction-tuning, existing approaches often suffer from limited temporal awareness and poor generalization. In this work, we introduce a two-stage training framework that integrates supervised fine-tuning with reinforcement learning (RL) to improve both the accuracy and robustness of VTG models. Our approach first leverages high-quality curated cold start data for SFT initialization, followed by difficulty-controlled RL to further enhance temporal localization and reasoning abilities. Comprehensive experiments on multiple VTG benchmarks demonstrate that our method consistently outperforms existing models, particularly in challenging and open-domain scenarios. We conduct an in-depth analysis of training strategies and dataset curation, highlighting the importance of both high-quality cold start data and difficulty-controlled RL. To facilitate further research and industrial adoption, we release all intermediate datasets, models, and code to the community.</li>
</ul>

<h3>Title: A Multimodal Seq2Seq Transformer for Predicting Brain Responses to Naturalistic Stimuli</h3>
<ul>
<li><strong>Authors: </strong>Qianyi He, Yuan Chang Leong</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18104">https://arxiv.org/abs/2507.18104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18104">https://arxiv.org/pdf/2507.18104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18104]] A Multimodal Seq2Seq Transformer for Predicting Brain Responses to Naturalistic Stimuli(https://arxiv.org/abs/2507.18104)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The Algonauts 2025 Challenge called on the community to develop encoding models that predict whole-brain fMRI responses to naturalistic multimodal movies. In this submission, we propose a sequence-to-sequence Transformer that autoregressively predicts fMRI activity from visual, auditory, and language inputs. Stimulus features were extracted using pretrained models including VideoMAE, HuBERT, Qwen, and BridgeTower. The decoder integrates information from prior brain states, current stimuli, and episode-level summaries via dual cross-attention mechanisms that attend to both perceptual information extracted from the stimulus as well as narrative information provided by high-level summaries of narrative content. One core innovation of our approach is the use of sequences of multimodal context to predict sequences of brain activity, enabling the model to capture long-range temporal structure in both stimuli and neural responses. Another is the combination of a shared encoder with partial subject-specific decoder, which leverages common structure across subjects while accounting for individual variability. Our model achieves strong performance on both in-distribution and out-of-distribution data, demonstrating the effectiveness of temporally-aware, multimodal sequence modeling for brain activity prediction. The code is available at this https URL.</li>
</ul>

<h3>Title: Distributional Uncertainty for Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>JinYoung Kim, DaeUng Jo, Kimin Yun, Jeonghyo Song, Youngjoon Yoo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18106">https://arxiv.org/abs/2507.18106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18106">https://arxiv.org/pdf/2507.18106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18106]] Distributional Uncertainty for Out-of-Distribution Detection(https://arxiv.org/abs/2507.18106)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Estimating uncertainty from deep neural networks is a widely used approach for detecting out-of-distribution (OoD) samples, which typically exhibit high predictive uncertainty. However, conventional methods such as Monte Carlo (MC) Dropout often focus solely on either model or data uncertainty, failing to align with the semantic objective of OoD detection. To address this, we propose the Free-Energy Posterior Network, a novel framework that jointly models distributional uncertainty and identifying OoD and misclassified regions using free energy. Our method introduces two key contributions: (1) a free-energy-based density estimator parameterized by a Beta distribution, which enables fine-grained uncertainty estimation near ambiguous or unseen regions; and (2) a loss integrated within a posterior network, allowing direct uncertainty estimation from learned parameters without requiring stochastic sampling. By integrating our approach with the residual prediction branch (RPL) framework, the proposed method goes beyond post-hoc energy thresholding and enables the network to learn OoD regions by leveraging the variance of the Beta distribution, resulting in a semantically meaningful and computationally efficient solution for uncertainty-aware segmentation. We validate the effectiveness of our method on challenging real-world benchmarks, including Fishyscapes, RoadAnomaly, and Segment-Me-If-You-Can.</li>
</ul>

<h3>Title: T2VWorldBench: A Benchmark for Evaluating World Knowledge in Text-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yubin Chen, Xuyang Guo, Zhenmei Shi, Zhao Song, Jiahao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18107">https://arxiv.org/abs/2507.18107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18107">https://arxiv.org/pdf/2507.18107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18107]] T2VWorldBench: A Benchmark for Evaluating World Knowledge in Text-to-Video Generation(https://arxiv.org/abs/2507.18107)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Text-to-video (T2V) models have shown remarkable performance in generating visually reasonable scenes, while their capability to leverage world knowledge for ensuring semantic consistency and factual accuracy remains largely understudied. In response to this challenge, we propose T2VWorldBench, the first systematic evaluation framework for evaluating the world knowledge generation abilities of text-to-video models, covering 6 major categories, 60 subcategories, and 1,200 prompts across a wide range of domains, including physics, nature, activity, culture, causality, and object. To address both human preference and scalable evaluation, our benchmark incorporates both human evaluation and automated evaluation using vision-language models (VLMs). We evaluated the 10 most advanced text-to-video models currently available, ranging from open source to commercial models, and found that most models are unable to understand world knowledge and generate truly correct videos. These findings point out a critical gap in the capability of current text-to-video models to leverage world knowledge, providing valuable research opportunities and entry points for constructing models with robust capabilities for commonsense reasoning and factual generation.</li>
</ul>

<h3>Title: Percentile-Based Deep Reinforcement Learning and Reward Based Personalization For Delay Aware RAN Slicing in O-RAN</h3>
<ul>
<li><strong>Authors: </strong>Peyman Tehrani, Anas Alsoliman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18111">https://arxiv.org/abs/2507.18111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18111">https://arxiv.org/pdf/2507.18111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18111]] Percentile-Based Deep Reinforcement Learning and Reward Based Personalization For Delay Aware RAN Slicing in O-RAN(https://arxiv.org/abs/2507.18111)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>In this paper, we tackle the challenge of radio access network (RAN) slicing within an open RAN (O-RAN) architecture. Our focus centers on a network that includes multiple mobile virtual network operators (MVNOs) competing for physical resource blocks (PRBs) with the goal of meeting probabilistic delay upper bound constraints for their clients while minimizing PRB utilization. Initially, we derive a reward function based on the law of large numbers (LLN), then implement practical modifications to adapt it for real-world experimental scenarios. We then propose our solution, the Percentile-based Delay-Aware Deep Reinforcement Learning (PDA-DRL), which demonstrates its superiority over several baselines, including DRL models optimized for average delay constraints, by achieving a 38\% reduction in resultant average delay. Furthermore, we delve into the issue of model weight sharing among multiple MVNOs to develop a robust personalized model. We introduce a reward-based personalization method where each agent prioritizes other agents' model weights based on their performance. This technique surpasses traditional aggregation methods, such as federated averaging, and strategies reliant on traffic patterns and model weight distance similarities.</li>
</ul>

<h3>Title: Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language Models and Critical State Identification</h3>
<ul>
<li><strong>Authors: </strong>Junyong Jiang, Buwei Tian, Chenxing Xu, Songze Li, Lu Dong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18113">https://arxiv.org/abs/2507.18113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18113">https://arxiv.org/pdf/2507.18113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18113]] Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language Models and Critical State Identification(https://arxiv.org/abs/2507.18113)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has achieved remarkable success in fields like robotics and autonomous driving, but adversarial attacks designed to mislead RL systems remain challenging. Existing approaches often rely on modifying the environment or policy, limiting their practicality. This paper proposes an adversarial attack method in which existing agents in the environment guide the target policy to output suboptimal actions without altering the environment. We propose a reward iteration optimization framework that leverages large language models (LLMs) to generate adversarial rewards explicitly tailored to the vulnerabilities of the target agent, thereby enhancing the effectiveness of inducing the target agent toward suboptimal decision-making. Additionally, a critical state identification algorithm is designed to pinpoint the target agent's most vulnerable states, where suboptimal behavior from the victim leads to significant degradation in overall performance. Experimental results in diverse environments demonstrate the superiority of our method over existing approaches.</li>
</ul>

<h3>Title: GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness</h3>
<ul>
<li><strong>Authors: </strong>Hongjie Chen, Zehan Li, Yaodong Song, Wenming Deng, Yitong Yao, Yuxin Zhang, Hang Lv, Xuechao Zhu, Jian Kang, Jie Lian, Jie Li, Chao Wang, Shuangyong Song, Yongxiang Li, Zhongjiang He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18119">https://arxiv.org/abs/2507.18119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18119">https://arxiv.org/pdf/2507.18119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18119]] GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness(https://arxiv.org/abs/2507.18119)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in end-to-end spoken language models (SLMs) have significantly improved the ability of AI systems to engage in natural spoken interactions. However, most existing models treat speech merely as a vehicle for linguistic content, often overlooking the rich paralinguistic and speaker characteristic cues embedded in human speech, such as dialect, age, emotion, and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel spoken language model with paralinguistic and speaker characteristic awareness, designed to extend spoken language modeling beyond text semantics. GOAT-SLM adopts a dual-modality head architecture that decouples linguistic modeling from acoustic realization, enabling robust language understanding while supporting expressive and adaptive speech generation. To enhance model efficiency and versatility, we propose a modular, staged training strategy that progressively aligns linguistic, paralinguistic, and speaker characteristic information using large-scale speech-text corpora. Experimental results on TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM achieves well-balanced performance across both semantic and non-semantic tasks, and outperforms existing open-source models in handling emotion, dialectal variation, and age-sensitive interactions. This work highlights the importance of modeling beyond linguistic content and advances the development of more natural, adaptive, and socially aware spoken language systems.</li>
</ul>

<h3>Title: Information Entropy-Based Framework for Quantifying Tortuosity in Meibomian Gland Uneven Atrophy</h3>
<ul>
<li><strong>Authors: </strong>Kesheng Wang, Xiaoyu Chen, Chunlei He, Fenfen Li, Xinxin Yu, Dexing Kong, Shoujun Huang, Qi Dai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18135">https://arxiv.org/abs/2507.18135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18135">https://arxiv.org/pdf/2507.18135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18135]] Information Entropy-Based Framework for Quantifying Tortuosity in Meibomian Gland Uneven Atrophy(https://arxiv.org/abs/2507.18135)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the medical image analysis field, precise quantification of curve tortuosity plays a critical role in the auxiliary diagnosis and pathological assessment of various diseases. In this study, we propose a novel framework for tortuosity quantification and demonstrate its effectiveness through the evaluation of meibomian gland atrophy uniformity,serving as a representative application scenario. We introduce an information entropy-based tortuosity quantification framework that integrates probability modeling with entropy theory and incorporates domain transformation of curve data. Unlike traditional methods such as curvature or arc-chord ratio, this approach evaluates the tortuosity of a target curve by comparing it to a designated reference curve. Consequently, it is more suitable for tortuosity assessment tasks in medical data where biologically plausible reference curves are available, providing a more robust and objective evaluation metric without relying on idealized straight-line comparisons. First, we conducted numerical simulation experiments to preliminarily assess the stability and validity of the method. Subsequently, the framework was applied to quantify the spatial uniformity of meibomian gland atrophy and to analyze the difference in this uniformity between \textit{Demodex}-negative and \textit{Demodex}-positive patient groups. The results demonstrated a significant difference in tortuosity-based uniformity between the two groups, with an area under the curve of 0.8768, sensitivity of 0.75, and specificity of 0.93. These findings highlight the clinical utility of the proposed framework in curve tortuosity analysis and its potential as a generalizable tool for quantitative morphological evaluation in medical diagnostics.</li>
</ul>

<h3>Title: Neuromorphic Computing for Embodied Intelligence in Autonomous Systems: Current Trends, Challenges, and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Alberto Marchisio, Muhammad Shafique</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18139">https://arxiv.org/abs/2507.18139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18139">https://arxiv.org/pdf/2507.18139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18139]] Neuromorphic Computing for Embodied Intelligence in Autonomous Systems: Current Trends, Challenges, and Future Directions(https://arxiv.org/abs/2507.18139)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust</a></li>
<li><strong>Abstract: </strong>The growing need for intelligent, adaptive, and energy-efficient autonomous systems across fields such as robotics, mobile agents (e.g., UAVs), and self-driving vehicles is driving interest in neuromorphic computing. By drawing inspiration from biological neural systems, neuromorphic approaches offer promising pathways to enhance the perception, decision-making, and responsiveness of autonomous platforms. This paper surveys recent progress in neuromorphic algorithms, specialized hardware, and cross-layer optimization strategies, with a focus on their deployment in real-world autonomous scenarios. Special attention is given to event-based dynamic vision sensors and their role in enabling fast, efficient perception. The discussion highlights new methods that improve energy efficiency, robustness, adaptability, and reliability through the integration of spiking neural networks into autonomous system architectures. We integrate perspectives from machine learning, robotics, neuroscience, and neuromorphic engineering to offer a comprehensive view of the state of the field. Finally, emerging trends and open challenges are explored, particularly in the areas of real-time decision-making, continual learning, and the development of secure, resilient autonomous systems.</li>
</ul>

<h3>Title: MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyuan Li, Moxin Li, Wenjie Wang, Rui Men, Yichang Zhang, Fuli Feng, Dayiheng Liu, Junyang Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18140">https://arxiv.org/abs/2507.18140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18140">https://arxiv.org/pdf/2507.18140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18140]] MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning(https://arxiv.org/abs/2507.18140)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent progress in Multi-modal Large Language Models (MLLMs) has enabled step-by-step multi-modal mathematical reasoning by performing visual operations based on the textual instructions. A promising approach uses code as an intermediate representation to precisely express and manipulate the images in the reasoning steps. However, existing evaluations focus mainly on text-only reasoning outputs, leaving the MLLM's ability to perform accurate visual operations via code largely unexplored. This work takes a first step toward addressing that gap by evaluating MLLM's code-based capabilities in multi-modal mathematical this http URL, our framework focuses on two key evaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's ability to accurately understand and construct visualizations from scratch. (2) Multi-modal Code Editing (MCE) assesses the model's capacity for fine-grained operations, which include three types: Deletion, Modification and Annotation. To evaluate the above tasks, we incorporate a dataset that covers the five most popular types of mathematical figures, including geometric diagrams, function plots, and three types of statistical charts, to provide a comprehensive and effective measurement of existing MLLMs. Our experimental evaluation involves nine mainstream MLLMs, and the results reveal that existing models still lag significantly behind human performance in performing fine-grained visual operations.</li>
</ul>

<h3>Title: HIVMedQA: Benchmarking large language models for HIV medical decision support</h3>
<ul>
<li><strong>Authors: </strong>Gonzalo Cardenal Antolin, Jacques Fellay, Bashkim Jaha, Roger Kouyos, Niko Beerenwinkel, Diane Duroux</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18143">https://arxiv.org/abs/2507.18143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18143">https://arxiv.org/pdf/2507.18143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18143]] HIVMedQA: Benchmarking large language models for HIV medical decision support(https://arxiv.org/abs/2507.18143)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are emerging as valuable tools to support clinicians in routine decision-making. HIV management is a compelling use case due to its complexity, including diverse treatment options, comorbidities, and adherence challenges. However, integrating LLMs into clinical practice raises concerns about accuracy, potential harm, and clinician acceptance. Despite their promise, AI applications in HIV care remain underexplored, and LLM benchmarking studies are scarce. This study evaluates the current capabilities of LLMs in HIV management, highlighting their strengths and limitations. We introduce HIVMedQA, a benchmark designed to assess open-ended medical question answering in HIV care. The dataset consists of curated, clinically relevant questions developed with input from an infectious disease physician. We evaluated seven general-purpose and three medically specialized LLMs, applying prompt engineering to enhance performance. Our evaluation framework incorporates both lexical similarity and an LLM-as-a-judge approach, extended to better reflect clinical relevance. We assessed performance across key dimensions: question comprehension, reasoning, knowledge recall, bias, potential harm, and factual accuracy. Results show that Gemini 2.5 Pro consistently outperformed other models across most dimensions. Notably, two of the top three models were proprietary. Performance declined as question complexity increased. Medically fine-tuned models did not always outperform general-purpose ones, and larger model size was not a reliable predictor of performance. Reasoning and comprehension were more challenging than factual recall, and cognitive biases such as recency and status quo were observed. These findings underscore the need for targeted development and evaluation to ensure safe, effective LLM integration in clinical care.</li>
</ul>

<h3>Title: Degradation-Consistent Learning via Bidirectional Diffusion for Low-Light Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Jinhong He, Minglong Xue, Zhipu Liu, Mingliang Zhou, Aoxiang Ning, Palaiahnakote Shivakumara</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18144">https://arxiv.org/abs/2507.18144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18144">https://arxiv.org/pdf/2507.18144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18144]] Degradation-Consistent Learning via Bidirectional Diffusion for Low-Light Image Enhancement(https://arxiv.org/abs/2507.18144)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Low-light image enhancement aims to improve the visibility of degraded images to better align with human visual perception. While diffusion-based methods have shown promising performance due to their strong generative capabilities. However, their unidirectional modelling of degradation often struggles to capture the complexity of real-world degradation patterns, leading to structural inconsistencies and pixel misalignments. To address these challenges, we propose a bidirectional diffusion optimization mechanism that jointly models the degradation processes of both low-light and normal-light images, enabling more precise degradation parameter matching and enhancing generation quality. Specifically, we perform bidirectional diffusion-from low-to-normal light and from normal-to-low light during training and introduce an adaptive feature interaction block (AFI) to refine feature representation. By leveraging the complementarity between these two paths, our approach imposes an implicit symmetry constraint on illumination attenuation and noise distribution, facilitating consistent degradation learning and improving the models ability to perceive illumination and detail degradation. Additionally, we design a reflection-aware correction module (RACM) to guide color restoration post-denoising and suppress overexposed regions, ensuring content consistency and generating high-quality images that align with human visual perception. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms state-of-the-art methods in both quantitative and qualitative evaluations while generalizing effectively to diverse degradation scenarios. Code at this https URL</li>
</ul>

<h3>Title: When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation Method with LLM and Pseudo Label</h3>
<ul>
<li><strong>Authors: </strong>Riting Xia, Rucong Wang, Yulin Liu, Anchen Li, Xueyan Liu, Yan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18153">https://arxiv.org/abs/2507.18153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18153">https://arxiv.org/pdf/2507.18153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18153]] When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation Method with LLM and Pseudo Label(https://arxiv.org/abs/2507.18153)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Class-imbalanced graph node classification is a practical yet underexplored research problem. Although recent studies have attempted to address this issue, they typically assume clean and reliable labels when processing class-imbalanced graphs. This assumption often violates the nature of real-world graphs, where labels frequently contain noise. Given this gap, this paper systematically investigates robust node classification for class-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph Augmentation framework based on Large language models (LLMs) and Pseudo-labeling techniques. Specifically, we design an LLM-based oversampling method to generate synthetic minority nodes, producing label-accurate minority nodes to alleviate class imbalance. Based on the class-balanced graphs, we develop a dynamically weighted pseudo-labeling method to obtain high-confidence pseudo labels to reduce label noise ratio. Additionally, we implement a secondary LLM-guided oversampling mechanism to mitigate potential class distribution skew caused by pseudo labels. Experimental results show that GraphALP achieves superior performance over state-of-the-art methods on class-imbalanced graphs with noisy labels.</li>
</ul>

<h3>Title: An Improved ChaCha Algorithm Based on Quantum Random Number</h3>
<ul>
<li><strong>Authors: </strong>Chao Liu, Shuai Zhao, Chenhao Jia, Gengran Hu, Tingting Cui</a></li>
<li><strong>Subjects: </strong>cs.CR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18157">https://arxiv.org/abs/2507.18157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18157">https://arxiv.org/pdf/2507.18157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18157]] An Improved ChaCha Algorithm Based on Quantum Random Number(https://arxiv.org/abs/2507.18157)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Due to the merits of high efficiency and strong security against timing and side-channel attacks, ChaCha has been widely applied in real-time communication and data streaming scenarios. However, with the rapid development of AI-assisted cryptanalysis and quantum computing technologies, there are serious challenges to the secure implementation of ChaCha cipher. To further strengthen the security of ChaCha cipher, we propose an improved variant based on quantum random numbers, i.e., Quantum Random Number Enhanced ChaCha (QRE-ChaCha). Specifically, the design XORs the initial constants with quantum random numbers and periodically injects quantum random numbers into selected state words during odd rounds to enhance diffusion. Compared with the original ChaCha, the present variant shows stronger resistance to differential attacks and generates a keystream with statistical randomness, thereby offering increased robustness against both classical and quantum attacks. To evaluate the security and performance of the present ChaCha, our analysis proceeds in three main parts. Firstly, we analyze its theoretical security in terms of quantum randomness and attack testing, and conduct differential cryptanalysis with an automated search method based on the Boolean satisfiability problem (SAT). Secondly, we subject the keystream generated by the cipher to randomness tests using the NIST statistical test suite and the GM/T 0005-2021 randomness testing standard. Finally, we assess its encryption and decryption performance by measuring its encryption speed on files of various sizes. According to the results, the present ChaCha is significantly improved to resist differential attacks while maintaining the high efficiency of the original ChaCha cipher, and its keystream successfully passes statistical randomness tests using the NIST and GM/T 0005-2021 standards, meeting cryptographic application requirements.</li>
</ul>

<h3>Title: Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models</h3>
<ul>
<li><strong>Authors: </strong>Kexin Chen, Dongxia Wang, Yi Liu, Haonan Zhang, Wenhai Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18171">https://arxiv.org/abs/2507.18171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18171">https://arxiv.org/pdf/2507.18171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18171]] Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models(https://arxiv.org/abs/2507.18171)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Despite the widespread use of Transformer-based text embedding models in NLP tasks, surprising 'sticky tokens' can undermine the reliability of embeddings. These tokens, when repeatedly inserted into sentences, pull sentence similarity toward a certain value, disrupting the normal distribution of embedding distances and degrading downstream performance. In this paper, we systematically investigate such anomalous tokens, formally defining them and introducing an efficient detection method, Sticky Token Detector (STD), based on sentence and token filtering. Applying STD to 40 checkpoints across 14 model families, we discover a total of 868 sticky tokens. Our analysis reveals that these tokens often originate from special or unused entries in the vocabulary, as well as fragmented subwords from multilingual corpora. Notably, their presence does not strictly correlate with model size or vocabulary size. We further evaluate how sticky tokens affect downstream tasks like clustering and retrieval, observing significant performance drops of up to 50%. Through attention-layer analysis, we show that sticky tokens disproportionately dominate the model's internal representations, raising concerns about tokenization robustness. Our findings show the need for better tokenization strategies and model design to mitigate the impact of sticky tokens in future text embedding applications.</li>
</ul>

<h3>Title: Unsupervised Domain Adaptation for 3D LiDAR Semantic Segmentation Using Contrastive Learning and Multi-Model Pseudo Labeling</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Kaushik, Norbert Haala, Uwe Soergel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18176">https://arxiv.org/abs/2507.18176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18176">https://arxiv.org/pdf/2507.18176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18176]] Unsupervised Domain Adaptation for 3D LiDAR Semantic Segmentation Using Contrastive Learning and Multi-Model Pseudo Labeling(https://arxiv.org/abs/2507.18176)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Addressing performance degradation in 3D LiDAR semantic segmentation due to domain shifts (e.g., sensor type, geographical location) is crucial for autonomous systems, yet manual annotation of target data is prohibitive. This study addresses the challenge using Unsupervised Domain Adaptation (UDA) and introduces a novel two-stage framework to tackle it. Initially, unsupervised contrastive learning at the segment level is used to pre-train a backbone network, enabling it to learn robust, domain-invariant features without labels. Subsequently, a multi-model pseudo-labeling strategy is introduced, utilizing an ensemble of diverse state-of-the-art architectures (including projection, voxel, hybrid, and cylinder-based methods). Predictions from these models are aggregated via hard voting to generate high-quality, refined pseudo-labels for the unlabeled target domain, mitigating single-model biases. The contrastively pre-trained network is then fine-tuned using these robust pseudo-labels. Experiments adapting from SemanticKITTI to unlabeled target datasets (SemanticPOSS, SemanticSlamantic) demonstrate significant improvements in segmentation accuracy compared to direct transfer and single-model UDA approaches. These results highlight the effectiveness of combining contrastive pre-training with refined ensemble pseudo-labeling for bridging complex domain gaps without requiring target domain annotations.</li>
</ul>

<h3>Title: Differential-UMamba: Rethinking Tumor Segmentation Under Limited Data Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Dhruv Jain, Romain Modzelewski, Romain Hérault, Clement Chatelain, Eva Torfeh, Sebastien Thureau</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18177">https://arxiv.org/abs/2507.18177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18177">https://arxiv.org/pdf/2507.18177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18177]] Differential-UMamba: Rethinking Tumor Segmentation Under Limited Data Scenarios(https://arxiv.org/abs/2507.18177)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In data-scarce scenarios, deep learning models often overfit to noise and irrelevant patterns, which limits their ability to generalize to unseen samples. To address these challenges in medical image segmentation, we introduce Diff-UMamba, a novel architecture that combines the UNet framework with the mamba mechanism for modeling long-range dependencies. At the heart of Diff-UMamba is a Noise Reduction Module (NRM), which employs a signal differencing strategy to suppress noisy or irrelevant activations within the encoder. This encourages the model to filter out spurious features and enhance task-relevant representations, thereby improving its focus on clinically meaningful regions. As a result, the architecture achieves improved segmentation accuracy and robustness, particularly in low-data settings. Diff-UMamba is evaluated on multiple public datasets, including MSD (lung and pancreas) and AIIB23, demonstrating consistent performance gains of 1-3% over baseline methods across diverse segmentation tasks. To further assess performance under limited-data conditions, additional experiments are conducted on the BraTS-21 dataset by varying the proportion of available training samples. The approach is also validated on a small internal non-small cell lung cancer (NSCLC) dataset for gross tumor volume (GTV) segmentation in cone beam CT (CBCT), where it achieves a 4-5% improvement over the baseline.</li>
</ul>

<h3>Title: SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wonjun Jeong, Dongseok Kim, Taegkeun Whangbo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18182">https://arxiv.org/abs/2507.18182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18182">https://arxiv.org/pdf/2507.18182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18182]] SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models(https://arxiv.org/abs/2507.18182)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can achieve inflated scores on multiple-choice tasks by exploiting inherent biases in option positions or labels, rather than demonstrating genuine understanding. This study introduces SCOPE, an evaluation framework designed to measure and mitigate such selection bias in a dataset-independent manner. By repeatedly invoking a null prompt that lacks semantic content, SCOPE estimates each model's unique position-bias distribution. It then redistributes the answer slot according to the inverse-bias distribution, thereby equalizing the lucky-rate, the probability of selecting the correct answer by chance. Furthermore, it prevents semantically similar distractors from being placed adjacent to the answer, thereby blocking near-miss guesses based on superficial proximity cues. Across multiple benchmark experiments, SCOPE consistently outperformed existing debiasing methods in terms of stable performance improvements and showed clearer confidence distributions over correct options. This framework thus offers a new standard for enhancing the fairness and reliability of LLM evaluations.</li>
</ul>

<h3>Title: ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory</h3>
<ul>
<li><strong>Authors: </strong>Jianchao Wang, Qingfeng Li, Pengcheng Zheng, Xiaorong Pu, Yazhou Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18183">https://arxiv.org/abs/2507.18183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18183">https://arxiv.org/pdf/2507.18183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18183]] ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory(https://arxiv.org/abs/2507.18183)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Training deep neural networks on real-world datasets is often hampered by the presence of noisy labels, which can be memorized by over-parameterized models, leading to significant degradation in generalization performance. While existing methods for learning with noisy labels (LNL) have made considerable progress, they fundamentally suffer from static snapshot evaluations and fail to leverage the rich temporal dynamics of learning evolution. In this paper, we propose ChronoSelect (chrono denoting its temporal nature), a novel framework featuring an innovative four-stage memory architecture that compresses prediction history into compact temporal distributions. Our unique sliding update mechanism with controlled decay maintains only four dynamic memory units per sample, progressively emphasizing recent patterns while retaining essential historical knowledge. This enables precise three-way sample partitioning into clean, boundary, and noisy subsets through temporal trajectory analysis and dual-branch consistency. Theoretical guarantees prove the mechanism's convergence and stability under noisy conditions. Extensive experiments demonstrate ChronoSelect's state-of-the-art performance across synthetic and real-world benchmarks.</li>
</ul>

<h3>Title: MatSSL: Robust Self-Supervised Representation Learning for Metallographic Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hoang Hai Nam Nguyen, Phan Nguyen Duc Hieu, Ho Won Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18184">https://arxiv.org/abs/2507.18184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18184">https://arxiv.org/pdf/2507.18184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18184]] MatSSL: Robust Self-Supervised Representation Learning for Metallographic Image Segmentation(https://arxiv.org/abs/2507.18184)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>MatSSL is a streamlined self-supervised learning (SSL) architecture that employs Gated Feature Fusion at each stage of the backbone to integrate multi-level representations effectively. Current micrograph analysis of metallic materials relies on supervised methods, which require retraining for each new dataset and often perform inconsistently with only a few labeled samples. While SSL offers a promising alternative by leveraging unlabeled data, most existing methods still depend on large-scale datasets to be effective. MatSSL is designed to overcome this limitation. We first perform self-supervised pretraining on a small-scale, unlabeled dataset and then fine-tune the model on multiple benchmark datasets. The resulting segmentation models achieve 69.13% mIoU on MetalDAM, outperforming the 66.73% achieved by an ImageNet-pretrained encoder, and delivers consistently up to nearly 40% improvement in average mIoU on the Environmental Barrier Coating benchmark dataset (EBC) compared to models pretrained with MicroNet. This suggests that MatSSL enables effective adaptation to the metallographic domain using only a small amount of unlabeled data, while preserving the rich and transferable features learned from large-scale pretraining on natural images.</li>
</ul>

<h3>Title: Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection</h3>
<ul>
<li><strong>Authors: </strong>San Kim, Jonghwi Kim, Yejin Jeon, Gary Geunbae Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18202">https://arxiv.org/abs/2507.18202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18202">https://arxiv.org/pdf/2507.18202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18202]] Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection(https://arxiv.org/abs/2507.18202)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by providing external knowledge for accurate and up-to-date responses. However, this reliance on external sources exposes a security risk, attackers can inject poisoned documents into the knowledge base to steer the generation process toward harmful or misleading outputs. In this paper, we propose Gradient-based Masked Token Probability (GMTP), a novel defense method to detect and filter out adversarially crafted documents. Specifically, GMTP identifies high-impact tokens by examining gradients of the retriever's similarity function. These key tokens are then masked, and their probabilities are checked via a Masked Language Model (MLM). Since injected tokens typically exhibit markedly low masked-token probabilities, this enables GMTP to easily detect malicious documents and achieve high-precision filtering. Experiments demonstrate that GMTP is able to eliminate over 90% of poisoned content while retaining relevant documents, thus maintaining robust retrieval and generation performance across diverse datasets and adversarial settings.</li>
</ul>

<h3>Title: Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation</h3>
<ul>
<li><strong>Authors: </strong>Kyubeen Han, Junseo Jang, Hongjin Kim, Geunyeong Jeong, Harksoo Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18203">https://arxiv.org/abs/2507.18203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18203">https://arxiv.org/pdf/2507.18203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18203]] Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation(https://arxiv.org/abs/2507.18203)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Instruction-tuning enhances the ability of large language models (LLMs) to follow user instructions more accurately, improving usability while reducing harmful outputs. However, this process may increase the model's dependence on user input, potentially leading to the unfiltered acceptance of misinformation and the generation of hallucinations. Existing studies primarily highlight that LLMs are receptive to external information that contradict their parametric knowledge, but little research has been conducted on the direct impact of instruction-tuning on this phenomenon. In our study, we investigate the impact of instruction-tuning on LLM's susceptibility to misinformation. Our analysis reveals that instruction-tuned LLMs are significantly more likely to accept misinformation when it is presented by the user. A comparison with base models shows that instruction-tuning increases reliance on user-provided information, shifting susceptibility from the assistant role to the user role. Furthermore, we explore additional factors influencing misinformation susceptibility, such as the role of the user in prompt structure, misinformation length, and the presence of warnings in the system prompt. Our findings underscore the need for systematic approaches to mitigate unintended consequences of instruction-tuning and enhance the reliability of LLMs in real-world applications.</li>
</ul>

<h3>Title: Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation</h3>
<ul>
<li><strong>Authors: </strong>Xinrui Chen, Hongxing Zhang, Fanyi Zeng, Yongxian Wei, Yizhi Wang, Xitong Ling, Guanghao Li, Chun Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18212">https://arxiv.org/abs/2507.18212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18212">https://arxiv.org/pdf/2507.18212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18212]] Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation(https://arxiv.org/abs/2507.18212)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Layer pruning has emerged as a promising technique for compressing large language models (LLMs) while achieving acceleration proportional to the pruning ratio. In this work, we identify that removing any layer induces a significant magnitude gap in hidden states, resulting in substantial performance degradation. To address this issue, we propose Prune&Comp, a novel plug-and-play layer pruning scheme that leverages magnitude compensation to mitigate such gaps in a training-free manner. Specifically, we first estimate the magnitude gap caused by layer removal and then eliminate this gap by rescaling the remaining weights offline, with zero runtime overhead incurred. We further demonstrate the advantages of Prune&Comp through an iterative pruning strategy. When integrated with an iterative prune-and-compensate loop, Prune&Comp consistently enhances existing layer pruning metrics. For instance, when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence metric, Prune&Comp nearly halves the perplexity and retains 93.19\% of the original model's question-answering performance, outperforming the baseline by 4.01%.</li>
</ul>

<h3>Title: LEAF: Latent Diffusion with Efficient Encoder Distillation for Aligned Features in Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Qilin Huang, Tianyu Lin, Zhiguang Chen, Fudan Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18214">https://arxiv.org/abs/2507.18214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18214">https://arxiv.org/pdf/2507.18214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18214]] LEAF: Latent Diffusion with Efficient Encoder Distillation for Aligned Features in Medical Image Segmentation(https://arxiv.org/abs/2507.18214)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Leveraging the powerful capabilities of diffusion models has yielded quite effective results in medical image segmentation tasks. However, existing methods typically transfer the original training process directly without specific adjustments for segmentation tasks. Furthermore, the commonly used pre-trained diffusion models still have deficiencies in feature extraction. Based on these considerations, we propose LEAF, a medical image segmentation model grounded in latent diffusion models. During the fine-tuning process, we replace the original noise prediction pattern with a direct prediction of the segmentation map, thereby reducing the variance of segmentation results. We also employ a feature distillation method to align the hidden states of the convolutional layers with the features from a transformer-based vision encoder. Experimental results demonstrate that our method enhances the performance of the original diffusion model across multiple segmentation datasets for different disease types. Notably, our approach does not alter the model architecture, nor does it increase the number of parameters or computation during the inference phase, making it highly efficient.</li>
</ul>

<h3>Title: Information Security Based on LLM Approaches: A Review</h3>
<ul>
<li><strong>Authors: </strong>Chang Gong, Zhongwen Li, Xiaoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18215">https://arxiv.org/abs/2507.18215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18215">https://arxiv.org/pdf/2507.18215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18215]] Information Security Based on LLM Approaches: A Review(https://arxiv.org/abs/2507.18215)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Information security is facing increasingly severe challenges, and traditional protection means are difficult to cope with complex and changing threats. In recent years, as an emerging intelligent technology, large language models (LLMs) have shown a broad application prospect in the field of information security. In this paper, we focus on the key role of LLM in information security, systematically review its application progress in malicious behavior prediction, network threat analysis, system vulnerability detection, malicious code identification, and cryptographic algorithm optimization, and explore its potential in enhancing security protection performance. Based on neural networks and Transformer architecture, this paper analyzes the technical basis of large language models and their advantages in natural language processing tasks. It is shown that the introduction of large language modeling helps to improve the detection accuracy and reduce the false alarm rate of security systems. Finally, this paper summarizes the current application results and points out that it still faces challenges in model transparency, interpretability, and scene adaptability, among other issues. It is necessary to explore further the optimization of the model structure and the improvement of the generalization ability to realize a more intelligent and accurate information security protection system.</li>
</ul>

<h3>Title: FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with Personalized Aggregation and Cluster-Aware Broadcasting</h3>
<ul>
<li><strong>Authors: </strong>Zhongzheng Yuan, Lianshuai Guo, Xunkai Li, Yinlin Zhu, Wenyu Wang, Meixia Qu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18219">https://arxiv.org/abs/2507.18219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18219">https://arxiv.org/pdf/2507.18219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18219]] FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with Personalized Aggregation and Cluster-Aware Broadcasting(https://arxiv.org/abs/2507.18219)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Graph Learning (FGL) is a distributed learning paradigm that enables collaborative training over large-scale subgraphs located on multiple local systems. However, most existing FGL approaches rely on synchronous communication, which leads to inefficiencies and is often impractical in real-world deployments. Meanwhile, current asynchronous federated learning (AFL) methods are primarily designed for conventional tasks such as image classification and natural language processing, without accounting for the unique topological properties of graph data. Directly applying these methods to graph learning can possibly result in semantic drift and representational inconsistency in the global model. To address these challenges, we propose FedSA-GCL, a semi-asynchronous federated framework that leverages both inter-client label distribution divergence and graph topological characteristics through a novel ClusterCast mechanism for efficient training. We evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain and Metis split algorithms, and compare it against 9 baselines. Extensive experiments demonstrate that our method achieves strong robustness and outstanding efficiency, outperforming the baselines by an average of 2.92% with the Louvain and by 3.4% with the Metis.</li>
</ul>

<h3>Title: Sparse identification of nonlinear dynamics with library optimization mechanism: Recursive long-term prediction perspective</h3>
<ul>
<li><strong>Authors: </strong>Ansei Yonezawa, Heisei Yonezawa, Shuichi Yahagi, Itsuro Kajiwara, Shinya Kijimoto, Hikaru Taniuchi, Kentaro Murakami</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18220">https://arxiv.org/abs/2507.18220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18220">https://arxiv.org/pdf/2507.18220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18220]] Sparse identification of nonlinear dynamics with library optimization mechanism: Recursive long-term prediction perspective(https://arxiv.org/abs/2507.18220)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The sparse identification of nonlinear dynamics (SINDy) approach can discover the governing equations of dynamical systems based on measurement data, where the dynamical model is identified as the sparse linear combination of the given basis functions. A major challenge in SINDy is the design of a library, which is a set of candidate basis functions, as the appropriate library is not trivial for many dynamical systems. To overcome this difficulty, this study proposes SINDy with library optimization mechanism (SINDy-LOM), which is a combination of the sparse regression technique and the novel learning strategy of the library. In the proposed approach, the basis functions are parametrized. The SINDy-LOM approach involves a two-layer optimization architecture: the inner-layer, in which the data-driven model is extracted as the sparse linear combination of the candidate basis functions, and the outer-layer, in which the basis functions are optimized from the viewpoint of the recursive long-term (RLT) prediction accuracy; thus, the library design is reformulated as the optimization of the parametrized basis functions. The resulting SINDy-LOM model has good interpretability and usability, as the proposed approach yields the parsimonious model. The library optimization mechanism significantly reduces user burden. The RLT perspective improves the reliability of the resulting model compared with the traditional SINDy approach that can only ensure the one-step-ahead prediction accuracy. The validity of the proposed approach is demonstrated by applying it to a diesel engine airpath system, which is a well-known complex industrial system.</li>
</ul>

<h3>Title: DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in Collaborative Perception</h3>
<ul>
<li><strong>Authors: </strong>Chengchang Tian, Jianwei Ma, Yan Huang, Zhanye Chen, Honghao Wei, Hui Zhang, Wei Hong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18237">https://arxiv.org/abs/2507.18237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18237">https://arxiv.org/pdf/2507.18237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18237]] DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in Collaborative Perception(https://arxiv.org/abs/2507.18237)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Feature-level fusion shows promise in collaborative perception (CP) through balanced performance and communication bandwidth trade-off. However, its effectiveness critically relies on input feature quality. The acquisition of high-quality features faces domain gaps from hardware diversity and deployment conditions, alongside temporal misalignment from transmission delays. These challenges degrade feature quality with cumulative effects throughout the collaborative network. In this paper, we present the Domain-And-Time Alignment (DATA) network, designed to systematically align features while maximizing their semantic representations for fusion. Specifically, we propose a Consistency-preserving Domain Alignment Module (CDAM) that reduces domain gaps through proximal-region hierarchical downsampling and observability-constrained discriminator. We further propose a Progressive Temporal Alignment Module (PTAM) to handle transmission delays via multi-scale motion modeling and two-stage compensation. Building upon the aligned features, an Instance-focused Feature Aggregation Module (IFAM) is developed to enhance semantic representations. Extensive experiments demonstrate that DATA achieves state-of-the-art performance on three typical datasets, maintaining robustness with severe communication delays and pose errors. The code will be released at this https URL.</li>
</ul>

<h3>Title: DepthDark: Robust Monocular Depth Estimation for Low-Light Environments</h3>
<ul>
<li><strong>Authors: </strong>Longjian Zeng, Zunjie Zhu, Rongfeng Lu, Ming Lu, Bolun Zheng, Chenggang Yan, Anke Xue</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18243">https://arxiv.org/abs/2507.18243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18243">https://arxiv.org/pdf/2507.18243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18243]] DepthDark: Robust Monocular Depth Estimation for Low-Light Environments(https://arxiv.org/abs/2507.18243)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In recent years, foundation models for monocular depth estimation have received increasing attention. Current methods mainly address typical daylight conditions, but their effectiveness notably decreases in low-light environments. There is a lack of robust foundational models for monocular depth estimation specifically designed for low-light scenarios. This largely stems from the absence of large-scale, high-quality paired depth datasets for low-light conditions and the effective parameter-efficient fine-tuning (PEFT) strategy. To address these challenges, we propose DepthDark, a robust foundation model for low-light monocular depth estimation. We first introduce a flare-simulation module and a noise-simulation module to accurately simulate the imaging process under nighttime conditions, producing high-quality paired depth datasets for low-light conditions. Additionally, we present an effective low-light PEFT strategy that utilizes illumination guidance and multiscale feature fusion to enhance the model's capability in low-light environments. Our method achieves state-of-the-art depth estimation performance on the challenging nuScenes-Night and RobotCar-Night datasets, validating its effectiveness using limited training data and computing resources.</li>
</ul>

<h3>Title: Auto-SGCR: Automated Generation of Smart Grid Cyber Range Using IEC 61850 Standard Models</h3>
<ul>
<li><strong>Authors: </strong>Muhammad M. Roomi, S. M. Suhail Hussain, Ee-Chien Chang, David M. Nicol, Daisuke Mashima</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18249">https://arxiv.org/abs/2507.18249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18249">https://arxiv.org/pdf/2507.18249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18249]] Auto-SGCR: Automated Generation of Smart Grid Cyber Range Using IEC 61850 Standard Models(https://arxiv.org/abs/2507.18249)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Digitalization of power grids have made them increasingly susceptible to cyber-attacks in the past decade. Iterative cybersecurity testing is indispensable to counter emerging attack vectors and to ensure dependability of critical infrastructure. Furthermore, these can be used to evaluate cybersecurity configuration, effectiveness of the cybersecurity measures against various attack vectors, as well as to train smart grid cybersecurity experts defending the system. Enabling extensive experiments narrows the gap between academic research and production environment. A high-fidelity cyber range is vital as it is often infeasible to conduct such experiments and training using production environment. However, the design and implementation of cyber range requires extensive domain knowledge of physical and cyber aspect of the infrastructure. Furthermore, costs incurred for setup and maintenance of cyber range are significant. Moreover, most existing smart grid cyber ranges are designed as a one-off, proprietary system, and are limited in terms of configurability, accessibility, portability, and reproducibility. To address these challenges, an automated Smart grid Cyber Range generation framework is presented in this paper. Initially a human-/machine-friendly, XML-based modeling language called Smart Grid Modeling Language was defined, which incorporates IEC 61850 System Configuration Language files. Subsequently, a toolchain to parse SG-ML model files and automatically instantiate a functional smart grid cyber range was developed. The developed SG-ML models can be easily shared and/or modified to reproduce or customize for any cyber range. The application of Auto-SGCR is demonstrated through case studies with large-scale substation models. The toolchain along with example SG-ML models have been open-sourced.</li>
</ul>

<h3>Title: Exploiting Gaussian Agnostic Representation Learning with Diffusion Priors for Enhanced Infrared Small Target Detection</h3>
<ul>
<li><strong>Authors: </strong>Junyao Li, Yahao Lu, Xingyuan Guo, Xiaoyu Xian, Tiantian Wang, Yukai Shi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18260">https://arxiv.org/abs/2507.18260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18260">https://arxiv.org/pdf/2507.18260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18260]] Exploiting Gaussian Agnostic Representation Learning with Diffusion Priors for Enhanced Infrared Small Target Detection(https://arxiv.org/abs/2507.18260)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Infrared small target detection (ISTD) plays a vital role in numerous practical applications. In pursuit of determining the performance boundaries, researchers employ large and expensive manual-labeling data for representation learning. Nevertheless, this approach renders the state-of-the-art ISTD methods highly fragile in real-world challenges. In this paper, we first study the variation in detection performance across several mainstream methods under various scarcity -- namely, the absence of high-quality infrared data -- that challenge the prevailing theories about practical ISTD. To address this concern, we introduce the Gaussian Agnostic Representation Learning. Specifically, we propose the Gaussian Group Squeezer, leveraging Gaussian sampling and compression for non-uniform quantization. By exploiting a diverse array of training samples, we enhance the resilience of ISTD models against various challenges. Then, we introduce two-stage diffusion models for real-world reconstruction. By aligning quantized signals closely with real-world distributions, we significantly elevate the quality and fidelity of the synthetic samples. Comparative evaluations against state-of-the-art detection methods in various scarcity scenarios demonstrate the efficacy of the proposed approach.</li>
</ul>

<h3>Title: Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models</h3>
<ul>
<li><strong>Authors: </strong>Suhang Wu, Jialong Tang, Chengyi Yang, Pei Zhang, Baosong Yang, Junhui Li, Junfeng Yao, Min Zhang, Jinsong Su</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18263">https://arxiv.org/abs/2507.18263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18263">https://arxiv.org/pdf/2507.18263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18263]] Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models(https://arxiv.org/abs/2507.18263)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Direct speech translation (ST) has garnered increasing attention nowadays, yet the accurate translation of terminology within utterances remains a great challenge. In this regard, current studies mainly concentrate on leveraging various translation knowledge into ST models. However, these methods often struggle with interference from irrelevant noise and can not fully utilize the translation knowledge. To address these issues, in this paper, we propose a novel Locate-and-Focus method for terminology translation. It first effectively locates the speech clips containing terminologies within the utterance to construct translation knowledge, minimizing irrelevant information for the ST model. Subsequently, it associates the translation knowledge with the utterance and hypothesis from both audio and textual modalities, allowing the ST model to better focus on translation knowledge during translation. Experimental results across various datasets demonstrate that our method effectively locates terminologies within utterances and enhances the success rate of terminology translation, while maintaining robust general translation performance.</li>
</ul>

<h3>Title: StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer</h3>
<ul>
<li><strong>Authors: </strong>Pritika Ramu, Apoorv Saxena, Meghanath M Y, Varsha Sankar, Debraj Basu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18294">https://arxiv.org/abs/2507.18294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18294">https://arxiv.org/pdf/2507.18294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18294]] StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer(https://arxiv.org/abs/2507.18294)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Adapting LLMs to specific stylistic characteristics, like brand voice or authorial tones, is crucial for enterprise communication but challenging to achieve from corpora which lacks instruction-response formatting without compromising instruction adherence. We introduce StyleAdaptedLM, a framework that efficiently transfers stylistic traits to instruction-following models using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base model with diverse unstructured stylistic corpora, then merged with a separate instruction-following model. This enables robust stylistic customization without paired data or sacrificing task performance. Experiments across multiple datasets and models demonstrate improved stylistic consistency while preserving instruction adherence, with human evaluations confirming brand-specific convention uptake. StyleAdaptedLM offers an efficient path for stylistic personalization in LLMs.</li>
</ul>

<h3>Title: LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models</h3>
<ul>
<li><strong>Authors: </strong>Delong Ran, Xinlei He, Tianshuo Cong, Anyu Wang, Qi Li, Xiaoyun Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18302">https://arxiv.org/abs/2507.18302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18302">https://arxiv.org/pdf/2507.18302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18302]] LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models(https://arxiv.org/abs/2507.18302)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Language Models (LMs) typically adhere to a "pre-training and fine-tuning" paradigm, where a universal pre-trained model can be fine-tuned to cater to various specialized domains. Low-Rank Adaptation (LoRA) has gained the most widespread use in LM fine-tuning due to its lightweight computational cost and remarkable performance. Because the proportion of parameters tuned by LoRA is relatively small, there might be a misleading impression that the LoRA fine-tuning data is invulnerable to Membership Inference Attacks (MIAs). However, we identify that utilizing the pre-trained model can induce more information leakage, which is neglected by existing MIAs. Therefore, we introduce LoRA-Leak, a holistic evaluation framework for MIAs against the fine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership inference attacks, including ten existing MIAs, and five improved MIAs that leverage the pre-trained model as a reference. In experiments, we apply LoRA-Leak to three advanced LMs across three popular natural language processing tasks, demonstrating that LoRA-based fine-tuned LMs are still vulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings). We also applied LoRA-Leak to different fine-tuning settings to understand the resulting privacy risks. We further explore four defenses and find that only dropout and excluding specific LM layers during fine-tuning effectively mitigate MIA risks while maintaining utility. We highlight that under the "pre-training and fine-tuning" paradigm, the existence of the pre-trained model makes MIA a more severe risk for LoRA-based LMs. We hope that our findings can provide guidance on data privacy protection for specialized LM providers.</li>
</ul>

<h3>Title: BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit</h3>
<ul>
<li><strong>Authors: </strong>Biao Yi, Zekun Fei, Jianing Geng, Tong Li, Lihai Nie, Zheli Liu, Yiming Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18305">https://arxiv.org/abs/2507.18305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18305">https://arxiv.org/pdf/2507.18305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18305]] BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit(https://arxiv.org/abs/2507.18305)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Large reasoning models (LRMs) have emerged as a significant advancement in artificial intelligence, representing a specialized class of large language models (LLMs) designed to tackle complex reasoning tasks. The defining characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning capabilities. In this paper, we identify a previously unexplored attack vector against LRMs, which we term "overthinking backdoors". We advance this concept by proposing a novel tunable backdoor, which moves beyond simple on/off attacks to one where an attacker can precisely control the extent of the model's reasoning verbosity. Our attack is implemented through a novel data poisoning methodology. It pairs a tunable trigger-where the number of repetitions signals the desired intensity-with a correspondingly verbose CoT response. These responses are programmatically generated by instructing a teacher LLM to inject a controlled number of redundant refinement steps into a correct reasoning process. The approach preserves output correctness, which ensures stealth and establishes the attack as a pure resource-consumption vector. Extensive empirical results on various LRMs demonstrate that our method can reliably trigger a controllable, multi-fold increase in the length of the reasoning process, without degrading the final answer's correctness. Our source code is available at this https URL.</li>
</ul>

<h3>Title: Regression-aware Continual Learning for Android Malware Detection</h3>
<ul>
<li><strong>Authors: </strong>Daniele Ghiani, Daniele Angioni, Giorgio Piras, Angelo Sotgiu, Luca Minnei, Srishti Gupta, Maura Pintor, Fabio Roli, Battista Biggio</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18313">https://arxiv.org/abs/2507.18313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18313">https://arxiv.org/pdf/2507.18313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18313]] Regression-aware Continual Learning for Android Malware Detection(https://arxiv.org/abs/2507.18313)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Malware evolves rapidly, forcing machine learning (ML)-based detectors to adapt continuously. With antivirus vendors processing hundreds of thousands of new samples daily, datasets can grow to billions of examples, making full retraining impractical. Continual learning (CL) has emerged as a scalable alternative, enabling incremental updates without full data access while mitigating catastrophic forgetting. In this work, we analyze a critical yet overlooked issue in this context: security regression. Unlike forgetting, which manifests as a general performance drop on previously seen data, security regression captures harmful prediction changes at the sample level, such as a malware sample that was once correctly detected but evades detection after a model update. Although often overlooked, regressions pose serious risks in security-critical applications, as the silent reintroduction of previously detected threats in the system may undermine users' trust in the whole updating process. To address this issue, we formalize and quantify security regression in CL-based malware detectors and propose a regression-aware penalty to mitigate it. Specifically, we adapt Positive Congruent Training (PCT) to the CL setting, preserving prior predictive behavior in a model-agnostic manner. Experiments on the ELSA, Tesseract, and AZ-Class datasets show that our method effectively reduces regression across different CL scenarios while maintaining strong detection performance over time.</li>
</ul>

<h3>Title: State of Health Estimation of Batteries Using a Time-Informed Dynamic Sequence-Inverted Transformer</h3>
<ul>
<li><strong>Authors: </strong>Janak M. Patel, Milad Ramezankhani, Anirudh Deodhar, Dagnachew Birru</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18320">https://arxiv.org/abs/2507.18320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18320">https://arxiv.org/pdf/2507.18320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18320]] State of Health Estimation of Batteries Using a Time-Informed Dynamic Sequence-Inverted Transformer(https://arxiv.org/abs/2507.18320)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The rapid adoption of battery-powered vehicles and energy storage systems over the past decade has made battery health monitoring increasingly critical. Batteries play a central role in the efficiency and safety of these systems, yet they inevitably degrade over time due to repeated charge-discharge cycles. This degradation leads to reduced energy efficiency and potential overheating, posing significant safety concerns. Accurate estimation of a State of Health (SoH) of battery is therefore essential for ensuring operational reliability and safety. Several machine learning architectures, such as LSTMs, transformers, and encoder-based models, have been proposed to estimate SoH from discharge cycle data. However, these models struggle with the irregularities inherent in real-world measurements: discharge readings are often recorded at non-uniform intervals, and the lengths of discharge cycles vary significantly. To address this, most existing approaches extract features from the sequences rather than processing them in full, which introduces information loss and compromises accuracy. To overcome these challenges, we propose a novel architecture: Time-Informed Dynamic Sequence Inverted Transformer (TIDSIT). TIDSIT incorporates continuous time embeddings to effectively represent irregularly sampled data and utilizes padded sequences with temporal attention mechanisms to manage variable-length inputs without discarding sequence information. Experimental results on the NASA battery degradation dataset show that TIDSIT significantly outperforms existing models, achieving over 50% reduction in prediction error and maintaining an SoH prediction error below 0.58%. Furthermore, the architecture is generalizable and holds promise for broader applications in health monitoring tasks involving irregular time-series data.</li>
</ul>

<h3>Title: A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation</h3>
<ul>
<li><strong>Authors: </strong>Minje Park, Jeonghwa Lim, Taehyung Yu, Sunghoon Joo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18323">https://arxiv.org/abs/2507.18323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18323">https://arxiv.org/pdf/2507.18323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18323]] A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation(https://arxiv.org/abs/2507.18323)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform features, is critical for clinical diagnosis. Despite recent advances using deep learning, progress has been limited by the scarcity of publicly available annotated datasets. Semi-supervised learning presents a promising solution by leveraging abundant unlabeled ECG data. In this study, we present the first systematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG delineation. We curated and unified multiple public datasets, including previously underused sources, to support robust and diverse evaluation. We adopted five representative SemiSeg algorithms from computer vision, implemented them on two different architectures: the convolutional network and the transformer, and evaluated them in two different settings: in-domain and cross-domain. Additionally, we propose ECG-specific training configurations and augmentation strategies and introduce a standardized evaluation framework. Our results show that the transformer outperforms the convolutional network in semi-supervised ECG delineation. We anticipate that our benchmark will serve as a foundation for advancing semi-supervised ECG delineation methods and will facilitate further research in this domain.</li>
</ul>

<h3>Title: Beyond Low-rankness: Guaranteed Matrix Recovery via Modified Nuclear Norm</h3>
<ul>
<li><strong>Authors: </strong>Jiangjun Peng, Yisi Luo, Xiangyong Cao, Shuang Xu, Deyu Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18327">https://arxiv.org/abs/2507.18327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18327">https://arxiv.org/pdf/2507.18327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18327]] Beyond Low-rankness: Guaranteed Matrix Recovery via Modified Nuclear Norm(https://arxiv.org/abs/2507.18327)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The nuclear norm (NN) has been widely explored in matrix recovery problems, such as Robust PCA and matrix completion, leveraging the inherent global low-rank structure of the data. In this study, we introduce a new modified nuclear norm (MNN) framework, where the MNN family norms are defined by adopting suitable transformations and performing the NN on the transformed matrix. The MNN framework offers two main advantages: (1) it jointly captures both local information and global low-rankness without requiring trade-off parameter tuning; (2) Under mild assumptions on the transformation, we provided exact theoretical recovery guarantees for both Robust PCA and MC tasks-an achievement not shared by existing methods that combine local and global information. Thanks to its general and flexible design, MNN can accommodate various proven transformations, enabling a unified and effective approach to structured low-rank recovery. Extensive experiments demonstrate the effectiveness of our method. Code and supplementary material are available at this https URL.</li>
</ul>

<h3>Title: GVCCS: A Dataset for Contrail Identification and Tracking on Visible Whole Sky Camera Sequences</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Jarry, Ramon Dalmau, Philippe Very, Franck Ballerini, Stephania-Denisa Bocu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18330">https://arxiv.org/abs/2507.18330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18330">https://arxiv.org/pdf/2507.18330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18330]] GVCCS: A Dataset for Contrail Identification and Tracking on Visible Whole Sky Camera Sequences(https://arxiv.org/abs/2507.18330)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Aviation's climate impact includes not only CO2 emissions but also significant non-CO2 effects, especially from contrails. These ice clouds can alter Earth's radiative balance, potentially rivaling the warming effect of aviation CO2. Physics-based models provide useful estimates of contrail formation and climate impact, but their accuracy depends heavily on the quality of atmospheric input data and on assumptions used to represent complex processes like ice particle formation and humidity-driven persistence. Observational data from remote sensors, such as satellites and ground cameras, could be used to validate and calibrate these models. However, existing datasets don't explore all aspect of contrail dynamics and formation: they typically lack temporal tracking, and do not attribute contrails to their source flights. To address these limitations, we present the Ground Visible Camera Contrail Sequences (GVCCS), a new open data set of contrails recorded with a ground-based all-sky camera in the visible range. Each contrail is individually labeled and tracked over time, allowing a detailed analysis of its lifecycle. The dataset contains 122 video sequences (24,228 frames) and includes flight identifiers for contrails that form above the camera. As reference, we also propose a unified deep learning framework for contrail analysis using a panoptic segmentation model that performs semantic segmentation (contrail pixel identification), instance segmentation (individual contrail separation), and temporal tracking in a single architecture. By providing high-quality, temporally resolved annotations and a benchmark for model evaluation, our work supports improved contrail monitoring and will facilitate better calibration of physical models. This sets the groundwork for more accurate climate impact understanding and assessments.</li>
</ul>

<h3>Title: EgoExoBench: A Benchmark for First- and Third-person View Video Understanding in MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Yuping He, Yifei Huang, Guo Chen, Baoqi Pei, Jilan Xu, Tong Lu, Jiangmiao Pang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18342">https://arxiv.org/abs/2507.18342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18342">https://arxiv.org/pdf/2507.18342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18342]] EgoExoBench: A Benchmark for First- and Third-person View Video Understanding in MLLMs(https://arxiv.org/abs/2507.18342)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Transferring and integrating knowledge across first-person (egocentric) and third-person (exocentric) viewpoints is intrinsic to human intelligence, enabling humans to learn from others and convey insights from their own experiences. Despite rapid progress in multimodal large language models (MLLMs), their ability to perform such cross-view reasoning remains unexplored. To address this, we introduce EgoExoBench, the first benchmark for egocentric-exocentric video understanding and reasoning. Built from publicly available datasets, EgoExoBench comprises over 7,300 question-answer pairs spanning eleven sub-tasks organized into three core challenges: semantic alignment, viewpoint association, and temporal reasoning. We evaluate 13 state-of-the-art MLLMs and find that while these models excel on single-view tasks, they struggle to align semantics across perspectives, accurately associate views, and infer temporal dynamics in the ego-exo context. We hope EgoExoBench can serve as a valuable resource for research on embodied agents and intelligent assistants seeking human-like cross-view intelligence.</li>
</ul>

<h3>Title: Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Ariana Sahitaj, Premtim Sahitaj, Veronika Solopova, Jiaao Li, Sebastian Möller, Vera Schmitt</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18343">https://arxiv.org/abs/2507.18343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18343">https://arxiv.org/pdf/2507.18343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18343]] Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence(https://arxiv.org/abs/2507.18343)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Propaganda detection on social media remains challenging due to task complexity and limited high-quality labeled data. This paper introduces a novel framework that combines human expertise with Large Language Model (LLM) assistance to improve both annotation consistency and scalability. We propose a hierarchical taxonomy that organizes 14 fine-grained propaganda techniques into three broader categories, conduct a human annotation study on the HQP dataset that reveals low inter-annotator agreement for fine-grained labels, and implement an LLM-assisted pre-annotation pipeline that extracts propagandistic spans, generates concise explanations, and assigns local labels as well as a global label. A secondary human verification study shows significant improvements in both agreement and time-efficiency. Building on this, we fine-tune smaller language models (SLMs) to perform structured annotation. Instead of fine-tuning on human annotations, we train on high-quality LLM-generated data, allowing a large model to produce these annotations and a smaller model to learn to generate them via knowledge distillation. Our work contributes towards the development of scalable and robust propaganda detection systems, supporting the idea of transparent and accountable media ecosystems in line with SDG 16. The code is publicly available at our GitHub repository.</li>
</ul>

<h3>Title: VB-Mitigator: An Open-source Framework for Evaluating and Advancing Visual Bias Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Ioannis Sarridis, Christos Koutlis, Symeon Papadopoulos, Christos Diou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18348">https://arxiv.org/abs/2507.18348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18348">https://arxiv.org/pdf/2507.18348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18348]] VB-Mitigator: An Open-source Framework for Evaluating and Advancing Visual Bias Mitigation(https://arxiv.org/abs/2507.18348)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Bias in computer vision models remains a significant challenge, often resulting in unfair, unreliable, and non-generalizable AI systems. Although research into bias mitigation has intensified, progress continues to be hindered by fragmented implementations and inconsistent evaluation practices. Disparate datasets and metrics used across studies complicate reproducibility, making it difficult to fairly assess and compare the effectiveness of various approaches. To overcome these limitations, we introduce the Visual Bias Mitigator (VB-Mitigator), an open-source framework designed to streamline the development, evaluation, and comparative analysis of visual bias mitigation techniques. VB-Mitigator offers a unified research environment encompassing 12 established mitigation methods, 7 diverse benchmark datasets. A key strength of VB-Mitigator is its extensibility, allowing for seamless integration of additional methods, datasets, metrics, and models. VB-Mitigator aims to accelerate research toward fairness-aware computer vision models by serving as a foundational codebase for the research community to develop and assess their approaches. To this end, we also recommend best evaluation practices and provide a comprehensive performance comparison among state-of-the-art methodologies.</li>
</ul>

<h3>Title: Deformable Convolution Module with Globally Learned Relative Offsets for Fundus Vessel Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Lexuan Zhu, Yuxuan Li, Yuning Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18354">https://arxiv.org/abs/2507.18354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18354">https://arxiv.org/pdf/2507.18354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18354]] Deformable Convolution Module with Globally Learned Relative Offsets for Fundus Vessel Segmentation(https://arxiv.org/abs/2507.18354)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Deformable convolution can adaptively change the shape of convolution kernel by learning offsets to deal with complex shape features. We propose a novel plug and play deformable convolutional module that uses attention and feedforward networks to learn offsets, so that the deformable patterns can capture long-distance global features. Compared with previously existing deformable convolutions, the proposed module learns the sub pixel displacement field and adaptively warps the feature maps across all channels rather than directly deforms the convolution kernel , which is equivalent to a relative deformation of the kernel sampling grids, achieving global feature deformation and the decoupling of kernel size and learning network. Considering that the fundus blood vessels have globally self similar complex edges, we design a deep learning model for fundus blood vessel segmentation, GDCUnet, based on the proposed convolutional module. Empirical evaluations under the same configuration and unified framework show that GDCUnet has achieved state of the art performance on public datasets. Further ablation experiments demonstrated that the proposed deformable convolutional module could more significantly learn the complex features of fundus blood vessels, enhancing the model representation and generalization this http URL proposed module is similar to the interface of conventional convolution, we suggest applying it to more machine vision tasks with complex global self similar features.</li>
</ul>

<h3>Title: Conformidade com os Requisitos Legais de Privacidade de Dados: Um Estudo sobre Técnicas de Anonimização</h3>
<ul>
<li><strong>Authors: </strong>André Menolli, Luiz Fernando Nunes, Thiago A. Coleti</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18360">https://arxiv.org/abs/2507.18360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18360">https://arxiv.org/pdf/2507.18360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18360]] Conformidade com os Requisitos Legais de Privacidade de Dados: Um Estudo sobre Técnicas de Anonimização(https://arxiv.org/abs/2507.18360)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>The protection of personal data has become a central topic in software development, especially with the implementation of the General Data Protection Law (LGPD) in Brazil and the General Data Protection Regulation (GDPR) in the European Union. With the enforcement of these laws, certain software quality criteria have become mandatory, such as data anonymization, which is one of the main aspects addressed by these regulations. The aim of this article is to analyze data anonymization techniques and assess their effectiveness in ensuring compliance with legal requirements and the utility of the data for its intended purpose. Techniques such as aggregation, generalization, perturbation, and k-anonymity were investigated and applied to datasets containing personal and sensitive data. The analysis revealed significant variations in the effectiveness of each method, highlighting the need to balance privacy and data utility.</li>
</ul>

<h3>Title: Efficient Uncertainty in LLMs through Evidential Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Lakshmana Sri Harsha Nemani, P.K. Srijith, Tomasz Kuśmierczyk</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18366">https://arxiv.org/abs/2507.18366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18366">https://arxiv.org/pdf/2507.18366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18366]] Efficient Uncertainty in LLMs through Evidential Knowledge Distillation(https://arxiv.org/abs/2507.18366)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate uncertainty quantification remains a key challenge for standard LLMs, prompting the adoption of Bayesian and ensemble-based methods. However, such methods typically necessitate computationally expensive sampling, involving multiple forward passes to effectively estimate predictive uncertainty. In this paper, we introduce a novel approach enabling efficient and effective uncertainty estimation in LLMs without sacrificing performance. Specifically, we distill uncertainty-aware teacher models - originally requiring multiple forward passes - into compact student models sharing the same architecture but fine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct distillation strategies: one in which the student employs traditional softmax-based outputs, and another in which the student leverages Dirichlet-distributed outputs to explicitly model epistemic uncertainty via evidential learning. Empirical evaluations on classification datasets demonstrate that such students can achieve comparable or superior predictive and uncertainty quantification performance relative to their teacher models, while critically requiring only a single forward pass. To our knowledge, this is the first demonstration that immediate and robust uncertainty quantification can be achieved in LLMs through evidential distillation.</li>
</ul>

<h3>Title: MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D Content Creation from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Xiaotian Chen, DongFu Yin, Fei Richard Yu, Xuanchen Li, Xinhao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18371">https://arxiv.org/abs/2507.18371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18371">https://arxiv.org/pdf/2507.18371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18371]] MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D Content Creation from a Single Image(https://arxiv.org/abs/2507.18371)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Advances in generative modeling have significantly enhanced digital content creation, extending from 2D images to complex 3D and 4D scenes. Despite substantial progress, producing high-fidelity and temporally consistent dynamic 4D content remains a challenge. In this paper, we propose MVG4D, a novel framework that generates dynamic 4D content from a single still image by combining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core, MVG4D employs an image matrix module that synthesizes temporally coherent and spatially diverse multi-view images, providing rich supervisory signals for downstream 3D and 4D reconstruction. These multi-view images are used to optimize a 3D Gaussian point cloud, which is further extended into the temporal domain via a lightweight deformation network. Our method effectively enhances temporal consistency, geometric fidelity, and visual realism, addressing key challenges in motion discontinuity and background degradation that affect prior 4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate that MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and time efficiency. Notably, it reduces flickering artifacts and sharpens structural details across views and time, enabling more immersive AR/VR experiences. MVG4D sets a new direction for efficient and controllable 4D generation from minimal inputs.</li>
</ul>

<h3>Title: A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Xing Hua, Haodong Chen, Qianqian Duan, Danfeng Hong, Ruijiao Li, Huiliang Shang, Linghua Jiang, Haima Yang, Dawei Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18376">https://arxiv.org/abs/2507.18376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18376">https://arxiv.org/pdf/2507.18376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18376]] A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges(https://arxiv.org/abs/2507.18376)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>With the global population growing and arable land resources becoming increasingly scarce,smart agriculture and precision agriculture have emerged as key directions for the future ofagricultural this http URL intelligence (AI) technologies, particularly deep learning models, have found widespread applications in areas such as crop monitoring and pest detection. As an emerging generative model, diffusion models have shown significant promise in tasks like agricultural image processing, data augmentation, and remote sensing. Compared to traditional generative adversarial networks (GANs), diffusion models offer superior training stability and generation quality, effectively addressing challenges such as limited agricultural data and imbalanced image samples. This paper reviews the latest advancements in the application of diffusion models in agriculture, focusing on their potential in crop pest and disease detection, remote sensing image enhancement, crop growth prediction, and agricultural resource management. Experimental results demonstrate that diffusion models significantly improve model accuracy and robustness in data augmentation, image generation, and denoising, especially in complex environments. Despite challenges related to computational efficiency and generalization capabilities, diffusion models are expected to play an increasingly important role in smart and precision agriculture as technology advances, providing substantial support for the sustainable development of global agriculture.</li>
</ul>

<h3>Title: CLEAR: Error Analysis via LLM-as-a-Judge Made Easy</h3>
<ul>
<li><strong>Authors: </strong>Asaf Yehudai, Lilach Eden, Yotam Perlitz, Roy Bar-Haim, Michal Shmueli-Scheuer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18392">https://arxiv.org/abs/2507.18392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18392">https://arxiv.org/pdf/2507.18392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18392]] CLEAR: Error Analysis via LLM-as-a-Judge Made Easy(https://arxiv.org/abs/2507.18392)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The evaluation of Large Language Models (LLMs) increasingly relies on other LLMs acting as judges. However, current evaluation paradigms typically yield a single score or ranking, answering which model is better but not why. While essential for benchmarking, these top-level scores obscure the specific, actionable reasons behind a model's performance. To bridge this gap, we introduce CLEAR, an interactive, open-source package for LLM-based error analysis. CLEAR first generates per-instance textual feedback, then it creates a set of system-level error issues, and quantifies the prevalence of each identified issue. Our package also provides users with an interactive dashboard that allows for a comprehensive error analysis through aggregate visualizations, applies interactive filters to isolate specific issues or score ranges, and drills down to the individual instances that exemplify a particular behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks, and showcase its utility through a user case study.</li>
</ul>

<h3>Title: Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows</h3>
<ul>
<li><strong>Authors: </strong>Simin Huo, Ning Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18405">https://arxiv.org/abs/2507.18405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18405">https://arxiv.org/pdf/2507.18405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18405]] Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows(https://arxiv.org/abs/2507.18405)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>We introduce Iwin Transformer, a novel position-embedding-free hierarchical vision transformer, which can be fine-tuned directly from low to high resolution, through the collaboration of innovative interleaved window attention and depthwise separable convolution. This approach uses attention to connect distant tokens and applies convolution to link neighboring tokens, enabling global information exchange within a single module, overcoming Swin Transformer's limitation of requiring two consecutive blocks to approximate global attention. Extensive experiments on visual benchmarks demonstrate that Iwin Transformer exhibits strong competitiveness in tasks such as image classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and video action recognition. We also validate the effectiveness of the core component in Iwin as a standalone module that can seamlessly replace the self-attention module in class-conditional image generation. The concepts and methods introduced by the Iwin Transformer have the potential to inspire future research, like Iwin 3D Attention in video generation. The code and models are available at this https URL.</li>
</ul>

<h3>Title: DCFFSNet: Deep Connectivity Feature Fusion Separation Network for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xun Ye, Ruixiang Tang, Mingda Zhang, Jianglong Qin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18407">https://arxiv.org/abs/2507.18407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18407">https://arxiv.org/pdf/2507.18407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18407]] DCFFSNet: Deep Connectivity Feature Fusion Separation Network for Medical Image Segmentation(https://arxiv.org/abs/2507.18407)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation leverages topological connectivity theory to enhance edge precision and regional consistency. However, existing deep networks integrating connectivity often forcibly inject it as an additional feature module, resulting in coupled feature spaces with no standardized mechanism to quantify different feature strengths. To address these issues, we propose DCFFSNet (Dual-Connectivity Feature Fusion-Separation Network). It introduces an innovative feature space decoupling strategy. This strategy quantifies the relative strength between connectivity features and other features. It then builds a deep connectivity feature fusion-separation architecture. This architecture dynamically balances multi-scale feature expression. Experiments were conducted on the ISIC2018, DSB2018, and MoNuSeg datasets. On ISIC2018, DCFFSNet outperformed the next best model (CMUNet) by 1.3% (Dice) and 1.2% (IoU). On DSB2018, it surpassed TransUNet by 0.7% (Dice) and 0.9% (IoU). On MoNuSeg, it exceeded CSCAUNet by 0.8% (Dice) and 0.9% (IoU). The results demonstrate that DCFFSNet exceeds existing mainstream methods across all metrics. It effectively resolves segmentation fragmentation and achieves smooth edge transitions. This significantly enhances clinical usability.</li>
</ul>

<h3>Title: FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Giorgos Iacovides, Wuyang Zhou, Danilo Mandic</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, q-fin.ST, q-fin.TR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18417">https://arxiv.org/abs/2507.18417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18417">https://arxiv.org/pdf/2507.18417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18417]] FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs(https://arxiv.org/abs/2507.18417)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Opinions expressed in online finance-related textual data are having an increasingly profound impact on trading decisions and market movements. This trend highlights the vital role of sentiment analysis as a tool for quantifying the nature and strength of such opinions. With the rapid development of Generative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs) have become the de facto standard for financial sentiment analysis. However, the SFT paradigm can lead to memorization of the training data and often fails to generalize to unseen samples. This is a critical limitation in financial domains, where models must adapt to previously unobserved events and the nuanced, domain-specific language of finance. To this end, we introduce FinDPO, the first finance-specific LLM framework based on post-training human preference alignment via Direct Preference Optimization (DPO). The proposed FinDPO achieves state-of-the-art performance on standard sentiment classification benchmarks, outperforming existing supervised fine-tuned models by 11% on the average. Uniquely, the FinDPO framework enables the integration of a fine-tuned causal LLM into realistic portfolio strategies through a novel 'logit-to-score' conversion, which transforms discrete sentiment predictions into continuous, rankable sentiment scores (probabilities). In this way, simulations demonstrate that FinDPO is the first sentiment-based approach to maintain substantial positive returns of 67% annually and strong risk-adjusted performance, as indicated by a Sharpe ratio of 2.0, even under realistic transaction costs of 5 basis points (bps).</li>
</ul>

<h3>Title: Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins</h3>
<ul>
<li><strong>Authors: </strong>Mizuki Funato, Yohei Sawada</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18423">https://arxiv.org/abs/2507.18423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18423">https://arxiv.org/pdf/2507.18423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18423]] Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins(https://arxiv.org/abs/2507.18423)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Despite the critical need for accurate flood prediction and water management, many regions lack sufficient river discharge observations, limiting the skill of rainfall-runoff analyses. Although numerous physically based and machine learning models exist, achieving high accuracy, interpretability, and computational efficiency under data-scarce conditions remains a major challenge. We address this challenge with a novel method, HYdrological Prediction with multi-model Ensemble and Reservoir computing (HYPER) that leverages multi-model ensemble and reservoir computing (RC). Our approach first applies Bayesian model averaging (BMA) to 43 "uncalibrated" catchment-based conceptual hydrological models. An RC model is then trained via linear regression to correct errors in the BMA output, a non-iterative process that ensures high computational efficiency. For ungauged basins, we infer the required BMA and RC weights by linking them to catchment attributes from gauged basins, creating a generalizable framework. We evaluated HYPER using data from 87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta Efficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55) but required only 5% of its computational time. In a data-scarce scenario (23% of basins gauged), HYPER maintained robust performance (KGE 0.55) and lower uncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04). These results reveal that individual conceptual hydrological models do not necessarily need to be calibrated when an effectively large ensemble is assembled and combined with machine-learning-based bias correction. HYPER provides a robust, efficient, and generalizable solution for discharge prediction, particularly in ungauged basins, making it applicable to a wide range of regions.</li>
</ul>

<h3>Title: Self-Supervised Ultrasound-Video Segmentation with Feature Prediction and 3D Localised Loss</h3>
<ul>
<li><strong>Authors: </strong>Edward Ellis, Robert Mendel, Andrew Bulpitt, Nasim Parsa, Michael F Byrne, Sharib Ali</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18424">https://arxiv.org/abs/2507.18424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18424">https://arxiv.org/pdf/2507.18424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18424]] Self-Supervised Ultrasound-Video Segmentation with Feature Prediction and 3D Localised Loss(https://arxiv.org/abs/2507.18424)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Acquiring and annotating large datasets in ultrasound imaging is challenging due to low contrast, high noise, and susceptibility to artefacts. This process requires significant time and clinical expertise. Self-supervised learning (SSL) offers a promising solution by leveraging unlabelled data to learn useful representations, enabling improved segmentation performance when annotated data is limited. Recent state-of-the-art developments in SSL for video data include V-JEPA, a framework solely based on feature prediction, avoiding pixel level reconstruction or negative samples. We hypothesise that V-JEPA is well-suited to ultrasound imaging, as it is less sensitive to noisy pixel-level detail while effectively leveraging temporal information. To the best of our knowledge, this is the first study to adopt V-JEPA for ultrasound video data. Similar to other patch-based masking SSL techniques such as VideoMAE, V-JEPA is well-suited to ViT-based models. However, ViTs can underperform on small medical datasets due to lack of inductive biases, limited spatial locality and absence of hierarchical feature learning. To improve locality understanding, we propose a novel 3D localisation auxiliary task to improve locality in ViT representations during V-JEPA pre-training. Our results show V-JEPA with our auxiliary task improves segmentation performance significantly across various frozen encoder configurations, with gains up to 3.4\% using 100\% and up to 8.35\% using only 10\% of the training data.</li>
</ul>

<h3>Title: AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Rana Alshaikh, Israa Alghanmi, Shelan Jeawak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18442">https://arxiv.org/abs/2507.18442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18442">https://arxiv.org/pdf/2507.18442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18442]] AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data(https://arxiv.org/abs/2507.18442)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The cognitive and reasoning abilities of large language models (LLMs) have enabled remarkable progress in natural language processing. However, their performance in interpreting structured data, especially in tabular formats, remains limited. Although benchmarks for English tabular data are widely available, Arabic is still underrepresented because of the limited availability of public resources and its unique language features. To address this gap, we present AraTable, a novel and comprehensive benchmark designed to evaluate the reasoning and understanding capabilities of LLMs when applied to Arabic tabular data. AraTable consists of various evaluation tasks, such as direct question answering, fact verification, and complex reasoning, involving a wide range of Arabic tabular sources. Our methodology follows a hybrid pipeline, where initial content is generated by LLMs and subsequently filtered and verified by human experts to ensure high dataset quality. Initial analyses using AraTable show that, while LLMs perform adequately on simpler tabular tasks such as direct question answering, they continue to face significant cognitive challenges when tasks require deeper reasoning and fact verification. This indicates that there are substantial opportunities for future work to improve performance on complex tabular reasoning tasks. We also propose a fully automated evaluation framework that uses a self-deliberation mechanism and achieves performance nearly identical to that of human judges. This research provides a valuable, publicly available resource and evaluation framework that can help accelerate the development of foundational models for processing and analysing Arabic structured data.</li>
</ul>

<h3>Title: DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place Recognition</h3>
<ul>
<li><strong>Authors: </strong>Haiyang Jiang, Songhao Piao, Chao Gao, Lei Yu, Liguo Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18444">https://arxiv.org/abs/2507.18444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18444">https://arxiv.org/pdf/2507.18444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18444]] DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place Recognition(https://arxiv.org/abs/2507.18444)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Visual Place Recognition (VPR) is crucial for robust mobile robot localization, yet it faces significant challenges in maintaining reliable performance under varying environmental conditions and viewpoints. To address this, we propose a novel framework that integrates Dual-Scale-Former (DSFormer), a Transformer-based cross-learning module, with an innovative block clustering strategy. DSFormer enhances feature representation by enabling bidirectional information transfer between dual-scale features extracted from the final two CNN layers, capturing both semantic richness and spatial details through self-attention for long-range dependencies within each scale and shared cross-attention for cross-scale learning. Complementing this, our block clustering strategy repartitions the widely used San Francisco eXtra Large (SF-XL) training dataset from multiple distinct perspectives, optimizing data organization to further bolster robustness against viewpoint variations. Together, these innovations not only yield a robust global embedding adaptable to environmental changes but also reduce the required training data volume by approximately 30\% compared to previous partitioning methods. Comprehensive experiments demonstrate that our approach achieves state-of-the-art performance across most benchmark datasets, surpassing advanced reranking methods like DELG, Patch-NetVLAD, TransVPR, and R2Former as a global retrieval solution using 512-dim global descriptors, while significantly improving computational efficiency.</li>
</ul>

<h3>Title: PDB-Eval: An Evaluation of Large Multimodal Models for Description and Explanation of Personalized Driving Behavior</h3>
<ul>
<li><strong>Authors: </strong>Junda Wu, Jessica Echterhoff, Kyungtae Han, Amr Abdelraouf, Rohit Gupta, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18447">https://arxiv.org/abs/2507.18447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18447">https://arxiv.org/pdf/2507.18447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18447]] PDB-Eval: An Evaluation of Large Multimodal Models for Description and Explanation of Personalized Driving Behavior(https://arxiv.org/abs/2507.18447)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Understanding a driver's behavior and intentions is important for potential risk assessment and early accident prevention. Safety and driver assistance systems can be tailored to individual drivers' behavior, significantly enhancing their effectiveness. However, existing datasets are limited in describing and explaining general vehicle movements based on external visual evidence. This paper introduces a benchmark, PDB-Eval, for a detailed understanding of Personalized Driver Behavior, and aligning Large Multimodal Models (MLLMs) with driving comprehension and reasoning. Our benchmark consists of two main components, PDB-X and PDB-QA. PDB-X can evaluate MLLMs' understanding of temporal driving scenes. Our dataset is designed to find valid visual evidence from the external view to explain the driver's behavior from the internal view. To align MLLMs' reasoning abilities with driving tasks, we propose PDB-QA as a visual explanation question-answering task for MLLM instruction fine-tuning. As a generic learning task for generative models like MLLMs, PDB-QA can bridge the domain gap without harming MLLMs' generalizability. Our evaluation indicates that fine-tuning MLLMs on fine-grained descriptions and explanations can effectively bridge the gap between MLLMs and the driving domain, which improves zero-shot performance on question-answering tasks by up to 73.2%. We further evaluate the MLLMs fine-tuned on PDB-X in Brain4Cars' intention prediction and AIDE's recognition tasks. We observe up to 12.5% performance improvements on the turn intention prediction task in Brain4Cars, and consistent performance improvements up to 11.0% on all tasks in AIDE.</li>
</ul>

<h3>Title: Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language</h3>
<ul>
<li><strong>Authors: </strong>Md Obyedullahil Mamun, Md Adyelullahil Mamun, Arif Ahmad, Md. Imran Hossain Emu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18448">https://arxiv.org/abs/2507.18448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18448">https://arxiv.org/pdf/2507.18448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18448]] Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language(https://arxiv.org/abs/2507.18448)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Punctuation restoration enhances the readability of text and is critical for post-processing tasks in Automatic Speech Recognition (ASR), especially for low-resource languages like Bangla. In this study, we explore the application of transformer-based models, specifically XLM-RoBERTa-large, to automatically restore punctuation in unpunctuated Bangla text. We focus on predicting four punctuation marks: period, comma, question mark, and exclamation mark across diverse text domains. To address the scarcity of annotated resources, we constructed a large, varied training corpus and applied data augmentation techniques. Our best-performing model, trained with an augmentation factor of alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the Reference set, and 90.2% on the ASR set. Results show strong generalization to reference and ASR transcripts, demonstrating the model's effectiveness in real-world, noisy scenarios. This work establishes a strong baseline for Bangla punctuation restoration and contributes publicly available datasets and code to support future research in low-resource NLP.</li>
</ul>

<h3>Title: Generation of Synthetic Clinical Text: A Systematic Review</h3>
<ul>
<li><strong>Authors: </strong>Basel Alshaikhdeeb, Ahmed Abdelmonem Hemedan, Soumyabrata Ghosh, Irina Balaur, Venkata Satagopam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18451">https://arxiv.org/abs/2507.18451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18451">https://arxiv.org/pdf/2507.18451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18451]] Generation of Synthetic Clinical Text: A Systematic Review(https://arxiv.org/abs/2507.18451)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, transformer</a></li>
<li><strong>Abstract: </strong>Generating clinical synthetic text represents an effective solution for common clinical NLP issues like sparsity and privacy. This paper aims to conduct a systematic review on generating synthetic medical free-text by formulating quantitative analysis to three research questions concerning (i) the purpose of generation, (ii) the techniques, and (iii) the evaluation methods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE, Google Scholar, and arXiv databases for publications associated with generating synthetic medical unstructured free-text. We have identified 94 relevant articles out of 1,398 collected ones. A great deal of attention has been given to the generation of synthetic medical text from 2018 onwards, where the main purpose of such a generation is towards text augmentation, assistive writing, corpus building, privacy-preserving, annotation, and usefulness. Transformer architectures were the main predominant technique used to generate the text, especially the GPTs. On the other hand, there were four main aspects of evaluation, including similarity, privacy, structure, and utility, where utility was the most frequent method used to assess the generated synthetic medical text. Although the generated synthetic medical text demonstrated a moderate possibility to act as real medical documents in different downstream NLP tasks, it has proven to be a great asset as augmented, complementary to the real documents, towards improving the accuracy and overcoming sparsity/undersampling issues. Yet, privacy is still a major issue behind generating synthetic medical text, where more human assessments are needed to check for the existence of any sensitive information. Despite that, advances in generating synthetic medical text will considerably accelerate the adoption of workflows and pipeline development, discarding the time-consuming legalities of data transfer.</li>
</ul>

<h3>Title: Revisiting Physically Realizable Adversarial Object Attack against LiDAR-based Detection: Clarifying Problem Formulation and Experimental Protocols</h3>
<ul>
<li><strong>Authors: </strong>Luo Cheng, Hanwei Zhang, Lijun Zhang, Holger Hermanns</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18457">https://arxiv.org/abs/2507.18457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18457">https://arxiv.org/pdf/2507.18457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18457]] Revisiting Physically Realizable Adversarial Object Attack against LiDAR-based Detection: Clarifying Problem Formulation and Experimental Protocols(https://arxiv.org/abs/2507.18457)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, fair</a></li>
<li><strong>Abstract: </strong>Adversarial robustness in LiDAR-based 3D object detection is a critical research area due to its widespread application in real-world scenarios. While many digital attacks manipulate point clouds or meshes, they often lack physical realizability, limiting their practical impact. Physical adversarial object attacks remain underexplored and suffer from poor reproducibility due to inconsistent setups and hardware differences. To address this, we propose a device-agnostic, standardized framework that abstracts key elements of physical adversarial object attacks, supports diverse methods, and provides open-source code with benchmarking protocols in simulation and real-world settings. Our framework enables fair comparison, accelerates research, and is validated by successfully transferring simulated attacks to a physical LiDAR system. Beyond the framework, we offer insights into factors influencing attack success and advance understanding of adversarial robustness in real-world LiDAR perception.</li>
</ul>

<h3>Title: Scout: Leveraging Large Language Models for Rapid Digital Evidence Discovery</h3>
<ul>
<li><strong>Authors: </strong>Shariq Murtuza</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18478">https://arxiv.org/abs/2507.18478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18478">https://arxiv.org/pdf/2507.18478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18478]] Scout: Leveraging Large Language Models for Rapid Digital Evidence Discovery(https://arxiv.org/abs/2507.18478)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent technological advancements and the prevalence of technology in day to day activities have caused a major increase in the likelihood of the involvement of digital evidence in more and more legal investigations. Consumer-grade hardware is growing more powerful, with expanding memory and storage sizes and enhanced processor capabilities. Forensics investigators often have to sift through gigabytes of data during an ongoing investigation making the process tedious. Memory forensics, disk analysis all are well supported by state of the art tools that significantly lower the effort required to be put in by a forensic investigator by providing string searches, analyzing images file etc. During the course of the investigation a lot of false positives are identified that need to be lowered. This work presents Scout, a digital forensics framework that performs preliminary evidence processing and prioritizing using large language models. Scout deploys foundational language models to identify relevant artifacts from a large number of potential evidence files (disk images, captured network packets, memory dumps etc.) which would have taken longer to get identified. Scout employs text based large language models can easily process files with textual information. For the forensic analysis of multimedia files like audio, image, video, office documents etc. multimodal models are employed by Scout. Scout was able to identify and realize the evidence file that were of potential interest for the investigator.</li>
</ul>

<h3>Title: A COCO-Formatted Instance-Level Dataset for Plasmodium Falciparum Detection in Giemsa-Stained Blood Smears</h3>
<ul>
<li><strong>Authors: </strong>Frauke Wilm, Luis Carlos Rivera Monroy, Mathias Öttl, Lukas Mürdter, Leonid Mill, Andreas Maier</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18483">https://arxiv.org/abs/2507.18483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18483">https://arxiv.org/pdf/2507.18483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18483]] A COCO-Formatted Instance-Level Dataset for Plasmodium Falciparum Detection in Giemsa-Stained Blood Smears(https://arxiv.org/abs/2507.18483)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate detection of Plasmodium falciparum in Giemsa-stained blood smears is an essential component of reliable malaria diagnosis, especially in developing countries. Deep learning-based object detection methods have demonstrated strong potential for automated Malaria diagnosis, but their adoption is limited by the scarcity of datasets with detailed instance-level annotations. In this work, we present an enhanced version of the publicly available NIH malaria dataset, with detailed bounding box annotations in COCO format to support object detection training. We validated the revised annotations by training a Faster R-CNN model to detect infected and non-infected red blood cells, as well as white blood cells. Cross-validation on the original dataset yielded F1 scores of up to 0.88 for infected cell detection. These results underscore the importance of annotation volume and consistency, and demonstrate that automated annotation refinement combined with targeted manual correction can produce training data of sufficient quality for robust detection performance. The updated annotations set is publicly available via GitHub: this https URL.</li>
</ul>

<h3>Title: Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments</h3>
<ul>
<li><strong>Authors: </strong>Xiao Yang, Lingxuan Wu, Lizhong Wang, Chengyang Ying, Hang Su, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18484">https://arxiv.org/abs/2507.18484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18484">https://arxiv.org/pdf/2507.18484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18484]] Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments(https://arxiv.org/abs/2507.18484)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Adversarial attacks in 3D environments have emerged as a critical threat to the reliability of visual perception systems, particularly in safety-sensitive applications such as identity verification and autonomous driving. These attacks employ adversarial patches and 3D objects to manipulate deep neural network (DNN) predictions by exploiting vulnerabilities within complex scenes. Existing defense mechanisms, such as adversarial training and purification, primarily employ passive strategies to enhance robustness. However, these approaches often rely on pre-defined assumptions about adversarial tactics, limiting their adaptability in dynamic 3D settings. To address these challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a proactive defense framework that leverages adaptive exploration and interaction with the environment to improve perception robustness in 3D adversarial contexts. By implementing a multi-step objective that balances immediate prediction accuracy with predictive entropy minimization, Rein-EAD optimizes defense strategies over a multi-step horizon. Additionally, Rein-EAD involves an uncertainty-oriented reward-shaping mechanism that facilitates efficient policy updates, thereby reducing computational overhead and supporting real-world applicability without the need for differentiable environments. Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating a substantial reduction in attack success rates while preserving standard accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization to unseen and adaptive attacks, making it suitable for real-world complex tasks, including 3D object classification, face recognition and autonomous driving.</li>
</ul>

<h3>Title: Delving into Mapping Uncertainty for Mapless Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Zongzheng Zhang, Xuchong Qiu, Boran Zhang, Guantian Zheng, Xunjiang Gu, Guoxuan Chi, Huan-ang Gao, Leichen Wang, Ziming Liu, Xinrun Li, Igor Gilitschenski, Hongyang Li, Hang Zhao, Hao Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18498">https://arxiv.org/abs/2507.18498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18498">https://arxiv.org/pdf/2507.18498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18498]] Delving into Mapping Uncertainty for Mapless Trajectory Prediction(https://arxiv.org/abs/2507.18498)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Recent advances in autonomous driving are moving towards mapless approaches, where High-Definition (HD) maps are generated online directly from sensor data, reducing the need for expensive labeling and maintenance. However, the reliability of these online-generated maps remains uncertain. While incorporating map uncertainty into downstream trajectory prediction tasks has shown potential for performance improvements, current strategies provide limited insights into the specific scenarios where this uncertainty is beneficial. In this work, we first analyze the driving scenarios in which mapping uncertainty has the greatest positive impact on trajectory prediction and identify a critical, previously overlooked factor: the agent's kinematic state. Building on these insights, we propose a novel Proprioceptive Scenario Gating that adaptively integrates map uncertainty into trajectory prediction based on forecasts of the ego vehicle's future kinematics. This lightweight, self-supervised approach enhances the synergy between online mapping and trajectory prediction, providing interpretability around where uncertainty is advantageous and outperforming previous integration methods. Additionally, we introduce a Covariance-based Map Uncertainty approach that better aligns with map geometry, further improving trajectory prediction. Extensive ablation studies confirm the effectiveness of our approach, achieving up to 23.6% improvement in mapless trajectory prediction performance over the state-of-the-art method using the real-world nuScenes driving dataset. Our code, data, and models are publicly available at this https URL.</li>
</ul>

<h3>Title: Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18504">https://arxiv.org/abs/2507.18504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18504">https://arxiv.org/pdf/2507.18504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18504]] Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models(https://arxiv.org/abs/2507.18504)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown strong potential for tabular data generation by modeling textualized feature-value pairs. However, tabular data inherently exhibits sparse feature-level dependencies, where many feature interactions are structurally insignificant. This creates a fundamental mismatch as LLMs' self-attention mechanism inevitably distributes focus across all pairs, diluting attention on critical relationships, particularly in datasets with complex dependencies or semantically ambiguous features. To address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a novel method that explicitly integrates sparse dependency graphs into LLMs' attention mechanism. GraDe employs a lightweight dynamic graph learning module guided by externally extracted functional dependencies, prioritizing key feature interactions while suppressing irrelevant ones. Our experiments across diverse real-world datasets demonstrate that GraDe outperforms existing LLM-based approaches by up to 12% on complex datasets while achieving competitive results with state-of-the-art approaches in synthetic data quality. Our method is minimally intrusive yet effective, offering a practical solution for structure-aware tabular data modeling with LLMs.</li>
</ul>

<h3>Title: Object segmentation in the wild with foundation models: application to vision assisted neuro-prostheses for upper limbs</h3>
<ul>
<li><strong>Authors: </strong>Bolutife Atoki, Jenny Benois-Pineau, Renaud Péteri, Fabien Baldacci, Aymar de Rugy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18517">https://arxiv.org/abs/2507.18517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18517">https://arxiv.org/pdf/2507.18517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18517]] Object segmentation in the wild with foundation models: application to vision assisted neuro-prostheses for upper limbs(https://arxiv.org/abs/2507.18517)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this work, we address the problem of semantic object segmentation using foundation models. We investigate whether foundation models, trained on a large number and variety of objects, can perform object segmentation without fine-tuning on specific images containing everyday objects, but in highly cluttered visual scenes. The ''in the wild'' context is driven by the target application of vision guided upper limb neuroprostheses. We propose a method for generating prompts based on gaze fixations to guide the Segment Anything Model (SAM) in our segmentation scenario, and fine-tune it on egocentric visual data. Evaluation results of our approach show an improvement of the IoU segmentation quality metric by up to 0.51 points on real-world challenging data of Grasping-in-the-Wild corpus which is made available on the RoboFlow Platform (this https URL)</li>
</ul>

<h3>Title: Revisiting Bisimulation Metric for Robust Representations in Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Leiji Zhang, Zeyu Wang, Xin Li, Yao-Hui Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18519">https://arxiv.org/abs/2507.18519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18519">https://arxiv.org/pdf/2507.18519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18519]] Revisiting Bisimulation Metric for Robust Representations in Reinforcement Learning(https://arxiv.org/abs/2507.18519)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Bisimulation metric has long been regarded as an effective control-related representation learning technique in various reinforcement learning tasks. However, in this paper, we identify two main issues with the conventional bisimulation metric: 1) an inability to represent certain distinctive scenarios, and 2) a reliance on predefined weights for differences in rewards and subsequent states during recursive updates. We find that the first issue arises from an imprecise definition of the reward gap, whereas the second issue stems from overlooking the varying importance of reward difference and next-state distinctions across different training stages and task settings. To address these issues, by introducing a measure for state-action pairs, we propose a revised bisimulation metric that features a more precise definition of reward gap and novel update operators with adaptive coefficient. We also offer theoretical guarantees of convergence for our proposed metric and its improved representation distinctiveness. In addition to our rigorous theoretical analysis, we conduct extensive experiments on two representative benchmarks, DeepMind Control and Meta-World, demonstrating the effectiveness of our approach.</li>
</ul>

<h3>Title: GLANCE: Graph Logic Attention Network with Cluster Enhancement for Heterophilous Graph Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhongtian Sun, Anoushka Harit, Alexandra Cristea, Christl A. Donnelly, Pietro Liò</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18521">https://arxiv.org/abs/2507.18521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18521">https://arxiv.org/pdf/2507.18521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18521]] GLANCE: Graph Logic Attention Network with Cluster Enhancement for Heterophilous Graph Representation Learning(https://arxiv.org/abs/2507.18521)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have demonstrated significant success in learning from graph-structured data but often struggle on heterophilous graphs, where connected nodes differ in features or class labels. This limitation arises from indiscriminate neighbor aggregation and insufficient incorporation of higher-order structural patterns. To address these challenges, we propose GLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel framework that integrates logic-guided reasoning, dynamic graph refinement, and adaptive clustering to enhance graph representation learning. GLANCE combines a logic layer for interpretable and structured embeddings, multi-head attention-based edge pruning for denoising graph structures, and clustering mechanisms for capturing global patterns. Experimental results in benchmark datasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE achieves competitive performance, offering robust and interpretable solutions for heterophilous graph scenarios. The proposed framework is lightweight, adaptable, and uniquely suited to the challenges of heterophilous graphs.</li>
</ul>

<h3>Title: GaussianFusionOcc: A Seamless Sensor Fusion Approach for 3D Occupancy Prediction Using 3D Gaussians</h3>
<ul>
<li><strong>Authors: </strong>Tomislav Pavković, Mohammad-Ali Nikouei Mahani, Johannes Niedermayer, Johannes Betz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18522">https://arxiv.org/abs/2507.18522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18522">https://arxiv.org/pdf/2507.18522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18522]] GaussianFusionOcc: A Seamless Sensor Fusion Approach for 3D Occupancy Prediction Using 3D Gaussians(https://arxiv.org/abs/2507.18522)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>3D semantic occupancy prediction is one of the crucial tasks of autonomous driving. It enables precise and safe interpretation and navigation in complex environments. Reliable predictions rely on effective sensor fusion, as different modalities can contain complementary information. Unlike conventional methods that depend on dense grid representations, our approach, GaussianFusionOcc, uses semantic 3D Gaussians alongside an innovative sensor fusion mechanism. Seamless integration of data from camera, LiDAR, and radar sensors enables more precise and scalable occupancy prediction, while 3D Gaussian representation significantly improves memory efficiency and inference speed. GaussianFusionOcc employs modality-agnostic deformable attention to extract essential features from each sensor type, which are then used to refine Gaussian properties, resulting in a more accurate representation of the environment. Extensive testing with various sensor combinations demonstrates the versatility of our approach. By leveraging the robustness of multi-modal fusion and the efficiency of Gaussian representation, GaussianFusionOcc outperforms current state-of-the-art models.</li>
</ul>

<h3>Title: The Moral Gap of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Maciej Skorski, Alina Landowska</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18523">https://arxiv.org/abs/2507.18523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18523">https://arxiv.org/pdf/2507.18523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18523]] The Moral Gap of Large Language Models(https://arxiv.org/abs/2507.18523)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Moral foundation detection is crucial for analyzing social discourse and developing ethically-aligned AI systems. While large language models excel across diverse tasks, their performance on specialized moral reasoning remains unclear. This study provides the first comprehensive comparison between state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit datasets using ROC, PR, and DET curve analysis. Results reveal substantial performance gaps, with LLMs exhibiting high false negative rates and systematic under-detection of moral content despite prompt engineering efforts. These findings demonstrate that task-specific fine-tuning remains superior to prompting for moral reasoning applications.</li>
</ul>

<h3>Title: COT-AD: Cotton Analysis Dataset</h3>
<ul>
<li><strong>Authors: </strong>Akbar Ali, Mahek Vyas, Soumyaratna Debnath, Chanda Grover Kamra, Jaidev Sanjay Khalane, Reuben Shibu Devanesan, Indra Deep Mastan, Subramanian Sankaranarayanan, Pankaj Khanna, Shanmuganathan Raman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18532">https://arxiv.org/abs/2507.18532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18532">https://arxiv.org/pdf/2507.18532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18532]] COT-AD: Cotton Analysis Dataset(https://arxiv.org/abs/2507.18532)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents COT-AD, a comprehensive Dataset designed to enhance cotton crop analysis through computer vision. Comprising over 25,000 images captured throughout the cotton growth cycle, with 5,000 annotated images, COT-AD includes aerial imagery for field-scale detection and segmentation and high-resolution DSLR images documenting key diseases. The annotations cover pest and disease recognition, vegetation, and weed analysis, addressing a critical gap in cotton-specific agricultural datasets. COT-AD supports tasks such as classification, segmentation, image restoration, enhancement, deep generative model-based cotton crop synthesis, and early disease management, advancing data-driven crop management</li>
</ul>

<h3>Title: C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Magnus Bengtsson, Kenneth Östberg</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18533">https://arxiv.org/abs/2507.18533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18533">https://arxiv.org/pdf/2507.18533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18533]] C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation(https://arxiv.org/abs/2507.18533)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free</a></li>
<li><strong>Abstract: </strong>We introduce C2G-KD, a data-free knowledge distillation framework where a class-conditional generator is trained to produce synthetic samples guided by a frozen teacher model and geometric constraints derived from PCA. The generator never observes real training data but instead learns to activate the teacher's output through a combination of semantic and structural losses. By constraining generated samples to lie within class-specific PCA subspaces estimated from as few as two real examples per class, we preserve topological consistency and diversity. Experiments on MNIST show that even minimal class structure is sufficient to bootstrap useful synthetic training pipelines.</li>
</ul>

<h3>Title: Elucidating the Design Space of Arbitrary-Noise-Based Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Qiu, Mengying Yang, Xinghua Ma, Dong Liang, Yuzhen Li, Fanding Li, Gongning Luo, Wei Wang, Kuanquan Wang, Shuo Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18534">https://arxiv.org/abs/2507.18534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18534">https://arxiv.org/pdf/2507.18534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18534]] Elucidating the Design Space of Arbitrary-Noise-Based Diffusion Models(https://arxiv.org/abs/2507.18534)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>EDM elucidates the unified design space of diffusion models, yet its fixed noise patterns restricted to pure Gaussian noise, limit advancements in image restoration. Our study indicates that forcibly injecting Gaussian noise corrupts the degraded images, overextends the image transformation distance, and increases restoration complexity. To address this problem, our proposed EDA Elucidates the Design space of Arbitrary-noise-based diffusion models. Theoretically, EDA expands the freedom of noise pattern while preserving the original module flexibility of EDM, with rigorous proof that increased noise complexity incurs no additional computational overhead during restoration. EDA is validated on three typical tasks: MRI bias field correction (global smooth noise), CT metal artifact reduction (global sharp noise), and natural image shadow removal (local boundary-aware noise). With only 5 sampling steps, EDA outperforms most task-specific methods and achieves state-of-the-art performance in bias field correction and shadow removal.</li>
</ul>

<h3>Title: GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface</h3>
<ul>
<li><strong>Authors: </strong>Urchade Zaratiana, Gil Pasternak, Oliver Boyd, George Hurn-Maloney, Ash Lewis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18546">https://arxiv.org/abs/2507.18546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18546">https://arxiv.org/pdf/2507.18546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18546]] GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface(https://arxiv.org/abs/2507.18546)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Information extraction (IE) is fundamental to numerous NLP applications, yet existing solutions often require specialized models for different tasks or rely on computationally expensive large language models. We present GLiNER2, a unified framework that enhances the original GLiNER architecture to support named entity recognition, text classification, and hierarchical structured data extraction within a single efficient model. Built pretrained transformer encoder architecture, GLiNER2 maintains CPU efficiency and compact size while introducing multi-task composition through an intuitive schema-based interface. Our experiments demonstrate competitive performance across extraction and classification tasks with substantial improvements in deployment accessibility compared to LLM-based alternatives. We release GLiNER2 as an open-source pip-installable library with pre-trained models and documentation at this https URL.</li>
</ul>

<h3>Title: A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration</h3>
<ul>
<li><strong>Authors: </strong>Daniil Morozov, Reuben Dorent, Nazim Haouchine</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18551">https://arxiv.org/abs/2507.18551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18551">https://arxiv.org/pdf/2507.18551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18551]] A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration(https://arxiv.org/abs/2507.18551)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Intraoperative registration of real-time ultrasound (iUS) to preoperative Magnetic Resonance Imaging (MRI) remains an unsolved problem due to severe modality-specific differences in appearance, resolution, and field-of-view. To address this, we propose a novel 3D cross-modal keypoint descriptor for MRI-iUS matching and registration. Our approach employs a patient-specific matching-by-synthesis approach, generating synthetic iUS volumes from preoperative MRI. This enables supervised contrastive training to learn a shared descriptor space. A probabilistic keypoint detection strategy is then employed to identify anatomically salient and modality-consistent locations. During training, a curriculum-based triplet loss with dynamic hard negative mining is used to learn descriptors that are i) robust to iUS artifacts such as speckle noise and limited coverage, and ii) rotation-invariant . At inference, the method detects keypoints in MR and real iUS images and identifies sparse matches, which are then used to perform rigid registration. Our approach is evaluated using 3D MRI-iUS pairs from the ReMIND dataset. Experiments show that our approach outperforms state-of-the-art keypoint matching methods across 11 patients, with an average precision of $69.8\%$. For image registration, our method achieves a competitive mean Target Registration Error of 2.39 mm on the ReMIND2Reg benchmark. Compared to existing iUS-MR registration approach, our framework is interpretable, requires no manual initialization, and shows robustness to iUS field-of-view variation. Code is available at this https URL.</li>
</ul>

<h3>Title: The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm</h3>
<ul>
<li><strong>Authors: </strong>Jiale Chen, Torsten Hoefler, Dan Alistarh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18553">https://arxiv.org/abs/2507.18553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18553">https://arxiv.org/pdf/2507.18553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18553]] The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm(https://arxiv.org/abs/2507.18553)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Quantizing the weights of large language models (LLMs) from 16-bit to lower bitwidth is the de facto approach to deploy massive transformers onto more affordable accelerators. GPTQ emerged as one of the standard methods for one-shot post-training quantization at LLM scale. Yet, its inner workings are described as a sequence of ad-hoc algebraic updates that obscure any geometric meaning or worst-case guarantees. In this work, we show that, when executed back-to-front (from the last to first dimension) for a linear layer, GPTQ is mathematically identical to Babai's nearest plane algorithm for the classical closest vector problem (CVP) on a lattice defined by the Hessian matrix of the layer's inputs. This equivalence is based on a sophisticated mathematical argument, and has two analytical consequences: (i) the GPTQ error propagation step gains an intuitive geometric interpretation; (ii) GPTQ inherits the error upper bound of Babai's algorithm under the no-clipping condition. Taken together, these results place GPTQ on firm theoretical footing and open the door to importing decades of progress in lattice algorithms towards the design of future quantization algorithms for billion-parameter models.</li>
</ul>

<h3>Title: Synthetic Data Augmentation for Enhanced Chicken Carcass Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yihong Feng, Chaitanya Pallerla, Xiaomin Lin, Pouya Sohrabipour Sr, Philip Crandall, Wan Shou, Yu She, Dongyi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18558">https://arxiv.org/abs/2507.18558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18558">https://arxiv.org/pdf/2507.18558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18558]] Synthetic Data Augmentation for Enhanced Chicken Carcass Instance Segmentation(https://arxiv.org/abs/2507.18558)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The poultry industry has been driven by broiler chicken production and has grown into the world's largest animal protein sector. Automated detection of chicken carcasses on processing lines is vital for quality control, food safety, and operational efficiency in slaughterhouses and poultry processing plants. However, developing robust deep learning models for tasks like instance segmentation in these fast-paced industrial environments is often hampered by the need for laborious acquisition and annotation of large-scale real-world image datasets. We present the first pipeline generating photo-realistic, automatically labeled synthetic images of chicken carcasses. We also introduce a new benchmark dataset containing 300 annotated real-world images, curated specifically for poultry segmentation research. Using these datasets, this study investigates the efficacy of synthetic data and automatic data annotation to enhance the instance segmentation of chicken carcasses, particularly when real annotated data from the processing line is scarce. A small real dataset with varying proportions of synthetic images was evaluated in prominent instance segmentation models. Results show that synthetic data significantly boosts segmentation performance for chicken carcasses across all models. This research underscores the value of synthetic data augmentation as a viable and effective strategy to mitigate data scarcity, reduce manual annotation efforts, and advance the development of robust AI-driven automated detection systems for chicken carcasses in the poultry processing industry.</li>
</ul>

<h3>Title: Beyond Internal Data: Constructing Complete Datasets for Fairness Testing</h3>
<ul>
<li><strong>Authors: </strong>Varsha Ramineni, Hossein A. Rahmani, Emine Yilmaz, David Barber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18561">https://arxiv.org/abs/2507.18561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18561">https://arxiv.org/pdf/2507.18561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18561]] Beyond Internal Data: Constructing Complete Datasets for Fairness Testing(https://arxiv.org/abs/2507.18561)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, fair</a></li>
<li><strong>Abstract: </strong>As AI becomes prevalent in high-risk domains and decision-making, it is essential to test for potential harms and biases. This urgency is reflected by the global emergence of AI regulations that emphasise fairness and adequate testing, with some mandating independent bias audits. However, procuring the necessary data for fairness testing remains a significant challenge. Particularly in industry settings, legal and privacy concerns restrict the collection of demographic data required to assess group disparities, and auditors face practical and cultural challenges in gaining access to data. Further, internal historical datasets are often insufficiently representative to identify real-world biases. This work focuses on evaluating classifier fairness when complete datasets including demographics are inaccessible. We propose leveraging separate overlapping datasets to construct complete synthetic data that includes demographic information and accurately reflects the underlying relationships between protected attributes and model features. We validate the fidelity of the synthetic data by comparing it to real data, and empirically demonstrate that fairness metrics derived from testing on such synthetic data are consistent with those obtained from real data. This work, therefore, offers a path to overcome real-world data scarcity for fairness testing, enabling independent, model-agnostic evaluation of fairness, and serving as a viable substitute where real data is limited.</li>
</ul>

<h3>Title: Deep Learning-Based Age Estimation and Gender Deep Learning-Based Age Estimation and Gender Classification for Targeted Advertisement</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Imran Zaman, Nisar Ahmed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18565">https://arxiv.org/abs/2507.18565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18565">https://arxiv.org/pdf/2507.18565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18565]] Deep Learning-Based Age Estimation and Gender Deep Learning-Based Age Estimation and Gender Classification for Targeted Advertisement(https://arxiv.org/abs/2507.18565)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a novel deep learning-based approach for simultaneous age and gender classification from facial images, designed to enhance the effectiveness of targeted advertising campaigns. We propose a custom Convolutional Neural Network (CNN) architecture, optimized for both tasks, which leverages the inherent correlation between age and gender information present in facial features. Unlike existing methods that often treat these tasks independently, our model learns shared representations, leading to improved performance. The network is trained on a large, diverse dataset of facial images, carefully pre-processed to ensure robustness against variations in lighting, pose, and image quality. Our experimental results demonstrate a significant improvement in gender classification accuracy, achieving 95%, and a competitive mean absolute error of 5.77 years for age estimation. Critically, we analyze the performance across different age groups, identifying specific challenges in accurately estimating the age of younger individuals. This analysis reveals the need for targeted data augmentation and model refinement to address these biases. Furthermore, we explore the impact of different CNN architectures and hyperparameter settings on the overall performance, providing valuable insights for future research.</li>
</ul>

<h3>Title: Facial Demorphing from a Single Morph Using a Latent Conditional GAN</h3>
<ul>
<li><strong>Authors: </strong>Nitish Shukla, Arun Ross</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18566">https://arxiv.org/abs/2507.18566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18566">https://arxiv.org/pdf/2507.18566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18566]] Facial Demorphing from a Single Morph Using a Latent Conditional GAN(https://arxiv.org/abs/2507.18566)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, biometric</a></li>
<li><strong>Abstract: </strong>A morph is created by combining two (or more) face images from two (or more) identities to create a composite image that is highly similar to both constituent identities, allowing the forged morph to be biometrically associated with more than one individual. Morph Attack Detection (MAD) can be used to detect a morph, but does not reveal the constituent images. Demorphing - the process of deducing the constituent images - is thus vital to provide additional evidence about a morph. Existing demorphing methods suffer from the morph replication problem, where the outputs tend to look very similar to the morph itself, or assume that train and test morphs are generated using the same morph technique. The proposed method overcomes these issues. The method decomposes a morph in latent space allowing it to demorph images created from unseen morph techniques and face styles. We train our method on morphs created from synthetic faces and test on morphs created from real faces using arbitrary morph techniques. Our method outperforms existing methods by a considerable margin and produces high fidelity demorphed face images.</li>
</ul>

<h3>Title: Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yanzuo Lu, Yuxi Ren, Xin Xia, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Andy J. Ma, Xiaohua Xie, Jian-Huang Lai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18569">https://arxiv.org/abs/2507.18569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18569">https://arxiv.org/pdf/2507.18569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18569]] Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis(https://arxiv.org/abs/2507.18569)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Distribution Matching Distillation (DMD) is a promising score distillation technique that compresses pre-trained teacher diffusion models into efficient one-step or multi-step student generators. Nevertheless, its reliance on the reverse Kullback-Leibler (KL) divergence minimization potentially induces mode collapse (or mode-seeking) in certain applications. To circumvent this inherent drawback, we propose Adversarial Distribution Matching (ADM), a novel framework that leverages diffusion-based discriminators to align the latent predictions between real and fake score estimators for score distillation in an adversarial manner. In the context of extremely challenging one-step distillation, we further improve the pre-trained generator by adversarial distillation with hybrid discriminators in both latent and pixel spaces. Different from the mean squared error used in DMD2 pre-training, our method incorporates the distributional loss on ODE pairs collected from the teacher model, and thus providing a better initialization for score distillation fine-tuning in the next stage. By combining the adversarial distillation pre-training with ADM fine-tuning into a unified pipeline termed DMDX, our proposed method achieves superior one-step performance on SDXL compared to DMD2 while consuming less GPU time. Additional experiments that apply multi-step ADM distillation on SD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient image and video synthesis.</li>
</ul>

<h3>Title: Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods</h3>
<ul>
<li><strong>Authors: </strong>Ganesh Sapkota, Md Hasibur Rahman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18570">https://arxiv.org/abs/2507.18570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18570">https://arxiv.org/pdf/2507.18570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18570]] Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods(https://arxiv.org/abs/2507.18570)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a novel hybrid tokenization strategy that enhances the performance of DNA Language Models (DLMs) by combining 6-mer tokenization with Byte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at capturing local DNA sequence structures but often faces challenges, including uneven token distribution and a limited understanding of global sequence context. To address these limitations, we propose merging unique 6mer tokens with optimally selected BPE tokens generated through 600 BPE cycles. This hybrid approach ensures a balanced and context-aware vocabulary, enabling the model to capture both short and long patterns within DNA sequences simultaneously. A foundational DLM trained on this hybrid vocabulary was evaluated using next-k-mer prediction as a fine-tuning task, demonstrating significantly improved performance. The model achieved prediction accuracies of 10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming state-of-the-art models such as NT, DNABERT2, and GROVER. These results highlight the ability of the hybrid tokenization strategy to preserve both the local sequence structure and global contextual information in DNA modeling. This work underscores the importance of advanced tokenization methods in genomic language modeling and lays a robust foundation for future applications in downstream DNA sequence analysis and biological research.</li>
</ul>

<h3>Title: HybridTM: Combining Transformer and Mamba for 3D Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Wang, Jinghua Hou, Zhe Liu, Yingying Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18575">https://arxiv.org/abs/2507.18575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18575">https://arxiv.org/pdf/2507.18575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18575]] HybridTM: Combining Transformer and Mamba for 3D Semantic Segmentation(https://arxiv.org/abs/2507.18575)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Transformer-based methods have demonstrated remarkable capabilities in 3D semantic segmentation through their powerful attention mechanisms, but the quadratic complexity limits their modeling of long-range dependencies in large-scale point clouds. While recent Mamba-based approaches offer efficient processing with linear complexity, they struggle with feature representation when extracting 3D features. However, effectively combining these complementary strengths remains an open challenge in this field. In this paper, we propose HybridTM, the first hybrid architecture that integrates Transformer and Mamba for 3D semantic segmentation. In addition, we propose the Inner Layer Hybrid Strategy, which combines attention and Mamba at a finer granularity, enabling simultaneous capture of long-range dependencies and fine-grained local features. Extensive experiments demonstrate the effectiveness and generalization of our HybridTM on diverse indoor and outdoor datasets. Furthermore, our HybridTM achieves state-of-the-art performance on ScanNet, ScanNet200, and nuScenes benchmarks. The code will be made available at this https URL.</li>
</ul>

<h3>Title: Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs</h3>
<ul>
<li><strong>Authors: </strong>Feng Hong, Geng Yu, Yushi Ye, Haicheng Huang, Huangjie Zheng, Ya Zhang, Yanfeng Wang, Jiangchao Yao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18578">https://arxiv.org/abs/2507.18578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18578">https://arxiv.org/pdf/2507.18578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18578]] Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs(https://arxiv.org/abs/2507.18578)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion Large Language Models (DLLMs) have emerged as a compelling alternative to Autoregressive models, designed for fast parallel generation. However, existing DLLMs are plagued by a severe quality-speed trade-off, where faster parallel decoding leads to significant performance degradation. We attribute this to the irreversibility of standard decoding in DLLMs, which is easily polarized into the wrong decoding direction along with early error context accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO), a training-free decoding algorithm that enables revokable decoding in DLLMs. WINO employs a parallel draft-and-verify mechanism, aggressively drafting multiple tokens while simultaneously using the model's bidirectional context to verify and re-mask suspicious ones for refinement. Verified in open-source DLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the quality-speed trade-off. For instance, on the GSM8K math benchmark, it accelerates inference by 6$\times$ while improving accuracy by 2.58%; on Flickr30K captioning, it achieves a 10$\times$ speedup with higher performance. More comprehensive experiments are conducted to demonstrate the superiority and provide an in-depth understanding of WINO.</li>
</ul>

<h3>Title: System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Wang, Ramen Liu, Longhui Zhang, Jing Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18580">https://arxiv.org/abs/2507.18580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18580">https://arxiv.org/pdf/2507.18580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18580]] System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition(https://arxiv.org/abs/2507.18580)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This paper presents our system for CCL25-Eval Task 10, addressing Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel SRAG-MAV framework that synergistically integrates task reformulation(TR), Self-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting (MAV). Our method reformulates the quadruplet extraction task into triplet extraction, uses dynamic retrieval from the training set to create contextual prompts, and applies multi-round inference with voting to improve output stability and performance. Our system, based on the Qwen2.5-7B model, achieves a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o (Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The code is available at this https URL.</li>
</ul>

<h3>Title: AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiaopeng Ke, Hexuan Deng, Xuebo Liu, Jun Rao, Zhenxi Song, Jun Yu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18584">https://arxiv.org/abs/2507.18584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18584">https://arxiv.org/pdf/2507.18584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18584]] AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs(https://arxiv.org/abs/2507.18584)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the impressive performance of large language models (LLMs) in general domains, they often underperform in specialized domains. Existing approaches typically rely on data synthesis methods and yield promising results by using unlabeled data to capture domain-specific features. However, these methods either incur high computational costs or suffer from performance limitations, while also demonstrating insufficient generalization across different tasks. To address these challenges, we propose AQuilt, a framework for constructing instruction-tuning data for any specialized domains from corresponding unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic, and Task type. By incorporating logic and inspection, we encourage reasoning processes and self-inspection to enhance model performance. Moreover, customizable task instructions enable high-quality data generation for any task. As a result, we construct a dataset of 703k examples to train a powerful data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3 while utilizing just 17% of the production cost. Further analysis demonstrates that our generated data exhibits higher relevance to downstream tasks. Source code, models, and scripts are available at this https URL.</li>
</ul>

<h3>Title: Linear Memory SE(2) Invariant Attention</h3>
<ul>
<li><strong>Authors: </strong>Ethan Pronovost, Neha Boloor, Peter Schleede, Noureldin Hendy, Andres Morales, Nicholas Roy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18597">https://arxiv.org/abs/2507.18597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18597">https://arxiv.org/pdf/2507.18597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18597]] Linear Memory SE(2) Invariant Attention(https://arxiv.org/abs/2507.18597)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Processing spatial data is a key component in many learning tasks for autonomous driving such as motion forecasting, multi-agent simulation, and planning. Prior works have demonstrated the value in using SE(2) invariant network architectures that consider only the relative poses between objects (e.g. other agents, scene features such as traffic lanes). However, these methods compute the relative poses for all pairs of objects explicitly, requiring quadratic memory. In this work, we propose a mechanism for SE(2) invariant scaled dot-product attention that requires linear memory relative to the number of objects in the scene. Our SE(2) invariant transformer architecture enjoys the same scaling properties that have benefited large language models in recent years. We demonstrate experimentally that our approach is practical to implement and improves performance compared to comparable non-invariant architectures.</li>
</ul>

<h3>Title: Demystify Protein Generation with Hierarchical Conditional Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zinan Ling, Yi Shi, Da Yan, Yang Zhou, Bo Hui</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18603">https://arxiv.org/abs/2507.18603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18603">https://arxiv.org/pdf/2507.18603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18603]] Demystify Protein Generation with Hierarchical Conditional Diffusion Models(https://arxiv.org/abs/2507.18603)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating novel and functional protein sequences is critical to a wide range of applications in biology. Recent advancements in conditional diffusion models have shown impressive empirical performance in protein generation tasks. However, reliable generations of protein remain an open research question in de novo protein design, especially when it comes to conditional diffusion models. Considering the biological function of a protein is determined by multi-level structures, we propose a novel multi-level conditional diffusion model that integrates both sequence-based and structure-based information for efficient end-to-end protein design guided by specified functions. By generating representations at different levels simultaneously, our framework can effectively model the inherent hierarchical relations between different levels, resulting in an informative and discriminative representation of the generated protein. We also propose a Protein-MMD, a new reliable evaluation metric, to evaluate the quality of generated protein with conditional diffusion models. Our new metric is able to capture both distributional and functional similarities between real and generated protein sequences while ensuring conditional consistency. We experiment with the benchmark datasets, and the results on conditional protein generation tasks demonstrate the efficacy of the proposed generation framework and evaluation metric.</li>
</ul>

<h3>Title: TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards</h3>
<ul>
<li><strong>Authors: </strong>Andreea Nica, Ivan Zakazov, Nicolas Mario Baldwin, Saibo Geng, Robert West</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18618">https://arxiv.org/abs/2507.18618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18618">https://arxiv.org/pdf/2507.18618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18618]] TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards(https://arxiv.org/abs/2507.18618)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompt optimization improves the reasoning abilities of large language models (LLMs) without requiring parameter updates to the target model. Following heuristic-based "Think step by step" approaches, the field has evolved in two main directions: while one group of methods uses textual feedback to elicit improved prompts from general-purpose LLMs in a training-free way, a concurrent line of research relies on numerical rewards to train a special prompt model, tailored for providing optimal prompts to the target model. In this paper, we introduce the Textual Reward Prompt framework (TRPrompt), which unifies these approaches by directly incorporating textual feedback into training of the prompt model. Our framework does not require prior dataset collection and is being iteratively improved with the feedback on the generated prompts. When coupled with the capacity of an LLM to internalize the notion of what a "good" prompt is, the high-resolution signal provided by the textual rewards allows us to train a prompt model yielding state-of-the-art query-specific prompts for the problems from the challenging math datasets GSMHard and MATH.</li>
</ul>

<h3>Title: Gait Recognition Based on Tiny ML and IMU Sensors</h3>
<ul>
<li><strong>Authors: </strong>Jiahang Zhang, Mingtong Chen, Zhengbao Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18627">https://arxiv.org/abs/2507.18627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18627">https://arxiv.org/pdf/2507.18627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18627]] Gait Recognition Based on Tiny ML and IMU Sensors(https://arxiv.org/abs/2507.18627)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This project presents the development of a gait recognition system using Tiny Machine Learning (Tiny ML) and Inertial Measurement Unit (IMU) sensors. The system leverages the XIAO-nRF52840 Sense microcontroller and the LSM6DS3 IMU sensor to capture motion data, including acceleration and angular velocity, from four distinct activities: walking, stationary, going upstairs, and going downstairs. The data collected is processed through Edge Impulse, an edge AI platform, which enables the training of machine learning models that can be deployed directly onto the microcontroller for real-time activity this http URL data preprocessing step involves extracting relevant features from the raw sensor data using techniques such as sliding windows and data normalization, followed by training a Deep Neural Network (DNN) classifier for activity recognition. The model achieves over 80% accuracy on a test dataset, demonstrating its ability to classify the four activities effectively. Additionally, the platform enables anomaly detection, further enhancing the robustness of the system. The integration of Tiny ML ensures low-power operation, making it suitable for battery-powered or energy-harvesting devices.</li>
</ul>

<h3>Title: Captain Cinema: Towards Short Movie Generation</h3>
<ul>
<li><strong>Authors: </strong>Junfei Xiao, Ceyuan Yang, Lvmin Zhang, Shengqu Cai, Yang Zhao, Yuwei Guo, Gordon Wetzstein, Maneesh Agrawala, Alan Yuille, Lu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18634">https://arxiv.org/abs/2507.18634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18634">https://arxiv.org/pdf/2507.18634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18634]] Captain Cinema: Towards Short Movie Generation(https://arxiv.org/abs/2507.18634)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We present Captain Cinema, a generation framework for short movie generation. Given a detailed textual description of a movie storyline, our approach firstly generates a sequence of keyframes that outline the entire narrative, which ensures long-range coherence in both the storyline and visual appearance (e.g., scenes and characters). We refer to this step as top-down keyframe planning. These keyframes then serve as conditioning signals for a video synthesis model, which supports long context learning, to produce the spatio-temporal dynamics between them. This step is referred to as bottom-up video synthesis. To support stable and efficient generation of multi-scene long narrative cinematic works, we introduce an interleaved training strategy for Multimodal Diffusion Transformers (MM-DiT), specifically adapted for long-context video data. Our model is trained on a specially curated cinematic dataset consisting of interleaved data pairs. Our experiments demonstrate that Captain Cinema performs favorably in the automated creation of visually coherent and narrative consistent short movies in high quality and efficiency. Project page: this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
