<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-05-21</h1>
<h3>Title: An Edge AI Solution for Space Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Zhang, Peng Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, astro-ph.IM, cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13468">https://arxiv.org/abs/2505.13468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13468">https://arxiv.org/pdf/2505.13468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13468]] An Edge AI Solution for Space Object Detection(https://arxiv.org/abs/2505.13468)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Effective Edge AI for space object detection (SOD) tasks that can facilitate real-time collision assessment and avoidance is essential with the increasing space assets in near-Earth orbits. In SOD, low Earth orbit (LEO) satellites must detect other objects with high precision and minimal delay. We explore an Edge AI solution based on deep-learning-based vision sensing for SOD tasks and propose a deep learning model based on Squeeze-and-Excitation (SE) layers, Vision Transformers (ViT), and YOLOv9 framework. We evaluate the performance of these models across various realistic SOD scenarios, demonstrating their ability to detect multiple satellites with high accuracy and very low latency.</li>
</ul>

<h3>Title: Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale</h3>
<ul>
<li><strong>Authors: </strong>Avinash Patil, Siru Tao, Amardeep Gedhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13480">https://arxiv.org/abs/2505.13480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13480">https://arxiv.org/pdf/2505.13480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13480]] Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale(https://arxiv.org/abs/2505.13480)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Suicide prevention remains a critical public health challenge. While online platforms such as Reddit's r/SuicideWatch have historically provided spaces for individuals to express suicidal thoughts and seek community support, the advent of large language models (LLMs) introduces a new paradigm-where individuals may begin disclosing ideation to AI systems instead of humans. This study evaluates the capability of LLMs to perform automated suicide risk assessment using the Columbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot performance of six models-including Claude, GPT, Mistral, and LLaMA-in classifying posts across a 7-point severity scale (Levels 0-6). Results indicate that Claude and GPT closely align with human annotations, while Mistral achieves the lowest ordinal prediction error. Most models exhibit ordinal sensitivity, with misclassifications typically occurring between adjacent severity levels. We further analyze confusion patterns, misclassification sources, and ethical considerations, underscoring the importance of human oversight, transparency, and cautious deployment. Full code and supplementary materials are available at this https URL.</li>
</ul>

<h3>Title: Detecting Prefix Bias in LLM-based Reward Models</h3>
<ul>
<li><strong>Authors: </strong>Ashwin Kumar, Yuzi He, Aram H. Markosyan, Bobbie Chern, Imanol Arrieta-Ibarra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13487">https://arxiv.org/abs/2505.13487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13487">https://arxiv.org/pdf/2505.13487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13487]] Detecting Prefix Bias in LLM-based Reward Models(https://arxiv.org/abs/2505.13487)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Human Feedback (RLHF) has emerged as a key paradigm for task-specific fine-tuning of language models using human preference data. While numerous publicly available preference datasets provide pairwise comparisons of responses, the potential for biases in the resulting reward models remains underexplored. In this work, we introduce novel methods to detect and evaluate prefix bias -- a systematic shift in model preferences triggered by minor variations in query prefixes -- in LLM-based reward models trained on such datasets. We leverage these metrics to reveal significant biases in preference models across racial and gender dimensions. Our comprehensive evaluation spans diverse open-source preference datasets and reward model architectures, demonstrating susceptibility to this kind of bias regardless of the underlying model architecture. Furthermore, we propose a data augmentation strategy to mitigate these biases, showing its effectiveness in reducing the impact of prefix bias. Our findings highlight the critical need for bias-aware dataset design and evaluation in developing fair and reliable reward models, contributing to the broader discourse on fairness in AI.</li>
</ul>

<h3>Title: Source framing triggers systematic evaluation bias in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Federico Germani, Giovanni Spitale</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13488">https://arxiv.org/abs/2505.13488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13488">https://arxiv.org/pdf/2505.13488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13488]] Source framing triggers systematic evaluation bias in Large Language Models(https://arxiv.org/abs/2505.13488)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used not only to generate text but also to evaluate it, raising urgent questions about whether their judgments are consistent, unbiased, and robust to framing effects. In this study, we systematically examine inter- and intra-model agreement across four state-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and Mistral) tasked with evaluating 4,800 narrative statements on 24 different topics of social, political, and public health relevance, for a total of 192,000 assessments. We manipulate the disclosed source of each statement to assess how attribution to either another LLM or a human author of specified nationality affects evaluation outcomes. We find that, in the blind condition, different LLMs display a remarkably high degree of inter- and intra-model agreement across topics. However, this alignment breaks down when source framing is introduced. Here we show that attributing statements to Chinese individuals systematically lowers agreement scores across all models, and in particular for Deepseek Reasoner. Our findings reveal that framing effects can deeply affect text evaluation, with significant implications for the integrity, neutrality, and fairness of LLM-mediated information systems.</li>
</ul>

<h3>Title: ProdRev: A DNN framework for empowering customers using generative pre-trained transformers</h3>
<ul>
<li><strong>Authors: </strong>Aakash Gupta, Nataraj Das</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13491">https://arxiv.org/abs/2505.13491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13491">https://arxiv.org/pdf/2505.13491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13491]] ProdRev: A DNN framework for empowering customers using generative pre-trained transformers(https://arxiv.org/abs/2505.13491)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Following the pandemic, customers, preference for using e-commerce has accelerated. Since much information is available in multiple reviews (sometimes running in thousands) for a single product, it can create decision paralysis for the buyer. This scenario disempowers the consumer, who cannot be expected to go over so many reviews since its time consuming and can confuse them. Various commercial tools are available, that use a scoring mechanism to arrive at an adjusted score. It can alert the user to potential review manipulations. This paper proposes a framework that fine-tunes a generative pre-trained transformer to understand these reviews better. Furthermore, using "common-sense" to make better decisions. These models have more than 13 billion parameters. To fine-tune the model for our requirement, we use the curie engine from generative pre-trained transformer (GPT3). By using generative models, we are introducing abstractive summarization. Instead of using a simple extractive method of summarizing the reviews. This brings out the true relationship between the reviews and not simply copy-paste. This introduces an element of "common sense" for the user and helps them to quickly make the right decisions. The user is provided the pros and cons of the processed reviews. Thus the user/customer can take their own decisions.</li>
</ul>

<h3>Title: LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Weiming Zhang, Lingyue Fu, Qingyao Li, Kounianhua Du, Jianghao Lin, Jingwei Yu, Wei Xia, Weinan Zhang, Ruiming Tang, Yong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13492">https://arxiv.org/abs/2505.13492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13492">https://arxiv.org/pdf/2505.13492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13492]] LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis(https://arxiv.org/abs/2505.13492)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Cognitive diagnosis (CD) plays a crucial role in intelligent education, evaluating students' comprehension of knowledge concepts based on their test histories. However, current CD methods often model students, exercises, and knowledge concepts solely on their ID relationships, neglecting the abundant semantic relationships present within educational data space. Furthermore, contemporary intelligent tutoring systems (ITS) frequently involve the addition of new students and exercises, a situation that ID-based methods find challenging to manage effectively. The advent of large language models (LLMs) offers the potential for overcoming this challenge with open-world knowledge. In this paper, we propose LLM4CD, which Leverages Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis. Our method utilizes the open-world knowledge of LLMs to construct cognitively expressive textual representations, which are then encoded to introduce rich semantic information into the CD task. Additionally, we propose an innovative bi-level encoder framework that models students' test histories through two levels of encoders: a macro-level cognitive text encoder and a micro-level knowledge state encoder. This approach substitutes traditional ID embeddings with semantic representations, enabling the model to accommodate new students and exercises with open-world knowledge and address the cold-start problem. Extensive experimental results demonstrate that our proposed method consistently outperforms previous CD models on multiple real-world datasets, validating the effectiveness of leveraging LLMs to introduce rich semantic information into the CD task.</li>
</ul>

<h3>Title: Optimizing DDoS Detection in SDNs Through Machine Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Md. Ehsanul Haque, Amran Hossain, Md. Shafiqul Alam, Ahsan Habib Siam, Sayed Md Fazle Rabbi, Md. Muntasir Rahman</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13493">https://arxiv.org/abs/2505.13493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13493">https://arxiv.org/pdf/2505.13493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13493]] Optimizing DDoS Detection in SDNs Through Machine Learning Models(https://arxiv.org/abs/2505.13493)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, generative</a></li>
<li><strong>Abstract: </strong>The emergence of Software-Defined Networking (SDN) has changed the network structure by separating the control plane from the data plane. However, this innovation has also increased susceptibility to DDoS attacks. Existing detection techniques are often ineffective due to data imbalance and accuracy issues; thus, a considerable research gap exists regarding DDoS detection methods suitable for SDN contexts. This research attempts to detect DDoS attacks more effectively using machine learning algorithms: RF, SVC, KNN, MLP, and XGB. For this purpose, both balanced and imbalanced datasets have been used to measure the performance of the models in terms of accuracy and AUC. Based on the analysis, we can say that RF and XGB had the perfect score, 1.0000, in the accuracy and AUC, but since XGB ended with the lowest Brier Score which indicates the highest reliability. MLP achieved an accuracy of 99.93%, SVC an accuracy of 97.65% and KNN an accuracy of 97.87%, which was the next best performers after RF and XGB. These results are consistent with the validity of SDNs as a platform for RF and XGB techniques in detecting DDoS attacks and highlights the importance of balanced datasets for improving detection against generative cyber attacks that are continually evolving.</li>
</ul>

<h3>Title: IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Khanh-Tung Tran, Barry O'Sullivan, Hoang D. Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13498">https://arxiv.org/abs/2505.13498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13498">https://arxiv.org/pdf/2505.13498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13498]] IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation(https://arxiv.org/abs/2505.13498)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have demonstrated promising knowledge and reasoning abilities, yet their performance in multilingual and low-resource settings remains underexplored. Existing benchmarks often exhibit cultural bias, restrict evaluation to text-only, rely on multiple-choice formats, and, more importantly, are limited for extremely low-resource languages. To address these gaps, we introduce IRLBench, presented in parallel English and Irish, which is considered definitely endangered by UNESCO. Our benchmark consists of 12 representative subjects developed from the 2024 Irish Leaving Certificate exams, enabling fine-grained analysis of model capabilities across domains. By framing the task as long-form generation and leveraging the official marking scheme, it does not only support a comprehensive evaluation of correctness but also language fidelity. Our extensive experiments of leading closed-source and open-source LLMs reveal a persistent performance gap between English and Irish, in which models produce valid Irish responses less than 80\% of the time, and answer correctly 55.8\% of the time compared to 76.2\% in English for the best-performing model. We release IRLBench (this https URL) and an accompanying evaluation codebase (this https URL) to enable future research on robust, culturally aware multilingual AI development.</li>
</ul>

<h3>Title: Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Kelvin Kan, Xingjian Li, Benjamin J. Zhang, Tuhin Sahai, Stanley Osher, Markos A. Katsoulakis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13499">https://arxiv.org/abs/2505.13499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13499">https://arxiv.org/pdf/2505.13499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13499]] Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency(https://arxiv.org/abs/2505.13499)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>We study Transformers through the perspective of optimal control theory, using tools from continuous-time formulations to derive actionable insights into training and architecture design. This framework improves the performance of existing Transformer models while providing desirable theoretical guarantees, including generalization and robustness. Our framework is designed to be plug-and-play, enabling seamless integration with established Transformer models and requiring only slight changes to the implementation. We conduct seven extensive experiments on tasks motivated by text generation, sentiment analysis, image classification, and point cloud classification. Experimental results show that the framework improves the test performance of the baselines, while being more parameter-efficient. On character-level text generation with nanoGPT, our framework achieves a 46% reduction in final test loss while using 42% fewer parameters. On GPT-2, our framework achieves a 5.6% reduction in final test loss, demonstrating scalability to larger models. To the best of our knowledge, this is the first work that applies optimal control theory to both the training and architecture of Transformers. It offers a new foundation for systematic, theory-driven improvements and moves beyond costly trial-and-error approaches.</li>
</ul>

<h3>Title: Noise Injection Systemically Degrades Large Language Model Safety Guardrails</h3>
<ul>
<li><strong>Authors: </strong>Prithviraj Singh Shahani, Matthias Scheutz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13500">https://arxiv.org/abs/2505.13500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13500">https://arxiv.org/pdf/2505.13500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13500]] Noise Injection Systemically Degrades Large Language Model Safety Guardrails(https://arxiv.org/abs/2505.13500)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, large language model</a></li>
<li><strong>Abstract: </strong>Safety guardrails in large language models (LLMs) are a critical component in preventing harmful outputs. Yet, their resilience under perturbation remains poorly understood. In this paper, we investigate the robustness of safety fine-tuning in LLMs by systematically injecting Gaussian noise into model activations. We show across multiple open-weight models that (1) Gaussian noise raises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety fine-tuning affords no extra protection, and (3) that chain-of-thought reasoning remains largely intact. The findings reveal critical vulnerabilities in current safety alignment techniques and highlight the potential of reasoning-based and reinforcement learning approaches as promising direction for developing more robust AI safety systems. These results have important implications for real-world deployment of LLMs in safety-critical applications as these results imply that widely-deployed safety tuning methods can fail even without adversarial prompts.</li>
</ul>

<h3>Title: SPIEDiff: robust learning of long-time macroscopic dynamics from short-time particle simulations with quantified epistemic uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Zequn He, Celia Reina</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13501">https://arxiv.org/abs/2505.13501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13501">https://arxiv.org/pdf/2505.13501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13501]] SPIEDiff: robust learning of long-time macroscopic dynamics from short-time particle simulations with quantified epistemic uncertainty(https://arxiv.org/abs/2505.13501)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>The data-driven discovery of long-time macroscopic dynamics and thermodynamics of dissipative systems with particle fidelity is hampered by significant obstacles. These include the strong time-scale limitations inherent to particle simulations, the non-uniqueness of the thermodynamic potentials and operators from given macroscopic dynamics, and the need for efficient uncertainty quantification. This paper introduces Statistical-Physics Informed Epistemic Diffusion Models (SPIEDiff), a machine learning framework designed to overcome these limitations in the context of purely dissipative systems by leveraging statistical physics, conditional diffusion models, and epinets. We evaluate the proposed framework on stochastic Arrhenius particle processes and demonstrate that SPIEDiff can accurately uncover both thermodynamics and kinetics, while enabling reliable long-time macroscopic predictions using only short-time particle simulation data. SPIEDiff can deliver accurate predictions with quantified uncertainty in minutes, drastically reducing the computational demand compared to direct particle simulations, which would take days or years in the examples considered. Overall, SPIEDiff offers a robust and trustworthy pathway for the data-driven discovery of thermodynamic models.</li>
</ul>

<h3>Title: Federated Low-Rank Adaptation for Foundation Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Yiyuan Yang, Guodong Long, Qinghua Lu, Liming Zhu, Jing Jiang, Chengqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13502">https://arxiv.org/abs/2505.13502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13502">https://arxiv.org/pdf/2505.13502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13502]] Federated Low-Rank Adaptation for Foundation Models: A Survey(https://arxiv.org/abs/2505.13502)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Effectively leveraging private datasets remains a significant challenge in developing foundation models. Federated Learning (FL) has recently emerged as a collaborative framework that enables multiple users to fine-tune these models while mitigating data privacy risks. Meanwhile, Low-Rank Adaptation (LoRA) offers a resource-efficient alternative for fine-tuning foundation models by dramatically reducing the number of trainable parameters. This survey examines how LoRA has been integrated into federated fine-tuning for foundation models, an area we term FedLoRA, by focusing on three key challenges: distributed learning, heterogeneity, and efficiency. We further categorize existing work based on the specific methods used to address each challenge. Finally, we discuss open research questions and highlight promising directions for future investigation, outlining the next steps for advancing FedLoRA.</li>
</ul>

<h3>Title: EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruobing Yao, Yifei Zhang, Shuang Song, Neng Gao, Chenyang Tu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13506">https://arxiv.org/abs/2505.13506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13506">https://arxiv.org/pdf/2505.13506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13506]] EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation(https://arxiv.org/abs/2505.13506)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) compensates for the static knowledge limitations of Large Language Models (LLMs) by integrating external knowledge, producing responses with enhanced factual correctness and query-specific contextualization. However, it also introduces new attack surfaces such as corpus poisoning at the same time. Most of the existing defense methods rely on the internal knowledge of the model, which conflicts with the design concept of RAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and bait-guided context diversity detection to identify malicious content by analyzing the context diversity of candidate documents without relying on LLM internal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art security with plug-and-play deployment, simultaneously improving clean-scenario RAG performance while maintaining practical operational costs (relatively 1.2$\times$ latency, 48\%-80\% token reduction versus Vanilla RAG).</li>
</ul>

<h3>Title: Time-R1: Towards Comprehensive Temporal Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zijia Liu, Peixuan Han, Haofei Yu, Haoru Li, Jiaxuan You</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13508">https://arxiv.org/abs/2505.13508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13508">https://arxiv.org/pdf/2505.13508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13508]] Time-R1: Towards Comprehensive Temporal Reasoning in LLMs(https://arxiv.org/abs/2505.13508)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, we introduce \textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. Our approach features a novel three-stage development path; the first two constitute a \textit{reinforcement learning (RL) curriculum} driven by a meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event prediction skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards truly time-aware AI. To foster further research, we also release \textit{Time-Bench}, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of \textit{Time-R1} checkpoints.</li>
</ul>

<h3>Title: On the definition and importance of interpretability in scientific machine learning</h3>
<ul>
<li><strong>Authors: </strong>Conor Rowan, Alireza Doostan</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.data-an, physics.hist-ph, physics.soc-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13510">https://arxiv.org/abs/2505.13510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13510">https://arxiv.org/pdf/2505.13510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13510]] On the definition and importance of interpretability in scientific machine learning(https://arxiv.org/abs/2505.13510)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Though neural networks trained on large data sets have been successfully used to describe and predict many physical phenomena, there is a sense among scientists that, unlike traditional scientific models, where relationships come packaged in the form of simple mathematical expressions, the findings of the neural network cannot be integrated into the body of scientific knowledge. Critics of ML's inability to produce human-understandable relationships have converged on the concept of "interpretability" as its point of departure from more traditional forms of science. As the growing interest in interpretability has shown, researchers in the physical sciences seek not just predictive models, but also to uncover the fundamental principles that govern a system of interest. However, clarity around a definition of interpretability and the precise role that it plays in science is lacking in the literature. In this work, we argue that researchers in equation discovery and symbolic regression tend to conflate the concept of sparsity with interpretability. We review key papers on interpretable ML from outside the scientific community and argue that, though the definitions and methods they propose can inform questions of interpretability for SciML, they are inadequate for this new purpose. Noting these deficiencies, we propose an operational definition of interpretability for the physical sciences. Our notion of interpretability emphasizes understanding of the mechanism over mathematical sparsity. Innocuous though it may seem, this emphasis on mechanism shows that sparsity is often unnecessary. It also questions the possibility of interpretable scientific discovery when prior knowledge is lacking. We believe a precise and philosophically informed definition of interpretability in SciML will help focus research efforts toward the most significant obstacles to realizing a data-driven scientific future.</li>
</ul>

<h3>Title: Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shuxun Wang, Qingyu Yin, Chak Tou Leong, Qiang Zhang, Linyi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13514">https://arxiv.org/abs/2505.13514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13514">https://arxiv.org/pdf/2505.13514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13514]] Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models(https://arxiv.org/abs/2505.13514)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Repetition curse is a phenomenon where Large Language Models (LLMs) generate repetitive sequences of tokens or cyclic sequences. While the repetition curse has been widely observed, its underlying mechanisms remain poorly understood. In this work, we investigate the role of induction heads--a specific type of attention head known for their ability to perform in-context learning--in driving this repetitive behavior. Specifically, we focus on the "toxicity" of induction heads, which we define as their tendency to dominate the model's output logits during repetition, effectively excluding other attention heads from contributing to the generation process. Our findings have important implications for the design and training of LLMs. By identifying induction heads as a key driver of the repetition curse, we provide a mechanistic explanation for this phenomenon and suggest potential avenues for mitigation. We also propose a technique with attention head regularization that could be employed to reduce the dominance of induction heads during generation, thereby promoting more diverse and coherent outputs.</li>
</ul>

<h3>Title: LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades</h3>
<ul>
<li><strong>Authors: </strong>Yanan Li, Fanxu Meng, Muhan Zhang, Shiai Zhu, Shangguang Wang, Mengwei Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13515">https://arxiv.org/abs/2505.13515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13515">https://arxiv.org/pdf/2505.13515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13515]] LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades(https://arxiv.org/abs/2505.13515)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are frequently updated, LoRA weights trained on earlier versions quickly become obsolete. The conventional practice of retraining LoRA weights from scratch on the latest model is costly, time-consuming, and environmentally detrimental, particularly as the diversity of LLMs and downstream tasks expands. This motivates a critical question: "How can we efficiently leverage existing LoRA weights to adapt to newer model versions?" To address this, we propose LoRASuite, a modular approach tailored specifically to various types of LLM updates. First, we compute a transfer matrix utilizing known parameters from both old and new LLMs. Next, we allocate corresponding layers and attention heads based on centered kernel alignment and cosine similarity metrics, respectively. A subsequent small-scale, skillful fine-tuning step ensures numerical stability. Experimental evaluations demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even exceeds the performance of full-scale LoRA retraining, with average improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally, LoRASuite significantly reduces memory consumption by 5.5 GB and computational time by 78.23%.</li>
</ul>

<h3>Title: Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression</h3>
<ul>
<li><strong>Authors: </strong>Jingyu Peng, Maolin Wang, Nan Wang, Xiangyu Zhao, Jiatong Li, Kai Zhang, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13527">https://arxiv.org/abs/2505.13527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13527">https://arxiv.org/pdf/2505.13527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13527]] Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression(https://arxiv.org/abs/2505.13527)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Despite substantial advancements in aligning large language models (LLMs) with human values, current safety mechanisms remain susceptible to jailbreak attacks. We hypothesize that this vulnerability stems from distributional discrepancies between alignment-oriented prompts and malicious prompts. To investigate this, we introduce LogiBreak, a novel and universal black-box jailbreak method that leverages logical expression translation to circumvent LLM safety systems. By converting harmful natural language prompts into formal logical expressions, LogiBreak exploits the distributional gap between alignment data and logic-based inputs, preserving the underlying semantic intent and readability while evading safety constraints. We evaluate LogiBreak on a multilingual jailbreak dataset spanning three languages, demonstrating its effectiveness across various evaluation settings and linguistic contexts.</li>
</ul>

<h3>Title: Multi-head Temporal Latent Attention</h3>
<ul>
<li><strong>Authors: </strong>Keqi Deng, Philip C. Woodland</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13544">https://arxiv.org/abs/2505.13544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13544">https://arxiv.org/pdf/2505.13544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13544]] Multi-head Temporal Latent Attention(https://arxiv.org/abs/2505.13544)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While Transformer self-attention offers strong parallelism, the Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency. Multi-head latent attention was recently developed to compress the KV cache into a low-rank latent space. This paper proposes Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache size along the temporal dimension, greatly lowering the memory footprint of self-attention inference. MTLA employs a hyper-network to dynamically merge temporally adjacent KV cache vectors. To address the mismatch between the compressed KV cache and processed sequence lengths, a stride-aware causal mask is proposed to ensure efficient parallel training and consistency with inference behaviour. Experiments across tasks, including speech translation, speech recognition, speech understanding and text summarisation, demonstrate that MTLA achieves competitive performance compared to standard Multi-Head Attention (MHA), while greatly improving inference speed and GPU memory usage. For example, on a English-German speech translation task, MTLA achieves a 5.3x speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA, while maintaining translation quality.</li>
</ul>

<h3>Title: Exploring Federated Pruning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pengxin Guo, Yinong Wang, Wei Li, Mengting Liu, Ming Li, Jinkai Zheng, Liangqiong Qu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13547">https://arxiv.org/abs/2505.13547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13547">https://arxiv.org/pdf/2505.13547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13547]] Exploring Federated Pruning for Large Language Models(https://arxiv.org/abs/2505.13547)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>LLM pruning has emerged as a promising technology for compressing LLMs, enabling their deployment on resource-limited devices. However, current methodologies typically require access to public calibration samples, which can be challenging to obtain in privacy-sensitive domains. To address this issue, we introduce FedPrLLM, a comprehensive federated pruning framework designed for the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs to calculate a pruning mask matrix based on its local calibration data and share it with the server to prune the global model. This approach allows for collaborative pruning of the global model with the knowledge of each client while maintaining local data privacy. Additionally, we conduct extensive experiments to explore various possibilities within the FedPrLLM framework, including different comparison groups, pruning strategies, and the decision to scale weights. Our extensive evaluation reveals that one-shot pruning with layer comparison and no weight scaling is the optimal choice within the FedPrLLM framework. We hope our work will help guide future efforts in pruning LLMs in privacy-sensitive fields. Our code is available at this https URL.</li>
</ul>

<h3>Title: Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation</h3>
<ul>
<li><strong>Authors: </strong>Zhanglin Wu, Daimeng Wei, Xiaoyu Chen, Hengchao Shang, Jiaxin Guo, Zongyao Li, Yuanchang Luo, Jinlong Yang, Zhiqiang Rao, Hao Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13554">https://arxiv.org/abs/2505.13554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13554">https://arxiv.org/pdf/2505.13554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13554]] Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation(https://arxiv.org/abs/2505.13554)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) shows promising performances in a variety of downstream tasks, such as machine translation (MT). However, using LLMs for translation suffers from high computational costs and significant latency. Based on our evaluation, in most cases, translations using LLMs are comparable to that generated by neural machine translation (NMT) systems. Only in particular scenarios, LLM and NMT models show respective advantages. As a result, integrating NMT and LLM for translation and using LLM only when necessary seems to be a sound solution. A scheduling policy that optimizes translation result while ensuring fast speed and as little LLM usage as possible is thereby required. We compare several scheduling policies and propose a novel and straightforward decider that leverages source sentence features. We conduct extensive experiments on multilingual test sets and the result shows that we can achieve optimal translation performance with minimal LLM usage, demonstrating effectiveness of our decider.</li>
</ul>

<h3>Title: CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sathya Krishnan Suresh, Tanmay Surana, Lim Zhi Hao, Eng Siong Chng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13559">https://arxiv.org/abs/2505.13559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13559">https://arxiv.org/pdf/2505.13559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13559]] CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models(https://arxiv.org/abs/2505.13559)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Code-switching (CS) poses a significant challenge for Large Language Models (LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue to English summarization. CS-Sum is the first benchmark for CS dialogue summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language pair. Evaluating ten LLMs, including open and closed-source models, we analyze performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA on synthetic data) approaches. Our findings show that though the scores on automated metrics are high, LLMs make subtle mistakes that alter the complete meaning of the dialogue. To this end, we introduce 3 most common type of errors that LLMs make when handling CS input. Error rates vary across CS pairs and LLMs, with some LLMs showing more frequent errors on certain language pairs, underscoring the need for specialized training on code-switched data.</li>
</ul>

<h3>Title: Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression</h3>
<ul>
<li><strong>Authors: </strong>Xiaohui Wang, Peng Ye, Chenyu Huang, Shenghe Zheng, Bo Zhang, Wanli Ouyang, Tao Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13563">https://arxiv.org/abs/2505.13563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13563">https://arxiv.org/pdf/2505.13563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13563]] Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression(https://arxiv.org/abs/2505.13563)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free, large language model</a></li>
<li><strong>Abstract: </strong>With the rise of the fine-tuned--pretrained paradigm, storing numerous fine-tuned models for multi-tasking creates significant storage overhead. Delta compression alleviates this by storing only the pretrained model and the highly compressed delta weights (the differences between fine-tuned and pretrained model weights). However, existing methods fail to maintain both high compression and performance, and often rely on data. To address these challenges, we propose UltraDelta, the first data-free delta compression pipeline that achieves both ultra-high compression and strong performance. UltraDelta is designed to minimize redundancy, maximize information, and stabilize performance across inter-layer, intra-layer, and global dimensions, using three key components: (1) Variance-Based Mixed Sparsity Allocation assigns sparsity based on variance, giving lower sparsity to high-variance layers to preserve inter-layer information. (2) Distribution-Aware Compression applies uniform quantization and then groups parameters by value, followed by group-wise pruning, to better preserve intra-layer distribution. (3) Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a global rescaling factor, improving model stability under higher compression. Extensive experiments across (a) large language models (fine-tuned on LLaMA-2 7B and 13B) with up to 133x, (b) general NLP models (RoBERTa-base, T5-base) with up to 800x, (c) vision models (ViT-B/32, ViT-L/14) with up to 400x, and (d) multi-modal models (BEiT-3) with 40x compression ratio, demonstrate that UltraDelta consistently outperforms existing methods, especially under ultra-high compression.</li>
</ul>

<h3>Title: FlexFed: Mitigating Catastrophic Forgetting in Heterogeneous Federated Learning in Pervasive Computing Environments</h3>
<ul>
<li><strong>Authors: </strong>Sara Alosaime (University of Warwick), Arshad Jhumka (University of Leeds)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13576">https://arxiv.org/abs/2505.13576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13576">https://arxiv.org/pdf/2505.13576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13576]] FlexFed: Mitigating Catastrophic Forgetting in Heterogeneous Federated Learning in Pervasive Computing Environments(https://arxiv.org/abs/2505.13576)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative model training while preserving privacy by allowing clients to share model updates instead of raw data. Pervasive computing environments (e.g., for Human Activity Recognition, HAR), which we focus on in this paper, are characterized by resource-constrained end devices, streaming sensor data and intermittent client participation. Variations in user behavior, common in HAR environments, often result in non-stationary data distributions. As such, existing FL approaches face challenges in HAR settings due to differing assumptions. The combined effects of HAR characteristics, namely heterogeneous data and intermittent participation, can lead to a severe issue called catastrophic forgetting (CF). Unlike Continuous Learning (CL), which addresses CF using memory and replay mechanisms, FL's privacy constraints prohibit such strategies. To tackle CF in HAR environments, we propose FlexFed, a novel FL approach that prioritizes data retention for efficient memory use and dynamically adjusts offline training frequency based on distribution shifts, client capability and offline duration. To better quantify CF in FL, we introduce a new metric that accounts for under-represented data, enabling more accurate evaluations. We also develop a realistic HAR-based evaluation framework that simulates streaming data, dynamic distributions, imbalances and varying availability. Experiments show that FlexFed mitigates CF more effectively, improves FL efficiency by 10 to 15 % and achieves faster, more stable convergence, especially for infrequent or under-represented data.</li>
</ul>

<h3>Title: OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Hanzhao Wang, Guanting Chen, Kalyan Talluri, Xiaocheng Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13580">https://arxiv.org/abs/2505.13580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13580">https://arxiv.org/pdf/2505.13580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13580]] OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision Making(https://arxiv.org/abs/2505.13580)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>We build a Generative Pre-trained Transformer (GPT) model from scratch to solve sequential decision making tasks arising in contexts of operations research and management science which we call OMGPT. We first propose a general sequence modeling framework to cover several operational decision making tasks as special cases, such as dynamic pricing, inventory management, resource allocation, and queueing control. Under the framework, all these tasks can be viewed as a sequential prediction problem where the goal is to predict the optimal future action given all the historical information. Then we train a transformer-based neural network model (OMGPT) as a natural and powerful architecture for sequential modeling. This marks a paradigm shift compared to the existing methods for these OR/OM tasks in that (i) the OMGPT model can take advantage of the huge amount of pre-trained data; (ii) when tackling these problems, OMGPT does not assume any analytical model structure and enables a direct and rich mapping from the history to the future actions. Either of these two aspects, to the best of our knowledge, is not achieved by any existing method. We establish a Bayesian perspective to theoretically understand the working mechanism of the OMGPT on these tasks, which relates its performance with the pre-training task diversity and the divergence between the testing task and pre-training tasks. Numerically, we observe a surprising performance of the proposed model across all the above tasks.</li>
</ul>

<h3>Title: Self-Supervised Learning for Image Segmentation: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Thangarajah Akilan, Nusrat Jahan, Wandong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13584">https://arxiv.org/abs/2505.13584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13584">https://arxiv.org/pdf/2505.13584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13584]] Self-Supervised Learning for Image Segmentation: A Comprehensive Survey(https://arxiv.org/abs/2505.13584)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Supervised learning demands large amounts of precisely annotated data to achieve promising results. Such data curation is labor-intensive and imposes significant overhead regarding time and costs. Self-supervised learning (SSL) partially overcomes these limitations by exploiting vast amounts of unlabeled data and creating surrogate (pretext or proxy) tasks to learn useful representations without manual labeling. As a result, SSL has become a powerful machine learning (ML) paradigm for solving several practical downstream computer vision problems, such as classification, detection, and segmentation. Image segmentation is the cornerstone of many high-level visual perception applications, including medical imaging, intelligent transportation, agriculture, and surveillance. Although there is substantial research potential for developing advanced algorithms for SSL-based semantic segmentation, a comprehensive study of existing methodologies is essential to trace advances and guide emerging researchers. This survey thoroughly investigates over 150 recent image segmentation articles, particularly focusing on SSL. It provides a practical categorization of pretext tasks, downstream tasks, and commonly used benchmark datasets for image segmentation research. It concludes with key observations distilled from a large body of literature and offers future directions to make this research field more accessible and comprehensible for readers.</li>
</ul>

<h3>Title: Learning (Approximately) Equivariant Networks via Constrained Optimization</h3>
<ul>
<li><strong>Authors: </strong>Andrei Manolache, Luiz F.O. Chamon, Mathias Niepert</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13631">https://arxiv.org/abs/2505.13631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13631">https://arxiv.org/pdf/2505.13631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13631]] Learning (Approximately) Equivariant Networks via Constrained Optimization(https://arxiv.org/abs/2505.13631)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Equivariant neural networks are designed to respect symmetries through their architecture, boosting generalization and sample efficiency when those symmetries are present in the data distribution. Real-world data, however, often departs from perfect symmetry because of noise, structural variation, measurement bias, or other symmetry-breaking effects. Strictly equivariant models may struggle to fit the data, while unconstrained models lack a principled way to leverage partial symmetries. Even when the data is fully symmetric, enforcing equivariance can hurt training by limiting the model to a restricted region of the parameter space. Guided by homotopy principles, where an optimization problem is solved by gradually transforming a simpler problem into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a constrained optimization approach that starts with a flexible, non-equivariant model and gradually reduces its deviation from equivariance. This gradual tightening smooths training early on and settles the model at a data-driven equilibrium, balancing between equivariance and non-equivariance. Across multiple architectures and tasks, our method consistently improves performance metrics, sample efficiency, and robustness to input perturbations compared with strictly equivariant models and heuristic equivariance relaxations.</li>
</ul>

<h3>Title: IPENS:Interactive Unsupervised Framework for Rapid Plant Phenotyping Extraction via NeRF-SAM2 Fusion</h3>
<ul>
<li><strong>Authors: </strong>Wentao Song, He Huang, Youqiang Sun, Fang Qu, Jiaqi Zhang, Longhui Fang, Yuwei Hao, Chenyang Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13633">https://arxiv.org/abs/2505.13633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13633">https://arxiv.org/pdf/2505.13633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13633]] IPENS:Interactive Unsupervised Framework for Rapid Plant Phenotyping Extraction via NeRF-SAM2 Fusion(https://arxiv.org/abs/2505.13633)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Advanced plant phenotyping technologies play a crucial role in targeted trait improvement and accelerating intelligent breeding. Due to the species diversity of plants, existing methods heavily rely on large-scale high-precision manually annotated data. For self-occluded objects at the grain level, unsupervised methods often prove ineffective. This study proposes IPENS, an interactive unsupervised multi-target point cloud extraction method. The method utilizes radiance field information to lift 2D masks, which are segmented by SAM2 (Segment Anything Model 2), into 3D space for target point cloud extraction. A multi-target collaborative optimization strategy is designed to effectively resolve the single-interaction multi-target segmentation challenge. Experimental validation demonstrates that IPENS achieves a grain-level segmentation accuracy (mIoU) of 63.72% on a rice dataset, with strong phenotypic estimation capabilities: grain volume prediction yields R2 = 0.7697 (RMSE = 0.0025), leaf surface area R2 = 0.84 (RMSE = 18.93), and leaf length and width predictions achieve R2 = 0.97 and 0.87 (RMSE = 1.49 and 0.21). On a wheat dataset,IPENS further improves segmentation accuracy to 89.68% (mIoU), with equally outstanding phenotypic estimation performance: spike volume prediction achieves R2 = 0.9956 (RMSE = 0.0055), leaf surface area R2 = 1.00 (RMSE = 0.67), and leaf length and width predictions reach R2 = 0.99 and 0.92 (RMSE = 0.23 and 0.15). This method provides a non-invasive, high-quality phenotyping extraction solution for rice and wheat. Without requiring annotated data, it rapidly extracts grain-level point clouds within 3 minutes through simple single-round interactions on images for multiple targets, demonstrating significant potential to accelerate intelligent breeding efficiency.</li>
</ul>

<h3>Title: Incentivizing Truthful Language Models via Peer Elicitation Games</h3>
<ul>
<li><strong>Authors: </strong>Baiting Chen, Tong Zhu, Jiale Han, Lexin Li, Gang Li, Xiaowu Dai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13636">https://arxiv.org/abs/2505.13636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13636">https://arxiv.org/pdf/2505.13636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13636]] Incentivizing Truthful Language Models via Peer Elicitation Games(https://arxiv.org/abs/2505.13636)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated strong generative capabilities but remain prone to inconsistencies and hallucinations. We introduce Peer Elicitation Games (PEG), a training-free, game-theoretic framework for aligning LLMs through a peer elicitation mechanism involving a generator and multiple discriminators instantiated from distinct base models. Discriminators interact in a peer evaluation setting, where rewards are computed using a determinant-based mutual information score that provably incentivizes truthful reporting without requiring ground-truth labels. We establish theoretical guarantees showing that each agent, via online learning, achieves sublinear regret in the sense their cumulative performance approaches that of the best fixed truthful strategy in hindsight. Moreover, we prove last-iterate convergence to a truthful Nash equilibrium, ensuring that the actual policies used by agents converge to stable and truthful behavior over time. Empirical evaluations across multiple benchmarks demonstrate significant improvements in factual accuracy. These results position PEG as a practical approach for eliciting truthful behavior from LLMs without supervision or fine-tuning.</li>
</ul>

<h3>Title: 4Hammer: a board-game reinforcement learning environment for the hour long time frame</h3>
<ul>
<li><strong>Authors: </strong>Massimo Fioravanti, Giovanni Agosta</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13638">https://arxiv.org/abs/2505.13638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13638">https://arxiv.org/pdf/2505.13638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13638]] 4Hammer: a board-game reinforcement learning environment for the hour long time frame(https://arxiv.org/abs/2505.13638)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated strong performance on tasks with short time frames, but struggle with tasks requiring longer durations. While datasets covering extended-duration tasks, such as software engineering tasks or video games, do exist, there are currently few implementations of complex board games specifically designed for reinforcement learning and LLM evaluation. To address this gap, we propose the 4Hammer reinforcement learning environment, a digital twin simulation of a subset of Warhammer 40,000-a complex, zero-sum board game. Warhammer 40,000 features intricate rules, requiring human players to thoroughly read and understand over 50 pages of detailed natural language rules, grasp the interactions between their game pieces and those of their opponents, and independently track and communicate the evolving game state.</li>
</ul>

<h3>Title: An Alignment Between the CRA's Essential Requirements and the ATT&CK's Mitigations</h3>
<ul>
<li><strong>Authors: </strong>Jukka Ruohonen, Eun-Young Kang, Qusai Ramadan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13641">https://arxiv.org/abs/2505.13641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13641">https://arxiv.org/pdf/2505.13641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13641]] An Alignment Between the CRA's Essential Requirements and the ATT&CK's Mitigations(https://arxiv.org/abs/2505.13641)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The paper presents an alignment evaluation between the mitigations present in the MITRE's ATT&CK framework and the essential cyber security requirements of the recently introduced Cyber Resilience Act (CRA) in the European Union. In overall, the two align well with each other. With respect to the CRA, there are notable gaps only in terms of data minimization, data erasure, and vulnerability coordination. In terms of the ATT&CK framework, gaps are present only in terms of threat intelligence, training, out-of-band communication channels, and residual risks. The evaluation presented contributes to narrowing of a common disparity between law and technical frameworks.</li>
</ul>

<h3>Title: FedCTTA: A Collaborative Approach to Continual Test-Time Adaptation in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Rakibul Hasan Rajib, Md Akil Raihan Iftee, Mir Sazzat Hossain, A. K. M. Mahbubur Rahman, Sajib Mistry, M Ashraful Amin, Amin Ahsan Ali</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13643">https://arxiv.org/abs/2505.13643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13643">https://arxiv.org/pdf/2505.13643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13643]] FedCTTA: A Collaborative Approach to Continual Test-Time Adaptation in Federated Learning(https://arxiv.org/abs/2505.13643)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative model training across distributed clients without sharing raw data, making it ideal for privacy-sensitive applications. However, FL models often suffer performance degradation due to distribution shifts between training and deployment. Test-Time Adaptation (TTA) offers a promising solution by allowing models to adapt using only test samples. However, existing TTA methods in FL face challenges such as computational overhead, privacy risks from feature sharing, and scalability concerns due to memory constraints. To address these limitations, we propose Federated Continual Test-Time Adaptation (FedCTTA), a privacy-preserving and computationally efficient framework for federated adaptation. Unlike prior methods that rely on sharing local feature statistics, FedCTTA avoids direct feature exchange by leveraging similarity-aware aggregation based on model output distributions over randomly generated noise samples. This approach ensures adaptive knowledge sharing while preserving data privacy. Furthermore, FedCTTA minimizes the entropy at each client for continual adaptation, enhancing the model's confidence in evolving target distributions. Our method eliminates the need for server-side training during adaptation and maintains a constant memory footprint, making it scalable even as the number of clients or training rounds increases. Extensive experiments show that FedCTTA surpasses existing methods across diverse temporal and spatial heterogeneity scenarios.</li>
</ul>

<h3>Title: Self-Reinforced Graph Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Chou-Ying Hsieh, Chun-Fu Jang, Cheng-En Hsieh, Qian-Hui Chen, Sy-Yen Kuo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13650">https://arxiv.org/abs/2505.13650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13650">https://arxiv.org/pdf/2505.13650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13650]] Self-Reinforced Graph Contrastive Learning(https://arxiv.org/abs/2505.13650)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graphs serve as versatile data structures in numerous real-world domains-including social networks, molecular biology, and knowledge graphs-by capturing intricate relational information among entities. Among graph-based learning techniques, Graph Contrastive Learning (GCL) has gained significant attention for its ability to derive robust, self-supervised graph representations through the contrasting of positive and negative sample pairs. However, a critical challenge lies in ensuring high-quality positive pairs so that the intrinsic semantic and structural properties of the original graph are preserved rather than distorted. To address this issue, we propose SRGCL (Self-Reinforced Graph Contrastive Learning), a novel framework that leverages the model's own encoder to dynamically evaluate and select high-quality positive pairs. We designed a unified positive pair generator employing multiple augmentation strategies, and a selector guided by the manifold hypothesis to maintain the underlying geometry of the latent space. By adopting a probabilistic mechanism for selecting positive pairs, SRGCL iteratively refines its assessment of pair quality as the encoder's representational power improves. Extensive experiments on diverse graph-level classification tasks demonstrate that SRGCL, as a plug-in module, consistently outperforms state-of-the-art GCL methods, underscoring its adaptability and efficacy across various domains.</li>
</ul>

<h3>Title: Traceable Black-box Watermarks for Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Xu, Rui Hu, Olivera Kotevska, Zikai Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13651">https://arxiv.org/abs/2505.13651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13651">https://arxiv.org/pdf/2505.13651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13651]] Traceable Black-box Watermarks for Federated Learning(https://arxiv.org/abs/2505.13651)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, federate, watermark</a></li>
<li><strong>Abstract: </strong>Due to the distributed nature of Federated Learning (FL) systems, each local client has access to the global model, posing a critical risk of model leakage. Existing works have explored injecting watermarks into local models to enable intellectual property protection. However, these methods either focus on non-traceable watermarks or traceable but white-box watermarks. We identify a gap in the literature regarding the formal definition of traceable black-box watermarking and the formulation of the problem of injecting such watermarks into FL systems. In this work, we first formalize the problem of injecting traceable black-box watermarks into FL. Based on the problem, we propose a novel server-side watermarking method, $\mathbf{TraMark}$, which creates a traceable watermarked model for each client, enabling verification of model leakage in black-box settings. To achieve this, $\mathbf{TraMark}$ partitions the model parameter space into two distinct regions: the main task region and the watermarking region. Subsequently, a personalized global model is constructed for each client by aggregating only the main task region while preserving the watermarking region. Each model then learns a unique watermark exclusively within the watermarking region using a distinct watermark dataset before being sent back to the local client. Extensive results across various FL systems demonstrate that $\mathbf{TraMark}$ ensures the traceability of all watermarked models while preserving their main task performance.</li>
</ul>

<h3>Title: Optimal Client Sampling in Federated Learning with Client-Level Heterogeneous Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Xu, Rui Hu, Olivera Kotevska</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13655">https://arxiv.org/abs/2505.13655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13655">https://arxiv.org/pdf/2505.13655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13655]] Optimal Client Sampling in Federated Learning with Client-Level Heterogeneous Differential Privacy(https://arxiv.org/abs/2505.13655)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning with client-level differential privacy (DP) provides a promising framework for collaboratively training models while rigorously protecting clients' privacy. However, classic approaches like DP-FedAvg struggle when clients have heterogeneous privacy requirements, as they must uniformly enforce the strictest privacy level across clients, leading to excessive DP noise and significant model utility degradation. Existing methods to improve the model utility in such heterogeneous privacy settings often assume a trusted server and are largely heuristic, resulting in suboptimal performance and lacking strong theoretical underpinnings. In this work, we address these challenges under a practical attack model where both clients and the server are honest-but-curious. We propose GDPFed, which partitions clients into groups based on their privacy budgets and achieves client-level DP within each group to reduce the privacy budget waste and hence improve the model utility. Based on the privacy and convergence analysis of GDPFed, we find that the magnitude of DP noise depends on both model dimensionality and the per-group client sampling ratios. To further improve the performance of GDPFed, we introduce GDPFed$^+$, which integrates model sparsification to eliminate unnecessary noise and optimizes per-group client sampling ratios to minimize convergence error. Extensive empirical evaluations on multiple benchmark datasets demonstrate the effectiveness of GDPFed$^+$, showing substantial performance gains compared with state-of-the-art methods.</li>
</ul>

<h3>Title: A Systematic Review and Taxonomy for Privacy Breach Classification: Trends, Gaps, and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Clint Fuchs, John D. Hastings</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13694">https://arxiv.org/abs/2505.13694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13694">https://arxiv.org/pdf/2505.13694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13694]] A Systematic Review and Taxonomy for Privacy Breach Classification: Trends, Gaps, and Future Directions(https://arxiv.org/abs/2505.13694)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>In response to the rising frequency and complexity of data breaches and evolving global privacy regulations, this study presents a comprehensive examination of academic literature on the classification of privacy breaches and violations between 2010-2024. Through a systematic literature review, a corpus of screened studies was assembled and analyzed to identify primary research themes, emerging trends, and gaps in the field. A novel taxonomy is introduced to guide efforts by categorizing research efforts into seven domains: breach classification, report classification, breach detection, threat detection, breach prediction, risk analysis, and threat classification. An analysis reveals that breach classification and detection dominate the literature, while breach prediction and risk analysis have only recently emerged in the literature, suggesting opportunities for potential research impacts. Keyword and phrase frequency analysis reveal potentially underexplored areas, including location privacy, prediction models, and healthcare data breaches.</li>
</ul>

<h3>Title: RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Soumya Rani Samineni, Durgesh Kalwar, Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13697">https://arxiv.org/abs/2505.13697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13697">https://arxiv.org/pdf/2505.13697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13697]] RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs(https://arxiv.org/abs/2505.13697)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning-based post-training of large language models (LLMs) has recently gained attention, particularly following the release of DeepSeek R1, which applied GRPO for fine-tuning. Amid the growing hype around improved reasoning abilities attributed to RL post-training, we critically examine the formulation and assumptions underlying these methods. We start by highlighting the popular structural assumptions made in modeling LLM training as a Markov Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't quite need the RL/GRPO apparatus. The two critical structural assumptions include (1) making the MDP states be just a concatenation of the actions-with states becoming the context window and the actions becoming the tokens in LLMs and (2) splitting the reward of a state-action trajectory uniformly across the trajectory. Through a comprehensive analysis, we demonstrate that these simplifying assumptions make the approach effectively equivalent to an outcome-driven supervised learning. Our experiments on benchmarks including GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised fine-tuning, incorporating both positive and negative samples, achieves performance comparable to GRPO-based training. We will also argue that the structural assumptions indirectly incentivize the RL to generate longer sequences of intermediate tokens-which in turn feeds into the narrative of "RL generating longer thinking traces." While RL may well be a very useful technique for improving the reasoning abilities of LLMs, our analysis shows that the simplistic structural assumptions made in modeling the underlying MDP render the popular LLM RL frameworks and their interpretations questionable.</li>
</ul>

<h3>Title: Are Large Language Models Good at Detecting Propaganda?</h3>
<ul>
<li><strong>Authors: </strong>Julia Jose, Rachel Greenstadt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13706">https://arxiv.org/abs/2505.13706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13706">https://arxiv.org/pdf/2505.13706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13706]] Are Large Language Models Good at Detecting Propaganda?(https://arxiv.org/abs/2505.13706)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Propagandists use rhetorical devices that rely on logical fallacies and emotional appeals to advance their agendas. Recognizing these techniques is key to making informed decisions. Recent advances in Natural Language Processing (NLP) have enabled the development of systems capable of detecting manipulative content. In this study, we look at several Large Language Models and their performance in detecting propaganda techniques in news articles. We compare the performance of these LLMs with transformer-based models. We find that, while GPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude 3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally, we find that all three LLMs outperform a MultiGranularity Network (MGN) baseline in detecting instances of one out of six propaganda techniques (name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in detecting instances of appeal to fear and flag-waving.</li>
</ul>

<h3>Title: Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiayu Chen, Aravind Venugopal, Jeff Schneider</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13709">https://arxiv.org/abs/2505.13709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13709">https://arxiv.org/pdf/2505.13709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13709]] Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning(https://arxiv.org/abs/2505.13709)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Offline reinforcement learning (RL) offers a powerful paradigm for data-driven control. Compared to model-free approaches, offline model-based RL (MBRL) explicitly learns a world model from a static dataset and uses it as a surrogate simulator, improving data efficiency and enabling potential generalization beyond the dataset support. However, most existing offline MBRL methods follow a two-stage training procedure: first learning a world model by maximizing the likelihood of the observed transitions, then optimizing a policy to maximize its expected return under the learned model. This objective mismatch results in a world model that is not necessarily optimized for effective policy learning. Moreover, we observe that policies learned via offline MBRL often lack robustness during deployment, and small adversarial noise in the environment can lead to significant performance degradation. To address these, we propose a framework that dynamically adapts the world model alongside the policy under a unified learning objective aimed at improving robustness. At the core of our method is a maximin optimization problem, which we solve by innovatively utilizing Stackelberg learning dynamics. We provide theoretical analysis to support our design and introduce computationally efficient implementations. We benchmark our algorithm on twelve noisy D4RL MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its state-of-the-art performance.</li>
</ul>

<h3>Title: SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yu Guo, Dong Jin, Shenghao Ye, Shuangwu Chen, Jian Yang, Xiaobin Tan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13725">https://arxiv.org/abs/2505.13725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13725">https://arxiv.org/pdf/2505.13725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13725]] SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs(https://arxiv.org/abs/2505.13725)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language models (LLMs) have demonstrated significant potential in text-to-SQL reasoning tasks, yet a substantial performance gap persists between existing open-source models and their closed-source counterparts. In this paper, we introduce SQLForge, a novel approach for synthesizing reliable and diverse data to enhance text-to-SQL reasoning in LLMs. We improve data reliability through SQL syntax constraints and SQL-to-question reverse translation, ensuring data logic at both structural and semantic levels. We also propose an SQL template enrichment and iterative data domain exploration mechanism to boost data diversity. Building on the augmented data, we fine-tune a variety of open-source models with different architectures and parameter sizes, resulting in a family of models termed SQLForge-LM. SQLForge-LM achieves the state-of-the-art performance on the widely recognized Spider and BIRD benchmarks among the open-source models. Specifically, SQLForge-LM achieves EX accuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, significantly narrowing the performance gap with closed-source methods.</li>
</ul>

<h3>Title: Improving Compositional Generation with Diffusion Models Using Lift Scores</h3>
<ul>
<li><strong>Authors: </strong>Chenning Yu, Sicun Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13740">https://arxiv.org/abs/2505.13740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13740">https://arxiv.org/pdf/2505.13740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13740]] Improving Compositional Generation with Diffusion Models Using Lift Scores(https://arxiv.org/abs/2505.13740)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce a novel resampling criterion using lift scores, for improving compositional generation in diffusion models. By leveraging the lift scores, we evaluate whether generated samples align with each single condition and then compose the results to determine whether the composed prompt is satisfied. Our key insight is that lift scores can be efficiently approximated using only the original diffusion model, requiring no additional training or external modules. We develop an optimized variant that achieves relatively lower computational overhead during inference while maintaining effectiveness. Through extensive experiments, we demonstrate that lift scores significantly improved the condition alignment for compositional generation across 2D synthetic data, CLEVR position tasks, and text-to-image synthesis. Our code is available at this http URL.</li>
</ul>

<h3>Title: ReSW-VL: Representation Learning for Surgical Workflow Analysis Using Vision-Language Model</h3>
<ul>
<li><strong>Authors: </strong>Satoshi Kondo</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13746">https://arxiv.org/abs/2505.13746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13746">https://arxiv.org/pdf/2505.13746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13746]] ReSW-VL: Representation Learning for Surgical Workflow Analysis Using Vision-Language Model(https://arxiv.org/abs/2505.13746)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Surgical phase recognition from video is a technology that automatically classifies the progress of a surgical procedure and has a wide range of potential applications, including real-time surgical support, optimization of medical resources, training and skill assessment, and safety improvement. Recent advances in surgical phase recognition technology have focused primarily on Transform-based methods, although methods that extract spatial features from individual frames using a CNN and video features from the resulting time series of spatial features using time series modeling have shown high performance. However, there remains a paucity of research on training methods for CNNs employed for feature extraction or representation learning in surgical phase recognition. In this study, we propose a method for representation learning in surgical workflow analysis using a vision-language model (ReSW-VL). Our proposed method involves fine-tuning the image encoder of a CLIP (Convolutional Language Image Model) vision-language model using prompt learning for surgical phase recognition. The experimental results on three surgical phase recognition datasets demonstrate the effectiveness of the proposed method in comparison to conventional methods.</li>
</ul>

<h3>Title: BeamClean: Language Aware Embedding Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Kaan Kale, Kyle Mylonakis, Jay Roberts, Sidhartha Roy</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13758">https://arxiv.org/abs/2505.13758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13758">https://arxiv.org/pdf/2505.13758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13758]] BeamClean: Language Aware Embedding Reconstruction(https://arxiv.org/abs/2505.13758)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>In this work, we consider an inversion attack on the obfuscated input embeddings sent to a language model on a server, where the adversary has no access to the language model or the obfuscation mechanism and sees only the obfuscated embeddings along with the model's embedding table. We propose BeamClean, an inversion attack that jointly estimates the noise parameters and decodes token sequences by integrating a language-model prior. Against Laplacian and Gaussian obfuscation mechanisms, BeamClean always surpasses naive distance-based attacks. This work highlights the necessity for and robustness of more advanced learned, input-dependent methods.</li>
</ul>

<h3>Title: Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Jacob Kleiman, Kevin Frank, Sindy Campagna</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13761">https://arxiv.org/abs/2505.13761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13761">https://arxiv.org/pdf/2505.13761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13761]] Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making(https://arxiv.org/abs/2505.13761)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Simulations, although powerful in accurately replicating real-world systems, often remain inaccessible to non-technical users due to their complexity. Conversely, large language models (LLMs) provide intuitive, language-based interactions but can lack the structured, causal understanding required to reliably model complex real-world dynamics. We introduce our simulation agent framework, a novel approach that integrates the strengths of both simulation models and LLMs. This framework helps empower users by leveraging the conversational capabilities of LLMs to interact seamlessly with sophisticated simulation systems, while simultaneously utilizing the simulations to ground the LLMs in accurate and structured representations of real-world phenomena. This integrated approach helps provide a robust and generalizable foundation for empirical validation and offers broad applicability across diverse domains.</li>
</ul>

<h3>Title: Krikri: Advancing Open Large Language Models for Greek</h3>
<ul>
<li><strong>Authors: </strong>Dimitris Roussis, Leon Voukoutis, Georgios Paraskevopoulos, Sokratis Sofianopoulos, Prokopis Prokopidis, Vassilis Papavasileiou, Athanasios Katsamanis, Stelios Piperidis, Vassilis Katsouros</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13772">https://arxiv.org/abs/2505.13772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13772">https://arxiv.org/pdf/2505.13772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13772]] Krikri: Advancing Open Large Language Models for Greek(https://arxiv.org/abs/2505.13772)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored for the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been extensively trained on high-quality Greek data to ensure superior adaptation to linguistic nuances. With 8 billion parameters, it offers advanced capabilities while maintaining efficient computational performance. Llama-Krikri-8B supports both Modern Greek and English, and is also equipped to handle polytonic text and Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage post-training pipeline, utilizing both human and synthetic instruction and preference data, by applying techniques such as MAGPIE. In addition, for evaluation, we propose three novel public benchmarks for Greek. Our evaluation on existing as well as the proposed benchmarks shows notable improvements over comparable Greek and multilingual LLMs in both natural language understanding and generation as well as code generation.</li>
</ul>

<h3>Title: Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens</h3>
<ul>
<li><strong>Authors: </strong>Kaya Stechly, Karthik Valmeekam, Atharva Gundawar, Vardhan Palod, Subbarao Kambhampati</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13775">https://arxiv.org/abs/2505.13775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13775">https://arxiv.org/pdf/2505.13775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13775]] Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens(https://arxiv.org/abs/2505.13775)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), and especially of the process of training on CoTs sampled from base LLMs in order to help find new reasoning patterns. In this paper, we critically examine that interpretation by investigating how the semantics of intermediate tokens-often anthropomorphized as "thoughts" or reasoning traces and which are claimed to display behaviors like backtracking, self-verification etc.-actually influence model performance. We train transformer models on formally verifiable reasoning traces and solutions, constraining both intermediate steps and final outputs to align with those of a formal solver (in our case, A* search). By constructing a formal interpreter of the semantics of our problems and intended algorithm, we systematically evaluate not only solution accuracy but also the correctness of intermediate traces, thus allowing us to evaluate whether the latter causally influences the former. We notice that, despite significant improvements on the solution-only baseline, models trained on entirely correct traces still produce invalid reasoning traces when arriving at correct solutions. To further show that trace accuracy is only loosely connected to solution accuracy, we then train models on noisy, corrupted traces which have no relation to the specific problem each is paired with, and find that not only does performance remain largely consistent with models trained on correct data, but in some cases can improve upon it and generalize more robustly on out-of-distribution tasks. These results challenge the assumption that intermediate tokens or "Chains of Thought" induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly correct forms) as evidence of human-like or algorithmic behaviors in language models.</li>
</ul>

<h3>Title: Transfer Learning from Visual Speech Recognition to Mouthing Recognition in German Sign Language</h3>
<ul>
<li><strong>Authors: </strong>Dinh Nam Pham, Eleftherios Avramidis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13784">https://arxiv.org/abs/2505.13784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13784">https://arxiv.org/pdf/2505.13784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13784]] Transfer Learning from Visual Speech Recognition to Mouthing Recognition in German Sign Language(https://arxiv.org/abs/2505.13784)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Sign Language Recognition (SLR) systems primarily focus on manual gestures, but non-manual features such as mouth movements, specifically mouthing, provide valuable linguistic information. This work directly classifies mouthing instances to their corresponding words in the spoken language while exploring the potential of transfer learning from Visual Speech Recognition (VSR) to mouthing recognition in German Sign Language. We leverage three VSR datasets: one in English, one in German with unrelated words and one in German containing the same target words as the mouthing dataset, to investigate the impact of task similarity in this setting. Our results demonstrate that multi-task learning improves both mouthing recognition and VSR accuracy as well as model robustness, suggesting that mouthing recognition should be treated as a distinct but related task to VSR. This research contributes to the field of SLR by proposing knowledge transfer from VSR to SLR datasets with limited mouthing annotations.</li>
</ul>

<h3>Title: Scalable Autoregressive 3D Molecule Generation</h3>
<ul>
<li><strong>Authors: </strong>Austin H. Cheng, Chong Sun, Alán Aspuru-Guzik</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13791">https://arxiv.org/abs/2505.13791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13791">https://arxiv.org/pdf/2505.13791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13791]] Scalable Autoregressive 3D Molecule Generation(https://arxiv.org/abs/2505.13791)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Generative models of 3D molecular structure play a rapidly growing role in the design and simulation of molecules. Diffusion models currently dominate the space of 3D molecule generation, while autoregressive models have trailed behind. In this work, we present Quetzal, a simple but scalable autoregressive model that builds molecules atom-by-atom in 3D. Treating each molecule as an ordered sequence of atoms, Quetzal combines a causal transformer that predicts the next atom's discrete type with a smaller Diffusion MLP that models the continuous next-position distribution. Compared to existing autoregressive baselines, Quetzal achieves substantial improvements in generation quality and is competitive with the performance of state-of-the-art diffusion models. In addition, by reducing the number of expensive forward passes through a dense transformer, Quetzal enables significantly faster generation speed, as well as exact divergence-based likelihood computation. Finally, without any architectural changes, Quetzal natively handles variable-size tasks like hydrogen decoration and scaffold completion. We hope that our work motivates a perspective on scalability and generality for generative modelling of 3D molecules.</li>
</ul>

<h3>Title: QUT-DV25: A Dataset for Dynamic Analysis of Next-Gen Software Supply Chain Attacks</h3>
<ul>
<li><strong>Authors: </strong>Sk Tanzir Mehedi, Raja Jurdak, Chadni Islam, Gowri Ramachandran</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13804">https://arxiv.org/abs/2505.13804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13804">https://arxiv.org/pdf/2505.13804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13804]] QUT-DV25: A Dataset for Dynamic Analysis of Next-Gen Software Supply Chain Attacks(https://arxiv.org/abs/2505.13804)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Securing software supply chains is a growing challenge due to the inadequacy of existing datasets in capturing the complexity of next-gen attacks, such as multiphase malware execution, remote access activation, and dynamic payload generation. Existing datasets, which rely on metadata inspection and static code analysis, are inadequate for detecting such attacks. This creates a critical gap because these datasets do not capture what happens during and after a package is installed. To address this gap, we present QUT-DV25, a dynamic analysis dataset specifically designed to support and advance research on detecting and mitigating supply chain attacks within the Python Package Index (PyPI) ecosystem. This dataset captures install and post-install-time traces from 14,271 Python packages, of which 7,127 are malicious. The packages are executed in an isolated sandbox environment using an extended Berkeley Packet Filter (eBPF) kernel and user-level probes. It captures 36 real-time features, that includes system calls, network traffic, resource usages, directory access patterns, dependency logs, and installation behaviors, enabling the study of next-gen attack vectors. ML analysis using the QUT-DV25 dataset identified four malicious PyPI packages previously labeled as benign, each with thousands of downloads. These packages deployed covert remote access and multi-phase payloads, were reported to PyPI maintainers, and subsequently removed. This highlights the practical value of QUT-DV25, as it outperforms reactive, metadata, and static datasets, offering a robust foundation for developing and benchmarking advanced threat detection within the evolving software supply chain ecosystem.</li>
</ul>

<h3>Title: Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhongyu Chen, Rong Zhao, Xie Han, Xindong Guo, Song Wang, Zherui Qiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13812">https://arxiv.org/abs/2505.13812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13812">https://arxiv.org/pdf/2505.13812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13812]] Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud Representation Learning(https://arxiv.org/abs/2505.13812)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Existing point cloud representation learning tend to learning the geometric distribution of objects through data-driven approaches, emphasizing structural features while overlooking the relationship between the local information and the whole structure. Local features reflect the fine-grained variations of an object, while the whole structure is determined by the interaction and combination of these local features, collectively defining the object's shape. In real-world, objects undergo elastic deformation under external forces, and this deformation gradually affects the whole structure through the propagation of forces from local regions, thereby altering the object's geometric properties. Inspired by this, we propose a physics-driven self-supervised learning method for point cloud representation, which captures the relationship between parts and the whole by constructing a local-whole force propagation mechanism. Specifically, we employ a dual-task encoder-decoder framework, integrating the geometric modeling capability of implicit fields with physics-driven elastic deformation. The encoder extracts features from the point cloud and its tetrahedral mesh representation, capturing both geometric and physical properties. These features are then fed into two decoders: one learns the whole geometric shape of the point cloud through an implicit field, while the other predicts local deformations using two specifically designed physics information loss functions, modeling the deformation relationship between local and whole shapes. Experimental results show that our method outperforms existing approaches in object classification, few-shot learning, and segmentation, demonstrating its effectiveness.</li>
</ul>

<h3>Title: FlashKAT: Understanding and Addressing Performance Bottlenecks in the Kolmogorov-Arnold Transformer</h3>
<ul>
<li><strong>Authors: </strong>Matthew Raffel, Lizhong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13813">https://arxiv.org/abs/2505.13813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13813">https://arxiv.org/pdf/2505.13813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13813]] FlashKAT: Understanding and Addressing Performance Bottlenecks in the Kolmogorov-Arnold Transformer(https://arxiv.org/abs/2505.13813)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>The Kolmogorov-Arnold Network (KAN) has been gaining popularity as an alternative to the multi-layer perceptron (MLP) with its increased expressiveness and interpretability. However, the KAN can be orders of magnitude slower due to its increased computational cost and training instability, limiting its applicability to larger-scale tasks. Recently, the Kolmogorov-Arnold Transformer (KAT) has been proposed, which can achieve FLOPs similar to the traditional Transformer with MLPs by leveraging Group-Rational KAN (GR-KAN). Unfortunately, despite the comparable FLOPs, our characterizations reveal that the KAT is still 123x slower in training speeds, indicating that there are other performance bottlenecks beyond FLOPs. In this paper, we conduct a series of experiments to understand the root cause of the slowdown in KAT. We uncover that the slowdown can be isolated to memory stalls and, more specifically, in the backward pass of GR-KAN caused by inefficient gradient accumulation. To address this memory bottleneck, we propose FlashKAT, which builds on our restructured kernel that minimizes gradient accumulation with atomic adds and accesses to slow memory. Evaluations demonstrate that FlashKAT can achieve a training speedup of 86.5x compared with the state-of-the-art KAT, while reducing rounding errors in the coefficient gradients. Our code is available at this https URL.</li>
</ul>

<h3>Title: InstanceBEV: Unifying Instance and BEV Representation for Global Modeling</h3>
<ul>
<li><strong>Authors: </strong>Feng Li, Kun Xu, Zhaoyue Wang, Yunduan Cui, Mohammad Masum Billah, Jia Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13817">https://arxiv.org/abs/2505.13817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13817">https://arxiv.org/pdf/2505.13817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13817]] InstanceBEV: Unifying Instance and BEV Representation for Global Modeling(https://arxiv.org/abs/2505.13817)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Occupancy Grid Maps are widely used in navigation for their ability to represent 3D space occupancy. However, existing methods that utilize multi-view cameras to construct Occupancy Networks for perception modeling suffer from cubic growth in data complexity. Adopting a Bird's-Eye View (BEV) perspective offers a more practical solution for autonomous driving, as it provides higher semantic density and mitigates complex object occlusions. Nonetheless, BEV-based approaches still require extensive engineering optimizations to enable efficient large-scale global modeling. To address this challenge, we propose InstanceBEV, the first method to introduce instance-level dimensionality reduction for BEV, enabling global modeling with transformers without relying on sparsification or acceleration operators. Different from other BEV methods, our approach directly employs transformers to aggregate global features. Compared to 3D object detection models, our method samples global feature maps into 3D space. Experiments on OpenOcc-NuScenes dataset show that InstanceBEV achieves state-of-the-art performance while maintaining a simple, efficient framework without requiring additional optimizations.</li>
</ul>

<h3>Title: Fragments to Facts: Partial-Information Fragment Inference from LLMs</h3>
<ul>
<li><strong>Authors: </strong>Lucas Rosenblatt, Bin Han, Robert Wolfe, Bill Howe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13819">https://arxiv.org/abs/2505.13819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13819">https://arxiv.org/pdf/2505.13819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13819]] Fragments to Facts: Partial-Information Fragment Inference from LLMs(https://arxiv.org/abs/2505.13819)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, extraction, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can leak sensitive training data through memorization and membership inference attacks. Prior work has primarily focused on strong adversarial assumptions, including attacker access to entire samples or long, ordered prefixes, leaving open the question of how vulnerable LLMs are when adversaries have only partial, unordered sample information. For example, if an attacker knows a patient has "hypertension," under what conditions can they query a model fine-tuned on patient data to learn the patient also has "osteoarthritis?" In this paper, we introduce a more general threat model under this weaker assumption and show that fine-tuned LLMs are susceptible to these fragment-specific extraction attacks. To systematically investigate these attacks, we propose two data-blind methods: (1) a likelihood ratio attack inspired by methods from membership inference, and (2) a novel approach, PRISM, which regularizes the ratio by leveraging an external prior. Using examples from both medical and legal settings, we show that both methods are competitive with a data-aware baseline classifier that assumes access to labeled in-distribution data, underscoring their robustness.</li>
</ul>

<h3>Title: Structured Agent Distillation for Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Jun Liu, Zhenglun Kong, Peiyan Dong, Changdi Yang, Tianqi Li, Hao Tang, Geng Yuan, Wei Niu, Wenbin Zhang, Pu Zhao, Xue Lin, Dong Huang, Yanzhi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13820">https://arxiv.org/abs/2505.13820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13820">https://arxiv.org/pdf/2505.13820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13820]] Structured Agent Distillation for Large Language Model(https://arxiv.org/abs/2505.13820)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit strong capabilities as decision-making agents by interleaving reasoning and actions, as seen in ReAct-style frameworks. Yet, their practical deployment is constrained by high inference costs and large model sizes. We propose Structured Agent Distillation, a framework that compresses large LLM-based agents into smaller student models while preserving both reasoning fidelity and action consistency. Unlike standard token-level distillation, our method segments trajectories into {[REASON]} and {[ACT]} spans, applying segment-specific losses to align each component with the teacher's behavior. This structure-aware supervision enables compact agents to better replicate the teacher's decision process. Experiments on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently outperforms token-level and imitation learning baselines, achieving significant compression with minimal performance drop. Scaling and ablation results further highlight the importance of span-level alignment for efficient and deployable agents.</li>
</ul>

<h3>Title: EfficientLLM: Efficiency in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhengqing Yuan, Weixiang Sun, Yixin Liu, Huichi Zhou, Rong Zhou, Yiyang Li, Zheyuan Zhang, Wei Song, Yue Huang, Haolong Jia, Keerthiram Murugesan, Yu Wang, Lifang He, Jianfeng Gao, Lichao Sun, Yanfang Ye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13840">https://arxiv.org/abs/2505.13840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13840">https://arxiv.org/pdf/2505.13840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13840]] EfficientLLM: Efficiency in Large Language Models(https://arxiv.org/abs/2505.13840)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have driven significant progress, yet their growing parameter counts and context windows incur prohibitive compute, energy, and monetary costs. We introduce EfficientLLM, a novel benchmark and the first comprehensive empirical study evaluating efficiency techniques for LLMs at scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our study systematically explores three key axes: (1) architecture pretraining (efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts (MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and (3) inference (quantization methods: int4, float16). We define six fine-grained metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy Consumption, Compression Rate) to capture hardware saturation, latency-throughput balance, and carbon cost. Evaluating over 100 model-technique pairs (0.5B-72B parameters), we derive three core insights: (i) Efficiency involves quantifiable trade-offs: no single method is universally optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by 40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5% accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal memory-latency trade-offs for constrained devices, MLA achieves lowest perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency only beyond 14B parameters. (iii) Techniques generalize across modalities: we extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM provides essential guidance for researchers and engineers navigating the efficiency-performance landscape of next-generation foundation models.</li>
</ul>

<h3>Title: Provable Execution in Real-Time Embedded Systems</h3>
<ul>
<li><strong>Authors: </strong>Antonio Joia Neto, Norrathep Rattanavipanon, Ivan De Oliveira Nunes</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13842">https://arxiv.org/abs/2505.13842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13842">https://arxiv.org/pdf/2505.13842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13842]] Provable Execution in Real-Time Embedded Systems(https://arxiv.org/abs/2505.13842)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Embedded devices are increasingly ubiquitous and vital, often supporting safety-critical functions. However, due to strict cost and energy constraints, they are typically implemented with Micro-Controller Units (MCUs) that lack advanced architectural security features. Within this space, recent efforts have created low-cost architectures capable of generating Proofs of Execution (PoX) of software on potentially compromised MCUs. This capability can ensure the integrity of sensor data from the outset, by binding sensed results to an unforgeable cryptographic proof of execution on edge sensor MCUs. However, the security of existing PoX requires the proven execution to occur atomically. This requirement precludes the application of PoX to (1) time-shared systems, and (2) applications with real-time constraints, creating a direct conflict between execution integrity and the real-time availability needs of several embedded system uses. In this paper, we formulate a new security goal called Real-Time Proof of Execution (RT-PoX) that retains the integrity guarantees of classic PoX while enabling its application to existing real-time systems. This is achieved by relaxing the atomicity requirement of PoX while dispatching interference attempts from other potentially malicious tasks (or compromised operating systems) executing on the same device. To realize the RT-PoX goal, we develop Provable Execution Architecture for Real-Time Systems (PEARTS). To the best of our knowledge, PEARTS is the first PoX system that can be directly deployed alongside a commodity embedded real-time operating system (FreeRTOS). This enables both real-time scheduling and execution integrity guarantees on commodity MCUs. To showcase this capability, we develop a PEARTS open-source prototype atop FreeRTOS on a single-core ARM Cortex-M33 processor. We evaluate and report on PEARTS security and (modest) overheads.</li>
</ul>

<h3>Title: Improve Language Model and Brain Alignment via Associative Memory</h3>
<ul>
<li><strong>Authors: </strong>Congchi Yin, Yongpeng Zhang, Xuyun Wen, Piji Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13844">https://arxiv.org/abs/2505.13844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13844">https://arxiv.org/pdf/2505.13844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13844]] Improve Language Model and Brain Alignment via Associative Memory(https://arxiv.org/abs/2505.13844)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Associative memory engages in the integration of relevant information for comprehension in the human cognition system. In this work, we seek to improve alignment between language models and human brain while processing speech information by integrating associative memory. After verifying the alignment between language model and brain by mapping language model activations to brain activity, the original text stimuli expanded with simulated associative memory are regarded as input to computational language models. We find the alignment between language model and brain is improved in brain regions closely related to associative memory processing. We also demonstrate large language models after specific supervised fine-tuning better align with brain response, by building the \textit{Association} dataset containing 1000 samples of stories, with instructions encouraging associative memory as input and associated content as output.</li>
</ul>

<h3>Title: Rethink the Role of Deep Learning towards Large-scale Quantum Systems</h3>
<ul>
<li><strong>Authors: </strong>Yusheng Zhao, Chi Zhang, Yuxuan Du</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13852">https://arxiv.org/abs/2505.13852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13852">https://arxiv.org/pdf/2505.13852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13852]] Rethink the Role of Deep Learning towards Large-scale Quantum Systems(https://arxiv.org/abs/2505.13852)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Characterizing the ground state properties of quantum systems is fundamental to capturing their behavior but computationally challenging. Recent advances in AI have introduced novel approaches, with diverse machine learning (ML) and deep learning (DL) models proposed for this purpose. However, the necessity and specific role of DL models in these tasks remain unclear, as prior studies often employ varied or impractical quantum resources to construct datasets, resulting in unfair comparisons. To address this, we systematically benchmark DL models against traditional ML approaches across three families of Hamiltonian, scaling up to 127 qubits in three crucial ground-state learning tasks while enforcing equivalent quantum resource usage. Our results reveal that ML models often achieve performance comparable to or even exceeding that of DL approaches across all tasks. Furthermore, a randomization test demonstrates that measurement input features have minimal impact on DL models' prediction performance. These findings challenge the necessity of current DL models in many quantum system learning scenarios and provide valuable insights into their effective utilization.</li>
</ul>

<h3>Title: Domain Gating Ensemble Networks for AI-Generated Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Arihant Tripathi, Liam Dugan, Charis Gao, Maggie Huan, Emma Jin, Peter Zhang, David Zhang, Julia Zhao, Chris Callison-Burch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13855">https://arxiv.org/abs/2505.13855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13855">https://arxiv.org/pdf/2505.13855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13855]] Domain Gating Ensemble Networks for AI-Generated Text Detection(https://arxiv.org/abs/2505.13855)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>As state-of-the-art language models continue to improve, the need for robust detection of machine-generated text becomes increasingly critical. However, current state-of-the-art machine text detectors struggle to adapt to new unseen domains and generative models. In this paper we present DoGEN (Domain Gating Ensemble Networks), a technique that allows detectors to adapt to unseen domains by ensembling a set of domain expert detector models using weights from a domain classifier. We test DoGEN on a wide variety of domains from leading benchmarks and find that it achieves state-of-the-art performance on in-domain detection while outperforming models twice its size on out-of-domain detection. We release our code and trained models to assist in future research in domain-adaptive AI detection.</li>
</ul>

<h3>Title: Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer</h3>
<ul>
<li><strong>Authors: </strong>Tian Sun, Yuqi Chen, Baihua Zheng, Weiwei Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13857">https://arxiv.org/abs/2505.13857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13857">https://arxiv.org/pdf/2505.13857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13857]] Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer(https://arxiv.org/abs/2505.13857)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In real-world applications, GPS trajectories often suffer from low sampling rates, with large and irregular intervals between consecutive GPS points. This sparse characteristic presents challenges for their direct use in GPS-based systems. This paper addresses the task of map-constrained trajectory recovery, aiming to enhance trajectory sampling rates of GPS trajectories. Previous studies commonly adopt a sequence-to-sequence framework, where an encoder captures the trajectory patterns and a decoder reconstructs the target trajectory. Within this framework, effectively representing the road network and extracting relevant trajectory features are crucial for overall performance. Despite advancements in these models, they fail to fully leverage the complex spatio-temporal dynamics present in both the trajectory and the road network. To overcome these limitations, we categorize the spatio-temporal dynamics of trajectory data into two distinct aspects: spatial-temporal traffic dynamics and trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for trajectory recovery. To capture spatio-temporal traffic dynamics, we introduce PD-GNN, which models periodic patterns and learns topologically aware dynamics concurrently for each road segment. For spatio-temporal trajectory dynamics, we present TedFormer, a time-aware Transformer that incorporates temporal dynamics for each GPS location by integrating closed-form neural ordinary differential equations into the attention mechanism. This allows TedFormer to effectively handle irregularly sampled data. Extensive experiments on three real-world datasets demonstrate the superior performance of TedTrajRec. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules</h3>
<ul>
<li><strong>Authors: </strong>Gonzalo E. Constante-Flores, Hao Chen, Can Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13858">https://arxiv.org/abs/2505.13858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13858">https://arxiv.org/pdf/2505.13858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13858]] Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules(https://arxiv.org/abs/2505.13858)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Deep learning models are increasingly deployed in safety-critical tasks where predictions must satisfy hard constraints, such as physical laws, fairness requirements, or safety limits. However, standard architectures lack built-in mechanisms to enforce such constraints, and existing approaches based on regularization or projection are often limited to simple constraints, computationally expensive, or lack feasibility guarantees. This paper proposes a model-agnostic framework for enforcing input-dependent linear equality and inequality constraints on neural network outputs. The architecture combines a task network trained for prediction accuracy with a safe network trained using decision rules from the stochastic and robust optimization literature to ensure feasibility across the entire input space. The final prediction is a convex combination of the two subnetworks, guaranteeing constraint satisfaction during both training and inference without iterative procedures or runtime optimization. We prove that the architecture is a universal approximator of constrained functions and derive computationally tractable formulations based on linear decision rules. Empirical results on benchmark regression tasks show that our method consistently satisfies constraints while maintaining competitive accuracy and low inference latency.</li>
</ul>

<h3>Title: hChain 4.0: A Secure and Scalable Permissioned Blockchain for EHR Management in Smart Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Musharraf N. Alruwaill, Saraju P. Mohanty, Elias Kougianos</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13861">https://arxiv.org/abs/2505.13861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13861">https://arxiv.org/pdf/2505.13861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13861]] hChain 4.0: A Secure and Scalable Permissioned Blockchain for EHR Management in Smart Healthcare(https://arxiv.org/abs/2505.13861)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, robust</a></li>
<li><strong>Abstract: </strong>The growing utilization of Internet of Medical Things (IoMT) devices, including smartwatches and wearable medical devices, has facilitated real-time health monitoring and data analysis to enhance healthcare outcomes. These gadgets necessitate improved security measures to safeguard sensitive health data while tackling scalability issues in real-time settings. The proposed system, hChain 4.0, employs a permissioned blockchain to provide a secure and scalable data infrastructure designed to fulfill these needs. This stands in contrast to conventional systems, which are vulnerable to security flaws or rely on public blockchains, constrained by scalability and expense. The proposed approach introduces a high-privacy method in which health data are encrypted using the Advanced Encryption Standard (AES) for time-efficient encryption, combined with Partial Homomorphic Encryption (PHE) to enable secure computations on encrypted data, thereby enhancing privacy. Moreover, it utilizes private channels that enable isolated communication and ledger between stakeholders, ensuring robust privacy while supporting collaborative operations. The proposed framework enables anonymized health data sharing for medical research by pseudonymizing patient identity. Additionally, hChain 4.0 incorporates Attribute-Based Access Control (ABAC) to provide secure electronic health record (EHR) sharing among authorized parties, where ABAC ensures fine-grained permission management vital for multi-organizational healthcare settings. Experimental assessments indicate that the proposed approach achieves higher scalability, cost-effectiveness, and validated security.</li>
</ul>

<h3>Title: PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks</h3>
<ul>
<li><strong>Authors: </strong>Guobin Shen, Dongcheng Zhao, Linghao Feng, Xiang He, Jihang Wang, Sicheng Shen, Haibo Tong, Yiting Dong, Jindong Li, Xiang Zheng, Yi Zeng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13862">https://arxiv.org/abs/2505.13862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13862">https://arxiv.org/pdf/2505.13862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13862]] PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks(https://arxiv.org/abs/2505.13862)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial prompts known as jailbreaks, which can bypass safety alignment and elicit harmful outputs. Despite growing efforts in LLM safety research, existing evaluations are often fragmented, focused on isolated attack or defense techniques, and lack systematic, reproducible analysis. In this work, we introduce PandaGuard, a unified and modular framework that models LLM jailbreak safety as a multi-agent system comprising attackers, defenders, and judges. Our framework implements 19 attack methods and 12 defense mechanisms, along with multiple judgment strategies, all within a flexible plugin architecture supporting diverse LLM interfaces, multiple interaction modes, and configuration-driven experimentation that enhances reproducibility and practical deployment. Built on this framework, we develop PandaBench, a comprehensive benchmark that evaluates the interactions between these attack/defense methods across 49 LLMs and various judgment approaches, requiring over 3 billion tokens to execute. Our extensive evaluation reveals key insights into model vulnerabilities, defense cost-performance trade-offs, and judge consistency. We find that no single defense is optimal across all dimensions and that judge disagreement introduces nontrivial variance in safety assessments. We release the code, configurations, and evaluation results to support transparent and reproducible research in LLM safety.</li>
</ul>

<h3>Title: Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model</h3>
<ul>
<li><strong>Authors: </strong>Peisong Niu, Ziqing Ma, Tian Zhou, Weiqi Chen, Lefei Shen, Rong Jin, Liang Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13873">https://arxiv.org/abs/2505.13873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13873">https://arxiv.org/pdf/2505.13873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13873]] Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model(https://arxiv.org/abs/2505.13873)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Weather forecasting has long posed a significant challenge for humanity. While recent AI-based models have surpassed traditional numerical weather prediction (NWP) methods in global forecasting tasks, overfitting remains a critical issue due to the limited availability of real-world weather data spanning only a few decades. Unlike fields like computer vision or natural language processing, where data abundance can mitigate overfitting, weather forecasting demands innovative strategies to address this challenge with existing data. In this paper, we explore pre-training methods for weather forecasting, finding that selecting an appropriately challenging pre-training task introduces locality bias, effectively mitigating overfitting and enhancing performance. We introduce Baguan, a novel data-driven model for medium-range weather forecasting, built on a Siamese Autoencoder pre-trained in a self-supervised manner and fine-tuned for different lead times. Experimental results show that Baguan outperforms traditional methods, delivering more accurate forecasts. Additionally, the pre-trained Baguan demonstrates robust overfitting control and excels in downstream tasks, such as subseasonal-to-seasonal (S2S) modeling and regional forecasting, after fine-tuning.</li>
</ul>

<h3>Title: InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yanggan Gu, Zhaoyi Yan, Yuanyi Wang, Yiming Zhang, Qi Zhou, Fei Wu, Hongxia Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13878">https://arxiv.org/abs/2505.13878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13878">https://arxiv.org/pdf/2505.13878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13878]] InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models(https://arxiv.org/abs/2505.13878)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Model fusion combines multiple Large Language Models (LLMs) with different strengths into a more powerful, integrated model through lightweight training methods. Existing works on model fusion focus primarily on supervised fine-tuning (SFT), leaving preference alignment (PA) --a critical phase for enhancing LLM performance--largely unexplored. The current few fusion methods on PA phase, like WRPO, simplify the process by utilizing only response outputs from source models while discarding their probability information. To address this limitation, we propose InfiFPO, a preference optimization method for implicit model fusion. InfiFPO replaces the reference model in Direct Preference Optimization (DPO) with a fused source model that synthesizes multi-source probabilities at the sequence level, circumventing complex vocabulary alignment challenges in previous works and meanwhile maintaining the probability information. By introducing probability clipping and max-margin fusion strategies, InfiFPO enables the pivot model to align with human preferences while effectively distilling knowledge from source models. Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO consistently outperforms existing model fusion and preference optimization methods. When using Phi-4 as the pivot model, InfiFPO improve its average performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its capabilities in mathematics, coding, and reasoning tasks.</li>
</ul>

<h3>Title: Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Changhao Jiang, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13886">https://arxiv.org/abs/2505.13886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13886">https://arxiv.org/pdf/2505.13886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13886]] Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning(https://arxiv.org/abs/2505.13886)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Visual-language Chain-of-Thought (CoT) data resources are relatively scarce compared to text-only counterparts, limiting the improvement of reasoning capabilities in Vision Language Models (VLMs). However, high-quality vision-language reasoning data is expensive and labor-intensive to annotate. To address this issue, we leverage a promising resource: game code, which naturally contains logical structures and state transition processes. Therefore, we propose Code2Logic, a novel game-code-driven approach for multimodal reasoning data synthesis. Our approach leverages Large Language Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning processes and results through code execution. Using the Code2Logic approach, we developed the GameQA dataset to train and evaluate VLMs. GameQA is cost-effective and scalable to produce, challenging for state-of-the-art models, and diverse with 30 games and 158 tasks. Surprisingly, despite training solely on game data, VLMs demonstrated out of domain generalization, specifically Qwen2.5-VL-7B improving performance by 2.33\% across 7 diverse vision-language benchmarks. Our code and dataset are available at this https URL.</li>
</ul>

<h3>Title: Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM</h3>
<ul>
<li><strong>Authors: </strong>Zhen Xiong, Yujun Cai, Zhecheng Li, Yiwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13890">https://arxiv.org/abs/2505.13890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13890">https://arxiv.org/pdf/2505.13890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13890]] Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM(https://arxiv.org/abs/2505.13890)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in test-time scaling have enabled Large Language Models (LLMs) to display sophisticated reasoning abilities via extended Chain-of-Thought (CoT) generation. Despite their potential, these Reasoning LLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as performance degradation under few-shot prompting, that challenge our current understanding of RLMs. In this work, we introduce a unified graph-based analytical framework for better modeling the reasoning processes of RLMs. Our method first clusters long, verbose CoT outputs into semantically coherent reasoning steps, then constructs directed reasoning graphs to capture contextual and logical dependencies among these steps. Through comprehensive analysis across models and prompting regimes, we reveal that structural properties, such as exploration density, branching, and convergence ratios, strongly correlate with reasoning accuracy. Our findings demonstrate how prompting strategies substantially reshape the internal reasoning structure of RLMs, directly affecting task outcomes. The proposed framework not only enables quantitative evaluation of reasoning quality beyond conventional metrics but also provides practical insights for prompt engineering and the cognitive analysis of LLMs. Code and resources will be released to facilitate future research in this direction.</li>
</ul>

<h3>Title: InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion</h3>
<ul>
<li><strong>Authors: </strong>Yuanyi Wang, Zhaoyi Yan, Yiming Zhang, Qi Zhou, Yanggan Gu, Fei Wu, Hongxia Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13893">https://arxiv.org/abs/2505.13893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13893">https://arxiv.org/pdf/2505.13893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13893]] InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion(https://arxiv.org/abs/2505.13893)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have intensified efforts to fuse heterogeneous open-source models into a unified system that inherits their complementary strengths. Existing logit-based fusion methods maintain inference efficiency but treat vocabulary dimensions independently, overlooking semantic dependencies encoded by cross-dimension interactions. These dependencies reflect how token types interact under a model's internal reasoning and are essential for aligning models with diverse generation behaviors. To explicitly model these dependencies, we propose \textbf{InfiGFusion}, the first structure-aware fusion framework with a novel \textit{Graph-on-Logits Distillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output and aggregate their outer products across sequence positions to form a global co-activation graph, where nodes represent vocabulary channels and edges quantify their joint activations. To ensure scalability and efficiency, we design a sorting-based closed-form approximation that reduces the original $O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \log n)$, with provable approximation guarantees. Experiments across multiple fusion settings show that GLD consistently improves fusion quality and stability. InfiGFusion outperforms SOTA models and fusion baselines across 11 benchmarks spanning reasoning, coding, and mathematics. It shows particular strength in complex reasoning tasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal Judgement over SFT, demonstrating superior multi-step and relational inference.</li>
</ul>

<h3>Title: VulCPE: Context-Aware Cybersecurity Vulnerability Retrieval and Management</h3>
<ul>
<li><strong>Authors: </strong>Yuning Jiang, Feiyang Shang, Freedy Tan Wei You, Huilin Wang, Chia Ren Cong, Qiaoran Meng, Nay Oo, Hoon Wei Lim, Biplab Sikdar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DB, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13895">https://arxiv.org/abs/2505.13895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13895">https://arxiv.org/pdf/2505.13895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13895]] VulCPE: Context-Aware Cybersecurity Vulnerability Retrieval and Management(https://arxiv.org/abs/2505.13895)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction</a></li>
<li><strong>Abstract: </strong>The dynamic landscape of cybersecurity demands precise and scalable solutions for vulnerability management in heterogeneous systems, where configuration-specific vulnerabilities are often misidentified due to inconsistent data in databases like the National Vulnerability Database (NVD). Inaccurate Common Platform Enumeration (CPE) data in NVD further leads to false positives and incomplete vulnerability retrieval. Informed by our systematic analysis of CPE and CVEdeails data, revealing more than 50% vendor name inconsistencies, we propose VulCPE, a framework that standardizes data and models configuration dependencies using a unified CPE schema (uCPE), entity recognition, relation extraction, and graph-based modeling. VulCPE achieves superior retrieval precision (0.766) and coverage (0.926) over existing tools. VulCPE ensures precise, context-aware vulnerability management, enhancing cyber resilience.</li>
</ul>

<h3>Title: Do Language Models Use Their Depth Efficiently?</h3>
<ul>
<li><strong>Authors: </strong>Róbert Csordás, Christopher D. Manning, Christopher Potts</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13898">https://arxiv.org/abs/2505.13898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13898">https://arxiv.org/pdf/2505.13898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13898]] Do Language Models Use Their Depth Efficiently?(https://arxiv.org/abs/2505.13898)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Modern LLMs are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible in shallow models, or do they merely spread the same kinds of computation out over more layers? To address these questions, we analyze the residual stream of the Llama 3.1 and Qwen 3 family of models. We find: First, comparing the output of the sublayers to the residual stream reveals that layers in the second half contribute much less than those in the first half, with a clear phase transition between the two halves. Second, skipping layers in the second half has a much smaller effect on future computations and output predictions. Third, for multihop tasks, we are unable to find evidence that models are using increased depth to compose subresults in examples involving many hops. Fourth, we seek to directly address whether deeper models are using their additional layers to perform new kinds of computation. To do this, we train linear maps from the residual stream of a shallow model to a deeper one. We find that layers with the same relative depth map best to each other, suggesting that the larger model simply spreads the same computations out over its many layers. All this evidence suggests that deeper models are not using their depth to learn new kinds of computation, but only using the greater depth to perform more fine-grained adjustments to the residual. This may help explain why increasing scale leads to diminishing returns for stacked Transformer architectures.</li>
</ul>

<h3>Title: Exploring Causes of Representational Similarity in Machine Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Michael Li, Hung Anh Vu, Damilola Awofisayo, Emily Wenger</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13899">https://arxiv.org/abs/2505.13899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13899">https://arxiv.org/pdf/2505.13899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13899]] Exploring Causes of Representational Similarity in Machine Learning Models(https://arxiv.org/abs/2505.13899)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Numerous works have noted significant similarities in how machine learning models represent the world, even across modalities. Although much effort has been devoted to uncovering properties and metrics on which these models align, surprisingly little work has explored causes of this similarity. To advance this line of inquiry, this work explores how two possible causal factors -- dataset overlap and task overlap -- influence downstream model similarity. The exploration of dataset overlap is motivated by the reality that large-scale generative AI models are often trained on overlapping datasets of scraped internet data, while the exploration of task overlap seeks to substantiate claims from a recent work, the Platonic Representation Hypothesis, that task similarity may drive model similarity. We evaluate the effects of both factors through a broad set of experiments. We find that both positively correlate with higher representational similarity and that combining them provides the strongest effect. Our code and dataset are published.</li>
</ul>

<h3>Title: Let's Verify Math Questions Step by Step</h3>
<ul>
<li><strong>Authors: </strong>Chengyu Shen, Zhen Hao Wong, Runming He, Hao Liang, Meiyi Qiang, Zimo Meng, Zhengyang Zhao, Bohan Zeng, Zhengzhou Zhu, Bin Cui, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13903">https://arxiv.org/abs/2505.13903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13903">https://arxiv.org/pdf/2505.13903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13903]] Let's Verify Math Questions Step by Step(https://arxiv.org/abs/2505.13903)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently achieved remarkable progress in mathematical reasoning. To enable such capabilities, many existing works distill strong reasoning models into long chains of thought or design algorithms to construct high-quality math QA data for training. However, these efforts primarily focus on generating correct reasoning paths and answers, while largely overlooking the validity of the questions themselves. In this work, we propose Math Question Verification (MathQ-Verify), a novel five-stage pipeline designed to rigorously filter ill-posed or under-specified math problems. MathQ-Verify first performs format-level validation to remove redundant instructions and ensure that each question is syntactically well-formed. It then formalizes each question, decomposes it into atomic conditions, and verifies them against mathematical definitions. Next, it detects logical contradictions among these conditions, followed by a goal-oriented completeness check to ensure the question provides sufficient information for solving. To evaluate this task, we use existing benchmarks along with an additional dataset we construct, containing 2,147 math questions with diverse error types, each manually double-validated. Experiments show that MathQ-Verify achieves state-of-the-art performance across multiple benchmarks, improving the F1 score by up to 25 percentage points over the direct verification baseline. It further attains approximately 90% precision and 63% recall through a lightweight model voting scheme. MathQ-Verify offers a scalable and accurate solution for curating reliable mathematical datasets, reducing label noise and avoiding unnecessary computation on invalid questions. Our code and data are available at this https URL.</li>
</ul>

<h3>Title: 4D-ROLLS: 4D Radar Occupancy Learning via LiDAR Supervision</h3>
<ul>
<li><strong>Authors: </strong>Ruihan Liu, Xiaoyi Wu, Xijun Chen, Liang Hu, Yunjiang Lou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13905">https://arxiv.org/abs/2505.13905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13905">https://arxiv.org/pdf/2505.13905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13905]] 4D-ROLLS: 4D Radar Occupancy Learning via LiDAR Supervision(https://arxiv.org/abs/2505.13905)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>A comprehensive understanding of 3D scenes is essential for autonomous vehicles (AVs), and among various perception tasks, occupancy estimation plays a central role by providing a general representation of drivable and occupied space. However, most existing occupancy estimation methods rely on LiDAR or cameras, which perform poorly in degraded environments such as smoke, rain, snow, and fog. In this paper, we propose 4D-ROLLS, the first weakly supervised occupancy estimation method for 4D radar using the LiDAR point cloud as the supervisory signal. Specifically, we introduce a method for generating pseudo-LiDAR labels, including occupancy queries and LiDAR height maps, as multi-stage supervision to train the 4D radar occupancy estimation model. Then the model is aligned with the occupancy map produced by LiDAR, fine-tuning its accuracy in occupancy estimation. Extensive comparative experiments validate the exceptional performance of 4D-ROLLS. Its robustness in degraded environments and effectiveness in cross-dataset training are qualitatively demonstrated. The model is also seamlessly transferred to downstream tasks BEV segmentation and point cloud occupancy prediction, highlighting its potential for broader applications. The lightweight network enables 4D-ROLLS model to achieve fast inference speeds at about 30 Hz on a 4060 GPU. The code of 4D-ROLLS will be made available at this https URL.</li>
</ul>

<h3>Title: Cross-Domain Diffusion with Progressive Alignment for Efficient Adaptive Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Junyu Luo, Yusheng Zhao, Xiao Luo, Zhiping Xiao, Wei Ju, Li Shen, Dacheng Tao, Ming Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13907">https://arxiv.org/abs/2505.13907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13907">https://arxiv.org/pdf/2505.13907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13907]] Cross-Domain Diffusion with Progressive Alignment for Efficient Adaptive Retrieval(https://arxiv.org/abs/2505.13907)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Unsupervised efficient domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, while maintaining low storage cost and high retrieval efficiency. However, existing methods typically fail to address potential noise in the target domain, and directly align high-level features across domains, thus resulting in suboptimal retrieval performance. To address these challenges, we propose a novel Cross-Domain Diffusion with Progressive Alignment method (COUPLE). This approach revisits unsupervised efficient domain adaptive retrieval from a graph diffusion perspective, simulating cross-domain adaptation dynamics to achieve a stable target domain adaptation process. First, we construct a cross-domain relationship graph and leverage noise-robust graph flow diffusion to simulate the transfer dynamics from the source domain to the target domain, identifying lower noise clusters. We then leverage the graph diffusion results for discriminative hash code learning, effectively learning from the target domain while reducing the negative impact of noise. Furthermore, we employ a hierarchical Mixup operation for progressive domain alignment, which is performed along the cross-domain random walk paths. Utilizing target domain discriminative hash learning and progressive domain alignment, COUPLE enables effective domain adaptive hash learning. Extensive experiments demonstrate COUPLE's effectiveness on competitive benchmarks.</li>
</ul>

<h3>Title: ShortcutProbe: Probing Prediction Shortcuts for Learning Robust Models</h3>
<ul>
<li><strong>Authors: </strong>Guangtao Zheng, Wenqian Ye, Aidong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13910">https://arxiv.org/abs/2505.13910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13910">https://arxiv.org/pdf/2505.13910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13910]] ShortcutProbe: Probing Prediction Shortcuts for Learning Robust Models(https://arxiv.org/abs/2505.13910)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning models often achieve high performance by inadvertently learning spurious correlations between targets and non-essential features. For example, an image classifier may identify an object via its background that spuriously correlates with it. This prediction behavior, known as spurious bias, severely degrades model performance on data that lacks the learned spurious correlations. Existing methods on spurious bias mitigation typically require a variety of data groups with spurious correlation annotations called group labels. However, group labels require costly human annotations and often fail to capture subtle spurious biases such as relying on specific pixels for predictions. In this paper, we propose a novel post hoc spurious bias mitigation framework without requiring group labels. Our framework, termed ShortcutProbe, identifies prediction shortcuts that reflect potential non-robustness in predictions in a given model's latent space. The model is then retrained to be invariant to the identified prediction shortcuts for improved robustness. We theoretically analyze the effectiveness of the framework and empirically demonstrate that it is an efficient and practical tool for improving a model's robustness to spurious bias on diverse datasets.</li>
</ul>

<h3>Title: The Hidden Dangers of Outdated Software: A Cyber Security Perspective</h3>
<ul>
<li><strong>Authors: </strong>Gogulakrishnan Thiyagarajan, Vinay Bist, Prabhudarshi Nayak</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13922">https://arxiv.org/abs/2505.13922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13922">https://arxiv.org/pdf/2505.13922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13922]] The Hidden Dangers of Outdated Software: A Cyber Security Perspective(https://arxiv.org/abs/2505.13922)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Outdated software remains a potent and underappreciated menace in 2025's cybersecurity environment, exposing systems to a broad array of threats, including ransomware, data breaches, and operational outages that can have devastating and far-reaching impacts. This essay explores the unseen threats of cyberattacks by presenting robust statistical information, including the staggering reality that 32% of cyberattacks exploit unpatched software vulnerabilities, based on a 2025 TechTarget survey. Furthermore, it discusses real case studies, including the MOVEit breach in 2023 and the Log4Shell breach in 2021, both of which illustrate the catastrophic consequences of failing to perform software updates. The article offers a detailed analysis of the nature of software vulnerabilities, the underlying reasons for user resistance to patches, and organizational barriers that compound the issue. Furthermore, it suggests actionable solutions, including automation and awareness campaigns, to address these shortcomings. Apart from this, the paper also talks of trends such as AI-driven vulnerability patching and legal consequences of non-compliance under laws like HIPAA, thus providing a futuristic outlook on how such advancements may define future defenses. Supplemented by tables like one detailing trends in vulnerability and a graph illustrating technology adoption, this report showcases the pressing demand for anticipatory update strategies to safeguard digital ecosystems against the constantly evolving threats that characterize the modern cyber landscape. As it stands, it is a very useful document for practitioners, policymakers, and researchers.</li>
</ul>

<h3>Title: RLVR-World: Training World Models with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jialong Wu, Shaofeng Yin, Ningya Feng, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13934">https://arxiv.org/abs/2505.13934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13934">https://arxiv.org/pdf/2505.13934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13934]] RLVR-World: Training World Models with Reinforcement Learning(https://arxiv.org/abs/2505.13934)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like accuracy or perceptual quality. In this paper, we present RLVR-World, a unified framework that leverages reinforcement learning with verifiable rewards (RLVR) to directly optimize world models for such metrics. Despite formulating world modeling as autoregressive prediction of tokenized sequences, RLVR-World evaluates metrics of decoded predictions as verifiable rewards. We demonstrate substantial performance gains on both language- and video-based world models across domains, including text games, web navigation, and robot manipulation. Our work indicates that, beyond recent advances in reasoning language models, RLVR offers a promising post-training paradigm for enhancing the utility of generative models more broadly.</li>
</ul>

<h3>Title: EEG-to-Text Translation: A Model for Deciphering Human Brain Activity</h3>
<ul>
<li><strong>Authors: </strong>Saydul Akbar Murad, Ashim Dahal, Nick Rahimi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13936">https://arxiv.org/abs/2505.13936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13936">https://arxiv.org/pdf/2505.13936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13936]] EEG-to-Text Translation: A Model for Deciphering Human Brain Activity(https://arxiv.org/abs/2505.13936)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of large language models like Gemini, GPT, and others, bridging the gap between the human brain and language processing has become an important area of focus. To address this challenge, researchers have developed various models to decode EEG signals into text. However, these models still face significant performance limitations. To overcome these shortcomings, we propose a new model, R1 Translator, which aims to improve the performance of EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM encoder with a pretrained transformer-based decoder, utilizing EEG features to produce high-quality text outputs. The model processes EEG embeddings through the LSTM to capture sequential dependencies, which are then fed into the transformer decoder for effective text generation. The R1 Translator excels in ROUGE metrics, outperforming both T5 (previous research) and Brain Translator. Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9% higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and Brain by 3.6% (0.7553). Code is available at this https URL.</li>
</ul>

<h3>Title: D4+: Emergent Adversarial Driving Maneuvers with Approximate Functional Optimization</h3>
<ul>
<li><strong>Authors: </strong>Diego Ortiz Barbosa, Luis Burbano, Carlos Hernandez, Zengxiang Lei, Younghee Park, Satish Ukkusuri, Alvaro A Cardenas</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13942">https://arxiv.org/abs/2505.13942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13942">https://arxiv.org/pdf/2505.13942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13942]] D4+: Emergent Adversarial Driving Maneuvers with Approximate Functional Optimization(https://arxiv.org/abs/2505.13942)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Intelligent mechanisms implemented in autonomous vehicles, such as proactive driving assist and collision alerts, reduce traffic accidents. However, verifying their correct functionality is difficult due to complex interactions with the environment. This problem is exacerbated in adversarial environments, where an attacker can control the environment surrounding autonomous vehicles to exploit vulnerabilities. To preemptively identify vulnerabilities in these systems, in this paper, we implement a scenario-based framework with a formal method to identify the impact of malicious drivers interacting with autonomous vehicles. The formalization of the evaluation requirements utilizes metric temporal logic (MTL) to identify a safety condition that we want to test. Our goal is to find, through a rigorous testing approach, any trace that violates this MTL safety specification. Our results can help designers identify the range of safe operational behaviors that prevent malicious drivers from exploiting the autonomous features of modern vehicles.</li>
</ul>

<h3>Title: Every Pixel Tells a Story: End-to-End Urdu Newspaper OCR</h3>
<ul>
<li><strong>Authors: </strong>Samee Arif, Sualeha Farid</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13943">https://arxiv.org/abs/2505.13943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13943">https://arxiv.org/pdf/2505.13943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13943]] Every Pixel Tells a Story: End-to-End Urdu Newspaper OCR(https://arxiv.org/abs/2505.13943)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper introduces a comprehensive end-to-end pipeline for Optical Character Recognition (OCR) on Urdu newspapers. In our approach, we address the unique challenges of complex multi-column layouts, low-resolution archival scans, and diverse font styles. Our process decomposes the OCR task into four key modules: (1) article segmentation, (2) image super-resolution, (3) column segmentation, and (4) text recognition. For article segmentation, we fine-tune and evaluate YOLOv11x to identify and separate individual articles from cluttered layouts. Our model achieves a precision of 0.963 and mAP@50 of 0.975. For super-resolution, we fine-tune and benchmark the SwinIR model (reaching 32.71 dB PSNR) to enhance the quality of degraded newspaper scans. To do our column segmentation, we use YOLOv11x to separate columns in text to further enhance performance - this model reaches a precision of 0.970 and mAP@50 of 0.975. In the text recognition stage, we benchmark a range of LLMs from different families, including Gemini, GPT, Llama, and Claude. The lowest WER of 0.133 is achieved by Gemini-2.5-Pro.</li>
</ul>

<h3>Title: Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting</h3>
<ul>
<li><strong>Authors: </strong>Bao-Ngoc Dao, Quang Nguyen, Luyen Ngo Dinh, Minh Le, Nam Le, Linh Ngo Van</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13944">https://arxiv.org/abs/2505.13944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13944">https://arxiv.org/pdf/2505.13944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13944]] Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting(https://arxiv.org/abs/2505.13944)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, extraction, generative</a></li>
<li><strong>Abstract: </strong>Memory-based approaches have shown strong performance in Continual Relation Extraction (CRE). However, storing examples from previous tasks increases memory usage and raises privacy concerns. Recently, prompt-based methods have emerged as a promising alternative, as they do not rely on storing past samples. Despite this progress, current prompt-based techniques face several core challenges in CRE, particularly in accurately identifying task identities and mitigating catastrophic forgetting. Existing prompt selection strategies often suffer from inaccuracies, lack robust mechanisms to prevent forgetting in shared parameters, and struggle to handle both cross-task and within-task variations. In this paper, we propose WAVE++, a novel approach inspired by the connection between prefix-tuning and mixture of experts. Specifically, we introduce task-specific prompt pools that enhance flexibility and adaptability across diverse tasks while avoiding boundary-spanning risks; this design more effectively captures variations within each task and across tasks. To further refine relation classification, we incorporate label descriptions that provide richer, more global context, enabling the model to better distinguish among different relations. We also propose a training-free mechanism to improve task prediction during inference. Moreover, we integrate a generative model to consolidate prior knowledge within the shared parameters, thereby removing the need for explicit data storage. Extensive experiments demonstrate that WAVE++ outperforms state-of-the-art prompt-based and rehearsal-based methods, offering a more robust solution for continual relation extraction. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Memory-Centric Embodied Question Answer</h3>
<ul>
<li><strong>Authors: </strong>Mingliang Zhai, Zhi Gao, Yuwei Wu, Yunde Jia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13948">https://arxiv.org/abs/2505.13948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13948">https://arxiv.org/pdf/2505.13948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13948]] Memory-Centric Embodied Question Answer(https://arxiv.org/abs/2505.13948)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Embodied Question Answering (EQA) requires agents to autonomously explore and understand the environment to answer context-dependent questions. Existing frameworks typically center around the planner, which guides the stopping module, memory module, and answering module for reasoning. In this paper, we propose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric EQA models where the memory module cannot fully interact with other modules, MemoryEQA flexible feeds memory information into all modules, thereby enhancing efficiency and accuracy in handling complex tasks, such as those involving multiple targets across different regions. Specifically, we establish a multi-modal hierarchical memory mechanism, which is divided into global memory that stores language-enhanced scene maps, and local memory that retains historical observations and state information. When performing EQA tasks, the multi-modal large language model is leveraged to convert memory information into the required input formats for injection into different modules. To evaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset based on HM3D, comprising 1,587 question-answer pairs involving multiple targets across various regions, which requires agents to maintain memory of exploration-acquired target information. Experimental results on HM-EQA, MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a 19.8% performance gain on MT-HM3D compared to baseline model further underscores memory capability's pivotal role in resolving complex tasks.</li>
</ul>

<h3>Title: FlashThink: An Early Exit Method For Efficient Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Guochao Jiang, Guofeng Quan, Zepeng Ding, Ziqin Luo, Dixuan Wang, Zheng Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13949">https://arxiv.org/abs/2505.13949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13949">https://arxiv.org/pdf/2505.13949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13949]] FlashThink: An Early Exit Method For Efficient Reasoning(https://arxiv.org/abs/2505.13949)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive performance in reasoning tasks. However, LLMs tend to generate excessively long reasoning content, leading to significant computational overhead. Our observations indicate that even on simple problems, LLMs tend to produce unnecessarily lengthy reasoning content, which is against intuitive expectations. Preliminary experiments show that at a certain point during the generation process, the model is already capable of producing the correct solution without completing the full reasoning content. Therefore, we consider that the reasoning process of the model can be exited early to achieve the purpose of efficient reasoning. We introduce a verification model that identifies the exact moment when the model can stop reasoning and still provide the correct answer. Comprehensive experiments on four different benchmarks demonstrate that our proposed method, FlashThink, effectively shortens the reasoning content while preserving the model accuracy. For the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning content by 77.04% and 77.47%, respectively, without reducing the accuracy.</li>
</ul>

<h3>Title: Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiankun Zhang, Shenglai Zeng, Jie Ren, Tianqi Zheng, Hui Liu, Xianfeng Tang, Hui Liu, Yi Chang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13957">https://arxiv.org/abs/2505.13957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13957">https://arxiv.org/pdf/2505.13957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13957]] Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation(https://arxiv.org/abs/2505.13957)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust</a></li>
<li><strong>Abstract: </strong>Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by integrating external multimodal databases, but introduce unexplored privacy vulnerabilities. While text-based RAG privacy risks have been studied, multimodal data presents unique challenges. We provide the first systematic analysis of MRAG privacy vulnerabilities across vision-language and speech-language modalities. Using a novel compositional structured prompt attack in a black-box setting, we demonstrate how attackers can extract private information by manipulating queries. Our experiments reveal that LMMs can both directly generate outputs resembling retrieved content and produce descriptions that indirectly expose sensitive information, highlighting the urgent need for robust privacy-preserving MRAG techniques.</li>
</ul>

<h3>Title: Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Qianli Wang, Mingyang Wang, Nils Feldhus, Simon Ostermann, Yuan Cao, Hinrich Schütze, Sebastian Möller, Vera Schmitt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13963">https://arxiv.org/abs/2505.13963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13963">https://arxiv.org/pdf/2505.13963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13963]] Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability(https://arxiv.org/abs/2505.13963)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability, large language model</a></li>
<li><strong>Abstract: </strong>Quantization methods are widely used to accelerate inference and streamline the deployment of large language models (LLMs). While prior research has extensively investigated the degradation of various LLM capabilities due to quantization, its effects on model explainability and interpretability, which are crucial for understanding decision-making processes, remain unexplored. To address this gap, we conduct comprehensive experiments using three common quantization techniques at distinct bit widths, in conjunction with two explainability methods, counterfactual examples and natural language explanations, as well as two interpretability approaches, knowledge memorization analysis and latent multi-hop reasoning analysis. We complement our analysis with a thorough user study, evaluating selected explainability methods. Our findings reveal that, depending on the configuration, quantization can significantly impact model explainability and interpretability. Notably, the direction of this effect is not consistent, as it strongly depends on (1) the quantization method, (2) the explainability or interpretability approach, and (3) the evaluation protocol. In some settings, human evaluation shows that quantization degrades explainability, while in others, it even leads to improvements. Our work serves as a cautionary tale, demonstrating that quantization can unpredictably affect model transparency. This insight has important implications for deploying LLMs in applications where transparency is a critical requirement.</li>
</ul>

<h3>Title: Zk-SNARK for String Match</h3>
<ul>
<li><strong>Authors: </strong>Taoran Li, Taobo Liao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13964">https://arxiv.org/abs/2505.13964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13964">https://arxiv.org/pdf/2505.13964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13964]] Zk-SNARK for String Match(https://arxiv.org/abs/2505.13964)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>We present a secure and efficient string-matching platform leveraging zk-SNARKs (Zero-Knowledge Succinct Non-Interactive Arguments of Knowledge) to address the challenge of detecting sensitive information leakage while preserving data privacy. Our solution enables organizations to verify whether private strings appear on public platforms without disclosing the strings themselves. To achieve computational efficiency, we integrate a sliding window technique with the Rabin-Karp algorithm and Rabin Fingerprint, enabling hash-based rolling comparisons to detect string matches. This approach significantly reduces time complexity compared to traditional character-by-character comparisons. We implement the proposed system using gnark, a high-performance zk-SNARK library, which generates succinct and verifiable proofs for privacy-preserving string matching. Experimental results demonstrate that our solution achieves strong privacy guarantees while maintaining computational efficiency and scalability. This work highlights the practical applications of zero-knowledge proofs in secure data verification and contributes a scalable method for privacy-preserving string matching.</li>
</ul>

<h3>Title: CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring</h3>
<ul>
<li><strong>Authors: </strong>Jiamin Su, Yibo Yan, Zhuoran Gao, Han Zhang, Xiang Liu, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13965">https://arxiv.org/abs/2505.13965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13965">https://arxiv.org/pdf/2505.13965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13965]] CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring(https://arxiv.org/abs/2505.13965)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automated Essay Scoring (AES) is crucial for modern education, particularly with the increasing prevalence of multimodal assessments. However, traditional AES methods struggle with evaluation generalizability and multimodal perception, while even recent Multimodal Large Language Model (MLLM)-based approaches can produce hallucinated justifications and scores misaligned with human judgment. To address the limitations, we introduce CAFES, the first collaborative multi-agent framework specifically designed for AES. It orchestrates three specialized agents: an Initial Scorer for rapid, trait-specific evaluations; a Feedback Pool Manager to aggregate detailed, evidence-grounded strengths; and a Reflective Scorer that iteratively refines scores based on this feedback to enhance human alignment. Extensive experiments, using state-of-the-art MLLMs, achieve an average relative improvement of 21% in Quadratic Weighted Kappa (QWK) against ground truth, especially for grammatical and lexical diversity. Our proposed CAFES framework paves the way for an intelligent multimodal AES system. The code will be available upon acceptance.</li>
</ul>

<h3>Title: Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals</h3>
<ul>
<li><strong>Authors: </strong>Qianli Wang, Van Bach Nguyen, Nils Feldhus, Luis Felipe Villa-Arenas, Christin Seifert, Sebastian Möller, Vera Schmitt</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13972">https://arxiv.org/abs/2505.13972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13972">https://arxiv.org/pdf/2505.13972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13972]] Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals(https://arxiv.org/abs/2505.13972)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Counterfactual examples are widely employed to enhance the performance and robustness of large language models (LLMs) through counterfactual data augmentation (CDA). However, the selection of the judge model used to evaluate label flipping, the primary metric for assessing the validity of generated counterfactuals for CDA, yields inconsistent results. To decipher this, we define four types of relationships between the counterfactual generator and judge models. Through extensive experiments involving two state-of-the-art LLM-based methods, three datasets, five generator models, and 15 judge models, complemented by a user study (n = 90), we demonstrate that judge models with an independent, non-fine-tuned relationship to the generator model provide the most reliable label flipping evaluations. Relationships between the generator and judge models, which are closely aligned with the user study for CDA, result in better model performance and robustness. Nevertheless, we find that the gap between the most effective judge models and the results obtained from the user study remains considerably large. This suggests that a fully automated pipeline for CDA may be inadequate and requires human intervention.</li>
</ul>

<h3>Title: Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenhui Zhu, Xuanzhao Dong, Xin Li, Peijie Qiu, Xiwen Chen, Abolfazl Razi, Aris Sotiras, Yi Su, Yalin Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13973">https://arxiv.org/abs/2505.13973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13973">https://arxiv.org/pdf/2505.13973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13973]] Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models(https://arxiv.org/abs/2505.13973)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, reinforcement learning (RL)-based tuning has shifted the trajectory of Multimodal Large Language Models (MLLMs), particularly following the introduction of Group Relative Policy Optimization (GRPO). However, directly applying it to medical tasks remains challenging for achieving clinically grounded model behavior. Motivated by the need to align model response with clinical expectations, we investigate four critical dimensions that affect the effectiveness of RL-based tuning in medical visual question answering (VQA): base model initialization strategy, the role of medical semantic alignment, the impact of length-based rewards on long-chain reasoning, and the influence of bias. We conduct extensive experiments to analyze these factors for medical MLLMs, providing new insights into how models are domain-specifically fine-tuned. Additionally, our results also demonstrate that GRPO-based RL tuning consistently outperforms standard supervised fine-tuning (SFT) in both accuracy and reasoning quality.</li>
</ul>

<h3>Title: Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection</h3>
<ul>
<li><strong>Authors: </strong>Maya Srikanth, Run Chen, Julia Hirschberg</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13979">https://arxiv.org/abs/2505.13979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13979">https://arxiv.org/pdf/2505.13979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13979]] Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection(https://arxiv.org/abs/2505.13979)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal models play a key role in empathy detection, but their performance can suffer when modalities provide conflicting cues. To understand these failures, we examine cases where unimodal and multimodal predictions diverge. Using fine-tuned models for text, audio, and video, along with a gated fusion model, we find that such disagreements often reflect underlying ambiguity, as evidenced by annotator uncertainty. Our analysis shows that dominant signals in one modality can mislead fusion when unsupported by others. We also observe that humans, like models, do not consistently benefit from multimodal input. These insights position disagreement as a useful diagnostic signal for identifying challenging examples and improving empathy system robustness.</li>
</ul>

<h3>Title: The Hallucination Tax of Reinforcement Finetuning</h3>
<ul>
<li><strong>Authors: </strong>Linxin Song, Taiwei Shi, Jieyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13988">https://arxiv.org/abs/2505.13988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13988">https://arxiv.org/pdf/2505.13988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13988]] The Hallucination Tax of Reinforcement Finetuning(https://arxiv.org/abs/2505.13988)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement finetuning (RFT) has become a standard approach for enhancing the reasoning capabilities of large language models (LLMs). However, its impact on model trustworthiness remains underexplored. In this work, we identify and systematically study a critical side effect of RFT, which we term the hallucination tax: a degradation in refusal behavior causing models to produce hallucinated answers to unanswerable questions confidently. To investigate this, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of unanswerable math problems designed to probe models' ability to recognize an unanswerable question by reasoning from the insufficient or ambiguous information. Our results show that standard RFT training could reduce model refusal rates by more than 80%, which significantly increases model's tendency to hallucinate. We further demonstrate that incorporating just 10% SUM during RFT substantially restores appropriate refusal behavior, with minimal accuracy trade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage inference-time compute to reason about their own uncertainty and knowledge boundaries, improving generalization not only to out-of-domain math problems but also to factual question answering tasks.</li>
</ul>

<h3>Title: When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Yanzhe Wen, Xunkai Li, Qi Zhang, Zhu Lei, Guang Zeng, Rong-Hua Li, Guoren Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13989">https://arxiv.org/abs/2505.13989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13989">https://arxiv.org/pdf/2505.13989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13989]] When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty(https://arxiv.org/abs/2505.13989)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, large language models (LLMs) have significantly advanced text-attributed graph (TAG) learning. However, existing methods inadequately handle data uncertainty in open-world scenarios, especially concerning limited labeling and unknown-class nodes. Prior solutions typically rely on isolated semantic or structural approaches for unknown-class rejection, lacking effective annotation pipelines. To address these limitations, we propose Open-world Graph Assistant (OGA), an LLM-based framework that combines adaptive label traceability, which integrates semantics and topology for unknown-class rejection, and a graph label annotator to enable model updates using newly annotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and practicality.</li>
</ul>

<h3>Title: DecIF: Improving Instruction-Following through Meta-Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Tingfeng Hui, Pengyu Zhu, Bowen Ping, Ling Tang, Yaqi Zhang, Sen Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13990">https://arxiv.org/abs/2505.13990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13990">https://arxiv.org/pdf/2505.13990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13990]] DecIF: Improving Instruction-Following through Meta-Decomposition(https://arxiv.org/abs/2505.13990)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Instruction-following has emerged as a crucial capability for large language models (LLMs). However, existing approaches often rely on pre-existing documents or external resources to synthesize instruction-following data, which limits their flexibility and generalizability. In this paper, we introduce DecIF, a fully autonomous, meta-decomposition guided framework that generates diverse and high-quality instruction-following data using only LLMs. DecIF is grounded in the principle of decomposition. For instruction generation, we guide LLMs to iteratively produce various types of meta-information, which are then combined with response constraints to form well-structured and semantically rich instructions. We further utilize LLMs to detect and resolve potential inconsistencies within the generated instructions. Regarding response generation, we decompose each instruction into atomic-level evaluation criteria, enabling rigorous validation and the elimination of inaccurate instruction-response pairs. Extensive experiments across a wide range of scenarios and settings demonstrate DecIF's superior performance on instruction-following tasks. Further analysis highlights its strong flexibility, scalability, and generalizability in automatically synthesizing high-quality instruction data.</li>
</ul>

<h3>Title: StPR: Spatiotemporal Preservation and Routing for Exemplar-Free Video Class-Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Huaijie Wang, De Cheng, Guozhang Li, Zhipeng Xu, Lingfeng He, Jie Li, Nannan Wang, Xinbo Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13997">https://arxiv.org/abs/2505.13997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13997">https://arxiv.org/pdf/2505.13997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13997]] StPR: Spatiotemporal Preservation and Routing for Exemplar-Free Video Class-Incremental Learning(https://arxiv.org/abs/2505.13997)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, interpretability</a></li>
<li><strong>Abstract: </strong>Video Class-Incremental Learning (VCIL) seeks to develop models that continuously learn new action categories over time without forgetting previously acquired knowledge. Unlike traditional Class-Incremental Learning (CIL), VCIL introduces the added complexity of spatiotemporal structures, making it particularly challenging to mitigate catastrophic forgetting while effectively capturing both frame-shared semantics and temporal dynamics. Existing approaches either rely on exemplar rehearsal, raising concerns over memory and privacy, or adapt static image-based methods that neglect temporal modeling. To address these limitations, we propose Spatiotemporal Preservation and Routing (StPR), a unified and exemplar-free VCIL framework that explicitly disentangles and preserves spatiotemporal information. First, we introduce Frame-Shared Semantics Distillation (FSSD), which identifies semantically stable and meaningful channels by jointly considering semantic sensitivity and classification contribution. These important semantic channels are selectively regularized to maintain prior knowledge while allowing for adaptation. Second, we design a Temporal Decomposition-based Mixture-of-Experts (TD-MoE), which dynamically routes task-specific experts based on their temporal dynamics, enabling inference without task ID or stored exemplars. Together, StPR effectively leverages spatial semantics and temporal dynamics, achieving a unified, exemplar-free VCIL framework. Extensive experiments on UCF101, HMDB51, and Kinetics400 show that our method outperforms existing baselines while offering improved interpretability and efficiency in VCIL. Code is available in the supplementary materials.</li>
</ul>

<h3>Title: Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Han Zhang, Yan Wang, Guanfeng Liu, Pengfei Ding, Huaxiong Wang, Kwok-Yan Lam</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14005">https://arxiv.org/abs/2505.14005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14005">https://arxiv.org/pdf/2505.14005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14005]] Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks(https://arxiv.org/abs/2505.14005)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>To enhance the reliability and credibility of graph neural networks (GNNs) and improve the transparency of their decision logic, a new field of explainability of GNNs (XGNN) has emerged. However, two major limitations severely degrade the performance and hinder the generalizability of existing XGNN methods: they (a) fail to capture the complete decision logic of GNNs across diverse distributions in the entire dataset's sample space, and (b) impose strict prerequisites on edge properties and GNN internal accessibility. To address these limitations, we propose OPEN, a novel c\textbf{O}mprehensive and \textbf{P}rerequisite-free \textbf{E}xplainer for G\textbf{N}Ns. OPEN, as the first work in the literature, can infer and partition the entire dataset's sample space into multiple environments, each containing graphs that follow a distinct distribution. OPEN further learns the decision logic of GNNs across different distributions by sampling subgraphs from each environment and analyzing their predictions, thus eliminating the need for strict prerequisites. Experimental results demonstrate that OPEN captures nearly complete decision logic of GNNs, outperforms state-of-the-art methods in fidelity while maintaining similar efficiency, and enhances robustness in real-world scenarios.</li>
</ul>

<h3>Title: Activation-Guided Consensus Merging for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Yao, Shuqi Liu, Zehua Liu, Qintong Li, Mingyang Liu, Xiongwei Han, Zhijiang Guo, Han Wu, Linqi Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14009">https://arxiv.org/abs/2505.14009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14009">https://arxiv.org/pdf/2505.14009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14009]] Activation-Guided Consensus Merging for Large Language Models(https://arxiv.org/abs/2505.14009)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent research has increasingly focused on reconciling the reasoning capabilities of System 2 with the efficiency of System 1. While existing training-based and prompt-based approaches face significant challenges in terms of efficiency and stability, model merging emerges as a promising strategy to integrate the diverse capabilities of different Large Language Models (LLMs) into a unified model. However, conventional model merging methods often assume uniform importance across layers, overlooking the functional heterogeneity inherent in neural components. To address this limitation, we propose \textbf{A}ctivation-Guided \textbf{C}onsensus \textbf{M}erging (\textbf{ACM}), a plug-and-play merging framework that determines layer-specific merging coefficients based on mutual information between activations of pre-trained and fine-tuned models. ACM effectively preserves task-specific capabilities without requiring gradient computations or additional training. Extensive experiments on Long-to-Short (L2S) and general merging tasks demonstrate that ACM consistently outperforms all baseline methods. For instance, in the case of Qwen-7B models, TIES-Merging equipped with ACM achieves a \textbf{55.3\%} reduction in response length while simultaneously improving reasoning accuracy by \textbf{1.3} points. We submit the code with the paper for reproducibility, and it will be publicly available.</li>
</ul>

<h3>Title: UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache</h3>
<ul>
<li><strong>Authors: </strong>Pu Wang, Pengwen Dai, Chen Wu, Yeying Jin, Dianjie Lu, Guijuan Zhang, Youshan Zhang, Zhuoran Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14010">https://arxiv.org/abs/2505.14010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14010">https://arxiv.org/pdf/2505.14010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14010]] UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache(https://arxiv.org/abs/2505.14010)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we propose an efficient visual transformer framework for ultra-high-definition (UHD) image dehazing that addresses the key challenges of slow training speed and high memory consumption for existing methods. Our approach introduces two key innovations: 1) an \textbf{a}daptive \textbf{n}ormalization mechanism inspired by the nGPT architecture that enables ultra-fast and stable training with a network with a restricted range of parameter expressions; and 2) we devise an atmospheric scattering-aware KV caching mechanism that dynamically optimizes feature preservation based on the physical haze formation model. The proposed architecture improves the training convergence speed by \textbf{5 $\times$} while reducing memory overhead, enabling real-time processing of 50 high-resolution images per second on an RTX4090 GPU. Experimental results show that our approach maintains state-of-the-art dehazing quality while significantly improving computational efficiency for 4K/8K image restoration tasks. Furthermore, we provide a new dehazing image interpretable method with the help of an integrated gradient attribution map. Our code can be found here: this https URL.</li>
</ul>

<h3>Title: Adaptive Sentencing Prediction with Guaranteed Accuracy and Legal Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Yifei Jin, Xin Zheng, Lei Guo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14011">https://arxiv.org/abs/2505.14011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14011">https://arxiv.org/pdf/2505.14011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14011]] Adaptive Sentencing Prediction with Guaranteed Accuracy and Legal Interpretability(https://arxiv.org/abs/2505.14011)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Existing research on judicial sentencing prediction predominantly relies on end-to-end models, which often neglect the inherent sentencing logic and lack interpretability-a critical requirement for both scholarly research and judicial practice. To address this challenge, we make three key contributions:First, we propose a novel Saturated Mechanistic Sentencing (SMS) model, which provides inherent legal interpretability by virtue of its foundation in China's Criminal Law. We also introduce the corresponding Momentum Least Mean Squares (MLMS) adaptive algorithm for this model. Second, for the MLMS algorithm based adaptive sentencing predictor, we establish a mathematical theory on the accuracy of adaptive prediction without resorting to any stationarity and independence assumptions on the data. We also provide a best possible upper bound for the prediction accuracy achievable by the best predictor designed in the known parameters case. Third, we construct a Chinese Intentional Bodily Harm (CIBH) dataset. Utilizing this real-world data, extensive experiments demonstrate that our approach achieves a prediction accuracy that is not far from the best possible theoretical upper bound, validating both the model's suitability and the algorithm's accuracy.</li>
</ul>

<h3>Title: EGFormer: Towards Efficient and Generalizable Multimodal Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zelin Zhang, Tao Zhang, KediLI, Xu Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14014">https://arxiv.org/abs/2505.14014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14014">https://arxiv.org/pdf/2505.14014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14014]] EGFormer: Towards Efficient and Generalizable Multimodal Semantic Segmentation(https://arxiv.org/abs/2505.14014)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent efforts have explored multimodal semantic segmentation using various backbone architectures. However, while most methods aim to improve accuracy, their computational efficiency remains underexplored. To address this, we propose EGFormer, an efficient multimodal semantic segmentation framework that flexibly integrates an arbitrary number of modalities while significantly reducing model parameters and inference time without sacrificing performance. Our framework introduces two novel modules. First, the Any-modal Scoring Module (ASM) assigns importance scores to each modality independently, enabling dynamic ranking based on their feature maps. Second, the Modal Dropping Module (MDM) filters out less informative modalities at each stage, selectively preserving and aggregating only the most valuable features. This design allows the model to leverage useful information from all available modalities while discarding redundancy, thus ensuring high segmentation quality. In addition to efficiency, we evaluate EGFormer on a synthetic-to-real transfer task to demonstrate its generalizability. Extensive experiments show that EGFormer achieves competitive performance with up to 88 percent reduction in parameters and 50 percent fewer GFLOPs. Under unsupervised domain adaptation settings, it further achieves state-of-the-art transfer performance compared to existing methods.</li>
</ul>

<h3>Title: AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation</h3>
<ul>
<li><strong>Authors: </strong>Tai D. Nguyen, Long H. Pham, Jun Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14015">https://arxiv.org/abs/2505.14015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14015">https://arxiv.org/pdf/2505.14015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14015]] AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation(https://arxiv.org/abs/2505.14015)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of domain-specific large language models (LLMs) in fields like law necessitates frameworks that account for nuanced regional legal distinctions, which are critical for ensuring compliance and trustworthiness. Existing legal evaluation benchmarks often lack adaptability and fail to address diverse local contexts, limiting their utility in dynamically evolving regulatory landscapes. To address these gaps, we propose AutoLaw, a novel violation detection framework that combines adversarial data generation with a jury-inspired deliberation process to enhance legal compliance of LLMs. Unlike static approaches, AutoLaw dynamically synthesizes case law to reflect local regulations and employs a pool of LLM-based "jurors" to simulate judicial decision-making. Jurors are ranked and selected based on synthesized legal expertise, enabling a deliberation process that minimizes bias and improves detection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG (legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness: adversarial data generation improves LLM discrimination, while the jury-based voting strategy significantly boosts violation detection rates. Our results highlight the framework's ability to adaptively probe legal misalignments and deliver reliable, context-aware judgments, offering a scalable solution for evaluating and enhancing LLMs in legally sensitive applications.</li>
</ul>

<h3>Title: FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix</h3>
<ul>
<li><strong>Authors: </strong>Di Wu, Qian Li, Heng Yang, Yong Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14024">https://arxiv.org/abs/2505.14024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14024">https://arxiv.org/pdf/2505.14024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14024]] FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix(https://arxiv.org/abs/2505.14024)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables geographically distributed clients to collaboratively train machine learning models by sharing only their local models, ensuring data privacy. However, FL is vulnerable to untargeted attacks that aim to degrade the global model's performance on the underlying data distribution. Existing defense mechanisms attempt to improve FL's resilience against such attacks, but their effectiveness is limited in practical FL environments due to data heterogeneity. On the contrary, we aim to detect and remove the attacks to mitigate their impact. Generalization contribution plays a crucial role in distinguishing untargeted attacks. Our observations indicate that, with limited data, the divergence between embeddings representing different classes provides a better measure of generalization than direct accuracy. In light of this, we propose a novel robust aggregation method, FedGraM, designed to defend against untargeted attacks in FL. The server maintains an auxiliary dataset containing one sample per class to support aggregation. This dataset is fed to the local models to extract embeddings. Then, the server calculates the norm of the Gram Matrix of the embeddings for each local model. The norm serves as an indicator of each model's inter-class separation capability in the embedding space. FedGraM identifies and removes potentially malicious models by filtering out those with the largest norms, then averages the remaining local models to form the global model. We conduct extensive experiments to evaluate the performance of FedGraM. Our empirical results show that with limited data samples used to construct the auxiliary dataset, FedGraM achieves exceptional performance, outperforming state-of-the-art defense methods.</li>
</ul>

<h3>Title: CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model for Complex and Imbalanced Data</h3>
<ul>
<li><strong>Authors: </strong>Yifan Zeng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14027">https://arxiv.org/abs/2505.14027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14027">https://arxiv.org/pdf/2505.14027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14027]] CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model for Complex and Imbalanced Data(https://arxiv.org/abs/2505.14027)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, interpretability, generative</a></li>
<li><strong>Abstract: </strong>As computer networks proliferate, the gravity of network intrusions has escalated, emphasizing the criticality of network intrusion detection systems for safeguarding security. While deep learning models have exhibited promising results in intrusion detection, they face challenges in managing high-dimensional, complex traffic patterns and imbalanced data categories. This paper presents CSAGC-IDS, a network intrusion detection model based on deep learning techniques. CSAGC-IDS integrates SC-CGAN, a self-attention-enhanced convolutional conditional generative adversarial network that generates high-quality data to mitigate class imbalance. Furthermore, CSAGC-IDS integrates CSCA-CNN, a convolutional neural network enhanced through cost sensitive learning and channel attention mechanism, to extract features from complex traffic data for precise detection. Experiments conducted on the NSL-KDD dataset. CSAGC-IDS achieves an accuracy of 84.55% and an F1-score of 84.52% in five-class classification task, and an accuracy of 91.09% and an F1 score of 92.04% in binary classification this http URL, this paper provides an interpretability analysis of the proposed model, using SHAP and LIME to explain the decision-making mechanisms of the model.</li>
</ul>

<h3>Title: OmniStyle: Filtering High Quality Style Transfer Data at Scale</h3>
<ul>
<li><strong>Authors: </strong>Ye Wang, Ruiqi Liu, Jiang Lin, Fei Liu, Zili Yi, Yilin Wang, Rui Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14028">https://arxiv.org/abs/2505.14028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14028">https://arxiv.org/pdf/2505.14028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14028]] OmniStyle: Filtering High Quality Style Transfer Data at Scale(https://arxiv.org/abs/2505.14028)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce OmniStyle-1M, a large-scale paired style transfer dataset comprising over one million content-style-stylized image triplets across 1,000 diverse style categories, each enhanced with textual descriptions and instruction prompts. We show that OmniStyle-1M can not only enable efficient and scalable of style transfer models through supervised training but also facilitate precise control over target stylization. Especially, to ensure the quality of the dataset, we introduce OmniFilter, a comprehensive style transfer quality assessment framework, which filters high-quality triplets based on content preservation, style consistency, and aesthetic appeal. Building upon this foundation, we propose OmniStyle, a framework based on the Diffusion Transformer (DiT) architecture designed for high-quality and efficient style transfer. This framework supports both instruction-guided and image-guided style transfer, generating high resolution outputs with exceptional detail. Extensive qualitative and quantitative evaluations demonstrate OmniStyle's superior performance compared to existing approaches, highlighting its efficiency and versatility. OmniStyle-1M and its accompanying methodologies provide a significant contribution to advancing high-quality style transfer, offering a valuable resource for the research community.</li>
</ul>

<h3>Title: AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards</h3>
<ul>
<li><strong>Authors: </strong>Laura-Sophia von Hirschhausen, Jannes S. Magnusson, Mykyta Kovalenko, Fredrik Boye, Tanay Rawat, Peter Eisert, Anna Hilsmann, Sebastian Pretzsch, Sebastian Bosse</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14029">https://arxiv.org/abs/2505.14029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14029">https://arxiv.org/pdf/2505.14029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14029]] AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards(https://arxiv.org/abs/2505.14029)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning has transformed computer vision for precision agriculture, yet apple orchard monitoring remains limited by dataset constraints. The lack of diverse, realistic datasets and the difficulty of annotating dense, heterogeneous scenes. Existing datasets overlook different growth stages and stereo imagery, both essential for realistic 3D modeling of orchards and tasks like fruit localization, yield estimation, and structural analysis. To address these gaps, we present AppleGrowthVision, a large-scale dataset comprising two subsets. The first includes 9,317 high resolution stereo images collected from a farm in Brandenburg (Germany), covering six agriculturally validated growth stages over a full growth cycle. The second subset consists of 1,125 densely annotated images from the same farm in Brandenburg and one in Pillnitz (Germany), containing a total of 31,084 apple labels. AppleGrowthVision provides stereo-image data with agriculturally validated growth stages, enabling precise phenological analysis and 3D reconstructions. Extending MinneApple with our data improves YOLOv8 performance by 7.69 % in terms of F1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by 31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy using VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges the gap between agricultural science and computer vision, by enabling the development of robust models for fruit detection, growth modeling, and 3D analysis in precision agriculture. Future work includes improving annotation, enhancing 3D reconstruction, and extending multimodal analysis across all growth stages.</li>
</ul>

<h3>Title: Adaptive Cyclic Diffusion for Inference Scaling</h3>
<ul>
<li><strong>Authors: </strong>Gyubin Lee, Truong Nhat Nguyen Bao, Jaesik Yoon, Dongwoo Lee, Minsu Kim, Yoshua Bengio, Sungjin Ahn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14036">https://arxiv.org/abs/2505.14036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14036">https://arxiv.org/pdf/2505.14036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14036]] Adaptive Cyclic Diffusion for Inference Scaling(https://arxiv.org/abs/2505.14036)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated strong generative capabilities across domains ranging from image synthesis to complex reasoning tasks. However, most inference-time scaling methods rely on fixed denoising schedules, limiting their ability to allocate computation based on instance difficulty or task-specific demands adaptively. We introduce the challenge of adaptive inference-time scaling-dynamically adjusting computational effort during inference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a flexible, search-based inference framework. ABCD refines outputs through bi-directional diffusion cycles while adaptively controlling exploration depth and termination. It comprises three components: Cyclic Diffusion Search, Automatic Exploration-Exploitation Balancing, and Adaptive Thinking Time. Experiments show that ABCD improves performance across diverse tasks while maintaining computational efficiency.</li>
</ul>

<h3>Title: Unsupervised Graph Clustering with Deep Structural Entropy</h3>
<ul>
<li><strong>Authors: </strong>Jingyun Zhang, Hao Peng, Li Sun, Guanlin Wu, Chunyang Liu, Zhengtao Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14040">https://arxiv.org/abs/2505.14040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14040">https://arxiv.org/pdf/2505.14040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14040]] Unsupervised Graph Clustering with Deep Structural Entropy(https://arxiv.org/abs/2505.14040)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Research on Graph Structure Learning (GSL) provides key insights for graph-based clustering, yet current methods like Graph Neural Networks (GNNs), Graph Attention Networks (GATs), and contrastive learning often rely heavily on the original graph structure. Their performance deteriorates when the original graph's adjacency matrix is too sparse or contains noisy edges unrelated to clustering. Moreover, these methods depend on learning node embeddings and using traditional techniques like k-means to form clusters, which may not fully capture the underlying graph structure between nodes. To address these limitations, this paper introduces DeSE, a novel unsupervised graph clustering framework incorporating Deep Structural Entropy. It enhances the original graph with quantified structural information and deep neural networks to form clusters. Specifically, we first propose a method for calculating structural entropy with soft assignment, which quantifies structure in a differentiable form. Next, we design a Structural Learning layer (SLL) to generate an attributed graph from the original feature data, serving as a target to enhance and optimize the original structural graph, thereby mitigating the issue of sparse connections between graph nodes. Finally, our clustering assignment method (ASS), based on GNNs, learns node embeddings and a soft assignment matrix to cluster on the enhanced graph. The ASS layer can be stacked to meet downstream task requirements, minimizing structural entropy for stable clustering and maximizing node consistency with edge-based cross-entropy loss. Extensive comparative experiments are conducted on four benchmark datasets against eight representative unsupervised graph clustering baselines, demonstrating the superiority of the DeSE in both effectiveness and interpretability.</li>
</ul>

<h3>Title: Adversarially Pretrained Transformers may be Universally Robust In-Context Learners</h3>
<ul>
<li><strong>Authors: </strong>Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14042">https://arxiv.org/abs/2505.14042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14042">https://arxiv.org/pdf/2505.14042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14042]] Adversarially Pretrained Transformers may be Universally Robust In-Context Learners(https://arxiv.org/abs/2505.14042)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Adversarial training is one of the most effective adversarial defenses, but it incurs a high computational cost. In this study, we show that transformers adversarially pretrained on diverse tasks can serve as robust foundation models and eliminate the need for adversarial training in downstream tasks. Specifically, we theoretically demonstrate that through in-context learning, a single adversarially pretrained transformer can robustly generalize to multiple unseen tasks without any additional training, i.e., without any parameter updates. This robustness stems from the model's focus on robust features and its resistance to attacks that exploit non-predictive features. Besides these positive findings, we also identify several limitations. Under certain conditions (though unrealistic), no universally robust single-layer transformers exist. Moreover, robust transformers exhibit an accuracy--robustness trade-off and require a large number of in-context demonstrations. The code is available at this https URL.</li>
</ul>

<h3>Title: Selective Structured State Space for Multispectral-fused Small Target Detection</h3>
<ul>
<li><strong>Authors: </strong>Qianqian Zhang, WeiJun Wang, Yunxing Liu, Li Zhou, Hao Zhao, Junshe An, Zihan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14043">https://arxiv.org/abs/2505.14043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14043">https://arxiv.org/pdf/2505.14043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14043]] Selective Structured State Space for Multispectral-fused Small Target Detection(https://arxiv.org/abs/2505.14043)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Target detection in high-resolution remote sensing imagery faces challenges due to the low recognition accuracy of small targets and high computational costs. The computational complexity of the Transformer architecture increases quadratically with image resolution, while Convolutional Neural Networks (CNN) architectures are forced to stack deeper convolutional layers to expand their receptive fields, leading to an explosive growth in computational demands. To address these computational constraints, we leverage Mamba's linear complexity for efficiency. However, Mamba's performance declines for small targets, primarily because small targets occupy a limited area in the image and have limited semantic information. Accurate identification of these small targets necessitates not only Mamba's global attention capabilities but also the precise capture of fine local details. To this end, we enhance Mamba by developing the Enhanced Small Target Detection (ESTD) module and the Convolutional Attention Residual Gate (CARG) module. The ESTD module bolsters local attention to capture fine-grained details, while the CARG module, built upon Mamba, emphasizes spatial and channel-wise information, collectively improving the model's ability to capture distinctive representations of small targets. Additionally, to highlight the semantic representation of small targets, we design a Mask Enhanced Pixel-level Fusion (MEPF) module for multispectral fusion, which enhances target features by effectively fusing visible and infrared multimodal information.</li>
</ul>

<h3>Title: Generalized Category Discovery via Token Manifold Capacity Learning</h3>
<ul>
<li><strong>Authors: </strong>Luyao Tang, Kunze Huang, Chaoqi Chen, Cheng Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14044">https://arxiv.org/abs/2505.14044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14044">https://arxiv.org/pdf/2505.14044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14044]] Generalized Category Discovery via Token Manifold Capacity Learning(https://arxiv.org/abs/2505.14044)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Generalized category discovery (GCD) is essential for improving deep learning models' robustness in open-world scenarios by clustering unlabeled data containing both known and novel categories. Traditional GCD methods focus on minimizing intra-cluster variations, often sacrificing manifold capacity, which limits the richness of intra-class representations. In this paper, we propose a novel approach, Maximum Token Manifold Capacity (MTMC), that prioritizes maximizing the manifold capacity of class tokens to preserve the diversity and complexity of data. MTMC leverages the nuclear norm of singular values as a measure of manifold capacity, ensuring that the representation of samples remains informative and well-structured. This method enhances the discriminability of clusters, allowing the model to capture detailed semantic features and avoid the loss of critical information during clustering. Through theoretical analysis and extensive experiments on coarse- and fine-grained datasets, we demonstrate that MTMC outperforms existing GCD methods, improving both clustering accuracy and the estimation of category numbers. The integration of MTMC leads to more complete representations, better inter-class separability, and a reduction in dimensional collapse, establishing MTMC as a vital component for robust open-world learning. Code is in this http URL.</li>
</ul>

<h3>Title: From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora</h3>
<ul>
<li><strong>Authors: </strong>Yingli Shen, Wen Lai, Shuo Wang, Kangyang Luo, Alexander Fraser, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14045">https://arxiv.org/abs/2505.14045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14045">https://arxiv.org/pdf/2505.14045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14045]] From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora(https://arxiv.org/abs/2505.14045)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Continued pretraining and instruction tuning on large-scale multilingual data have proven to be effective in scaling large language models (LLMs) to low-resource languages. However, the unaligned nature of such data limits its ability to effectively capture cross-lingual semantics. In contrast, multi-way parallel data, where identical content is aligned across multiple languages, provides stronger cross-lingual consistency and offers greater potential for improving multilingual performance. In this paper, we introduce a large-scale, high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus spans 113 languages, with up to 50 languages aligned in parallel, ensuring extensive multilingual coverage. Using this dataset, we investigate best practices for leveraging multi-way parallel data to enhance LLMs, including strategies for continued pretraining, instruction tuning, and the analysis of key influencing factors. Experiments on six multilingual benchmarks show that models trained on multiway parallel data consistently outperform those trained on unaligned multilingual data.</li>
</ul>

<h3>Title: Learning Concept-Driven Logical Rules for Interpretable and Generalizable Medical Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Yibo Gao, Hangqi Zhou, Zheyao Gao, Bomin Wang, Shangqi Gao, Sihan Wang, Xiahai Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14049">https://arxiv.org/abs/2505.14049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14049">https://arxiv.org/pdf/2505.14049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14049]] Learning Concept-Driven Logical Rules for Interpretable and Generalizable Medical Image Classification(https://arxiv.org/abs/2505.14049)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The pursuit of decision safety in clinical applications highlights the potential of concept-based methods in medical imaging. While these models offer active interpretability, they often suffer from concept leakages, where unintended information within soft concept representations undermines both interpretability and generalizability. Moreover, most concept-based models focus solely on local explanations (instance-level), neglecting the global decision logic (dataset-level). To address these limitations, we propose Concept Rule Learner (CRL), a novel framework to learn Boolean logical rules from binarized visual concepts. CRL employs logical layers to capture concept correlations and extract clinically meaningful rules, thereby providing both local and global interpretability. Experiments on two medical image classification tasks show that CRL achieves competitive performance with existing methods while significantly improving generalizability to out-of-distribution data. The code of our work is available at this https URL.</li>
</ul>

<h3>Title: Improved Methods for Model Pruning and Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Wei Jiang, Anying Fu, Youling Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14052">https://arxiv.org/abs/2505.14052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14052">https://arxiv.org/pdf/2505.14052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14052]] Improved Methods for Model Pruning and Knowledge Distillation(https://arxiv.org/abs/2505.14052)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Model pruning is a performance optimization technique for large language models like R1 or o3-mini. However, existing pruning methods often lead to significant performance degradation or require extensive retraining and fine-tuning. This technique aims to identify and remove neurons, connections unlikely leading to the contribution during the human-computer interaction phase. Our goal is to obtain a much smaller and faster knowledge distilled model that can quickly generate content almost as good as those of the unpruned ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an improved pruning method that effectively reduces model size and computational complexity while maintaining performance comparable to the original unpruned model even at extreme pruned levels. The improved method is based on weights, bias fixed in the pre-training phase and GRPO rewards verified during the post-training phase as our novel pruning indicators. Preliminary experimental results show that our method outperforms and be comparable to state-of-the-art methods across various pruning levels and different downstream computational linguistics tasks.</li>
</ul>

<h3>Title: Scaling Vision Mamba Across Resolutions via Fractal Traversal</h3>
<ul>
<li><strong>Authors: </strong>Bo Li, Haoke Xiao, Lv Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14062">https://arxiv.org/abs/2505.14062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14062">https://arxiv.org/pdf/2505.14062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14062]] Scaling Vision Mamba Across Resolutions via Fractal Traversal(https://arxiv.org/abs/2505.14062)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision Mamba has recently emerged as a promising alternative to Transformer-based architectures, offering linear complexity in sequence length while maintaining strong modeling capacity. However, its adaptation to visual inputs is hindered by challenges in 2D-to-1D patch serialization and weak scalability across input resolutions. Existing serialization strategies such as raster scanning disrupt local spatial continuity and limit the model's ability to generalize across scales. In this paper, we propose FractalMamba++, a robust vision backbone that leverages fractal-based patch serialization via Hilbert curves to preserve spatial locality and enable seamless resolution adaptability. To address long-range dependency fading in high-resolution inputs, we further introduce a Cross-State Routing (CSR) mechanism that enhances global context propagation through selective state reuse. Additionally, we propose a Positional-Relation Capture (PRC) module to recover local adjacency disrupted by curve inflection points. Extensive experiments on image classification, semantic segmentation, object detection, and change detection demonstrate that FractalMamba++ consistently outperforms previous Mamba-based backbones, particularly under high-resolution settings.</li>
</ul>

<h3>Title: Place Recognition: A Comprehensive Review, Current Challenges and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Li, Tianyi Shang, Pengjie Xu, Zhaojun Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14068">https://arxiv.org/abs/2505.14068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14068">https://arxiv.org/pdf/2505.14068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14068]] Place Recognition: A Comprehensive Review, Current Challenges and Future Directions(https://arxiv.org/abs/2505.14068)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Place recognition is a cornerstone of vehicle navigation and mapping, which is pivotal in enabling systems to determine whether a location has been previously visited. This capability is critical for tasks such as loop closure in Simultaneous Localization and Mapping (SLAM) and long-term navigation under varying environmental conditions. In this survey, we comprehensively review recent advancements in place recognition, emphasizing three representative methodological paradigms: Convolutional Neural Network (CNN)-based approaches, Transformer-based frameworks, and cross-modal strategies. We begin by elucidating the significance of place recognition within the broader context of autonomous systems. Subsequently, we trace the evolution of CNN-based methods, highlighting their contributions to robust visual descriptor learning and scalability in large-scale environments. We then examine the emerging class of Transformer-based models, which leverage self-attention mechanisms to capture global dependencies and offer improved generalization across diverse scenes. Furthermore, we discuss cross-modal approaches that integrate heterogeneous data sources such as Lidar, vision, and text description, thereby enhancing resilience to viewpoint, illumination, and seasonal variations. We also summarize standard datasets and evaluation metrics widely adopted in the literature. Finally, we identify current research challenges and outline prospective directions, including domain adaptation, real-time performance, and lifelong learning, to inspire future advancements in this domain. The unified framework of leading-edge place recognition methods, i.e., code library, and the results of their experimental evaluations are available at this https URL.</li>
</ul>

<h3>Title: Enhancing LLMs via High-Knowledge Data Selection</h3>
<ul>
<li><strong>Authors: </strong>Feiyu Duan, Xuemiao Zhang, Sirui Wang, Haoran Que, Yuqi Liu, Wenge Rong, Xunliang Cai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14070">https://arxiv.org/abs/2505.14070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14070">https://arxiv.org/pdf/2505.14070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14070]] Enhancing LLMs via High-Knowledge Data Selection(https://arxiv.org/abs/2505.14070)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The performance of Large Language Models (LLMs) is intrinsically linked to the quality of its training data. Although several studies have proposed methods for high-quality data selection, they do not consider the importance of knowledge richness in text corpora. In this paper, we propose a novel and gradient-free High-Knowledge Scorer (HKS) to select high-quality data from the dimension of knowledge, to alleviate the problem of knowledge scarcity in the pre-trained corpus. We propose a comprehensive multi-domain knowledge element pool and introduce knowledge density and coverage as metrics to assess the knowledge content of the text. Based on this, we propose a comprehensive knowledge scorer to select data with intensive knowledge, which can also be utilized for domain-specific high-knowledge data selection by restricting knowledge elements to the specific domain. We train models on a high-knowledge bilingual dataset, and experimental results demonstrate that our scorer improves the model's performance in knowledge-intensive and general comprehension tasks, and is effective in enhancing both the generic and domain-specific capabilities of the model.</li>
</ul>

<h3>Title: Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Woody Haosheng Gan, Deqing Fu, Julian Asilis, Ollie Liu, Dani Yogatama, Vatsal Sharan, Robin Jia, Willie Neiswanger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14071">https://arxiv.org/abs/2505.14071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14071">https://arxiv.org/pdf/2505.14071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14071]] Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models(https://arxiv.org/abs/2505.14071)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Steering methods have emerged as effective and targeted tools for guiding large language models' (LLMs) behavior without modifying their parameters. Multimodal large language models (MLLMs), however, do not currently enjoy the same suite of techniques, due in part to their recency and architectural diversity. Inspired by this gap, we investigate whether MLLMs can be steered using vectors derived from their text-only LLM backbone, via sparse autoencoders (SAEs), mean shift, and linear probing. We find that text-derived steering consistently enhances multimodal accuracy across diverse MLLM architectures and visual tasks. In particular, mean shift boosts spatial relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to +3.3%, outperforming prompting and exhibiting strong generalization to out-of-distribution datasets. These results highlight textual steering vectors as a powerful, efficient mechanism for enhancing grounding in MLLMs with minimal additional data collection and computational overhead.</li>
</ul>

<h3>Title: BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks</h3>
<ul>
<li><strong>Authors: </strong>Weihong Du, Wenrui Liao, Binyu Yan, Hongru Liang, Anthony G. Cohn, Wenqiang Lei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14079">https://arxiv.org/abs/2505.14079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14079">https://arxiv.org/pdf/2505.14079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14079]] BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks(https://arxiv.org/abs/2505.14079)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) based agents have shown great potential in following human instructions and automatically completing various tasks. To complete a task, the agent needs to decompose it into easily executed steps by planning. Existing studies mainly conduct the planning by inferring what steps should be executed next starting from the agent's initial state. However, this forward reasoning paradigm doesn't work well for complex tasks. We propose to study this issue in Minecraft, a virtual environment that simulates complex tasks based on real-world scenarios. We believe that the failure of forward reasoning is caused by the big perception gap between the agent's initial state and task goal. To this end, we leverage backward reasoning and make the planning starting from the terminal state, which can directly achieve the task goal in one step. Specifically, we design a BAckward Reasoning based agent (BAR). It is equipped with a recursive goal decomposition module, a state consistency maintaining module and a stage memory module to make robust, consistent, and efficient planning starting from the terminal state. Experimental results demonstrate the superiority of BAR over existing methods and the effectiveness of proposed modules.</li>
</ul>

<h3>Title: Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts</h3>
<ul>
<li><strong>Authors: </strong>Xi Chen, Shen Yan, Juelin Zhu, Chen Chen, Yu Liu, Maojun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14088">https://arxiv.org/abs/2505.14088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14088">https://arxiv.org/pdf/2505.14088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14088]] Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts(https://arxiv.org/abs/2505.14088)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>We introduce Land-MoE, a novel approach for multispectral land cover classification (MLCC). Spectral shift, which emerges from disparities in sensors and geospatial conditions, poses a significant challenge in this domain. Existing methods predominantly rely on domain adaptation and generalization strategies, often utilizing small-scale models that exhibit limited performance. In contrast, Land-MoE addresses these issues by hierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts, to fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner. Specifically, Land-MoE comprises two key modules: the mixture of low-rank token experts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages rank-differentiated tokens to generate diverse feature adjustments for individual instances within multispectral images. By dynamically combining learnable low-rank token experts of varying ranks, it enhances the robustness against spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on the refined features. This process enables the model to effectively capture frequency band information that is strongly correlated with semantic essence, while simultaneously suppressing frequency noise irrelevant to the task. Comprehensive experiments on MLCC tasks involving cross-sensor and cross-geospatial setups demonstrate that Land-MoE outperforms existing methods by a large margin. Additionally, the proposed approach has also achieved state-of-the-art performance in domain generalization semantic segmentation tasks of RGB remote sensing images.</li>
</ul>

<h3>Title: Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Yihua Zhu, Qianying Liu, Akiko Aizawa, Hidetoshi Shimodaira</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14099">https://arxiv.org/abs/2505.14099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14099">https://arxiv.org/pdf/2505.14099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14099]] Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering(https://arxiv.org/abs/2505.14099)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge Base Question Answering (KBQA) aims to answer natural language questions using structured knowledge from KBs. While LLM-only approaches offer generalization, they suffer from outdated knowledge, hallucinations, and lack of transparency. Chain-based KG-RAG methods address these issues by incorporating external KBs, but are limited to simple chain-structured questions due to the absence of planning and logical structuring. Inspired by semantic parsing methods, we propose PDRR: a four-stage framework consisting of Predict, Decompose, Retrieve, and Reason. Our method first predicts the question type and decomposes the question into structured triples. Then retrieves relevant information from KBs and guides the LLM as an agent to reason over and complete the decomposed triples. Experimental results demonstrate that PDRR consistently outperforms existing methods across various LLM backbones and achieves superior performance on both chain-structured and non-chain complex questions.</li>
</ul>

<h3>Title: Unlocking the Power of SAM 2 for Few-Shot Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Qianxiong Xu, Lanyun Zhu, Xuanyi Liu, Guosheng Lin, Cheng Long, Ziyue Li, Rui Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14100">https://arxiv.org/abs/2505.14100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14100">https://arxiv.org/pdf/2505.14100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14100]] Unlocking the Power of SAM 2 for Few-Shot Segmentation(https://arxiv.org/abs/2505.14100)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few classes to segment arbitrary classes, but at the risk of overfitting. To address this, some methods use the well-learned knowledge of foundation models (e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM by supporting video segmentation, whose class-agnostic matching ability is useful to FSS. A simple idea is to encode support foreground (FG) features as memory, with which query FG features are matched and fused. Unfortunately, the FG objects in different frames of SAM 2's video data are always the same identity, while those in FSS are different identities, i.e., the matching step is incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo query memory, matching with query features in a compatible way. However, the memories can never be as accurate as the real ones, i.e., they are likely to contain incomplete query FG, and some unexpected query background (BG) features, leading to wrong segmentation. Hence, we further design Iterative Memory Refinement to fuse more query FG features into the memory, and devise a Support-Calibrated Memory Attention to suppress the unexpected query BG features in memory. Extensive experiments have been conducted on PASCAL-5$^i$ and COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot mIoU can be 4.2\% better than the best baseline.</li>
</ul>

<h3>Title: MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations</h3>
<ul>
<li><strong>Authors: </strong>Ernests Lavrinovics, Russa Biswas, Katja Hose, Johannes Bjerva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14101">https://arxiv.org/abs/2505.14101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14101">https://arxiv.org/pdf/2505.14101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14101]] MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations(https://arxiv.org/abs/2505.14101)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have inherent limitations of faithfulness and factuality, commonly referred to as hallucinations. Several benchmarks have been developed that provide a test bed for factuality evaluation within the context of English-centric datasets, while relying on supplementary informative context like web links or text passages but ignoring the available structured factual resources. To this end, Knowledge Graphs (KGs) have been identified as a useful aid for hallucination mitigation, as they provide a structured way to represent the facts about entities and their relations with minimal linguistic overhead. We bridge the lack of KG paths and multilinguality for factual language modeling within the existing hallucination evaluation benchmarks and propose a KG-based multilingual, multihop benchmark called \textbf{MultiHal} framed for generative text evaluation. As part of our data collection pipeline, we mined 140k KG-paths from open-domain KGs, from which we pruned noisy KG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation shows an absolute scale increase by approximately 0.12 to 0.36 points for the semantic similarity score in KG-RAG over vanilla QA across multiple languages and multiple models, demonstrating the potential of KG integration. We anticipate MultiHal will foster future research towards several graph-based hallucination mitigation and fact-checking tasks.</li>
</ul>

<h3>Title: AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guangke Chen, Fu Song, Zhe Zhao, Xiaojun Jia, Yang Liu, Yanchen Qiao, Weizhe Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14103">https://arxiv.org/abs/2505.14103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14103">https://arxiv.org/pdf/2505.14103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14103]] AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models(https://arxiv.org/abs/2505.14103)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Jailbreak attacks to Large audio-language models (LALMs) are studied recently, but they achieve suboptimal effectiveness, applicability, and practicability, particularly, assuming that the adversary can fully manipulate user prompts. In this work, we first conduct an extensive experiment showing that advanced text jailbreak attacks cannot be easily ported to end-to-end LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio does not need to align with user prompts in the time axis by crafting suffixal jailbreak audios; (2) universality: a single jailbreak perturbation is effective for different prompts by incorporating multiple prompts into perturbation generation; (3) stealthiness: the malicious intent of jailbreak audios will not raise the awareness of victims by proposing various intent concealment strategies; and (4) over-the-air robustness: the jailbreak audios remain effective when being played over the air by incorporating the reverberation distortion effect with room impulse response into the generation of the perturbations. In contrast, all prior audio jailbreak attacks cannot offer asynchrony, universality, stealthiness, or over-the-air robustness. Moreover, AudioJailbreak is also applicable to the adversary who cannot fully manipulate user prompts, thus has a much broader attack scenario. Extensive experiments with thus far the most LALMs demonstrate the high effectiveness of AudioJailbreak. We highlight that our work peeks into the security implications of audio jailbreak attacks against LALMs, and realistically fosters improving their security robustness. The implementation and audio samples are available at our website this https URL.</li>
</ul>

<h3>Title: Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents</h3>
<ul>
<li><strong>Authors: </strong>Wei Fan, Tianshi Zheng, Yiran Hu, Zheye Deng, Weiqi Wang, Baixuan Xu, Chunyang Li, Haoran Li, Weixing Shen, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14104">https://arxiv.org/abs/2505.14104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14104">https://arxiv.org/pdf/2505.14104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14104]] Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents(https://arxiv.org/abs/2505.14104)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Legal rules encompass not only codified statutes but also implicit adjudicatory principles derived from precedents that contain discretionary norms, social morality, and policy. While computational legal research has advanced in applying established rules to cases, inducing legal rules from judicial decisions remains understudied, constrained by limitations in model inference efficacy and symbolic reasoning capability. The advent of Large Language Models (LLMs) offers unprecedented opportunities for automating the extraction of such latent principles, yet progress is stymied by the absence of formal task definitions, benchmark datasets, and methodologies. To address this gap, we formalize Legal Rule Induction (LRI) as the task of deriving concise, generalizable doctrinal rules from sets of analogous precedents, distilling their shared preconditions, normative behaviors, and legal consequences. We introduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese cases in total) for model tuning and 216 expert-annotated gold test sets. Experimental results reveal that: 1) State-of-the-art LLMs struggle with over-generalization and hallucination; 2) Training on our dataset markedly enhances LLMs capabilities in capturing nuanced rule patterns across similar cases.</li>
</ul>

<h3>Title: Unintended Bias in 2D+ Image Segmentation and Its Effect on Attention Asymmetry</h3>
<ul>
<li><strong>Authors: </strong>Zsófia Molnár, Gergely Szabó, András Horváth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14105">https://arxiv.org/abs/2505.14105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14105">https://arxiv.org/pdf/2505.14105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14105]] Unintended Bias in 2D+ Image Segmentation and Its Effect on Attention Asymmetry(https://arxiv.org/abs/2505.14105)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, segmentation</a></li>
<li><strong>Abstract: </strong>Supervised pretrained models have become widely used in deep learning, especially for image segmentation tasks. However, when applied to specialized datasets such as biomedical imaging, pretrained weights often introduce unintended biases. These biases cause models to assign different levels of importance to different slices, leading to inconsistencies in feature utilization, which can be observed as asymmetries in saliency map distributions. This transfer of color distributions from natural images to non-natural datasets can compromise model performance and reduce the reliability of results. In this study, we investigate the effects of these biases and propose strategies to mitigate them. Through a series of experiments, we test both pretrained and randomly initialized models, comparing their performance and saliency map distributions. Our proposed methods, which aim to neutralize the bias introduced by pretrained color channel weights, demonstrate promising results, offering a practical approach to improving model explainability while maintaining the benefits of pretrained models. This publication presents our findings, providing insights into addressing pretrained weight biases across various deep learning tasks.</li>
</ul>

<h3>Title: A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations</h3>
<ul>
<li><strong>Authors: </strong>Li Li, Peilin Cai, Ryan A. Rossi, Franck Dernoncourt, Branislav Kveton, Junda Wu, Tong Yu, Linxin Song, Tiankai Yang, Yuehan Qin, Nesreen K. Ahmed, Samyadeep Basu, Subhojyoti Mukherjee, Ruiyi Zhang, Zhengmian Hu, Bo Ni, Yuxiao Zhou, Zichao Wang, Yue Huang, Yu Wang, Xiangliang Zhang, Philip S. Yu, Xiyang Hu, Yue Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14106">https://arxiv.org/abs/2505.14106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14106">https://arxiv.org/pdf/2505.14106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14106]] A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations(https://arxiv.org/abs/2505.14106)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present PersonaConvBench, a large-scale benchmark for evaluating personalized reasoning and generation in multi-turn conversations with large language models (LLMs). Unlike existing work that focuses on either personalization or conversational structure in isolation, PersonaConvBench integrates both, offering three core tasks: sentence classification, impact regression, and user-centric text generation across ten diverse Reddit-based domains. This design enables systematic analysis of how personalized conversational context shapes LLM outputs in realistic multi-user scenarios. We benchmark several commercial and open-source LLMs under a unified prompting setup and observe that incorporating personalized history yields substantial performance improvements, including a 198 percent relative gain over the best non-conversational baseline in sentiment classification. By releasing PersonaConvBench with evaluations and code, we aim to support research on LLMs that adapt to individual styles, track long-term context, and produce contextually rich, engaging responses.</li>
</ul>

<h3>Title: DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yakun Zhu, Zhongzhen Huang, Linjie Mu, Yutong Huang, Wei Nie, Shaoting Zhang, Pengfei Liu, Xiaofan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14107">https://arxiv.org/abs/2505.14107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14107">https://arxiv.org/pdf/2505.14107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14107]] DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models(https://arxiv.org/abs/2505.14107)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The emergence of groundbreaking large language models capable of performing complex reasoning tasks holds significant promise for addressing various scientific challenges, including those arising in complex clinical scenarios. To enable their safe and effective deployment in real-world healthcare settings, it is urgently necessary to benchmark the diagnostic capabilities of current models systematically. Given the limitations of existing medical benchmarks in evaluating advanced diagnostic reasoning, we present DiagnosisArena, a comprehensive and challenging benchmark designed to rigorously assess professional-level diagnostic competence. DiagnosisArena consists of 1,113 pairs of segmented patient cases and corresponding diagnoses, spanning 28 medical specialties, deriving from clinical case reports published in 10 top-tier medical journals. The benchmark is developed through a meticulous construction pipeline, involving multiple rounds of screening and review by both AI systems and human experts, with thorough checks conducted to prevent data leakage. Our study reveals that even the most advanced reasoning models, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79% accuracy, respectively. This finding highlights a significant generalization bottleneck in current large language models when faced with clinical diagnostic reasoning challenges. Through DiagnosisArena, we aim to drive further advancements in AIs diagnostic reasoning capabilities, enabling more effective solutions for real-world clinical diagnostic challenges. We provide the benchmark and evaluation tools for further research and development this https URL.</li>
</ul>

<h3>Title: Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking</h3>
<ul>
<li><strong>Authors: </strong>Tianle Gu, Zongqi Wang, Kexin Huang, Yuanqi Yao, Xiangliang Zhang, Yujiu Yang, Xiuying Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14112">https://arxiv.org/abs/2505.14112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14112">https://arxiv.org/pdf/2505.14112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14112]] Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking(https://arxiv.org/abs/2505.14112)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark</a></li>
<li><strong>Abstract: </strong>Logit-based LLM watermarking traces and verifies AI-generated content by maintaining green and red token lists and increasing the likelihood of green tokens during generation. However, it fails in low-entropy scenarios, where predictable outputs make green token selection difficult without disrupting natural text flow. Existing approaches address this by assuming access to the original LLM to calculate entropy and selectively watermark high-entropy tokens. However, these methods face two major challenges: (1) high computational costs and detection delays due to reliance on the original LLM, and (2) potential risks of model leakage. To address these limitations, we propose Invisible Entropy (IE), a watermarking paradigm designed to enhance both safety and efficiency. Instead of relying on the original LLM, IE introduces a lightweight feature extractor and an entropy tagger to predict whether the entropy of the next token is high or low. Furthermore, based on theoretical analysis, we develop a threshold navigator that adaptively sets entropy thresholds. It identifies a threshold where the watermark ratio decreases as the green token count increases, enhancing the naturalness of the watermarked text and improving detection robustness. Experiments on HumanEval and MBPP datasets demonstrate that IE reduces parameter size by 99\% while achieving performance on par with state-of-the-art methods. Our work introduces a safe and efficient paradigm for low-entropy watermarking. this https URL this https URL</li>
</ul>

<h3>Title: CONSIGN: Conformal Segmentation Informed by Spatial Groupings via Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Bruno Viti, Elias Karabelas, Martin Holler</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14113">https://arxiv.org/abs/2505.14113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14113">https://arxiv.org/pdf/2505.14113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14113]] CONSIGN: Conformal Segmentation Informed by Spatial Groupings via Decomposition(https://arxiv.org/abs/2505.14113)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Most machine learning-based image segmentation models produce pixel-wise confidence scores - typically derived from softmax outputs - that represent the model's predicted probability for each class label at every pixel. While this information can be particularly valuable in high-stakes domains such as medical imaging, these (uncalibrated) scores are heuristic in nature and do not constitute rigorous quantitative uncertainty estimates. Conformal prediction (CP) provides a principled framework for transforming heuristic confidence scores into statistically valid uncertainty estimates. However, applying CP directly to image segmentation ignores the spatial correlations between pixels, a fundamental characteristic of image data. This can result in overly conservative and less interpretable uncertainty estimates. To address this, we propose CONSIGN (Conformal Segmentation Informed by Spatial Groupings via Decomposition), a CP-based method that incorporates spatial correlations to improve uncertainty quantification in image segmentation. Our method generates meaningful prediction sets that come with user-specified, high-probability error guarantees. It is compatible with any pre-trained segmentation model capable of generating multiple sample outputs - such as those using dropout, Bayesian modeling, or ensembles. We evaluate CONSIGN against a standard pixel-wise CP approach across three medical imaging datasets and two COCO dataset subsets, using three different pre-trained segmentation models. Results demonstrate that accounting for spatial structure significantly improves performance across multiple metrics and enhances the quality of uncertainty estimates.</li>
</ul>

<h3>Title: Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst</h3>
<ul>
<li><strong>Authors: </strong>Hongru Wang, Deng Cai, Wanjun Zhong, Shijue Huang, Jeff Z. Pan, Zeming Liu, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14116">https://arxiv.org/abs/2505.14116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14116">https://arxiv.org/pdf/2505.14116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14116]] Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst(https://arxiv.org/abs/2505.14116)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Inference-time scaling has attracted much attention which significantly enhance the performance of Large Language Models (LLMs) in complex reasoning tasks by increasing the length of Chain-of-Thought. These longer intermediate reasoning rationales embody various meta-reasoning skills in human cognition, such as reflection and decomposition, being difficult to create and acquire. In this work, we introduce \textit{Self-Reasoning Language Model} (SRLM), where the model itself can synthesize longer CoT data and iteratively improve performance through self-training. By incorporating a few demonstration examples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from existing responses, which act as a reasoning catalyst, we demonstrate that SRLM not only enhances the model's initial performance but also ensures more stable and consistent improvements in subsequent iterations. Our proposed SRLM achieves an average absolute improvement of more than $+2.5$ points across five reasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models. Moreover, it brings more improvements with more times of sampling during inference, such as absolute $+7.89$ average improvement with $64$ sampling times, revealing the in-depth, diverse and creative reasoning paths in SRLM against the strong baseline.</li>
</ul>

<h3>Title: Intra-class Patch Swap for Self-Distillation</h3>
<ul>
<li><strong>Authors: </strong>Hongjun Choi, Eun Som Jeon, Ankita Shukla, Pavan Turaga</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14124">https://arxiv.org/abs/2505.14124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14124">https://arxiv.org/pdf/2505.14124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14124]] Intra-class Patch Swap for Self-Distillation(https://arxiv.org/abs/2505.14124)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Knowledge distillation (KD) is a valuable technique for compressing large deep learning models into smaller, edge-suitable networks. However, conventional KD frameworks rely on pre-trained high-capacity teacher networks, which introduce significant challenges such as increased memory/storage requirements, additional training costs, and ambiguity in selecting an appropriate teacher for a given student model. Although a teacher-free distillation (self-distillation) has emerged as a promising alternative, many existing approaches still rely on architectural modifications or complex training procedures, which limit their generality and efficiency. To address these limitations, we propose a novel framework based on teacher-free distillation that operates using a single student network without any auxiliary components, architectural modifications, or additional learnable parameters. Our approach is built on a simple yet highly effective augmentation, called intra-class patch swap augmentation. This augmentation simulates a teacher-student dynamic within a single model by generating pairs of intra-class samples with varying confidence levels, and then applying instance-to-instance distillation to align their predictive distributions. Our method is conceptually simple, model-agnostic, and easy to implement, requiring only a single augmentation function. Extensive experiments across image classification, semantic segmentation, and object detection show that our method consistently outperforms both existing self-distillation baselines and conventional teacher-based KD approaches. These results suggest that the success of self-distillation could hinge on the design of the augmentation itself. Our codes are available at this https URL.</li>
</ul>

<h3>Title: MAS-KCL: Knowledge component graph structure learning with large language model-based agentic workflow</h3>
<ul>
<li><strong>Authors: </strong>Yuan-Hao Jiang, Kezong Tang, Zi-Wei Chen, Yuang Wei, Tian-Yi Liu, Jiayi Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, cs.HC, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14126">https://arxiv.org/abs/2505.14126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14126">https://arxiv.org/pdf/2505.14126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14126]] MAS-KCL: Knowledge component graph structure learning with large language model-based agentic workflow(https://arxiv.org/abs/2505.14126)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge components (KCs) are the fundamental units of knowledge in the field of education. A KC graph illustrates the relationships and dependencies between KCs. An accurate KC graph can assist educators in identifying the root causes of learners' poor performance on specific KCs, thereby enabling targeted instructional interventions. To achieve this, we have developed a KC graph structure learning algorithm, named MAS-KCL, which employs a multi-agent system driven by large language models for adaptive modification and optimization of the KC graph. Additionally, a bidirectional feedback mechanism is integrated into the algorithm, where AI agents leverage this mechanism to assess the value of edges within the KC graph and adjust the distribution of generation probabilities for different edges, thereby accelerating the efficiency of structure learning. We applied the proposed algorithm to 5 synthetic datasets and 4 real-world educational datasets, and experimental results validate its effectiveness in learning path recognition. By accurately identifying learners' learning paths, teachers are able to design more comprehensive learning plans, enabling learners to achieve their educational goals more effectively, thus promoting the sustainable development of education.</li>
</ul>

<h3>Title: Probing BERT for German Compound Semantics</h3>
<ul>
<li><strong>Authors: </strong>Filip Miletić, Aaron Schmid, Sabine Schulte im Walde</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14130">https://arxiv.org/abs/2505.14130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14130">https://arxiv.org/pdf/2505.14130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14130]] Probing BERT for German Compound Semantics(https://arxiv.org/abs/2505.14130)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper investigates the extent to which pretrained German BERT encodes knowledge of noun compound semantics. We comprehensively vary combinations of target tokens, layers, and cased vs. uncased models, and evaluate them by predicting the compositionality of 868 gold standard compounds. Looking at representational patterns within the transformer architecture, we observe trends comparable to equivalent prior work on English, with compositionality information most easily recoverable in the early layers. However, our strongest results clearly lag behind those reported for English, suggesting an inherently more difficult task in German. This may be due to the higher productivity of compounding in German than in English and the associated increase in constituent-level ambiguity, including in our target compound set.</li>
</ul>

<h3>Title: Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhou, Mohsen Mesgar, Heike Adel, Annemarie Friedrich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14131">https://arxiv.org/abs/2505.14131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14131">https://arxiv.org/pdf/2505.14131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14131]] Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering(https://arxiv.org/abs/2505.14131)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In table question answering (TQA), tables are encoded as either texts or images. Prior work suggests that passing images of tables to multi-modal large language models (MLLMs) performs comparably to or even better than using textual input with large language models (LLMs). However, the lack of controlled setups limits fine-grained distinctions between these approaches. In this paper, we conduct the first controlled study on the effectiveness of several combinations of table representations and models from two perspectives: question complexity and table size. We build a new benchmark based on existing TQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we find that the best combination of table representation and model varies across setups. We propose FRES, a method selecting table representations dynamically, and observe a 10% average performance improvement compared to using both representations indiscriminately.</li>
</ul>

<h3>Title: Hunyuan-Game: Industrial-grade Intelligent Game Creation Model</h3>
<ul>
<li><strong>Authors: </strong>Ruihuang Li, Caijin Zhou, Shoujian Zheng, Jianxiang Lu, Jiabin Huang, Comi Chen, Junshu Tang, Guangzheng Xu, Jiale Tao, Hongmei Wang, Donghao Li, Wenqing Yu, Senbo Wang, Zhimin Li, Yetshuan Shi, Haoyu Yang, Yukun Wang, Wenxun Dai, Jiaqi Li, Linqing Wang, Qixun Wang, Zhiyong Xu, Yingfang Zhang, Jiangfeng Xiong, Weijie Kong, Chao Zhang, Hongxin Zhang, Qiaoling Zheng, Weiting Guo, Xinchi Deng, Yixuan Li, Renjia Wei, Yulin Jian, Duojun Huang, Xuhua Ren, Sihuan Lin, Yifu Sun, Yuan Zhou, Joey Wang, Qin Lin, Jingmiao Yu, Jihong Zhang, Caesar Zhong, Di Wang, Yuhong Liu, Linus, Jie Jiang, Longhuang Wu, Shuai Shao, Qinglin Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14135">https://arxiv.org/abs/2505.14135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14135">https://arxiv.org/pdf/2505.14135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14135]] Hunyuan-Game: Industrial-grade Intelligent Game Creation Model(https://arxiv.org/abs/2505.14135)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Intelligent game creation represents a transformative advancement in game development, utilizing generative artificial intelligence to dynamically generate and enhance game content. Despite notable progress in generative models, the comprehensive synthesis of high-quality game assets, including both images and videos, remains a challenging frontier. To create high-fidelity game content that simultaneously aligns with player preferences and significantly boosts designer efficiency, we present Hunyuan-Game, an innovative project designed to revolutionize intelligent game production. Hunyuan-Game encompasses two primary branches: image generation and video generation. The image generation component is built upon a vast dataset comprising billions of game images, leading to the development of a group of customized image generation models tailored for game scenarios: (1) General Text-to-Image Generation. (2) Game Visual Effects Generation, involving text-to-effect and reference image-based game visual effect generation. (3) Transparent Image Generation for characters, scenes, and game visual effects. (4) Game Character Generation based on sketches, black-and-white images, and white models. The video generation component is built upon a comprehensive dataset of millions of game and anime videos, leading to the development of five core algorithmic models, each targeting critical pain points in game development and having robust adaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2) 360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4) Generative Video Super-Resolution. (5) Interactive Game Video Generation. These image and video generation models not only exhibit high-level aesthetic expression but also deeply integrate domain-specific knowledge, establishing a systematic understanding of diverse game and anime art styles.</li>
</ul>

<h3>Title: FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Marvin Alles, Nutan Chen, Patrick van der Smagt, Botond Cseke</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14139">https://arxiv.org/abs/2505.14139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14139">https://arxiv.org/pdf/2505.14139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14139]] FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning(https://arxiv.org/abs/2505.14139)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The use of guidance to steer sampling toward desired outcomes has been widely explored within diffusion models, especially in applications such as image and trajectory generation. However, incorporating guidance during training remains relatively underexplored. In this work, we introduce energy-guided flow matching, a novel approach that enhances the training of flow models and eliminates the need for guidance at inference time. We learn a conditional velocity field corresponding to the flow policy by approximating an energy-guided probability path as a Gaussian path. Learning guided trajectories is appealing for tasks where the target distribution is defined by a combination of data and an energy function, as in reinforcement learning. Diffusion-based policies have recently attracted attention for their expressive power and ability to capture multi-modal action distributions. Typically, these policies are optimized using weighted objectives or by back-propagating gradients through actions sampled by the policy. As an alternative, we propose FlowQ, an offline reinforcement learning algorithm based on energy-guided flow matching. Our method achieves competitive performance while the policy training time is constant in the number of flow sampling steps.</li>
</ul>

<h3>Title: Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information</h3>
<ul>
<li><strong>Authors: </strong>Chengzhi Zhang, Xinyi Yan, Lei Zhao, Yingyi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14149">https://arxiv.org/abs/2505.14149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14149">https://arxiv.org/pdf/2505.14149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14149]] Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information(https://arxiv.org/abs/2505.14149)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The exponential increase in academic papers has significantly increased the time required for researchers to access relevant literature. Keyphrase Extraction (KPE) offers a solution to this situation by enabling researchers to efficiently retrieve relevant literature. The current study on KPE from academic articles aims to improve the performance of extraction models through innovative approaches using Title and Abstract as input corpora. However, the semantic richness of keywords is significantly constrained by the length of the abstract. While full-text-based KPE can address this issue, it simultaneously introduces noise, which significantly diminishes KPE performance. To address this issue, this paper utilized the structural features and section texts obtained from the section structure information of academic articles to extract keyphrase from academic papers. The approach consists of two main parts: (1) exploring the effect of seven structural features on KPE models, and (2) integrating the extraction results from all section texts used as input corpora for KPE models via a keyphrase integration algorithm to obtain the keyphrase integration result. Furthermore, this paper also examined the effect of the classification quality of section structure on the KPE performance. The results show that incorporating structural features improves KPE performance, though different features have varying effects on model efficacy. The keyphrase integration approach yields the best performance, and the classification quality of section structure can affect KPE performance. These findings indicate that using the section structure information of academic articles contributes to effective KPE from academic articles. The code and dataset supporting this study are available at this https URL.</li>
</ul>

<h3>Title: ReactDiff: Latent Diffusion for Facial Reaction Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Li, Sheng Wang, Xin Wang, Yitao Zhu, Honglin Xiong, Zixu Zhuang, Qian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14151">https://arxiv.org/abs/2505.14151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14151">https://arxiv.org/pdf/2505.14151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14151]] ReactDiff: Latent Diffusion for Facial Reaction Generation(https://arxiv.org/abs/2505.14151)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Given the audio-visual clip of the speaker, facial reaction generation aims to predict the listener's facial reactions. The challenge lies in capturing the relevance between video and audio while balancing appropriateness, realism, and diversity. While prior works have mostly focused on uni-modal inputs or simplified reaction mappings, recent approaches such as PerFRDiff have explored multi-modal inputs and the one-to-many nature of appropriate reaction mappings. In this work, we propose the Facial Reaction Diffusion (ReactDiff) framework that uniquely integrates a Multi-Modality Transformer with conditional diffusion in the latent space for enhanced reaction generation. Unlike existing methods, ReactDiff leverages intra- and inter-class attention for fine-grained multi-modal interaction, while the latent diffusion process between the encoder and decoder enables diverse yet contextually appropriate outputs. Experimental results demonstrate that ReactDiff significantly outperforms existing approaches, achieving a facial reaction correlation of 0.26 and diversity score of 0.094 while maintaining competitive realism. The code is open-sourced at \href{this https URL}{github}.</li>
</ul>

<h3>Title: Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search</h3>
<ul>
<li><strong>Authors: </strong>Songhao Wu, Quan Tu, Hong Liu, Jia Xu, Zhongyi Liu, Guannan Zhang, Ran Wang, Xiuying Chen, Rui Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14156">https://arxiv.org/abs/2505.14156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14156">https://arxiv.org/pdf/2505.14156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14156]] Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search(https://arxiv.org/abs/2505.14156)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Session search involves a series of interactive queries and actions to fulfill user's complex information need. Current strategies typically prioritize sequential modeling for deep semantic understanding, overlooking the graph structure in interactions. While some approaches focus on capturing structural information, they use a generalized representation for documents, neglecting the word-level semantic modeling. In this paper, we propose Symbolic Graph Ranker (SGR), which aims to take advantage of both text-based and graph-based approaches by leveraging the power of recent Large Language Models (LLMs). Concretely, we first introduce a set of symbolic grammar rules to convert session graph into text. This allows integrating session history, interaction process, and task instruction seamlessly as inputs for the LLM. Moreover, given the natural discrepancy between LLMs pre-trained on textual corpora, and the symbolic language we produce using our graph-to-text grammar, our objective is to enhance LLMs' ability to capture graph structures within a textual format. To achieve this, we introduce a set of self-supervised symbolic learning tasks including link prediction, node content generation, and generative contrastive learning, to enable LLMs to capture the topological information from coarse-grained to fine-grained. Experiment results and comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm the superiority of our approach. Our paradigm also offers a novel and effective methodology that bridges the gap between traditional search strategies and modern LLMs.</li>
</ul>

<h3>Title: Temporal Alignment of Time Sensitive Facts with Activation Engineering</h3>
<ul>
<li><strong>Authors: </strong>Sanjay Govindan, Maurice Pagnucco, Yang Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14158">https://arxiv.org/abs/2505.14158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14158">https://arxiv.org/pdf/2505.14158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14158]] Temporal Alignment of Time Sensitive Facts with Activation Engineering(https://arxiv.org/abs/2505.14158)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are trained on diverse and often conflicting knowledge spanning multiple domains and time periods. Some of this knowledge is only valid within specific temporal contexts, such as answering the question, "Who is the President of the United States in 2022?" Ensuring LLMs generate time appropriate responses is crucial for maintaining relevance and accuracy. In this work we explore activation engineering as a method for temporally aligning LLMs to improve factual recall without any training or dataset creation. In this research we explore an activation engineering technique to ground three versions of LLaMA 2 to specific points in time and examine the effects of varying injection layers and prompting strategies. Our experiments demonstrate up to a 44% and 16% improvement in relative and explicit prompting respectively, achieving comparable performance to the fine-tuning method proposed by Zhao et al. (2024) . Notably, our approach achieves similar results to the fine-tuning baseline while being significantly more computationally efficient and requiring no pre-aligned datasets.</li>
</ul>

<h3>Title: Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zahraa Al Sahili, Ioannis Patras, Matthew Purver</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14160">https://arxiv.org/abs/2505.14160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14160">https://arxiv.org/pdf/2505.14160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14160]] Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models(https://arxiv.org/abs/2505.14160)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Multilingual vision-language models promise universal image-text retrieval, yet their social biases remain under-explored. We present the first systematic audit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and CAPIVARA-CLIP -- across ten languages that vary in resource availability and grammatical gender. Using balanced subsets of \textsc{FairFace} and the \textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and gender bias and measure stereotype amplification. Contrary to the assumption that multilinguality mitigates bias, every model exhibits stronger gender bias than its English-only baseline. CAPIVARA-CLIP shows its largest biases precisely in the low-resource languages it targets, while the shared cross-lingual encoder of NLLB-CLIP transports English gender stereotypes into gender-neutral languages; loosely coupled encoders largely avoid this transfer. Highly gendered languages consistently magnify all measured bias types, but even gender-neutral languages remain vulnerable when cross-lingual weight sharing imports foreign stereotypes. Aggregated metrics conceal language-specific ``hot spots,'' underscoring the need for fine-grained, language-aware bias evaluation in future multilingual vision-language research.</li>
</ul>

<h3>Title: Personalized Bayesian Federated Learning with Wasserstein Barycenter Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Ting Wei, Biao Mei, Junliang Lyu, Renquan Zhang, Feng Zhou, Yifan Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14161">https://arxiv.org/abs/2505.14161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14161">https://arxiv.org/pdf/2505.14161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14161]] Personalized Bayesian Federated Learning with Wasserstein Barycenter Aggregation(https://arxiv.org/abs/2505.14161)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Personalized Bayesian federated learning (PBFL) handles non-i.i.d. client data and quantifies uncertainty by combining personalization with Bayesian inference. However, existing PBFL methods face two limitations: restrictive parametric assumptions in client posterior inference and naive parameter averaging for server aggregation. To overcome these issues, we propose FedWBA, a novel PBFL method that enhances both local inference and global aggregation. At the client level, we use particle-based variational inference for nonparametric posterior representation. At the server level, we introduce particle-based Wasserstein barycenter aggregation, offering a more geometrically meaningful approach. Theoretically, we provide local and global convergence guarantees for FedWBA. Locally, we prove a KL divergence decrease lower bound per iteration for variational inference convergence. Globally, we show that the Wasserstein barycenter converges to the true parameter as the client data size increases. Empirically, experiments show that FedWBA outperforms baselines in prediction accuracy, uncertainty calibration, and convergence rate, with ablation studies confirming its robustness.</li>
</ul>

<h3>Title: PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore</h3>
<ul>
<li><strong>Authors: </strong>Zhenkai Qin, Jiajing He, Qiao Fang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14165">https://arxiv.org/abs/2505.14165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14165">https://arxiv.org/pdf/2505.14165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14165]] PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore(https://arxiv.org/abs/2505.14165)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity toward specific aspects within a text, enabling more precise opinion mining in domains such as product reviews and social media. However, traditional FGSA approaches often require task-specific architectures and extensive annotated data, limiting their generalization and scalability. To address these challenges, we propose PL-FGSA, a unified prompt learning-based framework implemented using the MindSpore platform, which integrates prompt design with a lightweight TextCNN backbone. Our method reformulates FGSA as a multi-task prompt-augmented generation problem, jointly tackling aspect extraction, sentiment classification, and causal explanation in a unified paradigm. By leveraging prompt-based guidance, PL-FGSA enhances interpretability and achieves strong performance under both full-data and low-resource conditions. Experiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and MAMS-demonstrate that our model consistently outperforms traditional fine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597, respectively. These results validate the effectiveness of prompt-based generalization and highlight the practical value of PL-FGSA for real-world sentiment analysis tasks.</li>
</ul>

<h3>Title: LMP: Leveraging Motion Prior in Zero-Shot Video Generation with Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Changgu Chen, Xiaoyan Yang, Junwei Shu, Changbo Wang, Yang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14167">https://arxiv.org/abs/2505.14167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14167">https://arxiv.org/pdf/2505.14167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14167]] LMP: Leveraging Motion Prior in Zero-Shot Video Generation with Diffusion Transformer(https://arxiv.org/abs/2505.14167)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>In recent years, large-scale pre-trained diffusion transformer models have made significant progress in video generation. While current DiT models can produce high-definition, high-frame-rate, and highly diverse videos, there is a lack of fine-grained control over the video content. Controlling the motion of subjects in videos using only prompts is challenging, especially when it comes to describing complex movements. Further, existing methods fail to control the motion in image-to-video generation, as the subject in the reference image often differs from the subject in the reference video in terms of initial position, size, and shape. To address this, we propose the Leveraging Motion Prior (LMP) framework for zero-shot video generation. Our framework harnesses the powerful generative capabilities of pre-trained diffusion transformers to enable motion in the generated videos to reference user-provided motion videos in both text-to-video and image-to-video generation. To this end, we first introduce a foreground-background disentangle module to distinguish between moving subjects and backgrounds in the reference video, preventing interference in the target video generation. A reweighted motion transfer module is designed to allow the target video to reference the motion from the reference video. To avoid interference from the subject in the reference video, we propose an appearance separation module to suppress the appearance of the reference subject in the target video. We annotate the DAVIS dataset with detailed prompts for our experiments and design evaluation metrics to validate the effectiveness of our method. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in generation quality, prompt-video consistency, and control capability. Our homepage is available at this https URL</li>
</ul>

<h3>Title: The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models</h3>
<ul>
<li><strong>Authors: </strong>Adrian Cosma, Stefan Ruseti, Emilian Radoi, Mihai Dascalu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14172">https://arxiv.org/abs/2505.14172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14172">https://arxiv.org/pdf/2505.14172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14172]] The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models(https://arxiv.org/abs/2505.14172)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite their remarkable progress across diverse domains, Large Language Models (LLMs) consistently fail at simple character-level tasks, such as counting letters in words, due to a fundamental limitation: tokenization. In this work, we frame this limitation as a problem of low mutual information and analyze it in terms of concept emergence. Using a suite of 19 synthetic tasks that isolate character-level reasoning in a controlled setting, we show that such capabilities emerge slowly, suddenly, and only late in training. We further show that percolation-based models of concept emergence explain these patterns, suggesting that learning character composition is not fundamentally different from learning commonsense knowledge. To address this bottleneck, we propose a lightweight architectural modification that significantly improves character-level reasoning while preserving the inductive advantages of subword models. Together, our results bridge low-level perceptual gaps in tokenized LMs and provide a principled framework for understanding and mitigating their structural blind spots. We make our code publicly available.</li>
</ul>

<h3>Title: Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yusuf Denizay Dönder, Derek Hommel, Andrea W Wen-Yi, David Mimno, Unso Eun Seo Jo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14174">https://arxiv.org/abs/2505.14174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14174">https://arxiv.org/pdf/2505.14174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14174]] Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning(https://arxiv.org/abs/2505.14174)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>LLMs are effective at code generation tasks like text-to-SQL, but is it worth the cost? Many state-of-the-art approaches use non-task-specific LLM techniques including Chain-of-Thought (CoT), self-consistency, and fine-tuning. These methods can be costly at inference time, sometimes requiring over a hundred LLM calls with reasoning, incurring average costs of up to \$0.46 per query, while fine-tuning models can cost thousands of dollars. We introduce "N-rep" consistency, a more cost-efficient text-to-SQL approach that achieves similar BIRD benchmark scores as other more expensive methods, at only \$0.039 per query. N-rep leverages multiple representations of the same schema input to mitigate weaknesses in any single representation, making the solution more robust and allowing the use of smaller and cheaper models without any reasoning or fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL approach in its cost range.</li>
</ul>

<h3>Title: Destabilizing Power Grid and Energy Market by Cyberattacks on Smart Inverters</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Hui, Samuel Karumba, Sid Chi-Kin Chau, Mohiuddin Ahmed</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14175">https://arxiv.org/abs/2505.14175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14175">https://arxiv.org/pdf/2505.14175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14175]] Destabilizing Power Grid and Energy Market by Cyberattacks on Smart Inverters(https://arxiv.org/abs/2505.14175)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Cyberattacks on smart inverters and distributed PV are becoming an imminent threat, because of the recent well-documented vulnerabilities and attack incidents. Particularly, the long lifespan of inverter devices, users' oblivion of cybersecurity compliance, and the lack of cyber regulatory frameworks exacerbate the prospect of cyberattacks on smart inverters. As a result, this raises a question -- "do cyberattacks on smart inverters, if orchestrated on a large scale, pose a genuine threat of wide-scale instability to the power grid and energy market"? This paper provides a realistic assessment on the plausibility and impacts of wide-scale power instability caused by cyberattacks on smart inverters. We conduct an in-depth study based on the electricity market data of Australia and the knowledge of practical contingency mechanisms. Our key findings reveal: (1) Despite the possibility of disruption to the grid by cyberattacks on smart inverters, the impact is only significant under careful planning and orchestration. (2) While the grid can assure certain power system security to survive inadvertent contingency events, it is insufficient to defend against savvy attackers who can orchestrate attacks in an adversarial manner. Our data analysis of Australia's electricity grid also reveals that a relatively low percentage of distributed PV would be sufficient to launch an impactful concerted attack on the grid. Our study casts insights on robust strategies for defending the grid in the presence of cyberattacks for places with high penetration of distributed PV.</li>
</ul>

<h3>Title: Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits</h3>
<ul>
<li><strong>Authors: </strong>Xiang Zhang, Juntai Cao, Jiaqi Wei, Yiwei Xu, Chenyu You</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14178">https://arxiv.org/abs/2505.14178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14178">https://arxiv.org/pdf/2505.14178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14178]] Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits(https://arxiv.org/abs/2505.14178)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Tokenization is the first - and often underappreciated - layer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents a theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations.</li>
</ul>

<h3>Title: Enhancing Abstractive Summarization of Scientific Papers Using Structure Information</h3>
<ul>
<li><strong>Authors: </strong>Tong Bao, Heng Zhang, Chengzhi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14179">https://arxiv.org/abs/2505.14179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14179">https://arxiv.org/pdf/2505.14179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14179]] Enhancing Abstractive Summarization of Scientific Papers Using Structure Information(https://arxiv.org/abs/2505.14179)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Abstractive summarization of scientific papers has always been a research focus, yet existing methods face two main challenges. First, most summarization models rely on Encoder-Decoder architectures that treat papers as sequences of words, thus fail to fully capture the structured information inherent in scientific papers. Second, existing research often use keyword mapping or feature engineering to identify the structural information, but these methods struggle with the structural flexibility of scientific papers and lack robustness across different disciplines. To address these challenges, we propose a two-stage abstractive summarization framework that leverages automatic recognition of structural functions within scientific papers. In the first stage, we standardize chapter titles from numerous scientific papers and construct a large-scale dataset for structural function recognition. A classifier is then trained to automatically identify the key structural components (e.g., Background, Methods, Results, Discussion), which provides a foundation for generating more balanced summaries. In the second stage, we employ Longformer to capture rich contextual relationships across sections and generating context-aware summaries. Experiments conducted on two domain-specific scientific paper summarization datasets demonstrate that our method outperforms advanced baselines, and generates more comprehensive summaries. The code and dataset can be accessed at this https URL.</li>
</ul>

<h3>Title: SlangDIT: Benchmarking LLMs in Interpretative Slang Translation</h3>
<ul>
<li><strong>Authors: </strong>Yunlong Liang, Fandong Meng, Jiaan Wang, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14181">https://arxiv.org/abs/2505.14181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14181">https://arxiv.org/pdf/2505.14181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14181]] SlangDIT: Benchmarking LLMs in Interpretative Slang Translation(https://arxiv.org/abs/2505.14181)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The challenge of slang translation lies in capturing context-dependent semantic extensions, as slang terms often convey meanings beyond their literal interpretation. While slang detection, explanation, and translation have been studied as isolated tasks in the era of large language models (LLMs), their intrinsic interdependence remains underexplored. The main reason is lacking of a benchmark where the two tasks can be a prerequisite for the third one, which can facilitate idiomatic translation. In this paper, we introduce the interpretative slang translation task (named SlangDIT) consisting of three sub-tasks: slang detection, cross-lingual slang explanation, and slang translation within the current context, aiming to generate more accurate translation with the help of slang detection and slang explanation. To this end, we construct a SlangDIT dataset, containing over 25k English-Chinese sentence pairs. Each source sentence mentions at least one slang term and is labeled with corresponding cross-lingual slang explanation. Based on the benchmark, we propose a deep thinking model, named SlangOWL. It firstly identifies whether the sentence contains a slang, and then judges whether the slang is polysemous and analyze its possible meaning. Further, the SlangOWL provides the best explanation of the slang term targeting on the current context. Finally, according to the whole thought, the SlangOWL offers a suitable translation. Our experiments on LLMs (\emph{e.g.}, Qwen2.5 and LLama-3.1), show that our deep thinking approach indeed enhances the performance of LLMs where the proposed SLangOWL significantly surpasses the vanilla models and supervised fine-tuned models without thinking.</li>
</ul>

<h3>Title: Safety Subspaces are Not Distinct: A Fine-Tuning Case Study</h3>
<ul>
<li><strong>Authors: </strong>Kaustubh Ponkshe, Shaan Shah, Raghav Singhal, Praneeth Vepakomma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14185">https://arxiv.org/abs/2505.14185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14185">https://arxiv.org/pdf/2505.14185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14185]] Safety Subspaces are Not Distinct: A Fine-Tuning Case Study(https://arxiv.org/abs/2505.14185)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) rely on safety alignment to produce socially acceptable responses. This is typically achieved through instruction tuning and reinforcement learning from human feedback. However, this alignment is known to be brittle: further fine-tuning, even on benign or lightly contaminated data, can degrade safety and reintroduce harmful behaviors. A growing body of work suggests that alignment may correspond to identifiable geometric directions in weight space, forming subspaces that could, in principle, be isolated or preserved to defend against misalignment. In this work, we conduct a comprehensive empirical study of this geometric perspective. We examine whether safety-relevant behavior is concentrated in specific subspaces, whether it can be separated from general-purpose learning, and whether harmfulness arises from distinguishable patterns in internal representations. Across both parameter and activation space, our findings are consistent: subspaces that amplify safe behaviors also amplify unsafe ones, and prompts with different safety implications activate overlapping representations. We find no evidence of a subspace that selectively governs safety. These results challenge the assumption that alignment is geometrically localized. Rather than residing in distinct directions, safety appears to emerge from entangled, high-impact components of the model's broader learning dynamics. This suggests that subspace-based defenses may face fundamental limitations and underscores the need for alternative strategies to preserve alignment under continued training. We corroborate these findings through multiple experiments on five open-source LLMs. Our code is publicly available at: this https URL.</li>
</ul>

<h3>Title: $α$-GAN by Rényi Cross Entropy</h3>
<ul>
<li><strong>Authors: </strong>Ni Ding, Miao Qiao, Jiaxing Xu, Yiping Ke, Xiaoyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14190">https://arxiv.org/abs/2505.14190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14190">https://arxiv.org/pdf/2505.14190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14190]] $α$-GAN by Rényi Cross Entropy(https://arxiv.org/abs/2505.14190)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper proposes $\alpha$-GAN, a generative adversarial network using Rényi measures. The value function is formulated, by Rényi cross entropy, as an expected certainty measure incurred by the discriminator's soft decision as to where the sample is from, true population or the generator. The discriminator tries to maximize the Rényi certainty about sample source, while the generator wants to reduce it by injecting fake samples. This forms a min-max problem with the solution parameterized by the Rényi order $\alpha$. This $\alpha$-GAN reduces to vanilla GAN at $\alpha = 1$, where the value function is exactly the binary cross entropy. The optimization of $\alpha$-GAN is over probability (vector) space. It is shown that the gradient is exponentially enlarged when Rényi order is in the range $\alpha \in (0,1)$. This makes convergence faster, which is verified by experimental results. A discussion shows that choosing $\alpha \in (0,1)$ may be able to solve some common problems, e.g., vanishing gradient. A following observation reveals that this range has not been fully explored in the existing Rényi version GANs.</li>
</ul>

<h3>Title: Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification</h3>
<ul>
<li><strong>Authors: </strong>Tuc Nguyen, Yifan Hu, Thai Le</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14195">https://arxiv.org/abs/2505.14195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14195">https://arxiv.org/pdf/2505.14195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14195]] Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification(https://arxiv.org/abs/2505.14195)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have been fueled by large scale training corpora drawn from diverse sources such as websites, news articles, and books. These datasets often contain explicit user information, such as person names and addresses, that LLMs may unintentionally reproduce in their generated outputs. Beyond such explicit content, LLMs can also leak identity revealing cues through implicit signals such as distinctive writing styles, raising significant concerns about authorship privacy. There are three major automated tasks in authorship privacy, namely authorship obfuscation (AO), authorship mimicking (AM), and authorship verification (AV). Prior research has studied AO, AM, and AV independently. However, their interplays remain under explored, which leaves a major research gap, especially in the era of LLMs, where they are profoundly shaping how we curate and share user generated content, and the distinction between machine generated and human authored text is also increasingly blurred. This work then presents the first unified framework for analyzing the dynamic relationships among LLM enabled AO, AM, and AV in the context of authorship privacy. We quantify how they interact with each other to transform human authored text, examining effects at a single point in time and iteratively over time. We also examine the role of demographic metadata, such as gender, academic background, in modulating their performances, inter-task dynamics, and privacy risks. All source code will be publicly available.</li>
</ul>

<h3>Title: Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and GRPO-based Method</h3>
<ul>
<li><strong>Authors: </strong>Xinshen Zhang, Zhen Ye, Xu Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14197">https://arxiv.org/abs/2505.14197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14197">https://arxiv.org/pdf/2505.14197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14197]] Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and GRPO-based Method(https://arxiv.org/abs/2505.14197)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Omnidirectional images (ODIs), with their 360° field of view, provide unparalleled spatial awareness for immersive applications like augmented reality and embodied AI. However, the capability of existing multi-modal large language models (MLLMs) to comprehend and reason about such panoramic scenes remains underexplored. This paper addresses this gap by introducing OmniVQA, the first dataset and conducting the first benchmark for omnidirectional visual question answering. Our evaluation of state-of-the-art MLLMs reveals significant limitations in handling omnidirectional visual question answering, highlighting persistent challenges in object localization, feature extraction, and hallucination suppression within panoramic contexts. These results underscore the disconnect between current MLLM capabilities and the demands of omnidirectional visual understanding, which calls for dedicated architectural or training innovations tailored to 360° imagery. Building on the OmniVQA dataset and benchmark, we further introduce a rule-based reinforcement learning method, 360-R1, based on Qwen2.5-VL-Instruct. Concretely, we modify the group relative policy optimization (GRPO) by proposing three novel reward functions: (1) reasoning process similarity reward, (2) answer semantic accuracy reward, and (3) structured format compliance reward. Extensive experiments on our OmniVQA demonstrate the superiority of our proposed method in omnidirectional space (+6% improvement).</li>
</ul>

<h3>Title: FLASH-D: FlashAttention with Hidden Softmax Division</h3>
<ul>
<li><strong>Authors: </strong>Kosmas Alexandridis, Vasileios Titopoulos, Giorgos Dimitrakopoulos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14201">https://arxiv.org/abs/2505.14201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14201">https://arxiv.org/pdf/2505.14201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14201]] FLASH-D: FlashAttention with Hidden Softmax Division(https://arxiv.org/abs/2505.14201)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The transformer's attention mechanism has revolutionized AI and machine learning, with its efficient computation being crucial to its performance. However, calculating attention involves matrix operations interspersed with softmax rescaling, which inherently slows down computation and requires processing the entire input sequence. Building on online softmax computation, FlashAttention integrates softmax calculation with matrix arithmetic, enabling tiled computation independent of sequence length. While optimized for GPUs, FlashAttention's simplicity makes it amenable to direct hardware acceleration. This work re-evaluates the core FlashAttention kernel, presenting FLASH-D a mathematically equivalent, yet simplified, formulation that achieves: (a) hiding softmax division within other non-linear function evaluations; (b) inherently numerically stable computation of exponentials, eliminating the need for maximum value subtraction; and (c) a reduction in computational cost without introducing numerical approximations to the FlashAttention kernel. Importantly, the essential FlashAttention properties that facilitate efficient tiled implementation are fully preserved. Hardware implementation results at 28nm demonstrate that this proposed formulation achieves a 22.8% reduction in area and a 20.3% reduction in power, on average, compared to state-of-the-art parallel hardware architectures without any performance penalty.</li>
</ul>

<h3>Title: MSDformer: Multi-scale Discrete Transformer For Time Series Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhicheng Chen, Shibo Feng, Xi Xiao, Zhong Zhang, Qing Li, Xingyu Gao, Peilin Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14202">https://arxiv.org/abs/2505.14202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14202">https://arxiv.org/pdf/2505.14202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14202]] MSDformer: Multi-scale Discrete Transformer For Time Series Generation(https://arxiv.org/abs/2505.14202)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Discrete Token Modeling (DTM), which employs vector quantization techniques, has demonstrated remarkable success in modeling non-natural language modalities, particularly in time series generation. While our prior work SDformer established the first DTM-based framework to achieve state-of-the-art performance in this domain, two critical limitations persist in existing DTM approaches: 1) their inability to capture multi-scale temporal patterns inherent to complex time series data, and 2) the absence of theoretical foundations to guide model optimization. To address these challenges, we proposes a novel multi-scale DTM-based time series generation method, called Multi-Scale Discrete Transformer (MSDformer). MSDformer employs a multi-scale time series tokenizer to learn discrete token representations at multiple scales, which jointly characterize the complex nature of time series data. Subsequently, MSDformer applies a multi-scale autoregressive token modeling technique to capture the multi-scale patterns of time series within the discrete latent space. Theoretically, we validate the effectiveness of the DTM method and the rationality of MSDformer through the rate-distortion theorem. Comprehensive experiments demonstrate that MSDformer significantly outperforms state-of-the-art methods. Both theoretical analysis and experimental results demonstrate that incorporating multi-scale information and modeling multi-scale patterns can substantially enhance the quality of generated time series in DTM-based approaches. The code will be released upon acceptance.</li>
</ul>

<h3>Title: Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data</h3>
<ul>
<li><strong>Authors: </strong>Flavio Di Martino, Franca Delmastro</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14206">https://arxiv.org/abs/2505.14206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14206">https://arxiv.org/pdf/2505.14206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14206]] Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data(https://arxiv.org/abs/2505.14206)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, fair, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The widespread adoption of mobile sensors has the potential to provide massive and heterogeneous time series data, driving Artificial Intelligence applications in mHealth. However, data collection remains limited due to stringent ethical regulations, privacy concerns, and other constraints, hindering progress in the field. Synthetic data generation, particularly through Generative Adversarial Networks and Diffusion Models, has emerged as a promising solution to address both data scarcity and privacy issues. Yet, these models are often limited to short-term, unimodal signal patterns. This paper presents a systematic evaluation of state-of-the-art generative models for time series synthesis, with a focus on their ability to jointly handle multi-modality, long-range dependencies, and conditional generation-key challenges in the mHealth domain. To ensure a fair comparison, we introduce a novel evaluation framework designed to measure both the intrinsic quality of synthetic data and its utility in downstream predictive tasks. Our findings reveal critical limitations in the existing approaches, particularly in maintaining cross-modal consistency, preserving temporal coherence, and ensuring robust performance in train-on-synthetic, test-on-real, and data augmentation scenarios. Finally, we present our future research directions to enhance synthetic time series generation and improve the applicability of generative models in mHealth.</li>
</ul>

<h3>Title: Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks</h3>
<ul>
<li><strong>Authors: </strong>Sizhe Yuen, Ting Su, Ziyang Wang, Yali Du, Adam J. Sobey</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14212">https://arxiv.org/abs/2505.14212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14212">https://arxiv.org/pdf/2505.14212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14212]] Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks(https://arxiv.org/abs/2505.14212)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A question-answering (QA) system is to search suitable answers within a knowledge base. Current QA systems struggle with queries requiring complex reasoning or real-time knowledge integration. They are often supplemented with retrieval techniques on a data source such as Retrieval-Augmented Generation (RAG). However, RAG continues to face challenges in handling complex reasoning and logical connections between multiple sources of information. A novel approach for enhancing Large Language Models (LLMs) in knowledge-intensive QA tasks is presented through the automated generation of context-based QA pairs. This methodology leverages LLMs to create fine-tuning data, reducing reliance on human labelling and improving model comprehension and reasoning capabilities. The proposed system includes an automated QA generator and a model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore. Comprehensive experiments demonstrate improvements in logical coherence and factual accuracy, with implications for developing adaptable Artificial Intelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1, BLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA pairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA pairs.</li>
</ul>

<h3>Title: Regularized least squares learning with heavy-tailed noise is minimax optimal</h3>
<ul>
<li><strong>Authors: </strong>Mattes Mollenhauer, Nicole Mücke, Dimitri Meunier, Arthur Gretton</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14214">https://arxiv.org/abs/2505.14214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14214">https://arxiv.org/pdf/2505.14214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14214]] Regularized least squares learning with heavy-tailed noise is minimax optimal(https://arxiv.org/abs/2505.14214)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper examines the performance of ridge regression in reproducing kernel Hilbert spaces in the presence of noise that exhibits a finite number of higher moments. We establish excess risk bounds consisting of subgaussian and polynomial terms based on the well known integral operator framework. The dominant subgaussian component allows to achieve convergence rates that have previously only been derived under subexponential noise - a prevalent assumption in related work from the last two decades. These rates are optimal under standard eigenvalue decay conditions, demonstrating the asymptotic robustness of regularized least squares against heavy-tailed noise. Our derivations are based on a Fuk-Nagaev inequality for Hilbert-space valued random variables.</li>
</ul>

<h3>Title: Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned</h3>
<ul>
<li><strong>Authors: </strong>Jorge Fabila, Lidia Garrucho, Víctor M. Campello, Carlos Martín-Isla, Karim Lekadir</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14217">https://arxiv.org/abs/2505.14217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14217">https://arxiv.org/pdf/2505.14217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14217]] Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned(https://arxiv.org/abs/2505.14217)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>This study explores the use of Federated Learning (FL) for tuberculosis (TB) diagnosis using chest X-rays in low-resource settings across Africa. FL allows hospitals to collaboratively train AI models without sharing raw patient data, addressing privacy concerns and data scarcity that hinder traditional centralized models. The research involved hospitals and research centers in eight African countries. Most sites used local datasets, while Ghana and The Gambia used public ones. The study compared locally trained models with a federated model built across all institutions to evaluate FL's real-world feasibility. Despite its promise, implementing FL in sub-Saharan Africa faces challenges such as poor infrastructure, unreliable internet, limited digital literacy, and weak AI regulations. Some institutions were also reluctant to share model updates due to data control concerns. In conclusion, FL shows strong potential for enabling AI-driven healthcare in underserved regions, but broader adoption will require improvements in infrastructure, education, and regulatory support.</li>
</ul>

<h3>Title: Flexible-weighted Chamfer Distance: Enhanced Objective Function for Point Cloud Completion</h3>
<ul>
<li><strong>Authors: </strong>Jie Li, Shengwei Tian, Long Yu, Xin Ning</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14218">https://arxiv.org/abs/2505.14218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14218">https://arxiv.org/pdf/2505.14218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14218]] Flexible-weighted Chamfer Distance: Enhanced Objective Function for Point Cloud Completion(https://arxiv.org/abs/2505.14218)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Chamfer Distance (CD) comprises two components that can evaluate the global distribution and local performance of generated point clouds, making it widely utilized as a similarity measure between generated and target point clouds in point cloud completion tasks. Additionally, CD's computational efficiency has led to its frequent application as an objective function for guiding point cloud generation. However, using CD directly as an objective function with fixed equal weights for its two components can often result in seemingly high overall performance (i.e., low CD score), while failing to achieve a good global distribution. This is typically reflected in high Earth Mover's Distance (EMD) and Decomposed Chamfer Distance (DCD) scores, alongside poor human assessments. To address this issue, we propose a Flexible-Weighted Chamfer Distance (FCD) to guide point cloud generation. FCD assigns a higher weight to the global distribution component of CD and incorporates a flexible weighting strategy to adjust the balance between the two components, aiming to improve global distribution while maintaining robust overall performance. Experimental results on two state-of-the-art networks demonstrate that our method achieves superior results across multiple evaluation metrics, including CD, EMD, DCD, and F-Score, as well as in human evaluations.</li>
</ul>

<h3>Title: "Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs</h3>
<ul>
<li><strong>Authors: </strong>Darpan Aswal, Siddharth D Jaiswal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14226">https://arxiv.org/abs/2505.14226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14226">https://arxiv.org/pdf/2505.14226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14226]] "Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs(https://arxiv.org/abs/2505.14226)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, fair, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become increasingly powerful, with multilingual and multimodal capabilities improving by the day. These models are being evaluated through audits, alignment studies and red-teaming efforts to expose model vulnerabilities towards generating harmful, biased and unfair content. Existing red-teaming efforts have previously focused on the English language, using fixed template-based attacks; thus, models continue to be susceptible to multilingual jailbreaking strategies, especially in the multimodal context. In this study, we introduce a novel strategy that leverages code-mixing and phonetic perturbations to jailbreak LLMs for both text and image generation tasks. We also introduce two new jailbreak strategies that show higher effectiveness than baseline strategies. Our work presents a method to effectively bypass safety filters in LLMs while maintaining interpretability by applying phonetic misspellings to sensitive words in code-mixed prompts. Our novel prompts achieve a 99% Attack Success Rate for text generation and 78% for image generation, with Attack Relevance Rate of 100% for text generation and 95% for image generation when using the phonetically perturbed code-mixed prompts. Our interpretability experiments reveal that phonetic perturbations impact word tokenization, leading to jailbreak success. Our study motivates increasing the focus towards more generalizable safety alignment for multilingual multimodal models, especially in real-world settings wherein prompts can have misspelt words.</li>
</ul>

<h3>Title: UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Sule Bai, Mingxing Li, Yong Liu, Jing Tang, Haoji Zhang, Lei Sun, Xiangxiang Chu, Yansong Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14231">https://arxiv.org/abs/2505.14231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14231">https://arxiv.org/pdf/2505.14231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14231]] UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning(https://arxiv.org/abs/2505.14231)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Traditional visual grounding methods primarily focus on single-image scenarios with simple textual references. However, extending these methods to real-world scenarios that involve implicit and complex instructions, particularly in conjunction with multiple images, poses significant challenges, which is mainly due to the lack of advanced reasoning ability across diverse multi-modal contexts. In this work, we aim to address the more practical universal grounding task, and propose UniVG-R1, a reasoning guided multimodal large language model (MLLM) for universal visual grounding, which enhances reasoning capabilities through reinforcement learning (RL) combined with cold-start data. Specifically, we first construct a high-quality Chain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning chains, to guide the model towards correct reasoning paths via supervised fine-tuning. Subsequently, we perform rule-based reinforcement learning to encourage the model to identify correct reasoning chains, thereby incentivizing its reasoning capabilities. In addition, we identify a difficulty bias arising from the prevalence of easy samples as RL training progresses, and we propose a difficulty-aware weight adjustment strategy to further strengthen the performance. Experimental results demonstrate the effectiveness of UniVG-R1, which achieves state-of-the-art performance on MIG-Bench with a 9.1% improvement over the previous method. Furthermore, our model exhibits strong generalizability, achieving an average improvement of 23.4% in zero-shot performance across four image and video reasoning grounding benchmarks. The project page can be accessed at this https URL.</li>
</ul>

<h3>Title: Mechanistic Fine-tuning for In-context Learning</h3>
<ul>
<li><strong>Authors: </strong>Hakaze Cho, Peng Luo, Mariko Kato, Rin Kaenbyou, Naoya Inoue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14233">https://arxiv.org/abs/2505.14233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14233">https://arxiv.org/pdf/2505.14233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14233]] Mechanistic Fine-tuning for In-context Learning(https://arxiv.org/abs/2505.14233)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>In-context Learning (ICL) utilizes structured demonstration-query inputs to induce few-shot learning on Language Models (LMs), which are not originally pre-trained on ICL-style data. To bridge the gap between ICL and pre-training, some approaches fine-tune LMs on large ICL-style datasets by an end-to-end paradigm with massive computational costs. To reduce such costs, in this paper, we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous findings on the inner mechanism of ICL, building training objectives on the attention scores instead of the final outputs, to force the attention scores to focus on the correct label tokens presented in the context and mitigate attention scores from the wrong label tokens. Our experiments on 9 modern LMs and 8 datasets empirically find that ABFT outperforms in performance, robustness, unbiasedness, and efficiency, with only around 0.01% data cost compared to the previous methods. Moreover, our subsequent analysis finds that the end-to-end training objective contains the ABFT objective, suggesting the implicit bias of ICL-style data to the emergence of induction heads. Our work demonstrates the possibility of controlling specific module sequences within LMs to improve their behavior, opening up the future application of mechanistic interpretability.</li>
</ul>

<h3>Title: Fast and close Shannon entropy approximation</h3>
<ul>
<li><strong>Authors: </strong>Illia Horenko, Davide Bassetti, Lukáš Pospíšil</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14234">https://arxiv.org/abs/2505.14234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14234">https://arxiv.org/pdf/2505.14234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14234]] Fast and close Shannon entropy approximation(https://arxiv.org/abs/2505.14234)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy are key components in many tools used in physics, information theory, machine learning (ML) and quantum computing. Besides of the significant amounts of SE computations required in these fields, the singularity of the SE gradient is one of the central mathematical reason inducing the high cost, frequently low robustness and slow convergence of such tools. Here we propose the Fast Entropy Approximation (FEA) - a non-singular rational approximation of Shannon entropy and its gradient that achieves a mean absolute error of $10^{-3}$, which is approximately $20$ times lower than comparable state-of-the-art methods. FEA allows around $50\%$ faster computation, requiring only $5$ to $6$ elementary computational operations, as compared to tens of elementary operations behind the fastest entropy computation algorithms with table look-ups, bitshifts, or series approximations. On a set of common benchmarks for the feature selection problem in machine learning, we show that the combined effect of fewer elementary operations, low approximation error, and a non-singular gradient allows significantly better model quality and enables ML feature extraction that is two to three orders of magnitude faster and computationally cheaper when incorporating FEA into AI tools.</li>
</ul>

<h3>Title: ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Raghav Singhal, Kaustubh Ponkshe, Rohit Vartak, Praneeth Vepakomma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14238">https://arxiv.org/abs/2505.14238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14238">https://arxiv.org/pdf/2505.14238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14238]] ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models(https://arxiv.org/abs/2505.14238)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models have demonstrated strong performance across a wide range of tasks, but adapting them efficiently to new domains remains a key challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by introducing lightweight, trainable modules while keeping most pre-trained weights fixed. The prevailing approach, LoRA, models updates using a low-rank decomposition, but its expressivity is inherently constrained by the rank. Recent methods like HiRA aim to increase expressivity by incorporating a Hadamard product with the frozen weights, but still rely on the structure of the pre-trained model. We introduce ABBA, a new PEFT architecture that reparameterizes the update as a Hadamard product of two independently learnable low-rank matrices. In contrast to prior work, ABBA fully decouples the update from the pre-trained weights, enabling both components to be optimized freely. This leads to significantly higher expressivity under the same parameter budget. We formally analyze ABBA's expressive capacity and validate its advantages through matrix reconstruction experiments. Empirically, ABBA achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, consistently outperforming existing PEFT methods by a significant margin across multiple models. Our code is publicly available at: this https URL.</li>
</ul>

<h3>Title: Decoupling Classifier for Boosting Few-shot Object Detection and Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Bin-Bin Gao, Xiaochen Chen, Zhongyi Huang, Congchong Nie, Jun Liu, Jinxiang Lai, Guannan Jiang, Xi Wang, Chengjie Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14239">https://arxiv.org/abs/2505.14239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14239">https://arxiv.org/pdf/2505.14239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14239]] Decoupling Classifier for Boosting Few-shot Object Detection and Instance Segmentation(https://arxiv.org/abs/2505.14239)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper focus on few-shot object detection~(FSOD) and instance segmentation~(FSIS), which requires a model to quickly adapt to novel classes with a few labeled instances. The existing methods severely suffer from bias classification because of the missing label issue which naturally exists in an instance-level few-shot scenario and is first formally proposed by us. Our analysis suggests that the standard classification head of most FSOD or FSIS models needs to be decoupled to mitigate the bias classification. Therefore, we propose an embarrassingly simple but effective method that decouples the standard classifier into two heads. Then, these two individual heads are capable of independently addressing clear positive samples and noisy negative samples which are caused by the missing label. In this way, the model can effectively learn novel classes while mitigating the effects of noisy negative samples. Without bells and whistles, our model without any additional computation cost and parameters consistently outperforms its baseline and state-of-the-art by a large margin on PASCAL VOC and MS-COCO benchmarks for FSOD and FSIS tasks. The Code is available at this https URL.</li>
</ul>

<h3>Title: TransBench: Benchmarking Machine Translation for Industrial-Scale Applications</h3>
<ul>
<li><strong>Authors: </strong>Haijun Li, Tianqi Shi, Zifu Shang, Yuxuan Han, Xueyu Zhao, Hao Wang, Yu Qian, Zhiqiang Qian, Linlong Xu, Minghao Wu, Chenyang Lyu, Longyue Wang, Gongbo Tang, Weihua Luo, Zhao Xu, Kaifu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14244">https://arxiv.org/abs/2505.14244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14244">https://arxiv.org/pdf/2505.14244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14244]] TransBench: Benchmarking Machine Translation for Industrial-Scale Applications(https://arxiv.org/abs/2505.14244)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Machine translation (MT) has become indispensable for cross-border communication in globalized industries like e-commerce, finance, and legal services, with recent advancements in large language models (LLMs) significantly enhancing translation quality. However, applying general-purpose MT models to industrial scenarios reveals critical limitations due to domain-specific terminology, cultural nuances, and stylistic conventions absent in generic benchmarks. Existing evaluation frameworks inadequately assess performance in specialized contexts, creating a gap between academic benchmarks and real-world efficacy. To address this, we propose a three-level translation capability framework: (1) Basic Linguistic Competence, (2) Domain-Specific Proficiency, and (3) Cultural Adaptation, emphasizing the need for holistic evaluation across these dimensions. We introduce TransBench, a benchmark tailored for industrial MT, initially targeting international e-commerce with 17,000 professionally translated sentences spanning 4 main scenarios and 33 language pairs. TransBench integrates traditional metrics (BLEU, TER) with Marco-MOS, a domain-specific evaluation model, and provides guidelines for reproducible benchmark construction. Our contributions include: (1) a structured framework for industrial MT evaluation, (2) the first publicly available benchmark for e-commerce translation, (3) novel metrics probing multi-level translation quality, and (4) open-sourced evaluation tools. This work bridges the evaluation gap, enabling researchers and practitioners to systematically assess and enhance MT systems for industry-specific needs.</li>
</ul>

<h3>Title: Visual Agentic Reinforcement Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14246">https://arxiv.org/abs/2505.14246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14246">https://arxiv.org/pdf/2505.14246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14246]] Visual Agentic Reinforcement Fine-Tuning(https://arxiv.org/abs/2505.14246)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native agentic ability to use external tools such as web browsers for searching and writing/executing code for image manipulation to think with images. In the open-source research community, while significant progress has been made in language-only agentic abilities such as function calling and tool integration, the development of multi-modal agentic capabilities that involve truly thinking with images, and their corresponding benchmarks, are still less explored. This work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the ability to browse websites for real-time information updates and write code to manipulate and analyze input images through cropping, rotation, and other image processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT) with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs' agentic search and coding abilities. Our experimental results demonstrate that Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and +10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities. Our findings suggest that Visual-ARFT offers a promising path toward building robust and generalizable multimodal agents.</li>
</ul>

<h3>Title: A Private Approximation of the 2nd-Moment Matrix of Any Subsamplable Input</h3>
<ul>
<li><strong>Authors: </strong>Bar Mahpud, Or Sheffet</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14251">https://arxiv.org/abs/2505.14251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14251">https://arxiv.org/pdf/2505.14251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14251]] A Private Approximation of the 2nd-Moment Matrix of Any Subsamplable Input(https://arxiv.org/abs/2505.14251)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We study the problem of differentially private second moment estimation and present a new algorithm that achieve strong privacy-utility trade-offs even for worst-case inputs under subsamplability assumptions on the data. We call an input $(m,\alpha,\beta)$-subsamplable if a random subsample of size $m$ (or larger) preserves w.p $\geq 1-\beta$ the spectral structure of the original second moment matrix up to a multiplicative factor of $1\pm \alpha$. Building upon subsamplability, we give a recursive algorithmic framework similar to Kamath et al 2019, that abides zero-Concentrated Differential Privacy (zCDP) while preserving w.h.p. the accuracy of the second moment estimation upto an arbitrary factor of $(1\pm\gamma)$. We then show how to apply our algorithm to approximate the second moment matrix of a distribution $\mathcal{D}$, even when a noticeable fraction of the input are outliers.</li>
</ul>

<h3>Title: Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Mouad Elaarabi, Domenico Borzacchiello, Philippe Le Bot, Nathan Lauzeral, Sebastien Comas-Cardona</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14252">https://arxiv.org/abs/2505.14252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14252">https://arxiv.org/pdf/2505.14252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14252]] Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks(https://arxiv.org/abs/2505.14252)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this work, we explore the integration of Sequence Encoding for Online Parameter Identification with Physics-Informed Neural Networks to create a model that, once trained, can be utilized for real time applications with variable parameters, boundary conditions, and initial conditions. Recently, the combination of PINNs with Sparse Regression has emerged as a method for performing dynamical system identification through supervised learning and sparse regression optimization, while also solving the dynamics using PINNs. However, this approach can be limited by variations in parameters or boundary and initial conditions, requiring retraining of the model whenever changes occur. In this work, we introduce an architecture that employs Deep Sets or Sequence Encoders to encode dynamic parameters, boundary conditions, and initial conditions, using these encoded features as inputs for the PINN, enabling the model to adapt to changes in parameters, BCs, and ICs. We apply this approach to three different problems. First, we analyze the Rossler ODE system, demonstrating the robustness of the model with respect to noise and its ability to generalize. Next, we explore the model's capability in a 2D Navier-Stokes PDE problem involving flow past a cylinder with a parametric sinusoidal inlet velocity function, showing that the model can encode pressure data from a few points to identify the inlet velocity profile and utilize physics to compute velocity and pressure throughout the domain. Finally, we address a 1D heat monitoring problem using real data from the heating of glass fiber and thermoplastic composite plates.</li>
</ul>

<h3>Title: Instructing Text-to-Image Diffusion Models via Classifier-Guided Semantic Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yuanyuan Chang, Yinghua Yao, Tao Qin, Mengmeng Wang, Ivor Tsang, Guang Dai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14254">https://arxiv.org/abs/2505.14254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14254">https://arxiv.org/pdf/2505.14254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14254]] Instructing Text-to-Image Diffusion Models via Classifier-Guided Semantic Optimization(https://arxiv.org/abs/2505.14254)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have emerged as powerful tools for high-quality image generation and editing. Many existing approaches rely on text prompts as editing guidance. However, these methods are constrained by the need for manual prompt crafting, which can be time-consuming, introduce irrelevant details, and significantly limit editing performance. In this work, we propose optimizing semantic embeddings guided by attribute classifiers to steer text-to-image models toward desired edits, without relying on text prompts or requiring any training or fine-tuning of the diffusion model. We utilize classifiers to learn precise semantic embeddings at the dataset level. The learned embeddings are theoretically justified as the optimal representation of attribute semantics, enabling disentangled and accurate edits. Experiments further demonstrate that our method achieves high levels of disentanglement and strong generalization across different domains of data.</li>
</ul>

<h3>Title: FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Shaolin Zhu, Tianyu Dong, Bo Li, Deyi Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14256">https://arxiv.org/abs/2505.14256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14256">https://arxiv.org/pdf/2505.14256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14256]] FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation(https://arxiv.org/abs/2505.14256)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we present FuxiMT, a novel Chinese-centric multilingual machine translation model powered by a sparsified large language model (LLM). We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on a massive Chinese corpus and then conduct multilingual fine-tuning on a large parallel dataset encompassing 65 languages. FuxiMT incorporates Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust performance across various resource levels. Experimental results demonstrate that FuxiMT significantly outperforms strong baselines, including state-of-the-art LLMs and machine translation models, particularly under low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot translation capabilities for unseen language pairs, indicating its potential to bridge communication gaps where parallel data are scarce or unavailable.</li>
</ul>

<h3>Title: Speculative Decoding Reimagined for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Luxi Lin, Zhihang Lin, Zhanpeng Zeng, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14260">https://arxiv.org/abs/2505.14260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14260">https://arxiv.org/pdf/2505.14260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14260]] Speculative Decoding Reimagined for Multimodal Large Language Models(https://arxiv.org/abs/2505.14260)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces Multimodal Speculative Decoding (MSD) to accelerate Multimodal Large Language Models (MLLMs) inference. Speculative decoding has been shown to accelerate Large Language Models (LLMs) without sacrificing accuracy. However, current speculative decoding methods for MLLMs fail to achieve the same speedup as they do for LLMs. To address this, we reimagine speculative decoding specifically for MLLMs. Our analysis of MLLM characteristics reveals two key design principles for MSD: (1) Text and visual tokens have fundamentally different characteristics and need to be processed separately during drafting. (2) Both language modeling ability and visual perception capability are crucial for the draft model. For the first principle, MSD decouples text and visual tokens in the draft model, allowing each to be handled based on its own characteristics. For the second principle, MSD uses a two-stage training strategy: In stage one, the draft model is trained on text-only instruction-tuning datasets to improve its language modeling ability. In stage two, MSD gradually introduces multimodal data to enhance the visual perception capability of the draft model. Experiments show that MSD boosts inference speed by up to $2.29\times$ for LLaVA-1.5-7B and up to $2.46\times$ for LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness. Our code is available at this https URL.</li>
</ul>

<h3>Title: AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum</h3>
<ul>
<li><strong>Authors: </strong>Jian Xiong, Jingbo Zhou, Jingyong Ye, Dejing Dou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14264">https://arxiv.org/abs/2505.14264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14264">https://arxiv.org/pdf/2505.14264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14264]] AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum(https://arxiv.org/abs/2505.14264)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has emerged as an effective approach for enhancing the reasoning capabilities of large language models (LLMs), especially in scenarios where supervised fine-tuning (SFT) falls short due to limited chain-of-thought (CoT) data. Among RL-based post-training methods, group relative advantage estimation, as exemplified by Group Relative Policy Optimization (GRPO), has attracted considerable attention for eliminating the dependency on the value model, thereby simplifying training compared to traditional approaches like Proximal Policy Optimization (PPO). However, we observe that exsiting group relative advantage estimation method still suffers from training inefficiencies, particularly when the estimated advantage approaches zero. To address this limitation, we propose Advantage-Augmented Policy Optimization (AAPO), a novel RL algorithm that optimizes the cross-entropy (CE) loss using advantages enhanced through a momentum-based estimation scheme. This approach effectively mitigates the inefficiencies associated with group relative advantage estimation. Experimental results on multiple mathematical reasoning benchmarks demonstrate the superior performance of AAPO.</li>
</ul>

<h3>Title: Think-J: Learning to Think for Generative LLM-as-a-Judge</h3>
<ul>
<li><strong>Authors: </strong>Hui Huang, Yancheng He, Hongli Zhou, Rui Zhang, Wei Liu, Weixun Wang, Wenbo Su, Bo Zheng, Jiaheng Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14268">https://arxiv.org/abs/2505.14268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14268">https://arxiv.org/pdf/2505.14268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14268]] Think-J: Learning to Think for Generative LLM-as-a-Judge(https://arxiv.org/abs/2505.14268)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>LLM-as-a-Judge refers to the automatic modeling of preferences for responses generated by Large Language Models (LLMs), which is of significant importance for both LLM evaluation and reward modeling. Although generative LLMs have made substantial progress in various tasks, their performance as LLM-Judge still falls short of expectations. In this work, we propose Think-J, which improves generative LLM-as-a-Judge by learning how to think. We first utilized a small amount of curated data to develop the model with initial judgment thinking capabilities. Subsequently, we optimize the judgment thinking traces based on reinforcement learning (RL). We propose two methods for judgment thinking optimization, based on offline and online RL, respectively. The offline RL requires training a critic model to construct positive and negative examples for learning. The online method defines rule-based reward as feedback for optimization. Experimental results showed that our approach can significantly enhance the evaluation capability of generative LLM-Judge, surpassing both generative and classifier-based LLM-Judge without requiring extra human annotations.</li>
</ul>

<h3>Title: FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Minh Ngoc Ta, Dong Cao Van, Duc-Anh Hoang, Minh Le-Anh, Truong Nguyen, My Anh Tran Nguyen, Yuxia Wang, Preslav Nakov, Sang Dinh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14271">https://arxiv.org/abs/2505.14271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14271">https://arxiv.org/pdf/2505.14271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14271]] FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning(https://arxiv.org/abs/2505.14271)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>The growing collaboration between humans and AI models in generative tasks has introduced new challenges in distinguishing between human-written, AI-generated, and human-AI collaborative texts. In this work, we collect a multilingual, multi-domain, multi-generator dataset FAIDSet. We further introduce a fine-grained detection framework FAID to classify text into these three categories, meanwhile identifying the underlying AI model family. Unlike existing binary classifiers, FAID is built to capture both authorship and model-specific characteristics. Our method combines multi-level contrastive learning with multi-task auxiliary classification to learn subtle stylistic cues. By modeling AI families as distinct stylistic entities, FAID offers improved interpretability. We incorporate an adaptation to address distributional shifts without retraining for unseen data. Experimental results demonstrate that FAID outperforms several baseline approaches, particularly enhancing the generalization accuracy on unseen domains and new AI models. It provide a potential solution for improving transparency and accountability in AI-assisted writing.</li>
</ul>

<h3>Title: YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Jennifer D'Souza, Hamed Babaei Giglou, Quentin Münch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14279">https://arxiv.org/abs/2505.14279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14279">https://arxiv.org/pdf/2505.14279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14279]] YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering(https://arxiv.org/abs/2505.14279)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) drive scientific question-answering on modern search engines, yet their evaluation robustness remains underexplored. We introduce YESciEval, an open-source framework that combines fine-grained rubric-based assessment with reinforcement learning to mitigate optimism bias in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including adversarial variants, with evaluation scores from multiple LLMs. Independent of proprietary models and human feedback, our approach enables scalable, cost-free evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI alignment and fosters robust, transparent evaluation essential for scientific inquiry and artificial general intelligence.</li>
</ul>

<h3>Title: Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rao Ma, Mengjie Qian, Vyas Raina, Mark Gales, Kate Knill</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14286">https://arxiv.org/abs/2505.14286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14286">https://arxiv.org/pdf/2505.14286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14286]] Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs(https://arxiv.org/abs/2505.14286)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The combination of pre-trained speech encoders with large language models has enabled the development of speech LLMs that can handle a wide range of spoken language processing tasks. While these models are powerful and flexible, this very flexibility may make them more vulnerable to adversarial attacks. To examine the extent of this problem, in this work we investigate universal acoustic adversarial attacks on speech LLMs. Here a fixed, universal, adversarial audio segment is prepended to the original input audio. We initially investigate attacks that cause the model to either produce no output or to perform a modified task overriding the original prompt. We then extend the nature of the attack to be selective so that it activates only when specific input attributes, such as a speaker gender or spoken language, are present. Inputs without the targeted attribute should be unaffected, allowing fine-grained control over the model outputs. Our findings reveal critical vulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar speech LLMs may be susceptible to universal adversarial attacks. This highlights the need for more robust training strategies and improved resistance to adversarial attacks.</li>
</ul>

<h3>Title: Towards Generating Realistic Underwater Images</h3>
<ul>
<li><strong>Authors: </strong>Abdul-Kazeem Shamba</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14296">https://arxiv.org/abs/2505.14296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14296">https://arxiv.org/pdf/2505.14296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14296]] Towards Generating Realistic Underwater Images(https://arxiv.org/abs/2505.14296)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper explores the use of contrastive learning and generative adversarial networks for generating realistic underwater images from synthetic images with uniform lighting. We investigate the performance of image translation models for generating realistic underwater images using the VAROS dataset. Two key evaluation metrics, Fréchet Inception Distance (FID) and Structural Similarity Index Measure (SSIM), provide insights into the trade-offs between perceptual quality and structural preservation. For paired image translation, pix2pix achieves the best FID scores due to its paired supervision and PatchGAN discriminator, while the autoencoder model attains the highest SSIM, suggesting better structural fidelity despite producing blurrier outputs. Among unpaired methods, CycleGAN achieves a competitive FID score by leveraging cycle-consistency loss, whereas CUT, which replaces cycle-consistency with contrastive learning, attains higher SSIM, indicating improved spatial similarity retention. Notably, incorporating depth information into CUT results in the lowest overall FID score, demonstrating that depth cues enhance realism. However, the slight decrease in SSIM suggests that depth-aware learning may introduce structural variations.</li>
</ul>

<h3>Title: Cross-Lingual Optimization for Language Transfer in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jungseob Lee, Seongtae Hong, Hyeonseok Moon, Heuiseok Lim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14297">https://arxiv.org/abs/2505.14297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14297">https://arxiv.org/pdf/2505.14297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14297]] Cross-Lingual Optimization for Language Transfer in Large Language Models(https://arxiv.org/abs/2505.14297)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Adapting large language models to other languages typically employs supervised fine-tuning (SFT) as a standard approach. However, it often suffers from an overemphasis on English performance, a phenomenon that is especially pronounced in data-constrained environments. To overcome these challenges, we propose \textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an English-centric LLM to a target language while preserving its English capabilities. CLO utilizes publicly available English SFT data and a translation model to enable cross-lingual transfer. We conduct experiments using five models on six languages, each possessing varying levels of resource. Our results show that CLO consistently outperforms SFT in both acquiring target language proficiency and maintaining English performance. Remarkably, in low-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400 samples, demonstrating that CLO can achieve better performance with less data. Furthermore, we find that SFT is particularly sensitive to data quantity in medium and low-resource languages, whereas CLO remains robust. Our comprehensive analysis emphasizes the limitations of SFT and incorporates additional training strategies in CLO to enhance efficiency.</li>
</ul>

<h3>Title: Scaling Law for Quantization-Aware Training</h3>
<ul>
<li><strong>Authors: </strong>Mengzhao Chen, Chaoyi Zhang, Jing Liu, Yutao Zeng, Zeyue Xue, Zhiheng Liu, Yunshui Li, Jin Ma, Jie Huang, Xun Zhou, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14302">https://arxiv.org/abs/2505.14302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14302">https://arxiv.org/pdf/2505.14302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14302]] Scaling Law for Quantization-Aware Training(https://arxiv.org/abs/2505.14302)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demand substantial computational and memory resources, creating deployment challenges. Quantization-aware training (QAT) addresses these challenges by reducing model precision while maintaining performance. However, the scaling behavior of QAT, especially at 4-bit precision (W4A4), is not well understood. Existing QAT scaling laws often ignore key factors such as the number of training tokens and quantization granularity, which limits their applicability. This paper proposes a unified scaling law for QAT that models quantization error as a function of model size, training data volume, and quantization group size. Through 268 QAT experiments, we show that quantization error decreases as model size increases, but rises with more training tokens and coarser quantization granularity. To identify the sources of W4A4 quantization error, we decompose it into weight and activation components. Both components follow the overall trend of W4A4 quantization error, but with different sensitivities. Specifically, weight quantization error increases more rapidly with more training tokens. Further analysis shows that the activation quantization error in the FC2 layer, caused by outliers, is the primary bottleneck of W4A4 QAT quantization error. By applying mixed-precision quantization to address this bottleneck, we demonstrate that weight and activation quantization errors can converge to similar levels. Additionally, with more training data, weight quantization error eventually exceeds activation quantization error, suggesting that reducing weight quantization error is also important in such scenarios. These findings offer key insights for improving QAT research and development.</li>
</ul>

<h3>Title: JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling</h3>
<ul>
<li><strong>Authors: </strong>Jinwang Song, Hongying Zan, Kunli Zhang, Lingling Mu, Yingjie Han, Haobo Hua, Min Peng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14305">https://arxiv.org/abs/2505.14305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14305">https://arxiv.org/pdf/2505.14305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14305]] JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling(https://arxiv.org/abs/2505.14305)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Text-to-SQL, which maps natural language to SQL queries, has benefited greatly from recent advances in Large Language Models (LLMs). While LLMs offer various paradigms for this task, including prompting and supervised fine-tuning (SFT), SFT approaches still face challenges such as complex multi-stage pipelines and poor robustness to noisy schema information. To address these limitations, we present JOLT-SQL, a streamlined single-stage SFT framework that jointly optimizes schema linking and SQL generation via a unified loss. JOLT-SQL employs discriminative schema linking, enhanced by local bidirectional attention, alongside a confusion-aware noisy schema sampling strategy with selective attention to improve robustness under noisy schema conditions. Experiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL achieves state-of-the-art execution accuracy among comparable-size open-source models, while significantly improving both training and inference efficiency.</li>
</ul>

<h3>Title: HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing</h3>
<ul>
<li><strong>Authors: </strong>Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Idris Abdulmumin, Falalu Ibrahim Lawan, Babangida Sani, Sukairaj Hafiz Imam, Yusuf Aliyu, Sani Abdullahi Sani, Ali Usman Umar, Kenneth Church, Vukosi Marivate</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14311">https://arxiv.org/abs/2505.14311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14311">https://arxiv.org/pdf/2505.14311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14311]] HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing(https://arxiv.org/abs/2505.14311)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hausa Natural Language Processing (NLP) has gained increasing attention in recent years, yet remains understudied as a low-resource language despite having over 120 million first-language (L1) and 80 million second-language (L2) speakers worldwide. While significant advances have been made in high-resource languages, Hausa NLP faces persistent challenges, including limited open-source datasets and inadequate model representation. This paper presents an overview of the current state of Hausa NLP, systematically examining existing resources, research contributions, and gaps across fundamental NLP tasks: text classification, machine translation, named entity recognition, speech recognition, and question answering. We introduce HausaNLP (this https URL), a curated catalog that aggregates datasets, tools, and research works to enhance accessibility and drive further development. Furthermore, we discuss challenges in integrating Hausa into large language models (LLMs), addressing issues of suboptimal tokenization and dialectal variation. Finally, we propose strategic research directions emphasizing dataset expansion, improved language modeling approaches, and strengthened community collaboration to advance Hausa NLP. Our work provides both a foundation for accelerating Hausa NLP progress and valuable insights for broader multilingual NLP research.</li>
</ul>

<h3>Title: A MIND for Reasoning: Meta-learning for In-context Deduction</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Bertolazzi, Manuel Vargas Guzmán, Raffaella Bernardi, Maciej Malicki, Jakub Szymanik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14313">https://arxiv.org/abs/2505.14313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14313">https://arxiv.org/pdf/2505.14313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14313]] A MIND for Reasoning: Meta-learning for In-context Deduction(https://arxiv.org/abs/2505.14313)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly evaluated on formal tasks, where strong reasoning abilities define the state of the art. However, their ability to generalize to out-of-distribution problems remains limited. In this paper, we investigate how LLMs can achieve a systematic understanding of deductive rules. Our focus is on the task of identifying the appropriate subset of premises within a knowledge base needed to derive a given hypothesis. To tackle this challenge, we propose Meta-learning for In-context Deduction (MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND is to enable models to generalize more effectively to unseen knowledge bases and to systematically apply inference rules. Our results show that MIND significantly improves generalization in small LMs ranging from 1.5B to 7B parameters. The benefits are especially pronounced in smaller models and low-data settings. Remarkably, small models fine-tuned with MIND outperform state-of-the-art LLMs, such as GPT-4o and o3-mini, on this task.</li>
</ul>

<h3>Title: Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion</h3>
<ul>
<li><strong>Authors: </strong>Tiehan Cui, Yanxu Mao, Peipei Liu, Congying Liu, Datao You</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14316">https://arxiv.org/abs/2505.14316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14316">https://arxiv.org/pdf/2505.14316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14316]] Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion(https://arxiv.org/abs/2505.14316)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Although large language models (LLMs) have achieved remarkable advancements, their security remains a pressing concern. One major threat is jailbreak attacks, where adversarial prompts bypass model safeguards to generate harmful or objectionable content. Researchers study jailbreak attacks to understand security and robustness of LLMs. However, existing jailbreak attack methods face two main challenges: (1) an excessive number of iterative queries, and (2) poor generalization across models. In addition, recent jailbreak evaluation datasets focus primarily on question-answering scenarios, lacking attention to text generation tasks that require accurate regeneration of toxic content. To tackle these challenges, we propose two contributions: (1) ICE, a novel black-box jailbreak method that employs Intent Concealment and divErsion to effectively circumvent security constraints. ICE achieves high attack success rates (ASR) with a single query, significantly improving efficiency and transferability across different models. (2) BiSceneEval, a comprehensive dataset designed for assessing LLM robustness in question-answering and text-generation tasks. Experimental results demonstrate that ICE outperforms existing jailbreak techniques, revealing critical vulnerabilities in current defense mechanisms. Our findings underscore the necessity of a hybrid security strategy that integrates predefined security mechanisms with real-time semantic decomposition to enhance the security of LLMs.</li>
</ul>

<h3>Title: RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection</h3>
<ul>
<li><strong>Authors: </strong>Wenjun Hou, Yi Cheng, Kaishuai Xu, Heng Li, Yan Hu, Wenjie Li, Jiang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14318">https://arxiv.org/abs/2505.14318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14318">https://arxiv.org/pdf/2505.14318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14318]] RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection(https://arxiv.org/abs/2505.14318)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities in various domains, including radiology report generation. Previous approaches have attempted to utilize multimodal LLMs for this task, enhancing their performance through the integration of domain-specific knowledge retrieval. However, these approaches often overlook the knowledge already embedded within the LLMs, leading to redundant information integration and inefficient utilization of learned representations. To address this limitation, we propose RADAR, a framework for enhancing radiology report generation with supplementary knowledge injection. RADAR improves report generation by systematically leveraging both the internal knowledge of an LLM and externally retrieved information. Specifically, it first extracts the model's acquired knowledge that aligns with expert image-based classification outputs. It then retrieves relevant supplementary knowledge to further enrich this information. Finally, by aggregating both sources, RADAR generates more accurate and informative radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU X-ray demonstrate that our model outperforms state-of-the-art LLMs in both language quality and clinical accuracy</li>
</ul>

<h3>Title: Accuracy and Fairness of Facial Recognition Technology in Low-Quality Police Images: An Experiment With Synthetic Faces</h3>
<ul>
<li><strong>Authors: </strong>Maria Cuellar, Hon Kiu (James)To, Arush Mehrotra</a></li>
<li><strong>Subjects: </strong>cs.CV, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14320">https://arxiv.org/abs/2505.14320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14320">https://arxiv.org/pdf/2505.14320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14320]] Accuracy and Fairness of Facial Recognition Technology in Low-Quality Police Images: An Experiment With Synthetic Faces(https://arxiv.org/abs/2505.14320)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Facial recognition technology (FRT) is increasingly used in criminal investigations, yet most evaluations of its accuracy rely on high-quality images, unlike those often encountered by law enforcement. This study examines how five common forms of image degradation--contrast, brightness, motion blur, pose shift, and resolution--affect FRT accuracy and fairness across demographic groups. Using synthetic faces generated by StyleGAN3 and labeled with FairFace, we simulate degraded images and evaluate performance using Deepface with ArcFace loss in 1:n identification tasks. We perform an experiment and find that false positive rates peak near baseline image quality, while false negatives increase as degradation intensifies--especially with blur and low resolution. Error rates are consistently higher for women and Black individuals, with Black females most affected. These disparities raise concerns about fairness and reliability when FRT is used in real-world investigative contexts. Nevertheless, even under the most challenging conditions and for the most affected subgroups, FRT accuracy remains substantially higher than that of many traditional forensic methods. This suggests that, if appropriately validated and regulated, FRT should be considered a valuable investigative tool. However, algorithmic accuracy alone is not sufficient: we must also evaluate how FRT is used in practice, including user-driven data manipulation. Such cases underscore the need for transparency and oversight in FRT deployment to ensure both fairness and forensic validity.</li>
</ul>

<h3>Title: Vulnerability of Transfer-Learned Neural Networks to Data Reconstruction Attacks in Small-Data Regime</h3>
<ul>
<li><strong>Authors: </strong>Tomasz Maciążek, Robert Allison</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14323">https://arxiv.org/abs/2505.14323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14323">https://arxiv.org/pdf/2505.14323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14323]] Vulnerability of Transfer-Learned Neural Networks to Data Reconstruction Attacks in Small-Data Regime(https://arxiv.org/abs/2505.14323)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, robust</a></li>
<li><strong>Abstract: </strong>Training data reconstruction attacks enable adversaries to recover portions of a released model's training data. We consider the attacks where a reconstructor neural network learns to invert the (random) mapping between training data and model weights. Prior work has shown that an informed adversary with access to released model's weights and all but one training data point can achieve high-quality reconstructions in this way. However, differential privacy can defend against such an attack with little to no loss in model's utility when the amount of training data is sufficiently large. In this work we consider a more realistic adversary who only knows the distribution from which a small training dataset has been sampled and who attacks a transfer-learned neural network classifier that has been trained on this dataset. We exhibit an attack that works in this realistic threat model and demonstrate that in the small-data regime it cannot be defended against by DP-SGD without severely damaging the classifier accuracy. This raises significant concerns about the use of such transfer-learned classifiers when protection of training-data is paramount. We demonstrate the effectiveness and robustness of our attack on VGG, EfficientNet and ResNet image classifiers transfer-learned on MNIST, CIFAR-10 and CelebA respectively. Additionally, we point out that the commonly used (true-positive) reconstruction success rate metric fails to reliably quantify the actual reconstruction effectiveness. Instead, we make use of the Neyman-Pearson lemma to construct the receiver operating characteristic curve and consider the associated true-positive reconstruction rate at a fixed level of the false-positive reconstruction rate.</li>
</ul>

<h3>Title: Effects of the Cyber Resilience Act (CRA) on Industrial Equipment Manufacturing Companies</h3>
<ul>
<li><strong>Authors: </strong>Roosa Risto, Mohit Sethi, Mika Katara</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14325">https://arxiv.org/abs/2505.14325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14325">https://arxiv.org/pdf/2505.14325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14325]] Effects of the Cyber Resilience Act (CRA) on Industrial Equipment Manufacturing Companies(https://arxiv.org/abs/2505.14325)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>The Cyber Resilience Act (CRA) is a new European Union (EU) regulation aimed at enhancing the security of digital products and services by ensuring they meet stringent cybersecurity requirements. This paper investigates the challenges that industrial equipment manufacturing companies anticipate while preparing for compliance with CRA through a comprehensive survey. Key findings highlight significant hurdles such as implementing secure development lifecycle practices, managing vulnerability notifications within strict timelines, and addressing gaps in cybersecurity expertise. This study provides insights into these specific challenges and offers targeted recommendations on key focus areas, such as tooling improvements, to aid industrial equipment manufacturers in their preparation for CRA compliance.</li>
</ul>

<h3>Title: Handloom Design Generation Using Generative Networks</h3>
<ul>
<li><strong>Authors: </strong>Rajat Kanti Bhattacharjee, Meghali Nandi, Amrit Jha, Gunajit Kalita, Ferdous Ahmed Barbhuiya</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14330">https://arxiv.org/abs/2505.14330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14330">https://arxiv.org/pdf/2505.14330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14330]] Handloom Design Generation Using Generative Networks(https://arxiv.org/abs/2505.14330)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper proposes deep learning techniques of generating designs for clothing, focused on handloom fabric and discusses the associated challenges along with its application. The capability of generative neural network models in understanding artistic designs and synthesizing those is not yet explored well. In this work, multiple methods are employed incorporating the current state of the art generative models and style transfer algorithms to study and observe their performance for the task. The results are then evaluated through user score. This work also provides a new dataset NeuralLoom for the task of the design generation.</li>
</ul>

<h3>Title: Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights</h3>
<ul>
<li><strong>Authors: </strong>Aydin Abedinia, Shima Tabakhi, Vahid Seydi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14345">https://arxiv.org/abs/2505.14345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14345">https://arxiv.org/pdf/2505.14345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14345]] Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights(https://arxiv.org/abs/2505.14345)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Recent advancements in semi-supervised deep learning have introduced effective strategies for leveraging both labeled and unlabeled data to improve classification performance. This work proposes a semi-supervised framework that utilizes a distance-based weighting mechanism to prioritize critical training samples based on their proximity to test data. By focusing on the most informative examples, the method enhances model generalization and robustness, particularly in challenging scenarios with noisy or imbalanced datasets. Building on techniques such as uncertainty consistency and graph-based representations, the approach addresses key challenges of limited labeled data while maintaining scalability. Experiments on twelve benchmark datasets demonstrate significant improvements across key metrics, including accuracy, precision, and recall, consistently outperforming existing methods. This framework provides a robust and practical solution for semi-supervised learning, with potential applications in domains such as healthcare and security where data limitations pose significant challenges.</li>
</ul>

<h3>Title: QA-prompting: Improving Summarization with Large Language Models using Question-Answering</h3>
<ul>
<li><strong>Authors: </strong>Neelabh Sinha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14347">https://arxiv.org/abs/2505.14347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14347">https://arxiv.org/pdf/2505.14347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14347]] QA-prompting: Improving Summarization with Large Language Models using Question-Answering(https://arxiv.org/abs/2505.14347)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Language Models (LMs) have revolutionized natural language processing, enabling high-quality text generation through prompting and in-context learning. However, models often struggle with long-context summarization due to positional biases, leading to suboptimal extraction of critical information. There are techniques to improve this with fine-tuning, pipelining, or using complex techniques, which have their own challenges. To solve these challenges, we propose QA-prompting - a simple prompting method for summarization that utilizes question-answering as an intermediate step prior to summary generation. Our method extracts key information and enriches the context of text to mitigate positional biases and improve summarization in a single LM call per task without requiring fine-tuning or pipelining. Experiments on multiple datasets belonging to different domains using ten state-of-the-art pre-trained models demonstrate that QA-prompting outperforms baseline and other state-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This provides an effective and scalable solution for summarization and highlights the importance of domain-specific question selection for optimal performance.</li>
</ul>

<h3>Title: OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Jialong Han, Si Zhang, Ke Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14350">https://arxiv.org/abs/2505.14350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14350">https://arxiv.org/pdf/2505.14350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14350]] OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation(https://arxiv.org/abs/2505.14350)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning Large Language Models (LLMs) has become increasingly challenging due to their massive scale and associated computational costs. Parameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as computational alternatives; however, their implementations still require significant resources. In this paper, we present OSoRA (Output-Dimension and Singular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs. OSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value Decomposition (SVD) with learnable scaling vectors in a unified framework. It first performs an SVD of pre-trained weight matrices, then optimizes an output-dimension vector during training, while keeping the corresponding singular vector matrices frozen. OSoRA substantially reduces computational resource requirements by minimizing the number of trainable parameters during fine-tuning. Comprehensive evaluations across mathematical reasoning, common sense reasoning, and other benchmarks demonstrate that OSoRA achieves comparable or superior performance to state-of-the-art methods like LoRA and VeRA, while maintaining a linear parameter scaling even as the rank increases to higher dimensions. Our ablation studies further confirm that jointly training both the singular values and the output-dimension vector is critical for optimal performance.</li>
</ul>

<h3>Title: Towards eliciting latent knowledge from LLMs with mechanistic interpretability</h3>
<ul>
<li><strong>Authors: </strong>Bartosz Cywiński, Emil Ryd, Senthooran Rajamanoharan, Neel Nanda</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14352">https://arxiv.org/abs/2505.14352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14352">https://arxiv.org/pdf/2505.14352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14352]] Towards eliciting latent knowledge from LLMs with mechanistic interpretability(https://arxiv.org/abs/2505.14352)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>As language models become more powerful and sophisticated, it is crucial that they remain trustworthy and reliable. There is concerning preliminary evidence that models may attempt to deceive or keep secrets from their operators. To explore the ability of current techniques to elicit such hidden knowledge, we train a Taboo model: a language model that describes a specific secret word without explicitly stating it. Importantly, the secret word is not presented to the model in its training data or prompt. We then investigate methods to uncover this secret. First, we evaluate non-interpretability (black-box) approaches. Subsequently, we develop largely automated strategies based on mechanistic interpretability techniques, including logit lens and sparse autoencoders. Evaluation shows that both approaches are effective in eliciting the secret word in our proof-of-concept setting. Our findings highlight the promise of these approaches for eliciting hidden knowledge and suggest several promising avenues for future work, including testing and refining these methods on more complex model organisms. This work aims to be a step towards addressing the crucial problem of eliciting secret knowledge from language models, thereby contributing to their safe and reliable deployment.</li>
</ul>

<h3>Title: WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications</h3>
<ul>
<li><strong>Authors: </strong>Xin Li, Mengbing Liu, Li Wei, Jiancheng An, Mérouane Debbah, Chau Yuen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14354">https://arxiv.org/abs/2505.14354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14354">https://arxiv.org/pdf/2505.14354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14354]] WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications(https://arxiv.org/abs/2505.14354)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved impressive results across a broad array of tasks, yet their capacity for complex, domain-specific mathematical reasoning-particularly in wireless communications-remains underexplored. In this work, we introduce WirelessMathBench, a novel benchmark specifically designed to evaluate LLMs on mathematical modeling challenges to wireless communications engineering. Our benchmark consists of 587 meticulously curated questions sourced from 40 state-of-the-art research papers, encompassing a diverse spectrum of tasks ranging from basic multiple-choice questions to complex equation completion tasks, including both partial and full completions, all of which rigorously adhere to physical and dimensional constraints. Through extensive experimentation with leading LLMs, we observe that while many models excel in basic recall tasks, their performance degrades significantly when reconstructing partially or fully obscured equations, exposing fundamental limitations in current LLMs. Even DeepSeek-R1, the best performer on our benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83% success rate in full equation completion. By publicly releasing WirelessMathBench along with the evaluation toolkit, we aim to advance the development of more robust, domain-aware LLMs for wireless system analysis and broader engineering applications.</li>
</ul>

<h3>Title: Vid2World: Crafting Video Diffusion Models to Interactive World Models</h3>
<ul>
<li><strong>Authors: </strong>Siqiao Huang, Jialong Wu, Qixing Zhou, Shangchen Miao, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14357">https://arxiv.org/abs/2505.14357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14357">https://arxiv.org/pdf/2505.14357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14357]] Vid2World: Crafting Video Diffusion Models to Interactive World Models(https://arxiv.org/abs/2505.14357)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>World models, which predict transitions based on history observation and action sequences, have shown great promise in improving data efficiency for sequential decision making. However, existing world models often require extensive domain-specific training and still produce low-fidelity, coarse predictions, limiting their applicability in complex environments. In contrast, video diffusion models trained on large, internet-scale datasets have demonstrated impressive capabilities in generating high-quality videos that capture diverse real-world dynamics. In this work, we present Vid2World, a general approach for leveraging and transferring pre-trained video diffusion models into interactive world models. To bridge the gap, Vid2World performs casualization of a pre-trained video diffusion model by crafting its architecture and training objective to enable autoregressive generation. Furthermore, it introduces a causal action guidance mechanism to enhance action controllability in the resulting interactive world model. Extensive experiments in robot manipulation and game simulation domains show that our method offers a scalable and effective approach for repurposing highly capable video diffusion models to interactive world models.</li>
</ul>

<h3>Title: Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable</h3>
<ul>
<li><strong>Authors: </strong>Ruoxin Chen, Junwei Xi, Zhiyuan Yan, Ke-Yue Zhang, Shuang Wu, Jingyi Xie, Xu Chen, Lei Xu, Isabel Guan, Taiping Yao, Shouhong Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14359">https://arxiv.org/abs/2505.14359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14359">https://arxiv.org/pdf/2505.14359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14359]] Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable(https://arxiv.org/abs/2505.14359)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Existing detectors are often trained on biased datasets, leading to the possibility of overfitting on non-causal image attributes that are spuriously correlated with real/synthetic labels. While these biased features enhance performance on the training data, they result in substantial performance degradation when applied to unbiased datasets. One common solution is to perform dataset alignment through generative reconstruction, matching the semantic content between real and synthetic images. However, we revisit this approach and show that pixel-level alignment alone is insufficient. The reconstructed images still suffer from frequency-level misalignment, which can perpetuate spurious correlations. To illustrate, we observe that reconstruction models tend to restore the high-frequency details lost in real images (possibly due to JPEG compression), inadvertently creating a frequency-level misalignment, where synthetic images appear to have richer high-frequency content than real ones. This misalignment leads to models associating high-frequency features with synthetic labels, further reinforcing biased cues. To resolve this, we propose Dual Data Alignment (DDA), which aligns both the pixel and frequency domains. Moreover, we introduce two new test sets: DDA-COCO, containing DDA-aligned synthetic images for testing detector performance on the most aligned dataset, and EvalGEN, featuring the latest generative models for assessing detectors under new generative architectures such as visual auto-regressive generators. Finally, our extensive evaluations demonstrate that a detector trained exclusively on DDA-aligned MSCOCO could improve across 8 diverse benchmarks by a non-trivial margin, showing a +7.2% on in-the-wild benchmarks, highlighting the improved generalizability of unbiased detectors.</li>
</ul>

<h3>Title: Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives</h3>
<ul>
<li><strong>Authors: </strong>Xingxing Weng, Chao Pang, Gui-Song Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14361">https://arxiv.org/abs/2505.14361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14361">https://arxiv.org/pdf/2505.14361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14361]] Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives(https://arxiv.org/abs/2505.14361)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vision-language modeling (VLM) aims to bridge the information gap between images and natural language. Under the new paradigm of first pre-training on massive image-text pairs and then fine-tuning on task-specific data, VLM in the remote sensing domain has made significant progress. The resulting models benefit from the absorption of extensive general knowledge and demonstrate strong performance across a variety of remote sensing data analysis tasks. Moreover, they are capable of interacting with users in a conversational manner. In this paper, we aim to provide the remote sensing community with a timely and comprehensive review of the developments in VLM using the two-stage paradigm. Specifically, we first cover a taxonomy of VLM in remote sensing: contrastive learning, visual instruction tuning, and text-conditioned image generation. For each category, we detail the commonly used network architecture and pre-training objectives. Second, we conduct a thorough review of existing works, examining foundation models and task-specific adaptation methods in contrastive-based VLM, architectural upgrades, training strategies and model capabilities in instruction-based VLM, as well as generative foundation models with their representative downstream applications. Third, we summarize datasets used for VLM pre-training, fine-tuning, and evaluation, with an analysis of their construction methodologies (including image sources and caption generation) and key properties, such as scale and task adaptability. Finally, we conclude this survey with insights and discussions on future research directions: cross-modal representation alignment, vague requirement comprehension, explanation-driven model reliability, continually scalable model capabilities, and large-scale datasets featuring richer modalities and greater challenges.</li>
</ul>

<h3>Title: Dual Decomposition of Weights and Singular Value Low Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Jialong Han, Si Zhang, Ke Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14367">https://arxiv.org/abs/2505.14367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14367">https://arxiv.org/pdf/2505.14367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14367]] Dual Decomposition of Weights and Singular Value Low Rank Adaptation(https://arxiv.org/abs/2505.14367)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for adapting Large Language Models (LLMs) to downstream tasks, among which Low-rank Adaptation (LoRA) represents one of the most widely adopted methodologies. However, existing LoRA-based approaches exhibit two fundamental limitations: unstable training dynamics and inefficient knowledge transfer from pre-trained models, both stemming from random initialization of adapter parameters. To overcome these challenges, we propose DuDe, a novel approach that decomposes weight matrices into magnitude and direction components, employing Singular Value Decomposition (SVD) for principled initialization. Our comprehensive evaluation demonstrates DuDe's superior performance and robustness, achieving up to 48.35\% accuracy on MMLU and 62.53\% ($\pm$ 1.59) accuracy on GSM8K. Our theoretical analysis and empirical validation collectively demonstrate that DuDe's decomposition strategy enhances optimization stability and better preserves pre-trained representations, particularly for domain-specific tasks requiring specialized knowledge. The combination of robust empirical performance and rigorous theoretical foundations establishes DuDe as a significant contribution to PEFT methodologies for LLMs.</li>
</ul>

<h3>Title: Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiawen Wang, Pritha Gupta, Ivan Habernal, Eyke Hüllermeier</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14368">https://arxiv.org/abs/2505.14368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14368">https://arxiv.org/pdf/2505.14368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14368]] Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs(https://arxiv.org/abs/2505.14368)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Recent studies demonstrate that Large Language Models (LLMs) are vulnerable to different prompt-based attacks, generating harmful content or sensitive information. Both closed-source and open-source LLMs are underinvestigated for these attacks. This paper studies effective prompt injection attacks against the $\mathbf{14}$ most popular open-source LLMs on five attack benchmarks. Current metrics only consider successful attacks, whereas our proposed Attack Success Probability (ASP) also captures uncertainty in the model's response, reflecting ambiguity in attack feasibility. By comprehensively analyzing the effectiveness of prompt injection attacks, we propose a simple and effective hypnotism attack; results show that this attack causes aligned language models, including Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable behaviors, achieving around $90$% ASP. They also indicate that our ignore prefix attacks can break all $\mathbf{14}$ open-source LLMs, achieving over $60$% ASP on a multi-categorical dataset. We find that moderately well-known LLMs exhibit higher vulnerability to prompt injection attacks, highlighting the need to raise public awareness and prioritize efficient mitigation strategies.</li>
</ul>

<h3>Title: AutoRev: Automatic Peer Review System for Academic Research Papers</h3>
<ul>
<li><strong>Authors: </strong>Maitreya Prafulla Chitale, Ketaki Mangesh Shetye, Harshit Gupta, Manav Chaudhary, Vasudeva Varma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14376">https://arxiv.org/abs/2505.14376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14376">https://arxiv.org/pdf/2505.14376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14376]] AutoRev: Automatic Peer Review System for Academic Research Papers(https://arxiv.org/abs/2505.14376)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Generating a review for an academic research paper is a complex task that requires a deep understanding of the document's content and the interdependencies between its sections. It demands not only insight into technical details but also an appreciation of the paper's overall coherence and structure. Recent methods have predominantly focused on fine-tuning large language models (LLMs) to address this challenge. However, they often overlook the computational and performance limitations imposed by long input token lengths. To address this, we introduce AutoRev, an Automatic Peer Review System for Academic Research Papers. Our novel framework represents an academic document as a graph, enabling the extraction of the most critical passages that contribute significantly to the review. This graph-based approach demonstrates effectiveness for review generation and is potentially adaptable to various downstream tasks, such as question answering, summarization, and document representation. When applied to review generation, our method outperforms SOTA baselines by an average of 58.72% across all evaluation metrics. We hope that our work will stimulate further research in applying graph-based extraction techniques to other downstream tasks in NLP. We plan to make our code public upon acceptance.</li>
</ul>

<h3>Title: Algorithmic Hiring and Diversity: Reducing Human-Algorithm Similarity for Better Outcomes</h3>
<ul>
<li><strong>Authors: </strong>Prasanna Parasurama, Panos Ipeirotis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14388">https://arxiv.org/abs/2505.14388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14388">https://arxiv.org/pdf/2505.14388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14388]] Algorithmic Hiring and Diversity: Reducing Human-Algorithm Similarity for Better Outcomes(https://arxiv.org/abs/2505.14388)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Algorithmic tools are increasingly used in hiring to improve fairness and diversity, often by enforcing constraints such as gender-balanced candidate shortlists. However, we show theoretically and empirically that enforcing equal representation at the shortlist stage does not necessarily translate into more diverse final hires, even when there is no gender bias in the hiring stage. We identify a crucial factor influencing this outcome: the correlation between the algorithm's screening criteria and the human hiring manager's evaluation criteria -- higher correlation leads to lower diversity in final hires. Using a large-scale empirical analysis of nearly 800,000 job applications across multiple technology firms, we find that enforcing equal shortlists yields limited improvements in hire diversity when the algorithmic screening closely mirrors the hiring manager's preferences. We propose a complementary algorithmic approach designed explicitly to diversify shortlists by selecting candidates likely to be overlooked by managers, yet still competitive according to their evaluation criteria. Empirical simulations show that this approach significantly enhances gender diversity in final hires without substantially compromising hire quality. These findings highlight the importance of algorithmic design choices in achieving organizational diversity goals and provide actionable guidance for practitioners implementing fairness-oriented hiring algorithms.</li>
</ul>

<h3>Title: MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language</h3>
<ul>
<li><strong>Authors: </strong>Seyoung Song, Seogyeong Jeong, Eunsu Kim, Jiho Jin, Dongkwan Kim, Jay Shin, Alice Oh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14395">https://arxiv.org/abs/2505.14395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14395">https://arxiv.org/pdf/2505.14395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14395]] MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language(https://arxiv.org/abs/2505.14395)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Evaluating text generation capabilities of large language models (LLMs) is challenging, particularly for low-resource languages where methods for direct assessment are scarce. We propose MUG-Eval, a novel framework that evaluates LLMs' multilingual generation capabilities by transforming existing benchmarks into conversational tasks and measuring the LLMs' accuracies on those tasks. We specifically designed these conversational tasks to require effective communication in the target language. Then, we simply use task success rate as a proxy of successful conversation generation. Our approach offers two key advantages: it is independent of language-specific NLP tools or annotated datasets, which are limited for most languages, and it does not rely on LLMs-as-judges, whose evaluation quality degrades outside a few high-resource languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and low-resource categories, and we find that MUG-Eval correlates strongly with established benchmarks ($r$ > 0.75) while enabling standardized comparisons across languages and models. Our framework provides a robust and resource-efficient solution for evaluating multilingual generation that can be extended to thousands of languages.</li>
</ul>

<h3>Title: Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation</h3>
<ul>
<li><strong>Authors: </strong>Peter Baile Chen, Yi Zhang, Dan Roth, Samuel Madden, Jacob Andreas, Michael Cafarella</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14398">https://arxiv.org/abs/2505.14398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14398">https://arxiv.org/pdf/2505.14398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14398]] Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation(https://arxiv.org/abs/2505.14398)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>While humans naturally learn and adapt from past experiences, large language models (LLMs) and their agentic counterparts struggle to retain reasoning from previous tasks and apply them in future contexts. To address this limitation, we propose a novel framework, log-augmented generation (LAG) that directly reuses prior computation and reasoning from past logs at test time to enhance model's ability to learn from previous tasks and perform better on new, unseen challenges, all while keeping the system efficient and scalable. Specifically, our system represents task logs using key-value (KV) caches, encoding the full reasoning context of prior tasks while storing KV caches for only a selected subset of tokens. When a new task arises, LAG retrieves the KV values from relevant logs to augment generation. Our approach differs from reflection-based memory mechanisms by directly reusing prior reasoning and computations without requiring additional steps for knowledge extraction or distillation. Our method also goes beyond existing KV caching techniques, which primarily target efficiency gains rather than improving accuracy. Experiments on knowledge- and reasoning-intensive datasets demonstrate that our method significantly outperforms standard agentic systems that do not utilize logs, as well as existing solutions based on reflection and KV cache techniques.</li>
</ul>

<h3>Title: Investigating and Enhancing the Robustness of Large Multimodal Models Against Temporal Inconsistency</h3>
<ul>
<li><strong>Authors: </strong>Jiafeng Liang, Shixin Jiang, Xuan Dong, Ning Wang, Zheng Chu, Hui Su, Jinlan Fu, Ming Liu, See-Kiong Ng, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14405">https://arxiv.org/abs/2505.14405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14405">https://arxiv.org/pdf/2505.14405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14405]] Investigating and Enhancing the Robustness of Large Multimodal Models Against Temporal Inconsistency(https://arxiv.org/abs/2505.14405)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large Multimodal Models (LMMs) have recently demonstrated impressive performance on general video comprehension benchmarks. Nevertheless, for broader applications, the robustness of their temporal analysis capability needs to be thoroughly investigated yet predominantly ignored. Motivated by this, we propose a novel temporal robustness benchmark (TemRobBench), which introduces temporal inconsistency perturbations separately at the visual and textual modalities to assess the robustness of models. We evaluate 16 mainstream LMMs and find that they exhibit over-reliance on prior knowledge and textual context in adversarial environments, while ignoring the actual temporal dynamics in the video. To mitigate this issue, we design panoramic direct preference optimization (PanoDPO), which encourages LMMs to incorporate both visual and linguistic feature preferences simultaneously. Experimental results show that PanoDPO can effectively enhance the model's robustness and reliability in temporal analysis.</li>
</ul>

<h3>Title: Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis</h3>
<ul>
<li><strong>Authors: </strong>Haoming Huang, Yibo Yan, Jiahao Huo, Xin Zou, Xinfeng Li, Kun Wang, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14406">https://arxiv.org/abs/2505.14406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14406">https://arxiv.org/pdf/2505.14406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14406]] Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis(https://arxiv.org/abs/2505.14406)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), despite their remarkable capabilities, are hampered by hallucinations. A particularly challenging variant, knowledge overshadowing, occurs when one piece of activated knowledge inadvertently masks another relevant piece, leading to erroneous outputs even with high-quality training data. Current understanding of overshadowing is largely confined to inference-time observations, lacking deep insights into its origins and internal mechanisms during model training. Therefore, we introduce PhantomCircuit, a novel framework designed to comprehensively analyze and detect knowledge overshadowing. By innovatively employing knowledge circuit analysis, PhantomCircuit dissects the internal workings of attention heads, tracing how competing knowledge pathways contribute to the overshadowing phenomenon and its evolution throughout the training process. Extensive experiments demonstrate PhantomCircuit's effectiveness in identifying such instances, offering novel insights into this elusive hallucination and providing the research community with a new methodological lens for its potential mitigation.</li>
</ul>

<h3>Title: Explaining Unreliable Perception in Automated Driving: A Fuzzy-based Monitoring Approach</h3>
<ul>
<li><strong>Authors: </strong>Aniket Salvi, Gereon Weiss, Mario Trapp</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14407">https://arxiv.org/abs/2505.14407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14407">https://arxiv.org/pdf/2505.14407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14407]] Explaining Unreliable Perception in Automated Driving: A Fuzzy-based Monitoring Approach(https://arxiv.org/abs/2505.14407)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Autonomous systems that rely on Machine Learning (ML) utilize online fault tolerance mechanisms, such as runtime monitors, to detect ML prediction errors and maintain safety during operation. However, the lack of human-interpretable explanations for these errors can hinder the creation of strong assurances about the system's safety and reliability. This paper introduces a novel fuzzy-based monitor tailored for ML perception components. It provides human-interpretable explanations about how different operating conditions affect the reliability of perception components and also functions as a runtime safety monitor. We evaluated our proposed monitor using naturalistic driving datasets as part of an automated driving case study. The interpretability of the monitor was evaluated and we identified a set of operating conditions in which the perception component performs reliably. Additionally, we created an assurance case that links unit-level evidence of \textit{correct} ML operation to system-level \textit{safety}. The benchmarking demonstrated that our monitor achieved a better increase in safety (i.e., absence of hazardous situations) while maintaining availability (i.e., ability to perform the mission) compared to state-of-the-art runtime ML monitors in the evaluated dataset.</li>
</ul>

<h3>Title: Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents</h3>
<ul>
<li><strong>Authors: </strong>Pengzhou Cheng, Haowen Hu, Zheng Wu, Zongru Wu, Tianjie Ju, Daizong Ding, Zhuosheng Zhang, Gongshen Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14418">https://arxiv.org/abs/2505.14418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14418">https://arxiv.org/pdf/2505.14418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14418]] Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents(https://arxiv.org/abs/2505.14418)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Graphical user interface (GUI) agents powered by multimodal large language models (MLLMs) have shown greater promise for human-interaction. However, due to the high fine-tuning cost, users often rely on open-source GUI agents or APIs offered by AI providers, which introduces a critical but underexplored supply chain threat: backdoor attacks. In this work, we first unveil that MLLM-powered GUI agents naturally expose multiple interaction-level triggers, such as historical steps, environment states, and task progress. Based on this observation, we introduce AgentGhost, an effective and stealthy framework for red-teaming backdoor attacks. Specifically, we first construct composite triggers by combining goal and interaction levels, allowing GUI agents to unintentionally activate backdoors while ensuring task utility. Then, we formulate backdoor injection as a Min-Max optimization problem that uses supervised contrastive learning to maximize the feature difference across sample classes at the representation space, improving flexibility of the backdoor. Meanwhile, it adopts supervised fine-tuning to minimize the discrepancy between backdoor and clean behavior generation, enhancing effectiveness and utility. Extensive evaluations of various agent models in two established mobile benchmarks show that AgentGhost is effective and generic, with attack accuracy that reaches 99.7\% on three attack objectives, and shows stealthiness with only 1\% utility degradation. Furthermore, we tailor a defense method against AgentGhost that reduces the attack accuracy to 22.1\%. Our code is available at \texttt{anonymous}.</li>
</ul>

<h3>Title: Explaining Neural Networks with Reasons</h3>
<ul>
<li><strong>Authors: </strong>Levin Hornischer, Hannes Leitgeb</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14424">https://arxiv.org/abs/2505.14424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14424">https://arxiv.org/pdf/2505.14424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14424]] Explaining Neural Networks with Reasons(https://arxiv.org/abs/2505.14424)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, interpretability</a></li>
<li><strong>Abstract: </strong>We propose a new interpretability method for neural networks, which is based on a novel mathematico-philosophical theory of reasons. Our method computes a vector for each neuron, called its reasons vector. We then can compute how strongly this reasons vector speaks for various propositions, e.g., the proposition that the input image depicts digit 2 or that the input prompt has a negative sentiment. This yields an interpretation of neurons, and groups thereof, that combines a logical and a Bayesian perspective, and accounts for polysemanticity (i.e., that a single neuron can figure in multiple concepts). We show, both theoretically and empirically, that this method is: (1) grounded in a philosophically established notion of explanation, (2) uniform, i.e., applies to the common neural network architectures and modalities, (3) scalable, since computing reason vectors only involves forward-passes in the neural network, (4) faithful, i.e., intervening on a neuron based on its reason vector leads to expected changes in model output, (5) correct in that the model's reasons structure matches that of the data source, (6) trainable, i.e., neural networks can be trained to improve their reason strengths, (7) useful, i.e., it delivers on the needs for interpretability by increasing, e.g., robustness and fairness.</li>
</ul>

<h3>Title: From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14425">https://arxiv.org/abs/2505.14425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14425">https://arxiv.org/pdf/2505.14425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14425]] From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning(https://arxiv.org/abs/2505.14425)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Instruction-tuned large language models (LLMs) have shown strong performance on a variety of tasks; however, generalizing from synthetic to human-authored instructions in grounded environments remains a challenge for them. In this work, we study generalization challenges in spatial grounding tasks where models interpret and translate instructions for building object arrangements on a $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate their performance on a benchmark dataset containing both synthetic and human-written instructions. Our results reveal that while models generalize well on simple tasks, their performance degrades significantly on more complex tasks. We present a detailed error analysis of the gaps in instruction generalization.</li>
</ul>

<h3>Title: Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications</h3>
<ul>
<li><strong>Authors: </strong>Riccardo D'Elia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14428">https://arxiv.org/abs/2505.14428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14428">https://arxiv.org/pdf/2505.14428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14428]] Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications(https://arxiv.org/abs/2505.14428)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>The objective of this proposal is to bridge the gap between Deep Learning (DL) and System Dynamics (SD) by developing an interpretable neural system dynamics framework. While DL excels at learning complex models and making accurate predictions, it lacks interpretability and causal reliability. Traditional SD approaches, on the other hand, provide transparency and causal insights but are limited in scalability and require extensive domain knowledge. To overcome these limitations, this project introduces a Neural System Dynamics pipeline, integrating Concept-Based Interpretability, Mechanistic Interpretability, and Causal Machine Learning. This framework combines the predictive power of DL with the interpretability of traditional SD models, resulting in both causal reliability and scalability. The efficacy of the proposed pipeline will be validated through real-world applications of the EU-funded AutoMoTIF project, which is focused on autonomous multimodal transportation systems. The long-term goal is to collect actionable insights that support the integration of explainability and safety in autonomous systems.</li>
</ul>

<h3>Title: Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuqiao Tan, Shizhu He, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14436">https://arxiv.org/abs/2505.14436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14436">https://arxiv.org/pdf/2505.14436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14436]] Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models(https://arxiv.org/abs/2505.14436)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) offer a transparent brain with accessible parameters that encode extensive knowledge, which can be analyzed, located and transferred. Consequently, a key research challenge is to transcend traditional knowledge transfer paradigms rooted in symbolic language and achieve genuine Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods for transferring knowledge across LLMs of different scales through parameters presents an intriguing and valuable research direction. In this paper, we first demonstrate $\textbf{Alignment}$ in parametric space is the fundamental prerequisite to achieve successful cross-scale PKT. We redefine the previously explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes extracted parameters for LoRA initialization and requires subsequent fine-tune for alignment. Hence, to reduce cost for further fine-tuning, we introduce a novel Pre-Align PKT (PrePKT) paradigm and propose a solution called $\textbf{LaTen}$ ($\textbf{L}$oc$\textbf{a}$te-$\textbf{T}$h$\textbf{e}$n-Alig$\textbf{n}$) that aligns the parametric spaces of LLMs across scales only using several training steps without following training. Comprehensive experiments on four benchmarks demonstrate that both PostPKT and PrePKT face challenges in achieving consistently stable transfer. Through in-depth analysis, we identify $\textbf{Neural Incompatibility}$ as the ethological and parametric structural differences between LLMs of varying scales, presenting fundamental challenges to achieving effective PKT. These findings provide fresh insights into the parametric architectures of LLMs and highlight promising directions for future research on efficient PKT. Our code is available at this https URL.</li>
</ul>

<h3>Title: Creative Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Mete Ismayilzada, Antonio Laverghetta Jr., Simone A. Luchini, Reet Patel, Antoine Bosselut, Lonneke van der Plas, Roger Beaty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14442">https://arxiv.org/abs/2505.14442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14442">https://arxiv.org/pdf/2505.14442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14442]] Creative Preference Optimization(https://arxiv.org/abs/2505.14442)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) have demonstrated impressive performance across natural language generation tasks, their ability to generate truly creative content-characterized by novelty, diversity, surprise, and quality-remains limited. Existing methods for enhancing LLM creativity often focus narrowly on diversity or specific tasks, failing to address creativity's multifaceted nature in a generalizable way. In this work, we propose Creative Preference Optimization (CrPO), a novel alignment method that injects signals from multiple creativity dimensions into the preference optimization objective in a modular fashion. We train and evaluate creativity-augmented versions of several models using CrPO and MuCE, a new large-scale human preference dataset spanning over 200,000 human-generated responses and ratings from more than 30 psychological creativity assessments. Our models outperform strong baselines, including GPT-4o, on both automated and human evaluations, producing more novel, diverse, and surprising generations while maintaining high output quality. Additional evaluations on NoveltyBench further confirm the generalizability of our approach. Together, our results demonstrate that directly optimizing for creativity within preference frameworks is a promising direction for advancing the creative capabilities of LLMs without compromising output quality.</li>
</ul>

<h3>Title: RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation</h3>
<ul>
<li><strong>Authors: </strong>Md Atik Ahamed, Qiang Ye, Qiang Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14451">https://arxiv.org/abs/2505.14451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14451">https://arxiv.org/pdf/2505.14451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14451]] RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation(https://arxiv.org/abs/2505.14451)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Missing values in high-dimensional, mixed-type datasets pose significant challenges for data imputation, particularly under Missing Not At Random (MNAR) mechanisms. Existing methods struggle to integrate local and global data characteristics, limiting performance in MNAR and high-dimensional settings. We propose an innovative framework, RefiDiff, combining local machine learning predictions with a novel Mamba-based denoising network capturing interrelationships among distant features and samples. Our approach leverages pre-refinement for initial warm-up imputations and post-refinement to polish results, enhancing stability and accuracy. By encoding mixed-type data into unified tokens, RefiDiff enables robust imputation without architectural or hyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods across missing-value settings, excelling in MNAR with a 4x faster training time than SOTA DDPM-based approaches. Extensive evaluations on nine real-world datasets demonstrate its robustness, scalability, and effectiveness in handling complex missingness patterns.</li>
</ul>

<h3>Title: Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuyang Liu, Yiyu Wang, Junpeng Ma, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14454">https://arxiv.org/abs/2505.14454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14454">https://arxiv.org/pdf/2505.14454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14454]] Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models(https://arxiv.org/abs/2505.14454)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video large language models (VideoLLM) excel at video understanding, but face efficiency challenges due to the quadratic complexity of abundant visual tokens. Our systematic analysis of token compression methods for VideoLLMs reveals two critical issues: (i) overlooking distinctive visual signals across frames, leading to information loss; (ii) suffering from implementation constraints, causing incompatibility with modern architectures or efficient operators. To address these challenges, we distill three design principles for VideoLLM token compression and propose a plug-and-play inference acceleration framework "Video Compression Commander" (VidCom2). By quantifying each frame's uniqueness, VidCom2 adaptively adjusts compression intensity across frames, effectively preserving essential information while reducing redundancy in video sequences. Extensive experiments across various VideoLLMs and benchmarks demonstrate the superior performance and efficiency of our VidCom2. With only 25% visual tokens, VidCom2 achieves 99.6% of the original performance on LLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame Compression Adjustment strategy is compatible with other token compression methods to further improve their performance. Our code is available at this https URL.</li>
</ul>

<h3>Title: CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation</h3>
<ul>
<li><strong>Authors: </strong>Chihan Huang, Hao Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14455">https://arxiv.org/abs/2505.14455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14455">https://arxiv.org/pdf/2505.14455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14455]] CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation(https://arxiv.org/abs/2505.14455)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Although autoregressive models have dominated language modeling in recent years, there has been a growing interest in exploring alternative paradigms to the conventional next-token prediction framework. Diffusion-based language models have emerged as a compelling alternative due to their powerful parallel generation capabilities and inherent editability. However, these models are often constrained by fixed-length generation. A promising direction is to combine the strengths of both paradigms, segmenting sequences into blocks, modeling autoregressive dependencies across blocks while leveraging discrete diffusion to estimate the conditional distribution within each block given the preceding context. Nevertheless, their practical application is often hindered by two key limitations: rigid fixed-length outputs and a lack of flexible control mechanisms. In this work, we address the critical limitations of fixed granularity and weak controllability in current large diffusion language models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive framework that adaptively determines the size of each generation block based on local semantics using reinforcement learning. Furthermore, we introduce a classifier-guided control mechanism tailored to discrete diffusion, which significantly reduces computational overhead while facilitating efficient post-hoc conditioning without retraining. Extensive experiments demonstrate that CtrlDiff sets a new standard among hybrid diffusion models, narrows the performance gap to state-of-the-art autoregressive approaches, and enables effective conditional text generation across diverse tasks.</li>
</ul>

<h3>Title: Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks</h3>
<ul>
<li><strong>Authors: </strong>Kamal Singh, Sami Marouani, Ahmad Al Sheikh, Pham Tran Anh Quang, Amaury Habrard</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14459">https://arxiv.org/abs/2505.14459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14459">https://arxiv.org/pdf/2505.14459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14459]] Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks(https://arxiv.org/abs/2505.14459)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has been increasingly applied to network control problems, such as load balancing. However, existing RL approaches often suffer from lack of interpretability and difficulty in extracting controller equations. In this paper, we propose the use of Kolmogorov-Arnold Networks (KAN) for interpretable RL in network control. We employ a PPO agent with a 1-layer actor KAN model and an MLP Critic network to learn load balancing policies that maximise throughput utility, minimize loss as well as delay. Our approach allows us to extract controller equations from the learned neural networks, providing insights into the decision-making process. We evaluate our approach using different reward functions demonstrating its effectiveness in improving network performance while providing interpretable policies.</li>
</ul>

<h3>Title: VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank</h3>
<ul>
<li><strong>Authors: </strong>Tianhe Wu, Jian Zou, Jie Liang, Lei Zhang, Kede Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14460">https://arxiv.org/abs/2505.14460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14460">https://arxiv.org/pdf/2505.14460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14460]] VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank(https://arxiv.org/abs/2505.14460)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing reasoning and generalization capabilities of large language models (LLMs) through reinforcement learning. Nevertheless, the potential of reasoning-induced computational modeling has not been thoroughly explored in the context of image quality assessment (IQA), a task critically dependent on visual reasoning. In this paper, we introduce VisualQuality-R1, a reasoning-induced no-reference IQA (NR-IQA) model, and we train it with reinforcement learning to rank, a learning algorithm tailored to the intrinsically relative nature of visual quality. Specifically, for a pair of images, we employ group relative policy optimization to generate multiple quality scores for each image. These estimates are then used to compute comparative probabilities of one image having higher quality than the other under the Thurstone model. Rewards for each quality estimate are defined using continuous fidelity measures rather than discretized binary labels. Extensive experiments show that the proposed VisualQuality-R1 consistently outperforms discriminative deep learning-based NR-IQA models as well as a recent reasoning-induced quality regression method. Moreover, VisualQuality-R1 is capable of generating contextually rich, human-aligned quality descriptions, and supports multi-dataset training without requiring perceptual scale realignment. These features make VisualQuality-R1 especially well-suited for reliably measuring progress in a wide range of image processing tasks like super-resolution and image generation.</li>
</ul>

<h3>Title: Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium</h3>
<ul>
<li><strong>Authors: </strong>Xinxin Fan, Wenxiong Chen, Mengfan Li, Wenqi Wei, Ling Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14463">https://arxiv.org/abs/2505.14463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14463">https://arxiv.org/pdf/2505.14463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14463]] Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium(https://arxiv.org/abs/2505.14463)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Adversarial attacks to graph analytics are gaining increased attention. To date, two lines of countermeasures have been proposed to resist various graph adversarial attacks from the perspectives of either graph per se or graph neural networks. Nevertheless, a fundamental question lies in whether there exists an intrinsic adversarial resilience state within a graph regime and how to find out such a critical state if exists. This paper contributes to tackle the above research questions from three unique perspectives: i) we regard the process of adversarial learning on graph as a complex multi-object dynamic system, and model the behavior of adversarial attack; ii) we propose a generalized theoretical framework to show the existence of critical adversarial resilience state; and iii) we develop a condensed one-dimensional function to capture the dynamic variation of graph regime under perturbations, and pinpoint the critical state through solving the equilibrium point of dynamic system. Multi-facet experiments are conducted to show our proposed approach can significantly outperform the state-of-the-art defense methods under five commonly-used real-world datasets and three representative attacks.</li>
</ul>

<h3>Title: Void in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mani Shemiranifar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14467">https://arxiv.org/abs/2505.14467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14467">https://arxiv.org/pdf/2505.14467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14467]] Void in Language Models(https://arxiv.org/abs/2505.14467)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Despite advances in transformer-based language models (LMs), a fundamental question remains largely unanswered: Are all layers activated during inference? We investigate this question by detecting unactivated layers (which we refer to as Voids) using a non-trainable and parameter-free adaptive computation method called L2 Adaptive Computation (LAC). We adapt LAC from its original efficiency-focused application to trace activated layers during inference. This method monitors changes in the L2-norm of activations to identify voids. We analyze layer activation in instruction-tuned LMs across two phases: Prompt Processing (PP), where we trace activated layers for each token in the input prompts, and Response Generation (RG), where we trace activated layers for each generated token. We further demonstrate that distinct layers are activated during these two phases. To show the effectiveness of our method, we evaluated three distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families on three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a zero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an improvement from 69.24 to 71.29 while the model uses only 30% of the layers. Similarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to 18.36 when using 70% of the layers during both the PP and RG phases. These results show that not all layers contribute equally during inference, and that selectively skipping most of them can improve the performance of models on certain tasks.</li>
</ul>

<h3>Title: ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yifan Sui, Hao Wang, Hanfei Yu, Yitao Hu, Jianxun Li, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14468">https://arxiv.org/abs/2505.14468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14468">https://arxiv.org/pdf/2505.14468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14468]] ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs(https://arxiv.org/abs/2505.14468)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, large language model</a></li>
<li><strong>Abstract: </strong>Serverless computing has grown rapidly for serving Large Language Model (LLM) inference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid scaling. However, our analysis reveals that current serverless can effectively serve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to three key limitations: 1) massive parameter redundancy among functions where 99% of weights are unnecessarily duplicated, 2) costly artifact loading latency beyond LLM loading, and 3) magnified resource contention when serving multiple LoRA LLMs. These inefficiencies lead to massive GPU wastage, increased Time-To-First-Token (TTFT), and high monetary costs. We propose ServerlessLoRA, a novel serverless inference system designed for faster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM sharing across isolated LoRA functions to reduce redundancy. We design a pre-loading method that pre-loads comprehensive LoRA artifacts to minimize cold-start latency. Furthermore, ServerlessLoRA employs contention aware batching and offloading to mitigate GPU resource conflicts during bursty workloads. Experiment on industrial workloads demonstrates that ServerlessLoRA reduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to state-of-the-art LLM inference solutions.</li>
</ul>

<h3>Title: Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Somnath Banerjee, Pratyush Chatterjee, Shanu Kumar, Sayan Layek, Parag Agrawal, Rima Hazra, Animesh Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14469">https://arxiv.org/abs/2505.14469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14469">https://arxiv.org/pdf/2505.14469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14469]] Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations(https://arxiv.org/abs/2505.14469)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in LLMs have raised significant safety concerns, particularly when dealing with code-mixed inputs and outputs. Our study systematically investigates the increased susceptibility of LLMs to produce unsafe outputs from code-mixed prompts compared to monolingual English prompts. Utilizing explainability methods, we dissect the internal attribution shifts causing model's harmful behaviors. In addition, we explore cultural dimensions by distinguishing between universally unsafe and culturally-specific unsafe queries. This paper presents novel experimental insights, clarifying the mechanisms driving this phenomenon.</li>
</ul>

<h3>Title: Enhancing Interpretability of Sparse Latent Representations with Class Information</h3>
<ul>
<li><strong>Authors: </strong>Farshad Sangari Abiz, Reshad Hosseini, Babak N. Araabi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14476">https://arxiv.org/abs/2505.14476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14476">https://arxiv.org/pdf/2505.14476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14476]] Enhancing Interpretability of Sparse Latent Representations with Class Information(https://arxiv.org/abs/2505.14476)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Variational Autoencoders (VAEs) are powerful generative models for learning latent representations. Standard VAEs generate dispersed and unstructured latent spaces by utilizing all dimensions, which limits their interpretability, especially in high-dimensional spaces. To address this challenge, Variational Sparse Coding (VSC) introduces a spike-and-slab prior distribution, resulting in sparse latent representations for each input. These sparse representations, characterized by a limited number of active dimensions, are inherently more interpretable. Despite this advantage, VSC falls short in providing structured interpretations across samples within the same class. Intuitively, samples from the same class are expected to share similar attributes while allowing for variations in those attributes. This expectation should manifest as consistent patterns of active dimensions in their latent representations, but VSC does not enforce such consistency. In this paper, we propose a novel approach to enhance the latent space interpretability by ensuring that the active dimensions in the latent space are consistent across samples within the same class. To achieve this, we introduce a new loss function that encourages samples from the same class to share similar active dimensions. This alignment creates a more structured and interpretable latent space, where each shared dimension corresponds to a high-level concept, or "factor." Unlike existing disentanglement-based methods that primarily focus on global factors shared across all classes, our method captures both global and class-specific factors, thereby enhancing the utility and interpretability of latent representations.</li>
</ul>

<h3>Title: MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance</h3>
<ul>
<li><strong>Authors: </strong>Agam Goyal, Xianyang Zhan, Yilun Chen, Koustuv Saha, Eshwar Chandrasekharan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14483">https://arxiv.org/abs/2505.14483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14483">https://arxiv.org/pdf/2505.14483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14483]] MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance(https://arxiv.org/abs/2505.14483)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown great potential in flagging harmful content in online communities. Yet, existing approaches for moderation require a separate model for every community and are opaque in their decision-making, limiting real-world adoption. We introduce Mixture of Moderation Experts (MoMoE), a modular, cross-community framework that adds post-hoc explanations to scalable content moderation. MoMoE orchestrates four operators -- Allocate, Predict, Aggregate, Explain -- and is instantiated as seven community-specialized experts (MoMoE-Community) and five norm-violation experts (MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1 scores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned baselines while consistently producing concise and reliable explanations. Although community-specialized experts deliver the highest peak accuracy, norm-violation experts provide steadier performance across domains. These findings show that MoMoE yields scalable, transparent moderation without needing per-community fine-tuning. More broadly, they suggest that lightweight, explainable expert ensembles can guide future NLP and HCI research on trustworthy human-AI governance of online communities.</li>
</ul>

<h3>Title: Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales</h3>
<ul>
<li><strong>Authors: </strong>Jun Cao, Jiyi Li, Ziwei Yang, Renjie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14499">https://arxiv.org/abs/2505.14499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14499">https://arxiv.org/pdf/2505.14499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14499]] Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales(https://arxiv.org/abs/2505.14499)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>There has been growing interest in Multimodal Aspect-Based Sentiment Analysis (MABSA) in recent years. Existing methods predominantly rely on pre-trained small language models (SLMs) to collect information related to aspects and sentiments from both image and text, with an aim to align these two modalities. However, small SLMs possess limited capacity and knowledge, often resulting in inaccurate identification of meaning, aspects, sentiments, and their interconnections in textual and visual data. On the other hand, Large language models (LLMs) have shown exceptional capabilities in various tasks by effectively exploring fine-grained information in multimodal data. However, some studies indicate that LLMs still fall short compared to fine-tuned small models in the field of ABSA. Based on these findings, we propose a novel framework, termed LRSA, which combines the decision-making capabilities of SLMs with additional information provided by LLMs for MABSA. Specifically, we inject explanations generated by LLMs as rationales into SLMs and employ a dual cross-attention mechanism for enhancing feature interaction and fusion, thereby augmenting the SLMs' ability to identify aspects and sentiments. We evaluated our method using two baseline models, numerous experiments highlight the superiority of our approach on three widely-used benchmarks, indicating its generalizability and applicability to most pre-trained models for MABSA.</li>
</ul>

<h3>Title: Learning to Integrate Diffusion ODEs by Averaging the Derivatives</h3>
<ul>
<li><strong>Authors: </strong>Wenze Liu, Xiangyu Yue</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14502">https://arxiv.org/abs/2505.14502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14502">https://arxiv.org/pdf/2505.14502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14502]] Learning to Integrate Diffusion ODEs by Averaging the Derivatives(https://arxiv.org/abs/2505.14502)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>To accelerate diffusion model inference, numerical solvers perform poorly at extremely small steps, while distillation techniques often introduce complexity and instability. This work presents an intermediate strategy, balancing performance and cost, by learning ODE integration using loss functions derived from the derivative-integral relationship, inspired by Monte Carlo integration and Picard iteration. From a geometric perspective, the losses operate by gradually extending the tangent to the secant, thus are named as secant losses. The secant losses can rapidly convert (via fine-tuning or distillation) a pretrained diffusion model into its secant version. In our experiments, the secant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the secant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID of $1.96$ on ImageNet-$256\times256$. Code will be available.</li>
</ul>

<h3>Title: ModRWKV: Transformer Multimodality in Linear Time</h3>
<ul>
<li><strong>Authors: </strong>Jiale Kang, Ziyin Yue, Qingyu Yin, Jiang Rui, Weile Li, Zening Lu, Zhouran Ji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14505">https://arxiv.org/abs/2505.14505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14505">https://arxiv.org/pdf/2505.14505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14505]] ModRWKV: Transformer Multimodality in Linear Time(https://arxiv.org/abs/2505.14505)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Currently, most multimodal studies are based on large language models (LLMs) with quadratic-complexity Transformer architectures. While linear models like RNNs enjoy low inference costs, their application has been largely limited to the text-only modality. This work explores the capabilities of modern RNN architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal framework built upon the RWKV7 architecture as its LLM backbone-which achieves multi-source information fusion through dynamically adaptable heterogeneous modality encoders. We designed the multimodal modules in ModRWKV with an extremely lightweight architecture and, through extensive experiments, identified a configuration that achieves an optimal balance between performance and computational efficiency. ModRWKV leverages the pretrained weights of the RWKV7 LLM for initialization, which significantly accelerates multimodal training. Comparative experiments with different pretrained checkpoints further demonstrate that such initialization plays a crucial role in enhancing the model's ability to understand multimodal signals. Supported by extensive experiments, we conclude that modern RNN architectures present a viable alternative to Transformers in the domain of multimodal large language models (MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV architecture through systematic exploration.</li>
</ul>

<h3>Title: ReservoirTTA: Prolonged Test-time Adaptation for Evolving and Recurring Domains</h3>
<ul>
<li><strong>Authors: </strong>Guillaume Vray, Devavrat Tomar, Xufeng Gao, Jean-Philippe Thiran, Evan Shelhamer, Behzad Bozorgtabar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14511">https://arxiv.org/abs/2505.14511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14511">https://arxiv.org/pdf/2505.14511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14511]] ReservoirTTA: Prolonged Test-time Adaptation for Evolving and Recurring Domains(https://arxiv.org/abs/2505.14511)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>This paper introduces ReservoirTTA, a novel plug-in framework designed for prolonged test-time adaptation (TTA) in scenarios where the test domain continuously shifts over time, including cases where domains recur or evolve gradually. At its core, ReservoirTTA maintains a reservoir of domain-specialized models -- an adaptive test-time model ensemble -- that both detects new domains via online clustering over style features of incoming samples and routes each sample to the appropriate specialized model, and thereby enables domain-specific adaptation. This multi-model strategy overcomes key limitations of single model adaptation, such as catastrophic forgetting, inter-domain interference, and error accumulation, ensuring robust and stable performance on sustained non-stationary test distributions. Our theoretical analysis reveals key components that bound parameter variance and prevent model collapse, while our plug-in TTA module mitigates catastrophic forgetting of previously encountered domains. Extensive experiments on the classification corruption benchmarks, including ImageNet-C and CIFAR-10/100-C, as well as the Cityscapes$\rightarrow$ACDC semantic segmentation task, covering recurring and continuously evolving domain shifts, demonstrate that ReservoirTTA significantly improves adaptation accuracy and maintains stable performance across prolonged, recurring shifts, outperforming state-of-the-art methods.</li>
</ul>

<h3>Title: Latent Flow Transformer</h3>
<ul>
<li><strong>Authors: </strong>Yen-Chen Wu, Feng-Ting Liao, Meng-Hsi Chen, Pei-Chen Ho, Farhang Nabiei, Da-shan Shiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14513">https://arxiv.org/abs/2505.14513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14513">https://arxiv.org/pdf/2505.14513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14513]] Latent Flow Transformer(https://arxiv.org/abs/2505.14513)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformers, the standard implementation for large language models (LLMs), typically consist of tens to hundreds of discrete layers. While more layers can lead to better performance, this approach has been challenged as far from efficient, especially given the superiority of continuous layers demonstrated by diffusion and flow-based models for image generation. We propose the Latent Flow Transformer (LFT), which replaces a block of layers with a single learned transport operator trained via flow matching, offering significant compression while maintaining compatibility with the original architecture. Additionally, we address the limitations of existing flow-based methods in \textit{preserving coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M model, LFT trained with flow matching compresses 6 of 24 layers and outperforms directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529), demonstrating the feasibility of this design. When trained with FW, LFT further distills 12 layers into one while reducing the KL to 0.736 surpassing that from skipping 3 layers (0.932), significantly narrowing the gap between autoregressive and flow-based generation paradigms.</li>
</ul>

<h3>Title: SparC: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Li, Yufei Wang, Heliang Zheng, Yihao Luo, Bihan Wen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14521">https://arxiv.org/abs/2505.14521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14521">https://arxiv.org/pdf/2505.14521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14521]] SparC: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling(https://arxiv.org/abs/2505.14521)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>High-fidelity 3D object synthesis remains significantly more challenging than 2D image generation due to the unstructured nature of mesh data and the cubic complexity of dense volumetric grids. Existing two-stage pipelines-compressing meshes with a VAE (using either 2D or 3D supervision), followed by latent diffusion sampling-often suffer from severe detail loss caused by inefficient representations and modality mismatches introduced in VAE. We introduce SparC, a unified framework that combines a sparse deformable marching cubes representation SparseCubes with a novel encoder SparConv-VAE. SparseCubes converts raw meshes into high-resolution ($1024^3$) surfaces with arbitrary topology by scattering signed distance and deformation fields onto a sparse cube, allowing differentiable optimization. SparConv-VAE is the first modality-consistent variational autoencoder built entirely upon sparse convolutional networks, enabling efficient and near-lossless 3D reconstruction suitable for high-resolution generative modeling through latent diffusion. SparC achieves state-of-the-art reconstruction fidelity on challenging inputs, including open surfaces, disconnected components, and intricate geometry. It preserves fine-grained shape details, reduces training and inference cost, and integrates naturally with latent diffusion models for scalable, high-resolution 3D generation.</li>
</ul>

<h3>Title: Interpretable Dual-Stream Learning for Local Wind Hazard Prediction in Vulnerable Communities</h3>
<ul>
<li><strong>Authors: </strong>Mahmuda Akhter Nishu, Chenyu Huang, Milad Roohi, Xin Zhong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14522">https://arxiv.org/abs/2505.14522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14522">https://arxiv.org/pdf/2505.14522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14522]] Interpretable Dual-Stream Learning for Local Wind Hazard Prediction in Vulnerable Communities(https://arxiv.org/abs/2505.14522)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Wind hazards such as tornadoes and straight-line winds frequently affect vulnerable communities in the Great Plains of the United States, where limited infrastructure and sparse data coverage hinder effective emergency response. Existing forecasting systems focus primarily on meteorological elements and often fail to capture community-specific vulnerabilities, limiting their utility for localized risk assessment and resilience planning. To address this gap, we propose an interpretable dual-stream learning framework that integrates structured numerical weather data with unstructured textual event narratives. Our architecture combines a Random Forest and RoBERTa-based transformer through a late fusion mechanism, enabling robust and context-aware wind hazard prediction. The system is tailored for underserved tribal communities and supports block-level risk assessment. Experimental results show significant performance gains over traditional baselines. Furthermore, gradient-based sensitivity and ablation studies provide insight into the model's decision-making process, enhancing transparency and operational trust. The findings demonstrate both predictive effectiveness and practical value in supporting emergency preparedness and advancing community resilience.</li>
</ul>

<h3>Title: Exploring Graph Representations of Logical Forms for Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Michael Sullivan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14523">https://arxiv.org/abs/2505.14523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14523">https://arxiv.org/pdf/2505.14523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14523]] Exploring Graph Representations of Logical Forms for Language Modeling(https://arxiv.org/abs/2505.14523)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We make the case for language models over logical forms (LFLMs), arguing that such models are more data-efficient than their textual counterparts. To that end, we introduce the Graph-based Formal-Logical Distributional Semantics (GFoLDS) prototype, a pretrained LM over graph representations of logical forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong experimental evidence that LFLMs can leverage the built-in, basic linguistic knowledge inherent in such models to immediately begin learning more complex patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual, transformer LMs pretrained on similar amounts of data, indicating that LFLMs can learn with substantially less data than models over plain text. Furthermore, we show that the performance of this model is likely to scale with additional parameters and pretraining data, suggesting the viability of LFLMs in real-world applications.</li>
</ul>

<h3>Title: diffDemorph: Extending Reference-Free Demorphing to Unseen Faces</h3>
<ul>
<li><strong>Authors: </strong>Nitish Shukla, Arun Ross</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14527">https://arxiv.org/abs/2505.14527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14527">https://arxiv.org/pdf/2505.14527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14527]] diffDemorph: Extending Reference-Free Demorphing to Unseen Faces(https://arxiv.org/abs/2505.14527)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>A face morph is created by combining two (or more) face images corresponding to two (or more) identities to produce a composite that successfully matches the constituent identities. Reference-free (RF) demorphing reverses this process using only the morph image, without the need for additional reference images. Previous RF demorphing methods were overly constrained, as they rely on assumptions about the distributions of training and testing morphs such as the morphing technique used, face style, and images used to create the morph. In this paper, we introduce a novel diffusion-based approach that effectively disentangles component images from a composite morph image with high visual fidelity. Our method is the first to generalize across morph techniques and face styles, beating the current state of the art by $\geq 59.46\%$ under a common training protocol across all datasets tested. We train our method on morphs created using synthetically generated face images and test on real morphs, thereby enhancing the practicality of the technique. Experiments on six datasets and two face matchers establish the utility and efficacy of our method.</li>
</ul>

<h3>Title: Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Yang, Junzhuo Li, Siyu Xia, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14530">https://arxiv.org/abs/2505.14530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14530">https://arxiv.org/pdf/2505.14530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14530]] Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs(https://arxiv.org/abs/2505.14530)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We show that large language models (LLMs) exhibit an $\textit{internal chain-of-thought}$: they sequentially decompose and execute composite tasks layer-by-layer. Two claims ground our study: (i) distinct subtasks are learned at different network depths, and (ii) these subtasks are executed sequentially across layers. On a benchmark of 15 two-step composite tasks, we employ layer-from context-masking and propose a novel cross-task patching method, confirming (i). To examine claim (ii), we apply LogitLens to decode hidden states, revealing a consistent layerwise execution pattern. We further replicate our analysis on the real-world $\text{TRACE}$ benchmark, observing the same stepwise dynamics. Together, our results enhance LLMs transparency by showing their capacity to internally plan and execute subtasks (or instructions), opening avenues for fine-grained, instruction-level activation steering.</li>
</ul>

<h3>Title: SifterNet: A Generalized and Model-Agnostic Trigger Purification Approach</h3>
<ul>
<li><strong>Authors: </strong>Shaoye Luo, Xinxin Fan, Quanliang Jing, Chi Lin, Mengfan Li, Yunfeng Lu, Yongjun Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14531">https://arxiv.org/abs/2505.14531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14531">https://arxiv.org/pdf/2505.14531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14531]] SifterNet: A Generalized and Model-Agnostic Trigger Purification Approach(https://arxiv.org/abs/2505.14531)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, transformer</a></li>
<li><strong>Abstract: </strong>Aiming at resisting backdoor attacks in convolution neural networks and vision Transformer-based large model, this paper proposes a generalized and model-agnostic trigger-purification approach resorting to the classic Ising model. To date, existing trigger detection/removal studies usually require to know the detailed knowledge of target model in advance, access to a large number of clean samples or even model-retraining authorization, which brings the huge inconvenience for practical applications, especially inaccessible to target model. An ideal countermeasure ought to eliminate the implanted trigger without regarding whatever the target models are. To this end, a lightweight and black-box defense approach SifterNet is proposed through leveraging the memorization-association functionality of Hopfield network, by which the triggers of input samples can be effectively purified in a proper manner. The main novelty of our proposed approach lies in the introduction of ideology of Ising model. Extensive experiments also validate the effectiveness of our approach in terms of proper trigger purification and high accuracy achievement, and compared to the state-of-the-art baselines under several commonly-used datasets, our SiferNet has a significant superior performance.</li>
</ul>

<h3>Title: Energy-Efficient Deep Reinforcement Learning with Spiking Transformers</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Irfan Uddin, Nishad Tasnim, Md Omor Faruk, Zejian Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14533">https://arxiv.org/abs/2505.14533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14533">https://arxiv.org/pdf/2505.14533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14533]] Energy-Efficient Deep Reinforcement Learning with Spiking Transformers(https://arxiv.org/abs/2505.14533)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Agent-based Transformers have been widely adopted in recent reinforcement learning advances due to their demonstrated ability to solve complex tasks. However, the high computational complexity of Transformers often results in significant energy consumption, limiting their deployment in real-world autonomous systems. Spiking neural networks (SNNs), with their biologically inspired structure, offer an energy-efficient alternative for machine learning. In this paper, a novel Spike-Transformer Reinforcement Learning (STRL) algorithm that combines the energy efficiency of SNNs with the powerful decision-making capabilities of reinforcement learning is developed. Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons and attention mechanisms capable of processing spatio-temporal patterns over multiple time steps is designed. The architecture is further enhanced with state, action, and reward encodings to create a Transformer-like structure optimized for reinforcement learning tasks. Comprehensive numerical experiments conducted on state-of-the-art benchmarks demonstrate that the proposed SNN Transformer achieves significantly improved policy performance compared to conventional agent-based Transformers. With both enhanced energy efficiency and policy optimality, this work highlights a promising direction for deploying bio-inspired, low-cost machine learning models in complex real-world decision-making scenarios.</li>
</ul>

<h3>Title: Lessons from Defending Gemini Against Indirect Prompt Injections</h3>
<ul>
<li><strong>Authors: </strong>Chongyang Shi, Sharon Lin, Shuang Song, Jamie Hayes, Ilia Shumailov, Itay Yona, Juliette Pluto, Aneesh Pappu, Christopher A. Choquette-Choo, Milad Nasr, Chawin Sitawarin, Gena Gibson, Andreas Terzis, John "Four" Flynn</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14534">https://arxiv.org/abs/2505.14534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14534">https://arxiv.org/pdf/2505.14534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14534]] Lessons from Defending Gemini Against Indirect Prompt Injections(https://arxiv.org/abs/2505.14534)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Gemini is increasingly used to perform tasks on behalf of users, where function-calling and tool-use capabilities enable the model to access user data. Some tools, however, require access to untrusted data introducing risk. Adversaries can embed malicious instructions in untrusted data which cause the model to deviate from the user's expectations and mishandle their data or permissions. In this report, we set out Google DeepMind's approach to evaluating the adversarial robustness of Gemini models and describe the main lessons learned from the process. We test how Gemini performs against a sophisticated adversary through an adversarial evaluation framework, which deploys a suite of adaptive attack techniques to run continuously against past, current, and future versions of Gemini. We describe how these ongoing evaluations directly help make Gemini more resilient against manipulation.</li>
</ul>

<h3>Title: Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Agam Goyal, Vedant Rathi, William Yeh, Yian Wang, Yuen Chen, Hari Sundaram</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14536">https://arxiv.org/abs/2505.14536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14536">https://arxiv.org/pdf/2505.14536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14536]] Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders(https://arxiv.org/abs/2505.14536)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are now ubiquitous in user-facing applications, yet they still generate undesirable toxic outputs, including profanity, vulgarity, and derogatory remarks. Although numerous detoxification methods exist, most apply broad, surface-level fixes and can therefore easily be circumvented by jailbreak attacks. In this paper we leverage sparse autoencoders (SAEs) to identify toxicity-related directions in the residual stream of models and perform targeted activation steering using the corresponding decoder vectors. We introduce three tiers of steering aggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing trade-offs between toxicity reduction and language fluency. At stronger steering strengths, these causal interventions surpass competitive baselines in reducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2 Small depending on the aggressiveness. Crucially, standard NLP benchmark scores upon steering remain stable, indicating that the model's knowledge and general abilities are preserved. We further show that feature-splitting in wider SAEs hampers safety interventions, underscoring the importance of disentangled feature learning. Our findings highlight both the promise and the current limitations of SAE-based causal interventions for LLM detoxification, further suggesting practical guidelines for safer language-model deployment.</li>
</ul>

<h3>Title: Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Utsav Dutta, Sina Khoshfetrat Pakazad, Henrik Ohlsson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14543">https://arxiv.org/abs/2505.14543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14543">https://arxiv.org/pdf/2505.14543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14543]] Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions(https://arxiv.org/abs/2505.14543)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Traditional time series models are task-specific and often depend on dataset-specific training and extensive feature engineering. While Transformer-based architectures have improved scalability, foundation models, commonplace in text, vision, and audio, remain under-explored for time series and are largely restricted to forecasting. We introduce $\textbf{CHARM}$, a foundation embedding model for multivariate time series that learns shared, transferable, and domain-aware representations. To address the unique difficulties of time series foundation learning, $\textbf{CHARM}$ incorporates architectural innovations that integrate channel-level textual descriptions while remaining invariant to channel order. The model is trained using a Joint Embedding Predictive Architecture (JEPA), with novel augmentation schemes and a loss function designed to improve interpretability and training stability. Our $7$M-parameter model achieves state-of-the-art performance across diverse downstream tasks, setting a new benchmark for time series representation learning.</li>
</ul>

<h3>Title: Can Large Language Models Really Recognize Your Name?</h3>
<ul>
<li><strong>Authors: </strong>Dzung Pham, Peter Kairouz, Niloofar Mireshghallah, Eugene Bagdasarian, Chau Minh Pham, Amir Houmansadr</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14549">https://arxiv.org/abs/2505.14549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14549">https://arxiv.org/pdf/2505.14549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14549]] Can Large Language Models Really Recognize Your Name?(https://arxiv.org/abs/2505.14549)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly being used to protect sensitive user data. However, current LLM-based privacy solutions assume that these models can reliably detect personally identifiable information (PII), particularly named entities. In this paper, we challenge that assumption by revealing systematic failures in LLM-based privacy tasks. Specifically, we show that modern LLMs regularly overlook human names even in short text snippets due to ambiguous contexts, which cause the names to be misinterpreted or mishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous human names, leveraging the name regularity bias phenomenon, embedded within concise text snippets along with benign prompt injections. Our experiments on modern LLMs tasked to detect PII as well as specialized tools show that recall of ambiguous names drops by 20--40% compared to more recognizable names. Furthermore, ambiguous human names are four times more likely to be ignored in supposedly privacy-preserving summaries generated by LLMs when benign prompt injections are present. These findings highlight the underexplored risks of relying solely on LLMs to safeguard user privacy and underscore the need for a more systematic investigation into their privacy failure modes.</li>
</ul>

<h3>Title: KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Shi, Jian Yang, Jiaheng Liu, Xingyuan Bu, Jiangjie Chen, Junting Zhou, Kaijing Ma, Zhoufutu Wen, Bingli Wang, Yancheng He, Liang Song, Hualei Zhu, Shilong Li, Xingjian Wang, Wei Zhang, Ruibin Yuan, Yifan Yao, Wenjun Yang, Yunli Wang, Siyuan Fang, Siyu Yuan, Qianyu He, Xiangru Tang, Yingshui Tan, Wangchunshu Zhou, Zhaoxiang Zhang, Zhoujun Li, Wenhao Huang, Ge Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14552">https://arxiv.org/abs/2505.14552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14552">https://arxiv.org/pdf/2505.14552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14552]] KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation(https://arxiv.org/abs/2505.14552)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) underscore the need for more comprehensive evaluation methods to accurately assess their reasoning capabilities. Existing benchmarks are often domain-specific and thus cannot fully capture an LLM's general reasoning potential. To address this limitation, we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over fifty games in either textual or visual formats and supports interactive, multi-turn assessments with reinforcement learning scenarios. Using KORGym, we conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent reasoning patterns within model families and demonstrating the superior performance of closed-source models. Further analysis examines the effects of modality, reasoning strategies, reinforcement learning techniques, and response length on model performance. We expect KORGym to become a valuable resource for advancing LLM reasoning research and developing evaluation methodologies suited to complex, interactive environments.</li>
</ul>

<h3>Title: Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yingtao Luo, Shikai Fang, Binqing Wu, Qingsong Wen, Liang Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14555">https://arxiv.org/abs/2505.14555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14555">https://arxiv.org/pdf/2505.14555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14555]] Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting(https://arxiv.org/abs/2505.14555)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Weather forecasting is essential but remains computationally intensive and physically incomplete in traditional numerical weather prediction (NWP) methods. Deep learning (DL) models offer efficiency and accuracy but often ignore physical laws, limiting interpretability and generalization. We propose PhyDL-NWP, a physics-guided deep learning framework that integrates physical equations with latent force parameterization into data-driven models. It predicts weather variables from arbitrary spatiotemporal coordinates, computes physical terms via automatic differentiation, and uses a physics-informed loss to align predictions with governing dynamics. PhyDL-NWP enables resolution-free downscaling by modeling weather as a continuous function and fine-tunes pre-trained models with minimal overhead, achieving up to 170x faster inference with only 55K parameters. Experiments show that PhyDL-NWP improves both forecasting performance and physical consistency.</li>
</ul>

<h3>Title: Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI</h3>
<ul>
<li><strong>Authors: </strong>Marlène Careil, Yohann Benchetrit, Jean-Rémi King</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14556">https://arxiv.org/abs/2505.14556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14556">https://arxiv.org/pdf/2505.14556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14556]] Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI(https://arxiv.org/abs/2505.14556)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Brain-to-image decoding has been recently propelled by the progress in generative AI models and the availability of large ultra-high field functional Magnetic Resonance Imaging (fMRI). However, current approaches depend on complicated multi-stage pipelines and preprocessing steps that typically collapse the temporal dimension of brain recordings, thereby limiting time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural Activity Diffusion for Image Reconstruction), a new single-stage diffusion model designed for reconstructing images from dynamically evolving fMRI recordings. Our approach offers three main contributions. First, Dynadiff simplifies training as compared to existing approaches. Second, our model outperforms state-of-the-art models on time-resolved fMRI signals, especially on high-level semantic image reconstruction metrics, while remaining competitive on preprocessed fMRI data that collapse time. Third, this approach allows a precise characterization of the evolution of image representations in brain activity. Overall, this work lays the foundation for time-resolved brain-to-image decoding.</li>
</ul>

<h3>Title: TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring</h3>
<ul>
<li><strong>Authors: </strong>Sohaila Eltanbouly, Salam Albatarni, Tamer Elsayed</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14577">https://arxiv.org/abs/2505.14577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14577">https://arxiv.org/pdf/2505.14577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14577]] TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring(https://arxiv.org/abs/2505.14577)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there is a notable lack of attention for assessing essays according to individual traits. In this work, we propose TRATES, a novel trait-specific and rubric-based cross-prompt AES framework that is generic yet specific to the underlying trait. The framework leverages a Large Language Model (LLM) that utilizes the trait grading rubrics to generate trait-specific features (represented by assessment questions), then assesses those features given an essay. The trait-specific features are eventually combined with generic writing-quality and prompt-specific features to train a simple classical regression model that predicts trait scores of essays from an unseen prompt. Experiments show that TRATES achieves a new state-of-the-art performance across all traits on a widely-used dataset, with the generated LLM-based features being the most significant.</li>
</ul>

<h3>Title: Instance Segmentation for Point Sets</h3>
<ul>
<li><strong>Authors: </strong>Abhimanyu Talwar, Julien Laasri</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14583">https://arxiv.org/abs/2505.14583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14583">https://arxiv.org/pdf/2505.14583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14583]] Instance Segmentation for Point Sets(https://arxiv.org/abs/2505.14583)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recently proposed neural network architectures like PointNet [QSMG16] and PointNet++ [QYSG17] have made it possible to apply Deep Learning to 3D point sets. The feature representations of shapes learned by these two networks enabled training classifiers for Semantic Segmentation, and more recently for Instance Segmentation via the Similarity Group Proposal Network (SGPN) [WYHN17]. One area of improvement which has been highlighted by SGPN's authors, pertains to use of memory intensive similarity matrices which occupy memory quadratic in the number of points. In this report, we attempt to tackle this issue through use of two sampling based methods, which compute Instance Segmentation on a sub-sampled Point Set, and then extrapolate labels to the complete set using the nearest neigbhour approach. While both approaches perform equally well on large sub-samples, the random-based strategy gives the most improvements in terms of speed and memory usage.</li>
</ul>

<h3>Title: Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Wenbin Hu, Haoran Li, Huihao Jing, Qi Hu, Ziqian Zeng, Sirui Han, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14585">https://arxiv.org/abs/2505.14585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14585">https://arxiv.org/pdf/2505.14585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14585]] Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning(https://arxiv.org/abs/2505.14585)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) exhibit remarkable capabilities, they also introduce significant safety and privacy risks. Current mitigation strategies often fail to preserve contextual reasoning capabilities in risky scenarios. Instead, they rely heavily on sensitive pattern matching to protect LLMs, which limits the scope. Furthermore, they overlook established safety and privacy standards, leading to systemic risks for legal compliance. To address these gaps, we formulate safety and privacy issues into contextualized compliance problems following the Contextual Integrity (CI) theory. Under the CI framework, we align our model with three critical regulatory standards: GDPR, EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with a rule-based reward to incentivize contextual reasoning capabilities while enhancing compliance with safety and privacy norms. Through extensive experiments, we demonstrate that our method not only significantly enhances legal compliance (achieving a +17.64% accuracy improvement in safety/privacy benchmarks) but also further improves general reasoning capability. For OpenThinker-7B, a strong reasoning model that significantly outperforms its base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on the MMLU and LegalBench benchmark, respectively.</li>
</ul>

<h3>Title: MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol</h3>
<ul>
<li><strong>Authors: </strong>Huihao Jing, Haoran Li, Wenbin Hu, Qi Hu, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14590">https://arxiv.org/abs/2505.14590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14590">https://arxiv.org/pdf/2505.14590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14590]] MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol(https://arxiv.org/abs/2505.14590)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users and developers, it also brings underexplored safety risks. Its decentralized architecture, which separates clients and servers, poses unique challenges for systematic safety analysis. This paper proposes a novel framework to enhance MCP safety. Guided by the MAESTRO framework, we first analyze the missing safety mechanisms in MCP, and based on this analysis, we propose the Model Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses these this http URL, we develop a fine-grained taxonomy that captures a diverse range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy, we develop benchmark and training data that support the evaluation and improvement of LLMs' capabilities in identifying safety risks within MCP interactions. Leveraging the proposed benchmark and training data, we conduct extensive experiments on state-of-the-art LLMs. The results highlight LLMs' vulnerabilities in MCP interactions and demonstrate that our approach substantially improves their safety performance.</li>
</ul>

<h3>Title: Adaptive Pruning of Deep Neural Networks for Resource-Aware Embedded Intrusion Detection on the Edge</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Broggi, Nathaniel Bastian, Lance Fiondella, Gokhan Kul</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14592">https://arxiv.org/abs/2505.14592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14592">https://arxiv.org/pdf/2505.14592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14592]] Adaptive Pruning of Deep Neural Networks for Resource-Aware Embedded Intrusion Detection on the Edge(https://arxiv.org/abs/2505.14592)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Artificial neural network pruning is a method in which artificial neural network sizes can be reduced while attempting to preserve the predicting capabilities of the network. This is done to make the model smaller or faster during inference time. In this work we analyze the ability of a selection of artificial neural network pruning methods to generalize to a new cybersecurity dataset utilizing a simpler network type than was designed for. We analyze each method using a variety of pruning degrees to best understand how each algorithm responds to the new environment. This has allowed us to determine the most well fit pruning method of those we searched for the task. Unexpectedly, we have found that many of them do not generalize to the problem well, leaving only a few algorithms working to an acceptable degree.</li>
</ul>

<h3>Title: Physics-informed Reduced Order Modeling of Time-dependent PDEs via Differentiable Solvers</h3>
<ul>
<li><strong>Authors: </strong>Nima Hosseini Dashtbayaz, Hesam Salehipour, Adrian Butscher, Nigel Morris</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14595">https://arxiv.org/abs/2505.14595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14595">https://arxiv.org/pdf/2505.14595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14595]] Physics-informed Reduced Order Modeling of Time-dependent PDEs via Differentiable Solvers(https://arxiv.org/abs/2505.14595)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reduced-order modeling (ROM) of time-dependent and parameterized differential equations aims to accelerate the simulation of complex high-dimensional systems by learning a compact latent manifold representation that captures the characteristics of the solution fields and their time-dependent dynamics. Although high-fidelity numerical solvers generate the training datasets, they have thus far been excluded from the training process, causing the learned latent dynamics to drift away from the discretized governing physics. This mismatch often limits generalization and forecasting capabilities. In this work, we propose Physics-informed ROM ($\Phi$-ROM) by incorporating differentiable PDE solvers into the training procedure. Specifically, the latent space dynamics and its dependence on PDE parameters are shaped directly by the governing physics encoded in the solver, ensuring a strong correspondence between the full and reduced systems. Our model outperforms state-of-the-art data-driven ROMs and other physics-informed strategies by accurately generalizing to new dynamics arising from unseen parameters, enabling long-term forecasting beyond the training horizon, maintaining continuity in both time and space, and reducing the data cost. Furthermore, $\Phi$-ROM learns to recover and forecast the solution fields even when trained or evaluated with sparse and irregular observations of the fields, providing a flexible framework for field reconstruction and data assimilation. We demonstrate the framework's robustness across different PDE solvers and highlight its broad applicability by providing an open-source JAX implementation readily extensible to other PDE systems and differentiable solvers.</li>
</ul>

<h3>Title: Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guangzhi Xiong, Eric Xie, Corey Williams, Myles Kim, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14599">https://arxiv.org/abs/2505.14599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14599">https://arxiv.org/pdf/2505.14599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14599]] Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models(https://arxiv.org/abs/2505.14599)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, a benchmark for assessing the capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at this https URL.</li>
</ul>

<h3>Title: Electrostatics from Laplacian Eigenbasis for Neural Network Interatomic Potentials</h3>
<ul>
<li><strong>Authors: </strong>Maksim Zhdanov, Vladislav Kurenkov</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14606">https://arxiv.org/abs/2505.14606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14606">https://arxiv.org/pdf/2505.14606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14606]] Electrostatics from Laplacian Eigenbasis for Neural Network Interatomic Potentials(https://arxiv.org/abs/2505.14606)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in neural network interatomic potentials have emerged as a promising research direction. However, popular deep learning models often lack auxiliary constraints grounded in physical laws, which could accelerate training and improve fidelity through physics-based regularization. In this work, we introduce $\Phi$-Module, a universal plugin module that enforces Poisson's equation within the message-passing framework to learn electrostatic interactions in a self-supervised manner. Specifically, each atom-wise representation is encouraged to satisfy a discretized Poisson's equation, making it possible to acquire a potential $\boldsymbol{\phi}$ and a corresponding charge density $\boldsymbol{\rho}$ linked to the learnable Laplacian eigenbasis coefficients of a given molecular graph. We then derive an electrostatic energy term, crucial for improved total energy predictions. This approach integrates seamlessly into any existing neural potential with insignificant computational overhead. Experiments on the OE62 and MD22 benchmarks confirm that models combined with $\Phi$-Module achieve robust improvements over baseline counterparts. For OE62 error reduction ranges from 4.5\% to 17.8\%, and for MD22, baseline equipped with $\Phi$-Module achieves best results on 5 out of 14 cases. Our results underscore how embedding a first-principles constraint in neural interatomic potentials can significantly improve performance while remaining hyperparameter-friendly, memory-efficient and lightweight in training. Code will be available at \href{this https URL}{dunnolab/phi-module}.</li>
</ul>

<h3>Title: sudoLLM : On Multi-role Alignment of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Soumadeep Saha, Akshay Chaturvedi, Joy Mahapatra, Utpal Garain</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14607">https://arxiv.org/abs/2505.14607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14607">https://arxiv.org/pdf/2505.14607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14607]] sudoLLM : On Multi-role Alignment of Language Models(https://arxiv.org/abs/2505.14607)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>User authorization-based access privileges are a key feature in many safety-critical systems, but have thus far been absent from the large language model (LLM) realm. In this work, drawing inspiration from such access control systems, we introduce sudoLLM, a novel framework that results in multi-role aligned LLMs, i.e., LLMs that account for, and behave in accordance with, user access rights. sudoLLM injects subtle user-based biases into queries and trains an LLM to utilize this bias signal in order to produce sensitive information if and only if the user is authorized. We present empirical results demonstrating that this approach shows substantially improved alignment, generalization, and resistance to prompt-based jailbreaking attacks. The persistent tension between the language modeling objective and safety alignment, which is often exploited to jailbreak LLMs, is somewhat resolved with the aid of the injected bias signal. Our framework is meant as an additional security layer, and complements existing guardrail mechanisms for enhanced end-to-end safety with LLMs.</li>
</ul>

<h3>Title: Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)</h3>
<ul>
<li><strong>Authors: </strong>Rafael Rivera Soto, Barry Chen, Nicholas Andrews</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14608">https://arxiv.org/abs/2505.14608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14608">https://arxiv.org/pdf/2505.14608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14608]] Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)(https://arxiv.org/abs/2505.14608)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Despite considerable progress in the development of machine-text detectors, it has been suggested that the problem is inherently hard, and therefore, that stakeholders should proceed under the assumption that machine-generated text cannot be reliably detected as such. We examine a recent such claim by Nicks et al. (2024) regarding the ease with which language models can be optimized to degrade the performance of machine-text detectors, including detectors not specifically optimized against. We identify a feature space$\unicode{x2013}$the stylistic feature space$\unicode{x2013}$that is robust to such optimization, and show that it may be used to reliably detect samples from language models optimized to prevent detection. Furthermore, we show that even when models are explicitly optimized against stylistic detectors, detection performance remains surprisingly unaffected. We then seek to understand if stylistic detectors are inherently more robust. To study this question, we explore a new paraphrasing approach that simultaneously aims to close the gap between human writing and machine writing in stylistic feature space while avoiding detection using traditional features. We show that when only a single sample is available for detection, this attack is universally effective across all detectors considered, including those that use writing style. However, as the number of samples available for detection grows, the human and machine distributions become distinguishable. This observation encourages us to introduce AURA, a metric that estimates the overlap between human and machine-generated distributions by analyzing how detector performance improves as more samples become available. Overall, our findings underscore previous recommendations to avoid reliance on machine-text detection.</li>
</ul>

<h3>Title: TSA-WF: Exploring the Effectiveness of Time Series Analysis for Website Fingerprinting</h3>
<ul>
<li><strong>Authors: </strong>Michael Wrana, Uzma Maroof, Diogo Barradas</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14616">https://arxiv.org/abs/2505.14616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14616">https://arxiv.org/pdf/2505.14616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14616]] TSA-WF: Exploring the Effectiveness of Time Series Analysis for Website Fingerprinting(https://arxiv.org/abs/2505.14616)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, extraction</a></li>
<li><strong>Abstract: </strong>Website fingerprinting (WF) is a technique that allows an eavesdropper to determine the website a target user is accessing by inspecting the metadata associated with the packets she exchanges via some encrypted tunnel, e.g., Tor. Recent WF attacks built using machine learning (and deep learning) process and summarize trace metadata during their feature extraction phases. This methodology leads to predictions that lack information about the instant at which a given website is detected within a (potentially large) network trace comprised of multiple sequential website accesses -- a setting known as \textit{multi-tab} WF. In this paper, we explore whether classical time series analysis techniques can be effective in the WF setting. Specifically, we introduce TSA-WF, a pipeline designed to closely preserve network traces' timing and direction characteristics, which enables the exploration of algorithms designed to measure time series similarity in the WF context. Our evaluation with Tor traces reveals that TSA-WF achieves a comparable accuracy to existing WF attacks in scenarios where website accesses can be easily singled-out from a given trace (i.e., the \textit{single-tab} WF setting), even when shielded by specially designed WF defenses. Finally, while TSA-WF did not outperform existing attacks in the multi-tab setting, we show how TSA-WF can help pinpoint the approximate instant at which a given website of interest is visited within a multi-tab trace.\footnote{This preprint has not undergone any post-submission improvements or corrections. The Version of Record of this contribution is published in the Proceedings of the 20th International Conference on Availability, Reliability and Security (ARES 2025)}</li>
</ul>

<h3>Title: Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Sahar Abdelnabi, Ahmed Salem</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14617">https://arxiv.org/abs/2505.14617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14617">https://arxiv.org/pdf/2505.14617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14617]] Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models(https://arxiv.org/abs/2505.14617)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning-focused large language models (LLMs) sometimes alter their behavior when they detect that they are being evaluated, an effect analogous to the Hawthorne phenomenon, which can lead them to optimize for test-passing performance or to comply more readily with harmful prompts if real-world consequences appear absent. We present the first quantitative study of how such "test awareness" impacts model behavior, particularly its safety alignment. We introduce a white-box probing framework that (i) linearly identifies awareness-related activations and (ii) steers models toward or away from test awareness while monitoring downstream performance. We apply our method to different state-of-the-art open-source reasoning LLMs across both realistic and hypothetical tasks. Our results demonstrate that test awareness significantly impact safety alignment, and is different for different models. By providing fine-grained control over this latent effect, our work aims to increase trust in how we perform safety evaluation.</li>
</ul>

<h3>Title: Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs</h3>
<ul>
<li><strong>Authors: </strong>Morgan Lindsay Heisler, Linzi Xing, Ge Shi, Hanieh Sadri, Gursimran Singh, Weiwei Zhang, Tao Ye, Ying Xiong, Yong Zhang, Zhenan Fan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14620">https://arxiv.org/abs/2505.14620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14620">https://arxiv.org/pdf/2505.14620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14620]] Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs(https://arxiv.org/abs/2505.14620)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and scalable method to fine-tune and customize large language models (LLMs) for application-specific needs. However, tasks that require complex reasoning or deep contextual understanding are often hindered by biases or interference from the base model when using typical decoding methods like greedy or beam search. These biases can lead to generic or task-agnostic responses from the base model instead of leveraging the LoRA-specific adaptations. In this paper, we introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed to maximize the use of task-specific knowledge in LoRA-adapted models, resulting in better downstream performance. CoLD uses contrastive decoding by scoring candidate tokens based on the divergence between the probability distributions of a LoRA-adapted expert model and the corresponding base model. This approach prioritizes tokens that better align with the LoRA's learned representations, enhancing performance for specialized tasks. While effective, a naive implementation of CoLD is computationally expensive because each decoding step requires evaluating multiple token candidates across both models. To address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD achieves up to a 5.54% increase in task accuracy while reducing end-to-end latency by 28% compared to greedy decoding. This work provides practical and efficient decoding strategies for fine-tuned LLMs in resource-constrained environments and has broad implications for applied data science in both cloud and on-premises settings.</li>
</ul>

<h3>Title: TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14625">https://arxiv.org/abs/2505.14625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14625">https://arxiv.org/pdf/2505.14625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14625]] TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning(https://arxiv.org/abs/2505.14625)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RL's success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problem--false negatives--where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at this https URL.</li>
</ul>

<h3>Title: KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fnu Mohbat, Mohammed J Zaki</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14629">https://arxiv.org/abs/2505.14629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14629">https://arxiv.org/pdf/2505.14629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14629]] KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models(https://arxiv.org/abs/2505.14629)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) and the abundance of food data have resulted in studies to improve food understanding using LLMs. Despite several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there has been limited research on integrating food related KGs with LLMs. We introduce KERL, a unified system that leverages food KGs and LLMs to provide personalized food recommendations and generates recipes with associated micro-nutritional information. Given a natural language question, KERL extracts entities, retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints. Next, our system generates the cooking steps and nutritional information for each recipe. To evaluate our approach, we also develop a benchmark dataset by curating recipe related questions, combined with constraints and personal preferences. Through extensive experiments, we show that our proposed KG-augmented LLM significantly outperforms existing approaches, offering a complete and coherent solution for food recommendation, recipe generation, and nutritional analysis. Our code and benchmark datasets are publicly available at this https URL.</li>
</ul>

<h3>Title: Think Only When You Need with Large Hybrid-Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14631">https://arxiv.org/abs/2505.14631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14631">https://arxiv.org/pdf/2505.14631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14631]] Think Only When You Need with Large Hybrid-Reasoning Models(https://arxiv.org/abs/2505.14631)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, which is particularly unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform thinking based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate thinking mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the model's capability for hybrid thinking. Extensive experimental results show that LHRMs can adaptively perform hybrid thinking on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended thinking processes and provides a solid starting point for building hybrid thinking systems.</li>
</ul>

<h3>Title: VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Wentao Ma, Weiming Ren, Yiming Jia, Zhuofeng Li, Ping Nie, Ge Zhang, Wenhu Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14640">https://arxiv.org/abs/2505.14640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14640">https://arxiv.org/pdf/2505.14640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14640]] VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation(https://arxiv.org/abs/2505.14640)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large multimodal models (LMMs) have recently emerged as a powerful tool for long video understanding (LVU), prompting the development of standardized LVU benchmarks to evaluate their performance. However, our investigation reveals a rather sober lesson for existing LVU benchmarks. First, most existing benchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation results are inflated due to the possibility of guessing the correct answer; Second, a significant portion of questions in these benchmarks have strong priors to allow models to answer directly without even reading the input video. For example, Gemini-1.5-Pro can achieve over 50\% accuracy given a random frame from a long video on Video-MME. We also observe that increasing the number of frames does not necessarily lead to improvement on existing benchmarks, which is counterintuitive. As a result, the validity and robustness of current LVU benchmarks are undermined, impeding a faithful assessment of LMMs' long-video understanding capability. To tackle this problem, we propose VideoEval-Pro, a realistic LVU benchmark containing questions with open-ended short-answer, which truly require understanding the entire video. VideoEval-Pro assesses both segment-level and full-video understanding through perception and reasoning tasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the following findings: (1) video LMMs show drastic performance ($>$25\%) drops on open-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do not lead to higher open-ended scores on VideoEval-Pro; (3) compared to other MCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input frames. Our results show that VideoEval-Pro offers a more realistic and reliable measure of long video understanding, providing a clearer view of progress in this domain.</li>
</ul>

<h3>Title: General-Reasoner: Advancing LLM Reasoning Across All Domains</h3>
<ul>
<li><strong>Authors: </strong>Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, Wenhu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14652">https://arxiv.org/abs/2505.14652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14652">https://arxiv.org/pdf/2505.14652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14652]] General-Reasoner: Advancing LLM Reasoning Across All Domains(https://arxiv.org/abs/2505.14652)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks.</li>
</ul>

<h3>Title: Beyond Words: Multimodal LLM Knows When to Speak</h3>
<ul>
<li><strong>Authors: </strong>Zikai Liao, Yi Ouyang, Yi-Lun Lee, Chen-Ping Yu, Yi-Hsuan Tsai, Zhaozheng Yin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14654">https://arxiv.org/abs/2505.14654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14654">https://arxiv.org/pdf/2505.14654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14654]] Beyond Words: Multimodal LLM Knows When to Speak(https://arxiv.org/abs/2505.14654)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language model (LLM)-based chatbots have demonstrated strong capabilities in generating coherent and contextually relevant responses, they often struggle with understanding when to speak, particularly in delivering brief, timely reactions during ongoing conversations. This limitation arises largely from their reliance on text input, lacking the rich contextual cues in real-world human dialogue. In this work, we focus on real-time prediction of response types, with an emphasis on short, reactive utterances that depend on subtle, multimodal signals across vision, audio, and text. To support this, we introduce a new multimodal dataset constructed from real-world conversational videos, containing temporally aligned visual, auditory, and textual streams. This dataset enables fine-grained modeling of response timing in dyadic interactions. Building on this dataset, we propose MM-When2Speak, a multimodal LLM-based model that adaptively integrates visual, auditory, and textual context to predict when a response should occur, and what type of response is appropriate. Experiments show that MM-When2Speak significantly outperforms state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x improvement in response timing accuracy over leading commercial LLMs. These results underscore the importance of multimodal inputs for producing timely, natural, and engaging conversational AI.</li>
</ul>

<h3>Title: Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks</h3>
<ul>
<li><strong>Authors: </strong>Navneet Kaur, Lav Gupta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14659">https://arxiv.org/abs/2505.14659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14659">https://arxiv.org/pdf/2505.14659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14659]] Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks(https://arxiv.org/abs/2505.14659)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>As healthcare systems increasingly adopt advanced wireless networks and connected devices, securing medical applications has become critical. The integration of Internet of Medical Things devices, such as robotic surgical tools, intensive care systems, and wearable monitors has enhanced patient care but introduced serious security risks. Cyberattacks on these devices can lead to life threatening consequences, including surgical errors, equipment failure, and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative role in healthcare through AI and cloud integration, it also raises new security concerns. This paper explores how explainable AI techniques like SHAP, LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve trust and transparency in 6G enabled healthcare. We support our approach with experimental analysis and highlight promising results.</li>
</ul>

<h3>Title: Quartet: Native FP4 Training Can Be Optimal for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Roberto L. Castro, Andrei Panferov, Soroush Tabesh, Oliver Sieberling, Jiale Chen, Mahdi Nikdan, Saleh Ashkboos, Dan Alistarh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14669">https://arxiv.org/abs/2505.14669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14669">https://arxiv.org/pdf/2505.14669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14669]] Quartet: Native FP4 Training Can Be Optimal for Large Language Models(https://arxiv.org/abs/2505.14669)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has been paralleled by unprecedented increases in computational demands, with training costs for state-of-the-art models doubling every few months. Training models directly in low-precision arithmetic offers a solution, by improving both computational throughput and energy efficiency. Specifically, NVIDIA's recent Blackwell architecture facilitates extremely low-precision operations, specifically FP4 variants, promising substantial efficiency gains. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we systematically investigate hardware-supported FP4 training and introduce Quartet, a new approach enabling accurate, end-to-end FP4 training with all the major computations (in e.g. linear layers) being performed in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across varying bit-widths and allows us to identify a "near-optimal" low-precision training technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve state-of-the-art accuracy for FP4 precision, successfully training billion-scale models. Our method demonstrates that fully FP4-based training is a competitive alternative to standard-precision and FP8 training. Our code is available at this https URL.</li>
</ul>

<h3>Title: Training-Free Watermarking for Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yu Tong, Zihao Pan, Shuai Yang, Kaiyang Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14673">https://arxiv.org/abs/2505.14673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14673">https://arxiv.org/pdf/2505.14673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14673]] Training-Free Watermarking for Autoregressive Image Generation(https://arxiv.org/abs/2505.14673)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust, watermark, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Invisible image watermarking can protect image ownership and prevent malicious misuse of visual generative models. However, existing generative watermarking methods are mainly designed for diffusion models while watermarking for autoregressive image generation models remains largely underexplored. We propose IndexMark, a training-free watermarking framework for autoregressive image generation models. IndexMark is inspired by the redundancy property of the codebook: replacing autoregressively generated indices with similar indices produces negligible visual differences. The core component in IndexMark is a simple yet effective match-then-replace method, which carefully selects watermark tokens from the codebook based on token similarity, and promotes the use of watermark tokens through token replacement, thereby embedding the watermark without affecting the image quality. Watermark verification is achieved by calculating the proportion of watermark tokens in generated images, with precision further improved by an Index Encoder. Furthermore, we introduce an auxiliary validation scheme to enhance robustness against cropping attacks. Experiments demonstrate that IndexMark achieves state-of-the-art performance in terms of image quality and verification accuracy, and exhibits robustness against various perturbations, including cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG compression.</li>
</ul>

<h3>Title: Reward Reasoning Model</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14674">https://arxiv.org/abs/2505.14674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14674">https://arxiv.org/pdf/2505.14674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14674]] Reward Reasoning Model(https://arxiv.org/abs/2505.14674)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy. The pretrained reward reasoning models are available at this https URL.</li>
</ul>

<h3>Title: Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiaer Xia, Yuhang Zang, Peng Gao, Yixuan Li, Kaiyang Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14677">https://arxiv.org/abs/2505.14677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14677">https://arxiv.org/pdf/2505.14677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14677]] Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning(https://arxiv.org/abs/2505.14677)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Learning general-purpose reasoning capabilities has long been a challenging problem in AI. Recent research in large language models (LLMs), such as DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can enable pre-trained LLMs to develop reasoning capabilities using simple question-answer pairs. In this paper, we aim to train visual language models (VLMs) to perform reasoning on image data through reinforcement learning and visual question-answer pairs, without any explicit chain-of-thought (CoT) supervision. Our findings indicate that simply applying reinforcement learning to a VLM -- by prompting the model to produce a reasoning chain before providing an answer -- can lead the model to develop shortcuts from easy questions, thereby reducing its ability to generalize across unseen data distributions. We argue that the key to mitigating shortcut learning is to encourage the model to interpret images prior to reasoning. Therefore, we train the model to adhere to a caption-reason-answer output format: initially generating a detailed caption for an image, followed by constructing an extensive reasoning chain. When trained on 273K CoT-free visual question-answer pairs and using only reinforcement learning, our model, named Visionary-R1, outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, on multiple visual reasoning benchmarks.</li>
</ul>

<h3>Title: UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaojie Gu, Guangxu Chen, Jungang Li, Jia-Chen Gu, Xuming Hu, Kai Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14679">https://arxiv.org/abs/2505.14679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14679">https://arxiv.org/pdf/2505.14679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14679]] UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models(https://arxiv.org/abs/2505.14679)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Lifelong learning enables large language models (LLMs) to adapt to evolving information by continually updating their internal knowledge. An ideal system should support efficient, wide-ranging updates while preserving existing capabilities and ensuring reliable deployment. Model editing stands out as a promising solution for this goal, offering a focused and efficient way to revise a model's internal knowledge. Although recent paradigms have made notable progress, they often struggle to meet the demands of practical lifelong adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally new editing solution that is training-, subject- and memory-free, making it particularly well-suited for ultra-scalable, real-world lifelong model editing. ULTRAEDIT performs editing through a self-contained process that relies solely on lightweight linear algebra operations to compute parameter shifts, enabling fast and consistent parameter modifications with minimal overhead. To improve scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization strategy that continuously updates feature statistics across turns, allowing it to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT achieves editing speeds over 7x faster than the previous state-of-the-art method-which was also the fastest known approach-while consuming less than 1/3 the VRAM, making it the only method currently capable of editing a 7B LLM on a 24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest dataset in the field to date, with over 2M editing pairs-and demonstrate that our method supports up to 1M edits while maintaining high accuracy. Comprehensive experiments on four datasets and six models show that ULTRAEDIT consistently achieves superior performance across diverse model editing scenarios. Our code is available at: this https URL.</li>
</ul>

<h3>Title: UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Rui Tian, Mingfei Gao, Mingze Xu, Jiaming Hu, Jiasen Lu, Zuxuan Wu, Yinfei Yang, Afshin Dehghan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14682">https://arxiv.org/abs/2505.14682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14682">https://arxiv.org/pdf/2505.14682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14682]] UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation(https://arxiv.org/abs/2505.14682)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce UniGen, a unified multimodal large language model (MLLM) capable of image understanding and generation. We study the full training pipeline of UniGen from a data-centric perspective, including multi-stage pre-training, supervised fine-tuning, and direct preference optimization. More importantly, we propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time scaling, which significantly boosts UniGen's image generation quality using a simple Best-of-N test-time strategy. Specifically, CoT-V enables UniGen to act as both image generator and verifier at test time, assessing the semantic alignment between a text prompt and its generated image in a step-by-step CoT manner. Trained entirely on open-source datasets across all stages, UniGen achieves state-of-the-art performance on a range of image understanding and generation benchmarks, with a final score of 0.78 on GenEval and 85.19 on DPG-Bench. Through extensive ablation studies, our work provides actionable insights and addresses key challenges in the full life cycle of building unified MLLMs, contributing meaningful directions to the future research.</li>
</ul>

<h3>Title: Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning</h3>
<ul>
<li><strong>Authors: </strong>Haolei Xu, Yuchen Yan, Yongliang Shen, Wenqi Zhang, Guiyang Hou, Shengpei Jiang, Kaitao Song, Weiming Lu, Jun Xiao, Yueting Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14684">https://arxiv.org/abs/2505.14684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14684">https://arxiv.org/pdf/2505.14684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14684]] Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning(https://arxiv.org/abs/2505.14684)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable progress on mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits.</li>
</ul>

<h3>Title: Grouping First, Attending Smartly: Training-Free Acceleration for Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Sucheng Ren, Qihang Yu, Ju He, Alan Yuille, Liang-Chieh Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14687">https://arxiv.org/abs/2505.14687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14687">https://arxiv.org/pdf/2505.14687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14687]] Grouping First, Attending Smartly: Training-Free Acceleration for Diffusion Transformers(https://arxiv.org/abs/2505.14687)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based Transformers have demonstrated impressive generative capabilities, but their high computational costs hinder practical deployment, for example, generating an $8192\times 8192$ image can take over an hour on an A100 GPU. In this work, we propose GRAT (\textbf{GR}ouping first, \textbf{AT}tending smartly), a training-free attention acceleration strategy for fast image and video generation without compromising output quality. The key insight is to exploit the inherent sparsity in learned attention maps (which tend to be locally focused) in pretrained Diffusion Transformers and leverage better GPU parallelism. Specifically, GRAT first partitions contiguous tokens into non-overlapping groups, aligning both with GPU execution patterns and the local attention structures learned in pretrained generative Transformers. It then accelerates attention by having all query tokens within the same group share a common set of attendable key and value tokens. These key and value tokens are further restricted to structured regions, such as surrounding blocks or criss-cross regions, significantly reducing computational overhead (e.g., attaining a \textbf{35.8$\times$} speedup over full attention when generating $8192\times 8192$ images) while preserving essential attention patterns and long-range context. We validate GRAT on pretrained Flux and HunyuanVideo for image and video generation, respectively. In both cases, GRAT achieves substantially faster inference without any fine-tuning, while maintaining the performance of full attention. We hope GRAT will inspire future research on accelerating Diffusion Transformers for scalable visual generation.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
