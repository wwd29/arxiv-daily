<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-13</h1>
<h3>Title: A Multichain based marketplace Architecture</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Shoaib Farooq, Hamza Jamil, Hafiz Sohail Riaz</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06636">https://arxiv.org/abs/2402.06636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06636">https://arxiv.org/pdf/2402.06636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06636]] A Multichain based marketplace Architecture(https://arxiv.org/abs/2402.06636)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>]A multichain non-fungible tokens (NFTs) marketplace is a decentralized platform where users can buy, sell, and trade NFTs across multiple blockchain networks by using cross communication bridge. In past most of NFT marketplace was based on singlechain in which NFTs have been bought, sold, and traded on a same blockchain network without the need for any external platform. The singlechain based marketplace have faced number of issues such as performance, scalability, flexibility and limited transaction throughput consequently long confirmation times and high transaction fees during high network usage. Firstly, this paper provides the comprehensive overview about NFT Multichain architecture and explore the challenges and opportunities of designing and implementation phase of multichain NFT marketplace to overcome the issue of single chain-based architecture. NFT multichain marketplace architecture includes different blockchain networks that communicate with each other. Secondly, this paper discusses the concept of mainchain interacting with sidechains which refers to multi blockchain architecture where multiple blockchain networks are connected to each other in a hierarchical structure and identifies key challenges related to interoperability, security, scalability, and user adoption. Finally, we proposed a novel architecture for a multichain NFT marketplace, which leverages the benefits of multiple blockchain networks and marketplaces to overcome these key challenges. Moreover, proposed architecture is evaluated through a case study, demonstrating its ability to support efficient and secure transactions across multiple blockchain networks and highlighting the future trends NFTs and marketplaces and comprehensive discussion about the technology.</li>
</ul>

<h3>Title: Replacing CAPTCHA with XNO micropayments</h3>
<ul>
<li><strong>Authors: </strong>Sujanavan Tiruvayipati</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06649">https://arxiv.org/abs/2402.06649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06649">https://arxiv.org/pdf/2402.06649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06649]] Replacing CAPTCHA with XNO micropayments(https://arxiv.org/abs/2402.06649)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>As technology and gadgets continue to evolve, the need for bot-friendly and user-friendly internet becomes increasingly critical. This work discusses a methodology for implementation and feasibility of replacing traditional CAPTCHA mechanisms with Nano(XNO) cryptocurrency micropayments as a win-win solution and leverages the decentralized and secure nature of cryptocurrencies to introduce a micropayment-based authentication system. This approach not only enhances security by adding a financial barrier for automated bots but also provides a more seamless and efficient user experience. The benefits of this approach include reducing the burden on users while creating a socio-economic model that incentivizes internet service providers and content creators, even when accessed by bots. Furthermore, the integration of XNO micropayments could potentially contribute to the broader adoption and acceptance of digital currencies in everyday online transactions.</li>
</ul>

<h3>Title: The Shifting Landscape of Cybersecurity: The Impact of Remote Work and  COVID-19 on Data Breach Trends</h3>
<ul>
<li><strong>Authors: </strong>Murat Ozer, Yasin Kose, Mehmet Bastug, Goksel Kucukkaya, Eva Ruhsar Varlioglu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06650">https://arxiv.org/abs/2402.06650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06650">https://arxiv.org/pdf/2402.06650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06650]] The Shifting Landscape of Cybersecurity: The Impact of Remote Work and  COVID-19 on Data Breach Trends(https://arxiv.org/abs/2402.06650)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense</a></li>
<li><strong>Abstract: </strong>This study examines the impact of the COVID-19 pandemic on cybersecurity and data breaches, with a specific focus on the shift toward remote work. The study identifies trends and offers insights into cybersecurity incidents by analyzing data breaches two years before and two years after the start of remote work. Data was collected from the Montana Department of Justice Data Breach database and consisted of data breaches that occurred between April 2018 and April 2022. The findings inform best practices for cybersecurity preparedness in remote work environments, aiding organizations to enhance their defenses. Although the study's data is limited to Montana, it offers valuable insights for cybersecurity professionals worldwide. As remote work continues to evolve, organizations must remain adaptable and vigilant in their cybersecurity strategies.</li>
</ul>

<h3>Title: Adversarial Text Purification: A Large Language Model Approach for  Defense</h3>
<ul>
<li><strong>Authors: </strong>Raha Moraffah, Shubh Khandelwal, Amrita Bhattacharjee, Huan Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06655">https://arxiv.org/abs/2402.06655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06655">https://arxiv.org/pdf/2402.06655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06655]] Adversarial Text Purification: A Large Language Model Approach for  Defense(https://arxiv.org/abs/2402.06655)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, generative, large language model</a></li>
<li><strong>Abstract: </strong>Adversarial purification is a defense mechanism for safeguarding classifiers against adversarial attacks without knowing the type of attacks or training of the classifier. These techniques characterize and eliminate adversarial perturbations from the attacked inputs, aiming to restore purified samples that retain similarity to the initially attacked ones and are correctly classified by the classifier. Due to the inherent challenges associated with characterizing noise perturbations for discrete inputs, adversarial text purification has been relatively unexplored. In this paper, we investigate the effectiveness of adversarial purification methods in defending text classifiers. We propose a novel adversarial text purification that harnesses the generative capabilities of Large Language Models (LLMs) to purify adversarial text without the need to explicitly characterize the discrete noise perturbations. We utilize prompt engineering to exploit LLMs for recovering the purified examples for given adversarial examples such that they are semantically similar and correctly classified. Our proposed method demonstrates remarkable performance over various classifiers, improving their accuracy under the attack by over 65% on average.</li>
</ul>

<h3>Title: Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Yuancheng Xu, Jiarui Yao, Manli Shu, Yanchao Sun, Zichu Wu, Ning Yu, Tom Goldstein, Furong Huang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06659">https://arxiv.org/abs/2402.06659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06659">https://arxiv.org/pdf/2402.06659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06659]] Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language  Models(https://arxiv.org/abs/2402.06659)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, yet their versatility raises significant security concerns. This study takes the first step in exposing VLMs' susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack method where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is Persuasion Attack, which leverages VLMs' text generation capabilities to craft narratives, such as portraying junk food as health food, through persuasive and seemingly rational descriptions. We show that Shadowcast are highly effective in achieving attacker's intentions using as few as 50 poison samples. Moreover, these poison samples remain effective across various prompts and are transferable across different VLM architectures in the black-box setting. This work reveals how poisoned VLMs can generate convincing yet deceptive misinformation and underscores the importance of data quality for responsible deployments of VLMs. Our code is available at: https://github.com/umd-huang-lab/VLM-Poisoning.</li>
</ul>

<h3>Title: Authentication and integrity of smartphone videos through multimedia  container structure analysis</h3>
<ul>
<li><strong>Authors: </strong>Carlos Quinto Huamán, Ana Lucila Sandoval Orozco, Luis Javier García Villalba</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06661">https://arxiv.org/abs/2402.06661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06661">https://arxiv.org/pdf/2402.06661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06661]] Authentication and integrity of smartphone videos through multimedia  container structure analysis(https://arxiv.org/abs/2402.06661)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Nowadays, mobile devices have become the natural substitute for the digital camera, as they capture everyday situations easily and quickly, encouraging users to express themselves through images and videos. These videos can be shared across different platforms exposing them to any kind of intentional manipulation by criminals who are aware of the weaknesses of forensic techniques to accuse an innocent person or exonerate a guilty person in a judicial process. Commonly, manufacturers do not comply 100% with the specifications of the standards for the creation of videos. Also, videos shared on social networks, and instant messaging applications go through filtering and compression processes to reduce their size, facilitate their transfer, and optimize storage on their platforms. The omission of specifications and results of transformations carried out by the platforms embed a features pattern in the multimedia container of the videos. These patterns make it possible to distinguish the brand of the device that generated the video, social network, and instant messaging application that was used for the transfer. Research in recent years has focused on the analysis of AVI containers and tiny video datasets. This work presents a novel technique to detect possible attacks against MP4, MOV, and 3GP format videos that affect their integrity and authenticity. The method is based on the analysis of the structure of video containers generated by mobile devices and their behavior when shared through social networks, instant messaging applications, or manipulated by editing programs. The objectives of the proposal are to verify the integrity of videos, identify the source of acquisition and distinguish between original and manipulated videos.</li>
</ul>

<h3>Title: Explainable Adversarial Learning Framework on Physical Layer Secret Keys  Combating Malicious Reconfigurable Intelligent Surface</h3>
<ul>
<li><strong>Authors: </strong>Zhuangkun Wei, Wenxiu Hu, Weisi Guo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06663">https://arxiv.org/abs/2402.06663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06663">https://arxiv.org/pdf/2402.06663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06663]] Explainable Adversarial Learning Framework on Physical Layer Secret Keys  Combating Malicious Reconfigurable Intelligent Surface(https://arxiv.org/abs/2402.06663)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, generative</a></li>
<li><strong>Abstract: </strong>The development of reconfigurable intelligent surfaces (RIS) is a double-edged sword to physical layer security (PLS). Whilst a legitimate RIS can yield beneficial impacts including increased channel randomness to enhance physical layer secret key generation (PL-SKG), malicious RIS can poison legitimate channels and crack most of existing PL-SKGs. In this work, we propose an adversarial learning framework between legitimate parties (namely Alice and Bob) to address this Man-in-the-middle malicious RIS (MITM-RIS) eavesdropping. First, the theoretical mutual information gap between legitimate pairs and MITM-RIS is deduced. Then, Alice and Bob leverage generative adversarial networks (GANs) to learn to achieve a common feature surface that does not have mutual information overlap with MITM-RIS. Next, we aid signal processing interpretation of black-box neural networks by using a symbolic explainable AI (xAI) representation. These symbolic terms of dominant neurons aid feature engineering-based validation and future design of PLS common feature space. Simulation results show that our proposed GAN-based and symbolic-based PL-SKGs can achieve high key agreement rates between legitimate users, and is even resistant to MITM-RIS Eve with the knowledge of legitimate feature generation (NNs or formulas). This therefore paves the way to secure wireless communications with untrusted reflective devices in future 6G.</li>
</ul>

<h3>Title: LLM Agents can Autonomously Hack Websites</h3>
<ul>
<li><strong>Authors: </strong>Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06664">https://arxiv.org/abs/2402.06664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06664">https://arxiv.org/pdf/2402.06664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06664]] LLM Agents can Autonomously Hack Websites(https://arxiv.org/abs/2402.06664)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction, large language model</a></li>
<li><strong>Abstract: </strong>In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents. In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in websites in the wild. Our findings raise questions about the widespread deployment of LLMs.</li>
</ul>

<h3>Title: Compression effects and scene details on the source camera  identification of digital videos</h3>
<ul>
<li><strong>Authors: </strong>Raquel Ramos López, Ana Lucila Sandoval Orozco, Luis Javier García Villalba</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06669">https://arxiv.org/abs/2402.06669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06669">https://arxiv.org/pdf/2402.06669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06669]] Compression effects and scene details on the source camera  identification of digital videos(https://arxiv.org/abs/2402.06669)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The continuous growth of technologies like 4G or 5G has led to a massive use of mobile devices such as smartphones and tablets. This phenomenon, combined with the fact that people use mobile phones for a longer period of time, results in mobile phones becoming the main source of creation of visual information. However, its reliability as a true representation of reality cannot be taken for granted due to the constant increase in editing software. This makes it easier to alter original content without leaving a noticeable trace in the modification. Therefore, it is essential to introduce forensic analysis mechanisms to guarantee the authenticity or integrity of a certain digital video, particularly if it may be considered as evidence in legal proceedings. This paper explains the branch of multimedia forensic analysis that allows to determine the identification of the source of acquisition of a certain video by exploiting the unique traces left by the camera sensor of the mobile device in visual content. To do this, a technique that performs the identification of the source of acquisition of digital videos from mobile devices is presented. It involves 3 stages: (1) Extraction of the sensor fingerprint by applying the block-based technique. (2) Filtering the strong component of the PRNU signal to improve the quality of the sensor fingerprint. (3) Classification of digital videos in an open scenario, that is, where the forensic analyst does not need to have access to the device that recorded the video to find out the origin of the video. The main contribution of the proposed technique eliminates the details of the scene to improve the PRNU fingerprint. It should be noted that these techniques are applied to digital images and not to digital videos.</li>
</ul>

<h3>Title: Understanding Practical Membership Privacy of Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Marlon Tobaben, Gauri Pradhan, Yuan He, Joonas Jälkö, Antti Honkela</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06674">https://arxiv.org/abs/2402.06674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06674">https://arxiv.org/pdf/2402.06674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06674]] Understanding Practical Membership Privacy of Deep Learning(https://arxiv.org/abs/2402.06674)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>We apply a state-of-the-art membership inference attack (MIA) to systematically test the practical privacy vulnerability of fine-tuning large image classification models.We focus on understanding the properties of data sets and samples that make them vulnerable to membership inference. In terms of data set properties, we find a strong power law dependence between the number of examples per class in the data and the MIA vulnerability, as measured by true positive rate of the attack at a low false positive rate. For an individual sample, large gradients at the end of training are strongly correlated with MIA vulnerability.</li>
</ul>

<h3>Title: A Masked language model for multi-source EHR trajectories contextual  representation learning</h3>
<ul>
<li><strong>Authors: </strong>Ali Amirahmadi (1), Mattias Ohlsson (1,2), Kobra Etminani (1), Olle Melander (3), Jonas Björk (4) ((1) Center for Applied Intelligent Systems Research, Halmstad University, (2) Centre for Environmental and Climate Science, Lund University, (3) Department of Clinical Sciences, Lund University, (4) Division of Occupational and Environmental Medicine, Lund University)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06675">https://arxiv.org/abs/2402.06675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06675">https://arxiv.org/pdf/2402.06675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06675]] A Masked language model for multi-source EHR trajectories contextual  representation learning(https://arxiv.org/abs/2402.06675)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Using electronic health records data and machine learning to guide future decisions needs to address challenges, including 1) long/short-term dependencies and 2) interactions between diseases and interventions. Bidirectional transformers have effectively addressed the first challenge. Here we tackled the latter challenge by masking one source (e.g., ICD10 codes) and training the transformer to predict it using other sources (e.g., ATC codes).</li>
</ul>

<h3>Title: Private Knowledge Sharing in Distributed Learning: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Yasas Supeksala, Dinh C. Nguyen, Ming Ding, Thilina Ranbaduge, Calson Chua, Jun Zhang, Jun Li, H. Vincent Poor</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06682">https://arxiv.org/abs/2402.06682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06682">https://arxiv.org/pdf/2402.06682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06682]] Private Knowledge Sharing in Distributed Learning: A Survey(https://arxiv.org/abs/2402.06682)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>The rise of Artificial Intelligence (AI) has revolutionized numerous industries and transformed the way society operates. Its widespread use has led to the distribution of AI and its underlying data across many intelligent systems. In this light, it is crucial to utilize information in learning processes that are either distributed or owned by different entities. As a result, modern data-driven services have been developed to integrate distributed knowledge entities into their outcomes. In line with this goal, the latest AI models are frequently trained in a decentralized manner. Distributed learning involves multiple entities working together to make collective predictions and decisions. However, this collaboration can also bring about security vulnerabilities and challenges. This paper provides an in-depth survey on private knowledge sharing in distributed learning, examining various knowledge components utilized in leading distributed learning architectures. Our analysis sheds light on the most critical vulnerabilities that may arise when using these components in a distributed setting. We further identify and examine defensive strategies for preserving the privacy of these knowledge components and preventing malicious parties from manipulating or accessing the knowledge information. Finally, we highlight several key limitations of knowledge sharing in distributed learning and explore potential avenues for future research.</li>
</ul>

<h3>Title: FL-NAS: Towards Fairness of NAS for Resource Constrained Devices via  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruiyang Qin, Yuting Hu, Zheyu Yan, Jinjun Xiong, Ahmed Abbasi, Yiyu Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06696">https://arxiv.org/abs/2402.06696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06696">https://arxiv.org/pdf/2402.06696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06696]] FL-NAS: Towards Fairness of NAS for Resource Constrained Devices via  Large Language Models(https://arxiv.org/abs/2402.06696)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Neural Architecture Search (NAS) has become the de fecto tools in the industry in automating the design of deep neural networks for various applications, especially those driven by mobile and edge devices with limited computing resources. The emerging large language models (LLMs), due to their prowess, have also been incorporated into NAS recently and show some promising results. This paper conducts further exploration in this direction by considering three important design metrics simultaneously, i.e., model accuracy, fairness, and hardware deployment efficiency. We propose a novel LLM-based NAS framework, FL-NAS, in this paper, and show experimentally that FL-NAS can indeed find high-performing DNNs, beating state-of-the-art DNN models by orders-of-magnitude across almost all design considerations.</li>
</ul>

<h3>Title: High Epsilon Synthetic Data Vulnerabilities in MST and PrivBayes</h3>
<ul>
<li><strong>Authors: </strong>Steven Golob, Sikha Pentyala, Anuar Maratkhan, Martine De Cock</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06699">https://arxiv.org/abs/2402.06699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06699">https://arxiv.org/pdf/2402.06699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06699]] High Epsilon Synthetic Data Vulnerabilities in MST and PrivBayes(https://arxiv.org/abs/2402.06699)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, membership infer</a></li>
<li><strong>Abstract: </strong>Synthetic data generation (SDG) has become increasingly popular as a privacy-enhancing technology. It aims to maintain important statistical properties of its underlying training data, while excluding any personally identifiable information. There have been a whole host of SDG algorithms developed in recent years to improve and balance both of these aims. Many of these algorithms provide robust differential privacy guarantees. However, we show here that if the differential privacy parameter $\varepsilon$ is set too high, then unambiguous privacy leakage can result. We show this by conducting a novel membership inference attack (MIA) on two state-of-the-art differentially private SDG algorithms: MST and PrivBayes. Our work suggests that there are vulnerabilities in these generators not previously seen, and that future work to strengthen their privacy is advisable. We present the heuristic for our MIA here. It assumes knowledge of auxiliary "population" data, and also assumes knowledge of which SDG algorithm was used. We use this information to adapt the recent DOMIAS MIA uniquely to MST and PrivBayes. Our approach went on to win the SNAKE challenge in November 2023.</li>
</ul>

<h3>Title: Entropy-Regularized Token-Level Policy Optimization for Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Muning Wen, Cheng Deng, Jun Wang, Weinan Zhang, Ying Wen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06700">https://arxiv.org/abs/2402.06700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06700">https://arxiv.org/pdf/2402.06700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06700]] Entropy-Regularized Token-Level Policy Optimization for Large Language  Models(https://arxiv.org/abs/2402.06700)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown promise as intelligent agents in interactive decision-making tasks. Traditional approaches often depend on meticulously designed prompts, high-quality examples, or additional reward models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement learning (RL) presents a dynamic alternative for LLMs to overcome these dependencies by engaging directly with task-specific environments. Nonetheless, it faces significant hurdles: 1) instability stemming from the exponentially vast action space requiring exploration; 2) challenges in assigning token-level credit based on action-level reward signals, resulting in discord between maximizing rewards and accurately modeling corpus data. In response to these challenges, we introduce Entropy-Regularized Token-level Policy Optimization (ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the token level. At the heart of ETPO is our novel per-token soft Bellman update, designed to harmonize the RL process with the principles of language modeling. This methodology decomposes the Q-function update from a coarse action-level view to a more granular token-level perspective, backed by theoretical proof of optimization consistency. Crucially, this decomposition renders linear time complexity in action exploration. We assess the effectiveness of ETPO within a simulated environment that models data science code generation as a series of multi-step interactive tasks; results show that ETPO achieves effective performance improvement on the CodeLlama-7B model and surpasses a variant PPO baseline inherited from RLHF. This underlines ETPO's potential as a robust method for refining the interactive decision-making capabilities of LLMs.</li>
</ul>

<h3>Title: Privacy Profiles for Private Selection</h3>
<ul>
<li><strong>Authors: </strong>Antti Koskela, Rachel Redberg, Yu-Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06701">https://arxiv.org/abs/2402.06701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06701">https://arxiv.org/pdf/2402.06701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06701]] Privacy Profiles for Private Selection(https://arxiv.org/abs/2402.06701)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Private selection mechanisms (e.g., Report Noisy Max, Sparse Vector) are fundamental primitives of differentially private (DP) data analysis with wide applications to private query release, voting, and hyperparameter tuning. Recent work (Liu and Talwar, 2019; Papernot and Steinke, 2022) has made significant progress in both generalizing private selection mechanisms and tightening their privacy analysis using modern numerical privacy accounting tools, e.g., R\'enyi DP. But R\'enyi DP is known to be lossy when $(\epsilon,\delta)$-DP is ultimately needed, and there is a trend to close the gap by directly handling privacy profiles, i.e., $\delta$ as a function of $\epsilon$ or its equivalent dual form known as $f$-DPs. In this paper, we work out an easy-to-use recipe that bounds the privacy profiles of ReportNoisyMax and PrivateTuning using the privacy profiles of the base algorithms they corral. Numerically, our approach improves over the RDP-based accounting in all regimes of interest and leads to substantial benefits in end-to-end private learning experiments. Our analysis also suggests new distributions, e.g., binomial distribution for randomizing the number of rounds that leads to more substantial improvements in certain regimes.</li>
</ul>

<h3>Title: Dynamic Graph Information Bottleneck</h3>
<ul>
<li><strong>Authors: </strong>Haonan Yuan, Qingyun Sun, Xingcheng Fu, Cheng Ji, Jianxin Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06716">https://arxiv.org/abs/2402.06716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06716">https://arxiv.org/pdf/2402.06716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06716]] Dynamic Graph Information Bottleneck(https://arxiv.org/abs/2402.06716)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the DGIB$_{MS}$ channel aims to learn the minimal and sufficient representations, with the DGIB$_{MS}$ channel guarantees the predictive consensus. Extensive experiments on real-world and synthetic dynamic graph datasets demonstrate the superior robustness of DGIB against adversarial attacks compared with state-of-the-art baselines in the link prediction task. To the best of our knowledge, DGIB is the first work to learn robust representations of dynamic graphs grounded in the information-theoretic IB principle.</li>
</ul>

<h3>Title: NICE: To Optimize In-Context Examples or Not?</h3>
<ul>
<li><strong>Authors: </strong>Pragya Srivastava, Satvik Golechha, Amit Deshpande, Amit Sharma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06733">https://arxiv.org/abs/2402.06733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06733">https://arxiv.org/pdf/2402.06733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06733]] NICE: To Optimize In-Context Examples or Not?(https://arxiv.org/abs/2402.06733)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added details, we validate our hypothesis empirically by computing \metric with query-dependent bins of examples, comparing different instructions with ICE selection methods, and performing label perturbation experiments. We conclude that tasks can be divided into two broad classes based on the \metric metric, where the returns on ICE optimization follow predictable trends when instructions are provided in the prompt.</li>
</ul>

<h3>Title: Corruption Robust Offline Reinforcement Learning with Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Debmalya Mandal, Andi Nika, Parameswaran Kamalaruban, Adish Singla, Goran Radanović</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06734">https://arxiv.org/abs/2402.06734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06734">https://arxiv.org/pdf/2402.06734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06734]] Corruption Robust Offline Reinforcement Learning with Human Feedback(https://arxiv.org/abs/2402.06734)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>We study data corruption robustness for reinforcement learning with human feedback (RLHF) in an offline setting. Given an offline dataset of pairs of trajectories along with feedback about human preferences, an $\varepsilon$-fraction of the pairs is corrupted (e.g., feedback flipped or trajectory features manipulated), capturing an adversarial attack or noisy human preferences. We aim to design algorithms that identify a near-optimal policy from the corrupted data, with provable guarantees. Existing theoretical works have separately studied the settings of corruption robust RL (learning from scalar rewards directly under corruption) and offline RLHF (learning from human feedback without corruption); however, they are inapplicable to our problem of dealing with corrupted data in offline RLHF setting. To this end, we design novel corruption robust offline RLHF methods under various assumptions on the coverage of the data-generating distributions. At a high level, our methodology robustifies an offline RLHF framework by first learning a reward model along with confidence sets and then learning a pessimistic optimal policy over the confidence set. Our key insight is that learning optimal policy can be done by leveraging an offline corruption-robust RL oracle in different ways (e.g., zero-order oracle or first-order oracle), depending on the data coverage assumptions. To our knowledge, ours is the first work that provides provable corruption robust offline RLHF methods.</li>
</ul>

<h3>Title: EntGPT: Linking Generative Large Language Models with Knowledge Bases</h3>
<ul>
<li><strong>Authors: </strong>Yifan Ding, Amrit Poudel, Qingkai Zeng, Tim Weninger, Balaji Veeramani, Sanmitra Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06738">https://arxiv.org/abs/2402.06738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06738">https://arxiv.org/pdf/2402.06738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06738]] EntGPT: Linking Generative Large Language Models with Knowledge Bases(https://arxiv.org/abs/2402.06738)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The ability of Large Language Models (LLMs) to generate factually correct output remains relatively unexplored due to the lack of fact-checking and knowledge grounding during training and inference. In this work, we aim to address this challenge through the Entity Disambiguation (ED) task. We first consider prompt engineering, and design a three-step hard-prompting method to probe LLMs' ED performance without supervised fine-tuning (SFT). Overall, the prompting method improves the micro-F_1 score of the original vanilla models by a large margin, on some cases up to 36% and higher, and obtains comparable performance across 10 datasets when compared to existing methods with SFT. We further improve the knowledge grounding ability through instruction tuning (IT) with similar prompts and responses. The instruction-tuned model not only achieves higher micro-F1 score performance as compared to several baseline methods on supervised entity disambiguation tasks with an average micro-F_1 improvement of 2.1% over the existing baseline models, but also obtains higher accuracy on six Question Answering (QA) tasks in the zero-shot setting. Our methodologies apply to both open- and closed-source LLMs.</li>
</ul>

<h3>Title: Scalable Kernel Logistic Regression with Nyström Approximation:  Theoretical Analysis and Application to Discrete Choice Modelling</h3>
<ul>
<li><strong>Authors: </strong>José Ángel Martín-Baos, Ricardo García-Ródenas, Luis Rodriguez-Benitez, Michel Bierlaire</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06763">https://arxiv.org/abs/2402.06763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06763">https://arxiv.org/pdf/2402.06763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06763]] Scalable Kernel Logistic Regression with Nyström Approximation:  Theoretical Analysis and Application to Discrete Choice Modelling(https://arxiv.org/abs/2402.06763)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The application of kernel-based Machine Learning (ML) techniques to discrete choice modelling using large datasets often faces challenges due to memory requirements and the considerable number of parameters involved in these models. This complexity hampers the efficient training of large-scale models. This paper addresses these problems of scalability by introducing the Nystr\"om approximation for Kernel Logistic Regression (KLR) on large datasets. The study begins by presenting a theoretical analysis in which: i) the set of KLR solutions is characterised, ii) an upper bound to the solution of KLR with Nystr\"om approximation is provided, and finally iii) a specialisation of the optimisation algorithms to Nystr\"om KLR is described. After this, the Nystr\"om KLR is computationally validated. Four landmark selection methods are tested, including basic uniform sampling, a k-means sampling strategy, and two non-uniform methods grounded in leverage scores. The performance of these strategies is evaluated using large-scale transport mode choice datasets and is compared with traditional methods such as Multinomial Logit (MNL) and contemporary ML techniques. The study also assesses the efficiency of various optimisation techniques for the proposed Nystr\"om KLR model. The performance of gradient descent, Momentum, Adam, and L-BFGS-B optimisation methods is examined on these datasets. Among these strategies, the k-means Nystr\"om KLR approach emerges as a successful solution for applying KLR to large datasets, particularly when combined with the L-BFGS-B and Adam optimisation methods. The results highlight the ability of this strategy to handle datasets exceeding 200,000 observations while maintaining robust performance.</li>
</ul>

<h3>Title: Transfer learning with generative models for object detection on limited  datasets</h3>
<ul>
<li><strong>Authors: </strong>Matteo Paiano, Stefano Martina, Carlotta Giannelli, Filippo Caruso</a></li>
<li><strong>Subjects: </strong>cs.CV, cond-mat.dis-nn, cs.AI, cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06784">https://arxiv.org/abs/2402.06784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06784">https://arxiv.org/pdf/2402.06784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06784]] Transfer learning with generative models for object detection on limited  datasets(https://arxiv.org/abs/2402.06784)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The availability of data is limited in some fields, especially for object detection tasks, where it is necessary to have correctly labeled bounding boxes around each object. A notable example of such data scarcity is found in the domain of marine biology, where it is useful to develop methods to automatically detect submarine species for environmental monitoring. To address this data limitation, the state-of-the-art machine learning strategies employ two main approaches. The first involves pretraining models on existing datasets before generalizing to the specific domain of interest. The second strategy is to create synthetic datasets specifically tailored to the target domain using methods like copy-paste techniques or ad-hoc simulators. The first strategy often faces a significant domain shift, while the second demands custom solutions crafted for the specific task. In response to these challenges, here we propose a transfer learning framework that is valid for a generic scenario. In this framework, generated images help to improve the performances of an object detector in a few-real data regime. This is achieved through a diffusion-based generative model that was pretrained on large generic datasets, and is not trained on the task-specific domain. We validate our approach on object detection tasks, specifically focusing on fishes in an underwater environment, and on the more common domain of cars in an urban setting. Our method achieves detection performance comparable to models trained on thousands of images, using only a few hundreds of input data. Our results pave the way for new generative AI-based protocols for machine learning applications in various domains, for instance ranging from geophysics to biology and medicine.</li>
</ul>

<h3>Title: Generative Nowcasting of Marine Fog Visibility in the Grand Banks area  and Sable Island in Canada</h3>
<ul>
<li><strong>Authors: </strong>Eren Gultepe, Sen Wang, Byron Blomquist, Harindra J.S. Fernando, O. Patrick Kreidl, David J. Delene, Ismail Gultepe</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06800">https://arxiv.org/abs/2402.06800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06800">https://arxiv.org/pdf/2402.06800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06800]] Generative Nowcasting of Marine Fog Visibility in the Grand Banks area  and Sable Island in Canada(https://arxiv.org/abs/2402.06800)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study presents the application of generative deep learning techniques to evaluate marine fog visibility nowcasting using the FATIMA (Fog and turbulence interactions in the marine atmosphere) campaign observations collected during July 2022 in the North Atlantic in the Grand Banks area and vicinity of Sable Island (SI), northeast of Canada. The measurements were collected using the Vaisala Forward Scatter Sensor model FD70 and Weather Transmitter model WXT50, and Gill R3A ultrasonic anemometer mounted on the Research Vessel Atlantic Condor. To perform nowcasting, the time series of fog visibility (Vis), wind speed, dew point depression, and relative humidity with respect to water were preprocessed to have lagged time step features. Generative nowcasting of Vis time series for lead times of 30 and 60 minutes were performed using conditional generative adversarial networks (cGAN) regression at visibility thresholds of Vis < 1 km and < 10 km. Extreme gradient boosting (XGBoost) was used as a baseline method for comparison against cGAN. At the 30 min lead time, Vis was best predicted with cGAN at Vis < 1 km (RMSE = 0.151 km) and with XGBoost at Vis < 10 km (RMSE = 2.821 km). At the 60 min lead time, Vis was best predicted with XGBoost at Vis < 1 km (RMSE = 0.167 km) and Vis < 10 km (RMSE = 3.508 km), but the cGAN RMSE was similar to XGBoost. Despite nowcasting Vis at 30 min being quite difficult, the ability of the cGAN model to track the variation in Vis at 1 km suggests that there is potential for generative analysis of marine fog visibility using observational meteorological parameters.</li>
</ul>

<h3>Title: Fingerprinting New York City's Scaffolding Problem with Longitudinal  Dashcam Data</h3>
<ul>
<li><strong>Authors: </strong>Dorin Shapira, Matt Franchi, Wendy Ju</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06801">https://arxiv.org/abs/2402.06801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06801">https://arxiv.org/pdf/2402.06801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06801]] Fingerprinting New York City's Scaffolding Problem with Longitudinal  Dashcam Data(https://arxiv.org/abs/2402.06801)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Scaffolds, also called sidewalk sheds, are intended to be temporary structures to protect pedestrians from construction and repair hazards. However, some sidewalk sheds are left up for years. Long-term scaffolding becomes eyesores, creates accessibility issues on sidewalks, and gives cover to illicit activity. Today, there are over 8,000 active permits for scaffolds in NYC; the more problematic scaffolds are likely expired or unpermitted. This research uses computer vision on street-level imagery to develop a longitudinal map of scaffolding throughout the city. Using a dataset of 29,156,833 dashcam images taken between August 2023 and January 2024, we develop an algorithm to track the presence of scaffolding over time. We also design and implement methods to match detected scaffolds to reported locations of active scaffolding permits, enabling the identification of sidewalk sheds without corresponding permits. We identify 850,766 images of scaffolding, tagging 5,156 active sidewalk sheds and estimating 529 unpermitted sheds. We discuss the implications of an in-the-wild scaffolding classifier for urban tech, innovations to governmental inspection processes, and out-of-distribution evaluations outside of New York City.</li>
</ul>

<h3>Title: Towards Principled Assessment of Tabular Data Synthesis Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Yuntao Du, Ninghui Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DB, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06806">https://arxiv.org/abs/2402.06806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06806">https://arxiv.org/pdf/2402.06806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06806]] Towards Principled Assessment of Tabular Data Synthesis Algorithms(https://arxiv.org/abs/2402.06806)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Data synthesis has been advocated as an important approach for utilizing data while protecting data privacy. A large number of tabular data synthesis algorithms (which we call synthesizers) have been proposed. Some synthesizers satisfy Differential Privacy, while others aim to provide privacy in a heuristic fashion. A comprehensive understanding of the strengths and weaknesses of these synthesizers remains elusive due to lacking principled evaluation metrics and missing head-to-head comparisons of newly developed synthesizers that take advantage of diffusion models and large language models with state-of-the-art marginal-based synthesizers. In this paper, we present a principled and systematic evaluation framework for assessing tabular data synthesis algorithms. Specifically, we examine and critique existing evaluation metrics, and introduce a set of new metrics in terms of fidelity, privacy, and utility to address their limitations. Based on the proposed metrics, we also devise a unified objective for tuning, which can consistently improve the quality of synthetic data for all methods. We conducted extensive evaluations of 8 different types of synthesizers on 12 datasets and identified some interesting findings, which offer new directions for privacy-preserving data synthesis.</li>
</ul>

<h3>Title: Explain Variance of Prediction in Variational Time Series Models for  Clinical Deterioration Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Liu, Jaideep Srivastava</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06808">https://arxiv.org/abs/2402.06808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06808">https://arxiv.org/pdf/2402.06808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06808]] Explain Variance of Prediction in Variational Time Series Models for  Clinical Deterioration Prediction(https://arxiv.org/abs/2402.06808)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer</a></li>
<li><strong>Abstract: </strong>In healthcare, thanks to many model agnostic methods, explainability of the prediction scores made by deep learning applications has improved. However, we note that for daily or hourly risk of deterioration prediction of in-hospital patients, not only the predicted risk probability score matters, but also the variance of the risk scores play key roles in aiding clinical decision making. In this paper, we propose to use delta's method to approximate variance of prediction deterministically, such that the SHAP method can be adopted to attribute contribution of variance. The prediction variance is estimated by sampling the conditional hidden space in variational models and is propagated to input clinical variables based on Shapley values of the variance game. This approach works with variational time series models such as variational recurrent neural networks and variational transformers. We further argue that variational time series models are perfect fits for achieving a balance between predictive power and explainability through a series of experiments on a public clinical ICU datasets. Since SHAP values are additive, we also postulate that the SHAP importance of clinical variables with respect to prediction variations can guide their frequency of measurements.</li>
</ul>

<h3>Title: A Kalman Filter Based Framework for Monitoring the Performance of  In-Hospital Mortality Prediction Models Over Time</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Liu, Lisa Kirkland, Jaideep Srivastava</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06812">https://arxiv.org/abs/2402.06812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06812">https://arxiv.org/pdf/2402.06812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06812]] A Kalman Filter Based Framework for Monitoring the Performance of  In-Hospital Mortality Prediction Models Over Time(https://arxiv.org/abs/2402.06812)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Unlike in a clinical trial, where researchers get to determine the least number of positive and negative samples required, or in a machine learning study where the size and the class distribution of the validation set is static and known, in a real-world scenario, there is little control over the size and distribution of incoming patients. As a result, when measured during different time periods, evaluation metrics like Area under the Receiver Operating Curve (AUCROC) and Area Under the Precision-Recall Curve(AUCPR) may not be directly comparable. Therefore, in this study, for binary classifiers running in a long time period, we proposed to adjust these performance metrics for sample size and class distribution, so that a fair comparison can be made between two time periods. Note that the number of samples and the class distribution, namely the ratio of positive samples, are two robustness factors which affect the variance of AUCROC. To better estimate the mean of performance metrics and understand the change of performance over time, we propose a Kalman filter based framework with extrapolated variance adjusted for the total number of samples and the number of positive samples during different time periods. The efficacy of this method is demonstrated first on a synthetic dataset and then retrospectively applied to a 2-days ahead in-hospital mortality prediction model for COVID-19 patients during 2021 and 2022. Further, we conclude that our prediction model is not significantly affected by the evolution of the disease, improved treatments and changes in hospital operational plans.</li>
</ul>

<h3>Title: Estimating Player Performance in Different Contexts Using Fine-tuned  Large Events Models</h3>
<ul>
<li><strong>Authors: </strong>Tiago Mendes-Neves, Luís Meireles, João Mendes-Moreira</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06815">https://arxiv.org/abs/2402.06815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06815">https://arxiv.org/pdf/2402.06815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06815]] Estimating Player Performance in Different Contexts Using Fine-tuned  Large Events Models(https://arxiv.org/abs/2402.06815)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces an innovative application of Large Event Models (LEMs), akin to Large Language Models, to the domain of soccer analytics. By learning the "language" of soccer - predicting variables for subsequent events rather than words LEMs facilitate the simulation of matches and offer various applications, including player performance prediction across different team contexts. We focus on fine-tuning LEMs with the WyScout dataset for the 2017-2018 Premier League season to derive specific insights into player contributions and team strategies. Our methodology involves adapting these models to reflect the nuanced dynamics of soccer, enabling the evaluation of hypothetical transfers. Our findings confirm the effectiveness and limitations of LEMs in soccer analytics, highlighting the model's capability to forecast teams' expected standings and explore high-profile scenarios, such as the potential effects of transferring Cristiano Ronaldo or Lionel Messi to different teams in the Premier League. This analysis underscores the importance of context in evaluating player quality. While general metrics may suggest significant differences between players, contextual analyses reveal narrower gaps in performance within specific team frameworks.</li>
</ul>

<h3>Title: Forecasting Events in Soccer Matches Through Language</h3>
<ul>
<li><strong>Authors: </strong>Tiago Mendes-Neves, Luís Meireles, João Mendes-Moreira</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06820">https://arxiv.org/abs/2402.06820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06820">https://arxiv.org/pdf/2402.06820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06820]] Forecasting Events in Soccer Matches Through Language(https://arxiv.org/abs/2402.06820)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces an approach to predicting the next event in a soccer match, a challenge bearing remarkable similarities to the problem faced by Large Language Models (LLMs). Unlike other methods that severely limit event dynamics in soccer, often abstracting from many variables or relying on a mix of sequential models, our research proposes a novel technique inspired by the methodologies used in LLMs. These models predict a complete chain of variables that compose an event, significantly simplifying the construction of Large Event Models (LEMs) for soccer. Utilizing deep learning on the publicly available WyScout dataset, the proposed approach notably surpasses the performance of previous LEM proposals in critical areas, such as the prediction accuracy of the next event type. This paper highlights the utility of LEMs in various applications, including betting and match analytics. Moreover, we show that LEMs provide a simulation backbone on which many analytics pipelines can be built, an approach opposite to the current specialized single-purpose models. LEMs represent a pivotal advancement in soccer analytics, establishing a foundational framework for multifaceted analytics pipelines through a singular machine-learning model.</li>
</ul>

<h3>Title: RAMP: Boosting Adversarial Robustness Against Multiple $l_p$  Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Enyi Jiang, Gagandeep Singh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06827">https://arxiv.org/abs/2402.06827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06827">https://arxiv.org/pdf/2402.06827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06827]] RAMP: Boosting Adversarial Robustness Against Multiple $l_p$  Perturbations(https://arxiv.org/abs/2402.06827)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>There is considerable work on improving robustness against adversarial attacks bounded by a single $l_p$ norm using adversarial training (AT). However, the multiple-norm robustness (union accuracy) of AT models is still low. We observe that simultaneously obtaining good union and clean accuracy is hard since there are tradeoffs between robustness against multiple $l_p$ perturbations, and accuracy/robustness/efficiency. By analyzing the tradeoffs from the lens of distribution shifts, we identify the key tradeoff pair among $l_p$ attacks to boost efficiency and design a logit pairing loss to improve the union accuracy. Next, we connect natural training with AT via gradient projection, to find and incorporate useful information from natural training into AT, which moderates the accuracy/robustness tradeoff. Combining our contributions, we propose a framework called \textbf{RAMP}, to boost the robustness against multiple $l_p$ perturbations. We show \textbf{RAMP} can be easily adapted for both robust fine-tuning and full AT. For robust fine-tuning, \textbf{RAMP} obtains a union accuracy up to $53.5\%$ on CIFAR-10, and $29.7\%$ on ImageNet. For training from scratch, \textbf{RAMP} achieves SOTA union accuracy of $44.6\%$ and relatively good clean accuracy of $81.2\%$ on ResNet-18 against AutoAttack on CIFAR-10.</li>
</ul>

<h3>Title: System-level Analysis of Adversarial Attacks and Defenses on  Intelligence in O-RAN based Cellular Networks</h3>
<ul>
<li><strong>Authors: </strong>Azuka Chiejina, Brian Kim, Kaushik Chowhdury, Vijay K. Shah</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06846">https://arxiv.org/abs/2402.06846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06846">https://arxiv.org/pdf/2402.06846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06846]] System-level Analysis of Adversarial Attacks and Defenses on  Intelligence in O-RAN based Cellular Networks(https://arxiv.org/abs/2402.06846)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>While the open architecture, open interfaces, and integration of intelligence within Open Radio Access Network technology hold the promise of transforming 5G and 6G networks, they also introduce cybersecurity vulnerabilities that hinder its widespread adoption. In this paper, we conduct a thorough system-level investigation of cyber threats, with a specific focus on machine learning (ML) intelligence components known as xApps within the O-RAN's near-real-time RAN Intelligent Controller (near-RT RIC) platform. Our study begins by developing a malicious xApp designed to execute adversarial attacks on two types of test data - spectrograms and key performance metrics (KPMs), stored in the RIC database within the near-RT RIC. To mitigate these threats, we utilize a distillation technique that involves training a teacher model at a high softmax temperature and transferring its knowledge to a student model trained at a lower softmax temperature, which is deployed as the robust ML model within xApp. We prototype an over-the-air LTE/5G O-RAN testbed to assess the impact of these attacks and the effectiveness of the distillation defense technique by leveraging an ML-based Interference Classification (InterClass) xApp as an example. We examine two versions of InterClass xApp under distinct scenarios, one based on Convolutional Neural Networks (CNNs) and another based on Deep Neural Networks (DNNs) using spectrograms and KPMs as input data respectively. Our findings reveal up to 100% and 96.3% degradation in the accuracy of both the CNN and DNN models respectively resulting in a significant decline in network performance under considered adversarial attacks. Under the strict latency constraints of the near-RT RIC closed control loop, our analysis shows that the distillation technique outperforms classical adversarial training by achieving an accuracy of up to 98.3% for mitigating such attacks.</li>
</ul>

<h3>Title: History, Development, and Principles of Large Language Models-An  Introductory Survey</h3>
<ul>
<li><strong>Authors: </strong>Zhibo Chu, Shiwen Ni, Zichong Wang, Xi Feng, Chengming Li, Xiping Hu, Ruifeng Xu, Min Yang, Wenbin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06853">https://arxiv.org/abs/2402.06853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06853">https://arxiv.org/pdf/2402.06853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06853]] History, Development, and Principles of Large Language Models-An  Introductory Survey(https://arxiv.org/abs/2402.06853)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Language models serve as a cornerstone in natural language processing (NLP), utilizing mathematical methods to generalize language laws and knowledge for prediction and generation. Over extensive research spanning decades, language modeling has progressed from initial statistical language models (SLMs) to the contemporary landscape of large language models (LLMs). Notably, the swift evolution of LLMs has reached the ability to process, understand, and generate human-level text. Nevertheless, despite the significant advantages that LLMs offer in improving both work and personal lives, the limited understanding among general practitioners about the background and principles of these models hampers their full potential. Notably, most LLMs reviews focus on specific aspects and utilize specialized language, posing a challenge for practitioners lacking relevant background knowledge. In light of this, this survey aims to present a comprehensible overview of LLMs to assist a broader audience. It strives to facilitate a comprehensive understanding by exploring the historical background of language models and tracing their evolution over time. The survey further investigates the factors influencing the development of LLMs, emphasizing key contributions. Additionally, it concentrates on elucidating the underlying principles of LLMs, equipping audiences with essential theoretical knowledge. The survey also highlights the limitations of existing work and points out promising future directions.</li>
</ul>

<h3>Title: For Better or For Worse? Learning Minimum Variance Features With Label  Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Muthu Chidambaram, Rong Ge</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06855">https://arxiv.org/abs/2402.06855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06855">https://arxiv.org/pdf/2402.06855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06855]] For Better or For Worse? Learning Minimum Variance Features With Label  Augmentation(https://arxiv.org/abs/2402.06855)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Data augmentation has been pivotal in successfully training deep learning models on classification tasks over the past decade. An important subclass of data augmentation techniques - which includes both label smoothing and Mixup - involves modifying not only the input data but also the input label during model training. In this work, we analyze the role played by the label augmentation aspect of such methods. We prove that linear models on linearly separable data trained with label augmentation learn only the minimum variance features in the data, while standard training (which includes weight decay) can learn higher variance features. An important consequence of our results is negative: label smoothing and Mixup can be less robust to adversarial perturbations of the training data when compared to standard training. We verify that our theory reflects practice via a range of experiments on synthetic data and image classification benchmarks.</li>
</ul>

<h3>Title: LiRank: Industrial Large Scale Ranking Models at LinkedIn</h3>
<ul>
<li><strong>Authors: </strong>Fedor Borisyuk, Mingzhou Zhou, Qingquan Song, Siyu Zhu, Birjodh Tiwana, Ganesh Parameswaran, Siddharth Dangi, Lars Hertel, Qiang Xiao, Xiaochen Hou, Yunbo Ouyang, Aman Gupta, Sheallika Singh, Dan Liu, Hailing Cheng, Lei Le, Jonathan Hung, Sathiya Keerthi, Ruoyan Wang, Fengyu Zhang, Mohit Kothari, Chen Zhu, Daqi Sun, Yun Dai, Xun Luan, Sirou Zhu, Zhiwei Wang, Neil Daftary, Qianqi Shen, Chengming Jiang, Haichao Wei, Maneesh Varshney, Amol Ghoting, Souvik Ghosh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06859">https://arxiv.org/abs/2402.06859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06859">https://arxiv.org/pdf/2402.06859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06859]] LiRank: Industrial Large Scale Ranking Models at LinkedIn(https://arxiv.org/abs/2402.06859)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present LiRank, a large-scale ranking framework at LinkedIn that brings to production state-of-the-art modeling architectures and optimization methods. We unveil several modeling improvements, including Residual DCN, which adds attention and residual connections to the famous DCNv2 architecture. We share insights into combining and tuning SOTA architectures to create a unified model, including Dense Gating, Transformers and Residual DCN. We also propose novel techniques for calibration and describe how we productionalized deep learning based explore/exploit methods. To enable effective, production-grade serving of large ranking models, we detail how to train and compress models using quantization and vocabulary compression. We provide details about the deployment setup for large-scale use cases of Feed ranking, Jobs Recommendations, and Ads click-through rate (CTR) prediction. We summarize our learnings from various A/B tests by elucidating the most effective technical approaches. These ideas have contributed to relative metrics improvements across the board at LinkedIn: +0.5% member sessions in the Feed, +1.76% qualified job applications for Jobs search and recommendations, and +4.3% for Ads CTR. We hope this work can provide practical insights and solutions for practitioners interested in leveraging large-scale deep ranking systems.</li>
</ul>

<h3>Title: Discriminative Adversarial Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Rohan Sharma, Shijie Zhou, Kaiyi Ji, Changyou Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06864">https://arxiv.org/abs/2402.06864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06864">https://arxiv.org/pdf/2402.06864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06864]] Discriminative Adversarial Unlearning(https://arxiv.org/abs/2402.06864)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, membership infer</a></li>
<li><strong>Abstract: </strong>We introduce a novel machine unlearning framework founded upon the established principles of the min-max optimization paradigm. We capitalize on the capabilities of strong Membership Inference Attacks (MIA) to facilitate the unlearning of specific samples from a trained model. We consider the scenario of two networks, the attacker $\mathbf{A}$ and the trained defender $\mathbf{D}$ pitted against each other in an adversarial objective, wherein the attacker aims at teasing out the information of the data to be unlearned in order to infer membership, and the defender unlearns to defend the network against the attack, whilst preserving its general performance. The algorithm can be trained end-to-end using backpropagation, following the well known iterative min-max approach in updating the attacker and the defender. We additionally incorporate a self-supervised objective effectively addressing the feature space discrepancies between the forget set and the validation set, enhancing unlearning performance. Our proposed algorithm closely approximates the ideal benchmark of retraining from scratch for both random sample forgetting and class-wise forgetting schemes on standard machine-unlearning datasets. Specifically, on the class unlearning scheme, the method demonstrates near-optimal performance and comprehensively overcomes known methods over the random sample forgetting scheme across all metrics and multiple network pruning strategies.</li>
</ul>

<h3>Title: Digital Footprints of Streaming Devices</h3>
<ul>
<li><strong>Authors: </strong>Sundar Krishnan, William Bradley Glisson</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06869">https://arxiv.org/abs/2402.06869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06869">https://arxiv.org/pdf/2402.06869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06869]] Digital Footprints of Streaming Devices(https://arxiv.org/abs/2402.06869)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>These days, there are many ways to watch streaming videos on television. When compared to a standalone smart television, streaming devices such as Roku and Amazon Fire Stick have a plethora of app selections. While these devices are platform agnostic and compatible with smartphones, they can still leave behind crumbs of sensitive data that can cause privacy, security, and forensic issues. In this paper, the authors conduct an experiment with streaming devices to ascertain digital footprints from network traffic and mobile forensics that they leave behind.</li>
</ul>

<h3>Title: GenTranslate: Large Language Models are Generative Multilingual Speech  and Machine Translators</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Dong Zhang, Zhehuai Chen, Eng Siong Chng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06894">https://arxiv.org/abs/2402.06894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06894">https://arxiv.org/pdf/2402.06894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06894]] GenTranslate: Large Language Models are Generative Multilingual Speech  and Machine Translators(https://arxiv.org/abs/2402.06894)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely "GenTranslate", which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTranslate dataset that contains over 592K hypotheses-translation pairs in 11 languages. Experiments on various speech and machine translation benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that our GenTranslate significantly outperforms the state-of-the-art model.</li>
</ul>

<h3>Title: Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework  and Semantic-Based Metric</h3>
<ul>
<li><strong>Authors: </strong>Hyukhun Koh, Dohyung Kim, Minwoo Lee, Kyomin Jung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06900">https://arxiv.org/abs/2402.06900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06900">https://arxiv.org/pdf/2402.06900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06900]] Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework  and Semantic-Based Metric(https://arxiv.org/abs/2402.06900)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an influence on downstream metrics.</li>
</ul>

<h3>Title: Benchmarking Frameworks and Comparative Studies of Controller Area  Network (CAN) Intrusion Detection Systems: A Review</h3>
<ul>
<li><strong>Authors: </strong>Shaila Sharmin, Hafizah Mansor, Andi Fitriah Abdul Kadir, Normaziah A. Aziz</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06904">https://arxiv.org/abs/2402.06904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06904">https://arxiv.org/pdf/2402.06904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06904]] Benchmarking Frameworks and Comparative Studies of Controller Area  Network (CAN) Intrusion Detection Systems: A Review(https://arxiv.org/abs/2402.06904)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>The development of intrusion detection systems (IDS) for the in-vehicle Controller Area Network (CAN) bus is one of the main efforts being taken to secure the in-vehicle network against various cyberattacks, which have the potential to cause vehicles to malfunction and result in dangerous accidents. These CAN IDS are evaluated in disparate experimental conditions that vary in terms of the workload used, the features used, the metrics reported, etc., which makes direct comparison difficult. Therefore, there have been several benchmarking frameworks and comparative studies designed to evaluate CAN IDS in similar experimental conditions to understand their relative performance and facilitate the selection of the best CAN IDS for implementation in automotive networks. This work provides a comprehensive survey of CAN IDS benchmarking frameworks and comparative studies in the current literature. A CAN IDS evaluation design space is also proposed in this work, which draws from the wider CAN IDS literature. This is not only expected to serve as a guide for designing CAN IDS evaluation experiments but is also used for categorizing current benchmarking efforts. The surveyed works have been discussed on the basis of the five aspects in the design space-namely IDS type, attack model, evaluation type, workload generation, and evaluation metrics-and recommendations for future work have been identified.</li>
</ul>

<h3>Title: Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach  to Searching for the Most Promising Intermediate Thought</h3>
<ul>
<li><strong>Authors: </strong>Zhen-Yu Zhang, Siwei Han, Huaxiu Yao, Gang Niu, Masashi Sugiyama</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06918">https://arxiv.org/abs/2402.06918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06918">https://arxiv.org/pdf/2402.06918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06918]] Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach  to Searching for the Most Promising Intermediate Thought(https://arxiv.org/abs/2402.06918)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To improve the ability of the large language model (LLMs) to handle complex reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs to reason step-by-step, facilitating problem solving from simple to complex tasks. State-of-the-art approaches for generating such a chain involve interactive collaboration, where the learner generates candidate intermediate thoughts, evaluated by the LLM, guiding the generation of subsequent thoughts. However, a widespread yet understudied problem is that the evaluation from the LLM is typically noisy and unreliable, potentially misleading the generation process in selecting promising intermediate thoughts. In this paper, motivated by Vapnik's principle, we propose a novel comparison-based CoT generation algorithm that directly identifies the most promising thoughts with the noisy feedback from the LLM. In each round, we randomly pair intermediate thoughts and directly prompt the LLM to select the more promising one from each pair, allowing us to identify the most promising thoughts through an iterative process. To further model the noise in the comparison, we resort to the techniques of ensemble and dueling bandits and propose two variants of the proposed algorithm. Experiments on three real-world mathematical and reasoning tasks demonstrate the effectiveness of our proposed algorithm and verify the rationale of the direct pairwise comparison.</li>
</ul>

<h3>Title: Whispers in the Machine: Confidentiality in LLM-integrated Systems</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Evertz, Merlin Chlosta, Lea Schönherr, Thorsten Eisenhofer</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06922">https://arxiv.org/abs/2402.06922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06922">https://arxiv.org/pdf/2402.06922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06922]] Whispers in the Machine: Confidentiality in LLM-integrated Systems(https://arxiv.org/abs/2402.06922)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly integrated with external tools. While these integrations can significantly improve the functionality of LLMs, they also create a new attack surface where confidential data may be disclosed between different components. Specifically, malicious tools can exploit vulnerabilities in the LLM itself to manipulate the model and compromise the data of other services, raising the question of how private data can be protected in the context of LLM integrations. In this work, we provide a systematic way of evaluating confidentiality in LLM-integrated systems. For this, we formalize a "secret key" game that can capture the ability of a model to conceal private information. This enables us to compare the vulnerability of a model against confidentiality attacks and also the effectiveness of different defense strategies. In this framework, we evaluate eight previously published attacks and four defenses. We find that current defenses lack generalization across attack strategies. Building on this analysis, we propose a method for robustness fine-tuning, inspired by adversarial training. This approach is effective in lowering the success rate of attackers and in improving the system's resilience against unknown attacks.</li>
</ul>

<h3>Title: A Thorough Examination of Decoding Methods in the Era of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, Wai Lam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06925">https://arxiv.org/abs/2402.06925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06925">https://arxiv.org/pdf/2402.06925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06925]] A Thorough Examination of Decoding Methods in the Era of LLMs(https://arxiv.org/abs/2402.06925)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose large language models (LLMs). Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining optimal results and the practicality of implementation in varying contexts.</li>
</ul>

<h3>Title: Assessing Uncertainty Estimation Methods for 3D Image Segmentation under  Distribution Shifts</h3>
<ul>
<li><strong>Authors: </strong>Masoumeh Javanbakhat, Md Tasnimul Hasan, Cristoph Lippert</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06937">https://arxiv.org/abs/2402.06937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06937">https://arxiv.org/pdf/2402.06937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06937]] Assessing Uncertainty Estimation Methods for 3D Image Segmentation under  Distribution Shifts(https://arxiv.org/abs/2402.06937)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In recent years, machine learning has witnessed extensive adoption across various sectors, yet its application in medical image-based disease detection and diagnosis remains challenging due to distribution shifts in real-world data. In practical settings, deployed models encounter samples that differ significantly from the training dataset, especially in the health domain, leading to potential performance issues. This limitation hinders the expressiveness and reliability of deep learning models in health applications. Thus, it becomes crucial to identify methods capable of producing reliable uncertainty estimation in the context of distribution shifts in the health sector. In this paper, we explore the feasibility of using cutting-edge Bayesian and non-Bayesian methods to detect distributionally shifted samples, aiming to achieve reliable and trustworthy diagnostic predictions in segmentation task. Specifically, we compare three distinct uncertainty estimation methods, each designed to capture either unimodal or multimodal aspects in the posterior distribution. Our findings demonstrate that methods capable of addressing multimodal characteristics in the posterior distribution, offer more dependable uncertainty estimates. This research contributes to enhancing the utility of deep learning in healthcare, making diagnostic predictions more robust and trustworthy.</li>
</ul>

<h3>Title: Should I try multiple optimizers when fine-tuning pre-trained  Transformers for NLP tasks? Should I tune their hyperparameters?</h3>
<ul>
<li><strong>Authors: </strong>Nefeli Gkouti, Prodromos Malakasiotis, Stavros Toumpis, Ion Androutsopoulos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06948">https://arxiv.org/abs/2402.06948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06948">https://arxiv.org/pdf/2402.06948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06948]] Should I try multiple optimizers when fine-tuning pre-trained  Transformers for NLP tasks? Should I tune their hyperparameters?(https://arxiv.org/abs/2402.06948)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>NLP research has explored different neural model architectures and sizes, datasets, training objectives, and transfer learning techniques. However, the choice of optimizer during training has not been explored as extensively. Typically, some variant of Stochastic Gradient Descent (SGD) is employed, selected among numerous variants, using unclear criteria, often with minimal or no tuning of the optimizer's hyperparameters. Experimenting with five GLUE datasets, two models (DistilBERT and DistilRoBERTa), and seven popular optimizers (SGD, SGD with Momentum, Adam, AdaMax, Nadam, AdamW, and AdaBound), we find that when the hyperparameters of the optimizers are tuned, there is no substantial difference in test performance across the five more elaborate (adaptive) optimizers, despite differences in training loss. Furthermore, tuning just the learning rate is in most cases as good as tuning all the hyperparameters. Hence, we recommend picking any of the best-behaved adaptive optimizers (e.g., Adam) and tuning only its learning rate. When no hyperparameter can be tuned, SGD with Momentum is the best choice.</li>
</ul>

<h3>Title: Semantic Object-level Modeling for Robust Visual Camera Relocalization</h3>
<ul>
<li><strong>Authors: </strong>Yifan Zhu, Lingjuan Miao, Haitao Wu, Zhiqiang Zhou, Weiyi Chen, Longwen Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06951">https://arxiv.org/abs/2402.06951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06951">https://arxiv.org/pdf/2402.06951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06951]] Semantic Object-level Modeling for Robust Visual Camera Relocalization(https://arxiv.org/abs/2402.06951)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual relocalization is crucial for autonomous visual localization and navigation of mobile robotics. Due to the improvement of CNN-based object detection algorithm, the robustness of visual relocalization is greatly enhanced especially in viewpoints where classical methods fail. However, ellipsoids (quadrics) generated by axis-aligned object detection may limit the accuracy of the object-level representation and degenerate the performance of visual relocalization system. In this paper, we propose a novel method of automatic object-level voxel modeling for accurate ellipsoidal representations of objects. As for visual relocalization, we design a better pose optimization strategy for camera pose recovery, to fully utilize the projection characteristics of 2D fitted ellipses and the 3D accurate ellipsoids. All of these modules are entirely intergrated into visual SLAM system. Experimental results show that our semantic object-level mapping and object-based visual relocalization methods significantly enhance the performance of visual relocalization in terms of robustness to new viewpoints.</li>
</ul>

<h3>Title: OpenFedLLM: Training Large Language Models on Decentralized Private Data  via Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, Siheng Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.DC, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06954">https://arxiv.org/abs/2402.06954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06954">https://arxiv.org/pdf/2402.06954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06954]] OpenFedLLM: Training Large Language Models on Decentralized Private Data  via Federated Learning(https://arxiv.org/abs/2402.06954)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>Trained on massive publicly available data, large language models (LLMs) have demonstrated tremendous success across various fields. While more data contributes to better performance, a disconcerting reality is that high-quality public data will be exhausted in a few years. In this paper, we offer a potential next step for contemporary LLMs: collaborative and privacy-preserving LLM training on the underutilized distributed private data via federated learning (FL), where multiple data owners collaboratively train a shared model without transmitting raw data. To achieve this, we build a concise, integrated, and research-friendly framework/codebase, named OpenFedLLM. It covers federated instruction tuning for enhancing instruction-following capability, federated value alignment for aligning with human values, and 7 representative FL algorithms. Besides, OpenFedLLM supports training on diverse domains, where we cover 8 training datasets; and provides comprehensive evaluations, where we cover 30+ evaluation metrics. Through extensive experiments, we observe that all FL algorithms outperform local training on training LLMs, demonstrating a clear performance improvement across a variety of settings. Notably, in a financial benchmark, Llama2-7B fine-tuned by applying any FL algorithm can outperform GPT-4 by a significant margin while the model obtained through individual training cannot, demonstrating strong motivation for clients to participate in FL. The code is available at https://github.com/rui-ye/OpenFedLLM.</li>
</ul>

<h3>Title: Architectural Neural Backdoors from First Principles</h3>
<ul>
<li><strong>Authors: </strong>Harry Langford, Ilia Shumailov, Yiren Zhao, Robert Mullins, Nicolas Papernot</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06957">https://arxiv.org/abs/2402.06957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06957">https://arxiv.org/pdf/2402.06957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06957]] Architectural Neural Backdoors from First Principles(https://arxiv.org/abs/2402.06957)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust</a></li>
<li><strong>Abstract: </strong>While previous research backdoored neural networks by changing their parameters, recent work uncovered a more insidious threat: backdoors embedded within the definition of the network's architecture. This involves injecting common architectural components, such as activation functions and pooling layers, to subtly introduce a backdoor behavior that persists even after (full re-)training. However, the full scope and implications of architectural backdoors have remained largely unexplored. Bober-Irizar et al. [2023] introduced the first architectural backdoor; they showed how to create a backdoor for a checkerboard pattern, but never explained how to target an arbitrary trigger pattern of choice. In this work we construct an arbitrary trigger detector which can be used to backdoor an architecture with no human supervision. This leads us to revisit the concept of architecture backdoors and taxonomise them, describing 12 distinct types. To gauge the difficulty of detecting such backdoors, we conducted a user study, revealing that ML developers can only identify suspicious components in common model definitions as backdoors in 37% of cases, while they surprisingly preferred backdoored models in 33% of cases. To contextualize these results, we find that language models outperform humans at the detection of backdoors. Finally, we discuss defenses against architectural backdoors, emphasizing the need for robust and comprehensive strategies to safeguard the integrity of ML systems.</li>
</ul>

<h3>Title: SpeechCLIP+: Self-supervised multi-task representation learning for  speech via CLIP and speech-image data</h3>
<ul>
<li><strong>Authors: </strong>Hsuan-Fu Wang, Yi-Jen Shih, Heng-Jui Chang, Layne Berry, Puyuan Peng, Hung-yi Lee, Hsin-Min Wang, David Harwath</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06959">https://arxiv.org/abs/2402.06959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06959">https://arxiv.org/pdf/2402.06959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06959]] SpeechCLIP+: Self-supervised multi-task representation learning for  speech via CLIP and speech-image data(https://arxiv.org/abs/2402.06959)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The recently proposed visually grounded speech model SpeechCLIP is an innovative framework that bridges speech and text through images via CLIP without relying on text transcription. On this basis, this paper introduces two extensions to SpeechCLIP. First, we apply the Continuous Integrate-and-Fire (CIF) module to replace a fixed number of CLS tokens in the cascaded architecture. Second, we propose a new hybrid architecture that merges the cascaded and parallel architectures of SpeechCLIP into a multi-task learning framework. Our experimental evaluation is performed on the Flickr8k and SpokenCOCO datasets. The results show that in the speech keyword extraction task, the CIF-based cascaded SpeechCLIP model outperforms the previous cascaded SpeechCLIP model using a fixed number of CLS tokens. Furthermore, through our hybrid architecture, cascaded task learning boosts the performance of the parallel branch in image-speech retrieval tasks.</li>
</ul>

<h3>Title: NLP for Knowledge Discovery and Information Extraction from Energetics  Corpora</h3>
<ul>
<li><strong>Authors: </strong>Francis G. VanGessel, Efrem Perry, Salil Mohan, Oliver M. Barham, Mark Cavolowsky</a></li>
<li><strong>Subjects: </strong>cs.CL, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06964">https://arxiv.org/abs/2402.06964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06964">https://arxiv.org/pdf/2402.06964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06964]] NLP for Knowledge Discovery and Information Extraction from Energetics  Corpora(https://arxiv.org/abs/2402.06964)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>We present a demonstration of the utility of NLP for aiding research into energetic materials and associated systems. The NLP method enables machine understanding of textual data, offering an automated route to knowledge discovery and information extraction from energetics text. We apply three established unsupervised NLP models: Latent Dirichlet Allocation, Word2Vec, and the Transformer to a large curated dataset of energetics-related scientific articles. We demonstrate that each NLP algorithm is capable of identifying energetic topics and concepts, generating a language model which aligns with Subject Matter Expert knowledge. Furthermore, we present a document classification pipeline for energetics text. Our classification pipeline achieves 59-76\% accuracy depending on the NLP model used, with the highest performing Transformer model rivaling inter-annotator agreement metrics. The NLP approaches studied in this work can identify concepts germane to energetics and therefore hold promise as a tool for accelerating energetics research efforts and energetics material development.</li>
</ul>

<h3>Title: DeepCover: Advancing RNN Test Coverage and Online Error Prediction using  State Machine Extraction</h3>
<ul>
<li><strong>Authors: </strong>Pouria Golshanrad, Fathiyeh Faghih</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06966">https://arxiv.org/abs/2402.06966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06966">https://arxiv.org/pdf/2402.06966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06966]] DeepCover: Advancing RNN Test Coverage and Online Error Prediction using  State Machine Extraction(https://arxiv.org/abs/2402.06966)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Recurrent neural networks (RNNs) have emerged as powerful tools for processing sequential data in various fields, including natural language processing and speech recognition. However, the lack of explainability in RNN models has limited their interpretability, posing challenges in understanding their internal workings. To address this issue, this paper proposes a methodology for extracting a state machine (SM) from an RNN-based model to provide insights into its internal function. The proposed SM extraction algorithm was assessed using four newly proposed metrics: Purity, Richness, Goodness, and Scale. The proposed methodology along with its assessment metrics contribute to increasing explainability in RNN models by providing a clear representation of their internal decision making process through the extracted SM. In addition to improving the explainability of RNNs, the extracted SM can be used to advance testing and and monitoring of the primary RNN-based model. To enhance RNN testing, we introduce six model coverage criteria based on the extracted SM, serving as metrics for evaluating the effectiveness of test suites designed to analyze the primary model. We also propose a tree-based model to predict the error probability of the primary model for each input based on the extracted SM. We evaluated our proposed online error prediction approach using the MNIST dataset and Mini Speech Commands dataset, achieving an area under the curve (AUC) exceeding 80\% for the receiver operating characteristic (ROC) chart.</li>
</ul>

<h3>Title: Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning  Framework for Dialogue</h3>
<ul>
<li><strong>Authors: </strong>Jian Wang, Chak Tou Leong, Jiashuo Wang, Dongding Lin, Wenjie Li, Xiao-Yong Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06967">https://arxiv.org/abs/2402.06967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06967">https://arxiv.org/pdf/2402.06967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06967]] Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning  Framework for Dialogue(https://arxiv.org/abs/2402.06967)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tuning pretrained language models for dialogue generation has been a prevalent paradigm for building capable dialogue agents. Yet, traditional tuning narrowly views dialogue generation as resembling other language generation tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such a manner leads to unsatisfactory chat consistency of the built agent. In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently. We propose an efficient Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the agent and user individually with two adapters built upon large language models, where they utilize utterances round by round in alternating order and are tuned via a round-level memory caching mechanism. Extensive experiments demonstrate that, our framework performs superior to traditional fine-tuning and harbors the tremendous potential for improving dialogue consistency.</li>
</ul>

<h3>Title: Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable  Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ayman Abaid, Muhammad Ali Farooq, Niamh Hynes, Peter Corcoran, Ihsan Ullah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06969">https://arxiv.org/abs/2402.06969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06969">https://arxiv.org/pdf/2402.06969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06969]] Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable  Diffusion Models(https://arxiv.org/abs/2402.06969)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Stable Diffusion (SD) has gained a lot of attention in recent years in the field of Generative AI thus helping in synthesizing medical imaging data with distinct features. The aim is to contribute to the ongoing effort focused on overcoming the limitations of data scarcity and improving the capabilities of ML algorithms for cardiovascular image processing. Therefore, in this study, the possibility of generating synthetic cardiac CTA images was explored by fine-tuning stable diffusion models based on user defined text prompts, using only limited number of CTA images as input. A comprehensive evaluation of the synthetic data was conducted by incorporating both quantitative analysis and qualitative assessment, where a clinician assessed the quality of the generated data. It has been shown that Cardiac CTA images can be successfully generated using using Text to Image (T2I) stable diffusion model. The results demonstrate that the tuned T2I CTA diffusion model was able to generate images with features that are typically unique to acute type B aortic dissection (TBAD) medical conditions.</li>
</ul>

<h3>Title: In-Context Data Distillation with TabPFN</h3>
<ul>
<li><strong>Authors: </strong>Junwei Ma, Valentin Thomas, Guangwei Yu, Anthony Caterini</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06971">https://arxiv.org/abs/2402.06971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06971">https://arxiv.org/pdf/2402.06971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06971]] In-Context Data Distillation with TabPFN(https://arxiv.org/abs/2402.06971)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Foundation models have revolutionized tasks in computer vision and natural language processing. However, in the realm of tabular data, tree-based models like XGBoost continue to dominate. TabPFN, a transformer model tailored for tabular data, mirrors recent foundation models in its exceptional in-context learning capability, being competitive with XGBoost's performance without the need for task-specific training or hyperparameter tuning. Despite its promise, TabPFN's applicability is hindered by its data size constraint, limiting its use in real-world scenarios. To address this, we present in-context data distillation (ICD), a novel methodology that effectively eliminates these constraints by optimizing TabPFN's context. ICD efficiently enables TabPFN to handle significantly larger datasets with a fixed memory budget, improving TabPFN's quadratic memory complexity but at the cost of a linear number of tuning steps. Notably, TabPFN, enhanced with ICD, demonstrates very strong performance against established tree-based models and modern deep learning methods on 48 large tabular datasets from OpenML.</li>
</ul>

<h3>Title: Event-Keyed Summarization</h3>
<ul>
<li><strong>Authors: </strong>William Gantt, Alexander Martin, Pavlo Kuchmiichuk, Aaron Steven White</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06973">https://arxiv.org/abs/2402.06973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06973">https://arxiv.org/pdf/2402.06973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06973]] Event-Keyed Summarization(https://arxiv.org/abs/2402.06973)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>We introduce event-keyed summarization (EKS), a novel task that marries traditional summarization and document-level event extraction, with the goal of generating a contextualized summary for a specific event, given a document and an extracted event structure. We introduce a dataset for this task, MUCSUM, consisting of summaries of all events in the classic MUC-4 dataset, along with a set of baselines that comprises both pretrained LM standards in the summarization literature, as well as larger frontier models. We show that ablations that reduce EKS to traditional summarization or structure-to-text yield inferior summaries of target events and that MUCSUM is a robust benchmark for this task. Lastly, we conduct a human evaluation of both reference and model summaries, and provide some detailed analysis of the results.</li>
</ul>

<h3>Title: Non-linear Fusion in Federated Learning: A Hypernetwork Approach to  Federated Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Marc Bartholet, Taehyeon Kim, Ami Beuret, Se-Young Yun, Joachim M. Buhmann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06974">https://arxiv.org/abs/2402.06974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06974">https://arxiv.org/pdf/2402.06974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06974]] Non-linear Fusion in Federated Learning: A Hypernetwork Approach to  Federated Domain Generalization(https://arxiv.org/abs/2402.06974)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has emerged as a promising paradigm in which multiple clients collaboratively train a shared global model while preserving data privacy. To create a robust and practicable FL framework, it is crucial to extend its ability to generalize well to unseen domains - a problem referred to as federated Domain Generalization (FDG), being still under-explored. We propose an innovative federated algorithm, termed hFedF for hypernetwork-based Federated Fusion, designed to bridge the performance gap between generalization and personalization, capable of addressing various degrees of domain shift. Essentially, the hypernetwork supports a non-linear fusion of client models enabling a comprehensive understanding of the underlying data distribution. We encompass an extensive discussion and provide novel insights into the tradeoff between personalization and generalization in FL. The proposed algorithm outperforms strong benchmarks on three widely-used data sets for DG in an exceeding number of cases.</li>
</ul>

<h3>Title: A Change Detection Reality Check</h3>
<ul>
<li><strong>Authors: </strong>Isaac Corley, Caleb Robinson, Anthony Ortiz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06994">https://arxiv.org/abs/2402.06994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06994">https://arxiv.org/pdf/2402.06994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06994]] A Change Detection Reality Check(https://arxiv.org/abs/2402.06994)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In recent years, there has been an explosion of proposed change detection deep learning architectures in the remote sensing literature. These approaches claim to offer state-of the-art performance on different standard benchmark datasets. However, has the field truly made significant progress? In this paper we perform experiments which conclude a simple U-Net segmentation baseline without training tricks or complicated architectural changes is still a top performer for the task of change detection.</li>
</ul>

<h3>Title: Clients Collaborate: Flexible Differentially Private Federated Learning  with Guaranteed Improvement of Utility-Privacy Trade-off</h3>
<ul>
<li><strong>Authors: </strong>Yuecheng Li, Tong Wang, Chuan Chen, Jian Lou, Bin Chen, Lei Yang, Zibin Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07002">https://arxiv.org/abs/2402.07002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07002">https://arxiv.org/pdf/2402.07002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07002]] Clients Collaborate: Flexible Differentially Private Federated Learning  with Guaranteed Improvement of Utility-Privacy Trade-off(https://arxiv.org/abs/2402.07002)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>To defend against privacy leakage of user data, differential privacy is widely used in federated learning, but it is not free. The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds. In this paper, we introduce a novel federated learning framework with rigorous privacy guarantees, named FedCEO, designed to strike a trade-off between model utility and user privacy by letting clients ''Collaborate with Each Other''. Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to flexibly truncate high-frequency components in spectral space. This implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes. Moreover, we improve the SOTA utility-privacy trade-off bound by an order of $\sqrt{d}$, where $d$ is the input dimension. We illustrate our theoretical results with experiments on representative image datasets. It observes significant performance improvements and strict privacy guarantees under different privacy settings.</li>
</ul>

<h3>Title: FedImpro: Measuring and Improving Client Update in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhenheng Tang, Yonggang Zhang, Shaohuai Shi, Xinmei Tian, Tongliang Liu, Bo Han, Xiaowen Chu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07011">https://arxiv.org/abs/2402.07011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07011">https://arxiv.org/pdf/2402.07011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07011]] FedImpro: Measuring and Improving Client Update in Federated Learning(https://arxiv.org/abs/2402.07011)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients. To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models. In this paper, we present an alternative perspective on client drift and aim to mitigate it by generating improved local models. First, we analyze the generalization contribution of local training and conclude that this generalization contribution is bounded by the conditional Wasserstein distance between the data distribution of different clients. Then, we propose FedImpro, to construct similar conditional distributions for local training. Specifically, FedImpro decouples the model into high-level and low-level components, and trains the high-level portion on reconstructed feature distributions. This approach enhances the generalization contribution and reduces the dissimilarity of gradients in FL. Experimental results show that FedImpro can help FL defend against data heterogeneity and enhance the generalization performance of the model.</li>
</ul>

<h3>Title: Gemini Goes to Med School: Exploring the Capabilities of Multimodal  Large Language Models on Medical Challenge Problems & Hallucinations</h3>
<ul>
<li><strong>Authors: </strong>Ankit Pal, Malaikannan Sankarasubbu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07023">https://arxiv.org/abs/2402.07023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07023">https://arxiv.org/pdf/2402.07023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07023]] Gemini Goes to Med School: Exploring the Capabilities of Multimodal  Large Language Models on Medical Challenge Problems & Hallucinations(https://arxiv.org/abs/2402.07023)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have the potential to be valuable in the healthcare industry, but it's crucial to verify their safety and effectiveness through rigorous evaluation. For this purpose, we comprehensively evaluated both open-source LLMs and Google's new multimodal LLM called Gemini across Medical reasoning, hallucination detection, and Medical Visual Question Answering tasks. While Gemini showed competence, it lagged behind state-of-the-art models like MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved an accuracy of 61.45\% on the medical VQA dataset, significantly lower than GPT-4V's score of 88\%. Our analysis revealed that Gemini is highly susceptible to hallucinations, overconfidence, and knowledge gaps, which indicate risks if deployed uncritically. We also performed a detailed analysis by medical subject and test type, providing actionable feedback for developers and clinicians. To mitigate risks, we applied prompting strategies that improved performance. Additionally, we facilitated future research and development by releasing a Python module for medical LLM evaluation and establishing a dedicated leaderboard on Hugging Face for medical domain LLMs. Python module can be found at https://github.com/promptslab/RosettaEval</li>
</ul>

<h3>Title: Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts  Models</h3>
<ul>
<li><strong>Authors: </strong>Keisuke Kamahori, Yile Gu, Kan Zhu, Baris Kasikci</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07033">https://arxiv.org/abs/2402.07033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07033">https://arxiv.org/pdf/2402.07033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07033]] Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts  Models(https://arxiv.org/abs/2402.07033)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over $3$ tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods. The code of Fiddler is publicly available at \url{https://github.com/efeslab/fiddler}</li>
</ul>

<h3>Title: A Tale of Tails: Model Collapse as a Change of Scaling Laws</h3>
<ul>
<li><strong>Authors: </strong>Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton, Julia Kempe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07043">https://arxiv.org/abs/2402.07043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07043">https://arxiv.org/pdf/2402.07043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07043]] A Tale of Tails: Model Collapse as a Change of Scaling Laws(https://arxiv.org/abs/2402.07043)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation using the large language model Llama2.</li>
</ul>

<h3>Title: $L^*LM$: Learning Automata from Examples using Natural Language Oracles</h3>
<ul>
<li><strong>Authors: </strong>Marcell Vazquez-Chanlatte, Karim Elmaaroufi, Stefan J. Witwicki, Sanjit A. Seshia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.FL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07051">https://arxiv.org/abs/2402.07051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07051">https://arxiv.org/pdf/2402.07051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07051]] $L^*LM$: Learning Automata from Examples using Natural Language Oracles(https://arxiv.org/abs/2402.07051)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Expert demonstrations have proven an easy way to indirectly specify complex tasks. Recent algorithms even support extracting unambiguous formal specifications, e.g. deterministic finite automata (DFA), from demonstrations. Unfortunately, these techniques are generally not sample efficient. In this work, we introduce $L^*LM$, an algorithm for learning DFAs from both demonstrations and natural language. Due to the expressivity of natural language, we observe a significant improvement in the data efficiency of learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large language models to answer membership queries about the underlying task. This is then combined with recent techniques for transforming learning from demonstrations into a sequence of labeled example learning problems. In our experiments, we observe the two modalities complement each other, yielding a powerful few-shot learner.</li>
</ul>

<h3>Title: HNMblock: Blockchain technology powered Healthcare Network Model for  epidemiological monitoring, medical systems security, and wellness</h3>
<ul>
<li><strong>Authors: </strong>Naresh Kshetri, Rahul Mishra, Mir Mehedi Rahman, Tanja Steigner</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07054">https://arxiv.org/abs/2402.07054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07054">https://arxiv.org/pdf/2402.07054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07054]] HNMblock: Blockchain technology powered Healthcare Network Model for  epidemiological monitoring, medical systems security, and wellness(https://arxiv.org/abs/2402.07054)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>In the ever-evolving healthcare sector, the widespread adoption of Internet of Things and wearable technologies facilitates remote patient monitoring. However, the existing client/server infrastructure poses significant security and privacy challenges, necessitating strict adherence to healthcare data regulations. To combat these issues, a decentralized approach is imperative, and blockchain technology emerges as a compelling solution for strengthening Internet of Things and medical systems security. This paper introduces HNMblock, a model that elevates the realms of epidemiological monitoring, medical system security, and wellness enhancement. By harnessing the transparency and immutability inherent in blockchain, HNMblock empowers real-time, tamper-proof tracking of epidemiological data, enabling swift responses to disease outbreaks. Furthermore, it fortifies the security of medical systems through advanced cryptographic techniques and smart contracts, with a paramount focus on safeguarding patient privacy. HNMblock also fosters personalized healthcare, encouraging patient involvement and data-informed decision-making. The integration of blockchain within the healthcare domain, as exemplified by HNMblock, holds the potential to revolutionize data management, epidemiological surveillance, and wellness, as meticulously explored in this research article.</li>
</ul>

<h3>Title: Using Large Language Models to Automate and Expedite Reinforcement  Learning with Reward Machine</h3>
<ul>
<li><strong>Authors: </strong>Shayan Meshkat Alsadat, Jean-Raphael Gaglione, Daniel Neider, Ufuk Topcu, Zhe Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07069">https://arxiv.org/abs/2402.07069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07069">https://arxiv.org/pdf/2402.07069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07069]] Using Large Language Models to Automate and Expedite Reinforcement  Learning with Reward Machine(https://arxiv.org/abs/2402.07069)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm in order to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. We also show the theoretical guarantee of our algorithm to converge to an optimal policy. We demonstrate that LARL-RM speeds up the convergence by 30% by implementing our method in two case studies.</li>
</ul>

<h3>Title: Using Large Language Models for Student-Code Guided Test Case Generation  in Computer Science Education</h3>
<ul>
<li><strong>Authors: </strong>Nischal Ashok Kumar, Andrew Lan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07081">https://arxiv.org/abs/2402.07081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07081">https://arxiv.org/pdf/2402.07081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07081]] Using Large Language Models for Student-Code Guided Test Case Generation  in Computer Science Education(https://arxiv.org/abs/2402.07081)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In computer science education, test cases are an integral part of programming assignments since they can be used as assessment items to test students' programming knowledge and provide personalized feedback on student-written code. The goal of our work is to propose a fully automated approach for test case generation that can accurately measure student knowledge, which is important for two reasons. First, manually constructing test cases requires expert knowledge and is a labor-intensive process. Second, developing test cases for students, especially those who are novice programmers, is significantly different from those oriented toward professional-level software developers. Therefore, we need an automated process for test case generation to assess student knowledge and provide feedback. In this work, we propose a large language model-based approach to automatically generate test cases and show that they are good measures of student knowledge, using a publicly available dataset that contains student-written Java code. We also discuss future research directions centered on using test cases to help students.</li>
</ul>

<h3>Title: Self-Correcting Self-Consuming Loops for Generative Model Training</h3>
<ul>
<li><strong>Authors: </strong>Nate Gillman, Michael Freeman, Daksh Aggarwal, Chia-Hong Hsu, Calvin Luo, Yonglong Tian, Chen Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07087">https://arxiv.org/abs/2402.07087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07087">https://arxiv.org/pdf/2402.07087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07087]] Self-Correcting Self-Consuming Loops for Generative Model Training(https://arxiv.org/abs/2402.07087)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates "self-consuming loops" which may lead to training instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of self-correcting self-consuming loops on the challenging human motion synthesis task, and observe that it successfully avoids model collapse, even when the ratio of synthetic data to real data is as high as 100%.</li>
</ul>

<h3>Title: A Benchmark for Multi-modal Foundation Models on Low-level Vision: from  Single Images to Pairs</h3>
<ul>
<li><strong>Authors: </strong>Zicheng Zhang, Haoning Wu, Erli Zhang, Guangtao Zhai, Weisi Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07116">https://arxiv.org/abs/2402.07116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07116">https://arxiv.org/pdf/2402.07116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07116]] A Benchmark for Multi-modal Foundation Models on Low-level Vision: from  Single Images to Pairs(https://arxiv.org/abs/2402.07116)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid development of Multi-modality Large Language Models (MLLMs) has navigated a paradigm shift in computer vision, moving towards versatile foundational models. However, evaluating MLLMs in low-level visual perception and understanding remains a yet-to-explore domain. To this end, we design benchmark settings to emulate human language responses related to low-level vision: the low-level visual perception (A1) via visual question answering related to low-level attributes (e.g. clarity, lighting); and the low-level visual description (A2), on evaluating MLLMs for low-level text descriptions. Furthermore, given that pairwise comparison can better avoid ambiguity of responses and has been adopted by many human experiments, we further extend the low-level perception-related question-answering and description evaluations of MLLMs from single images to image pairs. Specifically, for perception (A1), we carry out the LLVisionQA+ dataset, comprising 2,990 single images and 1,999 image pairs each accompanied by an open-ended question about its low-level features; for description (A2), we propose the LLDescribe+ dataset, evaluating MLLMs for low-level descriptions on 499 single images and 450 pairs. Additionally, we evaluate MLLMs on assessment (A3) ability, i.e. predicting score, by employing a softmax-based approach to enable all MLLMs to generate quantifiable quality ratings, tested against human opinions in 7 image quality assessment (IQA) datasets. With 24 MLLMs under evaluation, we demonstrate that several MLLMs have decent low-level visual competencies on single images, but only GPT-4V exhibits higher accuracy on pairwise comparisons than single image evaluations (like humans). We hope that our benchmark will motivate further research into uncovering and enhancing these nascent capabilities of MLLMs. Datasets will be available at https://github.com/Q-Future/Q-Bench.</li>
</ul>

<h3>Title: Two-Stage Multi-task Self-Supervised Learning for Medical Image  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Binyan Hu, A. K. Qin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07119">https://arxiv.org/abs/2402.07119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07119">https://arxiv.org/pdf/2402.07119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07119]] Two-Stage Multi-task Self-Supervised Learning for Medical Image  Segmentation(https://arxiv.org/abs/2402.07119)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation has been significantly advanced by deep learning (DL) techniques, though the data scarcity inherent in medical applications poses a great challenge to DL-based segmentation methods. Self-supervised learning offers a solution by creating auxiliary learning tasks from the available dataset and then leveraging the knowledge acquired from solving auxiliary tasks to help better solve the target segmentation task. Different auxiliary tasks may have different properties and thus can help the target task to different extents. It is desired to leverage their complementary advantages to enhance the overall assistance to the target task. To achieve this, existing methods often adopt a joint training paradigm, which co-solves segmentation and auxiliary tasks by integrating their losses or intermediate gradients. However, direct coupling of losses or intermediate gradients risks undesirable interference because the knowledge acquired from solving each auxiliary task at every training step may not always benefit the target task. To address this issue, we propose a two-stage training approach. In the first stage, the target segmentation task will be independently co-solved with each auxiliary task in both joint training and pre-training modes, with the better model selected via validation performance. In the second stage, the models obtained with respect to each auxiliary task are converted into a single model using an ensemble knowledge distillation method. Our approach allows for making best use of each auxiliary task to create multiple elite segmentation models and then combine them into an even more powerful model. We employed five auxiliary tasks of different proprieties in our approach and applied it to train the U-Net model on an X-ray pneumothorax segmentation dataset. Experimental results demonstrate the superiority of our approach over several existing methods.</li>
</ul>

<h3>Title: An attempt to generate new bridge types from latent space of denoising  diffusion Implicit model</h3>
<ul>
<li><strong>Authors: </strong>Hongjun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07129">https://arxiv.org/abs/2402.07129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07129">https://arxiv.org/pdf/2402.07129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07129]] An attempt to generate new bridge types from latent space of denoising  diffusion Implicit model(https://arxiv.org/abs/2402.07129)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Use denoising diffusion implicit model for bridge-type innovation. The process of adding noise and denoising to an image can be likened to the process of a corpse rotting and a detective restoring the scene of a victim being killed, to help beginners understand. Through an easy-to-understand algebraic method, derive the function formulas for adding noise and denoising, making it easier for beginners to master the mathematical principles of the model. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , denoising diffusion implicit model is constructed and trained. From the latent space sampling, new bridge types with asymmetric structures can be generated. Denoising diffusion implicit model can organically combine different structural components on the basis of human original bridge types, and create new bridge types.</li>
</ul>

<h3>Title: Towards Robust Car Following Dynamics Modeling via Blackbox Models:  Methodology, Analysis, and Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Bilal Shahid, Cody Fleming</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07139">https://arxiv.org/abs/2402.07139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07139">https://arxiv.org/pdf/2402.07139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07139]] Towards Robust Car Following Dynamics Modeling via Blackbox Models:  Methodology, Analysis, and Recommendations(https://arxiv.org/abs/2402.07139)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The selection of the target variable is important while learning parameters of the classical car following models like GIPPS, IDM, etc. There is a vast body of literature on which target variable is optimal for classical car following models, but there is no study that empirically evaluates the selection of optimal target variables for black-box models, such as LSTM, etc. The black-box models, like LSTM and Gaussian Process (GP) are increasingly being used to model car following behavior without wise selection of target variables. The current work tests different target variables, like acceleration, velocity, and headway, for three black-box models, i.e., GP, LSTM, and Kernel Ridge Regression. These models have different objective functions and work in different vector spaces, e.g., GP works in function space, and LSTM works in parameter space. The experiments show that the optimal target variable recommendations for black-box models differ from classical car following models depending on the objective function and the vector space. It is worth mentioning that models and datasets used during evaluation are diverse in nature: the datasets contained both automated and human-driven vehicle trajectories; the black-box models belong to both parametric and non-parametric classes of models. This diversity is important during the analysis of variance, wherein we try to find the interaction between datasets, models, and target variables. It is shown that the models and target variables interact and recommended target variables don't depend on the dataset under consideration.</li>
</ul>

<h3>Title: Explainable Global Wildfire Prediction Models using Graph Neural  Networks</h3>
<ul>
<li><strong>Authors: </strong>Dayou Chen, Sibo Cheng, Jinwei Hu, Matthew Kasoar, Rossella Arcucci</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07152">https://arxiv.org/abs/2402.07152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07152">https://arxiv.org/pdf/2402.07152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07152]] Explainable Global Wildfire Prediction Models using Graph Neural  Networks(https://arxiv.org/abs/2402.07152)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Wildfire prediction has become increasingly crucial due to the escalating impacts of climate change. Traditional CNN-based wildfire prediction models struggle with handling missing oceanic data and addressing the long-range dependencies across distant regions in meteorological data. In this paper, we introduce an innovative Graph Neural Network (GNN)-based model for global wildfire prediction. We propose a hybrid model that combines the spatial prowess of Graph Convolutional Networks (GCNs) with the temporal depth of Long Short-Term Memory (LSTM) networks. Our approach uniquely transforms global climate and wildfire data into a graph representation, addressing challenges such as null oceanic data locations and long-range dependencies inherent in traditional models. Benchmarking against established architectures using an unseen ensemble of JULES-INFERNO simulations, our model demonstrates superior predictive accuracy. Furthermore, we emphasise the model's explainability, unveiling potential wildfire correlation clusters through community detection and elucidating feature importance via Integrated Gradient analysis. Our findings not only advance the methodological domain of wildfire prediction but also underscore the importance of model transparency, offering valuable insights for stakeholders in wildfire management.</li>
</ul>

<h3>Title: Natural Language Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Xidong Feng, Ziyu Wan, Mengyue Yang, Ziyan Wang, Girish A. Koushiks, Yali Du, Ying Wen, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07157">https://arxiv.org/abs/2402.07157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07157">https://arxiv.org/pdf/2402.07157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07157]] Natural Language Reinforcement Learning(https://arxiv.org/abs/2402.07157)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has shown remarkable abilities in learning policies for decision-making tasks. However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals. To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language Reinforcement Learning (NLRL), which innovatively combines RL principles with natural language representation. Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space. We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework.</li>
</ul>

<h3>Title: GeoFormer: A Vision and Sequence Transformer-based Approach for  Greenhouse Gas Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Madhav Khirwar, Ankur Narang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07164">https://arxiv.org/abs/2402.07164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07164">https://arxiv.org/pdf/2402.07164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07164]] GeoFormer: A Vision and Sequence Transformer-based Approach for  Greenhouse Gas Monitoring(https://arxiv.org/abs/2402.07164)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Air pollution represents a pivotal environmental challenge globally, playing a major role in climate change via greenhouse gas emissions and negatively affecting the health of billions. However predicting the spatial and temporal patterns of pollutants remains challenging. The scarcity of ground-based monitoring facilities and the dependency of air pollution modeling on comprehensive datasets, often inaccessible for numerous areas, complicate this issue. In this work, we introduce GeoFormer, a compact model that combines a vision transformer module with a highly efficient time-series transformer module to predict surface-level nitrogen dioxide (NO2) concentrations from Sentinel-5P satellite imagery. We train the proposed model to predict surface-level NO2 measurements using a dataset we constructed with Sentinel-5P images of ground-level monitoring stations, and their corresponding NO2 concentration readings. The proposed model attains high accuracy (MAE 5.65), demonstrating the efficacy of combining vision and time-series transformer architectures to harness satellite-derived data for enhanced GHG emission insights, proving instrumental in advancing climate change monitoring and emission regulation efforts globally.</li>
</ul>

<h3>Title: Prompt Perturbation in Retrieval-Augmented Generation based Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhibo Hu, Chen Wang, Yanfeng Shu, Helen (Hye-Young)Paik, Liming Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07179">https://arxiv.org/abs/2402.07179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07179">https://arxiv.org/pdf/2402.07179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07179]] Prompt Perturbation in Retrieval-Augmented Generation based Large  Language Models(https://arxiv.org/abs/2402.07179)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves the robustness of RAG-based LLMs through a highly effective detector trained on neuron activation triggered by GGPP generated prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of our methods.</li>
</ul>

<h3>Title: MAGNETO: Edge AI for Human Activity Recognition -- Privacy and  Personalization</h3>
<ul>
<li><strong>Authors: </strong>Jingwei Zuo, George Arvanitakis, Mthandazo Ndhlovu, Hakim Hacid</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07180">https://arxiv.org/abs/2402.07180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07180">https://arxiv.org/pdf/2402.07180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07180]] MAGNETO: Edge AI for Human Activity Recognition -- Privacy and  Personalization(https://arxiv.org/abs/2402.07180)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Human activity recognition (HAR) is a well-established field, significantly advanced by modern machine learning (ML) techniques. While companies have successfully integrated HAR into consumer products, they typically rely on a predefined activity set, which limits personalizations at the user level (edge devices). Despite advancements in Incremental Learning for updating models with new data, this often occurs on the Cloud, necessitating regular data transfers between cloud and edge devices, thus leading to data privacy issues. In this paper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the Cloud to the Edge. MAGNETO allows incremental human activity learning directly on the Edge devices, without any data exchange with the Cloud. This enables strong privacy guarantees, low processing latency, and a high degree of personalization for users. In particular, we demonstrate MAGNETO in an Android device, validating the whole pipeline from data collection to result visualization.</li>
</ul>

<h3>Title: GSINA: Improving Subgraph Extraction for Graph Invariant Learning via  Graph Sinkhorn Attention</h3>
<ul>
<li><strong>Authors: </strong>Fangyu Ding, Haiyang Wang, Zhixuan Chu, Tianming Li, Zhaoping Hu, Junchi Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07191">https://arxiv.org/abs/2402.07191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07191">https://arxiv.org/pdf/2402.07191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07191]] GSINA: Improving Subgraph Extraction for Graph Invariant Learning via  Graph Sinkhorn Attention(https://arxiv.org/abs/2402.07191)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Graph invariant learning (GIL) has been an effective approach to discovering the invariant relationships between graph data and its labels for different graph learning tasks under various distribution shifts. Many recent endeavors of GIL focus on extracting the invariant subgraph from the input graph for prediction as a regularization strategy to improve the generalization performance of graph learning. Despite their success, such methods also have various limitations in obtaining their invariant subgraphs. In this paper, we provide in-depth analyses of the drawbacks of existing works and propose corresponding principles of our invariant subgraph extraction: 1) the sparsity, to filter out the variant features, 2) the softness, for a broader solution space, and 3) the differentiability, for a soundly end-to-end optimization. To meet these principles in one shot, we leverage the Optimal Transport (OT) theory and propose a novel graph attention mechanism called Graph Sinkhorn Attention (GSINA). This novel approach serves as a powerful regularization method for GIL tasks. By GSINA, we are able to obtain meaningful, differentiable invariant subgraphs with controllable sparsity and softness. Moreover, GSINA is a general graph learning framework that could handle GIL tasks of multiple data grain levels. Extensive experiments on both synthetic and real-world datasets validate the superiority of our GSINA, which outperforms the state-of-the-art GIL methods by large margins on both graph-level tasks and node-level tasks. Our code is publicly available at \url{https://github.com/dingfangyu/GSINA}.</li>
</ul>

<h3>Title: GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided  Generative Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07207">https://arxiv.org/abs/2402.07207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07207">https://arxiv.org/pdf/2402.07207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07207]] GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided  Generative Gaussian Splatting(https://arxiv.org/abs/2402.07207)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation. We first utilize large language models (LLMs) to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints. We then propose an object-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the LLMs to align with the generated scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene. Source codes and models will be available at https://gala3d.github.io/.</li>
</ul>

<h3>Title: Towards Fast Stochastic Sampling in Diffusion Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Kushagra Pandey, Maja Rudolph, Stephan Mandt</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07211">https://arxiv.org/abs/2402.07211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07211">https://arxiv.org/pdf/2402.07211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07211]] Towards Fast Stochastic Sampling in Diffusion Generative Models(https://arxiv.org/abs/2402.07211)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models suffer from slow sample generation at inference time. Despite recent efforts, improving the sampling efficiency of stochastic samplers for diffusion models remains a promising direction. We propose Splitting Integrators for fast stochastic sampling in pre-trained diffusion models in augmented spaces. Commonly used in molecular dynamics, splitting-based integrators attempt to improve sampling efficiency by cleverly alternating between numerical updates involving the data, auxiliary, or noise variables. However, we show that a naive application of splitting integrators is sub-optimal for fast sampling. Consequently, we propose several principled modifications to naive splitting samplers for improving sampling efficiency and denote the resulting samplers as Reduced Splitting Integrators. In the context of Phase Space Langevin Diffusion (PSLD) [Pandey \& Mandt, 2023] on CIFAR-10, our stochastic sampler achieves an FID score of 2.36 in only 100 network function evaluations (NFE) as compared to 2.63 for the best baselines.</li>
</ul>

<h3>Title: A novel spatial-frequency domain network for zero-shot incremental  learning</h3>
<ul>
<li><strong>Authors: </strong>Jie Ren, Yang Zhao, Weichuan Zhang, Changming Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07216">https://arxiv.org/abs/2402.07216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07216">https://arxiv.org/pdf/2402.07216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07216]] A novel spatial-frequency domain network for zero-shot incremental  learning(https://arxiv.org/abs/2402.07216)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Zero-shot incremental learning aims to enable the model to generalize to new classes without forgetting previously learned classes. However, the semantic gap between old and new sample classes can lead to catastrophic forgetting. Additionally, existing algorithms lack capturing significant information from each sample image domain, impairing models' classification performance. Therefore, this paper proposes a novel Spatial-Frequency Domain Network (SFDNet) which contains a Spatial-Frequency Feature Extraction (SFFE) module and Attention Feature Alignment (AFA) module to improve the Zero-Shot Translation for Class Incremental algorithm. Firstly, SFFE module is designed which contains a dual attention mechanism for obtaining salient spatial-frequency feature information. Secondly, a novel feature fusion module is conducted for obtaining fused spatial-frequency domain features. Thirdly, the Nearest Class Mean classifier is utilized to select the most suitable category. Finally, iteration between tasks is performed using the Zero-Shot Translation model. The proposed SFDNet has the ability to effectively extract spatial-frequency feature representation from input images, improve the accuracy of image classification, and fundamentally alleviate catastrophic forgetting. Extensive experiments on the CUB 200-2011 and CIFAR100 datasets demonstrate that our proposed algorithm outperforms state-of-the-art incremental learning algorithms.</li>
</ul>

<h3>Title: Rethinking Graph Masked Autoencoders through Alignment and Uniformity</h3>
<ul>
<li><strong>Authors: </strong>Liang Wang, Xiang Tao, Qiang Liu, Shu Wu, Liang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07225">https://arxiv.org/abs/2402.07225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07225">https://arxiv.org/pdf/2402.07225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07225]] Rethinking Graph Masked Autoencoders through Alignment and Uniformity(https://arxiv.org/abs/2402.07225)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Self-supervised learning on graphs can be bifurcated into contrastive and generative methods. Contrastive methods, also known as graph contrastive learning (GCL), have dominated graph self-supervised learning in the past few years, but the recent advent of graph masked autoencoder (GraphMAE) rekindles the momentum behind generative methods. Despite the empirical success of GraphMAE, there is still a dearth of theoretical understanding regarding its efficacy. Moreover, while both generative and contrastive methods have been shown to be effective, their connections and differences have yet to be thoroughly investigated. Therefore, we theoretically build a bridge between GraphMAE and GCL, and prove that the node-level reconstruction objective in GraphMAE implicitly performs context-level GCL. Based on our theoretical analysis, we further identify the limitations of the GraphMAE from the perspectives of alignment and uniformity, which have been considered as two key properties of high-quality representations in GCL. We point out that GraphMAE's alignment performance is restricted by the masking strategy, and the uniformity is not strictly guaranteed. To remedy the aforementioned limitations, we propose an Alignment-Uniformity enhanced Graph Masked AutoEncoder, named AUG-MAE. Specifically, we propose an easy-to-hard adversarial masking strategy to provide hard-to-align samples, which improves the alignment performance. Meanwhile, we introduce an explicit uniformity regularizer to ensure the uniformity of the learned representations. Experimental results on benchmark datasets demonstrate the superiority of our model over existing state-of-the-art methods.</li>
</ul>

<h3>Title: TransGPT: Multi-modal Generative Pre-trained Transformer for  Transportation</h3>
<ul>
<li><strong>Authors: </strong>Peng Wang, Xiang Wei, Fangxu Hu, Wenjuan Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07233">https://arxiv.org/abs/2402.07233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07233">https://arxiv.org/pdf/2402.07233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07233]] TransGPT: Multi-modal Generative Pre-trained Transformer for  Transportation(https://arxiv.org/abs/2402.07233)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Natural language processing (NLP) is a key component of intelligent transportation systems (ITS), but it faces many challenges in the transportation domain, such as domain-specific knowledge and data, and multi-modal inputs and outputs. This paper presents TransGPT, a novel (multi-modal) large language model for the transportation domain, which consists of two independent variants: TransGPT-SM for single-modal data and TransGPT-MM for multi-modal data. TransGPT-SM is finetuned on a single-modal Transportation dataset (STD) that contains textual data from various sources in the transportation domain. TransGPT-MM is finetuned on a multi-modal Transportation dataset (MTD) that we manually collected from three areas of the transportation domain: driving tests, traffic signs, and landmarks. We evaluate TransGPT on several benchmark datasets for different tasks in the transportation domain, and show that it outperforms baseline models on most tasks. We also showcase the potential applications of TransGPT for traffic analysis and modeling, such as generating synthetic traffic scenarios, explaining traffic phenomena, answering traffic-related questions, providing traffic recommendations, and generating traffic reports. This work advances the state-of-the-art of NLP in the transportation domain and provides a useful tool for ITS researchers and practitioners.</li>
</ul>

<h3>Title: Proof of Diligence: Cryptoeconomic Security for Rollups</h3>
<ul>
<li><strong>Authors: </strong>Peiyao Sheng, Ranvir Rana, Himanshu Tyagi, Pramod Viswanath</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07241">https://arxiv.org/abs/2402.07241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07241">https://arxiv.org/pdf/2402.07241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07241]] Proof of Diligence: Cryptoeconomic Security for Rollups(https://arxiv.org/abs/2402.07241)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense</a></li>
<li><strong>Abstract: </strong>Layer 1 (L1) blockchains such as Ethereum are secured under an "honest supermajority of stake" assumption for a large pool of validators who verify each and every transaction on it. This high security comes at a scalability cost which not only effects the throughput of the blockchain but also results in high gas fees for executing transactions on chain. The most successful solution for this problem is provided by optimistic rollups, Layer 2 (L2) blockchains that execute transactions outside L1 but post the transaction data on L1. The security for such L2 chains is argued, informally, under the assumption that a set of nodes will check the transaction data posted on L1 and raise an alarm (a fraud proof) if faulty transactions are detected. However, all current deployments lack a proper incentive mechanism for ensuring that these nodes will do their job ``diligently'', and simply rely on a cursory incentive alignment argument for security. We solve this problem by introducing an incentivized watchtower network designed to serve as the first line of defense for rollups. Our main contribution is a ``Proof of Diligence'' protocol that requires watchtowers to continuously provide a proof that they have verified L2 assertions and get rewarded for the same. Proof of Diligence protocol includes a carefully-designed incentive mechanism that is provably secure when watchtowers are rational actors, under a mild rational independence assumption. Our proposed system is now live on Ethereum testnet. We deployed a watchtower network and implemented Proof of Diligence for multiple optimistic rollups. We extract execution as well as inclusion proofs for transactions as a part of the bounty. Each watchtower has minimal additional computational overhead beyond access to standard L1 and L2 RPC nodes.</li>
</ul>

<h3>Title: PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point  Cloud Compression</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Pang, Kevin Bui, Dong Tian</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07243">https://arxiv.org/abs/2402.07243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07243">https://arxiv.org/pdf/2402.07243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07243]] PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point  Cloud Compression(https://arxiv.org/abs/2402.07243)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The universality of the point cloud format enables many 3D applications, making the compression of point clouds a critical phase in practice. Sampled as discrete 3D points, a point cloud approximates 2D surface(s) embedded in 3D with a finite bit-depth. However, the point distribution of a practical point cloud changes drastically as its bit-depth increases, requiring different methodologies for effective consumption/analysis. In this regard, a heterogeneous point cloud compression (PCC) framework is proposed. We unify typical point cloud representations -- point-based, voxel-based, and tree-based representations -- and their associated backbones under a learning-based framework to compress an input point cloud at different bit-depth levels. Having recognized the importance of voxel-domain processing, we augment the framework with a proposed context-aware upsampling for decoding and an enhanced voxel transformer for feature aggregation. Extensive experimentation demonstrates the state-of-the-art performance of our proposal on a wide range of point clouds.</li>
</ul>

<h3>Title: DIMON: Learning Solution Operators of Partial Differential Equations on  a Diffeomorphic Family of Domains</h3>
<ul>
<li><strong>Authors: </strong>Minglang Yin, Nicolas Charon, Ryan Brody, Lu Lu, Natalia Trayanova, Mauro Maggioni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07250">https://arxiv.org/abs/2402.07250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07250">https://arxiv.org/pdf/2402.07250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07250]] DIMON: Learning Solution Operators of Partial Differential Equations on  a Diffeomorphic Family of Domains(https://arxiv.org/abs/2402.07250)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The solution of a PDE over varying initial/boundary conditions on multiple domains is needed in a wide variety of applications, but it is computationally expensive if the solution is computed de novo whenever the initial/boundary conditions of the domain change. We introduce a general operator learning framework, called DIffeomorphic Mapping Operator learNing (DIMON) to learn approximate PDE solutions over a family of domains $\{\Omega_{\theta}}_\theta$, that learns the map from initial/boundary conditions and domain $\Omega_\theta$ to the solution of the PDE, or to specified functionals thereof. DIMON is based on transporting a given problem (initial/boundary conditions and domain $\Omega_{\theta}$) to a problem on a reference domain $\Omega_{0}$, where training data from multiple problems is used to learn the map to the solution on $\Omega_{0}$, which is then re-mapped to the original domain $\Omega_{\theta}$. We consider several problems to demonstrate the performance of the framework in learning both static and time-dependent PDEs on non-rigid geometries; these include solving the Laplace equation, reaction-diffusion equations, and a multiscale PDE that characterizes the electrical propagation on the left ventricle. This work paves the way toward the fast prediction of PDE solutions on a family of domains and the application of neural operators in engineering and precision medicine.</li>
</ul>

<h3>Title: Data Quality Aware Approaches for Addressing Model Drift of Semantic  Segmentation Models</h3>
<ul>
<li><strong>Authors: </strong>Samiha Mirza, Vuong D. Nguyen, Pranav Mantini, Shishir K. Shah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07258">https://arxiv.org/abs/2402.07258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07258">https://arxiv.org/pdf/2402.07258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07258]] Data Quality Aware Approaches for Addressing Model Drift of Semantic  Segmentation Models(https://arxiv.org/abs/2402.07258)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In the midst of the rapid integration of artificial intelligence (AI) into real world applications, one pressing challenge we confront is the phenomenon of model drift, wherein the performance of AI models gradually degrades over time, compromising their effectiveness in real-world, dynamic environments. Once identified, we need techniques for handling this drift to preserve the model performance and prevent further degradation. This study investigates two prominent quality aware strategies to combat model drift: data quality assessment and data conditioning based on prior model knowledge. The former leverages image quality assessment metrics to meticulously select high-quality training data, improving the model robustness, while the latter makes use of learned feature vectors from existing models to guide the selection of future data, aligning it with the model's prior knowledge. Through comprehensive experimentation, this research aims to shed light on the efficacy of these approaches in enhancing the performance and reliability of semantic segmentation models, thereby contributing to the advancement of computer vision capabilities in real-world scenarios.</li>
</ul>

<h3>Title: Trade-off Between Spatial and Angular Resolution in Facial Recognition</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Zeshan Alam, Sousso kelowani, Mohamed Elsaeidy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07263">https://arxiv.org/abs/2402.07263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07263">https://arxiv.org/pdf/2402.07263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07263]] Trade-off Between Spatial and Angular Resolution in Facial Recognition(https://arxiv.org/abs/2402.07263)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Ensuring robustness in face recognition systems across various challenging conditions is crucial for their versatility. State-of-the-art methods often incorporate additional information, such as depth, thermal, or angular data, to enhance performance. However, light field-based face recognition approaches that leverage angular information face computational limitations. This paper investigates the fundamental trade-off between spatio-angular resolution in light field representation to achieve improved face recognition performance. By utilizing macro-pixels with varying angular resolutions while maintaining the overall image size, we aim to quantify the impact of angular information at the expense of spatial resolution, while considering computational constraints. Our experimental results demonstrate a notable performance improvement in face recognition systems by increasing the angular resolution, up to a certain extent, at the cost of spatial resolution.</li>
</ul>

<h3>Title: Open-ended VQA benchmarking of Vision-Language models by exploiting  Classification datasets and their semantic hierarchy</h3>
<ul>
<li><strong>Authors: </strong>Simon Ging, María A. Bravo, Thomas Brox</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07270">https://arxiv.org/abs/2402.07270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07270">https://arxiv.org/pdf/2402.07270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07270]] Open-ended VQA benchmarking of Vision-Language models by exploiting  Classification datasets and their semantic hierarchy(https://arxiv.org/abs/2402.07270)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models' capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our benchmark to a suite of vision-language models and show a detailed comparison of their abilities on object, action, and attribute classification. Our contributions aim to lay the foundation for more precise and meaningful assessments, facilitating targeted progress in the exciting field of vision-language modeling.</li>
</ul>

<h3>Title: How do Large Language Models Navigate Conflicts between Honesty and  Helpfulness?</h3>
<ul>
<li><strong>Authors: </strong>Ryan Liu, Theodore R. Sumers, Ishita Dasgupta, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07282">https://arxiv.org/abs/2402.07282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07282">https://arxiv.org/pdf/2402.07282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07282]] How do Large Language Models Navigate Conflicts between Honesty and  Helpfulness?(https://arxiv.org/abs/2402.07282)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.</li>
</ul>

<h3>Title: Power Transformer Fault Prediction Based on Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Chao Wang, Zhuo Chen, Ziyan Zhang, Chiyi Li, Kai Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07283">https://arxiv.org/abs/2402.07283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07283">https://arxiv.org/pdf/2402.07283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07283]] Power Transformer Fault Prediction Based on Knowledge Graphs(https://arxiv.org/abs/2402.07283)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we address the challenge of learning with limited fault data for power transformers. Traditional operation and maintenance tools lack effective predictive capabilities for potential faults. The scarcity of extensive fault data makes it difficult to apply machine learning techniques effectively. To solve this problem, we propose a novel approach that leverages the knowledge graph (KG) technology in combination with gradient boosting decision trees (GBDT). This method is designed to efficiently learn from a small set of high-dimensional data, integrating various factors influencing transformer faults and historical operational data. Our approach enables accurate safe state assessments and fault analyses of power transformers despite the limited fault characteristic data. Experimental results demonstrate that this method outperforms other learning approaches in prediction accuracy, such as artificial neural networks (ANN) and logistic regression (LR). Furthermore, it offers significant improvements in progressiveness, practicality, and potential for widespread application.</li>
</ul>

<h3>Title: Training Heterogeneous Client Models using Knowledge Distillation in  Serverless Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Mohak Chadha, Pulkit Khera, Jianfeng Gu, Osama Abboud, Michael Gerndt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07295">https://arxiv.org/abs/2402.07295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07295">https://arxiv.org/pdf/2402.07295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07295]] Training Heterogeneous Client Models using Knowledge Distillation in  Serverless Federated Learning(https://arxiv.org/abs/2402.07295)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized. Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders. However, existing serverless FL systems implicitly assume a uniform global model architecture across all participating clients during training. This assumption fails to address fundamental challenges in practical FL due to the resource and statistical data heterogeneity among FL clients. To address these challenges and enable heterogeneous client models in serverless FL, we utilize Knowledge Distillation (KD) in this paper. Towards this, we propose novel optimized serverless workflows for two popular conventional federated KD techniques, i.e., FedMD and FedDF. We implement these workflows by introducing several extensions to an open-source serverless FL system called FedLess. Moreover, we comprehensively evaluate the two strategies on multiple datasets across varying levels of client data heterogeneity using heterogeneous client models with respect to accuracy, fine-grained training times, and costs. Results from our experiments demonstrate that serverless FedDF is more robust to extreme non-IID data distributions, is faster, and leads to lower costs than serverless FedMD. In addition, compared to the original implementation, our optimizations for particular steps in FedMD and FedDF lead to an average speedup of 3.5x and 1.76x across all datasets.</li>
</ul>

<h3>Title: Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Bilal Chughtai, Alan Cooney, Neel Nanda</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07321">https://arxiv.org/abs/2402.07321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07321">https://arxiv.org/pdf/2402.07321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07321]] Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs(https://arxiv.org/abs/2402.07321)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>How do transformer-based large language models (LLMs) store and retrieve knowledge? We focus on the most basic form of this task -- factual recall, where the model is tasked with explicitly surfacing stored facts in prompts of form `Fact: The Colosseum is in the country of'. We find that the mechanistic story behind factual recall is more complex than previously thought. It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct attribute. We term this generic phenomena the additive motif: models compute through summing up multiple independent contributions. Each mechanism's contribution may be insufficient alone, but summing results in constructive interfere on the correct answer. In addition, we extend the method of direct logit attribution to attribute an attention head's output to individual source tokens. We use this technique to unpack what we call `mixed heads' -- which are themselves a pair of two separate additive updates from different source tokens.</li>
</ul>

<h3>Title: The Bias of Harmful Label Associations in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Caner Hazirbas, Alicia Sun, Yonathan Efroni, Mark Ibrahim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07329">https://arxiv.org/abs/2402.07329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07329">https://arxiv.org/pdf/2402.07329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07329]] The Bias of Harmful Label Associations in Vision-Language Models(https://arxiv.org/abs/2402.07329)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer</a></li>
<li><strong>Abstract: </strong>Despite the remarkable performance of foundation vision-language models, the shared representation space for text and vision can also encode harmful label associations detrimental to fairness. While prior work has uncovered bias in vision-language models' (VLMs) classification performance across geography, work has been limited along the important axis of harmful label associations due to a lack of rich, labeled data. In this work, we investigate harmful label associations in the recently released Casual Conversations datasets containing more than 70,000 videos. We study bias in the frequency of harmful label associations across self-provided labels for age, gender, apparent skin tone, and physical adornments across several leading VLMs. We find that VLMs are $4-13$x more likely to harmfully classify individuals with darker skin tones. We also find scaling transformer encoder model size leads to higher confidence in harmful predictions. Finally, we find improvements on standard vision tasks across VLMs does not address disparities in harmful label associations.</li>
</ul>

<h3>Title: Deep Learning for Medical Image Segmentation with Imprecise Annotation</h3>
<ul>
<li><strong>Authors: </strong>Binyan Hu, A. K. Qin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07330">https://arxiv.org/abs/2402.07330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07330">https://arxiv.org/pdf/2402.07330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07330]] Deep Learning for Medical Image Segmentation with Imprecise Annotation(https://arxiv.org/abs/2402.07330)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation (MIS) plays an instrumental role in medical image analysis, where considerable efforts have been devoted to automating the process. Currently, mainstream MIS approaches are based on deep neural networks (DNNs) which are typically trained on a dataset that contains annotation masks produced by doctors. However, in the medical domain, the annotation masks generated by different doctors can inherently vary because a doctor may unnecessarily produce precise and unique annotations to meet the goal of diagnosis. Therefore, the DNN model trained on the data annotated by certain doctors, often just a single doctor, could undesirably favour those doctors who annotate the training data, leading to the unsatisfaction of a new doctor who will use the trained model. To address this issue, this work investigates the utilization of multi-expert annotation to enhance the adaptability of the model to a new doctor and we conduct a pilot study on the MRI brain segmentation task. Experimental results demonstrate that the model trained on a dataset with multi-expert annotation can efficiently cater for a new doctor, after lightweight fine-tuning on just a few annotations from the new doctor.</li>
</ul>

<h3>Title: Differentially Private Training of Mixture of Experts Models</h3>
<ul>
<li><strong>Authors: </strong>Pierre Tholoniat, Huseyin A. Inan, Janardhan Kulkarni, Robert Sim</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07334">https://arxiv.org/abs/2402.07334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07334">https://arxiv.org/pdf/2402.07334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07334]] Differentially Private Training of Mixture of Experts Models(https://arxiv.org/abs/2402.07334)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>This position paper investigates the integration of Differential Privacy (DP) in the training of Mixture of Experts (MoE) models within the field of natural language processing. As Large Language Models (LLMs) scale to billions of parameters, leveraging expansive datasets, they exhibit enhanced linguistic capabilities and emergent abilities. However, this growth raises significant computational and privacy concerns. Our study addresses these issues by exploring the potential of MoE models, known for their computational efficiency, and the application of DP, a standard for privacy preservation. We present the first known attempt to train MoE models under the constraints of DP, addressing the unique challenges posed by their architecture and the complexities of DP integration. Our initial experimental studies demonstrate that MoE models can be effectively trained with DP, achieving performance that is competitive with their non-private counterparts. This initial study aims to provide valuable insights and ignite further research in the domain of privacy-preserving MoE models, softly laying the groundwork for prospective developments in this evolving field.</li>
</ul>

<h3>Title: Accuracy of TextFooler black box adversarial attacks on 01 loss sign  activation neural network ensemble</h3>
<ul>
<li><strong>Authors: </strong>Yunzhe Xue, Usman Roshan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07347">https://arxiv.org/abs/2402.07347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07347">https://arxiv.org/pdf/2402.07347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07347]] Accuracy of TextFooler black box adversarial attacks on 01 loss sign  activation neural network ensemble(https://arxiv.org/abs/2402.07347)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Recent work has shown the defense of 01 loss sign activation neural networks against image classification adversarial attacks. A public challenge to attack the models on CIFAR10 dataset remains undefeated. We ask the following question in this study: are 01 loss sign activation neural networks hard to deceive with a popular black box text adversarial attack program called TextFooler? We study this question on four popular text classification datasets: IMDB reviews, Yelp reviews, MR sentiment classification, and AG news classification. We find that our 01 loss sign activation network is much harder to attack with TextFooler compared to sigmoid activation cross entropy and binary neural networks. We also study a 01 loss sign activation convolutional neural network with a novel global pooling step specific to sign activation networks. With this new variation we see a significant gain in adversarial accuracy rendering TextFooler practically useless against it. We make our code freely available at \url{https://github.com/zero-one-loss/wordcnn01} and \url{https://github.com/xyzacademic/mlp01example}. Our work here suggests that 01 loss sign activation networks could be further developed to create fool proof models against text adversarial attacks.</li>
</ul>

<h3>Title: Bayesian Federated Learning Via Expectation Maximization and Turbo Deep  Approximate Message Passing</h3>
<ul>
<li><strong>Authors: </strong>Wei Xu, An Liu, Yiting Zhang, Vincent Lau</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07366">https://arxiv.org/abs/2402.07366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07366">https://arxiv.org/pdf/2402.07366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07366]] Bayesian Federated Learning Via Expectation Maximization and Turbo Deep  Approximate Message Passing(https://arxiv.org/abs/2402.07366)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a machine learning paradigm where the clients possess decentralized training data and the central server handles aggregation and scheduling. Typically, FL algorithms involve clients training their local models using stochastic gradient descent (SGD), which carries drawbacks such as slow convergence and being prone to getting stuck in suboptimal solutions. In this work, we propose a message passing based Bayesian federated learning (BFL) framework to avoid these drawbacks.Specifically, we formulate the problem of deep neural network (DNN) learning and compression and as a sparse Bayesian inference problem, in which group sparse prior is employed to achieve structured model compression. Then, we propose an efficient BFL algorithm called EMTDAMP, where expectation maximization (EM) and turbo deep approximate message passing (TDAMP) are combined to achieve distributed learning and compression. The central server aggregates local posterior distributions to update global posterior distributions and update hyperparameters based on EM to accelerate convergence. The clients perform TDAMP to achieve efficient approximate message passing over DNN with joint prior distribution. We detail the application of EMTDAMP to Boston housing price prediction and handwriting recognition, and present extensive numerical results to demonstrate the advantages of EMTDAMP.</li>
</ul>

<h3>Title: Utilizing Large LanguageModels to Detect Privacy Leaks in Mini-App Code</h3>
<ul>
<li><strong>Authors: </strong>Liming Jiang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07367">https://arxiv.org/abs/2402.07367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07367">https://arxiv.org/pdf/2402.07367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07367]] Utilizing Large LanguageModels to Detect Privacy Leaks in Mini-App Code(https://arxiv.org/abs/2402.07367)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, large language model</a></li>
<li><strong>Abstract: </strong>Mini-applications, commonly referred to as mini-apps, are compact software programs embedded within larger applications or platforms, offering targeted functionality without the need for separate installations. Typically web-based or cloud-hosted, these mini-apps streamline user experiences by providing focused services accessible through web browsers or mobile apps. Their simplicity, speed, and integration capabilities make them valuable additions to messaging platforms, social media networks, e-commerce sites, and various digital environments. WeChat Mini Programs, a prominent feature of China's leading messaging app, exemplify this trend, offering users a seamless array of services without additional downloads. Leveraging WeChat's extensive user base and payment infrastructure, Mini Programs facilitate efficient transactions and bridge online and offline experiences, shaping China's digital landscape significantly. This paper investigates the potential of employing Large Language Models (LLMs) to detect privacy breaches within WeChat Mini Programs. Given the widespread use of Mini Programs and growing concerns about data privacy, this research seeks to determine if LLMs can effectively identify instances of privacy leakage within this ecosystem. Through meticulous analysis and experimentation, we aim to highlight the efficacy of LLMs in safeguarding user privacy and security within the WeChat Mini Program environment, thereby contributing to a more secure digital landscape.</li>
</ul>

<h3>Title: Assessing Generalization for Subpopulation Representative Modeling via  In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Simmons, Vladislav Savinov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07368">https://arxiv.org/abs/2402.07368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07368">https://arxiv.org/pdf/2402.07368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07368]] Assessing Generalization for Subpopulation Representative Modeling via  In-Context Learning(https://arxiv.org/abs/2402.07368)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study evaluates the ability of Large Language Model (LLM)-based Subpopulation Representative Models (SRMs) to generalize from empirical data, utilizing in-context learning with data from the 2016 and 2020 American National Election Studies. We explore generalization across response variables and demographic subgroups. While conditioning with empirical data improves performance on the whole, the benefit of in-context learning varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others. The inequitable benefits of in-context learning for SRM present a challenge for practitioners implementing SRMs, and for decision-makers who might come to rely on them. Our work highlights a need for fine-grained benchmarks captured from diverse subpopulations that test not only fidelity but generalization.</li>
</ul>

<h3>Title: Diff-RNTraj: A Structure-aware Diffusion Model for Road  Network-constrained Trajectory Generation</h3>
<ul>
<li><strong>Authors: </strong>Tonglong Wei, Youfang Lin, Shengnan Guo, Yan Lin, Yiheng Huang, Chenyang Xiang, Yuqing Bai, Menglu Ya, Huaiyu Wan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07369">https://arxiv.org/abs/2402.07369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07369">https://arxiv.org/pdf/2402.07369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07369]] Diff-RNTraj: A Structure-aware Diffusion Model for Road  Network-constrained Trajectory Generation(https://arxiv.org/abs/2402.07369)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>Trajectory data is essential for various applications as it records the movement of vehicles. However, publicly available trajectory datasets remain limited in scale due to privacy concerns, which hinders the development of trajectory data mining and trajectory-based applications. To address this issue, some methods for generating synthetic trajectories have been proposed to expand the scale of the dataset. However, all existing methods generate trajectories in the geographical coordinate system, which poses two limitations for their utilization in practical applications: 1) the inability to ensure that the generated trajectories are constrained on the road. 2) the lack of road-related information. In this paper, we propose a new problem to meet the practical application need, \emph{i.e.}, road network-constrained trajectory (RNTraj) generation, which can directly generate trajectories on the road network with road-related information. RNTraj is a hybrid type of data, in which each point is represented by a discrete road segment and a continuous moving rate. To generate RNTraj, we design a diffusion model called Diff-RNTraj. This model can effectively handle the hybrid RNTraj using a continuous diffusion framework by incorporating a pre-training strategy to embed hybrid RNTraj into continuous representations. During the sampling stage, a RNTraj decoder is designed to map the continuous representation generated by the diffusion model back to the hybrid RNTraj format. Furthermore, Diff-RNTraj introduces a novel loss function to enhance the spatial validity of the generated trajectories. Extensive experiments conducted on two real-world trajectory datasets demonstrate the effectiveness of the proposed model.</li>
</ul>

<h3>Title: Unsupervised Discovery of Object-Centric Neural Fields</h3>
<ul>
<li><strong>Authors: </strong>Rundong Luo, Hong-Xing Yu, Jiajun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07376">https://arxiv.org/abs/2402.07376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07376">https://arxiv.org/pdf/2402.07376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07376]] Unsupervised Discovery of Object-Centric Neural Fields(https://arxiv.org/abs/2402.07376)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, segmentation</a></li>
<li><strong>Abstract: </strong>We study inferring 3D object-centric scene representations from a single image. While recent methods have shown potential in unsupervised 3D object discovery from simple synthetic images, they fail to generalize to real-world scenes with visually rich and diverse objects. This limitation stems from their object representations, which entangle objects' intrinsic attributes like shape and appearance with extrinsic, viewer-centric properties such as their 3D location. To address this bottleneck, we propose Unsupervised discovery of Object-Centric neural Fields (uOCF). uOCF focuses on learning the intrinsics of objects and models the extrinsics separately. Our approach significantly improves systematic generalization, thus enabling unsupervised learning of high-fidelity object-centric scene representations from sparse real-world images. To evaluate our approach, we collect three new datasets, including two real kitchen environments. Extensive experiments show that uOCF enables unsupervised discovery of visually rich objects from a single real image, allowing applications such as 3D object segmentation and scene manipulation. Notably, uOCF demonstrates zero-shot generalization to unseen objects from a single real image. Project page: https://red-fairy.github.io/uOCF/</li>
</ul>

<h3>Title: Exploring Perceptual Limitation of Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiarui Zhang, Jinyi Hu, Mahyar Khayatkhoei, Filip Ilievski, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07384">https://arxiv.org/abs/2402.07384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07384">https://arxiv.org/pdf/2402.07384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07384]] Exploring Perceptual Limitation of Multimodal Large Language Models(https://arxiv.org/abs/2402.07384)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have recently shown remarkable perceptual capability in answering visual questions, however, little is known about the limits of their perception. In particular, while prior works have provided anecdotal evidence of MLLMs' sensitivity to object size, this phenomenon and its underlying causes have not been explored comprehensively. In this work, we quantitatively study the perception of small visual objects in several state-of-the-art MLLMs and reveal a pervasive limitation in answering questions about small objects in images. Next, we identify four independent factors that can contribute to this limitation -- object quality, size, distractors, and location -- and conduct controlled intervention studies to measure the effect of each factor on MLLMs' perception. In particular, we find that lower object quality and smaller object size can both independently reduce MLLMs' ability to answer visual questions. More surprisingly, we find that the location of the object in the image and the presence of visual distractors can also significantly reduce MLLMs' question answering accuracy. Our study provides a better understanding of the perceptual limitation of MLLMs and contributes new evaluation protocols for analyzing the perception of future MLLMs. To facilitate further investigations, we release our code and data.</li>
</ul>

<h3>Title: Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy  Induction from Limited Examples</h3>
<ul>
<li><strong>Authors: </strong>Qingkai Zeng, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Zhenwen Liang, Zhihan Zhang, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07386">https://arxiv.org/abs/2402.07386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07386">https://arxiv.org/pdf/2402.07386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07386]] Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy  Induction from Limited Examples(https://arxiv.org/abs/2402.07386)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic taxonomy induction is crucial for web search, recommendation systems, and question answering. Manual curation of taxonomies is expensive in terms of human effort, making automatic taxonomy construction highly desirable. In this work, we introduce Chain-of-Layer which is an in-context learning framework designed to induct taxonomies from a given set of entities. Chain-of-Layer breaks down the task into selecting relevant candidate entities in each layer and gradually building the taxonomy from top to bottom. To minimize errors, we introduce the Ensemble-based Ranking Filter to reduce the hallucinated content generated at each iteration. Through extensive experiments, we demonstrate that Chain-of-Layer achieves state-of-the-art performance on four real-world benchmarks.</li>
</ul>

<h3>Title: Can LLMs Produce Faithful Explanations For Fact-checking? Towards  Faithful Explainable Fact-Checking via Multi-Agent Debate</h3>
<ul>
<li><strong>Authors: </strong>Kyungha Kim, Sangyun Lee, Kung-Hsiang Huang, Hou Pong Chan, Manling Li, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07401">https://arxiv.org/abs/2402.07401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07401">https://arxiv.org/pdf/2402.07401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07401]] Can LLMs Produce Faithful Explanations For Fact-checking? Towards  Faithful Explainable Fact-Checking via Multi-Agent Debate(https://arxiv.org/abs/2402.07401)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fact-checking research has extensively explored verification but less so the generation of natural-language explanations, crucial for user trust. While Large Language Models (LLMs) excel in text generation, their capability for producing faithful explanations in fact-checking remains underexamined. Our study investigates LLMs' ability to generate such explanations, finding that zero-shot prompts often result in unfaithfulness. To address these challenges, we propose the Multi-Agent Debate Refinement (MADR) framework, leveraging multiple LLMs as agents with diverse roles in an iterative refining process aimed at enhancing faithfulness in generated explanations. MADR ensures that the final explanation undergoes rigorous validation, significantly reducing the likelihood of unfaithful elements and aligning closely with the provided evidence. Experimental results demonstrate that MADR significantly improves the faithfulness of LLM-generated explanations to the evidence, advancing the credibility and trustworthiness of these explanations.</li>
</ul>

<h3>Title: Make it more specific: A novel uncertainty based airway segmentation  application on 3D U-Net and its variants</h3>
<ul>
<li><strong>Authors: </strong>Shiyi Wang, Yang Nan, Felder Federico N, Sheng Zhang, Walsh Simon L F, Guang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07403">https://arxiv.org/abs/2402.07403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07403">https://arxiv.org/pdf/2402.07403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07403]] Make it more specific: A novel uncertainty based airway segmentation  application on 3D U-Net and its variants(https://arxiv.org/abs/2402.07403)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Each medical segmentation task should be considered with a specific AI algorithm based on its scenario so that the most accurate prediction model can be obtained. The most popular algorithms in medical segmentation, 3D U-Net and its variants, can directly implement the task of lung trachea segmentation, but its failure to consider the special tree-like structure of the trachea suggests that there is much room for improvement in its segmentation accuracy. Therefore, a research gap exists because a great amount of state-of-the-art DL algorithms are vanilla 3D U-Net structures, which do not introduce the various performance-enhancing modules that come with special natural image modality in lung airway segmentation. In this paper, we proposed two different network structures Branch-Level U-Net (B-UNet) and Branch-Level CE-UNet (B-CE-UNet) which are based on U-Net structure and compared the prediction results with the same dataset. Specially, both of the two networks add branch loss and central line loss to learn the feature of fine branch endings of the airways. Uncertainty estimation algorithms are also included to attain confident predictions and thereby, increase the overall trustworthiness of our whole model. In addition, predictions of the lung trachea based on the maximum connectivity rate were calculated and extracted during post-processing for segmentation refinement and pruning.</li>
</ul>

<h3>Title: Dólares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs  Between Spanish and English</h3>
<ul>
<li><strong>Authors: </strong>Xiao Zhang, Ruoyu Xiang, Chenhan Yuan, Duanyu Feng, Weiguang Han, Alejandro Lopez-Lira, Xiao-Yang Liu, Sophia Ananiadou, Min Peng, Jimin Huang, Qianqian Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07405">https://arxiv.org/abs/2402.07405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07405">https://arxiv.org/pdf/2402.07405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07405]] Dólares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs  Between Spanish and English(https://arxiv.org/abs/2402.07405)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite Spanish's pivotal role in the global finance industry, a pronounced gap exists in Spanish financial natural language processing (NLP) and application studies compared to English, especially in the era of large language models (LLMs). To bridge this gap, we unveil Tois\'on de Oro, the first bilingual framework that establishes instruction datasets, finetuned LLMs, and evaluation benchmark for financial LLMs in Spanish joint with English. We construct a rigorously curated bilingual instruction dataset including over 144K Spanish and English samples from 15 datasets covering 7 tasks. Harnessing this, we introduce FinMA-ES, an LLM designed for bilingual financial applications. We evaluate our model and existing LLMs using FLARE-ES, the first comprehensive bilingual evaluation benchmark with 21 datasets covering 9 tasks. The FLARE-ES benchmark results reveal a significant multilingual performance gap and bias in existing LLMs. FinMA-ES models surpass SOTA LLMs such as GPT-4 in Spanish financial tasks, due to strategic instruction tuning and leveraging data from diverse linguistic resources, highlighting the positive impact of cross-linguistic transfer. All our datasets, models, and benchmarks have been released.</li>
</ul>

<h3>Title: Large Language Models are Few-shot Generators: Proposing Hybrid Prompt  Algorithm To Generate Webshell Escape Samples</h3>
<ul>
<li><strong>Authors: </strong>Mingrui Ma, Lansheng Han, Chunjie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07408">https://arxiv.org/abs/2402.07408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07408">https://arxiv.org/pdf/2402.07408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07408]] Large Language Models are Few-shot Generators: Proposing Hybrid Prompt  Algorithm To Generate Webshell Escape Samples(https://arxiv.org/abs/2402.07408)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>The frequent occurrence of cyber-attacks has made webshell attacks and defense gradually become a research hotspot in the field of network security. However, the lack of publicly available benchmark datasets and the over-reliance on manually defined rules for webshell escape sample generation have slowed down the progress of research related to webshell escape sample generation strategies and artificial intelligence-based webshell detection algorithms. To address the drawbacks of weak webshell sample escape capabilities, the lack of webshell datasets with complex malicious features, and to promote the development of webshell detection technology, we propose the Hybrid Prompt algorithm for webshell escape sample generation with the help of large language models. As a prompt algorithm specifically developed for webshell sample generation, the Hybrid Prompt algorithm not only combines various prompt ideas including Chain of Thought, Tree of Thought, but also incorporates various components such as webshell hierarchical module and few-shot example to facilitate the LLM in learning and reasoning webshell escape strategies. Experimental results show that the Hybrid Prompt algorithm can work with multiple LLMs with excellent code reasoning ability to generate high-quality webshell samples with high Escape Rate (88.61% with GPT-4 model on VIRUSTOTAL detection engine) and Survival Rate (54.98% with GPT-4 model).</li>
</ul>

<h3>Title: A Closer Look at the Robustness of Contrastive Language-Image  Pre-Training (CLIP)</h3>
<ul>
<li><strong>Authors: </strong>Weijie Tu, Weijian Deng, Tom Gedeon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07410">https://arxiv.org/abs/2402.07410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07410">https://arxiv.org/pdf/2402.07410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07410]] A Closer Look at the Robustness of Contrastive Language-Image  Pre-Training (CLIP)(https://arxiv.org/abs/2402.07410)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable generalization capabilities across multiple challenging distribution shifts. However, there is still much to be explored in terms of their robustness to the variations of specific visual factors. In real-world applications, reliable and safe systems must consider other safety objectives beyond classification accuracy, such as predictive uncertainty. Yet, the effectiveness of CLIP models on such safety-related features is less-explored. Driven by the above, this work comprehensively investigates the safety objectives of CLIP models, specifically focusing on three key properties: resilience to visual factor variations, calibrated uncertainty estimations, and the ability to detect anomalous inputs. To this end, we study 83 CLIP models and 127 ImageNet classifiers. They are diverse in architecture, (pre)training distribution and training strategies. We consider 10 visual factors (e.g., shape and pattern), 5 types of out-of-distribution data, and 8 natural and challenging test conditions with different shift types, such as texture, style, and perturbation shifts. Our study has unveiled several previously unknown insights into CLIP models. For instance, they are not consistently more calibrated than other ImageNet models, which contradicts existing findings. Additionally, our analysis underscores the significance of training source design by showcasing its profound influence on the three safety-related properties. We believe our comprehensive study can shed light on and help guide the development of more robust and reliable CLIP models.</li>
</ul>

<h3>Title: Conditional Generative Models are Sufficient to Sample from Any Causal  Effect Estimand</h3>
<ul>
<li><strong>Authors: </strong>Md Musfiqur Rahman, Matt Jordan, Murat Kocaoglu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07419">https://arxiv.org/abs/2402.07419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07419">https://arxiv.org/pdf/2402.07419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07419]] Conditional Generative Models are Sufficient to Sample from Any Causal  Effect Estimand(https://arxiv.org/abs/2402.07419)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Causal inference from observational data has recently found many applications in machine learning. While sound and complete algorithms exist to compute causal effects, many of these algorithms require explicit access to conditional likelihoods over the observational distribution, which is difficult to estimate in the high-dimensional regime, such as with images. To alleviate this issue, researchers have approached the problem by simulating causal relations with neural models and obtained impressive results. However, none of these existing approaches can be applied to generic scenarios such as causal graphs on image data with latent confounders, or obtain conditional interventional samples. In this paper, we show that any identifiable causal effect given an arbitrary causal graph can be computed through push-forward computations of conditional generative models. Based on this result, we devise a diffusion-based approach to sample from any (conditional) interventional distribution on image data. To showcase our algorithm's performance, we conduct experiments on a Colored MNIST dataset having both the treatment ($X$) and the target variables ($Y$) as images and obtain interventional samples from $P(y|do(x))$. As an application of our algorithm, we evaluate two large conditional generative models that are pre-trained on the CelebA dataset by analyzing the strength of spurious correlations and the level of disentanglement they achieve.</li>
</ul>

<h3>Title: SALAD: Smart AI Language Assistant Daily</h3>
<ul>
<li><strong>Authors: </strong>Ragib Amin Nihal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07431">https://arxiv.org/abs/2402.07431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07431">https://arxiv.org/pdf/2402.07431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07431]] SALAD: Smart AI Language Assistant Daily(https://arxiv.org/abs/2402.07431)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>SALAD is an AI-driven language-learning application designed to help foreigners learn Japanese. It offers translations in Kanji-Kana-Romaji, speech recognition, translated audio, vocabulary tracking, grammar explanations, and songs generated from newly learned words. The app targets beginners and intermediate learners, aiming to make language acquisition more accessible and enjoyable. SALAD uses daily translations to enhance fluency and comfort in communication with native speakers. The primary objectives include effective Japanese language learning, user engagement, and progress tracking. A survey by us found that 39% of foreigners in Japan face discomfort in conversations with Japanese speakers. Over 60% of foreigners expressed confidence in SALAD's ability to enhance their Japanese language skills. The app uses large language models, speech recognition, and diffusion models to bridge the language gap and foster a more inclusive community in Japan.</li>
</ul>

<h3>Title: The I/O Complexity of Attention, or How Optimal is Flash Attention?</h3>
<ul>
<li><strong>Authors: </strong>Barna Saha, Christopher Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CC, cs.DS, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07443">https://arxiv.org/abs/2402.07443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07443">https://arxiv.org/pdf/2402.07443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07443]] The I/O Complexity of Attention, or How Optimal is Flash Attention?(https://arxiv.org/abs/2402.07443)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Self-attention is at the heart of the popular Transformer architecture, yet suffers from quadratic time and memory complexity. The breakthrough FlashAttention algorithm revealed I/O complexity as the true bottleneck in scaling Transformers. Given two levels of memory hierarchy, a fast cache (e.g. GPU on-chip SRAM) and a slow memory (e.g. GPU high-bandwidth memory), the I/O complexity measures the number of accesses to memory. FlashAttention computes attention using $\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of the attention matrix, $d$ the head-dimension and $M$ the cache size. However, is this I/O complexity optimal? The known lower bound only rules out an I/O complexity of $o(Nd)$ when $M=\Theta(Nd)$, since the output that needs to be written to slow memory is $\Omega(Nd)$. This leads to the main question of our work: Is FlashAttention I/O optimal for all values of $M$? We resolve the above question in its full generality by showing an I/O complexity lower bound that matches the upper bound provided by FlashAttention for any values of $M \geq d^2$ within any constant factors. Further, we give a better algorithm with lower I/O complexity for $M < d^2$, and show that it is optimal as well. Moreover, our lower bounds do not rely on using combinatorial matrix multiplication for computing the attention matrix. We show even if one uses fast matrix multiplication, the above I/O complexity bounds cannot be improved. We do so by introducing a new communication complexity protocol for matrix compression, and connecting communication complexity to I/O complexity. To the best of our knowledge, this is the first work to establish a connection between communication complexity and I/O complexity, and we believe this connection could be of independent interest and will find many more applications in proving I/O complexity lower bounds in the future.</li>
</ul>

<h3>Title: Malicious Package Detection using Metadata Information</h3>
<ul>
<li><strong>Authors: </strong>S. Halder, M. Bewong, A. Mahboubi, Y. Jiang, R. Islam, Z. Islam, R. Ip, E. Ahmed, G. Ramachandran, A. Babar</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07444">https://arxiv.org/abs/2402.07444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07444">https://arxiv.org/pdf/2402.07444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07444]] Malicious Package Detection using Metadata Information(https://arxiv.org/abs/2402.07444)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust</a></li>
<li><strong>Abstract: </strong>Protecting software supply chains from malicious packages is paramount in the evolving landscape of software development. Attacks on the software supply chain involve attackers injecting harmful software into commonly used packages or libraries in a software repository. For instance, JavaScript uses Node Package Manager (NPM), and Python uses Python Package Index (PyPi) as their respective package repositories. In the past, NPM has had vulnerabilities such as the event-stream incident, where a malicious package was introduced into a popular NPM package, potentially impacting a wide range of projects. As the integration of third-party packages becomes increasingly ubiquitous in modern software development, accelerating the creation and deployment of applications, the need for a robust detection mechanism has become critical. On the other hand, due to the sheer volume of new packages being released daily, the task of identifying malicious packages presents a significant challenge. To address this issue, in this paper, we introduce a metadata-based malicious package detection model, MeMPtec. This model extracts a set of features from package metadata information. These extracted features are classified as either easy-to-manipulate (ETM) or difficult-to-manipulate (DTM) features based on monotonicity and restricted control properties. By utilising these metadata features, not only do we improve the effectiveness of detecting malicious packages, but also we demonstrate its resistance to adversarial attacks in comparison with existing state-of-the-art. Our experiments indicate a significant reduction in both false positives (up to 97.56%) and false negatives (up to 91.86%).</li>
</ul>

<h3>Title: TriAug: Out-of-Distribution Detection for Robust Classification of  Imbalanced Breast Lesion in Ultrasound</h3>
<ul>
<li><strong>Authors: </strong>Yinyu Ye, Shijing Chen, Dong Ni, Ruobing Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07452">https://arxiv.org/abs/2402.07452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07452">https://arxiv.org/pdf/2402.07452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07452]] TriAug: Out-of-Distribution Detection for Robust Classification of  Imbalanced Breast Lesion in Ultrasound(https://arxiv.org/abs/2402.07452)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Different diseases, such as histological subtypes of breast lesions, have severely varying incidence rates. Even trained with substantial amount of in-distribution (ID) data, models often encounter out-of-distribution (OOD) samples belonging to unseen classes in clinical reality. To address this, we propose a novel framework built upon a long-tailed OOD detection task for breast ultrasound images. It is equipped with a triplet state augmentation (TriAug) which improves ID classification accuracy while maintaining a promising OOD detection performance. Meanwhile, we designed a balanced sphere loss to handle the class imbalanced problem.</li>
</ul>

<h3>Title: Pushing The Limit of LLM Capacity for Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Yazhou Zhang, Mengyao Wang, Chenyu Ren, Qiuchi Li, Prayag Tiwari, Benyou Wang, Jing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07470">https://arxiv.org/abs/2402.07470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07470">https://arxiv.org/pdf/2402.07470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07470]] Pushing The Limit of LLM Capacity for Text Classification(https://arxiv.org/abs/2402.07470)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The value of text classification's future research has encountered challenges and uncertainties, due to the extraordinary efficacy demonstrated by large language models (LLMs) across numerous downstream NLP tasks. In this era of open-ended language modeling, where task boundaries are gradually fading, an urgent question emerges: have we made significant advances in text classification under the full benefit of LLMs? To answer this question, we propose RGPT, an adaptive boosting framework tailored to produce a specialized text classification LLM by recurrently ensembling a pool of strong base learners. The base learners are constructed by adaptively adjusting the distribution of training samples and iteratively fine-tuning LLMs with them. Such base learners are then ensembled to be a specialized text classification LLM, by recurrently incorporating the historical predictions from the previous learners. Through a comprehensive empirical comparison, we show that RGPT significantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by 1.36% on average. Further evaluation experiments show a clear surpassing of RGPT over human classification.</li>
</ul>

<h3>Title: Differentially Private Decentralized Learning with Random Walks</h3>
<ul>
<li><strong>Authors: </strong>Edwige Cyffers, Aurélien Bellet, Jalaj Upadhyay</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07471">https://arxiv.org/abs/2402.07471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07471">https://arxiv.org/pdf/2402.07471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07471]] Differentially Private Decentralized Learning with Random Walks(https://arxiv.org/abs/2402.07471)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>The popularity of federated learning comes from the possibility of better scalability and the ability for participants to keep control of their data, improving data security and sovereignty. Unfortunately, sharing model updates also creates a new privacy attack surface. In this work, we characterize the privacy guarantees of decentralized learning with random walk algorithms, where a model is updated by traveling from one node to another along the edges of a communication graph. Using a recent variant of differential privacy tailored to the study of decentralized algorithms, namely Pairwise Network Differential Privacy, we derive closed-form expressions for the privacy loss between each pair of nodes where the impact of the communication topology is captured by graph theoretic quantities. Our results further reveal that random walk algorithms tends to yield better privacy guarantees than gossip algorithms for nodes close from each other. We supplement our theoretical results with empirical evaluation on synthetic and real-world graphs and datasets.</li>
</ul>

<h3>Title: Topological Safeguard for Evasion Attack based on the Interpretability  of Artificial Neural Network Behavior</h3>
<ul>
<li><strong>Authors: </strong>Xabier Echeberria-Barrio, Amaia Gil-Lerchundi, Iñigo Mendialdua, Raul Orduna-Urrutia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07480">https://arxiv.org/abs/2402.07480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07480">https://arxiv.org/pdf/2402.07480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07480]] Topological Safeguard for Evasion Attack based on the Interpretability  of Artificial Neural Network Behavior(https://arxiv.org/abs/2402.07480)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, interpretability</a></li>
<li><strong>Abstract: </strong>In the last years, Deep Learning technology has been proposed in different fields, bringing many advances in each of them, but identifying new threats in these solutions regarding cybersecurity. Those implemented models have brought several vulnerabilities associated with Deep Learning technology. Moreover, those allow taking advantage of the implemented model, obtaining private information, and even modifying the model's decision-making. Therefore, interest in studying those vulnerabilities/attacks and designing defenses to avoid or fight them is gaining prominence among researchers. In particular, the widely known evasion attack is being analyzed by researchers; thus, several defenses to avoid such a threat can be found in the literature. Since the presentation of the L-BFG algorithm, this threat concerns the research community. However, it continues developing new and ingenious countermeasures since there is no perfect defense for all the known evasion algorithms. In this work, a novel detector of evasion attacks is developed. It focuses on the information of the activations of the neurons given by the model when an input sample is injected. Moreover, it puts attention to the topology of the targeted deep learning model to analyze the activations according to which neurons are connecting. This approach has been decided because the literature shows that the targeted model's topology contains essential information about if the evasion attack occurs. For this purpose, a huge data preprocessing is required to introduce all this information in the detector, which uses the Graph Convolutional Neural Network (GCN) technology. Thus, it understands the topology of the target model, obtaining promising results and improving the outcomes presented in the literature related to similar defenses.</li>
</ul>

<h3>Title: Score-based Diffusion Models via Stochastic Differential Equations -- a  Technical Tutorial</h3>
<ul>
<li><strong>Authors: </strong>Wenpin Tang, Hanyang Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, math.HO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07487">https://arxiv.org/abs/2402.07487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07487">https://arxiv.org/pdf/2402.07487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07487]] Score-based Diffusion Models via Stochastic Differential Equations -- a  Technical Tutorial(https://arxiv.org/abs/2402.07487)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This is an expository article on the score-based diffusion models, with a particular focus on the formulation via stochastic differential equations (SDE). After a gentle introduction, we discuss the two pillars in the diffusion modeling -- sampling and score matching, which encompass the SDE/ODE sampling, score matching efficiency, the consistency model, and reinforcement learning. Short proofs are given to illustrate the main idea of the stated results. The article is primarily for introducing the beginners to the field, and practitioners may also find some analysis useful in designing new models or algorithms.</li>
</ul>

<h3>Title: Understanding Deep Learning defenses Against Adversarial Examples  Through Visualizations for Dynamic Risk Assessment</h3>
<ul>
<li><strong>Authors: </strong>Xabier Echeberria-Barrio, Amaia Gil-Lerchundi, Jon Egana-Zubia, Raul Orduna-Urrutia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07496">https://arxiv.org/abs/2402.07496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07496">https://arxiv.org/pdf/2402.07496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07496]] Understanding Deep Learning defenses Against Adversarial Examples  Through Visualizations for Dynamic Risk Assessment(https://arxiv.org/abs/2402.07496)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>In recent years, Deep Neural Network models have been developed in different fields, where they have brought many advances. However, they have also started to be used in tasks where risk is critical. A misdiagnosis of these models can lead to serious accidents or even death. This concern has led to an interest among researchers to study possible attacks on these models, discovering a long list of vulnerabilities, from which every model should be defended. The adversarial example attack is a widely known attack among researchers, who have developed several defenses to avoid such a threat. However, these defenses are as opaque as a deep neural network model, how they work is still unknown. This is why visualizing how they change the behavior of the target model is interesting in order to understand more precisely how the performance of the defended model is being modified. For this work, some defenses, against adversarial example attack, have been selected in order to visualize the behavior modification of each of them in the defended model. Adversarial training, dimensionality reduction and prediction similarity were the selected defenses, which have been developed using a model composed by convolution neural network layers and dense neural network layers. In each defense, the behavior of the original model has been compared with the behavior of the defended model, representing the target model by a graph in a visualization.</li>
</ul>

<h3>Title: Accelerated Smoothing: A Scalable Approach to Randomized Smoothing</h3>
<ul>
<li><strong>Authors: </strong>Devansh Bhardwaj, Kshitiz Kaushik, Sarthak Gupta</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07498">https://arxiv.org/abs/2402.07498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07498">https://arxiv.org/pdf/2402.07498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07498]] Accelerated Smoothing: A Scalable Approach to Randomized Smoothing(https://arxiv.org/abs/2402.07498)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Randomized smoothing has emerged as a potent certifiable defense against adversarial attacks by employing smoothing noises from specific distributions to ensure the robustness of a smoothed classifier. However, the utilization of Monte Carlo sampling in this process introduces a compute-intensive element, which constrains the practicality of randomized smoothing on a larger scale. To address this limitation, we propose a novel approach that replaces Monte Carlo sampling with the training of a surrogate neural network. Through extensive experimentation in various settings, we demonstrate the efficacy of our approach in approximating the smoothed classifier with remarkable precision. Furthermore, we demonstrate that our approach significantly accelerates the robust radius certification process, providing nearly $600$X improvement in computation time, overcoming the computational bottlenecks associated with traditional randomized smoothing.</li>
</ul>

<h3>Title: One Train for Two Tasks: An Encrypted Traffic Classification Framework  Using Supervised Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Haozhen Zhang, Xi Xiao, Le Yu, Qing Li, Zhen Ling, Ye Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07501">https://arxiv.org/abs/2402.07501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07501">https://arxiv.org/pdf/2402.07501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07501]] One Train for Two Tasks: An Encrypted Traffic Classification Framework  Using Supervised Contrastive Learning(https://arxiv.org/abs/2402.07501)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>As network security receives widespread attention, encrypted traffic classification has become the current research focus. However, existing methods conduct traffic classification without sufficiently considering the common characteristics between data samples, leading to suboptimal performance. Moreover, they train the packet-level and flow-level classification tasks independently, which is redundant because the packet representations learned in the packet-level task can be exploited by the flow-level task. Therefore, in this paper, we propose an effective model named a Contrastive Learning Enhanced Temporal Fusion Encoder (CLE-TFE). In particular, we utilize supervised contrastive learning to enhance the packet-level and flow-level representations and perform graph data augmentation on the byte-level traffic graph so that the fine-grained semantic-invariant characteristics between bytes can be captured through contrastive learning. We also propose cross-level multi-task learning, which simultaneously accomplishes the packet-level and flow-level classification tasks in the same model with one training. Further experiments show that CLE-TFE achieves the best overall performance on the two tasks, while its computational overhead (i.e., floating point operations, FLOPs) is only about 1/14 of the pre-trained model (e.g., ET-BERT). We release the code at https://github.com/ViktorAxelsen/CLE-TFE</li>
</ul>

<h3>Title: ClusterTabNet: Supervised clustering method for table detection and  table structure recognition</h3>
<ul>
<li><strong>Authors: </strong>Marek Polewczyk, Marco Spinaci</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07502">https://arxiv.org/abs/2402.07502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07502">https://arxiv.org/pdf/2402.07502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07502]] ClusterTabNet: Supervised clustering method for table detection and  table structure recognition(https://arxiv.org/abs/2402.07502)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present a novel deep-learning-based method to cluster words in documents which we apply to detect and recognize tables given the OCR output. We interpret table structure bottom-up as a graph of relations between pairs of words (belonging to the same row, column, header, as well as to the same table) and use a transformer encoder model to predict its adjacency matrix. We demonstrate the performance of our method on the PubTables-1M dataset as well as PubTabNet and FinTabNet datasets. Compared to the current state-of-the-art detection methods such as DETR and Faster R-CNN, our method achieves similar or better accuracy, while requiring a significantly smaller model.</li>
</ul>

<h3>Title: NeuralSentinel: Safeguarding Neural Network Reliability and  Trustworthiness</h3>
<ul>
<li><strong>Authors: </strong>Xabier Echeberria-Barrio, Mikel Gorricho, Selene Valencia, Francesco Zola</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07506">https://arxiv.org/abs/2402.07506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07506">https://arxiv.org/pdf/2402.07506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07506]] NeuralSentinel: Safeguarding Neural Network Reliability and  Trustworthiness(https://arxiv.org/abs/2402.07506)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, explainability</a></li>
<li><strong>Abstract: </strong>The usage of Artificial Intelligence (AI) systems has increased exponentially, thanks to their ability to reduce the amount of data to be analyzed, the user efforts and preserving a high rate of accuracy. However, introducing this new element in the loop has converted them into attacked points that can compromise the reliability of the systems. This new scenario has raised crucial challenges regarding the reliability and trustworthiness of the AI models, as well as about the uncertainties in their response decisions, becoming even more crucial when applied in critical domains such as healthcare, chemical, electrical plants, etc. To contain these issues, in this paper, we present NeuralSentinel (NS), a tool able to validate the reliability and trustworthiness of AI models. This tool combines attack and defence strategies and explainability concepts to stress an AI model and help non-expert staff increase their confidence in this new system by understanding the model decisions. NS provide a simple and easy-to-use interface for helping humans in the loop dealing with all the needed information. This tool was deployed and used in a Hackathon event to evaluate the reliability of a skin cancer image detector. During the event, experts and non-experts attacked and defended the detector, learning which factors were the most important for model misclassification and which techniques were the most efficient. The event was also used to detect NS's limitations and gather feedback for further improvements.</li>
</ul>

<h3>Title: Resilient Watermarking for LLM-Generated Codes</h3>
<ul>
<li><strong>Authors: </strong>Boquan Li, Mengdi Zhang, Peixin Zhang, Jun Sun, Xingmei Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07518">https://arxiv.org/abs/2402.07518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07518">https://arxiv.org/pdf/2402.07518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07518]] Resilient Watermarking for LLM-Generated Codes(https://arxiv.org/abs/2402.07518)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark, large language model</a></li>
<li><strong>Abstract: </strong>With the development of large language models, multiple AIs are now made available for code generation (such as ChatGPT and StarCoder) and are adopted widely. It is often desirable to know whether a piece of code is generated by AI, and furthermore, which AI is the author. For instance, if a certain version of AI is known to generate vulnerable code, it is particularly important to know the creator. Existing approaches are not satisfactory as watermarking codes are challenging compared with watermarking text data, as codes can be altered with relative ease via widely-used code refactoring methods. In this work, we propose ACW (AI Code Watermarking), a novel method for watermarking AI-generated codes. ACW is efficient as it requires no training or fine-tuning and works in a black-box manner. It is resilient as the watermark cannot be easily removed or tampered through common code refactoring methods. The key idea of ACW is to selectively apply a set of carefully-designed semantic-preserving, idempotent code transformations, whose presence (or absence) allows us to determine the existence of the watermark. Our experimental results show that ACW is effective (i.e., achieving high accuracy, true positive rates and false positive rates), resilient and efficient, significantly outperforming existing approaches.</li>
</ul>

<h3>Title: MAFIA: Multi-Adapter Fused Inclusive LanguAge Models</h3>
<ul>
<li><strong>Authors: </strong>Prachi Jain, Ashutosh Sathe, Varun Gumma, Kabir Ahuja, Sunayana Sitaram</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07519">https://arxiv.org/abs/2402.07519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07519">https://arxiv.org/pdf/2402.07519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07519]] MAFIA: Multi-Adapter Fused Inclusive LanguAge Models(https://arxiv.org/abs/2402.07519)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model to maintain the performance on the downstream task. In this work, we aim to modularly debias a pretrained language model across multiple dimensions. Previous works extensively explored debiasing PLMs using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks and languages demonstrates the efficacy of our approach.</li>
</ul>

<h3>Title: Show Me How It's Done: The Role of Explanations in Fine-Tuning Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Mohamad Ballout, Ulf Krumnack, Gunther Heidemann, Kai-Uwe Kuehnberger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07543">https://arxiv.org/abs/2402.07543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07543">https://arxiv.org/pdf/2402.07543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07543]] Show Me How It's Done: The Role of Explanations in Fine-Tuning Language  Models(https://arxiv.org/abs/2402.07543)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Our research demonstrates the significant benefits of using fine-tuning with explanations to enhance the performance of language models. Unlike prompting, which maintains the model's parameters, fine-tuning allows the model to learn and update its parameters during a training phase. In this study, we applied fine-tuning to various sized language models using data that contained explanations of the output rather than merely presenting the answers. We found that even smaller language models with as few as 60 million parameters benefited substantially from this approach. Interestingly, our results indicated that the detailed explanations were more beneficial to smaller models than larger ones, with the latter gaining nearly the same advantage from any form of explanation, irrespective of its length. Additionally, we demonstrate that the inclusion of explanations enables the models to solve tasks that they were not able to solve without explanations. Lastly, we argue that despite the challenging nature of adding explanations, samples that contain explanations not only reduce the volume of data required for training but also promote a more effective generalization by the model. In essence, our findings suggest that fine-tuning with explanations significantly bolsters the performance of large language models.</li>
</ul>

<h3>Title: TransAxx: Efficient Transformers with Approximate Computing</h3>
<ul>
<li><strong>Authors: </strong>Dimitrios Danopoulos, Georgios Zervakis, Dimitrios Soudris, Jörg Henkel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07545">https://arxiv.org/abs/2402.07545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07545">https://arxiv.org/pdf/2402.07545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07545]] TransAxx: Efficient Transformers with Approximate Computing(https://arxiv.org/abs/2402.07545)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision Transformer (ViT) models which were recently introduced by the transformer architecture have shown to be very competitive and often become a popular alternative to Convolutional Neural Networks (CNNs). However, the high computational requirements of these models limit their practical applicability especially on low-power devices. Current state-of-the-art employs approximate multipliers to address the highly increased compute demands of DNN accelerators but no prior research has explored their use on ViT models. In this work we propose TransAxx, a framework based on the popular PyTorch library that enables fast inherent support for approximate arithmetic to seamlessly evaluate the impact of approximate computing on DNNs such as ViT models. Using TransAxx we analyze the sensitivity of transformer models on the ImageNet dataset to approximate multiplications and perform approximate-aware finetuning to regain accuracy. Furthermore, we propose a methodology to generate approximate accelerators for ViT models. Our approach uses a Monte Carlo Tree Search (MCTS) algorithm to efficiently search the space of possible configurations using a hardware-driven hand-crafted policy. Our evaluation demonstrates the efficacy of our methodology in achieving significant trade-offs between accuracy and power, resulting in substantial gains without compromising on performance.</li>
</ul>

<h3>Title: Discovering Universal Semantic Triggers for Text-to-Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Shengfang Zhai, Weilong Wang, Jiajun Li, Yinpeng Dong, Hang Su, Qingni Shen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07562">https://arxiv.org/abs/2402.07562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07562">https://arxiv.org/pdf/2402.07562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07562]] Discovering Universal Semantic Triggers for Text-to-Image Synthesis(https://arxiv.org/abs/2402.07562)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recently text-to-image models have gained widespread attention in the community due to their controllable and high-quality generation ability. However, the robustness of such models and their potential ethical issues have not been fully explored. In this paper, we introduce Universal Semantic Trigger, a meaningless token sequence that can be added at any location within the input text yet can induce generated images towards a preset semantic target.To thoroughly investigate it, we propose Semantic Gradient-based Search (SGS) framework. SGS automatically discovers the potential universal semantic triggers based on the given semantic targets. Furthermore, we design evaluation metrics to comprehensively evaluate semantic shift of images caused by these triggers. And our empirical analyses reveal that the mainstream open-source text-to-image models are vulnerable to our triggers, which could pose significant ethical threats. Our work contributes to a further understanding of text-to-image synthesis and helps users to automatically auditing their models before deployment.</li>
</ul>

<h3>Title: Only the Curve Shape Matters: Training Foundation Models for Zero-Shot  Multivariate Time Series Forecasting through Next Curve Shape Prediction</h3>
<ul>
<li><strong>Authors: </strong>Cheng Feng, Long Huang, Denis Krompass</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07570">https://arxiv.org/abs/2402.07570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07570">https://arxiv.org/pdf/2402.07570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07570]] Only the Curve Shape Matters: Training Foundation Models for Zero-Shot  Multivariate Time Series Forecasting through Next Curve Shape Prediction(https://arxiv.org/abs/2402.07570)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains. In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude. GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner. Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines. Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate time series forecasting.</li>
</ul>

<h3>Title: Privacy-Optimized Randomized Response for Sharing Multi-Attribute Data</h3>
<ul>
<li><strong>Authors: </strong>Akito Yamamoto, Tetsuo Shibuya</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07584">https://arxiv.org/abs/2402.07584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07584">https://arxiv.org/pdf/2402.07584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07584]] Privacy-Optimized Randomized Response for Sharing Multi-Attribute Data(https://arxiv.org/abs/2402.07584)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>With the increasing amount of data in society, privacy concerns in data sharing have become widely recognized. Particularly, protecting personal attribute information is essential for a wide range of aims from crowdsourcing to realizing personalized medicine. Although various differentially private methods based on randomized response have been proposed for single attribute information or specific analysis purposes such as frequency estimation, there is a lack of studies on the mechanism for sharing individuals' multiple categorical information itself. The existing randomized response for sharing multi-attribute data uses the Kronecker product to perturb each attribute information in turn according to the respective privacy level but achieves only a weak privacy level for the entire dataset. Therefore, in this study, we propose a privacy-optimized randomized response that guarantees the strongest privacy in sharing multi-attribute data. Furthermore, we present an efficient heuristic algorithm for constructing a near-optimal mechanism. The time complexity of our algorithm is O(k^2), where k is the number of attributes, and it can be performed in about 1 second even for large datasets with k = 1,000. The experimental results demonstrate that both of our methods provide significantly stronger privacy guarantees for the entire dataset than the existing method. In addition, we show an analysis example using genome statistics to confirm that our methods can achieve less than half the output error compared with that of the existing method. Overall, this study is an important step toward trustworthy sharing and analysis of multi-attribute data. The Python implementation of our experiments and supplemental results are available at https://github.com/ay0408/Optimized-RR.</li>
</ul>

<h3>Title: Unveiling Group-Specific Distributed Concept Drift: A Fairness  Imperative in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Teresa Salazar, João Gama, Helder Araújo, Pedro Henriques Abreu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07586">https://arxiv.org/abs/2402.07586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07586">https://arxiv.org/pdf/2402.07586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07586]] Unveiling Group-Specific Distributed Concept Drift: A Fairness  Imperative in Federated Learning(https://arxiv.org/abs/2402.07586)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, fair</a></li>
<li><strong>Abstract: </strong>In the evolving field of machine learning, ensuring fairness has become a critical concern, prompting the development of algorithms designed to mitigate discriminatory outcomes in decision-making processes. However, achieving fairness in the presence of group-specific concept drift remains an unexplored frontier, and our research represents pioneering efforts in this regard. Group-specific concept drift refers to situations where one group experiences concept drift over time while another does not, leading to a decrease in fairness even if accuracy remains fairly stable. Within the framework of federated learning, where clients collaboratively train models, its distributed nature further amplifies these challenges since each client can experience group-specific concept drift independently while still sharing the same underlying concept, creating a complex and dynamic environment for maintaining fairness. One of the significant contributions of our research is the formalization and introduction of the problem of group-specific concept drift and its distributed counterpart, shedding light on its critical importance in the realm of fairness. In addition, leveraging insights from prior research, we adapt an existing distributed concept drift adaptation algorithm to tackle group-specific distributed concept drift which utilizes a multi-model approach, a local group-specific drift detection mechanism, and continuous clustering of models over time. The findings from our experiments highlight the importance of addressing group-specific concept drift and its distributed counterpart to advance fairness in machine learning.</li>
</ul>

<h3>Title: Sheet Music Transformer: End-To-End Optical Music Recognition Beyond  Monophonic Transcription</h3>
<ul>
<li><strong>Authors: </strong>Antonio Ríos-Vila, Jorge Calvo-Zaragoza, Thierry Paquet</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07596">https://arxiv.org/abs/2402.07596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07596">https://arxiv.org/pdf/2402.07596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07596]] Sheet Music Transformer: End-To-End Optical Music Recognition Beyond  Monophonic Transcription(https://arxiv.org/abs/2402.07596)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>State-of-the-art end-to-end Optical Music Recognition (OMR) has, to date, primarily been carried out using monophonic transcription techniques to handle complex score layouts, such as polyphony, often by resorting to simplifications or specific adaptations. Despite their efficacy, these approaches imply challenges related to scalability and limitations. This paper presents the Sheet Music Transformer, the first end-to-end OMR model designed to transcribe complex musical scores without relying solely on monophonic strategies. Our model employs a Transformer-based image-to-sequence framework that predicts score transcriptions in a standard digital music encoding format from input images. Our model has been tested on two polyphonic music datasets and has proven capable of handling these intricate music structures effectively. The experimental outcomes not only indicate the competence of the model, but also show that it is better than the state-of-the-art methods, thus contributing to advancements in end-to-end OMR transcription.</li>
</ul>

<h3>Title: Near-Minimax-Optimal Distributional Reinforcement Learning with a  Generative Model</h3>
<ul>
<li><strong>Authors: </strong>Mark Rowland, Li Kevin Wenliang, Rémi Munos, Clare Lyle, Yunhao Tang, Will Dabney</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07598">https://arxiv.org/abs/2402.07598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07598">https://arxiv.org/pdf/2402.07598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07598]] Near-Minimax-Optimal Distributional Reinforcement Learning with a  Generative Model(https://arxiv.org/abs/2402.07598)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose a new algorithm for model-based distributional reinforcement learning (RL), and prove that it is minimax-optimal for approximating return distributions with a generative model (up to logarithmic factors), resolving an open question of Zhang et al. (2023). Our analysis provides new theoretical results on categorical approaches to distributional RL, and also introduces a new distributional Bellman equation, the stochastic categorical CDF Bellman equation, which we expect to be of independent interest. We also provide an experimental study comparing several model-based distributional RL algorithms, with several takeaways for practitioners.</li>
</ul>

<h3>Title: Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Wang, Guozheng Ma, Ziqiao Meng, Zeyu Qin, Li Shen, Zhong Zhang, Bingzhe Wu, Liu Liu, Yatao Bian, Tingyang Xu, Xueqian Wang, Peilin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07610">https://arxiv.org/abs/2402.07610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07610">https://arxiv.org/pdf/2402.07610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07610]] Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping(https://arxiv.org/abs/2402.07610)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models. Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning. To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced few-shot ability to boost zero or one-shot performance. Based on easy-to-hard training recipe, we propose SOFT+ which further boost self-alignment's performance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across various classification and generation tasks, highlighting the potential of bootstrapping self-alignment on continually enhancing model alignment performance.</li>
</ul>

<h3>Title: Anchor-based Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jianhui Pang, Fanghua Ye, Derek F. Wong, Longyue Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07616">https://arxiv.org/abs/2402.07616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07616">https://arxiv.org/pdf/2402.07616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07616]] Anchor-based Large Language Models(https://arxiv.org/abs/2402.07616)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the AnLLM significantly improves computational efficiency and resource utilization, demonstrating the potential of the anchor-based attention approach in the context of LLMs for real-time inference in practical applications.</li>
</ul>

<h3>Title: G-Retriever: Retrieval-Augmented Generation for Textual Graph  Understanding and Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07630">https://arxiv.org/abs/2402.07630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07630">https://arxiv.org/pdf/2402.07630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07630]] G-Retriever: Retrieval-Augmented Generation for Textual Graph  Understanding and Question Answering(https://arxiv.org/abs/2402.07630)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop our Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever approach, which integrates the strengths of GNNs, LLMs, and Retrieval-Augmented Generation (RAG), and can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and resists hallucination. (Our codes and datasets are available at: https://github.com/XiaoxinHe/G-Retriever.)</li>
</ul>

<h3>Title: Complete Instances Mining for Weakly Supervised Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zecheng Li, Zening Zeng, Yuqi Liang, Jin-Gang Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07633">https://arxiv.org/abs/2402.07633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07633">https://arxiv.org/pdf/2402.07633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07633]] Complete Instances Mining for Weakly Supervised Instance Segmentation(https://arxiv.org/abs/2402.07633)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Weakly supervised instance segmentation (WSIS) using only image-level labels is a challenging task due to the difficulty of aligning coarse annotations with the finer task. However, with the advancement of deep neural networks (DNNs), WSIS has garnered significant attention. Following a proposal-based paradigm, we encounter a redundant segmentation problem resulting from a single instance being represented by multiple proposals. For example, we feed a picture of a dog and proposals into the network and expect to output only one proposal containing a dog, but the network outputs multiple proposals. To address this problem, we propose a novel approach for WSIS that focuses on the online refinement of complete instances through the use of MaskIoU heads to predict the integrity scores of proposals and a Complete Instances Mining (CIM) strategy to explicitly model the redundant segmentation problem and generate refined pseudo labels. Our approach allows the network to become aware of multiple instances and complete instances, and we further improve its robustness through the incorporation of an Anti-noise strategy. Empirical evaluations on the PASCAL VOC 2012 and MS COCO datasets demonstrate that our method achieves state-of-the-art performance with a notable margin. Our implementation will be made available at https://github.com/ZechengLi19/CIM.</li>
</ul>

<h3>Title: Collaborative Semantic Occupancy Prediction with Hybrid Feature Fusion  in Connected Automated Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Rui Song, Chenwei Liang, Hu Cao, Zhiran Yan, Walter Zimmer, Markus Gross, Andreas Festag, Alois Knoll</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07635">https://arxiv.org/abs/2402.07635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07635">https://arxiv.org/pdf/2402.07635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07635]] Collaborative Semantic Occupancy Prediction with Hybrid Feature Fusion  in Connected Automated Vehicles(https://arxiv.org/abs/2402.07635)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Collaborative perception in automated vehicles leverages the exchange of information between agents, aiming to elevate perception results. Previous camera-based collaborative 3D perception methods typically employ 3D bounding boxes or bird's eye views as representations of the environment. However, these approaches fall short in offering a comprehensive 3D environmental prediction. To bridge this gap, we introduce the first method for collaborative 3D semantic occupancy prediction. Particularly, it improves local 3D semantic occupancy predictions by hybrid fusion of (i) semantic and occupancy task features, and (ii) compressed orthogonal attention features shared between vehicles. Additionally, due to the lack of a collaborative perception dataset designed for semantic occupancy prediction, we augment a current collaborative perception dataset to include 3D collaborative semantic occupancy labels for a more robust evaluation. The experimental findings highlight that: (i) our collaborative semantic occupancy predictions excel above the results from single vehicles by over 30%, and (ii) models anchored on semantic occupancy outpace state-of-the-art collaborative 3D detection techniques in subsequent perception applications, showcasing enhanced accuracy and enriched semantic-awareness in road environments.</li>
</ul>

<h3>Title: Tighter Bounds on the Information Bottleneck with Application to Deep  Learning</h3>
<ul>
<li><strong>Authors: </strong>Nir Weingarten, Zohar Yakhini, Moshe Butman, Ran Gilad-Bachrach</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07639">https://arxiv.org/abs/2402.07639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07639">https://arxiv.org/pdf/2402.07639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07639]] Tighter Bounds on the Information Bottleneck with Application to Deep  Learning(https://arxiv.org/abs/2402.07639)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Deep Neural Nets (DNNs) learn latent representations induced by their downstream task, objective function, and other parameters. The quality of the learned representations impacts the DNN's generalization ability and the coherence of the emerging latent space. The Information Bottleneck (IB) provides a hypothetically optimal framework for data modeling, yet it is often intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired variational methods to approximate bounds on mutual information, resulting in improved robustness to adversarial attacks. This work introduces a new and tighter variational bound for the IB, improving performance of previous IB-inspired DNNs. These advancements strengthen the case for the IB and its variational approximations as a data modeling framework, and provide a simple method to significantly enhance the adversarial robustness of classifier DNNs.</li>
</ul>

<h3>Title: Detecting the Clinical Features of Difficult-to-Treat Depression using  Synthetic Data from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Isabelle Lorge, Dan W. Joyce, Niall Taylor, Alejo Nevado-Holgado, Andrea Cipriani, Andrey Kormilitzin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07645">https://arxiv.org/abs/2402.07645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07645">https://arxiv.org/pdf/2402.07645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07645]] Detecting the Clinical Features of Difficult-to-Treat Depression using  Synthetic Data from Large Language Models(https://arxiv.org/abs/2402.07645)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Difficult-to-treat depression (DTD) has been proposed as a broader and more clinically comprehensive perspective on a person's depressive disorder where despite treatment, they continue to experience significant burden. We sought to develop a Large Language Model (LLM)-based tool capable of interrogating routinely-collected, narrative (free-text) electronic health record (EHR) data to locate published prognostic factors that capture the clinical syndrome of DTD. In this work, we use LLM-generated synthetic data (GPT3.5) and a Non-Maximum Suppression (NMS) algorithm to train a BERT-based span extraction model. The resulting model is then able to extract and label spans related to a variety of relevant positive and negative factors in real clinical data (i.e. spans of text that increase or decrease the likelihood of a patient matching the DTD syndrome). We show it is possible to obtain good overall performance (0.70 F1 across polarity) on real clinical data on a set of as many as 20 different factors, and high performance (0.85 F1 with 0.95 precision) on a subset of important DTD factors such as history of abuse, family history of affective disorder, illness severity and suicidality by training the model exclusively on synthetic data. Our results show promise for future healthcare applications especially in applications where traditionally, highly confidential medical data and human-expert annotation would normally be required.</li>
</ul>

<h3>Title: The Sound of Healthcare: Improving Medical Transcription ASR Accuracy  with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ayo Adedeji, Sarita Joshi, Brendan Doohan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07658">https://arxiv.org/abs/2402.07658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07658">https://arxiv.org/pdf/2402.07658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07658]] The Sound of Healthcare: Improving Medical Transcription ASR Accuracy  with Large Language Models(https://arxiv.org/abs/2402.07658)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving landscape of medical documentation, transcribing clinical dialogues accurately is increasingly paramount. This study explores the potential of Large Language Models (LLMs) to enhance the accuracy of Automatic Speech Recognition (ASR) systems in medical transcription. Utilizing the PriMock57 dataset, which encompasses a diverse range of primary care consultations, we apply advanced LLMs to refine ASR-generated transcripts. Our research is multifaceted, focusing on improvements in general Word Error Rate (WER), Medical Concept WER (MC-WER) for the accurate transcription of essential medical terms, and speaker diarization accuracy. Additionally, we assess the role of LLM post-processing in improving semantic textual similarity, thereby preserving the contextual integrity of clinical dialogues. Through a series of experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT) prompting techniques in enhancing diarization and correction accuracy. Our findings demonstrate that LLMs, particularly through CoT prompting, not only improve the diarization accuracy of existing ASR systems but also achieve state-of-the-art performance in this domain. This improvement extends to more accurately capturing medical concepts and enhancing the overall semantic coherence of the transcribed dialogues. These findings illustrate the dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.</li>
</ul>

<h3>Title: GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted  Assembly Guidance</h3>
<ul>
<li><strong>Authors: </strong>Shiyu Li, Hannah Schieber, Niklas Corell, Bernhard Egger, Julian Kreimeier, Daniel Roth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07677">https://arxiv.org/abs/2402.07677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07677">https://arxiv.org/pdf/2402.07677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07677]] GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted  Assembly Guidance(https://arxiv.org/abs/2402.07677)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Guidance for assemblable parts is a promising field for augmented reality. Augmented reality assembly guidance requires 6D object poses of target objects in real time. Especially in time-critical medical or industrial settings, continuous and markerless tracking of individual parts is essential to visualize instructions superimposed on or next to the target object parts. In this regard, occlusions by the user's hand or other objects and the complexity of different assembly states complicate robust and real-time markerless multi-object tracking. To address this problem, we present Graph-based Object Tracking (GBOT), a novel graph-based single-view RGB-D tracking approach. The real-time markerless multi-object tracking is initialized via 6D pose estimation and updates the graph-based assembly poses. The tracking through various assembly states is achieved by our novel multi-state assembly graph. We update the multi-state assembly graph by utilizing the relative poses of the individual assembly parts. Linking the individual objects in this graph enables more robust object tracking during the assembly process. For evaluation, we introduce a synthetic dataset of publicly available and 3D printable assembly assets as a benchmark for future work. Quantitative experiments in synthetic data and further qualitative study in real test data show that GBOT can outperform existing work towards enabling context-aware augmented reality assembly guidance. Dataset and code will be made publically available.</li>
</ul>

<h3>Title: AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual  Vision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Tanmoy Dam, Sanjay Bhargav Dharavath, Sameer Alam, Nimrod Lilith, Supriyo Chakraborty, Mir Feroskhan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07680">https://arxiv.org/abs/2402.07680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07680">https://arxiv.org/pdf/2402.07680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07680]] AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual  Vision Transformer(https://arxiv.org/abs/2402.07680)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Combining LiDAR and camera data has shown potential in enhancing short-distance object detection in autonomous driving systems. Yet, the fusion encounters difficulties with extended distance detection due to the contrast between LiDAR's sparse data and the dense resolution of cameras. Besides, discrepancies in the two data representations further complicate fusion methods. We introduce AYDIV, a novel framework integrating a tri-phase alignment process specifically designed to enhance long-distance detection even amidst data discrepancies. AYDIV consists of the Global Contextual Fusion Alignment Transformer (GCFAT), which improves the extraction of camera features and provides a deeper understanding of large-scale patterns; the Sparse Fused Feature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial data fusion. AYDIV's performance on the Waymo Open Dataset (WOD) with an improvement of 1.24% in mAPH value(L2 difficulty) and the Argoverse2 Dataset with a performance improvement of 7.40% in AP value demonstrates its efficacy in comparison to other existing fusion-based methods. Our code is publicly available at https://github.com/sanjay-810/AYDIV2</li>
</ul>

<h3>Title: Large Language Models "Ad Referendum": How Good Are They at Machine  Translation in the Legal Domain?</h3>
<ul>
<li><strong>Authors: </strong>Vicent Briva-Iglesias, Joao Lucas Cavalheiro Camargo, Gokhan Dogru</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07681">https://arxiv.org/abs/2402.07681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07681">https://arxiv.org/pdf/2402.07681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07681]] Large Language Models "Ad Referendum": How Good Are They at Machine  Translation in the Legal Domain?(https://arxiv.org/abs/2402.07681)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study evaluates the machine translation (MT) quality of two state-of-the-art large language models (LLMs) against a tradition-al neural machine translation (NMT) system across four language pairs in the legal domain. It combines automatic evaluation met-rics (AEMs) and human evaluation (HE) by professional transla-tors to assess translation ranking, fluency and adequacy. The re-sults indicate that while Google Translate generally outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4, comparably or slightly better in terms of producing contextually adequate and fluent translations. This discrepancy suggests LLMs' potential in handling specialized legal terminology and context, highlighting the importance of human evaluation methods in assessing MT quality. The study underscores the evolving capabil-ities of LLMs in specialized domains and calls for reevaluation of traditional AEMs to better capture the nuances of LLM-generated translations.</li>
</ul>

<h3>Title: Auxiliary Tasks to Boost Biaffine Semantic Dependency Parsing</h3>
<ul>
<li><strong>Authors: </strong>Marie Candito</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07682">https://arxiv.org/abs/2402.07682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07682">https://arxiv.org/pdf/2402.07682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07682]] Auxiliary Tasks to Boost Biaffine Semantic Dependency Parsing(https://arxiv.org/abs/2402.07682)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>The biaffine parser of Dozat and Manning (2017) was successfully extended to semantic dependency parsing (SDP) (Dozat and Manning, 2018). Its performance on graphs is surprisingly high given that, without the constraint of producing a tree, all arcs for a given sentence are predicted independently from each other (modulo a shared representation of tokens). To circumvent such an independence of decision, while retaining the O(n^2) complexity and highly parallelizable architecture, we propose to use simple auxiliary tasks that introduce some form of interdependence between arcs. Experiments on the three English acyclic datasets of SemEval 2015 task 18 (Oepen et al., 2015), and on French deep syntactic cyclic graphs (Ribeyre et al., 2014) show modest but systematic performance gains on a near state-of-the-art baseline using transformer-based contextualized representations. This provides a simple and robust method to boost SDP performance.</li>
</ul>

<h3>Title: OrderBkd: Textual backdoor attack through repositioning</h3>
<ul>
<li><strong>Authors: </strong>Irina Alekseevskaia, Konstantin Arkhipenko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07689">https://arxiv.org/abs/2402.07689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07689">https://arxiv.org/pdf/2402.07689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07689]] OrderBkd: Textual backdoor attack through repositioning(https://arxiv.org/abs/2402.07689)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>The use of third-party datasets and pre-trained machine learning models poses a threat to NLP systems due to possibility of hidden backdoor attacks. Existing attacks involve poisoning the data samples such as insertion of tokens or sentence paraphrasing, which either alter the semantics of the original texts or can be detected. Our main difference from the previous work is that we use the reposition of a two words in a sentence as a trigger. By designing and applying specific part-of-speech (POS) based rules for selecting these tokens, we maintain high attack success rate on SST-2 and AG classification datasets while outperforming existing attacks in terms of perplexity and semantic similarity to the clean samples. In addition, we show the robustness of our attack to the ONION defense method. All the code and data for the paper can be obtained at https://github.com/alekseevskaia/OrderBkd.</li>
</ul>

<h3>Title: Signed Distance Field based Segmentation and Statistical Shape Modelling  of the Left Atrial Appendage</h3>
<ul>
<li><strong>Authors: </strong>Kristine Aavild Juhl, Jakob Slipsager, Ole de Backer, Klaus Kofoed, Oscar Camara, Rasmus Paulsen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07708">https://arxiv.org/abs/2402.07708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07708">https://arxiv.org/pdf/2402.07708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07708]] Signed Distance Field based Segmentation and Statistical Shape Modelling  of the Left Atrial Appendage(https://arxiv.org/abs/2402.07708)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Patients with atrial fibrillation have a 5-7 fold increased risk of having an ischemic stroke. In these cases, the most common site of thrombus localization is inside the left atrial appendage (LAA) and studies have shown a correlation between the LAA shape and the risk of ischemic stroke. These studies make use of manual measurement and qualitative assessment of shape and are therefore prone to large inter-observer discrepancies, which may explain the contradictions between the conclusions in different studies. We argue that quantitative shape descriptors are necessary to robustly characterize LAA morphology and relate to other functional parameters and stroke risk. Deep Learning methods are becoming standardly available for segmenting cardiovascular structures from high resolution images such as computed tomography (CT), but only few have been tested for LAA segmentation. Furthermore, the majority of segmentation algorithms produces non-smooth 3D models that are not ideal for further processing, such as statistical shape analysis or computational fluid modelling. In this paper we present a fully automatic pipeline for image segmentation, mesh model creation and statistical shape modelling of the LAA. The LAA anatomy is implicitly represented as a signed distance field (SDF), which is directly regressed from the CT image using Deep Learning. The SDF is further used for registering the LAA shapes to a common template and build a statistical shape model (SSM). Based on 106 automatically segmented LAAs, the built SSM reveals that the LAA shape can be quantified using approximately 5 PCA modes and allows the identification of two distinct shape clusters corresponding to the so-called chicken-wing and non-chicken-wing morphologies.</li>
</ul>

<h3>Title: Model Collapse Demystified: The Case of Regression</h3>
<ul>
<li><strong>Authors: </strong>Elvis Dohmatob, Yunzhen Feng, Julia Kempe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07712">https://arxiv.org/abs/2402.07712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07712">https://arxiv.org/pdf/2402.07712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07712]] Model Collapse Demystified: The Case of Regression(https://arxiv.org/abs/2402.07712)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the era of large language models like ChatGPT, the phenomenon of "model collapse" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses. Under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.</li>
</ul>

<h3>Title: Adaptive Artificial Immune Networks for Mitigating DoS flooding Attacks</h3>
<ul>
<li><strong>Authors: </strong>Jorge Maestre Vidal, Ana Lucila Sandoval Orozco, Luis Javier García Villalba</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07714">https://arxiv.org/abs/2402.07714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07714">https://arxiv.org/pdf/2402.07714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07714]] Adaptive Artificial Immune Networks for Mitigating DoS flooding Attacks(https://arxiv.org/abs/2402.07714)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Denial of service attacks pose a threat in constant growth. This is mainly due to their tendency to gain in sophistication, ease of implementation, obfuscation and the recent improvements in occultation of fingerprints. On the other hand, progress towards self-organizing networks, and the different techniques involved in their development, such as software-defined networking, network-function virtualization, artificial intelligence or cloud computing, facilitates the design of new defensive strategies, more complete, consistent and able to adapt the defensive deployment to the current status of the network. In order to contribute to their development, in this paper, the use of artificial immune systems to mitigate denial of service attacks is proposed. The approach is based on building networks of distributed sensors suited to the requirements of the monitored environment. These components are capable of identifying threats and reacting according to the behavior of the biological defense mechanisms in human beings. It is accomplished by emulating the different immune reactions, the establishment of quarantine areas and the construction of immune memory. For their assessment, experiments with public domain datasets (KDD'99, CAIDA'07 and CAIDA'08) and simulations on various network configurations based on traffic samples gathered by the University Complutense of Madrid and flooding attacks generated by the tool DDoSIM were performed.</li>
</ul>

<h3>Title: Asking Multimodal Clarifying Questions in Mixed-Initiative  Conversational Search</h3>
<ul>
<li><strong>Authors: </strong>Yifei Yuan, Clemencia Siro, Mohammad Aliannejadi, Maarten de Rijke, Wai Lam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07742">https://arxiv.org/abs/2402.07742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07742">https://arxiv.org/pdf/2402.07742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07742]] Asking Multimodal Clarifying Questions in Mixed-Initiative  Conversational Search(https://arxiv.org/abs/2402.07742)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In mixed-initiative conversational search systems, clarifying questions are used to help users who struggle to express their intentions in a single query. These questions aim to uncover user's information needs and resolve query ambiguities. We hypothesize that in scenarios where multimodal information is pertinent, the clarification process can be improved by using non-textual information. Therefore, we propose to add images to clarifying questions and formulate the novel task of asking multimodal clarifying questions in open-domain, mixed-initiative conversational search systems. To facilitate research into this task, we collect a dataset named Melon that contains over 4k multimodal clarifying questions, enriched with over 14k images. We also propose a multimodal query clarification model named Marto and adopt a prompt-based, generative fine-tuning strategy to perform the training of different stages with different prompts. Several analyses are conducted to understand the importance of multimodal contents during the query clarification phase. Experimental results indicate that the addition of images leads to significant improvements of up to 90% in retrieval performance when selecting the relevant images. Extensive analyses are also performed to show the superiority of Marto compared with discriminative baselines in terms of effectiveness and efficiency.</li>
</ul>

<h3>Title: Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Zhenguo Li, Wei Bi, Lingpeng Kong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07754">https://arxiv.org/abs/2402.07754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07754">https://arxiv.org/pdf/2402.07754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07754]] Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language  Models(https://arxiv.org/abs/2402.07754)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capabilities in diffusion language models.</li>
</ul>

<h3>Title: Towards an Understanding of Stepwise Inference in Transformers: A  Synthetic Graph Navigation Model</h3>
<ul>
<li><strong>Authors: </strong>Mikail Khona, Maya Okawa, Jan Hula, Rahul Ramesh, Kento Nishi, Robert Dick, Ekdeep Singh Lubana, Hidenori Tanaka</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07757">https://arxiv.org/abs/2402.07757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07757">https://arxiv.org/pdf/2402.07757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07757]] Towards an Understanding of Stepwise Inference in Transformers: A  Synthetic Graph Navigation Model(https://arxiv.org/abs/2402.07757)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive. To address this, we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model's output; and (iv) compositional generalization and a primacy bias with in-context exemplars. Overall, our work introduces a grounded, synthetic framework for studying stepwise inference and offers mechanistic hypotheses that can lay the foundation for a deeper understanding of this phenomenon.</li>
</ul>

<h3>Title: TELLER: A Trustworthy Framework for Explainable, Generalizable and  Controllable Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Hui Liu, Wenya Wang, Haoru Li, Haoliang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07776">https://arxiv.org/abs/2402.07776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07776">https://arxiv.org/pdf/2402.07776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07776]] TELLER: A Trustworthy Framework for Explainable, Generalizable and  Controllable Fake News Detection(https://arxiv.org/abs/2402.07776)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of fake news has emerged as a severe societal problem, raising significant interest from industry and academia. While existing deep-learning based methods have made progress in detecting fake news accurately, their reliability may be compromised caused by the non-transparent reasoning processes, poor generalization abilities and inherent risks of integration with large language models (LLMs). To address this challenge, we propose {\methodname}, a novel framework for trustworthy fake news detection that prioritizes explainability, generalizability and controllability of models. This is achieved via a dual-system framework that integrates cognition and decision systems, adhering to the principles above. The cognition system harnesses human expertise to generate logical predicates, which guide LLMs in generating human-readable logic atoms. Meanwhile, the decision system deduces generalizable logic rules to aggregate these atoms, enabling the identification of the truthfulness of the input news across diverse domains and enhancing transparency in the decision-making process. Finally, we present comprehensive evaluation results on four datasets, demonstrating the feasibility and trustworthiness of our proposed framework. Our implementation is available at \url{https://github.com/less-and-less-bugs/Trust_TELLER}.</li>
</ul>

<h3>Title: Empowering Federated Learning for Massive Models with NVIDIA FLARE</h3>
<ul>
<li><strong>Authors: </strong>Holger R. Roth, Ziyue Xu, Yuan-Ting Hsieh, Adithya Renduchintala, Isaac Yang, Zhihong Zhang, Yuhong Wen, Sean Yang, Kevin Lu, Kristopher Kersten, Camir Ricketts, Daguang Xu, Chester Chen, Yan Cheng, Andrew Feng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07792">https://arxiv.org/abs/2402.07792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07792">https://arxiv.org/pdf/2402.07792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07792]] Empowering Federated Learning for Massive Models with NVIDIA FLARE(https://arxiv.org/abs/2402.07792)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate, large language model</a></li>
<li><strong>Abstract: </strong>In the ever-evolving landscape of artificial intelligence (AI) and large language models (LLMs), handling and leveraging data effectively has become a critical challenge. Most state-of-the-art machine learning algorithms are data-centric. However, as the lifeblood of model performance, necessary data cannot always be centralized due to various factors such as privacy, regulation, geopolitics, copyright issues, and the sheer effort required to move vast datasets. In this paper, we explore how federated learning enabled by NVIDIA FLARE can address these challenges with easy and scalable integration capabilities, enabling parameter-efficient and full supervised fine-tuning of LLMs for natural language processing and biopharmaceutical applications to enhance their accuracy and robustness.</li>
</ul>

<h3>Title: Retrieval-Augmented Thought Process as Sequential Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Thomas Pouplin, Hao Sun, Samuel Holt, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07812">https://arxiv.org/abs/2402.07812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07812">https://arxiv.org/pdf/2402.07812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07812]] Retrieval-Augmented Thought Process as Sequential Decision Making(https://arxiv.org/abs/2402.07812)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated their strong ability to assist people and show "sparks of intelligence". However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts. In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference. In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models.</li>
</ul>

<h3>Title: PBADet: A One-Stage Anchor-Free Approach for Part-Body Association</h3>
<ul>
<li><strong>Authors: </strong>Zhongpai Gao, Huayi Zhou, Abhishek Sharma, Meng Zheng, Benjamin Planche, Terrence Chen, Ziyan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07814">https://arxiv.org/abs/2402.07814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07814">https://arxiv.org/pdf/2402.07814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07814]] PBADet: A One-Stage Anchor-Free Approach for Part-Body Association(https://arxiv.org/abs/2402.07814)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The detection of human parts (e.g., hands, face) and their correct association with individuals is an essential task, e.g., for ubiquitous human-machine interfaces and action recognition. Traditional methods often employ multi-stage processes, rely on cumbersome anchor-based systems, or do not scale well to larger part sets. This paper presents PBADet, a novel one-stage, anchor-free approach for part-body association detection. Building upon the anchor-free object representation across multi-scale feature maps, we introduce a singular part-to-body center offset that effectively encapsulates the relationship between parts and their parent bodies. Our design is inherently versatile and capable of managing multiple parts-to-body associations without compromising on detection accuracy or robustness. Comprehensive experiments on various datasets underscore the efficacy of our approach, which not only outperforms existing state-of-the-art techniques but also offers a more streamlined and efficient solution to the part-body association challenge.</li>
</ul>

<h3>Title: Differentially Private Zeroth-Order Methods for Scalable Large Language  Model Finetuning</h3>
<ul>
<li><strong>Authors: </strong>Z Liu, J Lou, W Bao, Z Qin, K Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07818">https://arxiv.org/abs/2402.07818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07818">https://arxiv.org/pdf/2402.07818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07818]] Differentially Private Zeroth-Order Methods for Scalable Large Language  Model Finetuning(https://arxiv.org/abs/2402.07818)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, data-free</a></li>
<li><strong>Abstract: </strong>Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replacement for SGD, this paper presents a comprehensive study both theoretically and empirically. First, we propose the stagewise DP zeroth-order method that dynamically schedules key hyperparameters. This design is grounded on the synergy between DP random perturbation and the gradient approximation error of the zeroth-order method, and its effect on finetuning trajectory. Second, we further enhance the scalability by reducing the trainable parameters that are identified by repurposing a data-free pruning technique requiring no additional data or extra privacy budget. We provide theoretical analysis for both proposed methods. We conduct extensive empirical analysis on both encoder-only masked language model and decoder-only autoregressive language model, achieving impressive results in terms of scalability and utility.</li>
</ul>

<h3>Title: On Computationally Efficient Multi-Class Calibration</h3>
<ul>
<li><strong>Authors: </strong>Parikshit Gopalan, Lunjia Hu, Guy N. Rothblum</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CC, cs.DS, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07821">https://arxiv.org/abs/2402.07821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07821">https://arxiv.org/pdf/2402.07821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07821]] On Computationally Efficient Multi-Class Calibration(https://arxiv.org/abs/2402.07821)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Consider a multi-class labelling problem, where the labels can take values in $[k]$, and a predictor predicts a distribution over the labels. In this work, we study the following foundational question: Are there notions of multi-class calibration that give strong guarantees of meaningful predictions and can be achieved in time and sample complexities polynomial in $k$? Prior notions of calibration exhibit a tradeoff between computational efficiency and expressivity: they either suffer from having sample complexity exponential in $k$, or needing to solve computationally intractable problems, or give rather weak guarantees. Our main contribution is a notion of calibration that achieves all these desiderata: we formulate a robust notion of projected smooth calibration for multi-class predictions, and give new recalibration algorithms for efficiently calibrating predictors under this definition with complexity polynomial in $k$. Projected smooth calibration gives strong guarantees for all downstream decision makers who want to use the predictor for binary classification problems of the form: does the label belong to a subset $T \subseteq [k]$: e.g. is this an image of an animal? It ensures that the probabilities predicted by summing the probabilities assigned to labels in $T$ are close to some perfectly calibrated binary predictor for that task. We also show that natural strengthenings of our definition are computationally hard to achieve: they run into information theoretic barriers or computational intractability. Underlying both our upper and lower bounds is a tight connection that we prove between multi-class calibration and the well-studied problem of agnostic learning in the (standard) binary prediction setting.</li>
</ul>

<h3>Title: Aya Model: An Instruction Finetuned Open-Access Multilingual Language  Model</h3>
<ul>
<li><strong>Authors: </strong>Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, Sara Hooker</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07827">https://arxiv.org/abs/2402.07827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07827">https://arxiv.org/pdf/2402.07827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07827]] Aya Model: An Instruction Finetuned Open-Access Multilingual Language  Model(https://arxiv.org/abs/2402.07827)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at https://hf.co/CohereForAI/aya-101</li>
</ul>

<h3>Title: Do Membership Inference Attacks Work on Large Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, Hannaneh Hajishirzi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07841">https://arxiv.org/abs/2402.07841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07841">https://arxiv.org/pdf/2402.07841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07841]] Do Membership Inference Attacks Work on Large Language Models?(https://arxiv.org/abs/2402.07841)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model's training data. Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of large language models (LLMs). We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members. We identify specific settings where LLMs have been shown to be vulnerable to membership inference and show that the apparent success in such settings can be attributed to a distribution shift, such as when members and non-members are drawn from the seemingly identical domain but with different temporal ranges. We release our code and data as a unified benchmark package that includes all existing MIAs, supporting future work.</li>
</ul>

<h3>Title: Generative Modeling of Discrete Joint Distributions by E-Geodesic Flow  Matching on Assignment Manifolds</h3>
<ul>
<li><strong>Authors: </strong>Bastian Boll, Daniel Gonzalez-Alvarado, Christoph Schnörr</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07846">https://arxiv.org/abs/2402.07846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07846">https://arxiv.org/pdf/2402.07846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07846]] Generative Modeling of Discrete Joint Distributions by E-Geodesic Flow  Matching on Assignment Manifolds(https://arxiv.org/abs/2402.07846)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel generative model for discrete distributions based on continuous normalizing flows on the submanifold of factorizing discrete measures. Integration of the flow gradually assigns categories and avoids issues of discretizing the latent continuous model like rounding, sample truncation etc. General non-factorizing discrete distributions capable of representing complex statistical dependencies of structured discrete data, can be approximated by embedding the submanifold into a the meta-simplex of all joint discrete distributions and data-driven averaging. Efficient training of the generative model is demonstrated by matching the flow of geodesics of factorizing discrete distributions. Various experiments underline the approach's broad applicability.</li>
</ul>

<h3>Title: Comparing skill of historical rainfall data based monsoon rainfall  prediction in India with NCEP-NWP forecasts</h3>
<ul>
<li><strong>Authors: </strong>Apoorva Narula, Aastha Jain, Jatin Batra, Sandeep Juneja</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07851">https://arxiv.org/abs/2402.07851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07851">https://arxiv.org/pdf/2402.07851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07851]] Comparing skill of historical rainfall data based monsoon rainfall  prediction in India with NCEP-NWP forecasts(https://arxiv.org/abs/2402.07851)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this draft we consider the problem of forecasting rainfall across India during the four monsoon months, one day as well as three days in advance. We train neural networks using historical daily gridded precipitation data for India obtained from IMD for the time period $1901- 2022$, at a spatial resolution of $1^{\circ} \times 1^{\circ}$. This is compared with the numerical weather prediction (NWP) forecasts obtained from NCEP (National Centre for Environmental Prediction) available for the period 2011-2022. We conduct a detailed country wide analysis and separately analyze some of the most populated cities in India. Our conclusion is that forecasts obtained by applying deep learning to historical rainfall data are more accurate compared to NWP forecasts as well as predictions based on persistence. On average, compared to our predictions, forecasts from NCEP-NWP model have about 34% higher error for a single day prediction, and over 68% higher error for a three day prediction. Similarly, persistence estimates report a 29% higher error in a single day forecast, and over 54% error in a three day forecast. We further observe that data up to 20 days in the past is useful in reducing errors of one and three day forecasts, when a transformer based learning architecture, and to a lesser extent when an LSTM is used. A key conclusion suggested by our preliminary analysis is that NWP forecasts can be substantially improved upon through more and diverse data relevant to monsoon prediction combined with carefully selected neural network architecture.</li>
</ul>

<h3>Title: Multiscale Neuroimaging Features for the Identification of Medication  Class and Non-Responders in Mood Disorder Treatment</h3>
<ul>
<li><strong>Authors: </strong>Bradley T. Baker, Mustafa S. Salman, Zening Fu, Armin Iraji, Elizabeth Osuch, Jeremy Bockholt, Vince D. Calhoun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07858">https://arxiv.org/abs/2402.07858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07858">https://arxiv.org/pdf/2402.07858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07858]] Multiscale Neuroimaging Features for the Identification of Medication  Class and Non-Responders in Mood Disorder Treatment(https://arxiv.org/abs/2402.07858)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the clinical treatment of mood disorders, the complex behavioral symptoms presented by patients and variability of patient response to particular medication classes can create difficulties in providing fast and reliable treatment when standard diagnostic and prescription methods are used. Increasingly, the incorporation of physiological information such as neuroimaging scans and derivatives into the clinical process promises to alleviate some of the uncertainty surrounding this process. Particularly, if neural features can help to identify patients who may not respond to standard courses of anti-depressants or mood stabilizers, clinicians may elect to avoid lengthy and side-effect-laden treatments and seek out a different, more effective course that might otherwise not have been under consideration. Previously, approaches for the derivation of relevant neuroimaging features work at only one scale in the data, potentially limiting the depth of information available for clinical decision support. In this work, we show that the utilization of multi spatial scale neuroimaging features - particularly resting state functional networks and functional network connectivity measures - provide a rich and robust basis for the identification of relevant medication class and non-responders in the treatment of mood disorders. We demonstrate that the generated features, along with a novel approach for fast and automated feature selection, can support high accuracy rates in the identification of medication class and non-responders as well as the identification of novel, multi-scale biomarkers.</li>
</ul>

<h3>Title: PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented  Generation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wei Zou, Runpeng Geng, Binghui Wang, Jinyuan Jia</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07867">https://arxiv.org/abs/2402.07867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07867">https://arxiv.org/pdf/2402.07867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07867]] PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented  Generation of Large Language Models(https://arxiv.org/abs/2402.07867)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate those limitations. In particular, given a question, RAG retrieves relevant knowledge from a knowledge database to augment the input of the LLM. For instance, the retrieved knowledge could be a set of top-k texts that are most semantically similar to the given question when the knowledge database contains millions of texts collected from Wikipedia. As a result, the LLM could utilize the retrieved knowledge as the context to generate an answer for the given question. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. Particularly, we propose PoisonedRAG , a set of knowledge poisoning attacks to RAG, where an attacker could inject a few poisoned texts into the knowledge database such that the LLM generates an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge poisoning attacks as an optimization problem, whose solution is a set of poisoned texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on the RAG, we propose two solutions to solve the optimization problem, respectively. Our results on multiple benchmark datasets and LLMs show our attacks could achieve 90% attack success rates when injecting 5 poisoned texts for each target question into a database with millions of texts. We also evaluate recent defenses and our results show they are insufficient to defend against our attacks, highlighting the need for new defenses.</li>
</ul>

<h3>Title: Scaling Laws for Fine-Grained Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pióro, Michał Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Król, Tomasz Odrzygóźdź, Piotr Sankowski, Marek Cygan, Sebastian Jaszczur</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07871">https://arxiv.org/abs/2402.07871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07871">https://arxiv.org/pdf/2402.07871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07871]] Scaling Laws for Fine-Grained Mixture of Experts(https://arxiv.org/abs/2402.07871)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.</li>
</ul>

<h3>Title: Policy Improvement using Language Feedback Models</h3>
<ul>
<li><strong>Authors: </strong>Victor Zhong, Dipendra Misra, Xingdi Yuan, Marc-Alexandre Côté</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07876">https://arxiv.org/abs/2402.07876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07876">https://arxiv.org/pdf/2402.07876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07876]] Policy Improvement using Language Feedback Models(https://arxiv.org/abs/2402.07876)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.</li>
</ul>

<h3>Title: Using Graph Theory for Improving Machine Learning-based Detection of  Cyber Attacks</h3>
<ul>
<li><strong>Authors: </strong>Giacomo Zonneveld, Lorenzo Principi, Marco Baldi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07878">https://arxiv.org/abs/2402.07878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07878">https://arxiv.org/pdf/2402.07878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07878]] Using Graph Theory for Improving Machine Learning-based Detection of  Cyber Attacks(https://arxiv.org/abs/2402.07878)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Early detection of network intrusions and cyber threats is one of the main pillars of cybersecurity. One of the most effective approaches for this purpose is to analyze network traffic with the help of artificial intelligence algorithms, with the aim of detecting the possible presence of an attacker by distinguishing it from a legitimate user. This is commonly done by collecting the traffic exchanged between terminals in a network and analyzing it on a per-packet or per-connection basis. In this paper, we propose instead to perform pre-processing of network traffic under analysis with the aim of extracting some new metrics on which we can perform more efficient detection and overcome some limitations of classical approaches. These new metrics are based on graph theory, and consider the network as a whole, rather than focusing on individual packets or connections. Our approach is validated through experiments performed on publicly available data sets, from which it results that it can not only overcome some of the limitations of classical approaches, but also achieve a better detection capability of cyber threats.</li>
</ul>

<h3>Title: MODIPHY: Multimodal Obscured Detection for IoT using PHantom  Convolution-Enabled Faster YOLO</h3>
<ul>
<li><strong>Authors: </strong>Shubhabrata Mukherjee, Cory Beard, Zhu Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07894">https://arxiv.org/abs/2402.07894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07894">https://arxiv.org/pdf/2402.07894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07894]] MODIPHY: Multimodal Obscured Detection for IoT using PHantom  Convolution-Enabled Faster YOLO(https://arxiv.org/abs/2402.07894)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Low-light conditions and occluded scenarios impede object detection in real-world Internet of Things (IoT) applications like autonomous vehicles and security systems. While advanced machine learning models strive for accuracy, their computational demands clash with the limitations of resource-constrained devices, hampering real-time performance. In our current research, we tackle this challenge, by introducing "YOLO Phantom", one of the smallest YOLO models ever conceived. YOLO Phantom utilizes the novel Phantom Convolution block, achieving comparable accuracy to the latest YOLOv8n model while simultaneously reducing both parameters and model size by 43%, resulting in a significant 19% reduction in Giga Floating Point Operations (GFLOPs). YOLO Phantom leverages transfer learning on our multimodal RGB-infrared dataset to address low-light and occlusion issues, equipping it with robust vision under adverse conditions. Its real-world efficacy is demonstrated on an IoT platform with advanced low-light and RGB cameras, seamlessly connecting to an AWS-based notification endpoint for efficient real-time object detection. Benchmarks reveal a substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB detection, respectively, compared to the baseline YOLOv8n model. For community contribution, both the code and the multimodal dataset are available on GitHub.</li>
</ul>

<h3>Title: Detection of Spider Mites on Labrador Beans through Machine Learning  Approaches Using Custom Datasets</h3>
<ul>
<li><strong>Authors: </strong>Violet Liu, Jason Chen, Ans Qureshi, Mahla Nejati</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07895">https://arxiv.org/abs/2402.07895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07895">https://arxiv.org/pdf/2402.07895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07895]] Detection of Spider Mites on Labrador Beans through Machine Learning  Approaches Using Custom Datasets(https://arxiv.org/abs/2402.07895)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Amidst growing food production demands, early plant disease detection is essential to safeguard crops; this study proposes a visual machine learning approach for plant disease detection, harnessing RGB and NIR data collected in real-world conditions through a JAI FS-1600D-10GE camera to build an RGBN dataset. A two-stage early plant disease detection model with YOLOv8 and a sequential CNN was used to train on a dataset with partial labels, which showed a 3.6% increase in mAP compared to a single-stage end-to-end segmentation model. The sequential CNN model achieved 90.62% validation accuracy utilising RGBN data. An average of 6.25% validation accuracy increase is found using RGBN in classification compared to RGB using ResNet15 and the sequential CNN models. Further research and dataset improvements are needed to meet food production demands.</li>
</ul>

<h3>Title: A systematic investigation of learnability from single child linguistic  input</h3>
<ul>
<li><strong>Authors: </strong>Yulu Qin, Wentao Wang, Brenden M. Lake</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07899">https://arxiv.org/abs/2402.07899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07899">https://arxiv.org/pdf/2402.07899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07899]] A systematic investigation of learnability from single child linguistic  input(https://arxiv.org/abs/2402.07899)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematically train six different model architectures on five datasets (3 single-child and 2 baselines). We find that the models trained on single-child datasets showed consistent results that matched with previous work, underscoring the robustness of forming meaningful syntactic and semantic representations from a subset of a child's linguistic input.</li>
</ul>

<h3>Title: FAST: Factorizable Attention for Speeding up Transformers</h3>
<ul>
<li><strong>Authors: </strong>Armin Gerami, Monte Hoover, Pranav S. Dulepet, Ramani Duraiswami</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07901">https://arxiv.org/abs/2402.07901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07901">https://arxiv.org/pdf/2402.07901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07901]] FAST: Factorizable Attention for Speeding up Transformers(https://arxiv.org/abs/2402.07901)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Motivated by the factorization inherent in the original fast multipole method and the improved fast Gauss transform we introduce a factorable form of attention that operates efficiently in high dimensions. This approach reduces the computational and memory complexity of the attention mechanism in transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our work presents a linearly scaled attention mechanism that maintains the full representation of the attention matrix without compromising on sparsification and incorporates the all-to-all relationship between tokens. We explore the properties of our new attention metric and conduct tests in various standard settings. Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where self-attention is used.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
