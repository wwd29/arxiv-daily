<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Beyond Detection: Visual Realism Assessment of Deepfakes. (arXiv:2306.05985v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05985">http://arxiv.org/abs/2306.05985</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05985] Beyond Detection: Visual Realism Assessment of Deepfakes](http://arxiv.org/abs/2306.05985) #secure</code></li>
<li>Summary: <p>In the era of rapid digitalization and artificial intelligence advancements,
the development of DeepFake technology has posed significant security and
privacy concerns. This paper presents an effective measure to assess the visual
realism of DeepFake videos. We utilize an ensemble of two Convolutional Neural
Network (CNN) models: Eva and ConvNext. These models have been trained on the
DeepFake Game Competition (DFGC) 2022 dataset and aim to predict Mean Opinion
Scores (MOS) from DeepFake videos based on features extracted from sequences of
frames. Our method secured the third place in the recent DFGC on Visual Realism
Assessment held in conjunction with the 2023 International Joint Conference on
Biometrics (IJCB 2023). We provide an over-view of the models, data
preprocessing, and training procedures. We also report the performance of our
models against the competition's baseline model and discuss the implications of
our findings.
</p></li>
</ul>

<h3>Title: McFIL: Model Counting Functionality-Inherent Leakage. (arXiv:2306.05633v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05633">http://arxiv.org/abs/2306.05633</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05633] McFIL: Model Counting Functionality-Inherent Leakage](http://arxiv.org/abs/2306.05633) #secure</code></li>
<li>Summary: <p>Protecting the confidentiality of private data and using it for useful
collaboration have long been at odds. Modern cryptography is bridging this gap
through rapid growth in secure protocols such as multi-party computation,
fully-homomorphic encryption, and zero-knowledge proofs. However, even with
provable indistinguishability or zero-knowledgeness, confidentiality loss from
leakage inherent to the functionality may partially or even completely
compromise secret values without ever falsifying proofs of security. In this
work, we describe McFIL, an algorithmic approach and accompanying software
implementation which automatically quantifies intrinsic leakage for a given
functionality. Extending and generalizing the Chosen-Ciphertext attack
framework of Beck et al. with a practical heuristic, our approach not only
quantifies but maximizes functionality-inherent leakage using Maximum Model
Counting within a SAT solver. As a result, McFIL automatically derives
approximately-optimal adversary inputs that, when used in secure protocols,
maximize information leakage of private values.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: FuncTeller: How Well Does eFPGA Hide Functionality?. (arXiv:2306.05532v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05532">http://arxiv.org/abs/2306.05532</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05532] FuncTeller: How Well Does eFPGA Hide Functionality?](http://arxiv.org/abs/2306.05532) #security</code></li>
<li>Summary: <p>Hardware intellectual property (IP) piracy is an emerging threat to the
global supply chain. Correspondingly, various countermeasures aim to protect
hardware IPs, such as logic locking, camouflaging, and split manufacturing.
However, these countermeasures cannot always guarantee IP security. A malicious
attacker can access the layout/netlist of the hardware IP protected by these
countermeasures and further retrieve the design. To eliminate/bypass these
vulnerabilities, a recent approach redacts the design's IP to an embedded
field-programmable gate array (eFPGA), disabling the attacker's access to the
layout/netlist. eFPGAs can be programmed with arbitrary functionality. Without
the bitstream, the attacker cannot recover the functionality of the protected
IP. Consequently, state-of-the-art attacks are inapplicable to pirate the
redacted hardware IP. In this paper, we challenge the assumed security of
eFPGA-based redaction. We present an attack to retrieve the hardware IP with
only black-box access to a programmed eFPGA. We observe the effect of modern
electronic design automation (EDA) tools on practical hardware circuits and
leverage the observation to guide our attack. Thus, our proposed method
FuncTeller selects minterms to query, recovering the circuit function within a
reasonable time. We demonstrate the effectiveness and efficiency of FuncTeller
on multiple circuits, including academic benchmark circuits, Stanford MIPS
processor, IBEX processor, Common Evaluation Platform GPS, and Cybersecurity
Awareness Worldwide competition circuits. Our results show that FuncTeller
achieves an average accuracy greater than 85% over these tested circuits
retrieving the design's functionality.
</p></li>
</ul>

<h3>Title: Detecting Phishing Sites Using ChatGPT. (arXiv:2306.05816v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05816">http://arxiv.org/abs/2306.05816</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05816] Detecting Phishing Sites Using ChatGPT](http://arxiv.org/abs/2306.05816) #security</code></li>
<li>Summary: <p>The rise of large language models (LLMs) has had a significant impact on
various domains, including natural language processing and artificial
intelligence. While LLMs such as ChatGPT have been extensively researched for
tasks such as code generation and text synthesis, their application in
detecting malicious web content, particularly phishing sites, has been largely
unexplored. To combat the rising tide of automated cyber attacks facilitated by
LLMs, it is imperative to automate the detection of malicious web content,
which requires approaches that leverage the power of LLMs to analyze and
classify phishing sites. In this paper, we propose a novel method that utilizes
ChatGPT to detect phishing sites. Our approach involves leveraging a web
crawler to gather information from websites and generate prompts based on this
collected data. This approach enables us to detect various phishing sites
without the need for fine-tuning machine learning models and identify social
engineering techniques from the context of entire websites and URLs. To
evaluate the performance of our proposed method, we conducted experiments using
a dataset. The experimental results using GPT-4 demonstrated promising
performance, with a precision of 98.3% and a recall of 98.4%. Comparative
analysis between GPT-3.5 and GPT-4 revealed an enhancement in the latter's
capability to reduce false negatives. These findings not only highlight the
potential of LLMs in efficiently identifying phishing sites but also have
significant implications for enhancing cybersecurity measures and protecting
users from the dangers of online fraudulent activities.
</p></li>
</ul>

<h3>Title: "My sex-related data is more sensitive than my financial data and I want the same level of security and privacy": User Risk Perceptions and Protective Actions in Female-oriented Technologies. (arXiv:2306.05956v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05956">http://arxiv.org/abs/2306.05956</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05956] "My sex-related data is more sensitive than my financial data and I want the same level of security and privacy": User Risk Perceptions and Protective Actions in Female-oriented Technologies](http://arxiv.org/abs/2306.05956) #security</code></li>
<li>Summary: <p>The digitalization of the reproductive body has engaged myriads of
cutting-edge technologies in supporting people to know and tackle their
intimate health. Generally understood as female technologies (aka
female-oriented technologies or 'FemTech'), these products and systems collect
a wide range of intimate data which are processed, transferred, saved and
shared with other parties. In this paper, we explore how the "data-hungry"
nature of this industry and the lack of proper safeguarding mechanisms,
standards, and regulations for vulnerable data can lead to complex harms or
faint agentic potential. We adopted mixed methods in exploring users'
understanding of the security and privacy (SP) of these technologies. Our
findings show that while users can speculate the range of harms and risks
associated with these technologies, they are not equipped and provided with the
technological skills to protect themselves against such risks. We discuss a
number of approaches, including participatory threat modelling and SP by
design, in the context of this work and conclude that such approaches are
critical to protect users in these sensitive systems.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: On the Challenges and Perspectives of Foundation Models for Medical Image Analysis. (arXiv:2306.05705v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05705">http://arxiv.org/abs/2306.05705</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05705] On the Challenges and Perspectives of Foundation Models for Medical Image Analysis](http://arxiv.org/abs/2306.05705) #privacy</code></li>
<li>Summary: <p>This article discusses the opportunities, applications and future directions
of large-scale pre-trained models, i.e., foundation models, for analyzing
medical images. Medical foundation models have immense potential in solving a
wide range of downstream tasks, as they can help to accelerate the development
of accurate and robust models, reduce the large amounts of required labeled
data, preserve the privacy and confidentiality of patient data. Specifically,
we illustrate the "spectrum" of medical foundation models, ranging from general
vision models, modality-specific models, to organ/task-specific models,
highlighting their challenges, opportunities and applications. We also discuss
how foundation models can be leveraged in downstream medical tasks to enhance
the accuracy and efficiency of medical image analysis, leading to more precise
diagnosis and treatment decisions.
</p></li>
</ul>

<h3>Title: Differentially Private Image Classification by Learning Priors from Random Processes. (arXiv:2306.06076v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06076">http://arxiv.org/abs/2306.06076</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06076] Differentially Private Image Classification by Learning Priors from Random Processes](http://arxiv.org/abs/2306.06076) #privacy</code></li>
<li>Summary: <p>In privacy-preserving machine learning, differentially private stochastic
gradient descent (DP-SGD) performs worse than SGD due to per-sample gradient
clipping and noise addition. A recent focus in private learning research is
improving the performance of DP-SGD on private data by incorporating priors
that are learned on real-world public data. In this work, we explore how we can
improve the privacy-utility tradeoff of DP-SGD by learning priors from images
generated by random processes and transferring these priors to private data. We
propose DP-RandP, a three-phase approach. We attain new state-of-the-art
accuracy when training from scratch on CIFAR10, CIFAR100, and MedMNIST for a
range of privacy budgets $\varepsilon \in [1, 8]$. In particular, we improve
the previous best reported accuracy on CIFAR10 from $60.6 \%$ to $72.3 \%$ for
$\varepsilon=1$. Our code is available at
https://github.com/inspire-group/DP-RandP.
</p></li>
</ul>

<h3>Title: ChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery. (arXiv:2306.05552v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05552">http://arxiv.org/abs/2306.05552</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05552] ChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery](http://arxiv.org/abs/2306.05552) #privacy</code></li>
<li>Summary: <p>Large language models have been useful in expanding mental health care
delivery. ChatGPT, in particular, has gained popularity for its ability to
generate human-like dialogue. However, data-sensitive domains -- including but
not limited to healthcare -- face challenges in using ChatGPT due to privacy
and data-ownership concerns. To enable its utilization, we propose a text
ambiguation framework that preserves user privacy. We ground this in the task
of addressing stress prompted by user-provided texts to demonstrate the
viability and helpfulness of privacy-preserved generations. Our results suggest
that chatGPT recommendations are still able to be moderately helpful and
relevant, even when the original user text is not provided.
</p></li>
</ul>

<h3>Title: Privacy- and Utility-Preserving NLP with Anonymized Data: A case study of Pseudonymization. (arXiv:2306.05561v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05561">http://arxiv.org/abs/2306.05561</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05561] Privacy- and Utility-Preserving NLP with Anonymized Data: A case study of Pseudonymization](http://arxiv.org/abs/2306.05561) #privacy</code></li>
<li>Summary: <p>This work investigates the effectiveness of different pseudonymization
techniques, ranging from rule-based substitutions to using pre-trained Large
Language Models (LLMs), on a variety of datasets and models used for two widely
used NLP tasks: text classification and summarization. Our work provides
crucial insights into the gaps between original and anonymized data (focusing
on the pseudonymization technique) and model quality and fosters future
research into higher-quality anonymization techniques to better balance the
trade-offs between data protection and utility preservation. We make our code,
pseudonymized datasets, and downstream models publicly available
</p></li>
</ul>

<h3>Title: Privacy Aware Question-Answering System for Online Mental Health Risk Assessment. (arXiv:2306.05652v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05652">http://arxiv.org/abs/2306.05652</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05652] Privacy Aware Question-Answering System for Online Mental Health Risk Assessment](http://arxiv.org/abs/2306.05652) #privacy</code></li>
<li>Summary: <p>Social media platforms have enabled individuals suffering from mental
illnesses to share their lived experiences and find the online support
necessary to cope. However, many users fail to receive genuine clinical
support, thus exacerbating their symptoms. Screening users based on what they
post online can aid providers in administering targeted healthcare and minimize
false positives. Pre-trained Language Models (LMs) can assess users' social
media data and classify them in terms of their mental health risk. We propose a
Question-Answering (QA) approach to assess mental health risk using the
Unified-QA model on two large mental health datasets. To protect user data, we
extend Unified-QA by anonymizing the model training process using differential
privacy. Our results demonstrate the effectiveness of modeling risk assessment
as a QA task, specifically for mental health use cases. Furthermore, the
model's performance decreases by less than 1% with the inclusion of
differential privacy. The proposed system's performance is indicative of a
promising research direction that will lead to the development of privacy-aware
diagnostic systems.
</p></li>
</ul>

<h3>Title: Differentially Private Sharpness-Aware Training. (arXiv:2306.05651v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05651">http://arxiv.org/abs/2306.05651</a></li>
<li>Code URL: <a href="https://github.com/jinseongp/dpsat">https://github.com/jinseongp/dpsat</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05651] Differentially Private Sharpness-Aware Training](http://arxiv.org/abs/2306.05651) #privacy</code></li>
<li>Summary: <p>Training deep learning models with differential privacy (DP) results in a
degradation of performance. The training dynamics of models with DP show a
significant difference from standard training, whereas understanding the
geometric properties of private learning remains largely unexplored. In this
paper, we investigate sharpness, a key factor in achieving better
generalization, in private learning. We show that flat minima can help reduce
the negative effects of per-example gradient clipping and the addition of
Gaussian noise. We then verify the effectiveness of Sharpness-Aware
Minimization (SAM) for seeking flat minima in private learning. However, we
also discover that SAM is detrimental to the privacy budget and computational
time due to its two-step optimization. Thus, we propose a new sharpness-aware
training method that mitigates the privacy-optimization trade-off. Our
experimental results demonstrate that the proposed method improves the
performance of deep learning models with DP from both scratch and fine-tuning.
Code is available at https://github.com/jinseongP/DPSAT.
</p></li>
</ul>

<h3>Title: DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework. (arXiv:2306.05734v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05734">http://arxiv.org/abs/2306.05734</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05734] DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework](http://arxiv.org/abs/2306.05734) #privacy</code></li>
<li>Summary: <p>Hyperparameter optimization, also known as hyperparameter tuning, is a widely
recognized technique for improving model performance. Regrettably, when
training private ML models, many practitioners often overlook the privacy risks
associated with hyperparameter optimization, which could potentially expose
sensitive information about the underlying dataset. Currently, the sole
existing approach to allow privacy-preserving hyperparameter optimization is to
uniformly and randomly select hyperparameters for a number of runs,
subsequently reporting the best-performing hyperparameter. In contrast, in
non-private settings, practitioners commonly utilize "adaptive" hyperparameter
optimization methods such as Gaussian process-based optimization, which select
the next candidate based on information gathered from previous outputs. This
substantial contrast between private and non-private hyperparameter
optimization underscores a critical concern. In our paper, we introduce
DP-HyPO, a pioneering framework for "adaptive" private hyperparameter
optimization, aiming to bridge the gap between private and non-private
hyperparameter optimization. To accomplish this, we provide a comprehensive
differential privacy analysis of our framework. Furthermore, we empirically
demonstrate the effectiveness of DP-HyPO on a diverse set of real-world and
synthetic datasets.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Overcoming Adversarial Attacks for Human-in-the-Loop Applications. (arXiv:2306.05952v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05952">http://arxiv.org/abs/2306.05952</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05952] Overcoming Adversarial Attacks for Human-in-the-Loop Applications](http://arxiv.org/abs/2306.05952) #attack</code></li>
<li>Summary: <p>Including human analysis has the potential to positively affect the
robustness of Deep Neural Networks and is relatively unexplored in the
Adversarial Machine Learning literature. Neural network visual explanation maps
have been shown to be prone to adversarial attacks. Further research is needed
in order to select robust visualizations of explanations for the image analyst
to evaluate a given model. These factors greatly impact Human-In-The-Loop
(HITL) evaluation tools due to their reliance on adversarial images, including
explanation maps and measurements of robustness. We believe models of human
visual attention may improve interpretability and robustness of human-machine
imagery analysis systems. Our challenge remains, how can HITL evaluation be
robust in this adversarial landscape?
</p></li>
</ul>

<h3>Title: Adversarial Attack On Yolov5 For Traffic And Road Sign Detection. (arXiv:2306.06071v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06071">http://arxiv.org/abs/2306.06071</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06071] Adversarial Attack On Yolov5 For Traffic And Road Sign Detection](http://arxiv.org/abs/2306.06071) #attack</code></li>
<li>Summary: <p>This paper implements and investigates popular adversarial attacks on the
YOLOv5 Object Detection algorithm. The paper explores the vulnerability of the
YOLOv5 to adversarial attacks in the context of traffic and road sign
detection. The paper investigates the impact of different types of attacks,
including the Limited memory Broyden Fletcher Goldfarb Shanno (L-BFGS), the
Fast Gradient Sign Method (FGSM) attack, the Carlini and Wagner (C&amp;W) attack,
the Basic Iterative Method (BIM) attack, the Projected Gradient Descent (PGD)
attack, One Pixel Attack, and the Universal Adversarial Perturbations attack on
the accuracy of YOLOv5 in detecting traffic and road signs. The results show
that YOLOv5 is susceptible to these attacks, with misclassification rates
increasing as the magnitude of the perturbations increases. We also explain the
results using saliency maps. The findings of this paper have important
implications for the safety and reliability of object detection algorithms used
in traffic and transportation systems, highlighting the need for more robust
and secure models to ensure their effectiveness in real-world applications.
</p></li>
</ul>

<h3>Title: CARSO: Counter-Adversarial Recall of Synthetic Observations. (arXiv:2306.06081v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06081">http://arxiv.org/abs/2306.06081</a></li>
<li>Code URL: <a href="https://github.com/emaballarin/CARSO">https://github.com/emaballarin/CARSO</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06081] CARSO: Counter-Adversarial Recall of Synthetic Observations](http://arxiv.org/abs/2306.06081) #attack</code></li>
<li>Summary: <p>In this paper, we propose a novel adversarial defence mechanism for image
classification -- CARSO -- inspired by cues from cognitive neuroscience. The
method is synergistically complementary to adversarial training and relies on
knowledge of the internal representation of the attacked classifier. Exploiting
a generative model for adversarial purification, conditioned on such
representation, it samples reconstructions of inputs to be finally classified.
Experimental evaluation by a well-established benchmark of varied, strong
adaptive attacks, across diverse image datasets and classifier architectures,
shows that CARSO is able to defend the classifier significantly better than
state-of-the-art adversarial training alone -- with a tolerable clean accuracy
toll. Furthermore, the defensive architecture succeeds in effectively shielding
itself from unforeseen threats, and end-to-end attacks adapted to fool
stochastic defences. Code and pre-trained models are available at
https://github.com/emaballarin/CARSO .
</p></li>
</ul>

<h3>Title: Prompt Injection attack against LLM-integrated Applications. (arXiv:2306.05499v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05499">http://arxiv.org/abs/2306.05499</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05499] Prompt Injection attack against LLM-integrated Applications](http://arxiv.org/abs/2306.05499) #attack</code></li>
<li>Summary: <p>Large Language Models (LLMs), renowned for their superior proficiency in
language comprehension and generation, stimulate a vibrant ecosystem of
applications around them. However, their extensive assimilation into various
services introduces significant security risks. This study deconstructs the
complexities and implications of prompt injection attacks on actual
LLM-integrated applications. Initially, we conduct an exploratory analysis on
ten commercial applications, highlighting the constraints of current attack
strategies in practice. Prompted by these limitations, we subsequently
formulate HouYi, a novel black-box prompt injection attack technique, which
draws inspiration from traditional web injection attacks. HouYi is
compartmentalized into three crucial elements: a seamlessly-incorporated
pre-constructed prompt, an injection prompt inducing context partition, and a
malicious payload designed to fulfill the attack objectives. Leveraging HouYi,
we unveil previously unknown and severe attack outcomes, such as unrestricted
arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi
on 36 actual LLM-integrated applications and discern 31 applications
susceptible to prompt injection. 10 vendors have validated our discoveries,
including Notion, which has the potential to impact millions of users. Our
investigation illuminates both the possible risks of prompt injection attacks
and the possible tactics for mitigation.
</p></li>
</ul>

<h3>Title: COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models. (arXiv:2306.05659v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05659">http://arxiv.org/abs/2306.05659</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05659] COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models](http://arxiv.org/abs/2306.05659) #attack</code></li>
<li>Summary: <p>Prompt-based learning has been proved to be an effective way in pre-trained
language models (PLMs), especially in low-resource scenarios like few-shot
settings. However, the trustworthiness of PLMs is of paramount significance and
potential vulnerabilities have been shown in prompt-based templates that could
mislead the predictions of language models, causing serious security concerns.
In this paper, we will shed light on some vulnerabilities of PLMs, by proposing
a prompt-based adversarial attack on manual templates in black box scenarios.
First of all, we design character-level and word-level heuristic approaches to
break manual templates separately. Then we present a greedy algorithm for the
attack based on the above heuristic destructive approaches. Finally, we
evaluate our approach with the classification tasks on three variants of BERT
series models and eight datasets. And comprehensive experimental results
justify the effectiveness of our approach in terms of attack success rate and
attack speed. Further experimental studies indicate that our proposed method
also displays good capabilities in scenarios with varying shot counts, template
lengths and query counts, exhibiting good generalizability.
</p></li>
</ul>

<h3>Title: Adversarial Evasion Attacks Practicality in Networks: Testing the Impact of Dynamic Learning. (arXiv:2306.05494v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05494">http://arxiv.org/abs/2306.05494</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05494] Adversarial Evasion Attacks Practicality in Networks: Testing the Impact of Dynamic Learning](http://arxiv.org/abs/2306.05494) #attack</code></li>
<li>Summary: <p>Machine Learning (ML) has become ubiquitous, and its deployment in Network
Intrusion Detection Systems (NIDS) is inevitable due to its automated nature
and high accuracy in processing and classifying large volumes of data. However,
ML has been found to have several flaws, on top of them are adversarial
attacks, which aim to trick ML models into producing faulty predictions. While
most adversarial attack research focuses on computer vision datasets, recent
studies have explored the practicality of such attacks against ML-based network
security entities, especially NIDS.
</p></li>
</ul>

<p>This paper presents two distinct contributions: a taxonomy of practicality
issues associated with adversarial attacks against ML-based NIDS and an
investigation of the impact of continuous training on adversarial attacks
against NIDS. Our experiments indicate that continuous re-training, even
without adversarial training, can reduce the effect of adversarial attacks.
While adversarial attacks can harm ML-based NIDSs, our aim is to highlight that
there is a significant gap between research and real-world practicality in this
domain which requires attention.
</p>

<h3>Title: Cross-Consensus Measurement of Individual-level Decentralization in Blockchains. (arXiv:2306.05788v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05788">http://arxiv.org/abs/2306.05788</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05788] Cross-Consensus Measurement of Individual-level Decentralization in Blockchains](http://arxiv.org/abs/2306.05788) #attack</code></li>
<li>Summary: <p>Decentralization is widely recognized as a crucial characteristic of
blockchains that enables them to resist malicious attacks such as the 51%
attack and the takeover attack. Prior research has primarily examined
decentralization in blockchains employing the same consensus protocol or at the
level of block producers. This paper presents the first individual-level
measurement study comparing the decentralization of blockchains employing
different consensus protocols. To facilitate cross-consensus evaluation, we
present a two-level comparison framework and a new metric. We apply the
proposed methods to Ethereum and Steem, two representative blockchains for
which decentralization has garnered considerable interest. Our findings dive
deeper into the level of decentralization, suggest the existence of
centralization risk at the individual level in Steem, and provide novel
insights into the cross-consensus comparison of decentralization in
blockchains.
</p></li>
</ul>

<h3>Title: GAN-CAN: A Novel Attack to Behavior-Based Driver Authentication Systems. (arXiv:2306.05923v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05923">http://arxiv.org/abs/2306.05923</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05923] GAN-CAN: A Novel Attack to Behavior-Based Driver Authentication Systems](http://arxiv.org/abs/2306.05923) #attack</code></li>
<li>Summary: <p>For many years, car keys have been the sole mean of authentication in
vehicles. Whether the access control process is physical or wireless,
entrusting the ownership of a vehicle to a single token is prone to stealing
attempts. For this reason, many researchers started developing behavior-based
authentication systems. By collecting data in a moving vehicle, Deep Learning
(DL) models can recognize patterns in the data and identify drivers based on
their driving behavior. This can be used as an anti-theft system, as a thief
would exhibit a different driving style compared to the vehicle owner's.
However, the assumption that an attacker cannot replicate the legitimate driver
behavior falls under certain conditions.
</p></li>
</ul>

<p>In this paper, we propose GAN-CAN, the first attack capable of fooling
state-of-the-art behavior-based driver authentication systems in a vehicle.
Based on the adversary's knowledge, we propose different GAN-CAN
implementations. Our attack leverages the lack of security in the Controller
Area Network (CAN) to inject suitably designed time-series data to mimic the
legitimate driver. Our design of the malicious time series results from the
combination of different Generative Adversarial Networks (GANs) and our study
on the safety importance of the injected values during the attack. We tested
GAN-CAN in an improved version of the most efficient driver behavior-based
authentication model in the literature. We prove that our attack can fool it
with an attack success rate of up to 0.99. We show how an attacker, without
prior knowledge of the authentication system, can steal a car by deploying
GAN-CAN in an off-the-shelf system in under 22 minutes.
</p>

<h2>robust</h2>
<h3>Title: Is Attentional Channel Processing Design Required? Comprehensive Analysis Of Robustness Between Vision Transformers And Fully Attentional Networks. (arXiv:2306.05495v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05495">http://arxiv.org/abs/2306.05495</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05495] Is Attentional Channel Processing Design Required? Comprehensive Analysis Of Robustness Between Vision Transformers And Fully Attentional Networks](http://arxiv.org/abs/2306.05495) #robust</code></li>
<li>Summary: <p>The robustness testing has been performed for standard CNN models and Vision
Transformers, however there is a lack of comprehensive study between the
robustness of traditional Vision Transformers without an extra attentional
channel design and the latest fully attentional network(FAN) models. So in this
paper, we use the ImageNet dataset to compare the robustness of fully
attentional network(FAN) models with traditional Vision Transformers to
understand the role of an attentional channel processing design using white box
attacks and also study the transferability between the same using black box
attacks.
</p></li>
</ul>

<h3>Title: Spatial Re-parameterization for N:M Sparsity. (arXiv:2306.05612v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05612">http://arxiv.org/abs/2306.05612</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05612] Spatial Re-parameterization for N:M Sparsity](http://arxiv.org/abs/2306.05612) #robust</code></li>
<li>Summary: <p>This paper presents a Spatial Re-parameterization (SpRe) method for the N:M
sparsity in CNNs. SpRe is stemmed from an observation regarding the restricted
variety in spatial sparsity present in N:M sparsity compared with unstructured
sparsity. Particularly, N:M sparsity exhibits a fixed sparsity rate within the
spatial domains due to its distinctive pattern that mandates N non-zero
components among M successive weights in the input channel dimension of
convolution filters. On the contrary, we observe that unstructured sparsity
displays a substantial divergence in sparsity across the spatial domains, which
we experimentally verified to be very crucial for its robust performance
retention compared with N:M sparsity. Therefore, SpRe employs the
spatial-sparsity distribution of unstructured sparsity to assign an extra
branch in conjunction with the original N:M branch at training time, which
allows the N:M sparse network to sustain a similar distribution of spatial
sparsity with unstructured sparsity. During inference, the extra branch can be
further re-parameterized into the main N:M branch, without exerting any
distortion on the sparse pattern or additional computation costs. SpRe has
achieved a commendable feat by matching the performance of N:M sparsity methods
with state-of-the-art unstructured sparsity methods across various benchmarks.
Code and models are anonymously available at
\url{https://github.com/zyxxmu/SpRe}.
</p></li>
</ul>

<h3>Title: Learning Domain-Aware Detection Head with Prompt Tuning. (arXiv:2306.05718v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05718">http://arxiv.org/abs/2306.05718</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05718] Learning Domain-Aware Detection Head with Prompt Tuning](http://arxiv.org/abs/2306.05718) #robust</code></li>
<li>Summary: <p>Domain adaptive object detection (DAOD) aims to generalize detectors trained
on an annotated source domain to an unlabelled target domain. However, existing
methods focus on reducing the domain bias of the detection backbone by
inferring a discriminative visual encoder, while ignoring the domain bias in
the detection head. Inspired by the high generalization of vision-language
models (VLMs), applying a VLM as the robust detection backbone following a
domain-aware detection head is a reasonable way to learn the discriminative
detector for each domain, rather than reducing the domain bias in traditional
methods. To achieve the above issue, we thus propose a novel DAOD framework
named Domain-Aware detection head with Prompt tuning (DA-Pro), which applies
the learnable domain-adaptive prompt to generate the dynamic detection head for
each domain. Formally, the domain-adaptive prompt consists of the
domain-invariant tokens, domain-specific tokens, and the domain-related textual
description along with the class label. Furthermore, two constraints between
the source and target domains are applied to ensure that the domain-adaptive
prompt can capture the domains-shared and domain-specific knowledge. A prompt
ensemble strategy is also proposed to reduce the effect of prompt disturbance.
Comprehensive experiments over multiple cross-domain adaptation tasks
demonstrate that using the domain-adaptive prompt can produce an effectively
domain-related detection head for boosting domain-adaptive object detection.
</p></li>
</ul>

<h3>Title: DocAligner: Annotating Real-world Photographic Document Images by Simply Taking Pictures. (arXiv:2306.05749v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05749">http://arxiv.org/abs/2306.05749</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05749] DocAligner: Annotating Real-world Photographic Document Images by Simply Taking Pictures](http://arxiv.org/abs/2306.05749) #robust</code></li>
<li>Summary: <p>Recently, there has been a growing interest in research concerning document
image analysis and recognition in photographic scenarios. However, the lack of
labeled datasets for this emerging challenge poses a significant obstacle, as
manual annotation can be time-consuming and impractical. To tackle this issue,
we present DocAligner, a novel method that streamlines the manual annotation
process to a simple step of taking pictures. DocAligner achieves this by
establishing dense correspondence between photographic document images and
their clean counterparts. It enables the automatic transfer of existing
annotations in clean document images to photographic ones and helps to
automatically acquire labels that are unavailable through manual labeling.
Considering the distinctive characteristics of document images, DocAligner
incorporates several innovative features. First, we propose a non-rigid
pre-alignment technique based on the document's edges, which effectively
eliminates interference caused by significant global shifts and repetitive
patterns present in document images. Second, to handle large shifts and ensure
high accuracy, we introduce a hierarchical aligning approach that combines
global and local correlation layers. Furthermore, considering the importance of
fine-grained elements in document images, we present a details recurrent
refinement module to enhance the output in a high-resolution space. To train
DocAligner, we construct a synthetic dataset and introduce a self-supervised
learning approach to enhance its robustness for real-world data. Through
extensive experiments, we demonstrate the effectiveness of DocAligner and the
acquired dataset. Datasets and codes will be publicly available.
</p></li>
</ul>

<h3>Title: Adaptive Contextual Perception: How to Generalize to New Backgrounds and Ambiguous Objects. (arXiv:2306.05963v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05963">http://arxiv.org/abs/2306.05963</a></li>
<li>Code URL: <a href="https://github.com/zfying/adaptivecontext">https://github.com/zfying/adaptivecontext</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05963] Adaptive Contextual Perception: How to Generalize to New Backgrounds and Ambiguous Objects](http://arxiv.org/abs/2306.05963) #robust</code></li>
<li>Summary: <p>Biological vision systems make adaptive use of context to recognize objects
in new settings with novel contexts as well as occluded or blurry objects in
familiar settings. In this paper, we investigate how vision models adaptively
use context for out-of-distribution (OOD) generalization and leverage our
analysis results to improve model OOD generalization. First, we formulate two
distinct OOD settings where the contexts are either irrelevant
(Background-Invariance) or beneficial (Object-Disambiguation), reflecting the
diverse contextual challenges faced in biological vision. We then analyze model
performance in these two different OOD settings and demonstrate that models
that excel in one setting tend to struggle in the other. Notably, prior works
on learning causal features improve on one setting but hurt in the other. This
underscores the importance of generalizing across both OOD settings, as this
ability is crucial for both human cognition and robust AI systems. Next, to
better understand the model properties contributing to OOD generalization, we
use representational geometry analysis and our own probing methods to examine a
population of models, and we discover that those with more factorized
representations and appropriate feature weighting are more successful in
handling Background-Invariance and Object-Disambiguation tests. We further
validate these findings through causal intervention on representation
factorization and feature weighting to demonstrate their causal effect on
performance. Lastly, we propose new augmentation methods to enhance model
generalization. These methods outperform strong baselines, yielding
improvements in both in-distribution and OOD tests. In conclusion, to replicate
the generalization abilities of biological vision, computer vision models must
have factorized object vs. background representations and appropriately weight
both kinds of features.
</p></li>
</ul>

<h3>Title: Gemtelligence: Accelerating Gemstone classification with Deep Learning. (arXiv:2306.06069v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06069">http://arxiv.org/abs/2306.06069</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06069] Gemtelligence: Accelerating Gemstone classification with Deep Learning](http://arxiv.org/abs/2306.06069) #robust</code></li>
<li>Summary: <p>The value of luxury goods, particularly investment-grade gemstones, is
greatly influenced by their origin and authenticity, sometimes resulting in
differences worth millions of dollars. Traditionally, human experts have
determined the origin and detected treatments on gemstones through visual
inspections and a range of analytical methods. However, the interpretation of
the data can be subjective and time-consuming, resulting in inconsistencies. In
this study, we propose Gemtelligence, a novel approach based on deep learning
that enables accurate and consistent origin determination and treatment
detection. Gemtelligence comprises convolutional and attention-based neural
networks that process heterogeneous data types collected by multiple
instruments. Notably, the algorithm demonstrated comparable predictive
performance to expensive laser-ablation inductively-coupled-plasma
mass-spectrometry (ICP-MS) analysis and visual examination by human experts,
despite using input data from relatively inexpensive analytical methods. Our
innovative methodology represents a major breakthrough in the field of gemstone
analysis by significantly improving the automation and robustness of the entire
analytical process pipeline.
</p></li>
</ul>

<h3>Title: DeepSeaNet: Improving Underwater Object Detection using EfficientDet. (arXiv:2306.06075v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06075">http://arxiv.org/abs/2306.06075</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06075] DeepSeaNet: Improving Underwater Object Detection using EfficientDet](http://arxiv.org/abs/2306.06075) #robust</code></li>
<li>Summary: <p>Marine animals and deep underwater objects are difficult to recognize and
monitor for safety of aquatic life. There is an increasing challenge when the
water is saline with granular particles and impurities. In such natural
adversarial environment, traditional approaches like CNN start to fail and are
expensive to compute. This project involves implementing and evaluating various
object detection models, including EfficientDet, YOLOv5, YOLOv8, and
Detectron2, on an existing annotated underwater dataset, called the
Brackish-Dataset. The dataset comprises annotated image sequences of fish,
crabs, starfish, and other aquatic animals captured in Limfjorden water with
limited visibility. The aim of this research project is to study the efficiency
of newer models on the same dataset and contrast them with the previous results
based on accuracy and inference time. Firstly, I compare the results of YOLOv3
(31.10% mean Average Precision (mAP)), YOLOv4 (83.72% mAP), YOLOv5 (97.6%),
YOLOv8 (98.20%), EfficientDet (98.56% mAP) and Detectron2 (95.20% mAP) on the
same dataset. Secondly, I provide a modified BiSkFPN mechanism (BiFPN neck with
skip connections) to perform complex feature fusion in adversarial noise which
makes modified EfficientDet robust to perturbations. Third, analyzed the effect
on accuracy of EfficientDet (98.63% mAP) and YOLOv5 by adversarial learning
(98.04% mAP). Last, I provide class activation map based explanations (CAM) for
the two models to promote Explainability in black box models. Overall, the
results indicate that modified EfficientDet achieved higher accuracy with
five-fold cross validation than the other models with 88.54% IoU of feature
maps.
</p></li>
</ul>

<h3>Title: Augmentation-aware Self-supervised Learning with Guided Projector. (arXiv:2306.06082v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06082">http://arxiv.org/abs/2306.06082</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06082] Augmentation-aware Self-supervised Learning with Guided Projector](http://arxiv.org/abs/2306.06082) #robust</code></li>
<li>Summary: <p>Self-supervised learning (SSL) is a powerful technique for learning robust
representations from unlabeled data. By learning to remain invariant to applied
data augmentations, methods such as SimCLR and MoCo are able to reach quality
on par with supervised approaches. However, this invariance may be harmful to
solving some downstream tasks which depend on traits affected by augmentations
used during pretraining, such as color. In this paper, we propose to foster
sensitivity to such characteristics in the representation space by modifying
the projector network, a common component of self-supervised architectures.
Specifically, we supplement the projector with information about augmentations
applied to images. In order for the projector to take advantage of this
auxiliary guidance when solving the SSL task, the feature extractor learns to
preserve the augmentation information in its representations. Our approach,
coined Conditional Augmentation-aware Selfsupervised Learning (CASSLE), is
directly applicable to typical joint-embedding SSL methods regardless of their
objective functions. Moreover, it does not require major changes in the network
architecture or prior knowledge of downstream tasks. In addition to an analysis
of sensitivity towards different data augmentations, we conduct a series of
experiments, which show that CASSLE improves over various SSL methods, reaching
state-of-the-art performance in multiple downstream tasks.
</p></li>
</ul>

<h3>Title: Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect?. (arXiv:2306.05871v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05871">http://arxiv.org/abs/2306.05871</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05871] Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect?](http://arxiv.org/abs/2306.05871) #robust</code></li>
<li>Summary: <p>Recent advances in natural language processing (NLP) have led to the
development of large language models (LLMs) such as ChatGPT. This paper
proposes a methodology for developing and evaluating ChatGPT detectors for
French text, with a focus on investigating their robustness on out-of-domain
data and against common attack schemes. The proposed method involves
translating an English dataset into French and training a classifier on the
translated data. Results show that the detectors can effectively detect
ChatGPT-generated text, with a degree of robustness against basic attack
techniques in in-domain settings. However, vulnerabilities are evident in
out-of-domain contexts, highlighting the challenge of detecting adversarial
text. The study emphasizes caution when applying in-domain testing results to a
wider variety of content. We provide our translated datasets and models as
open-source resources. https://gitlab.inria.fr/wantoun/robust-chatgpt-detection
</p></li>
</ul>

<h3>Title: Detecting Adversarial Directions in Deep Reinforcement Learning to Make Robust Decisions. (arXiv:2306.05873v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05873">http://arxiv.org/abs/2306.05873</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05873] Detecting Adversarial Directions in Deep Reinforcement Learning to Make Robust Decisions](http://arxiv.org/abs/2306.05873) #robust</code></li>
<li>Summary: <p>Learning in MDPs with highly complex state representations is currently
possible due to multiple advancements in reinforcement learning algorithm
design. However, this incline in complexity, and furthermore the increase in
the dimensions of the observation came at the cost of volatility that can be
taken advantage of via adversarial attacks (i.e. moving along worst-case
directions in the observation space). To solve this policy instability problem
we propose a novel method to detect the presence of these non-robust directions
via local quadratic approximation of the deep neural policy loss. Our method
provides a theoretical basis for the fundamental cut-off between safe
observations and adversarial observations. Furthermore, our technique is
computationally efficient, and does not depend on the methods used to produce
the worst-case directions. We conduct extensive experiments in the Arcade
Learning Environment with several different adversarial attack techniques. Most
significantly, we demonstrate the effectiveness of our approach even in the
setting where non-robust directions are explicitly optimized to circumvent our
proposed method.
</p></li>
</ul>

<h3>Title: Reevaluating Loss Functions: Enhancing Robustness to Label Noise in Deep Learning Models. (arXiv:2306.05497v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05497">http://arxiv.org/abs/2306.05497</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05497] Reevaluating Loss Functions: Enhancing Robustness to Label Noise in Deep Learning Models](http://arxiv.org/abs/2306.05497) #robust</code></li>
<li>Summary: <p>Large annotated datasets inevitably contain incorrect labels, which poses a
major challenge for the training of deep neural networks as they easily fit the
labels. Only when training with a robust model that is not easily distracted by
the noise, a good generalization performance can be achieved. A simple yet
effective way to create a noise robust model is to use a noise robust loss
function. However, the number of proposed loss functions is large, they often
come with hyperparameters, and may learn slower than the widely used but noise
sensitive Cross Entropy loss. By heuristic considerations and extensive
numerical experiments, we study in which situations the proposed loss functions
are applicable and give suggestions on how to choose an appropriate loss.
Additionally, we propose a novel technique to enhance learning with bounded
loss functions: the inclusion of an output bias, i.e. a slight increase in the
neuron pre-activation corresponding to the correct label. Surprisingly, we find
that this not only significantly improves the learning of bounded losses, but
also leads to the Mean Absolute Error loss outperforming the Cross Entropy loss
on the Cifar-100 dataset - even in the absence of additional label noise. This
suggests that training with a bounded loss function can be advantageous even in
the presence of minimal label noise. To further strengthen our analysis of the
learning behavior of different loss functions, we additionally design and test
a novel loss function denoted as Bounded Cross Entropy.
</p></li>
</ul>

<h3>Title: AMEE: A Robust Framework for Explanation Evaluation in Time Series Classification. (arXiv:2306.05501v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05501">http://arxiv.org/abs/2306.05501</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05501] AMEE: A Robust Framework for Explanation Evaluation in Time Series Classification](http://arxiv.org/abs/2306.05501) #robust</code></li>
<li>Summary: <p>This paper aims to provide a framework to quantitatively evaluate and rank
explanation methods for the time series classification task, which deals with a
prevalent data type in critical domains such as healthcare and finance. The
recent surge of research interest in explanation methods for time series
classification has provided a great variety of explanation techniques.
Nevertheless, when these explanation techniques disagree on a specific problem,
it remains unclear which of them to use. Comparing the explanations to find the
right answer is non-trivial. Two key challenges remain: how to quantitatively
and robustly evaluate the informativeness (i.e., relevance for the
classification task) of a given explanation method, and how to compare
explanation methods side-by-side. We propose AMEE, a Model-Agnostic Explanation
Evaluation framework for quantifying and comparing multiple saliency-based
explanations for time series classification. Perturbation is added to the input
time series guided by the saliency maps (i.e., importance weights for each
point in the time series). The impact of perturbation on classification
accuracy is measured and used for explanation evaluation. The results show that
perturbing discriminative parts of the time series leads to significant changes
in classification accuracy. To be robust to different types of perturbations
and different types of classifiers, we aggregate the accuracy loss across
perturbations and classifiers. This allows us to objectively quantify and rank
different explanation methods. We provide a quantitative and qualitative
analysis for synthetic datasets, a variety of UCR benchmark datasets, as well
as a real-world dataset with known expert ground truth.
</p></li>
</ul>

<h3>Title: Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with Heterogeneous Rewards. (arXiv:2306.05579v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05579">http://arxiv.org/abs/2306.05579</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05579] Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with Heterogeneous Rewards](http://arxiv.org/abs/2306.05579) #robust</code></li>
<li>Summary: <p>We study a decentralized multi-agent multi-armed bandit problem in which
multiple clients are connected by time dependent random graphs provided by an
environment. The reward distributions of each arm vary across clients and
rewards are generated independently over time by an environment based on
distributions that include both sub-exponential and sub-gaussian distributions.
Each client pulls an arm and communicates with neighbors based on the graph
provided by the environment. The goal is to minimize the overall regret of the
entire system through collaborations. To this end, we introduce a novel
algorithmic framework, which first provides robust simulation methods for
generating random graphs using rapidly mixing Markov chains or the random graph
model, and then combines an averaging-based consensus approach with a newly
proposed weighting technique and the upper confidence bound to deliver a
UCB-type solution. Our algorithms account for the randomness in the graphs,
removing the conventional doubly stochasticity assumption, and only require the
knowledge of the number of clients at initialization. We derive optimal
instance-dependent regret upper bounds of order $\log{T}$ in both sub-gaussian
and sub-exponential environments, and a nearly optimal mean-gap independent
regret upper bound of order $\sqrt{T}\log T$ up to a $\log T$ factor.
Importantly, our regret bounds hold with high probability and capture graph
randomness, whereas prior works consider expected regret under assumptions and
require more stringent reward distributions.
</p></li>
</ul>

<h3>Title: Extending Kernel PCA through Dualization: Sparsity, Robustness and Fast Algorithms. (arXiv:2306.05815v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05815">http://arxiv.org/abs/2306.05815</a></li>
<li>Code URL: <a href="https://github.com/taralloc/dc-kpca">https://github.com/taralloc/dc-kpca</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05815] Extending Kernel PCA through Dualization: Sparsity, Robustness and Fast Algorithms](http://arxiv.org/abs/2306.05815) #robust</code></li>
<li>Summary: <p>The goal of this paper is to revisit Kernel Principal Component Analysis
(KPCA) through dualization of a difference of convex functions. This allows to
naturally extend KPCA to multiple objective functions and leads to efficient
gradient-based algorithms avoiding the expensive SVD of the Gram matrix.
Particularly, we consider objective functions that can be written as Moreau
envelopes, demonstrating how to promote robustness and sparsity within the same
framework. The proposed method is evaluated on synthetic and real-world
benchmarks, showing significant speedup in KPCA training time as well as
highlighting the benefits in terms of robustness and sparsity.
</p></li>
</ul>

<h3>Title: Robust Reinforcement Learning via Adversarial Kernel Approximation. (arXiv:2306.05859v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05859">http://arxiv.org/abs/2306.05859</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05859] Robust Reinforcement Learning via Adversarial Kernel Approximation](http://arxiv.org/abs/2306.05859) #robust</code></li>
<li>Summary: <p>Robust Markov Decision Processes (RMDPs) provide a framework for sequential
decision-making that is robust to perturbations on the transition kernel.
However, robust reinforcement learning (RL) approaches in RMDPs do not scale
well to realistic online settings with high-dimensional domains. By
characterizing the adversarial kernel in RMDPs, we propose a novel approach for
online robust RL that approximates the adversarial kernel and uses a standard
(non-robust) RL algorithm to learn a robust policy. Notably, our approach can
be applied on top of any underlying RL algorithm, enabling easy scaling to
high-dimensional domains. Experiments in classic control tasks, MinAtar and
DeepMind Control Suite demonstrate the effectiveness and the applicability of
our method.
</p></li>
</ul>

<h3>Title: A Dynamical Graph Prior for Relational Inference. (arXiv:2306.06041v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06041">http://arxiv.org/abs/2306.06041</a></li>
<li>Code URL: <a href="https://github.com/dadacheng/dygr">https://github.com/dadacheng/dygr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06041] A Dynamical Graph Prior for Relational Inference](http://arxiv.org/abs/2306.06041) #robust</code></li>
<li>Summary: <p>Relational inference aims to identify interactions between parts of a
dynamical system from the observed dynamics. Current state-of-the-art methods
fit a graph neural network (GNN) on a learnable graph to the dynamics. They use
one-step message-passing GNNs -- intuitively the right choice since
non-locality of multi-step or spectral GNNs may confuse direct and indirect
interactions. But the \textit{effective} interaction graph depends on the
sampling rate and it is rarely localized to direct neighbors, leading to local
minima for the one-step model. In this work, we propose a \textit{dynamical
graph prior} (DYGR) for relational inference. The reason we call it a prior is
that, contrary to established practice, it constructively uses error
amplification in high-degree non-local polynomial filters to generate good
gradients for graph learning. To deal with non-uniqueness, DYGR simultaneously
fits a ``shallow'' one-step model with shared graph topology. Experiments show
that DYGR reconstructs graphs far more accurately than earlier methods, with
remarkable robustness to under-sampling. Since appropriate sampling rates for
unknown dynamical systems are not known a priori, this robustness makes DYGR
suitable for real applications in scientific machine learning.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: DeepStay: Stay Region Extraction from Location Trajectories using Weak Supervision. (arXiv:2306.06068v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06068">http://arxiv.org/abs/2306.06068</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06068] DeepStay: Stay Region Extraction from Location Trajectories using Weak Supervision](http://arxiv.org/abs/2306.06068) #extraction</code></li>
<li>Summary: <p>Nowadays, mobile devices enable constant tracking of the user's position and
location trajectories can be used to infer personal points of interest (POIs)
like homes, workplaces, or stores. A common way to extract POIs is to first
identify spatio-temporal regions where a user spends a significant amount of
time, known as stay regions (SRs).
</p></li>
</ul>

<p>Common approaches to SR extraction are evaluated either solely unsupervised
or on a small-scale private dataset, as popular public datasets are unlabeled.
Most of these methods rely on hand-crafted features or thresholds and do not
learn beyond hyperparameter optimization. Therefore, we propose a weakly and
self-supervised transformer-based model called DeepStay, which is trained on
location trajectories to predict stay regions. To the best of our knowledge,
this is the first approach based on deep learning and the first approach that
is evaluated on a public, labeled dataset. Our SR extraction method outperforms
state-of-the-art methods. In addition, we conducted a limited experiment on the
task of transportation mode detection from GPS trajectories using the same
architecture and achieved significantly higher scores than the
state-of-the-art. Our code is available at
https://github.com/christianll9/deepstay.
</p>

<h3>Title: Automated Labeling of German Chest X-Ray Radiology Reports using Deep Learning. (arXiv:2306.05997v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05997">http://arxiv.org/abs/2306.05997</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05997] Automated Labeling of German Chest X-Ray Radiology Reports using Deep Learning](http://arxiv.org/abs/2306.05997) #extraction</code></li>
<li>Summary: <p>Radiologists are in short supply globally, and deep learning models offer a
promising solution to address this shortage as part of clinical
decision-support systems. However, training such models often requires
expensive and time-consuming manual labeling of large datasets. Automatic label
extraction from radiology reports can reduce the time required to obtain
labeled datasets, but this task is challenging due to semantically similar
words and missing annotated data. In this work, we explore the potential of
weak supervision of a deep learning-based label prediction model, using a
rule-based labeler. We propose a deep learning-based CheXpert label prediction
model, pre-trained on reports labeled by a rule-based German CheXpert model and
fine-tuned on a small dataset of manually labeled reports. Our results
demonstrate the effectiveness of our approach, which significantly outperformed
the rule-based model on all three tasks. Our findings highlight the benefits of
employing deep learning-based models even in scenarios with sparse data and the
use of the rule-based labeler as a tool for weak supervision.
</p></li>
</ul>

<h3>Title: DynamoRep: Trajectory-Based Population Dynamics for Classification of Black-box Optimization Problems. (arXiv:2306.05438v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05438">http://arxiv.org/abs/2306.05438</a></li>
<li>Code URL: <a href="https://github.com/gjorgjinac/dynamorep_problem_classification">https://github.com/gjorgjinac/dynamorep_problem_classification</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05438] DynamoRep: Trajectory-Based Population Dynamics for Classification of Black-box Optimization Problems](http://arxiv.org/abs/2306.05438) #extraction</code></li>
<li>Summary: <p>The application of machine learning (ML) models to the analysis of
optimization algorithms requires the representation of optimization problems
using numerical features. These features can be used as input for ML models
that are trained to select or to configure a suitable algorithm for the problem
at hand. Since in pure black-box optimization information about the problem
instance can only be obtained through function evaluation, a common approach is
to dedicate some function evaluations for feature extraction, e.g., using
random sampling. This approach has two key downsides: (1) It reduces the budget
left for the actual optimization phase, and (2) it neglects valuable
information that could be obtained from a problem-solver interaction.
</p></li>
</ul>

<p>In this paper, we propose a feature extraction method that describes the
trajectories of optimization algorithms using simple descriptive statistics. We
evaluate the generated features for the task of classifying problem classes
from the Black Box Optimization Benchmarking (BBOB) suite. We demonstrate that
the proposed DynamoRep features capture enough information to identify the
problem class on which the optimization algorithm is running, achieving a mean
classification accuracy of 95% across all experiments.
</p>

<h3>Title: Quantitative Ink Analysis: Estimating the Number of Inks in Documents through Hyperspectral Imaging. (arXiv:2306.05784v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05784">http://arxiv.org/abs/2306.05784</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05784] Quantitative Ink Analysis: Estimating the Number of Inks in Documents through Hyperspectral Imaging](http://arxiv.org/abs/2306.05784) #extraction</code></li>
<li>Summary: <p>In the field of document forensics, ink analysis plays a crucial role in
determining the authenticity of legal and historic documents and detecting
forgery. Visual examination alone is insufficient for distinguishing visually
similar inks, necessitating the use of advanced scientific techniques. This
paper proposes an ink analysis technique based on hyperspectral imaging, which
enables the examination of documents in hundreds of narrowly spaced spectral
bands, revealing hidden details. The main objective of this study is to
identify the number of distinct inks used in a document. Three clustering
algorithms, namely k-means, Agglomerative, and c-means, are employed to
estimate the number of inks present. The methodology involves data extraction,
ink pixel segmentation, and ink number determination. The results demonstrate
the effectiveness of the proposed technique in identifying ink clusters and
distinguishing between different inks. The analysis of a hyperspectral cube
dataset reveals variations in spectral reflectance across different bands and
distinct spectral responses among the 12 lines, indicating the presence of
multiple inks. The clustering algorithms successfully identify ink clusters,
with k-means clustering showing superior classification performance. These
findings contribute to the development of reliable methodologies for ink
analysis using hyperspectral imaging, enhancing the
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Is Normalization Indispensable for Multi-domain Federated Learning?. (arXiv:2306.05879v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05879">http://arxiv.org/abs/2306.05879</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05879] Is Normalization Indispensable for Multi-domain Federated Learning?](http://arxiv.org/abs/2306.05879) #federate</code></li>
<li>Summary: <p>Federated learning (FL) enhances data privacy with collaborative in-situ
training on decentralized clients. Nevertheless, FL encounters challenges due
to non-independent and identically distributed (non-i.i.d) data, leading to
potential performance degradation and hindered convergence. While prior studies
predominantly addressed the issue of skewed label distribution, our research
addresses a crucial yet frequently overlooked problem known as multi-domain FL.
In this scenario, clients' data originate from diverse domains with distinct
feature distributions, as opposed to label distributions. To address the
multi-domain problem in FL, we propose a novel method called Federated learning
Without normalizations (FedWon). FedWon draws inspiration from the observation
that batch normalization (BN) faces challenges in effectively modeling the
statistics of multiple domains, while alternative normalization techniques
possess their own limitations. In order to address these issues, FedWon
eliminates all normalizations in FL and reparameterizes convolution layers with
scaled weight standardization. Through comprehensive experimentation on four
datasets and four models, our results demonstrate that FedWon surpasses both
FedAvg and the current state-of-the-art method (FedBN) across all experimental
setups, achieving notable improvements of over 10% in certain domains.
Furthermore, FedWon is versatile for both cross-silo and cross-device FL,
exhibiting strong performance even with a batch size as small as 1, thereby
catering to resource-constrained devices. Additionally, FedWon effectively
tackles the challenge of skewed label distribution.
</p></li>
</ul>

<h3>Title: Federated Learning for Medical Image Analysis: A Survey. (arXiv:2306.05980v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05980">http://arxiv.org/abs/2306.05980</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05980] Federated Learning for Medical Image Analysis: A Survey](http://arxiv.org/abs/2306.05980) #federate</code></li>
<li>Summary: <p>Machine learning in medical imaging often faces a fundamental dilemma, namely
the small sample size problem. Many recent studies suggest using multi-domain
data pooled from different acquisition sites/datasets to improve statistical
power. However, medical images from different sites cannot be easily shared to
build large datasets for model training due to privacy protection reasons. As a
promising solution, federated learning, which enables collaborative training of
machine learning models based on data from different sites without cross-site
data sharing, has attracted considerable attention recently. In this paper, we
conduct a comprehensive survey of the recent development of federated learning
methods in medical image analysis. We first introduce the background and
motivation of federated learning for dealing with privacy protection and
collaborative learning issues in medical imaging. We then present a
comprehensive review of recent advances in federated learning methods for
medical image analysis. Specifically, existing methods are categorized based on
three critical aspects of a federated learning system, including client end,
server end, and communication techniques. In each category, we summarize the
existing federated learning methods according to specific research problems in
medical image analysis and also provide insights into the motivations of
different approaches. In addition, we provide a review of existing benchmark
medical imaging datasets and software platforms for current federated learning
research. We also conduct an experimental study to empirically evaluate typical
federated learning methods for medical image analysis. This survey can help to
better understand the current research status, challenges and potential
research opportunities in this promising research field.
</p></li>
</ul>

<h3>Title: PeFLL: A Lifelong Learning Approach to Personalized Federated Learning. (arXiv:2306.05515v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05515">http://arxiv.org/abs/2306.05515</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05515] PeFLL: A Lifelong Learning Approach to Personalized Federated Learning](http://arxiv.org/abs/2306.05515) #federate</code></li>
<li>Summary: <p>Personalized federated learning (pFL) has emerged as a popular approach to
dealing with the challenge of statistical heterogeneity between the data
distributions of the participating clients. Instead of learning a single global
model, pFL aims to learn an individual model for each client while still making
use of the data available at other clients. In this work, we present PeFLL, a
new pFL approach rooted in lifelong learning that performs well not only on
clients present during its training phase, but also on any that may emerge in
the future. PeFLL learns to output client specific models by jointly training
an embedding network and a hypernetwork. The embedding network learns to
represent clients in a latent descriptor space in a way that reflects their
similarity to each other. The hypernetwork learns a mapping from this latent
space to the space of possible client models. We demonstrate experimentally
that PeFLL produces models of superior accuracy compared to previous methods,
especially for clients not seen during training, and that it scales well to
large numbers of clients. Moreover, generating a personalized model for a new
client is efficient as no additional fine-tuning or optimization is required by
either the client or the server. We also present theoretical results supporting
PeFLL in the form of a new PAC-Bayesian generalization bound for lifelong
learning and we prove the convergence of our proposed optimization procedure.
</p></li>
</ul>

<h3>Title: Communication-Efficient Zeroth-Order Distributed Online Optimization: Algorithm, Theory, and Applications. (arXiv:2306.05655v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05655">http://arxiv.org/abs/2306.05655</a></li>
<li>Code URL: <a href="https://github.com/sunses-hub/fed-ef-zo-sgd">https://github.com/sunses-hub/fed-ef-zo-sgd</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05655] Communication-Efficient Zeroth-Order Distributed Online Optimization: Algorithm, Theory, and Applications](http://arxiv.org/abs/2306.05655) #federate</code></li>
<li>Summary: <p>This paper focuses on a multi-agent zeroth-order online optimization problem
in a federated learning setting for target tracking. The agents only sense
their current distances to their targets and aim to maintain a minimum safe
distance from each other to prevent collisions. The coordination among the
agents and dissemination of collision-prevention information is managed by a
central server using the federated learning paradigm. The proposed formulation
leads to an instance of distributed online nonconvex optimization problem that
is solved via a group of communication-constrained agents. To deal with the
communication limitations of the agents, an error feedback-based compression
scheme is utilized for agent-to-server communication. The proposed algorithm is
analyzed theoretically for the general class of distributed online nonconvex
optimization problems. We provide non-asymptotic convergence rates that show
the dominant term is independent of the characteristics of the compression
scheme. Our theoretical results feature a new approach that employs
significantly more relaxed assumptions in comparison to standard literature.
The performance of the proposed solution is further analyzed numerically in
terms of tracking errors and collisions between agents in two relevant
applications.
</p></li>
</ul>

<h3>Title: Understanding How Consistency Works in Federated Learning via Stage-wise Relaxed Initialization. (arXiv:2306.05706v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05706">http://arxiv.org/abs/2306.05706</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05706] Understanding How Consistency Works in Federated Learning via Stage-wise Relaxed Initialization](http://arxiv.org/abs/2306.05706) #federate</code></li>
<li>Summary: <p>Federated learning (FL) is a distributed paradigm that coordinates massive
local clients to collaboratively train a global model via stage-wise local
training processes on the heterogeneous dataset. Previous works have implicitly
studied that FL suffers from the <code>client-drift'' problem, which is caused by
the inconsistent optimum across local clients. However, till now it still lacks
solid theoretical analysis to explain the impact of this local inconsistency.
To alleviate the negative impact of the</code>client drift'' and explore its
substance in FL, in this paper, we first design an efficient FL algorithm
\textit{FedInit}, which allows employing the personalized relaxed
initialization state at the beginning of each local training stage.
Specifically, \textit{FedInit} initializes the local state by moving away from
the current global state towards the reverse direction of the latest local
state. This relaxed initialization helps to revise the local divergence and
enhance the local consistency level. Moreover, to further understand how
inconsistency disrupts performance in FL, we introduce the excess risk analysis
and study the divergence term to investigate the test error of the proposed
\textit{FedInit} method. Our studies show that optimization error is not
sensitive to this local inconsistency, while it mainly affects the
generalization error bound in \textit{FedInit}. Extensive experiments are
conducted to validate this conclusion. Our proposed \textit{FedInit} could
achieve state-of-the-art~(SOTA) results compared to several advanced benchmarks
without any additional costs. Meanwhile, stage-wise relaxed initialization
could also be incorporated into the current advanced algorithms to achieve
higher performance in the FL paradigm.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Good, but not always Fair: An Evaluation of Gender Bias for three commercial Machine Translation Systems. (arXiv:2306.05882v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05882">http://arxiv.org/abs/2306.05882</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05882] Good, but not always Fair: An Evaluation of Gender Bias for three commercial Machine Translation Systems](http://arxiv.org/abs/2306.05882) #fair</code></li>
<li>Summary: <p>Machine Translation (MT) continues to make significant strides in quality and
is increasingly adopted on a larger scale. Consequently, analyses have been
redirected to more nuanced aspects, intricate phenomena, as well as potential
risks that may arise from the widespread use of MT tools. Along this line, this
paper offers a meticulous assessment of three commercial MT systems - Google
Translate, DeepL, and Modern MT - with a specific focus on gender translation
and bias. For three language pairs (English/Spanish, English/Italian, and
English/French), we scrutinize the behavior of such systems at several levels
of granularity and on a variety of naturally occurring gender phenomena in
translation. Our study takes stock of the current state of online MT tools, by
revealing significant discrepancies in the gender translation of the three
systems, with each system displaying varying degrees of bias despite their
overall translation quality.
</p></li>
</ul>

<h3>Title: Fair yet Asymptotically Equal Collaborative Learning. (arXiv:2306.05764v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05764">http://arxiv.org/abs/2306.05764</a></li>
<li>Code URL: <a href="https://github.com/xqlin98/fair-yet-equal-cml">https://github.com/xqlin98/fair-yet-equal-cml</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05764] Fair yet Asymptotically Equal Collaborative Learning](http://arxiv.org/abs/2306.05764) #fair</code></li>
<li>Summary: <p>In collaborative learning with streaming data, nodes (e.g., organizations)
jointly and continuously learn a machine learning (ML) model by sharing the
latest model updates computed from their latest streaming data. For the more
resourceful nodes to be willing to share their model updates, they need to be
fairly incentivized. This paper explores an incentive design that guarantees
fairness so that nodes receive rewards commensurate to their contributions. Our
approach leverages an explore-then-exploit formulation to estimate the nodes'
contributions (i.e., exploration) for realizing our theoretically guaranteed
fair incentives (i.e., exploitation). However, we observe a "rich get richer"
phenomenon arising from the existing approaches to guarantee fairness and it
discourages the participation of the less resourceful nodes. To remedy this, we
additionally preserve asymptotic equality, i.e., less resourceful nodes achieve
equal performance eventually to the more resourceful/"rich" nodes. We
empirically demonstrate in two settings with real-world streaming data:
federated online incremental learning and federated reinforcement learning,
that our proposed approach outperforms existing baselines in fairness and
learning performance while remaining competitive in preserving equality.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Efficient GNN Explanation via Learning Removal-based Attribution. (arXiv:2306.05760v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05760">http://arxiv.org/abs/2306.05760</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05760] Efficient GNN Explanation via Learning Removal-based Attribution](http://arxiv.org/abs/2306.05760) #interpretability</code></li>
<li>Summary: <p>As Graph Neural Networks (GNNs) have been widely used in real-world
applications, model explanations are required not only by users but also by
legal regulations. However, simultaneously achieving high fidelity and low
computational costs in generating explanations has been a challenge for current
methods. In this work, we propose a framework of GNN explanation named LeArn
Removal-based Attribution (LARA) to address this problem. Specifically, we
introduce removal-based attribution and demonstrate its substantiated link to
interpretability fidelity theoretically and experimentally. The explainer in
LARA learns to generate removal-based attribution which enables providing
explanations with high fidelity. A strategy of subgraph sampling is designed in
LARA to improve the scalability of the training process. In the deployment,
LARA can efficiently generate the explanation through a feed-forward pass. We
benchmark our approach with other state-of-the-art GNN explanation methods on
six datasets. Results highlight the effectiveness of our framework regarding
both efficiency and fidelity. In particular, LARA is 3.5 times faster and
achieves higher fidelity than the state-of-the-art method on the large dataset
ogbn-arxiv (more than 160K nodes and 1M edges), showing its great potential in
real-world applications. Our source code is available at
https://anonymous.4open.science/r/LARA-10D8/README.md.
</p></li>
</ul>

<h3>Title: Incorporating Prior Knowledge in Deep Learning Models via Pathway Activity Autoencoders. (arXiv:2306.05813v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05813">http://arxiv.org/abs/2306.05813</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05813] Incorporating Prior Knowledge in Deep Learning Models via Pathway Activity Autoencoders](http://arxiv.org/abs/2306.05813) #interpretability</code></li>
<li>Summary: <p>Motivation: Despite advances in the computational analysis of high-throughput
molecular profiling assays (e.g. transcriptomics), a dichotomy exists between
methods that are simple and interpretable, and ones that are complex but with
lower degree of interpretability. Furthermore, very few methods deal with
trying to translate interpretability in biologically relevant terms, such as
known pathway cascades. Biological pathways reflecting signalling events or
metabolic conversions are Small improvements or modifications of existing
algorithms will generally not be suitable, unless novel biological results have
been predicted and verified. Determining which pathways are implicated in
disease and incorporating such pathway data as prior knowledge may enhance
predictive modelling and personalised strategies for diagnosis, treatment and
prevention of disease.
</p></li>
</ul>

<p>Results: We propose a novel prior-knowledge-based deep auto-encoding
framework, PAAE, together with its accompanying generative variant, PAVAE, for
RNA-seq data in cancer. Through comprehensive comparisons among various
learning models, we show that, despite having access to a smaller set of
features, our PAAE and PAVAE models achieve better out-of-set reconstruction
results compared to common methodologies. Furthermore, we compare our model
with equivalent baselines on a classification task and show that they achieve
better results than models which have access to the full input gene set.
Another result is that using vanilla variational frameworks might negatively
impact both reconstruction outputs as well as classification performance.
Finally, our work directly contributes by providing comprehensive
interpretability analyses on our models on top of improving prognostication for
translational medicine.
</p>

<h2>explainability</h2>
<h3>Title: FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering. (arXiv:2306.05523v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05523">http://arxiv.org/abs/2306.05523</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05523] FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering](http://arxiv.org/abs/2306.05523) #explainability</code></li>
<li>Summary: <p>Combating disinformation is one of the burning societal crises -- about 67%
of the American population believes that disinformation produces a lot of
uncertainty, and 10% of them knowingly propagate disinformation. Evidence shows
that disinformation can manipulate democratic processes and public opinion,
causing disruption in the share market, panic and anxiety in society, and even
death during crises. Therefore, disinformation should be identified promptly
and, if possible, mitigated. With approximately 3.2 billion images and 720,000
hours of video shared online daily on social media platforms, scalable
detection of multimodal disinformation requires efficient fact verification.
Despite progress in automatic text-based fact verification (e.g., FEVER, LIAR),
the research community lacks substantial effort in multimodal fact
verification. To address this gap, we introduce FACTIFY 3M, a dataset of 3
million samples that pushes the boundaries of the domain of fact verification
via a multimodal fake news dataset, in addition to offering explainability
through the concept of 5W question-answering. Salient features of the dataset
include: (i) textual claims, (ii) ChatGPT-generated paraphrased claims, (iii)
associated images, (iv) stable diffusion-generated additional images (i.e.,
visual paraphrases), (v) pixel-level image heatmap to foster image-text
explainability of the claim, (vi) 5W QA pairs, and (vii) adversarial fake news
stories.
</p></li>
</ul>

<h3>Title: Check Me If You Can: Detecting ChatGPT-Generated Academic Writing using CheckGPT. (arXiv:2306.05524v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05524">http://arxiv.org/abs/2306.05524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05524] Check Me If You Can: Detecting ChatGPT-Generated Academic Writing using CheckGPT](http://arxiv.org/abs/2306.05524) #explainability</code></li>
<li>Summary: <p>With ChatGPT under the spotlight, utilizing large language models (LLMs) for
academic writing has drawn a significant amount of discussions and concerns in
the community. While substantial research efforts have been stimulated for
detecting LLM-Generated Content (LLM-content), most of the attempts are still
in the early stage of exploration. In this paper, we present a holistic
investigation of detecting LLM-generate academic writing, by providing a
dataset, evidence, and algorithms, in order to inspire more community effort to
address the concern of LLM academic misuse. We first present GPABenchmark, a
benchmarking dataset of 600,000 samples of human-written, GPT-written,
GPT-completed, and GPT-polished abstracts of research papers in CS, physics,
and humanities and social sciences (HSS). We show that existing open-source and
commercial GPT detectors provide unsatisfactory performance on GPABenchmark,
especially for GPT-polished text. Moreover, through a user study of 150+
participants, we show that it is highly challenging for human users, including
experienced faculty members and researchers, to identify GPT-generated
abstracts. We then present CheckGPT, a novel LLM-content detector consisting of
a general representation module and an attentive-BiLSTM classification module,
which is accurate, transferable, and interpretable. Experimental results show
that CheckGPT achieves an average classification accuracy of 98% to 99% for the
task-specific discipline-specific detectors and the unified detectors. CheckGPT
is also highly transferable that, without tuning, it achieves ~90% accuracy in
new domains, such as news articles, while a model tuned with approximately
2,000 samples in the target domain achieves ~98% accuracy. Finally, we
demonstrate the explainability insights obtained from CheckGPT to reveal the
key behaviors of how LLM generates texts.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Word-Level Explanations for Analyzing Bias in Text-to-Image Models. (arXiv:2306.05500v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05500">http://arxiv.org/abs/2306.05500</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05500] Word-Level Explanations for Analyzing Bias in Text-to-Image Models](http://arxiv.org/abs/2306.05500) #diffusion</code></li>
<li>Summary: <p>Text-to-image models take a sentence (i.e., prompt) and generate images
associated with this input prompt. These models have created award wining-art,
videos, and even synthetic datasets. However, text-to-image (T2I) models can
generate images that underrepresent minorities based on race and sex. This
paper investigates which word in the input prompt is responsible for bias in
generated images. We introduce a method for computing scores for each word in
the prompt; these scores represent its influence on biases in the model's
output. Our method follows the principle of \emph{explaining by removing},
leveraging masked language models to calculate the influence scores. We perform
experiments on Stable Diffusion to demonstrate that our method identifies the
replication of societal stereotypes in generated images.
</p></li>
</ul>

<h3>Title: BOOT: Data-free Distillation of Denoising Diffusion Models with Bootstrapping. (arXiv:2306.05544v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05544">http://arxiv.org/abs/2306.05544</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05544] BOOT: Data-free Distillation of Denoising Diffusion Models with Bootstrapping](http://arxiv.org/abs/2306.05544) #diffusion</code></li>
<li>Summary: <p>Diffusion models have demonstrated excellent potential for generating diverse
images. However, their performance often suffers from slow generation due to
iterative denoising. Knowledge distillation has been recently proposed as a
remedy that can reduce the number of inference steps to one or a few without
significant quality degradation. However, existing distillation methods either
require significant amounts of offline computation for generating synthetic
training data from the teacher model or need to perform expensive online
learning with the help of real data. In this work, we present a novel technique
called BOOT, that overcomes these limitations with an efficient data-free
distillation algorithm. The core idea is to learn a time-conditioned model that
predicts the output of a pre-trained diffusion model teacher given any time
step. Such a model can be efficiently trained based on bootstrapping from two
consecutive sampled steps. Furthermore, our method can be easily adapted to
large-scale text-to-image diffusion models, which are challenging for
conventional methods given the fact that the training sets are often large and
difficult to access. We demonstrate the effectiveness of our approach on
several benchmark datasets in the DDIM setting, achieving comparable generation
quality while being orders of magnitude faster than the diffusion teacher. The
text-to-image results show that the proposed approach is able to handle highly
complex distributions, shedding light on more efficient generative modeling.
</p></li>
</ul>

<h3>Title: Reconstructing the somatotopic organization of the corticospinal tract remains a challenge for modern tractography methods. (arXiv:2306.05623v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05623">http://arxiv.org/abs/2306.05623</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05623] Reconstructing the somatotopic organization of the corticospinal tract remains a challenge for modern tractography methods](http://arxiv.org/abs/2306.05623) #diffusion</code></li>
<li>Summary: <p>The corticospinal tract (CST) is a critically important white matter fiber
tract in the human brain that enables control of voluntary movements of the
body. Diffusion MRI tractography is the only method that enables the study of
the anatomy and variability of the CST pathway in human health. In this work,
we explored the performance of six widely used tractography methods for
reconstructing the CST and its somatotopic organization. We perform experiments
using diffusion MRI data from the Human Connectome Project. Four quantitative
measurements including reconstruction rate, the WM-GM interface coverage,
anatomical distribution of streamlines, and correlation with cortical volumes
to assess the advantages and limitations of each method. Overall, we conclude
that while current tractography methods have made progress toward the
well-known challenge of improving the reconstruction of the lateral projections
of the CST, the overall problem of performing a comprehensive CST
reconstruction, including clinically important projections in the lateral (hand
and face area) and medial portions (leg area), remains an important challenge
for diffusion MRI tractography.
</p></li>
</ul>

<h3>Title: RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models. (arXiv:2306.05668v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05668">http://arxiv.org/abs/2306.05668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05668] RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models](http://arxiv.org/abs/2306.05668) #diffusion</code></li>
<li>Summary: <p>The emergence of Neural Radiance Fields (NeRF) has promoted the development
of synthesized high-fidelity views of the intricate real world. However, it is
still a very demanding task to repaint the content in NeRF. In this paper, we
propose a novel framework that can take RGB images as input and alter the 3D
content in neural scenes. Our work leverages existing diffusion models to guide
changes in the designated 3D content. Specifically, we semantically select the
target object and a pre-trained diffusion model will guide the NeRF model to
generate new 3D objects, which can improve the editability, diversity, and
application range of NeRF. Experiment results show that our algorithm is
effective for editing 3D objects in NeRF under different text prompts,
including editing appearance, shape, and more. We validate our method on both
real-world datasets and synthetic-world datasets for these editing tasks.
Please visit https://repaintnerf.github.io for a better view of our results.
</p></li>
</ul>

<h3>Title: Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model. (arXiv:2306.05720v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05720">http://arxiv.org/abs/2306.05720</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05720] Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model](http://arxiv.org/abs/2306.05720) #diffusion</code></li>
<li>Summary: <p>Latent diffusion models (LDMs) exhibit an impressive ability to produce
realistic images, yet the inner workings of these models remain mysterious.
Even when trained purely on images without explicit depth information, they
typically output coherent pictures of 3D scenes. In this work, we investigate a
basic interpretability question: does an LDM create and use an internal
representation of simple scene geometry? Using linear probes, we find evidence
that the internal activations of the LDM encode linear representations of both
3D depth data and a salient-object / background distinction. These
representations appear surprisingly early in the denoising process$-$well
before a human can easily make sense of the noisy images. Intervention
experiments further indicate these representations play a causal role in image
synthesis, and may be used for simple high-level editing of an LDM's output.
</p></li>
</ul>

<h3>Title: DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles. (arXiv:2306.05957v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05957">http://arxiv.org/abs/2306.05957</a></li>
<li>Code URL: <a href="https://github.com/taldatech/ddlp">https://github.com/taldatech/ddlp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05957] DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles](http://arxiv.org/abs/2306.05957) #diffusion</code></li>
<li>Summary: <p>We propose a new object-centric video prediction algorithm based on the deep
latent particle (DLP) representation. In comparison to existing slot- or
patch-based representations, DLPs model the scene using a set of keypoints with
learned parameters for properties such as position and size, and are both
efficient and interpretable. Our method, deep dynamic latent particles (DDLP),
yields state-of-the-art object-centric video prediction results on several
challenging datasets. The interpretable nature of DDLP allows us to perform
``what-if'' generation -- predict the consequence of changing properties of
objects in the initial frames, and DLP's compact structure enables efficient
diffusion-based unconditional video generation. Videos, code and pre-trained
models are available: https://taldatech.github.io/ddlp-web
</p></li>
</ul>

<h3>Title: Neural FIM for learning Fisher Information Metrics from point cloud data. (arXiv:2306.06062v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06062">http://arxiv.org/abs/2306.06062</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06062] Neural FIM for learning Fisher Information Metrics from point cloud data](http://arxiv.org/abs/2306.06062) #diffusion</code></li>
<li>Summary: <p>Although data diffusion embeddings are ubiquitous in unsupervised learning
and have proven to be a viable technique for uncovering the underlying
intrinsic geometry of data, diffusion embeddings are inherently limited due to
their discrete nature. To this end, we propose neural FIM, a method for
computing the Fisher information metric (FIM) from point cloud data - allowing
for a continuous manifold model for the data. Neural FIM creates an extensible
metric space from discrete point cloud data such that information from the
metric can inform us of manifold characteristics such as volume and geodesics.
We demonstrate Neural FIM's utility in selecting parameters for the PHATE
visualization method as well as its ability to obtain information pertaining to
local volume illuminating branching points and cluster centers embeddings of a
toy dataset and two single-cell datasets of IPSC reprogramming and PBMCs
(immune cells).
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: FlowFormer: A Transformer Architecture and Its Masked Cost Volume Autoencoding for Optical Flow. (arXiv:2306.05442v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05442">http://arxiv.org/abs/2306.05442</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05442] FlowFormer: A Transformer Architecture and Its Masked Cost Volume Autoencoding for Optical Flow](http://arxiv.org/abs/2306.05442) #transformer</code></li>
<li>Summary: <p>This paper introduces a novel transformer-based network architecture,
FlowFormer, along with the Masked Cost Volume AutoEncoding (MCVA) for
pretraining it to tackle the problem of optical flow estimation. FlowFormer
tokenizes the 4D cost-volume built from the source-target image pair and
iteratively refines flow estimation with a cost-volume encoder-decoder
architecture. The cost-volume encoder derives a cost memory with
alternate-group transformer~(AGT) layers in a latent space and the decoder
recurrently decodes flow from the cost memory with dynamic positional cost
queries. On the Sintel benchmark, FlowFormer architecture achieves 1.16 and
2.09 average end-point-error~(AEPE) on the clean and final pass, a 16.5\% and
15.5\% error reduction from the GMA~(1.388 and 2.47). MCVA enhances FlowFormer
by pretraining the cost-volume encoder with a masked autoencoding scheme, which
further unleashes the capability of FlowFormer with unlabeled data. This is
especially critical in optical flow estimation because ground truth flows are
more expensive to acquire than labels in other vision tasks. MCVA improves
FlowFormer all-sided and FlowFormer+MCVA ranks 1st among all published methods
on both Sintel and KITTI-2015 benchmarks and achieves the best generalization
performance. Specifically, FlowFormer+MCVA achieves 1.07 and 1.94 AEPE on the
Sintel benchmark, leading to 7.76\% and 7.18\% error reductions from
FlowFormer.
</p></li>
</ul>

<h3>Title: Customizing General-Purpose Foundation Models for Medical Report Generation. (arXiv:2306.05642v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05642">http://arxiv.org/abs/2306.05642</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05642] Customizing General-Purpose Foundation Models for Medical Report Generation](http://arxiv.org/abs/2306.05642) #transformer</code></li>
<li>Summary: <p>Medical caption prediction which can be regarded as a task of medical report
generation (MRG), requires the automatic generation of coherent and accurate
captions for the given medical images. However, the scarcity of labelled
medical image-report pairs presents great challenges in the development of deep
and large-scale neural networks capable of harnessing the potential artificial
general intelligence power like large language models (LLMs). In this work, we
propose customizing off-the-shelf general-purpose large-scale pre-trained
models, i.e., foundation models (FMs), in computer vision and natural language
processing with a specific focus on medical report generation. Specifically,
following BLIP-2, a state-of-the-art vision-language pre-training approach, we
introduce our encoder-decoder-based MRG model. This model utilizes a
lightweight query Transformer to connect two FMs: the giant vision Transformer
EVA-ViT-g and a bilingual LLM trained to align with human intentions (referred
to as ChatGLM-6B). Furthermore, we conduct ablative experiments on the
trainable components of the model to identify the crucial factors for effective
transfer learning. Our findings demonstrate that unfreezing EVA-ViT-g to learn
medical image representations, followed by parameter-efficient training of
ChatGLM-6B to capture the writing styles of medical reports, is essential for
achieving optimal results. Our best attempt (PCLmed Team) achieved the 4th and
the 2nd, respectively, out of 13 participating teams, based on the BERTScore
and ROUGE-1 metrics, in the ImageCLEFmedical Caption 2023 Caption Prediction
Task competition.
</p></li>
</ul>

<h3>Title: GMS-3DQA: Projection-based Grid Mini-patch Sampling for 3D Model Quality Assessment. (arXiv:2306.05658v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05658">http://arxiv.org/abs/2306.05658</a></li>
<li>Code URL: <a href="https://github.com/zzc-1998/gms-3dqa">https://github.com/zzc-1998/gms-3dqa</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05658] GMS-3DQA: Projection-based Grid Mini-patch Sampling for 3D Model Quality Assessment](http://arxiv.org/abs/2306.05658) #transformer</code></li>
<li>Summary: <p>Nowadays, most 3D model quality assessment (3DQA) methods have been aimed at
improving performance. However, little attention has been paid to the
computational cost and inference time required for practical applications.
Model-based 3DQA methods extract features directly from the 3D models, which
are characterized by their high degree of complexity. As a result, many
researchers are inclined towards utilizing projection-based 3DQA methods.
Nevertheless, previous projection-based 3DQA methods directly extract features
from multi-projections to ensure quality prediction accuracy, which calls for
more resource consumption and inevitably leads to inefficiency. Thus in this
paper, we address this challenge by proposing a no-reference (NR)
projection-based \textit{\underline{G}rid \underline{M}ini-patch
\underline{S}ampling \underline{3D} Model \underline{Q}uality
\underline{A}ssessment (GMS-3DQA)} method. The projection images are rendered
from six perpendicular viewpoints of the 3D model to cover sufficient quality
information. To reduce redundancy and inference resources, we propose a
multi-projection grid mini-patch sampling strategy (MP-GMS), which samples grid
mini-patches from the multi-projections and forms the sampled grid mini-patches
into one quality mini-patch map (QMM). The Swin-Transformer tiny backbone is
then used to extract quality-aware features from the QMMs. The experimental
results show that the proposed GMS-3DQA outperforms existing state-of-the-art
NR-3DQA methods on the point cloud quality assessment databases. The efficiency
analysis reveals that the proposed GMS-3DQA requires far less computational
resources and inference time than other 3DQA competitors. The code will be
available at https://github.com/zzc-1998/GMS-3DQA.
</p></li>
</ul>

<h3>Title: Illumination Controllable Dehazing Network based on Unsupervised Retinex Embedding. (arXiv:2306.05675v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05675">http://arxiv.org/abs/2306.05675</a></li>
<li>Code URL: <a href="https://github.com/xiaofeng-life/icdehazing">https://github.com/xiaofeng-life/icdehazing</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05675] Illumination Controllable Dehazing Network based on Unsupervised Retinex Embedding](http://arxiv.org/abs/2306.05675) #transformer</code></li>
<li>Summary: <p>On the one hand, the dehazing task is an illposedness problem, which means
that no unique solution exists. On the other hand, the dehazing task should
take into account the subjective factor, which is to give the user selectable
dehazed images rather than a single result. Therefore, this paper proposes a
multi-output dehazing network by introducing illumination controllable ability,
called IC-Dehazing. The proposed IC-Dehazing can change the illumination
intensity by adjusting the factor of the illumination controllable module,
which is realized based on the interpretable Retinex theory. Moreover, the
backbone dehazing network of IC-Dehazing consists of a Transformer with double
decoders for high-quality image restoration. Further, the prior-based loss
function and unsupervised training strategy enable IC-Dehazing to complete the
parameter learning process without the need for paired data. To demonstrate the
effectiveness of the proposed IC-Dehazing, quantitative and qualitative
experiments are conducted on image dehazing, semantic segmentation, and object
detection tasks. Code is available at
https://github.com/Xiaofeng-life/ICDehazing.
</p></li>
</ul>

<h3>Title: Lightweight Monocular Depth Estimation via Token-Sharing Transformer. (arXiv:2306.05682v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05682">http://arxiv.org/abs/2306.05682</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05682] Lightweight Monocular Depth Estimation via Token-Sharing Transformer](http://arxiv.org/abs/2306.05682) #transformer</code></li>
<li>Summary: <p>Depth estimation is an important task in various robotics systems and
applications. In mobile robotics systems, monocular depth estimation is
desirable since a single RGB camera can be deployable at a low cost and compact
size. Due to its significant and growing needs, many lightweight monocular
depth estimation networks have been proposed for mobile robotics systems. While
most lightweight monocular depth estimation methods have been developed using
convolution neural networks, the Transformer has been gradually utilized in
monocular depth estimation recently. However, massive parameters and large
computational costs in the Transformer disturb the deployment to embedded
devices. In this paper, we present a Token-Sharing Transformer (TST), an
architecture using the Transformer for monocular depth estimation, optimized
especially in embedded devices. The proposed TST utilizes global token sharing,
which enables the model to obtain an accurate depth prediction with high
throughput in embedded devices. Experimental results show that TST outperforms
the existing lightweight monocular depth estimation methods. On the NYU Depth
v2 dataset, TST can deliver depth maps up to 63.4 FPS in NVIDIA Jetson nano and
142.6 FPS in NVIDIA Jetson TX2, with lower errors than the existing methods.
Furthermore, TST achieves real-time depth estimation of high-resolution images
on Jetson TX2 with competitive results.
</p></li>
</ul>

<h3>Title: ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer. (arXiv:2306.05688v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05688">http://arxiv.org/abs/2306.05688</a></li>
<li>Code URL: <a href="https://github.com/ZAX130/SmileCode">https://github.com/ZAX130/SmileCode</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05688] ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer](http://arxiv.org/abs/2306.05688) #transformer</code></li>
<li>Summary: <p>The Transformer structures have been widely used in computer vision and have
recently made an impact in the area of medical image registration. However, the
use of Transformer in most registration networks is straightforward. These
networks often merely use the attention mechanism to boost the feature learning
as the segmentation networks do, but do not sufficiently design to be adapted
for the registration task. In this paper, we propose a novel motion
decomposition Transformer (ModeT) to explicitly model multiple motion
modalities by fully exploiting the intrinsic capability of the Transformer
structure for deformation estimation. The proposed ModeT naturally transforms
the multi-head neighborhood attention relationship into the multi-coordinate
relationship to model multiple motion modes. Then the competitive weighting
module (CWM) fuses multiple deformation sub-fields to generate the resulting
deformation field. Extensive experiments on two public brain magnetic resonance
imaging (MRI) datasets show that our method outperforms current
state-of-the-art registration networks and Transformers, demonstrating the
potential of our ModeT for the challenging non-rigid deformation estimation
problem. The benchmarks and our code are publicly available at
https://github.com/ZAX130/SmileCode.
</p></li>
</ul>

<h3>Title: Single-Stage Visual Relationship Learning using Conditional Queries. (arXiv:2306.05689v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05689">http://arxiv.org/abs/2306.05689</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05689] Single-Stage Visual Relationship Learning using Conditional Queries](http://arxiv.org/abs/2306.05689) #transformer</code></li>
<li>Summary: <p>Research in scene graph generation (SGG) usually considers two-stage models,
that is, detecting a set of entities, followed by combining them and labeling
all possible relationships. While showing promising results, the pipeline
structure induces large parameter and computation overhead, and typically
hinders end-to-end optimizations. To address this, recent research attempts to
train single-stage models that are computationally efficient. With the advent
of DETR, a set based detection model, one-stage models attempt to predict a set
of subject-predicate-object triplets directly in a single shot. However, SGG is
inherently a multi-task learning problem that requires modeling entity and
predicate distributions simultaneously. In this paper, we propose Transformers
with conditional queries for SGG, namely, TraCQ with a new formulation for SGG
that avoids the multi-task learning problem and the combinatorial entity pair
distribution. We employ a DETR-based encoder-decoder design and leverage
conditional queries to significantly reduce the entity label space as well,
which leads to 20% fewer parameters compared to state-of-the-art single-stage
models. Experimental results show that TraCQ not only outperforms existing
single-stage scene graph generation methods, it also beats many
state-of-the-art two-stage methods on the Visual Genome dataset, yet is capable
of end-to-end training and faster inference.
</p></li>
</ul>

<h3>Title: Exploring Effective Mask Sampling Modeling for Neural Image Compression. (arXiv:2306.05704v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05704">http://arxiv.org/abs/2306.05704</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05704] Exploring Effective Mask Sampling Modeling for Neural Image Compression](http://arxiv.org/abs/2306.05704) #transformer</code></li>
<li>Summary: <p>Image compression aims to reduce the information redundancy in images. Most
existing neural image compression methods rely on side information from
hyperprior or context models to eliminate spatial redundancy, but rarely
address the channel redundancy. Inspired by the mask sampling modeling in
recent self-supervised learning methods for natural language processing and
high-level vision, we propose a novel pretraining strategy for neural image
compression. Specifically, Cube Mask Sampling Module (CMSM) is proposed to
apply both spatial and channel mask sampling modeling to image compression in
the pre-training stage. Moreover, to further reduce channel redundancy, we
propose the Learnable Channel Mask Module (LCMM) and the Learnable Channel
Completion Module (LCCM). Our plug-and-play CMSM, LCMM, LCCM modules can apply
to both CNN-based and Transformer-based architectures, significantly reduce the
computational cost, and improve the quality of images. Experiments on the
public Kodak and Tecnick datasets demonstrate that our method achieves
competitive performance with lower computational complexity compared to
state-of-the-art image compression methods.
</p></li>
</ul>

<h3>Title: A Dual-Source Attention Transformer for Multi-Person Pose Tracking. (arXiv:2306.05807v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05807">http://arxiv.org/abs/2306.05807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05807] A Dual-Source Attention Transformer for Multi-Person Pose Tracking](http://arxiv.org/abs/2306.05807) #transformer</code></li>
<li>Summary: <p>Multi-person pose tracking is an important element for many applications and
requires to estimate the human poses of all persons in a video and to track
them over time. The association of poses across frames remains an open research
problem, in particular for online tracking methods, due to motion blur, crowded
scenes and occlusions. To tackle the association challenge, we propose a
Dual-Source Attention Transformer that incorporates three core aspects: i) In
order to re-identify persons that have been occluded, we propose a
pose-conditioned re-identification network that provides an initial embedding
and allows to match persons even if the number of visible joints differs
between the frames. ii) We incorporate edge embeddings based on temporal pose
similarity and the impact of appearance and pose similarity is automatically
adapted. iii) We propose an attention based matching layer for pose-to-track
association and duplicate removal. We evaluate our approach on Market1501,
PoseTrack 2018 and PoseTrack21.
</p></li>
</ul>

<h3>Title: TrajectoryFormer: 3D Object Tracking Transformer with Predictive Trajectory Hypotheses. (arXiv:2306.05888v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05888">http://arxiv.org/abs/2306.05888</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05888] TrajectoryFormer: 3D Object Tracking Transformer with Predictive Trajectory Hypotheses](http://arxiv.org/abs/2306.05888) #transformer</code></li>
<li>Summary: <p>3D multi-object tracking (MOT) is vital for many applications including
autonomous driving vehicles and service robots. With the commonly used
tracking-by-detection paradigm, 3D MOT has made important progress in recent
years. However, these methods only use the detection boxes of the current frame
to obtain trajectory-box association results, which makes it impossible for the
tracker to recover objects missed by the detector. In this paper, we present
TrajectoryFormer, a novel point-cloud-based 3D MOT framework. To recover the
missed object by detector, we generates multiple trajectory hypotheses with
hybrid candidate boxes, including temporally predicted boxes and current-frame
detection boxes, for trajectory-box association. The predicted boxes can
propagate object's history trajectory information to the current frame and thus
the network can tolerate short-term miss detection of the tracked objects. We
combine long-term object motion feature and short-term object appearance
feature to create per-hypothesis feature embedding, which reduces the
computational overhead for spatial-temporal encoding. Additionally, we
introduce a Global-Local Interaction Module to conduct information interaction
among all hypotheses and models their spatial relations, leading to accurate
estimation of hypotheses. Our TrajectoryFormer achieves state-of-the-art
performance on the Waymo 3D MOT benchmarks.
</p></li>
</ul>

<h3>Title: Word sense extension. (arXiv:2306.05609v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05609">http://arxiv.org/abs/2306.05609</a></li>
<li>Code URL: <a href="https://github.com/jadeleiyu/word_sense_extension">https://github.com/jadeleiyu/word_sense_extension</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05609] Word sense extension](http://arxiv.org/abs/2306.05609) #transformer</code></li>
<li>Summary: <p>Humans often make creative use of words to express novel senses. A
long-standing effort in natural language processing has been focusing on word
sense disambiguation (WSD), but little has been explored about how the sense
inventory of a word may be extended toward novel meanings. We present a
paradigm of word sense extension (WSE) that enables words to spawn new senses
toward novel context. We develop a framework that simulates novel word sense
extension by first partitioning a polysemous word type into two pseudo-tokens
that mark its different senses, and then inferring whether the meaning of a
pseudo-token can be extended to convey the sense denoted by the token
partitioned from the same word type. Our framework combines cognitive models of
chaining with a learning scheme that transforms a language model embedding
space to support various types of word sense extension. We evaluate our
framework against several competitive baselines and show that it is superior in
predicting plausible novel senses for over 7,500 English words. Furthermore, we
show that our WSE framework improves performance over a range of
transformer-based WSD models in predicting rare word senses with few or zero
mentions in the training data.
</p></li>
</ul>

<h3>Title: Transformer-based Time-to-Event Prediction for Chronic Kidney Disease Deterioration. (arXiv:2306.05779v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05779">http://arxiv.org/abs/2306.05779</a></li>
<li>Code URL: <a href="https://github.com/dviraran/strafe">https://github.com/dviraran/strafe</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05779] Transformer-based Time-to-Event Prediction for Chronic Kidney Disease Deterioration](http://arxiv.org/abs/2306.05779) #transformer</code></li>
<li>Summary: <p>Deep-learning techniques, particularly the transformer model, have shown
great potential in enhancing the prediction performance of longitudinal health
records. While previous methods have mainly focused on fixed-time risk
prediction, time-to-event prediction (also known as survival analysis) is often
more appropriate for clinical scenarios. Here, we present a novel deep-learning
architecture we named STRAFE, a generalizable survival analysis
transformer-based architecture for electronic health records. The performance
of STRAFE was evaluated using a real-world claim dataset of over 130,000
individuals with stage 3 chronic kidney disease (CKD) and was found to
outperform other time-to-event prediction algorithms in predicting the exact
time of deterioration to stage 5. Additionally, STRAFE was found to outperform
binary outcome algorithms in predicting fixed-time risk, possibly due to its
ability to train on censored data. We show that STRAFE predictions can improve
the positive predictive value of high-risk patients by 3-fold, demonstrating
possible usage to improve targeting for intervention programs. Finally, we
suggest a novel visualization approach to predictions on a per-patient basis.
In conclusion, STRAFE is a cutting-edge time-to-event prediction algorithm that
has the potential to enhance risk predictions in large claims datasets.
</p></li>
</ul>

<h3>Title: Virtual Node Tuning for Few-shot Node Classification. (arXiv:2306.06063v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06063">http://arxiv.org/abs/2306.06063</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06063] Virtual Node Tuning for Few-shot Node Classification](http://arxiv.org/abs/2306.06063) #transformer</code></li>
<li>Summary: <p>Few-shot Node Classification (FSNC) is a challenge in graph representation
learning where only a few labeled nodes per class are available for training.
To tackle this issue, meta-learning has been proposed to transfer structural
knowledge from base classes with abundant labels to target novel classes.
However, existing solutions become ineffective or inapplicable when base
classes have no or limited labeled nodes. To address this challenge, we propose
an innovative method dubbed Virtual Node Tuning (VNT). Our approach utilizes a
pretrained graph transformer as the encoder and injects virtual nodes as soft
prompts in the embedding space, which can be optimized with few-shot labels in
novel classes to modulate node embeddings for each specific FSNC task. A unique
feature of VNT is that, by incorporating a Graph-based Pseudo Prompt Evolution
(GPPE) module, VNT-GPPE can handle scenarios with sparse labels in base
classes. Experimental results on four datasets demonstrate the superiority of
the proposed approach in addressing FSNC with unlabeled or sparsely labeled
base classes, outperforming existing state-of-the-art methods and even fully
supervised baselines.
</p></li>
</ul>

<h3>Title: Prodigy: An Expeditiously Adaptive Parameter-Free Learner. (arXiv:2306.06101v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06101">http://arxiv.org/abs/2306.06101</a></li>
<li>Code URL: <a href="https://github.com/konstmish/prodigy">https://github.com/konstmish/prodigy</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06101] Prodigy: An Expeditiously Adaptive Parameter-Free Learner](http://arxiv.org/abs/2306.06101) #transformer</code></li>
<li>Summary: <p>We consider the problem of estimating the learning rate in adaptive methods,
such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to
provably estimate the distance to the solution $D$, which is needed to set the
learning rate optimally. Our techniques are modifications of the D-Adaptation
method for learning-rate-free learning. Our methods improve upon the
convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where
$d_0$ is the initial estimate of $D$. We test our methods on 12 common
logistic-regression benchmark datasets, VGG11 and ResNet-50 training on
CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on
Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT
transformer training on BookWiki. Our experimental results show that our
approaches consistently outperform D-Adaptation and reach test accuracy values
close to that of hand-tuned Adam.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Motion-DVAE: Unsupervised learning for fast human motion denoising. (arXiv:2306.05846v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05846">http://arxiv.org/abs/2306.05846</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05846] Motion-DVAE: Unsupervised learning for fast human motion denoising](http://arxiv.org/abs/2306.05846) #generative</code></li>
<li>Summary: <p>Pose and motion priors are crucial for recovering realistic and accurate
human motion from noisy observations. Substantial progress has been made on
pose and shape estimation from images, and recent works showed impressive
results using priors to refine frame-wise predictions. However, a lot of motion
priors only model transitions between consecutive poses and are used in
time-consuming optimization procedures, which is problematic for many
applications requiring real-time motion capture. We introduce Motion-DVAE, a
motion prior to capture the short-term dependencies of human motion. As part of
the dynamical variational autoencoder (DVAE) models family, Motion-DVAE
combines the generative capability of VAE models and the temporal modeling of
recurrent architectures. Together with Motion-DVAE, we introduce an
unsupervised learned denoising method unifying regression- and
optimization-based approaches in a single framework for real-time 3D human pose
estimation. Experiments show that the proposed approach reaches competitive
performance with state-of-the-art methods while being much faster.
</p></li>
</ul>

<h3>Title: Sketch2Stress: Sketching with Structural Stress Awareness. (arXiv:2306.05911v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05911">http://arxiv.org/abs/2306.05911</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05911] Sketch2Stress: Sketching with Structural Stress Awareness](http://arxiv.org/abs/2306.05911) #generative</code></li>
<li>Summary: <p>In the process of product design and digital fabrication, the structural
analysis of a designed prototype is a fundamental and essential step. However,
such a step is usually invisible or inaccessible to designers at the early
sketching phase. This limits the user's ability to consider a shape's physical
properties and structural soundness. To bridge this gap, we introduce a novel
approach Sketch2Stress that allows users to perform structural analysis of
desired objects at the sketching stage. This method takes as input a 2D
freehand sketch and one or multiple locations of user-assigned external forces.
With the specially-designed two-branch generative-adversarial framework, it
automatically predicts a normal map and a corresponding structural stress map
distributed over the user-sketched underlying object. In this way, our method
empowers designers to easily examine the stress sustained everywhere and
identify potential problematic regions of their sketched object. Furthermore,
combined with the predicted normal map, users are able to conduct a region-wise
structural analysis efficiently by aggregating the stress effects of multiple
forces in the same direction. Finally, we demonstrate the effectiveness and
practicality of our system with extensive experiments and user studies.
</p></li>
</ul>

<h3>Title: GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields. (arXiv:2306.06044v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06044">http://arxiv.org/abs/2306.06044</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06044] GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields](http://arxiv.org/abs/2306.06044) #generative</code></li>
<li>Summary: <p>Neural Radiance Fields (NeRF) have shown impressive novel view synthesis
results; nonetheless, even thorough recordings yield imperfections in
reconstructions, for instance due to poorly observed areas or minor lighting
changes. Our goal is to mitigate these imperfections from various sources with
a joint solution: we take advantage of the ability of generative adversarial
networks (GANs) to produce realistic images and use them to enhance realism in
3D scene reconstruction with NeRFs. To this end, we learn the patch
distribution of a scene using an adversarial discriminator, which provides
feedback to the radiance field reconstruction, thus improving realism in a
3D-consistent fashion. Thereby, rendering artifacts are repaired directly in
the underlying 3D representation by imposing multi-view path rendering
constraints. In addition, we condition a generator with multi-resolution NeRF
renderings which is adversarially trained to further improve rendering quality.
We demonstrate that our approach significantly improves rendering quality,
e.g., nearly halving LPIPS scores compared to Nerfacto while at the same time
improving PSNR by 1.4dB on the advanced indoor scenes of Tanks and Temples.
</p></li>
</ul>

<h3>Title: LexGPT 0.1: pre-trained GPT-J models with Pile of Law. (arXiv:2306.05431v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05431">http://arxiv.org/abs/2306.05431</a></li>
<li>Code URL: <a href="https://github.com/jiehsheng/lexgpt">https://github.com/jiehsheng/lexgpt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05431] LexGPT 0](http://arxiv.org/abs/2306.05431) #generative</code></li>
<li>Summary: <p>This research aims to build generative language models specialized for the
legal domain. The manuscript presents the development of LexGPT models based on
GPT-J models and pre-trained with Pile of Law. The foundation model built in
this manuscript is the initial step for the development of future applications
in the legal domain, such as further training with reinforcement learning from
human feedback. Another objective of this manuscript is to assist legal
professionals in utilizing language models through the ``No Code'' approach. By
fine-tuning models with specialized data and without modifying any source code,
legal professionals can create custom language models for downstream tasks with
minimum effort and technical knowledge. The downstream task in this manuscript
is to turn a LexGPT model into a classifier, although the performance is
notably lower than the state-of-the-art result. How to enhance downstream task
performance without modifying the model or its source code is a research topic
for future exploration.
</p></li>
</ul>

<h3>Title: A Unified Generative Approach to Product Attribute-Value Identification. (arXiv:2306.05605v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05605">http://arxiv.org/abs/2306.05605</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05605] A Unified Generative Approach to Product Attribute-Value Identification](http://arxiv.org/abs/2306.05605) #generative</code></li>
<li>Summary: <p>Product attribute-value identification (PAVI) has been studied to link
products on e-commerce sites with their attribute values (e.g., <Material,
Cotton>) using product text as clues. Technical demands from real-world
e-commerce platforms require PAVI methods to handle unseen values,
multi-attribute values, and canonicalized values, which are only partly
addressed in existing extraction- and classification-based approaches.
Motivated by this, we explore a generative approach to the PAVI task. We
finetune a pre-trained generative model, T5, to decode a set of attribute-value
pairs as a target sequence from the given product text. Since the attribute
value pairs are unordered set elements, how to linearize them will matter; we,
thus, explore methods of composing an attribute-value pair and ordering the
pairs for the task. Experimental results confirm that our generation-based
approach outperforms the existing extraction and classification-based methods
on large-scale real-world datasets meant for those methods.
</p></li>
</ul>

<h3>Title: Trapping LLM Hallucinations Using Tagged Context Prompts. (arXiv:2306.06085v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06085">http://arxiv.org/abs/2306.06085</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06085] Trapping LLM Hallucinations Using Tagged Context Prompts](http://arxiv.org/abs/2306.06085) #generative</code></li>
<li>Summary: <p>Recent advances in large language models (LLMs), such as ChatGPT, have led to
highly sophisticated conversation agents. However, these models suffer from
"hallucinations," where the model generates false or fabricated information.
Addressing this challenge is crucial, particularly with AI-driven platforms
being adopted across various sectors. In this paper, we propose a novel method
to recognize and flag instances when LLMs perform outside their domain
knowledge, and ensuring users receive accurate information.
</p></li>
</ul>

<p>We find that the use of context combined with embedded tags can successfully
combat hallucinations within generative language models. To do this, we
baseline hallucination frequency in no-context prompt-response pairs using
generated URLs as easily-tested indicators of fabricated data. We observed a
significant reduction in overall hallucination when context was supplied along
with question prompts for tested generative engines. Lastly, we evaluated how
placing tags within contexts impacted model responses and were able to
eliminate hallucinations in responses with 98.88% effectiveness.
</p>

<h3>Title: Prediction of Transportation Index for Urban Patterns in Small and Medium-sized Indian Cities using Hybrid RidgeGAN Model. (arXiv:2306.05951v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05951">http://arxiv.org/abs/2306.05951</a></li>
<li>Code URL: <a href="https://github.com/rahisha-thottolil/ridgegan">https://github.com/rahisha-thottolil/ridgegan</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05951] Prediction of Transportation Index for Urban Patterns in Small and Medium-sized Indian Cities using Hybrid RidgeGAN Model](http://arxiv.org/abs/2306.05951) #generative</code></li>
<li>Summary: <p>The rapid urbanization trend in most developing countries including India is
creating a plethora of civic concerns such as loss of green space, degradation
of environmental health, clean water availability, air pollution, traffic
congestion leading to delays in vehicular transportation, etc. Transportation
and network modeling through transportation indices have been widely used to
understand transportation problems in the recent past. This necessitates
predicting transportation indices to facilitate sustainable urban planning and
traffic management. Recent advancements in deep learning research, in
particular, Generative Adversarial Networks (GANs), and their modifications in
spatial data analysis such as CityGAN, Conditional GAN, and MetroGAN have
enabled urban planners to simulate hyper-realistic urban patterns. These
synthetic urban universes mimic global urban patterns and evaluating their
landscape structures through spatial pattern analysis can aid in comprehending
landscape dynamics, thereby enhancing sustainable urban planning. This research
addresses several challenges in predicting the urban transportation index for
small and medium-sized Indian cities. A hybrid framework based on Kernel Ridge
Regression (KRR) and CityGAN is introduced to predict transportation index
using spatial indicators of human settlement patterns. This paper establishes a
relationship between the transportation index and human settlement indicators
and models it using KRR for the selected 503 Indian cities. The proposed hybrid
pipeline, we call it RidgeGAN model, can evaluate the sustainability of urban
sprawl associated with infrastructure development and transportation systems in
sprawling cities. Experimental results show that the two-step pipeline approach
outperforms existing benchmarks based on spatial and statistical measures.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Multi-Modal Classifiers for Open-Vocabulary Object Detection. (arXiv:2306.05493v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05493">http://arxiv.org/abs/2306.05493</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05493] Multi-Modal Classifiers for Open-Vocabulary Object Detection](http://arxiv.org/abs/2306.05493) #large language model</code></li>
<li>Summary: <p>The goal of this paper is open-vocabulary object detection (OVOD)
$\unicode{x2013}$ building a model that can detect objects beyond the set of
categories seen at training, thus enabling the user to specify categories of
interest at inference without the need for model retraining. We adopt a
standard two-stage object detector architecture, and explore three ways for
specifying novel categories: via language descriptions, via image exemplars, or
via a combination of the two. We make three contributions: first, we prompt a
large language model (LLM) to generate informative language descriptions for
object classes, and construct powerful text-based classifiers; second, we
employ a visual aggregator on image exemplars that can ingest any number of
images as input, forming vision-based classifiers; and third, we provide a
simple method to fuse information from language descriptions and image
exemplars, yielding a multi-modal classifier. When evaluating on the
challenging LVIS open-vocabulary benchmark we demonstrate that: (i) our
text-based classifiers outperform all previous OVOD works; (ii) our
vision-based classifiers perform as well as text-based classifiers in prior
work; (iii) using multi-modal classifiers perform better than either modality
alone; and finally, (iv) our text-based and multi-modal classifiers yield
better performance than a fully-supervised detector.
</p></li>
</ul>

<h3>Title: Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding. (arXiv:2306.06094v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06094">http://arxiv.org/abs/2306.06094</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06094] Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding](http://arxiv.org/abs/2306.06094) #large language model</code></li>
<li>Summary: <p>Recently, large language models (LLMs) have made significant advancements in
natural language understanding and generation. However, their potential in
computer vision remains largely unexplored. In this paper, we introduce a new,
exploratory approach that enables LLMs to process images using the Scalable
Vector Graphics (SVG) format. By leveraging the XML-based textual descriptions
of SVG representations instead of raster images, we aim to bridge the gap
between the visual and textual modalities, allowing LLMs to directly understand
and manipulate images without the need for parameterized visual components. Our
method facilitates simple image classification, generation, and in-context
learning using only LLM capabilities. We demonstrate the promise of our
approach across discriminative and generative tasks, highlighting its (i)
robustness against distribution shift, (ii) substantial improvements achieved
by tapping into the in-context learning abilities of LLMs, and (iii) image
understanding and generation capabilities with human guidance. Our code, data,
and models can be found here https://github.com/mu-cai/svg-llm.
</p></li>
</ul>

<h3>Title: Towards End-to-end Speech-to-text Summarization. (arXiv:2306.05432v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05432">http://arxiv.org/abs/2306.05432</a></li>
<li>Code URL: <a href="https://github.com/priberam/s2tsumm">https://github.com/priberam/s2tsumm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05432] Towards End-to-end Speech-to-text Summarization](http://arxiv.org/abs/2306.05432) #large language model</code></li>
<li>Summary: <p>Speech-to-text (S2T) summarization is a time-saving technique for filtering
and keeping up with the broadcast news uploaded online on a daily basis. The
rise of large language models from deep learning with impressive text
generation capabilities has placed the research focus on summarization systems
that produce paraphrased compact versions of the document content, also known
as abstractive summaries. End-to-end (E2E) modelling of S2T abstractive
summarization is a promising approach that offers the possibility of generating
rich latent representations that leverage non-verbal and acoustic information,
as opposed to the use of only linguistic information from automatically
generated transcripts in cascade systems. However, the few literature on E2E
modelling of this task fails on exploring different domains, namely broadcast
news, which is challenging domain where large and diversified volumes of data
are presented to the user every day. We model S2T summarization both with a
cascade and an E2E system for a corpus of broadcast news in French. Our novel
E2E model leverages external data by resorting to transfer learning from a
pre-trained T2T summarizer. Experiments show that both our cascade and E2E
abstractive summarizers are stronger than an extractive baseline. However, the
performance of the E2E model still lies behind the cascade one, which is object
of an extensive analysis that includes future directions to close that gap.
</p></li>
</ul>

<h3>Title: PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance. (arXiv:2306.05443v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05443">http://arxiv.org/abs/2306.05443</a></li>
<li>Code URL: <a href="https://github.com/chancefocus/pixiu">https://github.com/chancefocus/pixiu</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05443] PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance](http://arxiv.org/abs/2306.05443) #large language model</code></li>
<li>Summary: <p>Although large language models (LLMs) has shown great performance on natural
language processing (NLP) in the financial domain, there are no publicly
available financial tailtored LLMs, instruction tuning datasets, and evaluation
benchmarks, which is critical for continually pushing forward the open-source
development of financial artificial intelligence (AI). This paper introduces
PIXIU, a comprehensive framework including the first financial LLM based on
fine-tuning LLaMA with instruction data, the first instruction data with 136K
data samples to support the fine-tuning, and an evaluation benchmark with 5
tasks and 9 datasets. We first construct the large-scale multi-task instruction
data considering a variety of financial tasks, financial document types, and
financial data modalities. We then propose a financial LLM called FinMA by
fine-tuning LLaMA with the constructed dataset to be able to follow
instructions for various financial tasks. To support the evaluation of
financial LLMs, we propose a standardized benchmark that covers a set of
critical financial tasks, including five financial NLP tasks and one financial
prediction task. With this benchmark, we conduct a detailed analysis of FinMA
and several existing LLMs, uncovering their strengths and weaknesses in
handling critical financial tasks. The model, datasets, benchmark, and
experimental results are open-sourced to facilitate future research in
financial AI.
</p></li>
</ul>

<h3>Title: AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization. (arXiv:2306.05537v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05537">http://arxiv.org/abs/2306.05537</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05537] AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization](http://arxiv.org/abs/2306.05537) #large language model</code></li>
<li>Summary: <p>The rapid growth of information on the Internet has led to an overwhelming
amount of opinions and comments on various activities, products, and services.
This makes it difficult and time-consuming for users to process all the
available information when making decisions. Text summarization, a Natural
Language Processing (NLP) task, has been widely explored to help users quickly
retrieve relevant information by generating short and salient content from long
or multiple documents. Recent advances in pre-trained language models, such as
ChatGPT, have demonstrated the potential of Large Language Models (LLMs) in
text generation. However, LLMs require massive amounts of data and resources
and are challenging to implement as offline applications. Furthermore, existing
text summarization approaches often lack the ``adaptive" nature required to
capture diverse aspects in opinion summarization, which is particularly
detrimental to users with specific requirements or preferences. In this paper,
we propose an Aspect-adaptive Knowledge-based Opinion Summarization model for
product reviews, which effectively captures the adaptive nature required for
opinion summarization. The model generates aspect-oriented summaries given a
set of reviews for a particular product, efficiently providing users with
useful information on specific aspects they are interested in, ensuring the
generated summaries are more personalized and informative. Extensive
experiments have been conducted using real-world datasets to evaluate the
proposed model. The results demonstrate that our model outperforms
state-of-the-art approaches and is adaptive and efficient in generating
summaries that focus on particular aspects, enabling users to make
well-informed decisions and catering to their diverse interests and
preferences.
</p></li>
</ul>

<h3>Title: DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text. (arXiv:2306.05540v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05540">http://arxiv.org/abs/2306.05540</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05540] DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text](http://arxiv.org/abs/2306.05540) #large language model</code></li>
<li>Summary: <p>With the rapid progress of large language models (LLMs) and the huge amount
of text they generated, it becomes more and more impractical to manually
distinguish whether a text is machine-generated. Given the growing use of LLMs
in social media and education, it prompts us to develop methods to detect
machine-generated text, preventing malicious usage such as plagiarism,
misinformation, and propaganda. Previous work has studied several zero-shot
methods, which require no training data. These methods achieve good
performance, but there is still a lot of room for improvement. In this paper,
we introduce two novel zero-shot methods for detecting machine-generated text
by leveraging the log rank information. One is called DetectLLM-LRR, which is
fast and efficient, and the other is called DetectLLM-NPR, which is more
accurate, but slower due to the need for perturbations. Our experiments on
three datasets and seven language models show that our proposed methods improve
over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover,
DetectLLM-NPR needs fewer perturbations than previous work to achieve the same
level of performance, which makes it more practical for real-world use. We also
investigate the efficiency--performance trade-off based on users preference on
these two measures and we provide intuition for using them in practice
effectively. We release the data and the code of both methods in
https://github.com/mbzuai-nlp/DetectLLM
</p></li>
</ul>

<h3>Title: Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05685">http://arxiv.org/abs/2306.05685</a></li>
<li>Code URL: <a href="https://github.com/lm-sys/fastchat">https://github.com/lm-sys/fastchat</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05685] Judging LLM-as-a-judge with MT-Bench and Chatbot Arena](http://arxiv.org/abs/2306.05685) #large language model</code></li>
<li>Summary: <p>Evaluating large language model (LLM) based chat assistants is challenging
due to their broad capabilities and the inadequacy of existing benchmarks in
measuring human preferences. To address this, we explore using strong LLMs as
judges to evaluate these models on more open-ended questions. We examine the
usage and limitations of LLM-as-a-judge, such as position and verbosity biases
and limited reasoning ability, and propose solutions to migrate some of them.
We then verify the agreement between LLM judges and human preferences by
introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot
Arena, a crowdsourced battle platform. Our results reveal that strong LLM
judges like GPT-4 can match both controlled and crowdsourced human preferences
well, achieving over 80\% agreement, the same level of agreement between
humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate
human preferences, which are otherwise very expensive to obtain. Additionally,
we show our benchmark and traditional benchmarks complement each other by
evaluating several variants of LLaMA/Vicuna. We will publicly release 80
MT-bench questions, 3K expert votes, and 30K conversations with human
preferences from Chatbot Arena.
</p></li>
</ul>

<h3>Title: Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. (arXiv:2306.05783v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05783">http://arxiv.org/abs/2306.05783</a></li>
<li>Code URL: <a href="https://github.com/mikegu721/xiezhibenchmark">https://github.com/mikegu721/xiezhibenchmark</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05783] Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation](http://arxiv.org/abs/2306.05783) #large language model</code></li>
<li>Summary: <p>New Natural Langauge Process~(NLP) benchmarks are urgently needed to align
with the rapid development of large language models (LLMs). We present Xiezhi,
the most comprehensive evaluation suite designed to assess holistic domain
knowledge. Xiezhi comprises multiple-choice questions across 516 diverse
disciplines ranging from 13 different subjects with 220,000 questions and
accompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k
questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results
indicate that LLMs exceed average performance of humans in science,
engineering, agronomy, medicine, and art, but fall short in economics,
jurisprudence, pedagogy, literature, history, and management. We anticipate
Xiezhi will help analyze important strengths and shortcomings of LLMs, and the
benchmark is released in https://github.com/MikeGu721/XiezhiBenchmark .
</p></li>
</ul>

<h3>Title: Towards the Exploitation of LLM-based Chatbot for Providing Legal Support to Palestinian Cooperatives. (arXiv:2306.05827v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05827">http://arxiv.org/abs/2306.05827</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05827] Towards the Exploitation of LLM-based Chatbot for Providing Legal Support to Palestinian Cooperatives](http://arxiv.org/abs/2306.05827) #large language model</code></li>
<li>Summary: <p>With the ever-increasing utilization of natural language processing (NLP), we
started to witness over the past few years a significant transformation in our
interaction with legal texts. This technology has advanced the analysis and
enhanced the understanding of complex legal terminology and contexts. The
development of recent large language models (LLMs), particularly ChatGPT, has
also introduced a revolutionary contribution to the way that legal texts can be
processed and comprehended. In this paper, we present our work on a
cooperative-legal question-answering LLM-based chatbot, where we developed a
set of legal questions about Palestinian cooperatives, associated with their
regulations and compared the auto-generated answers by the chatbot to their
correspondences that are designed by a legal expert. To evaluate the proposed
chatbot, we have used 50 queries generated by the legal expert and compared the
answers produced by the chart to their relevance judgments. Finding
demonstrated that an overall accuracy rate of 82% has been achieved when
answering the queries, while exhibiting an F1 score equivalent to 79%.
</p></li>
</ul>

<h3>Title: Can Large Language Models Infer Causation from Correlation?. (arXiv:2306.05836v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05836">http://arxiv.org/abs/2306.05836</a></li>
<li>Code URL: <a href="https://github.com/causalnlp/corr2cause">https://github.com/causalnlp/corr2cause</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05836] Can Large Language Models Infer Causation from Correlation?](http://arxiv.org/abs/2306.05836) #large language model</code></li>
<li>Summary: <p>Causal inference is one of the hallmarks of human intelligence. While the
field of CausalNLP has attracted much interest in the recent years, existing
causal inference datasets in NLP primarily rely on discovering causality from
empirical knowledge (e.g., commonsense knowledge). In this work, we propose the
first benchmark dataset to test the pure causal inference skills of large
language models (LLMs). Specifically, we formulate a novel task Corr2Cause,
which takes a set of correlational statements and determines the causal
relationship between the variables. We curate a large-scale dataset of more
than 400K samples, on which we evaluate seventeen existing LLMs. Through our
experiments, we identify a key shortcoming of LLMs in terms of their causal
inference skills, and show that these models achieve almost close to random
performance on the task. This shortcoming is somewhat mitigated when we try to
re-purpose LLMs for this skill via finetuning, but we find that these models
still fail to generalize -- they can only perform causal inference in
in-distribution settings when variable names and textual expressions used in
the queries are similar to those in the training set, but fail in
out-of-distribution settings generated by perturbing these queries. Corr2Cause
is a challenging task for LLMs, and would be helpful in guiding future research
on improving LLMs' pure reasoning skills and generalizability. Our data is at
https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at
https://github.com/causalNLP/corr2cause.
</p></li>
</ul>

<h3>Title: Mind2Web: Towards a Generalist Agent for the Web. (arXiv:2306.06070v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06070">http://arxiv.org/abs/2306.06070</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06070] Mind2Web: Towards a Generalist Agent for the Web](http://arxiv.org/abs/2306.06070) #large language model</code></li>
<li>Summary: <p>We introduce Mind2Web, the first dataset for developing and evaluating
generalist agents for the web that can follow language instructions to complete
complex tasks on any website. Existing datasets for web agents either use
simulated websites or only cover a limited set of websites and tasks, thus not
suitable for generalist web agents. With over 2,000 open-ended tasks collected
from 137 websites spanning 31 domains and crowdsourced action sequences for the
tasks, Mind2Web provides three necessary ingredients for building generalist
web agents: 1) diverse domains, websites, and tasks, 2) use of real-world
websites instead of simulated and simplified ones, and 3) a broad spectrum of
user interaction patterns. Based on Mind2Web, we conduct an initial exploration
of using large language models (LLMs) for building generalist web agents. While
the raw HTML of real-world websites are often too large to be fed to LLMs, we
show that first filtering it with a small LM significantly improves the
effectiveness and efficiency of LLMs. Our solution demonstrates a decent level
of performance, even on websites or entire domains the model has never seen
before, but there is still a substantial room to improve towards truly
generalizable agents. We open-source our dataset, model implementation, and
trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further
research on building a generalist agent for the web.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: A Novel Confidence Induced Class Activation Mapping for MRI Brain Tumor Segmentation. (arXiv:2306.05476v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05476">http://arxiv.org/abs/2306.05476</a></li>
<li>Code URL: <a href="https://github.com/windstormer/Cfd-CAM">https://github.com/windstormer/Cfd-CAM</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05476] A Novel Confidence Induced Class Activation Mapping for MRI Brain Tumor Segmentation](http://arxiv.org/abs/2306.05476) #segmentation</code></li>
<li>Summary: <p>Magnetic resonance imaging (MRI) is a commonly used technique for brain tumor
segmentation, which is critical for evaluating patients and planning treatment.
To make the labeling process less laborious and dependent on expertise,
weakly-supervised semantic segmentation (WSSS) methods using class activation
mapping (CAM) have been proposed. However, current CAM-based WSSS methods
generate the object localization map using internal neural network information,
such as gradient or trainable parameters, which can lead to suboptimal
solutions. To address these issues, we propose the confidence-induced CAM
(Cfd-CAM), which calculates the weight of each feature map by using the
confidence of the target class. Our experiments on two brain tumor datasets
show that Cfd-CAM outperforms existing state-of-the-art methods under the same
level of supervision. Overall, our proposed Cfd-CAM approach improves the
accuracy of brain tumor segmentation and may provide valuable insights for
developing better WSSS methods for other medical imaging tasks.
</p></li>
</ul>

<h3>Title: Multi-body SE(3) Equivariance for Unsupervised Rigid Segmentation and Motion Estimation. (arXiv:2306.05584v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05584">http://arxiv.org/abs/2306.05584</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05584] Multi-body SE(3) Equivariance for Unsupervised Rigid Segmentation and Motion Estimation](http://arxiv.org/abs/2306.05584) #segmentation</code></li>
<li>Summary: <p>A truly generalizable approach to rigid segmentation and motion estimation is
fundamental to 3D understanding of articulated objects and moving scenes. In
view of the tightly coupled relationship between segmentation and motion
estimates, we present an SE(3) equivariant architecture and a training strategy
to tackle this task in an unsupervised manner. Our architecture comprises two
lightweight and inter-connected heads that predict segmentation masks using
point-level invariant features and motion estimates from SE(3) equivariant
features without the prerequisites of category information. Our unified
training strategy can be performed online while jointly optimizing the two
predictions by exploiting the interrelations among scene flow, segmentation
mask, and rigid transformations. We show experiments on four datasets as
evidence of the superiority of our method both in terms of model performance
and computational efficiency with only 0.25M parameters and 0.92G FLOPs. To the
best of our knowledge, this is the first work designed for category-agnostic
part-level SE(3) equivariance in dynamic point clouds.
</p></li>
</ul>

<h3>Title: Topology-Aware Uncertainty for Image Segmentation. (arXiv:2306.05671v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05671">http://arxiv.org/abs/2306.05671</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05671] Topology-Aware Uncertainty for Image Segmentation](http://arxiv.org/abs/2306.05671) #segmentation</code></li>
<li>Summary: <p>Segmentation of curvilinear structures such as vasculature and road networks
is challenging due to relatively weak signals and complex geometry/topology. To
facilitate and accelerate large scale annotation, one has to adopt
semi-automatic approaches such as proofreading by experts. In this work, we
focus on uncertainty estimation for such tasks, so that highly uncertain, and
thus error-prone structures can be identified for human annotators to verify.
Unlike most existing works, which provide pixel-wise uncertainty maps, we
stipulate it is crucial to estimate uncertainty in the units of topological
structures, e.g., small pieces of connections and branches. To achieve this, we
leverage tools from topological data analysis, specifically discrete Morse
theory (DMT), to first capture the structures, and then reason about their
uncertainties. To model the uncertainty, we (1) propose a joint prediction
model that estimates the uncertainty of a structure while taking the
neighboring structures into consideration (inter-structural uncertainty); (2)
propose a novel Probabilistic DMT to model the inherent uncertainty within each
structure (intra-structural uncertainty) by sampling its representations via a
perturb-and-walk scheme. On various 2D and 3D datasets, our method produces
better structure-wise uncertainty maps compared to existing works.
</p></li>
</ul>

<h3>Title: How Object Information Improves Skeleton-based Human Action Recognition in Assembly Tasks. (arXiv:2306.05844v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05844">http://arxiv.org/abs/2306.05844</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05844] How Object Information Improves Skeleton-based Human Action Recognition in Assembly Tasks](http://arxiv.org/abs/2306.05844) #segmentation</code></li>
<li>Summary: <p>As the use of collaborative robots (cobots) in industrial manufacturing
continues to grow, human action recognition for effective human-robot
collaboration becomes increasingly important. This ability is crucial for
cobots to act autonomously and assist in assembly tasks. Recently,
skeleton-based approaches are often used as they tend to generalize better to
different people and environments. However, when processing skeletons alone,
information about the objects a human interacts with is lost. Therefore, we
present a novel approach of integrating object information into skeleton-based
action recognition. We enhance two state-of-the-art methods by treating object
centers as further skeleton joints. Our experiments on the assembly dataset
IKEA ASM show that our approach improves the performance of these
state-of-the-art methods to a large extent when combining skeleton joints with
objects predicted by a state-of-the-art instance segmentation model. Our
research sheds light on the benefits of combining skeleton joints with object
information for human action recognition in assembly tasks. We analyze the
effect of the object detector on the combination for action classification and
discuss the important factors that must be taken into account.
</p></li>
</ul>

<h3>Title: 3D objects and scenes classification, recognition, segmentation, and reconstruction using 3D point cloud data: A review. (arXiv:2306.05978v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05978">http://arxiv.org/abs/2306.05978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05978] 3D objects and scenes classification, recognition, segmentation, and reconstruction using 3D point cloud data: A review](http://arxiv.org/abs/2306.05978) #segmentation</code></li>
<li>Summary: <p>Three-dimensional (3D) point cloud analysis has become one of the attractive
subjects in realistic imaging and machine visions due to its simplicity,
flexibility and powerful capacity of visualization. Actually, the
representation of scenes and buildings using 3D shapes and formats leveraged
many applications among which automatic driving, scenes and objects
reconstruction, etc. Nevertheless, working with this emerging type of data has
been a challenging task for objects representation, scenes recognition,
segmentation, and reconstruction. In this regard, a significant effort has
recently been devoted to developing novel strategies, using different
techniques such as deep learning models. To that end, we present in this paper
a comprehensive review of existing tasks on 3D point cloud: a well-defined
taxonomy of existing techniques is performed based on the nature of the adopted
algorithms, application scenarios, and main objectives. Various tasks performed
on 3D point could data are investigated, including objects and scenes
detection, recognition, segmentation and reconstruction. In addition, we
introduce a list of used datasets, we discuss respective evaluation metrics and
we compare the performance of existing solutions to better inform the
state-of-the-art and identify their limitations and strengths. Lastly, we
elaborate on current challenges facing the subject of technology and future
trends attracting considerable interest, which could be a starting point for
upcoming research studies
</p></li>
</ul>

<h3>Title: Detection of Late Blight Disease in Tomato Leaf Using Image Processing Techniques. (arXiv:2306.06080v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06080">http://arxiv.org/abs/2306.06080</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06080] Detection of Late Blight Disease in Tomato Leaf Using Image Processing Techniques](http://arxiv.org/abs/2306.06080) #segmentation</code></li>
<li>Summary: <p>=One of the most frequently farmed crops is the tomato crop. Late blight is
the most prevalent tomato disease in the world, and often causes a significant
reduction in the production of tomato crops. The importance of tomatoes as an
agricultural product necessitates early detection of late blight. It is
produced by the fungus Phytophthora. The earliest signs of late blight on
tomatoes are unevenly formed, water-soaked lesions on the leaves located on the
plant canopy's younger leave White cottony growth may appear in humid
environments evident on the undersides of the leaves that have been impacted.
Lesions increase as the disease proceeds, turning the leaves brown to shrivel
up and die. Using picture segmentation and the Multi-class SVM technique, late
blight disorder is discovered in this work. Image segmentation is employed for
separating damaged areas on leaves, and the Multi-class SVM method is used for
reliable disease categorization. 30 reputable studies were chosen from a total
of 2770 recognized papers. The primary goal of this study is to compile
cutting-edge research that identifies current research trends, problems, and
prospects for late blight detection. It also looks at current approaches for
applying image processing to diagnose and detect late blight. A suggested
taxonomy for late blight detection has also been provided. In the same way, a
model for the development of the solutions to problems is also presented.
Finally, the research gaps have been presented in terms of open issues for the
provision of future directions in image processing for the researchers.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
