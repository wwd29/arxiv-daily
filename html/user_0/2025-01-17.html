<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-01-17</h1>
<h3>Title: Cyber-Physical Security Vulnerabilities Identification and Classification in Smart Manufacturing - A Defense-in-Depth Driven Framework and Taxonomy</h3>
<ul>
<li><strong>Authors: </strong>Md Habibor Rahman (1), Mohammed Shafae (2) ((1) University of Massachusetts Dartmouth, (2) The University of Arizona)</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09023">https://arxiv.org/abs/2501.09023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09023">https://arxiv.org/pdf/2501.09023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09023]] Cyber-Physical Security Vulnerabilities Identification and Classification in Smart Manufacturing - A Defense-in-Depth Driven Framework and Taxonomy(https://arxiv.org/abs/2501.09023)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense</a></li>
<li><strong>Abstract: </strong>The increasing cybersecurity threats to critical manufacturing infrastructure necessitate proactive strategies for vulnerability identification, classification, and assessment. Traditional approaches, which define vulnerabilities as weaknesses in computational logic or information systems, often overlook the physical and cyber-physical dimensions critical to manufacturing systems, comprising intertwined cyber, physical, and human elements. As a result, existing solutions fall short in addressing the complex, domain-specific vulnerabilities of manufacturing environments. To bridge this gap, this work redefines vulnerabilities in the manufacturing context by introducing a novel characterization based on the duality between vulnerabilities and defenses. Vulnerabilities are conceptualized as exploitable gaps within various defense layers, enabling a structured investigation of manufacturing systems. This paper presents a manufacturing-specific cyber-physical defense-in-depth model, highlighting how security-aware personnel, post-production inspection systems, and process monitoring approaches can complement traditional cyber defenses to enhance system resilience. Leveraging this model, we systematically identify and classify vulnerabilities across the manufacturing cyberspace, human element, post-production inspection systems, production process monitoring, and organizational policies and procedures. This comprehensive classification introduces the first taxonomy of cyber-physical vulnerabilities in smart manufacturing systems, providing practitioners with a structured framework for addressing vulnerabilities at both the system and process levels. Finally, the effectiveness of the proposed model and framework is demonstrated through an illustrative smart manufacturing system and its corresponding threat model.</li>
</ul>

<h3>Title: Cyber Shadows: Neutralizing Security Threats with AI and Targeted Policy Measures</h3>
<ul>
<li><strong>Authors: </strong>Marc Schmitt, Pantelis Koutroumpis</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09025">https://arxiv.org/abs/2501.09025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09025">https://arxiv.org/pdf/2501.09025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09025]] Cyber Shadows: Neutralizing Security Threats with AI and Targeted Policy Measures(https://arxiv.org/abs/2501.09025)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>The digital age, driven by the AI revolution, brings significant opportunities but also conceals security threats, which we refer to as cyber shadows. These threats pose risks at individual, organizational, and societal levels. This paper examines the systemic impact of these cyber threats and proposes a comprehensive cybersecurity strategy that integrates AI-driven solutions, such as Intrusion Detection Systems (IDS), with targeted policy interventions. By combining technological and regulatory measures, we create a multilevel defense capable of addressing both direct threats and indirect negative externalities. We emphasize that the synergy between AI-driven solutions and policy interventions is essential for neutralizing cyber threats and mitigating their negative impact on the digital economy. Finally, we underscore the need for continuous adaptation of these strategies, especially in response to the rapid advancement of autonomous AI-driven attacks, to ensure the creation of secure and resilient digital ecosystems.</li>
</ul>

<h3>Title: Enhancing Data Integrity through Provenance Tracking in Semantic Web Frameworks</h3>
<ul>
<li><strong>Authors: </strong>Nilesh Jain</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09029">https://arxiv.org/abs/2501.09029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09029">https://arxiv.org/pdf/2501.09029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09029]] Enhancing Data Integrity through Provenance Tracking in Semantic Web Frameworks(https://arxiv.org/abs/2501.09029)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper explores the integration of provenance tracking systems within the context of Semantic Web technologies to enhance data integrity in diverse operational environments. SURROUND Australia Pty Ltd demonstrates innovative applica-tions of the PROV Data Model (PROV-DM) and its Semantic Web variant, PROV-O, to systematically record and manage provenance information across multiple data processing domains. By employing RDF and Knowledge Graphs, SURROUND ad-dresses the critical challenges of shared entity identification and provenance granularity. The paper highlights the company's architecture for capturing comprehensive provenance data, en-abling robust validation, traceability, and knowledge inference. Through the examination of two projects, we illustrate how provenance mechanisms not only improve data reliability but also facilitate seamless integration across heterogeneous systems. Our findings underscore the importance of sophisticated provenance solutions in maintaining data integrity, serving as a reference for industry peers and academics engaged in provenance research and implementation.</li>
</ul>

<h3>Title: Synthetic Data and Health Privacy</h3>
<ul>
<li><strong>Authors: </strong>Gwénolé Abgrall, Xavier Monnet, Anmol Arora</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09031">https://arxiv.org/abs/2501.09031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09031">https://arxiv.org/pdf/2501.09031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09031]] Synthetic Data and Health Privacy(https://arxiv.org/abs/2501.09031)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>This Viewpoint discusses generative artificial intelligence and safeguarding privacy by using synthetic data as a substitute for private health data.</li>
</ul>

<h3>Title: Distributed Identity for Zero Trust and Segmented Access Control: A Novel Approach to Securing Network Infrastructure</h3>
<ul>
<li><strong>Authors: </strong>Sina Ahmadi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09032">https://arxiv.org/abs/2501.09032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09032">https://arxiv.org/pdf/2501.09032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09032]] Distributed Identity for Zero Trust and Segmented Access Control: A Novel Approach to Securing Network Infrastructure(https://arxiv.org/abs/2501.09032)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>"Distributed Identity" refers to the transition from centralized identity systems using Decentralized Identifiers (DID) and Verifiable Credentials (VC) for secure and privacy-preserving authentications. With distributed identity, control of identity data is returned to the user, making credential-based attacks impossible due to the lack of a single point of failure. This study assesses the security improvements achieved when distributed identity is employed with the ZTA principle, particularly concerning lateral movements within segmented networks. It also considers areas such as the implementation specifications of the framework, the advantages and disadvantages of the method to organizations, and the issues of compatibility and generalizability. Furthermore, the study highlights privacy and regulatory compliance, including the General Data Protection Regulation (GDPR) and California Consumer Data Privacy Act (CCPA), analyzing potential solutions to these problems. The study implies that adopting distributed identities can enhance overall security postures by an order of magnitude, providing contextual and least-privilege authorization and user privacy. The research recommends refining technical standards, expanding the use of distributed identity in practice, and discussing its applications for the contemporary digital security landscape.</li>
</ul>

<h3>Title: Adaptive Cybersecurity: Dynamically Retrainable Firewalls for Real-Time Network Protection</h3>
<ul>
<li><strong>Authors: </strong>Sina Ahmadi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09033">https://arxiv.org/abs/2501.09033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09033">https://arxiv.org/pdf/2501.09033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09033]] Adaptive Cybersecurity: Dynamically Retrainable Firewalls for Real-Time Network Protection(https://arxiv.org/abs/2501.09033)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, robust</a></li>
<li><strong>Abstract: </strong>The growing complexity of cyber attacks has necessitated the evolution of firewall technologies from static models to adaptive, machine learning-driven systems. This research introduces "Dynamically Retrainable Firewalls", which respond to emerging threats in real-time. Unlike traditional firewalls that rely on static rules to inspect traffic, these advanced systems leverage machine learning algorithms to analyze network traffic pattern dynamically and identify threats. The study explores architectures such as micro-services and distributed systems for real-time adaptability, data sources for model retraining, and dynamic threat identification through reinforcement and continual learning. It also discusses strategies to improve performance, reduce latency, optimize resource utilization, and address integration issues with present- day concepts such as Zero Trust and mixed environments. By critically assessing the literature, analyzing case studies, and elucidating areas of future research, this work suggests dynamically retrainable firewalls as a more robust form of network security. Additionally, it considers emerging trends such as advancements in AI and quantum computing, ethical issues, and other regulatory questions surrounding future AI systems. These findings provide valuable information on the future state of adaptive cyber security, focusing on the need for proactive and adaptive measures that counter cyber threats that continue to evolve.</li>
</ul>

<h3>Title: Do generative video models learn physical principles from watching videos?</h3>
<ul>
<li><strong>Authors: </strong>Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, Robert Geirhos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09038">https://arxiv.org/abs/2501.09038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09038">https://arxiv.org/pdf/2501.09038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09038]] Do generative video models learn physical principles from watching videos?(https://arxiv.org/abs/2501.09038)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>AI video generation is undergoing a revolution, with quality and realism advancing rapidly. These advances have led to a passionate scientific debate: Do video models learn ``world models'' that discover laws of physics -- or, alternatively, are they merely sophisticated pixel predictors that achieve visual realism without understanding the physical principles of reality? We address this question by developing Physics-IQ, a comprehensive benchmark dataset that can only be solved by acquiring a deep understanding of various physical principles, like fluid dynamics, optics, solid mechanics, magnetism and thermodynamics. We find that across a range of current models (Sora, Runway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical understanding is severely limited, and unrelated to visual realism. At the same time, some test cases can already be successfully solved. This indicates that acquiring certain physical principles from observation alone may be possible, but significant challenges remain. While we expect rapid advances ahead, our work demonstrates that visual realism does not imply physical understanding. Our project page is at this https URL; code at this https URL.</li>
</ul>

<h3>Title: Playing Devil's Advocate: Unmasking Toxicity and Vulnerabilities in Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Abdulkadir Erol, Trilok Padhi, Agnik Saha, Ugur Kursuncu, Mehmet Emin Aktas</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09039">https://arxiv.org/abs/2501.09039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09039">https://arxiv.org/pdf/2501.09039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09039]] Playing Devil's Advocate: Unmasking Toxicity and Vulnerabilities in Large Vision-Language Models(https://arxiv.org/abs/2501.09039)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Vision-Language Models (LVLMs) has enhanced capabilities offering potential applications from content creation to productivity enhancement. Despite their innovative potential, LVLMs exhibit vulnerabilities, especially in generating potentially toxic or unsafe responses. Malicious actors can exploit these vulnerabilities to propagate toxic content in an automated (or semi-) manner, leveraging the susceptibility of LVLMs to deception via strategically crafted prompts without fine-tuning or compute-intensive procedures. Despite the red-teaming efforts and inherent potential risks associated with the LVLMs, exploring vulnerabilities of LVLMs remains nascent and yet to be fully addressed in a systematic manner. This study systematically examines the vulnerabilities of open-source LVLMs, including LLaVA, InstructBLIP, Fuyu, and Qwen, using adversarial prompt strategies that simulate real-world social manipulation tactics informed by social theories. Our findings show that (i) toxicity and insulting are the most prevalent behaviors, with the mean rates of 16.13% and 9.75%, respectively; (ii) Qwen-VL-Chat, LLaVA-v1.6-Vicuna-7b, and InstructBLIP-Vicuna-7b are the most vulnerable models, exhibiting toxic response rates of 21.50%, 18.30% and 17.90%, and insulting responses of 13.40%, 11.70% and 10.10%, respectively; (iii) prompting strategies incorporating dark humor and multimodal toxic prompt completion significantly elevated these vulnerabilities. Despite being fine-tuned for safety, these models still generate content with varying degrees of toxicity when prompted with adversarial inputs, highlighting the urgent need for enhanced safety mechanisms and robust guardrails in LVLM development.</li>
</ul>

<h3>Title: Pseudolabel guided pixels contrast for domain adaptive semantic segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jianzi Xiang, Cailu Wan, Zhu Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09040">https://arxiv.org/abs/2501.09040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09040">https://arxiv.org/pdf/2501.09040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09040]] Pseudolabel guided pixels contrast for domain adaptive semantic segmentation(https://arxiv.org/abs/2501.09040)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation is essential for comprehending images, but the process necessitates a substantial amount of detailed annotations at the pixel level. Acquiring such annotations can be costly in the real-world. Unsupervised domain adaptation (UDA) for semantic segmentation is a technique that uses virtual data with labels to train a model and adapts it to real data without labels. Some recent works use contrastive learning, which is a powerful method for self-supervised learning, to help with this technique. However, these works do not take into account the diversity of features within each class when using contrastive learning, which leads to errors in class prediction. We analyze the limitations of these works and propose a novel framework called Pseudo-label Guided Pixel Contrast (PGPC), which overcomes the disadvantages of previous methods. We also investigate how to use more information from target images without adding noise from pseudo-labels. We test our method on two standard UDA benchmarks and show that it outperforms existing methods. Specifically, we achieve relative improvements of 5.1% mIoU and 4.6% mIoU on the Grand Theft Auto V (GTA5) to Cityscapes and SYNTHIA to Cityscapes tasks based on DAFormer, respectively. Furthermore, our approach can enhance the performance of other UDA approaches without increasing model complexity. Code is available at this https URL</li>
</ul>

<h3>Title: Generative Visual Commonsense Answering and Explaining with Generative Scene Graph Constructing</h3>
<ul>
<li><strong>Authors: </strong>Fan Yuan, Xiaoyuan Fang, Rong Quan, Jing Li, Wei Bi, Xiaogang Xu, Piji Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09041">https://arxiv.org/abs/2501.09041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09041">https://arxiv.org/pdf/2501.09041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09041]] Generative Visual Commonsense Answering and Explaining with Generative Scene Graph Constructing(https://arxiv.org/abs/2501.09041)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visual Commonsense Reasoning, which is regarded as one challenging task to pursue advanced visual scene comprehension, has been used to diagnose the reasoning ability of AI systems. However, reliable reasoning requires a good grasp of the scene's details. Existing work fails to effectively exploit the real-world object relationship information present within the scene, and instead overly relies on knowledge from training memory. Based on these observations, we propose a novel scene-graph-enhanced visual commonsense reasoning generation method named \textit{\textbf{G2}}, which first utilizes the image patches and LLMs to construct a location-free scene graph, and then answer and explain based on the scene graph's information. We also propose automatic scene graph filtering and selection strategies to absorb valuable scene graph information during training. Extensive experiments are conducted on the tasks and datasets of scene graph constructing and visual commonsense answering and explaining, respectively. Experimental results and ablation analysis demonstrate the effectiveness of our proposed framework.</li>
</ul>

<h3>Title: CookingDiffusion: Cooking Procedural Image Generation with Stable Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yuan Wang, Bin Xhu, Yanbin Hao, Chong-Wah Ngo, Yi Tan, Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09042">https://arxiv.org/abs/2501.09042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09042">https://arxiv.org/pdf/2501.09042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09042]] CookingDiffusion: Cooking Procedural Image Generation with Stable Diffusion(https://arxiv.org/abs/2501.09042)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image generation models have excelled in creating diverse and realistic images. This success extends to food imagery, where various conditional inputs like cooking styles, ingredients, and recipes are utilized. However, a yet-unexplored challenge is generating a sequence of procedural images based on cooking steps from a recipe. This could enhance the cooking experience with visual guidance and possibly lead to an intelligent cooking simulation system. To fill this gap, we introduce a novel task called \textbf{cooking procedural image generation}. This task is inherently demanding, as it strives to create photo-realistic images that align with cooking steps while preserving sequential consistency. To collectively tackle these challenges, we present \textbf{CookingDiffusion}, a novel approach that leverages Stable Diffusion and three innovative Memory Nets to model procedural prompts. These prompts encompass text prompts (representing cooking steps), image prompts (corresponding to cooking images), and multi-modal prompts (mixing cooking steps and images), ensuring the consistent generation of cooking procedural images. To validate the effectiveness of our approach, we preprocess the YouCookII dataset, establishing a new benchmark. Our experimental results demonstrate that our model excels at generating high-quality cooking procedural images with remarkable consistency across sequential cooking steps, as measured by both the FID and the proposed Average Procedure Consistency metrics. Furthermore, CookingDiffusion demonstrates the ability to manipulate ingredients and cooking methods in a recipe. We will make our code, models, and dataset publicly accessible.</li>
</ul>

<h3>Title: TCMM: Token Constraint and Multi-Scale Memory Bank of Contrastive Learning for Unsupervised Person Re-identification</h3>
<ul>
<li><strong>Authors: </strong>Zheng-An Zhu, Hsin-Che Chien, Chen-Kuo Chiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09044">https://arxiv.org/abs/2501.09044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09044">https://arxiv.org/pdf/2501.09044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09044]] TCMM: Token Constraint and Multi-Scale Memory Bank of Contrastive Learning for Unsupervised Person Re-identification(https://arxiv.org/abs/2501.09044)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper proposes the ViT Token Constraint and Multi-scale Memory bank (TCMM) method to address the patch noises and feature inconsistency in unsupervised person re-identification works. Many excellent methods use ViT features to obtain pseudo labels and clustering prototypes, then train the model with contrastive learning. However, ViT processes images by performing patch embedding, which inevitably introduces noise in patches and may compromise the performance of the re-identification model. On the other hand, previous memory bank based contrastive methods may lead data inconsistency due to the limitation of batch size. Furthermore, existing pseudo label methods often discard outlier samples that are difficult to cluster. It sacrifices the potential value of outlier samples, leading to limited model diversity and robustness. This paper introduces the ViT Token Constraint to mitigate the damage caused by patch noises to the ViT architecture. The proposed Multi-scale Memory enhances the exploration of outlier samples and maintains feature consistency. Experimental results demonstrate that our system achieves state-of-the-art performance on common benchmarks. The project is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Anthropomorphic Features for On-Line Signatures</h3>
<ul>
<li><strong>Authors: </strong>Moises Diaz, Miguel A. Ferrer, Jose J. Quintana</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09048">https://arxiv.org/abs/2501.09048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09048">https://arxiv.org/pdf/2501.09048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09048]] Anthropomorphic Features for On-Line Signatures(https://arxiv.org/abs/2501.09048)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Many features have been proposed in on-line signature verification. Generally, these features rely on the position of the on-line signature samples and their dynamic properties, as recorded by a tablet. This paper proposes a novel feature space to describe efficiently on-line signatures. Since producing a signature requires a skeletal arm system and its associated muscles, the new feature space is based on characterizing the movement of the shoulder, the elbow and the wrist joints when signing. As this motion is not directly obtained from a digital tablet, the new features are calculated by means of a virtual skeletal arm (VSA) model, which simulates the architecture of a real arm and forearm. Specifically, the VSA motion is described by its 3D joint position and its joint angles. These anthropomorphic features are worked out from both pen position and orientation through the VSA forward and direct kinematic model. The anthropomorphic features' robustness is proved by achieving state-of-the-art performance with several verifiers and multiple benchmarks on third party signature databases, which were collected with different devices and in different languages and scripts.</li>
</ul>

<h3>Title: Generating Realistic Synthetic Head Rotation Data for Extended Reality using Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Jakob Struye, Filip Lemic, Jeroen Famaey</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09050">https://arxiv.org/abs/2501.09050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09050">https://arxiv.org/pdf/2501.09050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09050]] Generating Realistic Synthetic Head Rotation Data for Extended Reality using Deep Learning(https://arxiv.org/abs/2501.09050)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Extended Reality is a revolutionary method of delivering multimedia content to users. A large contributor to its popularity is the sense of immersion and interactivity enabled by having real-world motion reflected in the virtual experience accurately and immediately. This user motion, mainly caused by head rotations, induces several technical challenges. For instance, which content is generated and transmitted depends heavily on where the user is looking. Seamless systems, taking user motion into account proactively, will therefore require accurate predictions of upcoming rotations. Training and evaluating such predictors requires vast amounts of orientational input data, which is expensive to gather, as it requires human test subjects. A more feasible approach is to gather a modest dataset through test subjects, and then extend it to a more sizeable set using synthetic data generation methods. In this work, we present a head rotation time series generator based on TimeGAN, an extension of the well-known Generative Adversarial Network, designed specifically for generating time series. This approach is able to extend a dataset of head rotations with new samples closely matching the distribution of the measured time series.</li>
</ul>

<h3>Title: SHYI: Action Support for Contrastive Learning in High-Fidelity Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Tianxiang Xia, Lin Xiao, Yannick Montorfani, Francesco Pavia, Enis Simsar, Thomas Hofmann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09055">https://arxiv.org/abs/2501.09055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09055">https://arxiv.org/pdf/2501.09055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09055]] SHYI: Action Support for Contrastive Learning in High-Fidelity Text-to-Image Generation(https://arxiv.org/abs/2501.09055)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this project, we address the issue of infidelity in text-to-image generation, particularly for actions involving multiple objects. For this we build on top of the CONFORM framework which uses Contrastive Learning to improve the accuracy of the generated image for multiple objects. However the depiction of actions which involves multiple different object has still large room for improvement. To improve, we employ semantically hypergraphic contrastive adjacency learning, a comprehension of enhanced contrastive structure and "contrast but link" technique. We further amend Stable Diffusion's understanding of actions by InteractDiffusion. As evaluation metrics we use image-text similarity CLIP and TIFA. In addition, we conducted a user study. Our method shows promising results even with verbs that Stable Diffusion understands mediocrely. We then provide future directions by analyzing the results. Our codebase can be found on polybox under the link: this https URL</li>
</ul>

<h3>Title: Decompose-ToM: Enhancing Theory of Mind Reasoning in Large Language Models through Simulation and Task Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Sneheel Sarangi, Maha Elgarf, Hanan Salam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09056">https://arxiv.org/abs/2501.09056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09056">https://arxiv.org/pdf/2501.09056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09056]] Decompose-ToM: Enhancing Theory of Mind Reasoning in Large Language Models through Simulation and Task Decomposition(https://arxiv.org/abs/2501.09056)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Theory of Mind (ToM) is the ability to understand and reflect on the mental states of others. Although this capability is crucial for human interaction, testing on Large Language Models (LLMs) reveals that they possess only a rudimentary understanding of it. Although the most capable closed-source LLMs have come close to human performance on some ToM tasks, they still perform poorly on complex variations of the task that involve more structured reasoning. In this work, we utilize the concept of "pretend-play", or ``Simulation Theory'' from cognitive psychology to propose ``Decompose-ToM'': an LLM-based inference algorithm that improves model performance on complex ToM tasks. We recursively simulate user perspectives and decompose the ToM task into a simpler set of functions: subject identification, question-reframing, world model updation, and knowledge availability. We test the algorithm on higher-order ToM tasks and a task testing for ToM capabilities in a conversational setting, demonstrating that our approach shows significant improvement across models compared to baseline methods while requiring minimal prompt tuning across tasks and no additional model training.</li>
</ul>

<h3>Title: Average-Reward Reinforcement Learning with Entropy Regularization</h3>
<ul>
<li><strong>Authors: </strong>Jacob Adamczyk, Volodymyr Makarenko, Stas Tiomkin, Rahul V. Kulkarni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09080">https://arxiv.org/abs/2501.09080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09080">https://arxiv.org/pdf/2501.09080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09080]] Average-Reward Reinforcement Learning with Entropy Regularization(https://arxiv.org/abs/2501.09080)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The average-reward formulation of reinforcement learning (RL) has drawn increased interest in recent years due to its ability to solve temporally-extended problems without discounting. Independently, RL algorithms have benefited from entropy-regularization: an approach used to make the optimal policy stochastic, thereby more robust to noise. Despite the distinct benefits of the two approaches, the combination of entropy regularization with an average-reward objective is not well-studied in the literature and there has been limited development of algorithms for this setting. To address this gap in the field, we develop algorithms for solving entropy-regularized average-reward RL problems with function approximation. We experimentally validate our method, comparing it with existing algorithms on standard benchmarks for RL.</li>
</ul>

<h3>Title: Salient Information Preserving Adversarial Training Improves Clean and Robust Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Timothy Redgrave, Adam Czajka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09086">https://arxiv.org/abs/2501.09086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09086">https://arxiv.org/pdf/2501.09086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09086]] Salient Information Preserving Adversarial Training Improves Clean and Robust Accuracy(https://arxiv.org/abs/2501.09086)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>In this work we introduce Salient Information Preserving Adversarial Training (SIP-AT), an intuitive method for relieving the robustness-accuracy trade-off incurred by traditional adversarial training. SIP-AT uses salient image regions to guide the adversarial training process in such a way that fragile features deemed meaningful by an annotator remain unperturbed during training, allowing models to learn highly predictive non-robust features without sacrificing overall robustness. This technique is compatible with both human-based and automatically generated salience estimates, allowing SIP-AT to be used as a part of human-driven model development without forcing SIP-AT to be reliant upon additional human data. We perform experiments across multiple datasets and architectures and demonstrate that SIP-AT is able to boost the clean accuracy of models while maintaining a high degree of robustness against attacks at multiple epsilon levels. We complement our central experiments with an observational study measuring the rate at which human subjects successfully identify perturbed images. This study helps build a more intuitive understanding of adversarial attack strength and demonstrates the heightened importance of low-epsilon robustness. Our results demonstrate the efficacy of SIP-AT and provide valuable insight into the risks posed by adversarial samples of various strengths.</li>
</ul>

<h3>Title: SteLLA: A Structured Grading System Using LLMs with RAG</h3>
<ul>
<li><strong>Authors: </strong>Hefei Qiu, Brian White, Ashley Ding, Reinaldo Costa, Ali Hachem, Wei Ding, Ping Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09092">https://arxiv.org/abs/2501.09092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09092">https://arxiv.org/pdf/2501.09092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09092]] SteLLA: A Structured Grading System Using LLMs with RAG(https://arxiv.org/abs/2501.09092)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown strong general capabilities in many applications. However, how to make them reliable tools for some specific tasks such as automated short answer grading (ASAG) remains a challenge. We present SteLLA (Structured Grading System Using LLMs with RAG) in which a) Retrieval Augmented Generation (RAG) approach is used to empower LLMs specifically on the ASAG task by extracting structured information from the highly relevant and reliable external knowledge based on the instructor-provided reference answer and rubric, b) an LLM performs a structured and question-answering-based evaluation of student answers to provide analytical grades and feedback. A real-world dataset that contains students' answers in an exam was collected from a college-level Biology course. Experiments show that our proposed system can achieve substantial agreement with the human grader while providing break-down grades and feedback on all the knowledge points examined in the problem. A qualitative and error analysis of the feedback generated by GPT4 shows that GPT4 is good at capturing facts while may be prone to inferring too much implication from the given text in the grading task which provides insights into the usage of LLMs in the ASAG system.</li>
</ul>

<h3>Title: Rethinking Post-Training Quantization: Introducing a Statistical Pre-Calibration Approach</h3>
<ul>
<li><strong>Authors: </strong>Alireza Ghaffari, Sharareh Younesian, Boxing Chen, Vahid Partovi Nia, Masoud Asgharian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09107">https://arxiv.org/abs/2501.09107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09107">https://arxiv.org/pdf/2501.09107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09107]] Rethinking Post-Training Quantization: Introducing a Statistical Pre-Calibration Approach(https://arxiv.org/abs/2501.09107)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) become increasingly computationally complex, developing efficient deployment strategies, such as quantization, becomes crucial. State-of-the-art Post-training Quantization (PTQ) techniques often rely on calibration processes to maintain the accuracy of these models. However, while these calibration techniques can enhance performance in certain domains, they may not be as effective in others. This paper aims to draw attention to robust statistical approaches that can mitigate such issues. We propose a weight-adaptive PTQ method that can be considered a precursor to calibration-based PTQ methods, guiding the quantization process to preserve the distribution of weights by minimizing the Kullback-Leibler divergence between the quantized weights and the originally trained weights. This minimization ensures that the quantized model retains the Shannon information content of the original model to a great extent, guaranteeing robust and efficient deployment across many tasks. As such, our proposed approach can perform on par with most common calibration-based PTQ methods, establishing a new pre-calibration step for further adjusting the quantized weights with calibration. We show that our pre-calibration results achieve the same accuracy as some existing calibration-based PTQ methods on various LLMs.</li>
</ul>

<h3>Title: Generative Medical Image Anonymization Based on Latent Code Projection and Optimization</h3>
<ul>
<li><strong>Authors: </strong>Huiyu Li, Nicholas Ayache, Hervé Delingette</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09114">https://arxiv.org/abs/2501.09114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09114">https://arxiv.org/pdf/2501.09114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09114]] Generative Medical Image Anonymization Based on Latent Code Projection and Optimization(https://arxiv.org/abs/2501.09114)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, generative</a></li>
<li><strong>Abstract: </strong>Medical image anonymization aims to protect patient privacy by removing identifying information, while preserving the data utility to solve downstream tasks. In this paper, we address the medical image anonymization problem with a two-stage solution: latent code projection and optimization. In the projection stage, we design a streamlined encoder to project input images into a latent space and propose a co-training scheme to enhance the projection process. In the optimization stage, we refine the latent code using two deep loss functions designed to address the trade-off between identity protection and data utility dedicated to medical images. Through a comprehensive set of qualitative and quantitative experiments, we showcase the effectiveness of our approach on the MIMIC-CXR chest X-ray dataset by generating anonymized synthetic images that can serve as training set for detecting lung pathologies. Source codes are available at this https URL.</li>
</ul>

<h3>Title: Augmenting Human-Annotated Training Data with Large Language Model Generation and Distillation in Open-Response Assessment</h3>
<ul>
<li><strong>Authors: </strong>Conrad Borchers, Danielle R. Thomas, Jionghao Lin, Ralph Abboud, Kenneth R. Koedinger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09126">https://arxiv.org/abs/2501.09126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09126">https://arxiv.org/pdf/2501.09126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09126]] Augmenting Human-Annotated Training Data with Large Language Model Generation and Distillation in Open-Response Assessment(https://arxiv.org/abs/2501.09126)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) like GPT-4o can help automate text classification tasks at low cost and scale. However, there are major concerns about the validity and reliability of LLM outputs. By contrast, human coding is generally more reliable but expensive to procure at scale. In this study, we propose a hybrid solution to leverage the strengths of both. We combine human-coded data and synthetic LLM-produced data to fine-tune a classical machine learning classifier, distilling both into a smaller BERT model. We evaluate our method on a human-coded test set as a validity measure for LLM output quality. In three experiments, we systematically vary LLM-generated samples' size, variety, and consistency, informed by best practices in LLM tuning. Our findings indicate that augmenting datasets with synthetic samples improves classifier performance, with optimal results achieved at an 80% synthetic to 20% human-coded data ratio. Lower temperature settings of 0.3, corresponding to less variability in LLM generations, produced more stable improvements but also limited model learning from augmented samples. In contrast, higher temperature settings (0.7 and above) introduced greater variability in performance estimates and, at times, lower performance. Hence, LLMs may produce more uniform output that classifiers overfit to earlier or produce more diverse output that runs the risk of deteriorating model performance through information irrelevant to the prediction task. Filtering out inconsistent synthetic samples did not enhance performance. We conclude that integrating human and LLM-generated data to improve text classification models in assessment offers a scalable solution that leverages both the accuracy of human coding and the variety of LLM outputs.</li>
</ul>

<h3>Title: Multilingual LLMs Struggle to Link Orthography and Semantics in Bilingual Word Processing</h3>
<ul>
<li><strong>Authors: </strong>Eshaan Tanwar, Gayatri Oke, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09127">https://arxiv.org/abs/2501.09127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09127">https://arxiv.org/pdf/2501.09127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09127]] Multilingual LLMs Struggle to Link Orthography and Semantics in Bilingual Word Processing(https://arxiv.org/abs/2501.09127)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Bilingual lexical processing is shaped by the complex interplay of phonological, orthographic, and semantic features of two languages within an integrated mental lexicon. In humans, this is evident in the ease with which cognate words - words similar in both orthographic form and meaning (e.g., blind, meaning "sightless" in both English and German) - are processed, compared to the challenges posed by interlingual homographs, which share orthographic form but differ in meaning (e.g., gift, meaning "present" in English but "poison" in German). We investigate how multilingual Large Language Models (LLMs) handle such phenomena, focusing on English-Spanish, English-French, and English-German cognates, non-cognate, and interlingual homographs. Specifically, we evaluate their ability to disambiguate meanings and make semantic judgments, both when these word types are presented in isolation or within sentence contexts. Our findings reveal that while certain LLMs demonstrate strong performance in recognizing cognates and non-cognates in isolation, they exhibit significant difficulty in disambiguating interlingual homographs, often performing below random baselines. This suggests LLMs tend to rely heavily on orthographic similarities rather than semantic understanding when interpreting interlingual homographs. Further, we find LLMs exhibit difficulty in retrieving word meanings, with performance in isolative disambiguation tasks having no correlation with semantic understanding. Finally, we study how the LLM processes interlingual homographs in incongruent sentences. We find models to opt for different strategies in understanding English and non-English homographs, highlighting a lack of a unified approach to handling cross-lingual ambiguities.</li>
</ul>

<h3>Title: Deep Self-Supervised Disturbance Mapping with the OPERA Sentinel-1 Radiometric Terrain Corrected SAR Backscatter Product</h3>
<ul>
<li><strong>Authors: </strong>Harris Hardiman-Mostow, Charles Marshak, Alexander L. Handwerger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09129">https://arxiv.org/abs/2501.09129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09129">https://arxiv.org/pdf/2501.09129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09129]] Deep Self-Supervised Disturbance Mapping with the OPERA Sentinel-1 Radiometric Terrain Corrected SAR Backscatter Product(https://arxiv.org/abs/2501.09129)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Mapping land surface disturbances supports disaster response, resource and ecosystem management, and climate adaptation efforts. Synthetic aperture radar (SAR) is an invaluable tool for disturbance mapping, providing consistent time-series images of the ground regardless of weather or illumination conditions. Despite SAR's potential for disturbance mapping, processing SAR data to an analysis-ready format requires expertise and significant compute resources, particularly for large-scale global analysis. In October 2023, NASA's Observational Products for End-Users from Remote Sensing Analysis (OPERA) project released the near-global Radiometric Terrain Corrected SAR backscatter from Sentinel-1 (RTC-S1) dataset, providing publicly available, analysis-ready SAR imagery. In this work, we utilize this new dataset to systematically analyze land surface disturbances. As labeling SAR data is often prohibitively time-consuming, we train a self-supervised vision transformer - which requires no labels to train - on OPERA RTC-S1 data to estimate a per-pixel distribution from the set of baseline imagery and assess disturbances when there is significant deviation from the modeled distribution. To test our model's capability and generality, we evaluate three different natural disasters - which represent high-intensity, abrupt disturbances - from three different regions of the world. Across events, our approach yields high quality delineations: F1 scores exceeding 0.6 and Areas Under the Precision-Recall Curve exceeding 0.65, consistently outperforming existing SAR disturbance methods. Our findings suggest that a self-supervised vision transformer is well-suited for global disturbance mapping and can be a valuable tool for operational, near-global disturbance monitoring, particularly when labeled data does not exist.</li>
</ul>

<h3>Title: Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Demetrio Deanda, Yuktha Priya Masupalli, Jeong Yang, Young Lee, Zechun Cao, Gongbo Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09134">https://arxiv.org/abs/2501.09134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09134">https://arxiv.org/pdf/2501.09134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09134]] Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report Retrieval(https://arxiv.org/abs/2501.09134)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Medical images and reports offer invaluable insights into patient health. The heterogeneity and complexity of these data hinder effective analysis. To bridge this gap, we investigate contrastive learning models for cross-domain retrieval, which associates medical images with their corresponding clinical reports. This study benchmarks the robustness of four state-of-the-art contrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We introduce an occlusion retrieval task to evaluate model performance under varying levels of image corruption. Our findings reveal that all evaluated models are highly sensitive to out-of-distribution data, as evidenced by the proportional decrease in performance with increasing occlusion levels. While MedCLIP exhibits slightly more robustness, its overall performance remains significantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a general-purpose dataset, struggles with medical image-report retrieval, highlighting the importance of domain-specific training data. The evaluation of this work suggests that more effort needs to be spent on improving the robustness of these models. By addressing these limitations, we can develop more reliable cross-domain retrieval models for medical applications.</li>
</ul>

<h3>Title: Few-Shot Adaptation of Training-Free Foundation Model for 3D Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xingxin He, Yifan Hu, Zhaoye Zhou, Mohamed Jarraya, Fang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09138">https://arxiv.org/abs/2501.09138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09138">https://arxiv.org/pdf/2501.09138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09138]] Few-Shot Adaptation of Training-Free Foundation Model for 3D Medical Image Segmentation(https://arxiv.org/abs/2501.09138)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Vision foundation models have achieved remarkable progress across various image analysis tasks. In the image segmentation task, foundation models like the Segment Anything Model (SAM) enable generalizable zero-shot segmentation through user-provided prompts. However, SAM primarily trained on natural images, lacks the domain-specific expertise of medical imaging. This limitation poses challenges when applying SAM to medical image segmentation, including the need for extensive fine-tuning on specialized medical datasets and a dependency on manual prompts, which are both labor-intensive and require intervention from medical experts. This work introduces the Few-shot Adaptation of Training-frEe SAM (FATE-SAM), a novel method designed to adapt the advanced Segment Anything Model 2 (SAM2) for 3D medical image segmentation. FATE-SAM reassembles pre-trained modules of SAM2 to enable few-shot adaptation, leveraging a small number of support examples to capture anatomical knowledge and perform prompt-free segmentation, without requiring model fine-tuning. To handle the volumetric nature of medical images, we incorporate a Volumetric Consistency mechanism that enhances spatial coherence across 3D slices. We evaluate FATE-SAM on multiple medical imaging datasets and compare it with supervised learning methods, zero-shot SAM approaches, and fine-tuned medical SAM methods. Results show that FATE-SAM delivers robust and accurate segmentation while eliminating the need for large annotated datasets and expert intervention. FATE-SAM provides a practical, efficient solution for medical image segmentation, making it more accessible for clinical applications.</li>
</ul>

<h3>Title: Towards Federated Multi-Armed Bandit Learning for Content Dissemination using Swarm of UAVs</h3>
<ul>
<li><strong>Authors: </strong>Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09146">https://arxiv.org/abs/2501.09146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09146">https://arxiv.org/pdf/2501.09146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09146]] Towards Federated Multi-Armed Bandit Learning for Content Dissemination using Swarm of UAVs(https://arxiv.org/abs/2501.09146)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>This paper introduces an Unmanned Aerial Vehicle - enabled content management architecture that is suitable for critical content access in communities of users that are communication-isolated during diverse types of disaster scenarios. The proposed architecture leverages a hybrid network of stationary anchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The anchor UAVs are equipped with both vertical and lateral communication links, and they serve local users, while the mobile micro-ferrying UAVs extend coverage across communities with increased mobility. The focus is on developing a content dissemination system that dynamically learns optimal caching policies to maximize content availability. The core innovation is an adaptive content dissemination framework based on distributed Federated Multi-Armed Bandit learning. The goal is to optimize UAV content caching decisions based on geo-temporal content popularity and user demand variations. A Selective Caching Algorithm is also introduced to reduce redundant content replication by incorporating inter-UAV information sharing. This method strategically preserves the uniqueness in user preferences while amalgamating the intelligence across a distributed learning system. This approach improves the learning algorithm's ability to adapt to diverse user preferences. Functional verification and performance evaluation confirm the proposed architecture's utility across different network sizes, UAV swarms, and content popularity patterns.</li>
</ul>

<h3>Title: Towards Multilingual LLM Evaluation for Baltic and Nordic languages: A study on Lithuanian History</h3>
<ul>
<li><strong>Authors: </strong>Yevhen Kostiuk, Oxana Vitman, Łukasz Gagała, Artur Kiulian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09154">https://arxiv.org/abs/2501.09154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09154">https://arxiv.org/pdf/2501.09154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09154]] Towards Multilingual LLM Evaluation for Baltic and Nordic languages: A study on Lithuanian History(https://arxiv.org/abs/2501.09154)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this work, we evaluated Lithuanian and general history knowledge of multilingual Large Language Models (LLMs) on a multiple-choice question-answering task. The models were tested on a dataset of Lithuanian national and general history questions translated into Baltic, Nordic, and other languages (English, Ukrainian, Arabic) to assess the knowledge sharing from culturally and historically connected groups. We evaluated GPT-4o, LLaMa3.1 8b and 70b, QWEN2.5 7b and 72b, Mistral Nemo 12b, LLaMa3 8b, Mistral 7b, LLaMa3.2 3b, and Nordic fine-tuned models (GPT-SW3 and LLaMa3 8b). Our results show that GPT-4o consistently outperformed all other models across language groups, with slightly better results for Baltic and Nordic languages. Larger open-source models like QWEN2.5 72b and LLaMa3.1 70b performed well but showed weaker alignment with Baltic languages. Smaller models (Mistral Nemo 12b, LLaMa3.2 3b, QWEN 7B, LLaMa3.1 8B, and LLaMa3 8b) demonstrated gaps with LT-related alignment with Baltic languages while performing better on Nordic and other languages. The Nordic fine-tuned models did not surpass multilingual models, indicating that shared cultural or historical context alone does not guarantee better performance.</li>
</ul>

<h3>Title: VCRScore: Image captioning metric based on V\&L Transformers, CLIP, and precision-recall</h3>
<ul>
<li><strong>Authors: </strong>Guillermo Ruiz, Tania Ramírez, Daniela Moctezuma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09155">https://arxiv.org/abs/2501.09155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09155">https://arxiv.org/pdf/2501.09155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09155]] VCRScore: Image captioning metric based on V\&L Transformers, CLIP, and precision-recall(https://arxiv.org/abs/2501.09155)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Image captioning has become an essential Vision & Language research task. It is about predicting the most accurate caption given a specific image or video. The research community has achieved impressive results by continuously proposing new models and approaches to improve the overall model's performance. Nevertheless, despite increasing proposals, the performance metrics used to measure their advances have remained practically untouched through the years. A probe of that, nowadays metrics like BLEU, METEOR, CIDEr, and ROUGE are still very used, aside from more sophisticated metrics such as BertScore and ClipScore. Hence, it is essential to adjust how are measure the advances, limitations, and scopes of the new image captioning proposals, as well as to adapt new metrics to these new advanced image captioning approaches. This work proposes a new evaluation metric for the image captioning problem. To do that, first, it was generated a human-labeled dataset to assess to which degree the captions correlate with the image's content. Taking these human scores as ground truth, we propose a new metric, and compare it with several well-known metrics, from classical to newer ones. Outperformed results were also found, and interesting insights were presented and discussed.</li>
</ul>

<h3>Title: Evaluating GenAI for Simplifying Texts for Education: Improving Accuracy and Consistency for Enhanced Readability</h3>
<ul>
<li><strong>Authors: </strong>Stephanie L. Day, Jacapo Cirica, Steven R. Clapp, Veronika Penkova, Amy E. Giroux, Abbey Banta, Catherine Bordeau, Poojitha Mutteneni, Ben D. Sawyer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09158">https://arxiv.org/abs/2501.09158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09158">https://arxiv.org/pdf/2501.09158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09158]] Evaluating GenAI for Simplifying Texts for Education: Improving Accuracy and Consistency for Enhanced Readability(https://arxiv.org/abs/2501.09158)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence (GenAI) holds great promise as a tool to support personalized learning. Teachers need tools to efficiently and effectively enhance content readability of educational texts so that they are matched to individual students reading levels, while retaining key details. Large Language Models (LLMs) show potential to fill this need, but previous research notes multiple shortcomings in current approaches. In this study, we introduced a generalized approach and metrics for the systematic evaluation of the accuracy and consistency in which LLMs, prompting techniques, and a novel multi-agent architecture to simplify sixty informational reading passages, reducing each from the twelfth grade level down to the eighth, sixth, and fourth grade levels. We calculated the degree to which each LLM and prompting technique accurately achieved the targeted grade level for each passage, percentage change in word count, and consistency in maintaining keywords and key phrases (semantic similarity). One-sample t-tests and multiple regression models revealed significant differences in the best performing LLM and prompt technique for each of the four metrics. Both LLMs and prompting techniques demonstrated variable utility in grade level accuracy and consistency of keywords and key phrases when attempting to level content down to the fourth grade reading level. These results demonstrate the promise of the application of LLMs for efficient and precise automated text simplification, the shortcomings of current models and prompting methods in attaining an ideal balance across various evaluation criteria, and a generalizable method to evaluate future systems.</li>
</ul>

<h3>Title: A Vessel Bifurcation Landmark Pair Dataset for Abdominal CT Deformable Image Registration (DIR) Validation</h3>
<ul>
<li><strong>Authors: </strong>Edward R Criscuolo, Yao Hao, Zhendong Zhang, Trevor McKeown, Deshan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09162">https://arxiv.org/abs/2501.09162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09162">https://arxiv.org/pdf/2501.09162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09162]] A Vessel Bifurcation Landmark Pair Dataset for Abdominal CT Deformable Image Registration (DIR) Validation(https://arxiv.org/abs/2501.09162)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deformable image registration (DIR) is an enabling technology in many diagnostic and therapeutic tasks. Despite this, DIR algorithms have limited clinical use, largely due to a lack of benchmark datasets for quality assurance during development. To support future algorithm development, here we introduce our first-of-its-kind abdominal CT DIR benchmark dataset, comprising large numbers of highly accurate landmark pairs on matching blood vessel bifurcations. Abdominal CT image pairs of 30 patients were acquired from several public repositories as well as the authors' institution with IRB approval. The two CTs of each pair were originally acquired for the same patient on different days. An image processing workflow was developed and applied to each image pair: 1) Abdominal organs were segmented with a deep learning model, and image intensity within organ masks was overwritten. 2) Matching image patches were manually identified between two CTs of each image pair 3) Vessel bifurcation landmarks were labeled on one image of each image patch pair. 4) Image patches were deformably registered, and landmarks were projected onto the second image. 5) Landmark pair locations were refined manually or with an automated process. This workflow resulted in 1895 total landmark pairs, or 63 per case on average. Estimates of the landmark pair accuracy using digital phantoms were 0.7+/-1.2mm. The data is published in Zenodo at this https URL. Instructions for use can be found at this https URL. This dataset is a first-of-its-kind for abdominal DIR validation. The number, accuracy, and distribution of landmark pairs will allow for robust validation of DIR algorithms with precision beyond what is currently available.</li>
</ul>

<h3>Title: The Veln(ia)s is in the Details: Evaluating LLM Judgment on Latvian and Lithuanian Short Answer Matching</h3>
<ul>
<li><strong>Authors: </strong>Yevhen Kostiuk, Oxana Vitman, Łukasz Gagała, Artur Kiulian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09164">https://arxiv.org/abs/2501.09164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09164">https://arxiv.org/pdf/2501.09164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09164]] The Veln(ia)s is in the Details: Evaluating LLM Judgment on Latvian and Lithuanian Short Answer Matching(https://arxiv.org/abs/2501.09164)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this work, we address the challenge of evaluating large language models (LLMs) on the short answer matching task for Latvian and Lithuanian languages. We introduce novel datasets consisting of 502 Latvian and 690 Lithuanian question-answer pairs. For each question-answer pair, we generated matched and non-matched answers using a set of alteration rules specifically designed to introduce small but meaningful changes in the text. These generated answers serve as test cases to assess the ability of LLMs to detect subtle differences in matching of the original answers. A subset of the datasets was manually verified for quality and accuracy. Our results show that while larger LLMs, such as QWEN2.5 72b and LLaMa3.1 70b, demonstrate near-perfect performance in distinguishing matched and non-matched answers, smaller models show more variance. For instance, LLaMa3.1 8b and EuroLLM 9b benefited from few-shot examples, while Mistral Nemo 12b underperformed on detection of subtle text alteration, particularly in Lithuanian, even with additional examples. QWEN2.5 7b and Mistral 7b were able to obtain a strong and comparable performance to the larger 70b models in zero and few shot experiments. Moreover, the performance of Mistral 7b was weaker in few shot experiments.</li>
</ul>

<h3>Title: Attention is All You Need Until You Need Retention</h3>
<ul>
<li><strong>Authors: </strong>M. Murat Yaslioglu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09166">https://arxiv.org/abs/2501.09166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09166">https://arxiv.org/pdf/2501.09166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09166]] Attention is All You Need Until You Need Retention(https://arxiv.org/abs/2501.09166)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>This work introduces a novel Retention Layer mechanism for Transformer based architectures, addressing their inherent lack of intrinsic retention capabilities. Unlike human cognition, which can encode and dynamically recall symbolic templates, Generative Pretrained Transformers rely solely on fixed pretrained weights and ephemeral context windows, limiting their adaptability. The proposed Retention Layer incorporates a persistent memory module capable of real time data population, dynamic recall, and guided output generation. This enhancement allows models to store, update, and reuse observed patterns across sessions, enabling incremental learning and bridging the gap between static pretraining and dynamic, context sensitive adaptation. The Retention Layer design parallels social learning processes, encompassing attention, retention, reproduction, and motivation stages. Technically, it integrates a memory attention mechanism and episodic buffers to manage memory scalability, mitigate overfitting, and ensure efficient recall. Applications span adaptive personal assistants, real time fraud detection, autonomous robotics, content moderation, and healthcare diagnostics. In each domain, the retention mechanism enables systems to learn incrementally, personalize outputs, and respond to evolving real world challenges effectively. By emulating key aspects of human learning, this retention enhanced architecture fosters a more fluid and responsive AI paradigm, paving the way for dynamic, session aware models that extend the capabilities of traditional Transformers into domains requiring continual adaptation.</li>
</ul>

<h3>Title: Grounding Text-To-Image Diffusion Models For Controlled High-Quality Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Süleyman, Göksel Biricik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09194">https://arxiv.org/abs/2501.09194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09194">https://arxiv.org/pdf/2501.09194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09194]] Grounding Text-To-Image Diffusion Models For Controlled High-Quality Image Generation(https://arxiv.org/abs/2501.09194)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Large-scale text-to-image (T2I) diffusion models have demonstrated an outstanding performance in synthesizing diverse high-quality visuals from natural language text captions. Multiple layout-to-image models have been developed to control the generation process by utilizing a broad array of layouts such as segmentation maps, edges, and human keypoints. In this work, we present ObjectDiffusion, a model that takes inspirations from the top cutting-edge image generative frameworks to seamlessly condition T2I models with new bounding boxes capabilities. Specifically, we make substantial modifications to the network architecture introduced in ContorlNet to integrate it with the condition processing and injection techniques proposed in GLIGEN. ObjectDiffusion is initialized with pretraining parameters to leverage the generation knowledge obtained from training on large-scale datasets. We fine-tune ObjectDiffusion on the COCO2017 training dataset and evaluate it on the COCO2017 validation dataset. Our model achieves an AP$_{50}$ of 46.6, an AR of 44.5, and a FID of 19.8 outperforming the current SOTA model trained on open-source datasets in all of the three metrics. ObjectDiffusion demonstrates a distinctive capability in synthesizing diverse, high-quality, high-fidelity images that seamlessly conform to the semantic and spatial control layout. Evaluated in qualitative and quantitative tests, ObjectDiffusion exhibits remarkable grounding abilities on closed-set and open-set settings across a wide variety of contexts. The qualitative assessment verifies the ability of ObjectDiffusion to generate multiple objects of different sizes and locations.</li>
</ul>

<h3>Title: Unified Few-shot Crack Segmentation and its Precise 3D Automatic Measurement in Concrete Structures</h3>
<ul>
<li><strong>Authors: </strong>Pengru Deng, Jiapeng Yao, Chun Li, Su Wang, Xinrun Li, Varun Ojha, Xuhui He, Takashi Matsumoto</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09203">https://arxiv.org/abs/2501.09203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09203">https://arxiv.org/pdf/2501.09203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09203]] Unified Few-shot Crack Segmentation and its Precise 3D Automatic Measurement in Concrete Structures(https://arxiv.org/abs/2501.09203)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Visual-Spatial Systems has become increasingly essential in concrete crack inspection. However, existing methods often lacks adaptability to diverse scenarios, exhibits limited robustness in image-based approaches, and struggles with curved or complex geometries. To address these limitations, an innovative framework for two-dimensional (2D) crack detection, three-dimensional (3D) reconstruction, and 3D automatic crack measurement was proposed by integrating computer vision technologies and multi-modal Simultaneous localization and mapping (SLAM) in this study. Firstly, building on a base DeepLabv3+ segmentation model, and incorporating specific refinements utilizing foundation model Segment Anything Model (SAM), we developed a crack segmentation method with strong generalization across unfamiliar scenarios, enabling the generation of precise 2D crack masks. To enhance the accuracy and robustness of 3D reconstruction, Light Detection and Ranging (LiDAR) point clouds were utilized together with image data and segmentation masks. By leveraging both image- and LiDAR-SLAM, we developed a multi-frame and multi-modal fusion framework that produces dense, colorized point clouds, effectively capturing crack semantics at a 3D real-world scale. Furthermore, the crack geometric attributions were measured automatically and directly within 3D dense point cloud space, surpassing the limitations of conventional 2D image-based measurements. This advancement makes the method suitable for structural components with curved and complex 3D geometries. Experimental results across various concrete structures highlight the significant improvements and unique advantages of the proposed method, demonstrating its effectiveness, accuracy, and robustness in real-world applications.</li>
</ul>

<h3>Title: FineMedLM-o1: Enhancing the Medical Reasoning Ability of LLM from Supervised Fine-Tuning to Test-Time Training</h3>
<ul>
<li><strong>Authors: </strong>Hongzhou Yu, Tianhao Cheng, Ying Cheng, Rui Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09213">https://arxiv.org/abs/2501.09213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09213">https://arxiv.org/pdf/2501.09213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09213]] FineMedLM-o1: Enhancing the Medical Reasoning Ability of LLM from Supervised Fine-Tuning to Test-Time Training(https://arxiv.org/abs/2501.09213)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have shown promise in medical applications such as disease diagnosis and treatment planning. However, most existing medical LLMs struggle with the advanced reasoning required for complex clinical scenarios, such as differential diagnosis or personalized treatment suggestions. We proposed FineMedLM-o1, which leverages high-quality synthetic medical data and long-form reasoning data for Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), enabling advanced dialogue and deep reasoning capabilities. Additionally, we introduced Test-Time Training (TTT) in the medical domain for the first time, facilitating domain adaptation and ensuring reliable, accurate reasoning. Experimental results demonstrate that FineMedLM-o1 achieves a 23% average performance improvement over prior models on key medical benchmarks. Furthermore, the introduction of TTT provides an additional 14% performance boost, highlighting its effectiveness in enhancing medical reasoning capabilities. To support this process, we also proposed a novel method for synthesizing medical dialogue. Compared to other open-source datasets, our dataset stands out as superior in both quality and complexity. The project and data will be released on GitHub.</li>
</ul>

<h3>Title: Boosting Short Text Classification with Multi-Source Information Exploration and Dual-Level Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Yonghao Liu, Mengyu Li, Wei Pang, Fausto Giunchiglia, Lan Huang, Xiaoyue Feng, Renchu Guan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09214">https://arxiv.org/abs/2501.09214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09214">https://arxiv.org/pdf/2501.09214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09214]] Boosting Short Text Classification with Multi-Source Information Exploration and Dual-Level Contrastive Learning(https://arxiv.org/abs/2501.09214)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Short text classification, as a research subtopic in natural language processing, is more challenging due to its semantic sparsity and insufficient labeled samples in practical scenarios. We propose a novel model named MI-DELIGHT for short text classification in this work. Specifically, it first performs multi-source information (i.e., statistical information, linguistic information, and factual information) exploration to alleviate the sparsity issues. Then, the graph learning approach is adopted to learn the representation of short texts, which are presented in graph forms. Moreover, we introduce a dual-level (i.e., instance-level and cluster-level) contrastive learning auxiliary task to effectively capture different-grained contrastive information within massive unlabeled data. Meanwhile, previous models merely perform the main task and auxiliary tasks in parallel, without considering the relationship among tasks. Therefore, we introduce a hierarchical architecture to explicitly model the correlations between tasks. We conduct extensive experiments across various benchmark datasets, demonstrating that MI-DELIGHT significantly surpasses previous competitive models. It even outperforms popular large language models on several datasets.</li>
</ul>

<h3>Title: EILID: Execution Integrity for Low-end IoT Devices</h3>
<ul>
<li><strong>Authors: </strong>Sashidhar Jakkamsetti, Youngil Kim, Andrew Searles, Gene Tsudik</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09216">https://arxiv.org/abs/2501.09216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09216">https://arxiv.org/pdf/2501.09216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09216]] EILID: Execution Integrity for Low-end IoT Devices(https://arxiv.org/abs/2501.09216)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Prior research yielded many techniques to mitigate software compromise for low-end Internet of Things (IoT) devices. Some of them detect software modifications via remote attestation and similar services, while others preventatively ensure software (static) integrity. However, achieving run-time (dynamic) security, e.g., control-flow integrity (CFI), remains a challenge. Control-flow attestation (CFA) is one approach that minimizes the burden on devices. However, CFA is not a real-time countermeasure against run-time attacks since it requires communication with a verifying entity. This poses significant risks if safety- or time-critical tasks have memory vulnerabilities. To address this issue, we construct EILID - a hybrid architecture that ensures software execution integrity by actively monitoring control-flow violations on low-end devices. EILID is built atop CASU, a prevention-based (i.e., active) hybrid Root-of-Trust (RoT) that guarantees software immutability. EILID achieves fine-grained backward-edge and function-level forward-edge CFI via semi-automatic code instrumentation and a secure shadow stack.</li>
</ul>

<h3>Title: Adaptive Law-Based Transformation (ALT): A Lightweight Feature Representation for Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Marcell T. Kurbucz, Balázs Hajós, Balázs P. Halmos, Vince Á. Molnár, Antal Jakovác</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09217">https://arxiv.org/abs/2501.09217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09217">https://arxiv.org/pdf/2501.09217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09217]] Adaptive Law-Based Transformation (ALT): A Lightweight Feature Representation for Time Series Classification(https://arxiv.org/abs/2501.09217)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Time series classification (TSC) is fundamental in numerous domains, including finance, healthcare, and environmental monitoring. However, traditional TSC methods often struggle with the inherent complexity and variability of time series data. Building on our previous work with the linear law-based transformation (LLT) - which improved classification accuracy by transforming the feature space based on key data patterns - we introduce adaptive law-based transformation (ALT). ALT enhances LLT by incorporating variable-length shifted time windows, enabling it to capture distinguishing patterns of various lengths and thereby handle complex time series more effectively. By mapping features into a linearly separable space, ALT provides a fast, robust, and transparent solution that achieves state-of-the-art performance with only a few hyperparameters.</li>
</ul>

<h3>Title: A Simple Graph Contrastive Learning Framework for Short Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Yonghao Liu, Fausto Giunchiglia, Lan Huang, Ximing Li, Xiaoyue Feng, Renchu Guan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09219">https://arxiv.org/abs/2501.09219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09219">https://arxiv.org/pdf/2501.09219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09219]] A Simple Graph Contrastive Learning Framework for Short Text Classification(https://arxiv.org/abs/2501.09219)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Short text classification has gained significant attention in the information age due to its prevalence and real-world applications. Recent advancements in graph learning combined with contrastive learning have shown promising results in addressing the challenges of semantic sparsity and limited labeled data in short text classification. However, existing models have certain limitations. They rely on explicit data augmentation techniques to generate contrastive views, resulting in semantic corruption and noise. Additionally, these models only focus on learning the intrinsic consistency between the generated views, neglecting valuable discriminative information from other potential views. To address these issues, we propose a Simple graph contrastive learning framework for Short Text Classification (SimSTC). Our approach involves performing graph learning on multiple text-related component graphs to obtain multi-view text embeddings. Subsequently, we directly apply contrastive learning on these embeddings. Notably, our method eliminates the need for data augmentation operations to generate contrastive views while still leveraging the benefits of multi-view contrastive learning. Despite its simplicity, our model achieves outstanding performance, surpassing large language models on various datasets.</li>
</ul>

<h3>Title: Leveraging Scale-aware Representations for improved Concept-Representation Alignment in ViTs</h3>
<ul>
<li><strong>Authors: </strong>Sanchit Sinha, Guangzhi Xiong, Aidong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09221">https://arxiv.org/abs/2501.09221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09221">https://arxiv.org/pdf/2501.09221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09221]] Leveraging Scale-aware Representations for improved Concept-Representation Alignment in ViTs(https://arxiv.org/abs/2501.09221)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability, transformer</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) are increasingly being adopted in various sensitive vision applications - like medical diagnosis, facial recognition, etc. To improve the interpretability of such models, many approaches attempt to forward-align them with carefully annotated abstract, human-understandable semantic entities - concepts. Concepts provide global rationales to the model predictions and can be quickly understood/intervened on by domain experts. Most current research focuses on designing model-agnostic, plug-and-play generic concept-based explainability modules that do not incorporate the inner workings of foundation models (e.g., inductive biases, scale invariance, etc.) during training. To alleviate this issue for ViTs, in this paper, we propose a novel Concept Representation Alignment Module (CRAM) which learns both scale and position-aware representations from multi-scale feature pyramids and patch representations respectively. CRAM further aligns these representations with concept annotations through an attention matrix. The proposed CRAM module improves the predictive performance of ViT architectures and also provides accurate and robust concept explanations as demonstrated on five datasets - including three widely used benchmarks (CUB, Pascal APY, Concept-MNIST) and 2 real-world datasets (AWA2, KITS).</li>
</ul>

<h3>Title: Foundations of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tong Xiao, Jingbo Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09223">https://arxiv.org/abs/2501.09223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09223">https://arxiv.org/pdf/2501.09223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09223]] Foundations of Large Language Models(https://arxiv.org/abs/2501.09223)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting-edge technologies. The book is structured into four main chapters, each exploring a key area: pre-training, generative models, prompting techniques, and alignment methods. It is intended for college students, professionals, and practitioners in natural language processing and related fields, and can serve as a reference for anyone interested in large language models.</li>
</ul>

<h3>Title: Tessellated Linear Model for Age Prediction from Voice</h3>
<ul>
<li><strong>Authors: </strong>Dareen Alharthi, Mahsa Zamani, Bhiksha Raj, Rita Singh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09229">https://arxiv.org/abs/2501.09229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09229">https://arxiv.org/pdf/2501.09229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09229]] Tessellated Linear Model for Age Prediction from Voice(https://arxiv.org/abs/2501.09229)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>Voice biometric tasks, such as age estimation require modeling the often complex relationship between voice features and the biometric variable. While deep learning models can handle such complexity, they typically require large amounts of accurately labeled data to perform well. Such data are often scarce for biometric tasks such as voice-based age prediction. On the other hand, simpler models like linear regression can work with smaller datasets but often fail to generalize to the underlying non-linear patterns present in the data. In this paper we propose the Tessellated Linear Model (TLM), a piecewise linear approach that combines the simplicity of linear models with the capacity of non-linear functions. TLM tessellates the feature space into convex regions and fits a linear model within each region. We optimize the tessellation and the linear models using a hierarchical greedy partitioning. We evaluated TLM on the TIMIT dataset on the task of age prediction from voice, where it outperformed state-of-the-art deep learning models.</li>
</ul>

<h3>Title: Task Vectors in In-Context Learning: Emergence, Formation, and Benefit</h3>
<ul>
<li><strong>Authors: </strong>Liu Yang, Ziqian Lin, Kangwook Lee, Dimitris Papailiopoulos, Robert Nowak</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09240">https://arxiv.org/abs/2501.09240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09240">https://arxiv.org/pdf/2501.09240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09240]] Task Vectors in In-Context Learning: Emergence, Formation, and Benefit(https://arxiv.org/abs/2501.09240)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>In-context learning is a remarkable capability of transformers, referring to their ability to adapt to specific tasks based on a short history or context. Previous research has found that task-specific information is locally encoded within models, though their emergence and functionality remain unclear due to opaque pre-training processes. In this work, we investigate the formation of task vectors in a controlled setting, using models trained from scratch on synthetic datasets. Our findings confirm that task vectors naturally emerge under certain conditions, but the tasks may be relatively weakly and/or non-locally encoded within the model. To promote strong task vectors encoded at a prescribed location within the model, we propose an auxiliary training mechanism based on a task vector prompting loss (TVP-loss). This method eliminates the need to search for task-correlated encodings within the trained model and demonstrably improves robustness and generalization.</li>
</ul>

<h3>Title: Practical Spoofing Attacks on Galileo Open Service Navigation Message Authentication</h3>
<ul>
<li><strong>Authors: </strong>Haiyang Wang, Yuanyu Zhang, Xinghui Zhu, Ji He, Shuangtrui Zhao, Yulong Shen, Xiaohong Jiang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09246">https://arxiv.org/abs/2501.09246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09246">https://arxiv.org/pdf/2501.09246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09246]] Practical Spoofing Attacks on Galileo Open Service Navigation Message Authentication(https://arxiv.org/abs/2501.09246)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>This paper examines the Galileo Open Service Navigation Message Authentication (OSNMA) and, for the first time, discovers two critical vulnerabilities, namely artificially-manipulated time synchronization (ATS) and interruptible message authentication (IMA). ATS allows attackers falsify a receiver's signals and/or local reference time (LRT) while still fulfilling the time synchronization (TS) requirement. IMA allows temporary interruption of the navigation data authentication process due to the reception of a broken message (probably caused by spoofing attacks) and restores the authentication later. By exploiting the ATS vulnerability, we propose a TS-comply replay (TSR) attack with two variants (real-time and non-real-time), where attackers replay signals to a victim receiver while strictly complying with the TS rule. We further propose a TS-comply forgery (TSF) attack, where attackers first use a previously-disclosed key to forge a message based on the OSNMA protocol, then tamper with the vitcim receiver's LRT correspondingly to comply with the TS rule and finally transmit the forged message to the receiver. Finally, we propose a concatenating replay (CR) attack based on the IMA vulnerability, where attackers concatenate replayed signals to the victim receiver's signals in a way that still enables correct verification of the navigation data in the replayed signals. To validate the effectiveness of the proposed attacks, we conduct real-world experiments with a commercial Galileo receiver manufactured by Septentrio, two software-defined radio (SDR) devices, open-source Galileo-SDR-SIM and OSNMAlib software. The results showed that all the attacks can successfully pass the OSNMA scheme and the TSF attack can spoof receivers to arbitrary locations.</li>
</ul>

<h3>Title: Clone-Robust AI Alignment</h3>
<ul>
<li><strong>Authors: </strong>Ariel D. Procaccia, Benjamin Schiffer, Shirley Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09254">https://arxiv.org/abs/2501.09254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09254">https://arxiv.org/pdf/2501.09254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09254]] Clone-Robust AI Alignment(https://arxiv.org/abs/2501.09254)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>A key challenge in training Large Language Models (LLMs) is properly aligning them with human preferences. Reinforcement Learning with Human Feedback (RLHF) uses pairwise comparisons from human annotators to train reward functions and has emerged as a popular alignment method. However, input datasets in RLHF are not necessarily balanced in the types of questions and answers that are included. Therefore, we want RLHF algorithms to perform well even when the set of alternatives is not uniformly distributed. Drawing on insights from social choice theory, we introduce robustness to approximate clones, a desirable property of RLHF algorithms which requires that adding near-duplicate alternatives does not significantly change the learned reward function. We first demonstrate that the standard RLHF algorithm based on regularized maximum likelihood estimation (MLE) fails to satisfy this property. We then propose the weighted MLE, a new RLHF algorithm that modifies the standard regularized MLE by weighting alternatives based on their similarity to other alternatives. This new algorithm guarantees robustness to approximate clones while preserving desirable theoretical properties.</li>
</ul>

<h3>Title: Delayed Fusion: Integrating Large Language Models into First-Pass Decoding in End-to-end Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Takaaki Hori, Martin Kocour, Adnan Haider, Erik McDermott, Xiaodan Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09258">https://arxiv.org/abs/2501.09258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09258">https://arxiv.org/pdf/2501.09258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09258]] Delayed Fusion: Integrating Large Language Models into First-Pass Decoding in End-to-end Speech Recognition(https://arxiv.org/abs/2501.09258)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents an efficient decoding approach for end-to-end automatic speech recognition (E2E-ASR) with large language models (LLMs). Although shallow fusion is the most common approach to incorporate language models into E2E-ASR decoding, we face two practical problems with LLMs. (1) LLM inference is computationally costly. (2) There may be a vocabulary mismatch between the ASR model and the LLM. To resolve this mismatch, we need to retrain the ASR model and/or the LLM, which is at best time-consuming and in many cases not feasible. We propose "delayed fusion," which applies LLM scores to ASR hypotheses with a delay during decoding and enables easier use of pre-trained LLMs in ASR tasks. This method can reduce not only the number of hypotheses scored by the LLM but also the number of LLM inference calls. It also allows re-tokenizion of ASR hypotheses during decoding if ASR and LLM employ different tokenizations. We demonstrate that delayed fusion provides improved decoding speed and accuracy compared to shallow fusion and N-best rescoring using the LibriHeavy ASR corpus and three public LLMs, OpenLLaMA 3B & 7B and Mistral 7B.</li>
</ul>

<h3>Title: Perspective Transition of Large Language Models for Solving Subjective Tasks</h3>
<ul>
<li><strong>Authors: </strong>Xiaolong Wang, Yuanchi Zhang, Ziyue Wang, Yuzhuang Xu, Fuwen Luo, Yile Wang, Peng Li, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09265">https://arxiv.org/abs/2501.09265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09265">https://arxiv.org/pdf/2501.09265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09265]] Perspective Transition of Large Language Models for Solving Subjective Tasks(https://arxiv.org/abs/2501.09265)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized the field of natural language processing, enabling remarkable progress in various tasks. Different from objective tasks such as commonsense reasoning and arithmetic question-answering, the performance of LLMs on subjective tasks is still limited, where the perspective on the specific problem plays crucial roles for better interpreting the context and giving proper response. For example, in certain scenarios, LLMs may perform better when answering from an expert role perspective, potentially eliciting their relevant domain knowledge. In contrast, in some scenarios, LLMs may provide more accurate responses when answering from a third-person standpoint, enabling a more comprehensive understanding of the problem and potentially mitigating inherent biases. In this paper, we propose Reasoning through Perspective Transition (RPT), a method based on in-context learning that enables LLMs to dynamically select among direct, role, and third-person perspectives for the best way to solve corresponding subjective problem. Through extensive experiments on totally 12 subjective tasks by using both closed-source and open-source LLMs including GPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single fixed perspective based methods such as chain-of-thought prompting and expert prompting, highlights the intricate ways that LLMs can adapt their perspectives to provide nuanced and contextually appropriate responses for different problems.</li>
</ul>

<h3>Title: Knowledge Distillation for Image Restoration : Simultaneous Learning from Degraded and Clean Images</h3>
<ul>
<li><strong>Authors: </strong>Yongheng Zhang, Danfeng Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09268">https://arxiv.org/abs/2501.09268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09268">https://arxiv.org/pdf/2501.09268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09268]] Knowledge Distillation for Image Restoration : Simultaneous Learning from Degraded and Clean Images(https://arxiv.org/abs/2501.09268)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Model compression through knowledge distillation has seen extensive application in classification and segmentation tasks. However, its potential in image-to-image translation, particularly in image restoration, remains underexplored. To address this gap, we propose a Simultaneous Learning Knowledge Distillation (SLKD) framework tailored for model compression in image restoration tasks. SLKD employs a dual-teacher, single-student architecture with two distinct learning strategies: Degradation Removal Learning (DRL) and Image Reconstruction Learning (IRL), simultaneously. In DRL, the student encoder learns from Teacher A to focus on removing degradation factors, guided by a novel BRISQUE extractor. In IRL, the student decoder learns from Teacher B to reconstruct clean images, with the assistance of a proposed PIQE extractor. These strategies enable the student to learn from degraded and clean images simultaneously, ensuring high-quality compression of image restoration models. Experimental results across five datasets and three tasks demonstrate that SLKD achieves substantial reductions in FLOPs and parameters, exceeding 80\%, while maintaining strong image restoration performance.</li>
</ul>

<h3>Title: Large Language Model is Secretly a Protein Sequence Optimizer</h3>
<ul>
<li><strong>Authors: </strong>Yinkai Wang, Jiaxing He, Yuanqi Du, Xiaohui Chen, Jianan Canal Li, Li-Ping Liu, Xiaolin Xu, Soha Hassoun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09274">https://arxiv.org/abs/2501.09274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09274">https://arxiv.org/pdf/2501.09274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09274]] Large Language Model is Secretly a Protein Sequence Optimizer(https://arxiv.org/abs/2501.09274)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We consider the protein sequence engineering problem, which aims to find protein sequences with high fitness levels, starting from a given wild-type sequence. Directed evolution has been a dominating paradigm in this field which has an iterative process to generate variants and select via experimental feedback. We demonstrate large language models (LLMs), despite being trained on massive texts, are secretly protein sequence optimizers. With a directed evolutionary method, LLM can perform protein engineering through Pareto and experiment-budget constrained optimization, demonstrating success on both synthetic and experimental fitness landscapes.</li>
</ul>

<h3>Title: Text-guided Synthetic Geometric Augmentation for Zero-shot 3D Understanding</h3>
<ul>
<li><strong>Authors: </strong>Kohei Torimi, Ryosuke Yamada, Daichi Otsuka, Kensho Hara, Yuki M. Asano, Hirokatsu Kataoka, Yoshimitsu Aoki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09278">https://arxiv.org/abs/2501.09278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09278">https://arxiv.org/pdf/2501.09278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09278]] Text-guided Synthetic Geometric Augmentation for Zero-shot 3D Understanding(https://arxiv.org/abs/2501.09278)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Zero-shot recognition models require extensive training data for generalization. However, in zero-shot 3D classification, collecting 3D data and captions is costly and laborintensive, posing a significant barrier compared to 2D vision. Recent advances in generative models have achieved unprecedented realism in synthetic data production, and recent research shows the potential for using generated data as training data. Here, naturally raising the question: Can synthetic 3D data generated by generative models be used as expanding limited 3D datasets? In response, we present a synthetic 3D dataset expansion method, Textguided Geometric Augmentation (TeGA). TeGA is tailored for language-image-3D pretraining, which achieves SoTA in zero-shot 3D classification, and uses a generative textto-3D model to enhance and extend limited 3D datasets. Specifically, we automatically generate text-guided synthetic 3D data and introduce a consistency filtering strategy to discard noisy samples where semantics and geometric shapes do not match with text. In the experiment to double the original dataset size using TeGA, our approach demonstrates improvements over the baselines, achieving zeroshot performance gains of 3.0% on Objaverse-LVIS, 4.6% on ScanObjectNN, and 8.7% on ModelNet40. These results demonstrate that TeGA effectively bridges the 3D data gap, enabling robust zero-shot 3D classification even with limited real training data and paving the way for zero-shot 3D vision application.</li>
</ul>

<h3>Title: To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Kaustubh D. Dhole</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09292">https://arxiv.org/abs/2501.09292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09292">https://arxiv.org/pdf/2501.09292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09292]] To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic Retrieval Augmented Generation(https://arxiv.org/abs/2501.09292)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation equips large language models with the capability to retrieve external knowledge, thereby mitigating hallucinations by incorporating information beyond the model's intrinsic abilities. However, most prior works have focused on invoking retrieval deterministically, which makes it unsuitable for tasks such as long-form question answering. Instead, dynamically performing retrieval by invoking it only when the underlying LLM lacks the required knowledge can be more efficient. In this context, we delve deeper into the question, "To Retrieve or Not to Retrieve?" by exploring multiple uncertainty detection methods. We evaluate these methods for the task of long-form question answering, employing dynamic retrieval, and present our comparisons. Our findings suggest that uncertainty detection metrics, such as Degree Matrix Jaccard and Eccentricity, can reduce the number of retrieval calls by almost half, with only a slight reduction in question-answering accuracy.</li>
</ul>

<h3>Title: Efficient Few-Shot Medical Image Analysis via Hierarchical Contrastive Vision-Language Learning</h3>
<ul>
<li><strong>Authors: </strong>Harrison Fuller, Fernando Gabriela Garcia, Victor Flores</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09294">https://arxiv.org/abs/2501.09294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09294">https://arxiv.org/pdf/2501.09294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09294]] Efficient Few-Shot Medical Image Analysis via Hierarchical Contrastive Vision-Language Learning(https://arxiv.org/abs/2501.09294)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Few-shot learning in medical image classification presents a significant challenge due to the limited availability of annotated data and the complex nature of medical imagery. In this work, we propose Adaptive Vision-Language Fine-tuning with Hierarchical Contrastive Alignment (HiCA), a novel framework that leverages the capabilities of Large Vision-Language Models (LVLMs) for medical image analysis. HiCA introduces a two-stage fine-tuning strategy, combining domain-specific pretraining and hierarchical contrastive learning to align visual and textual representations at multiple levels. We evaluate our approach on two benchmark datasets, Chest X-ray and Breast Ultrasound, achieving state-of-the-art performance in both few-shot and zero-shot settings. Further analyses demonstrate the robustness, generalizability, and interpretability of our method, with substantial improvements in performance compared to existing baselines. Our work highlights the potential of hierarchical contrastive strategies in adapting LVLMs to the unique challenges of medical imaging tasks.</li>
</ul>

<h3>Title: A Study of In-Context-Learning-Based Text-to-SQL Errors</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Shen, Chengcheng Wan, Ruoyi Qiao, Jiazhen Zou, Hang Xu, Yuchen Shao, Yueling Zhang, Weikai Miao, Geguang Pu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09310">https://arxiv.org/abs/2501.09310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09310">https://arxiv.org/pdf/2501.09310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09310]] A Study of In-Context-Learning-Based Text-to-SQL Errors(https://arxiv.org/abs/2501.09310)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been adopted to perform text-to-SQL tasks, utilizing their in-context learning (ICL) capability to translate natural language questions into structured query language (SQL). However, such a technique faces correctness problems and requires efficient repairing solutions. In this paper, we conduct the first comprehensive study of text-to-SQL errors. Our study covers four representative ICL-based techniques, five basic repairing methods, two benchmarks, and two LLM settings. We find that text-to-SQL errors are widespread and summarize 29 error types of 7 categories. We also find that existing repairing attempts have limited correctness improvement at the cost of high computational overhead with many mis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL error detection and repairing framework. The evaluation demonstrates that MapleRepair outperforms existing solutions by repairing 13.8% more queries with neglectable mis-repairs and 67.4% less overhead.</li>
</ul>

<h3>Title: Shape-Based Single Object Classification Using Ensemble Method Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Nur Shazwani Kamarudin, Mokhairi Makhtar, Syadiah Nor Wan Shamsuddin, Syed Abdullah Fadzli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09311">https://arxiv.org/abs/2501.09311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09311">https://arxiv.org/pdf/2501.09311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09311]] Shape-Based Single Object Classification Using Ensemble Method Classifiers(https://arxiv.org/abs/2501.09311)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Nowadays, more and more images are available. Annotation and retrieval of the images pose classification problems, where each class is defined as the group of database images labelled with a common semantic label. Various systems have been proposed for content-based retrieval, as well as for image classification and indexing. In this paper, a hierarchical classification framework has been proposed for bridging the semantic gap effectively and achieving multi-category image classification. A well known pre-processing and post-processing method was used and applied to three problems; image segmentation, object identification and image classification. The method was applied to classify single object images from Amazon and Google datasets. The classification was tested for four different classifiers; BayesNetwork (BN), Random Forest (RF), Bagging and Vote. The estimated classification accuracies ranged from 20% to 99% (using 10-fold cross validation). The Bagging classifier presents the best performance, followed by the Random Forest classifier.</li>
</ul>

<h3>Title: Cooperative Decentralized Backdoor Attacks on Vertical Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Seohyun Lee, Wenzhi Fang, Anindya Bijoy Das, Seyyedali Hosseinalipour, David J. Love, Christopher G. Brinton</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09320">https://arxiv.org/abs/2501.09320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09320">https://arxiv.org/pdf/2501.09320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09320]] Cooperative Decentralized Backdoor Attacks on Vertical Federated Learning(https://arxiv.org/abs/2501.09320)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is vulnerable to backdoor attacks, where adversaries alter model behavior on target classification labels by embedding triggers into data samples. While these attacks have received considerable attention in horizontal FL, they are less understood for vertical FL (VFL), where devices hold different features of the samples, and only the server holds the labels. In this work, we propose a novel backdoor attack on VFL which (i) does not rely on gradient information from the server and (ii) considers potential collusion among multiple adversaries for sample selection and trigger embedding. Our label inference model augments variational autoencoders with metric learning, which adversaries can train locally. A consensus process over the adversary graph topology determines which datapoints to poison. We further propose methods for trigger splitting across the adversaries, with an intensity-based implantation scheme skewing the server towards the trigger. Our convergence analysis reveals the impact of backdoor perturbations on VFL indicated by a stationarity gap for the trained model, which we verify empirically as well. We conduct experiments comparing our attack with recent backdoor VFL approaches, finding that ours obtains significantly higher success rates for the same main task performance despite not using server information. Additionally, our results verify the impact of collusion on attack performance.</li>
</ul>

<h3>Title: Soft Knowledge Distillation with Multi-Dimensional Cross-Net Attention for Image Restoration Models Compression</h3>
<ul>
<li><strong>Authors: </strong>Yongheng Zhang, Danfeng Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09321">https://arxiv.org/abs/2501.09321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09321">https://arxiv.org/pdf/2501.09321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09321]] Soft Knowledge Distillation with Multi-Dimensional Cross-Net Attention for Image Restoration Models Compression(https://arxiv.org/abs/2501.09321)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based encoder-decoder models have achieved remarkable success in image-to-image transfer tasks, particularly in image restoration. However, their high computational complexity-manifested in elevated FLOPs and parameter counts-limits their application in real-world scenarios. Existing knowledge distillation methods in image restoration typically employ lightweight student models that directly mimic the intermediate features and reconstruction results of the teacher, overlooking the implicit attention relationships between them. To address this, we propose a Soft Knowledge Distillation (SKD) strategy that incorporates a Multi-dimensional Cross-net Attention (MCA) mechanism for compressing image restoration models. This mechanism facilitates interaction between the student and teacher across both channel and spatial dimensions, enabling the student to implicitly learn the attention matrices. Additionally, we employ a Gaussian kernel function to measure the distance between student and teacher features in kernel space, ensuring stable and efficient feature learning. To further enhance the quality of reconstructed images, we replace the commonly used L1 or KL divergence loss with a contrastive learning loss at the image level. Experiments on three tasks-image deraining, deblurring, and denoising-demonstrate that our SKD strategy significantly reduces computational complexity while maintaining strong image restoration capabilities.</li>
</ul>

<h3>Title: Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks</h3>
<ul>
<li><strong>Authors: </strong>Yixiao Xu, Binxing Fang, Rui Wang, Yinghai Zhou, Shouling Ji, Yuan Liu, Mohan Li, Zhihong Tian</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09328">https://arxiv.org/abs/2501.09328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09328">https://arxiv.org/pdf/2501.09328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09328]] Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks(https://arxiv.org/abs/2501.09328)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, extraction, watermark</a></li>
<li><strong>Abstract: </strong>Developing high-performance deep learning models is resource-intensive, leading model owners to utilize Machine Learning as a Service (MLaaS) platforms instead of publicly releasing their models. However, malicious users may exploit query interfaces to execute model extraction attacks, reconstructing the target model's functionality locally. While prior research has investigated triggerable watermarking techniques for asserting ownership, existing methods face significant challenges: (1) most approaches require additional training, resulting in high overhead and limited flexibility, and (2) they often fail to account for advanced attackers, leaving them vulnerable to adaptive attacks. In this paper, we propose Neural Honeytrace, a robust plug-and-play watermarking framework against model extraction attacks. We first formulate a watermark transmission model from an information-theoretic perspective, providing an interpretable account of the principles and limitations of existing triggerable watermarking. Guided by the model, we further introduce: (1) a similarity-based training-free watermarking method for plug-and-play and flexible watermarking, and (2) a distribution-based multi-step watermark information transmission strategy for robust watermarking. Comprehensive experiments on four datasets demonstrate that Neural Honeytrace outperforms previous methods in efficiency and resisting adaptive attacks. Neural Honeytrace reduces the average number of samples required for a worst-case t-Test-based copyright claim from $12,000$ to $200$ with zero training cost.</li>
</ul>

<h3>Title: Prompt-CAM: A Simpler Interpretable Transformer for Fine-Grained Analysis</h3>
<ul>
<li><strong>Authors: </strong>Arpita Chowdhury, Dipanjyoti Paul, Zheda Mai, Jianyang Gu, Ziheng Zhang, Kazi Sajeed Mehrab, Elizabeth G. Campolongo, Daniel Rubenstein, Charles V. Stewart, Anuj Karpatne, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09333">https://arxiv.org/abs/2501.09333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09333">https://arxiv.org/pdf/2501.09333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09333]] Prompt-CAM: A Simpler Interpretable Transformer for Fine-Grained Analysis(https://arxiv.org/abs/2501.09333)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer</a></li>
<li><strong>Abstract: </strong>We present a simple usage of pre-trained Vision Transformers (ViTs) for fine-grained analysis, aiming to identify and localize the traits that distinguish visually similar categories, such as different bird species or dog breeds. Pre-trained ViTs such as DINO have shown remarkable capabilities to extract localized, informative features. However, using saliency maps like Grad-CAM can hardly point out the traits: they often locate the whole object by a blurred, coarse heatmap, not traits. We propose a novel approach Prompt Class Attention Map (Prompt-CAM) to the rescue. Prompt-CAM learns class-specific prompts to a pre-trained ViT and uses the corresponding outputs for classification. To classify an image correctly, the true-class prompt must attend to the unique image patches not seen in other classes' images, i.e., traits. As such, the true class's multi-head attention maps reveal traits and their locations. Implementation-wise, Prompt-CAM is almost a free lunch by simply modifying the prediction head of Visual Prompt Tuning (VPT). This makes Prompt-CAM fairly easy to train and apply, sharply contrasting other interpretable methods that design specific models and training processes. It is even simpler than the recently published INterpretable TRansformer (INTR), whose encoder-decoder architecture prevents it from leveraging pre-trained ViTs. Extensive empirical studies on a dozen datasets from various domains (e.g., birds, fishes, insects, fungi, flowers, food, and cars) validate Prompt-CAM superior interpretation capability.</li>
</ul>

<h3>Title: Jodes: Efficient Oblivious Join in the Distributed Setting</h3>
<ul>
<li><strong>Authors: </strong>Yilei Wang, Xiangdong Zeng, Sheng Wang, Feifei Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DB, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09334">https://arxiv.org/abs/2501.09334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09334">https://arxiv.org/pdf/2501.09334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09334]] Jodes: Efficient Oblivious Join in the Distributed Setting(https://arxiv.org/abs/2501.09334)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack</a></li>
<li><strong>Abstract: </strong>Trusted execution environment (TEE) has provided an isolated and secure environment for building cloud-based analytic systems, but it still suffers from access pattern leakages caused by side-channel attacks. To better secure the data, computation inside TEE enclave should be made oblivious, which introduces significant overhead and severely slows down the computation. A natural way to speed up is to build the analytic system with multiple servers in the distributed setting. However, this setting raises a new security concern -- the volumes of the transmissions among these servers can leak sensitive information to a network adversary. Existing works have designed specialized algorithms to address this concern, but their supports for equi-join, one of the most important but non-trivial database operators, are either inefficient, limited, or under a weak security assumption. In this paper, we present Jodes, an efficient oblivious join algorithm in the distributed setting. Jodes prevents the leakage on both the network and enclave sides, supports a general equi-join operation, and provides a high security level protection that only publicizes the input sizes and the output size. Meanwhile, it achieves both communication cost and computation cost asymptotically superior to existing algorithms. To demonstrate the practicality of Jodes, we conduct experiments in the distributed setting comprising 16 servers. Empirical results show that Jodes achieves up to a sixfold performance improvement over state-of-the-art join algorithms.</li>
</ul>

<h3>Title: Rational Tuning of LLM Cascades via Probabilistic Modeling</h3>
<ul>
<li><strong>Authors: </strong>Michael J. Zellinger, Matt Thomson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09345">https://arxiv.org/abs/2501.09345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09345">https://arxiv.org/pdf/2501.09345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09345]] Rational Tuning of LLM Cascades via Probabilistic Modeling(https://arxiv.org/abs/2501.09345)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding the reliability of large language models (LLMs) has recently garnered significant attention. Given LLMs' propensity to hallucinate, as well as their high sensitivity to prompt design, it is already challenging to predict the performance of an individual LLM. However, the problem becomes more complex for compound LLM systems such as cascades, where in addition to each model's standalone performance, we must understand how the error rates of different models interact. In this paper, we present a probabilistic model for the joint performance distribution of a sequence of LLMs, which enables a framework for rationally tuning the confidence thresholds of a LLM cascade using continuous optimization. Compared to selecting confidence thresholds using grid search, our parametric Markov-copula model significantly improves runtime scaling with respect to the length of the cascade and the desired resolution of the cost-error curve, turning them from intractable into low-order polynomial. In addition, the optimal thresholds computed using our continuous optimization-based algorithm increasingly outperform those found via grid search as cascade length grows, improving the area under the cost-error curve by 1.9% on average for cascades consisting of at least three models. Overall, our Markov-copula model provides a rational basis for tuning LLM cascade performance and points to the potential of probabilistic methods in analyzing LLM systems.</li>
</ul>

<h3>Title: UVRM: A Scalable 3D Reconstruction Model from Unposed Videos</h3>
<ul>
<li><strong>Authors: </strong>Shiu-hong Kao, Xiao Li, Jinglu Wang, Chi-Keung Tang, Yu-Wing Tai, Yan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09347">https://arxiv.org/abs/2501.09347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09347">https://arxiv.org/pdf/2501.09347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09347]] UVRM: A Scalable 3D Reconstruction Model from Unposed Videos(https://arxiv.org/abs/2501.09347)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Large Reconstruction Models (LRMs) have recently become a popular method for creating 3D foundational models. Training 3D reconstruction models with 2D visual data traditionally requires prior knowledge of camera poses for the training samples, a process that is both time-consuming and prone to errors. Consequently, 3D reconstruction training has been confined to either synthetic 3D datasets or small-scale datasets with annotated poses. In this study, we investigate the feasibility of 3D reconstruction using unposed video data of various objects. We introduce UVRM, a novel 3D reconstruction model capable of being trained and evaluated on monocular videos without requiring any information about the pose. UVRM uses a transformer network to implicitly aggregate video frames into a pose-invariant latent feature space, which is then decoded into a tri-plane 3D representation. To obviate the need for ground-truth pose annotations during training, UVRM employs a combination of the score distillation sampling (SDS) method and an analysis-by-synthesis approach, progressively synthesizing pseudo novel-views using a pre-trained diffusion model. We qualitatively and quantitatively evaluate UVRM's performance on the G-Objaverse and CO3D datasets without relying on pose information. Extensive experiments show that UVRM is capable of effectively and efficiently reconstructing a wide range of 3D objects from unposed videos.</li>
</ul>

<h3>Title: PAL: Prompting Analytic Learning with Missing Modality for Multi-Modal Class-Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Xianghu Yue, Yiming Chen, Xueyi Zhang, Xiaoxue Gao, Mengling Feng, Mingrui Lao, Huiping Zhuang, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MM, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09352">https://arxiv.org/abs/2501.09352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09352">https://arxiv.org/pdf/2501.09352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09352]] PAL: Prompting Analytic Learning with Missing Modality for Multi-Modal Class-Incremental Learning(https://arxiv.org/abs/2501.09352)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-modal class-incremental learning (MMCIL) seeks to leverage multi-modal data, such as audio-visual and image-text pairs, thereby enabling models to learn continuously across a sequence of tasks while mitigating forgetting. While existing studies primarily focus on the integration and utilization of multi-modal information for MMCIL, a critical challenge remains: the issue of missing modalities during incremental learning phases. This oversight can exacerbate severe forgetting and significantly impair model performance. To bridge this gap, we propose PAL, a novel exemplar-free framework tailored to MMCIL under missing-modality scenarios. Concretely, we devise modality-specific prompts to compensate for missing information, facilitating the model to maintain a holistic representation of the data. On this foundation, we reformulate the MMCIL problem into a Recursive Least-Squares task, delivering an analytical linear solution. Building upon these, PAL not only alleviates the inherent under-fitting limitation in analytic learning but also preserves the holistic representation of missing-modality data, achieving superior performance with less forgetting across various multi-modal incremental scenarios. Extensive experiments demonstrate that PAL significantly outperforms competitive methods across various datasets, including UPMC-Food101 and N24News, showcasing its robustness towards modality absence and its anti-forgetting ability to maintain high incremental accuracy.</li>
</ul>

<h3>Title: Identification of Traditional Medicinal Plant Leaves Using an effective Deep Learning model and Self-Curated Dataset</h3>
<ul>
<li><strong>Authors: </strong>Deepjyoti Chetia, Sanjib Kr Kalita, Prof Partha Pratim Baruah, Debasish Dutta, Tanaz Akhter</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09363">https://arxiv.org/abs/2501.09363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09363">https://arxiv.org/pdf/2501.09363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09363]] Identification of Traditional Medicinal Plant Leaves Using an effective Deep Learning model and Self-Curated Dataset(https://arxiv.org/abs/2501.09363)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Medicinal plants have been a key component in producing traditional and modern medicines, especially in the field of Ayurveda, an ancient Indian medical system. Producing these medicines and collecting and extracting the right plant is a crucial step due to the visually similar nature of some plants. The extraction of these plants from nonmedicinal plants requires human expert intervention. To solve the issue of accurate plant identification and reduce the need for a human expert in the collection process; employing computer vision methods will be efficient and beneficial. In this paper, we have proposed a model that solves such issues. The proposed model is a custom convolutional neural network (CNN) architecture with 6 convolution layers, max-pooling layers, and dense layers. The model was tested on three different datasets named Indian Medicinal Leaves Image Dataset,MED117 Medicinal Plant Leaf Dataset, and the self-curated dataset by the authors. The proposed model achieved respective accuracies of 99.5%, 98.4%, and 99.7% using various optimizers including Adam, RMSprop, and SGD with momentum.</li>
</ul>

<h3>Title: Image Segmentation with transformers: An Overview, Challenges and Future</h3>
<ul>
<li><strong>Authors: </strong>Deepjyoti Chetia, Debasish Dutta, Sanjib Kr Kalita</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09372">https://arxiv.org/abs/2501.09372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09372">https://arxiv.org/pdf/2501.09372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09372]] Image Segmentation with transformers: An Overview, Challenges and Future(https://arxiv.org/abs/2501.09372)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Image segmentation, a key task in computer vision, has traditionally relied on convolutional neural networks (CNNs), yet these models struggle with capturing complex spatial dependencies, objects with varying scales, need for manually crafted architecture components and contextual information. This paper explores the shortcomings of CNN-based models and the shift towards transformer architectures -to overcome those limitations. This work reviews state-of-the-art transformer-based segmentation models, addressing segmentation-specific challenges and their solutions. The paper discusses current challenges in transformer-based segmentation and outlines promising future trends, such as lightweight architectures and enhanced data efficiency. This survey serves as a guide for understanding the impact of transformers in advancing segmentation capabilities and overcoming the limitations of traditional models.</li>
</ul>

<h3>Title: Evaluating LLM Abilities to Understand Tabular Electronic Health Records: A Comprehensive Study of Patient Data Extraction and Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Jesus Lovon (IRIT-IRIS), Martin Mouysset (IRIT-IRIS), Jo Oleiwan (IRIT-IRIS), Jose G. Moreno (IRIT-IRIS), Christine Damase-Michel, Lynda Tamine (IRIT-IRIS)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09384">https://arxiv.org/abs/2501.09384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09384">https://arxiv.org/pdf/2501.09384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09384]] Evaluating LLM Abilities to Understand Tabular Electronic Health Records: A Comprehensive Study of Patient Data Extraction and Retrieval(https://arxiv.org/abs/2501.09384)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Electronic Health Record (EHR) tables pose unique challenges among which is the presence of hidden contextual dependencies between medical features with a high level of data dimensionality and sparsity. This study presents the first investigation into the abilities of LLMs to comprehend EHRs for patient data extraction and retrieval. We conduct extensive experiments using the MIMICSQL dataset to explore the impact of the prompt structure, instruction, context, and demonstration, of two backbone LLMs, Llama2 and Meditron, based on task performance. Through quantitative and qualitative analyses, our findings show that optimal feature selection and serialization methods can enhance task performance by up to 26.79% compared to naive approaches. Similarly, in-context learning setups with relevant example selection improve data extraction performance by 5.95%. Based on our study findings, we propose guidelines that we believe would help the design of LLM-based models to support health search.</li>
</ul>

<h3>Title: SVIA: A Street View Image Anonymization Framework for Self-Driving Applications</h3>
<ul>
<li><strong>Authors: </strong>Dongyu Liu, Xuhong Wang, Cen Chen, Yanhao Wang, Shengyue Yao, Yilun Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09393">https://arxiv.org/abs/2501.09393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09393">https://arxiv.org/pdf/2501.09393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09393]] SVIA: A Street View Image Anonymization Framework for Self-Driving Applications(https://arxiv.org/abs/2501.09393)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>In recent years, there has been an increasing interest in image anonymization, particularly focusing on the de-identification of faces and individuals. However, for self-driving applications, merely de-identifying faces and individuals might not provide sufficient privacy protection since street views like vehicles and buildings can still disclose locations, trajectories, and other sensitive information. Therefore, it remains crucial to extend anonymization techniques to street view images to fully preserve the privacy of users, pedestrians, and vehicles. In this paper, we propose a Street View Image Anonymization (SVIA) framework for self-driving applications. The SVIA framework consists of three integral components: a semantic segmenter to segment an input image into functional regions, an inpainter to generate alternatives to privacy-sensitive regions, and a harmonizer to seamlessly stitch modified regions to guarantee visual coherence. Compared to existing methods, SVIA achieves a much better trade-off between image generation quality and privacy protection, as evidenced by experimental results for five common metrics on two widely used public datasets.</li>
</ul>

<h3>Title: Collision Risk Analysis for LEO Satellites with Confidential Orbital Data</h3>
<ul>
<li><strong>Authors: </strong>Svenja Lage, Felicitas Hörmann, Felix Hanke, Michael Karl</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09397">https://arxiv.org/abs/2501.09397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09397">https://arxiv.org/pdf/2501.09397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09397]] Collision Risk Analysis for LEO Satellites with Confidential Orbital Data(https://arxiv.org/abs/2501.09397)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>The growing number of satellites in low Earth orbit (LEO) has increased concerns about the risk of satellite collisions, which can ultimately result in the irretrievable loss of satellites and a growing amount of space debris. To mitigate this risk, accurate collision risk analysis is essential. However, this requires access to sensitive orbital data, which satellite operators are often unwilling to share due to privacy concerns. This contribution proposes a solution based on fully homomorphic encryption (FHE) and thus enables secure and private collision risk analysis. In contrast to existing methods, this approach ensures that collision risk analysis can be performed on sensitive orbital data without revealing it to other parties. To display the challenges and opportunities of FHE in this context, an implementation of the CKKS scheme is adapted and analyzed for its capacity to satisfy the theoretical requirements of precision and run time.</li>
</ul>

<h3>Title: Fast Searching of Extreme Operating Conditions for Relay Protection Setting Calculation Based on Graph Neural Network and Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yan Li, Jingyu Wang, Jiankang Zhang, Huaiqiang Li, Longfei Ren, Yinhong Li, Dongyuan Shi, Xianzhong Duan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09399">https://arxiv.org/abs/2501.09399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09399">https://arxiv.org/pdf/2501.09399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09399]] Fast Searching of Extreme Operating Conditions for Relay Protection Setting Calculation Based on Graph Neural Network and Reinforcement Learning(https://arxiv.org/abs/2501.09399)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Searching for the Extreme Operating Conditions (EOCs) is one of the core problems of power system relay protection setting calculation. The current methods based on brute-force search, heuristic algorithms, and mathematical programming can hardly meet the requirements of today's power systems in terms of computation speed due to the drastic changes in operating conditions induced by renewables and power electronics. This paper proposes an EOC fast search method, named Graph Dueling Double Deep Q Network (Graph D3QN), which combines graph neural network and deep reinforcement learning to address this challenge. First, the EOC search problem is modeled as a Markov decision process, where the information of the underlying power system is extracted using graph neural networks, so that the EOC of the system can be found via deep reinforcement learning. Then, a two-stage Guided Learning and Free Exploration (GLFE) training framework is constructed to accelerate the convergence speed of reinforcement learning. Finally, the proposed Graph D3QN method is validated through case studies of searching maximum fault current for relay protection setting calculation on the IEEE 39-bus and 118-bus systems. The experimental results demonstrate that Graph D3QN can reduce the computation time by 10 to 1000 times while guaranteeing the accuracy of the selected EOCs.</li>
</ul>

<h3>Title: mGeNTE: A Multilingual Resource for Gender-Neutral Language and Translation</h3>
<ul>
<li><strong>Authors: </strong>Beatrice Savoldi, Eleonora Cupin, Manjinder Thind, Anne Lauscher, Luisa Bentivogli</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09409">https://arxiv.org/abs/2501.09409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09409">https://arxiv.org/pdf/2501.09409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09409]] mGeNTE: A Multilingual Resource for Gender-Neutral Language and Translation(https://arxiv.org/abs/2501.09409)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Gender-neutral language reflects societal and linguistic shifts towards greater inclusivity by avoiding the implication that one gender is the norm over others. This is particularly relevant for grammatical gender languages, which heavily encode the gender of terms for human referents and over-relies on masculine forms, even when gender is unspecified or irrelevant. Language technologies are known to mirror these inequalities, being affected by a male bias and perpetuating stereotypical associations when translating into languages with extensive gendered morphology. In such cases, gender-neutral language can help avoid undue binary assumptions. However, despite its importance for creating fairer multi- and cross-lingual technologies, inclusive language research remains scarce and insufficiently supported in current resources. To address this gap, we present the multilingual mGeNTe dataset. Derived from the bilingual GeNTE (Piergentili et al., 2023), mGeNTE extends the original corpus to include the English-Italian/German/Spanish language pairs. Since each language pair is English-aligned with gendered and neutral sentences in the target languages, mGeNTE enables research in both automatic Gender-Neutral Translation (GNT) and language modelling for three grammatical gender languages.</li>
</ul>

<h3>Title: Towards Robust and Realistic Human Pose Estimation via WiFi Signals</h3>
<ul>
<li><strong>Authors: </strong>Yang Chen, Jingcai Guo, Song Guo, Jingren Zhou, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09411">https://arxiv.org/abs/2501.09411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09411">https://arxiv.org/pdf/2501.09411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09411]] Towards Robust and Realistic Human Pose Estimation via WiFi Signals(https://arxiv.org/abs/2501.09411)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Robust WiFi-based human pose estimation is a challenging task that bridges discrete and subtle WiFi signals to human skeletons. This paper revisits this problem and reveals two critical yet overlooked issues: 1) cross-domain gap, i.e., due to significant variations between source-target domain pose distributions; and 2) structural fidelity gap, i.e., predicted skeletal poses manifest distorted topology, usually with misplaced joints and disproportionate bone lengths. This paper fills these gaps by reformulating the task into a novel two-phase framework dubbed DT-Pose: Domain-consistent representation learning and Topology-constrained Pose decoding. Concretely, we first propose a temporal-consistent contrastive learning strategy with uniformity regularization, coupled with self-supervised masking-reconstruction operations, to enable robust learning of domain-consistent and motion-discriminative WiFi-specific representations. Beyond this, we introduce a simple yet effective pose decoder with task prompts, which integrates Graph Convolution Network (GCN) and Transformer layers to constrain the topology structure of the generated skeleton by exploring the adjacent-overarching relationships among human joints. Extensive experiments conducted on various benchmark datasets highlight the superior performance of our method in tackling these fundamental challenges in both 2D/3D human pose estimation tasks.</li>
</ul>

<h3>Title: FASP: Fast and Accurate Structured Pruning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hanyu Hu, Pengxiang Zhao, Ping Li, Yi Zheng, Zhefeng Wang, Xiaoming Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09412">https://arxiv.org/abs/2501.09412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09412">https://arxiv.org/pdf/2501.09412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09412]] FASP: Fast and Accurate Structured Pruning of Large Language Models(https://arxiv.org/abs/2501.09412)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid increase in the size of large language models (LLMs) has significantly escalated their computational and memory demands, posing challenges for efficient deployment, especially on resource-constrained devices. Structured pruning has emerged as an effective model compression method that can reduce these demands while preserving performance. In this paper, we introduce FASP (Fast and Accurate Structured Pruning), a novel structured pruning framework for LLMs that emphasizes both speed and accuracy. FASP employs a distinctive pruning structure that interlinks sequential layers, allowing for the removal of columns in one layer while simultaneously eliminating corresponding rows in the preceding layer without incurring additional performance loss. The pruning metric, inspired by Wanda, is computationally efficient and effectively selects components to prune. Additionally, we propose a restoration mechanism that enhances model fidelity by adjusting the remaining weights post-pruning. We evaluate FASP on the OPT and LLaMA model families, demonstrating superior performance in terms of perplexity and accuracy on downstream tasks compared to state-of-the-art methods. Our approach achieves significant speed-ups, pruning models such as OPT-125M in 17 seconds and LLaMA-30B in 15 minutes on a single NVIDIA RTX 4090 GPU, making it a highly practical solution for optimizing LLMs.</li>
</ul>

<h3>Title: Dynamic Neural Style Transfer for Artistic Image Generation using VGG19</h3>
<ul>
<li><strong>Authors: </strong>Kapil Kashyap, Mehak Garg, Sean Fargose, Sindhu Nair</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09420">https://arxiv.org/abs/2501.09420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09420">https://arxiv.org/pdf/2501.09420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09420]] Dynamic Neural Style Transfer for Artistic Image Generation using VGG19(https://arxiv.org/abs/2501.09420)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Throughout history, humans have created remark- able works of art, but artificial intelligence has only recently started to make strides in generating visually compelling art. Breakthroughs in the past few years have focused on using convolutional neural networks (CNNs) to separate and manipulate the content and style of images, applying texture synthesis techniques. Nevertheless, a number of current techniques continue to encounter obstacles, including lengthy processing times, restricted choices of style images, and the inability to modify the weight ratio of styles. We proposed a neural style transfer system that can add various artistic styles to a desired image to address these constraints allowing flexible adjustments to style weight ratios and reducing processing time. The system uses the VGG19 model for feature extraction, ensuring high-quality, flexible stylization without compromising content integrity.</li>
</ul>

<h3>Title: AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral Therapy in Psychological Counseling</h3>
<ul>
<li><strong>Authors: </strong>Ancheng Xu, Di Yang, Renhao Li, Jingwei Zhu, Minghuan Tan, Min Yang, Wanxin Qiu, Mingchen Ma, Haihong Wu, Bingyu Li, Feng Sha, Chengming Li, Xiping Hu, Qiang Qu, Derek F.Wong, Ruifeng Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09426">https://arxiv.org/abs/2501.09426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09426">https://arxiv.org/pdf/2501.09426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09426]] AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral Therapy in Psychological Counseling(https://arxiv.org/abs/2501.09426)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Traditional in-person psychological counseling remains primarily niche, often chosen by individuals with psychological issues, while online automated counseling offers a potential solution for those hesitant to seek help due to feelings of shame. Cognitive Behavioral Therapy (CBT) is an essential and widely used approach in psychological counseling. The advent of large language models (LLMs) and agent technology enables automatic CBT diagnosis and treatment. However, current LLM-based CBT systems use agents with a fixed structure, limiting their self-optimization capabilities, or providing hollow, unhelpful suggestions due to redundant response patterns. In this work, we utilize Quora-like and YiXinLi single-round consultation models to build a general agent framework that generates high-quality responses for single-turn psychological consultation scenarios. We use a bilingual dataset to evaluate the quality of single-response consultations generated by each framework. Then, we incorporate dynamic routing and supervisory mechanisms inspired by real psychological counseling to construct a CBT-oriented autonomous multi-agent framework, demonstrating its general applicability. Experimental results indicate that AutoCBT can provide higher-quality automated psychological counseling services.</li>
</ul>

<h3>Title: CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation</h3>
<ul>
<li><strong>Authors: </strong>Hwan Heo, Jangyeong Kim, Seongyeong Lee, Jeong A Wi, Junyoung Choi, Sangjun Ahn</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09433">https://arxiv.org/abs/2501.09433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09433">https://arxiv.org/pdf/2501.09433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09433]] CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation(https://arxiv.org/abs/2501.09433)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The synthesis of high-quality 3D assets from textual or visual inputs has become a central objective in modern generative modeling. Despite the proliferation of 3D generation algorithms, they frequently grapple with challenges such as multi-view inconsistency, slow generation times, low fidelity, and surface reconstruction problems. While some studies have addressed some of these issues, a comprehensive solution remains elusive. In this paper, we introduce \textbf{CaPa}, a carve-and-paint framework that generates high-fidelity 3D assets efficiently. CaPa employs a two-stage process, decoupling geometry generation from texture synthesis. Initially, a 3D latent diffusion model generates geometry guided by multi-view inputs, ensuring structural consistency across perspectives. Subsequently, leveraging a novel, model-agnostic Spatially Decoupled Attention, the framework synthesizes high-resolution textures (up to 4K) for a given geometry. Furthermore, we propose a 3D-aware occlusion inpainting algorithm that fills untextured regions, resulting in cohesive results across the entire model. This pipeline generates high-quality 3D assets in less than 30 seconds, providing ready-to-use outputs for commercial applications. Experimental results demonstrate that CaPa excels in both texture fidelity and geometric stability, establishing a new standard for practical, scalable 3D asset generation.</li>
</ul>

<h3>Title: Scaling up self-supervised learning for improved surgical foundation models</h3>
<ul>
<li><strong>Authors: </strong>Tim J.M. Jaspers, Ronald L.P.D. de Jong, Yiping Li, Carolus H.J. Kusters, Franciscus H.A. Bakker, Romy C. van Jaarsveld, Gino M. Kuiper, Richard van Hillegersberg, Jelle P. Ruurda, Willem M. Brinkman, Josien P.W. Pluim, Peter H.N. de With, Marcel Breeuwer, Yasmina Al Khalil, Fons van der Sommen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09436">https://arxiv.org/abs/2501.09436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09436">https://arxiv.org/pdf/2501.09436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09436]] Scaling up self-supervised learning for improved surgical foundation models(https://arxiv.org/abs/2501.09436)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Foundation models have revolutionized computer vision by achieving vastly superior performance across diverse tasks through large-scale pretraining on extensive datasets. However, their application in surgical computer vision has been limited. This study addresses this gap by introducing SurgeNetXL, a novel surgical foundation model that sets a new benchmark in surgical computer vision. Trained on the largest reported surgical dataset to date, comprising over 4.7 million video frames, SurgeNetXL achieves consistent top-tier performance across six datasets spanning four surgical procedures and three tasks, including semantic segmentation, phase recognition, and critical view of safety (CVS) classification. Compared with the best-performing surgical foundation models, SurgeNetXL shows mean improvements of 2.4, 9.0, and 12.6 percent for semantic segmentation, phase recognition, and CVS classification, respectively. Additionally, SurgeNetXL outperforms the best-performing ImageNet-based variants by 14.4, 4.0, and 1.6 percent in the respective tasks. In addition to advancing model performance, this study provides key insights into scaling pretraining datasets, extending training durations, and optimizing model architectures specifically for surgical computer vision. These findings pave the way for improved generalizability and robustness in data-scarce scenarios, offering a comprehensive framework for future research in this domain. All models and a subset of the SurgeNetXL dataset, including over 2 million video frames, are publicly available at: this https URL.</li>
</ul>

<h3>Title: Solving the unsolvable: Translating case law in Hong Kong</h3>
<ul>
<li><strong>Authors: </strong>King-kui Sin, Xi Xuan, Chunyu Kit, Clara Ho-yan Chan, Honic Ho-kin Ip</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09444">https://arxiv.org/abs/2501.09444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09444">https://arxiv.org/pdf/2501.09444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09444]] Solving the unsolvable: Translating case law in Hong Kong(https://arxiv.org/abs/2501.09444)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper addresses the challenges translating case law under Hong Kong's bilingual legal system. It highlights the initial success of translating all written statutes into Chinese before the 1997 handover, a task mandated by the Basic Law. The effort involved significant collaboration among legal, linguistic, and translation experts, resulting in a comprehensive and culturally appropriate bilingual legal system. However, translating case law remains a significant challenge due to the sheer volume and continuous growth of judicial decisions. The paper critiques the governments and judiciarys sporadic and uncoordinated efforts to translate case law, contrasting it with the thorough approach previously taken for statute translation. Although the government acknowledges the importance of legal bilingualism, it lacks a sustainable strategy for translating case law. The Judiciarys position that translating all judgments is unnecessary, unrealistic, and not cost-effectiveis analyzed and critiqued for its impact on legal transparency and public trust. A proposed solution involves leveraging machine translation technology through a human-machine interactive translation platform, which undergoes two major transitions. Initially based on a neural model, the platform transitions to using a large language model for improved translation accuracy. Furthermore, it evolves from a single-agent system to a multi-agent system, incorporating Translator, Annotator, and Proofreader agents. This multi-agent approach, supported by a grant, aims to facilitate efficient, high-quality translation of judicial judgments by integrating advanced artificial intelligence and continuous feedback mechanisms, thus better meeting the needs of a bilingual legal system.</li>
</ul>

<h3>Title: Double Visual Defense: Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Wang, Cihang Xie, Brian Bartoldson, Bhavya Kailkhura</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09446">https://arxiv.org/abs/2501.09446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09446">https://arxiv.org/pdf/2501.09446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09446]] Double Visual Defense: Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness(https://arxiv.org/abs/2501.09446)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust</a></li>
<li><strong>Abstract: </strong>This paper investigates the robustness of vision-language models against adversarial visual perturbations and introduces a novel ``double visual defense" to enhance this robustness. Unlike previous approaches that resort to lightweight adversarial fine-tuning of a pre-trained CLIP model, we perform large-scale adversarial vision-language pre-training from scratch using web-scale data. We then strengthen the defense by incorporating adversarial visual instruction tuning. The resulting models from each stage, $\Delta$CLIP and $\Delta^2$LLaVA, show substantially enhanced zero-shot robustness and set a new state-of-the-art in adversarial defense for vision-language models. For example, the adversarial robustness of $\Delta$CLIP surpasses that of the previous best models on ImageNet-1k by ~20%. %For example, $\Delta$CLIP surpasses the previous best models on ImageNet-1k by ~20% in terms of adversarial robustness. Similarly, compared to prior art, $\Delta^2$LLaVA brings a ~30% robustness improvement to image captioning task and a ~20% robustness improvement to visual question answering task. Furthermore, our models exhibit stronger zero-shot recognition capability, fewer hallucinations, and superior reasoning performance compared to baselines. Our project page is this https URL.</li>
</ul>

<h3>Title: Scaling Graph-Based Dependency Parsing with Arc Vectorization and Attention-Based Refinement</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Floquet, Joseph Le Roux, Nadi Tomeh, Thierry Charnois</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09451">https://arxiv.org/abs/2501.09451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09451">https://arxiv.org/pdf/2501.09451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09451]] Scaling Graph-Based Dependency Parsing with Arc Vectorization and Attention-Based Refinement(https://arxiv.org/abs/2501.09451)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We propose a novel architecture for graph-based dependency parsing that explicitly constructs vectors, from which both arcs and labels are scored. Our method addresses key limitations of the standard two-pipeline approach by unifying arc scoring and labeling into a single network, reducing scalability issues caused by the information bottleneck and lack of parameter sharing. Additionally, our architecture overcomes limited arc interactions with transformer layers to efficiently simulate higher-order dependencies. Experiments on PTB and UD show that our model outperforms state-of-the-art parsers in both accuracy and efficiency.</li>
</ul>

<h3>Title: Normal-NeRF: Ambiguity-Robust Normal Estimation for Highly Reflective Scenes</h3>
<ul>
<li><strong>Authors: </strong>Ji Shi, Xianghua Ying, Ruohao Guo, Bowei Xing, Wenzhen Yue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09460">https://arxiv.org/abs/2501.09460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09460">https://arxiv.org/pdf/2501.09460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09460]] Normal-NeRF: Ambiguity-Robust Normal Estimation for Highly Reflective Scenes(https://arxiv.org/abs/2501.09460)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural Radiance Fields (NeRF) often struggle with reconstructing and rendering highly reflective scenes. Recent advancements have developed various reflection-aware appearance models to enhance NeRF's capability to render specular reflections. However, the robust reconstruction of highly reflective scenes is still hindered by the inherent shape ambiguity on specular surfaces. Existing methods typically rely on additional geometry priors to regularize the shape prediction, but this can lead to oversmoothed geometry in complex scenes. Observing the critical role of surface normals in parameterizing reflections, we introduce a transmittance-gradient-based normal estimation technique that remains robust even under ambiguous shape conditions. Furthermore, we propose a dual activated densities module that effectively bridges the gap between smooth surface normals and sharp object boundaries. Combined with a reflection-aware appearance model, our proposed method achieves robust reconstruction and high-fidelity rendering of scenes featuring both highly specular reflections and intricate geometric structures. Extensive experiments demonstrate that our method outperforms existing state-of-the-art methods on various datasets.</li>
</ul>

<h3>Title: Pruning for Sparse Diffusion Models based on Gradient Flow</h3>
<ul>
<li><strong>Authors: </strong>Ben Wan, Tianyi Zheng, Zhaoyu Chen, Yuxiao Wang, Jia Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09464">https://arxiv.org/abs/2501.09464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09464">https://arxiv.org/pdf/2501.09464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09464]] Pruning for Sparse Diffusion Models based on Gradient Flow(https://arxiv.org/abs/2501.09464)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Models (DMs) have impressive capabilities among generation models, but are limited to slower inference speeds and higher computational costs. Previous works utilize one-shot structure pruning to derive lightweight DMs from pre-trained ones, but this approach often leads to a significant drop in generation quality and may result in the removal of crucial weights. Thus we propose a iterative pruning method based on gradient flow, including the gradient flow pruning process and the gradient flow pruning criterion. We employ a progressive soft pruning strategy to maintain the continuity of the mask matrix and guide it along the gradient flow of the energy function based on the pruning criterion in sparse space, thereby avoiding the sudden information loss typically caused by one-shot pruning. Gradient-flow based criterion prune parameters whose removal increases the gradient norm of loss function and can enable fast convergence for a pruned model in iterative pruning stage. Our extensive experiments on widely used datasets demonstrate that our method achieves superior performance in efficiency and consistency with pre-trained models.</li>
</ul>

<h3>Title: RE-POSE: Synergizing Reinforcement Learning-Based Partitioning and Offloading for Edge Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Jianrui Shi, Yong Zhao, Zeyang Cui, Xiaoming Shen, Minhang Zeng, Xiaojie Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09465">https://arxiv.org/abs/2501.09465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09465">https://arxiv.org/pdf/2501.09465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09465]] RE-POSE: Synergizing Reinforcement Learning-Based Partitioning and Offloading for Edge Object Detection(https://arxiv.org/abs/2501.09465)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Object detection plays a crucial role in smart video analysis, with applications ranging from autonomous driving and security to smart cities. However, achieving real-time object detection on edge devices presents significant challenges due to their limited computational resources and the high demands of deep neural network (DNN)-based detection models, particularly when processing high-resolution video. Conventional strategies, such as input down-sampling and network up-scaling, often compromise detection accuracy for faster performance or lead to higher inference latency. To address these issues, this paper introduces RE-POSE, a Reinforcement Learning (RL)-Driven Partitioning and Edge Offloading framework designed to optimize the accuracy-latency trade-off in resource-constrained edge environments. Our approach features an RL-Based Dynamic Clustering Algorithm (RL-DCA) that partitions video frames into non-uniform blocks based on object distribution and the computational characteristics of DNNs. Furthermore, a parallel edge offloading scheme is implemented to distribute these blocks across multiple edge servers for concurrent processing. Experimental evaluations show that RE-POSE significantly enhances detection accuracy and reduces inference latency, surpassing existing methods.</li>
</ul>

<h3>Title: DEFOM-Stereo: Depth Foundation Model Based Stereo Matching</h3>
<ul>
<li><strong>Authors: </strong>Hualie Jiang, Zhiqiang Lou, Laiyan Ding, Rui Xu, Minglang Tan, Wenjie Jiang, Rui Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09466">https://arxiv.org/abs/2501.09466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09466">https://arxiv.org/pdf/2501.09466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09466]] DEFOM-Stereo: Depth Foundation Model Based Stereo Matching(https://arxiv.org/abs/2501.09466)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Stereo matching is a key technique for metric depth estimation in computer vision and robotics. Real-world challenges like occlusion and non-texture hinder accurate disparity estimation from binocular matching cues. Recently, monocular relative depth estimation has shown remarkable generalization using vision foundation models. Thus, to facilitate robust stereo matching with monocular depth cues, we incorporate a robust monocular relative depth model into the recurrent stereo-matching framework, building a new framework for depth foundation model-based stereo-matching, DEFOM-Stereo. In the feature extraction stage, we construct the combined context and matching feature encoder by integrating features from conventional CNNs and DEFOM. In the update stage, we use the depth predicted by DEFOM to initialize the recurrent disparity and introduce a scale update module to refine the disparity at the correct scale. DEFOM-Stereo is verified to have comparable performance on the Scene Flow dataset with state-of-the-art (SOTA) methods and notably shows much stronger zero-shot generalization. Moreover, DEFOM-Stereo achieves SOTA performance on the KITTI 2012, KITTI 2015, Middlebury, and ETH3D benchmarks, ranking 1st on many metrics. In the joint evaluation under the robust vision challenge, our model simultaneously outperforms previous models on the individual benchmarks. Both results demonstrate the outstanding capabilities of the proposed model.</li>
</ul>

<h3>Title: Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators</h3>
<ul>
<li><strong>Authors: </strong>Zhaocheng Liu, Quan Tu, Wen Ye, Yu Xiao, Zhishou Zhang, Hengfu Cui, Yalun Zhu, Qiang Ju, Shizheng Li, Jian Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09484">https://arxiv.org/abs/2501.09484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09484">https://arxiv.org/pdf/2501.09484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09484]] Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators(https://arxiv.org/abs/2501.09484)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Online medical consultation (OMC) restricts doctors to gathering patient information solely through inquiries, making the already complex sequential decision-making process of diagnosis even more challenging. Recently, the rapid advancement of large language models has demonstrated a significant potential to transform OMC. However, most studies have primarily focused on improving diagnostic accuracy under conditions of relatively sufficient information, while paying limited attention to the "inquiry" phase of the consultation process. This lack of focus has left the relationship between "inquiry" and "diagnosis" insufficiently explored. In this paper, we first extract real patient interaction strategies from authentic doctor-patient conversations and use these strategies to guide the training of a patient simulator that closely mirrors real-world behavior. By inputting medical records into our patient simulator to simulate patient responses, we conduct extensive experiments to explore the relationship between "inquiry" and "diagnosis" in the consultation process. Experimental results demonstrate that inquiry and diagnosis adhere to the Liebig's law: poor inquiry quality limits the effectiveness of diagnosis, regardless of diagnostic capability, and vice versa. Furthermore, the experiments reveal significant differences in the inquiry performance of various models. To investigate this phenomenon, we categorize the inquiry process into four types: (1) chief complaint inquiry; (2) specification of known symptoms; (3) inquiry about accompanying symptoms; and (4) gathering family or medical history. We analyze the distribution of inquiries across the four types for different models to explore the reasons behind their significant performance differences. We plan to open-source the weights and related code of our patient simulator at this https URL.</li>
</ul>

<h3>Title: The Devil is in the Details: Simple Remedies for Image-to-LiDAR Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Wonjun Jo, Kwon Byung-Ki, Kim Ji-Yeon, Hawook Jeong, Kyungdon Joo, Tae-Hyun Oh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09485">https://arxiv.org/abs/2501.09485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09485">https://arxiv.org/pdf/2501.09485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09485]] The Devil is in the Details: Simple Remedies for Image-to-LiDAR Representation Learning(https://arxiv.org/abs/2501.09485)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>LiDAR is a crucial sensor in autonomous driving, commonly used alongside cameras. By exploiting this camera-LiDAR setup and recent advances in image representation learning, prior studies have shown the promising potential of image-to-LiDAR distillation. These prior arts focus on the designs of their own losses to effectively distill the pre-trained 2D image representations into a 3D model. However, the other parts of the designs have been surprisingly unexplored. We find that fundamental design elements, e.g., the LiDAR coordinate system, quantization according to the existing input interface, and data utilization, are more critical than developing loss functions, which have been overlooked in prior works. In this work, we show that simple fixes to these designs notably outperform existing methods by 16% in 3D semantic segmentation on the nuScenes dataset and 13% in 3D object detection on the KITTI dataset in downstream task performance. We focus on overlooked design choices along the spatial and temporal axes. Spatially, prior work has used cylindrical coordinate and voxel sizes without considering their side effects yielded with a commonly deployed sparse convolution layer input interface, leading to spatial quantization errors in 3D models. Temporally, existing work has avoided cumbersome data curation by discarding unsynced data, limiting the use to only the small portion of data that is temporally synced across sensors. We analyze these effects and propose simple solutions for each overlooked aspect.</li>
</ul>

<h3>Title: VanGogh: A Unified Multimodal Diffusion-based Framework for Video Colorization</h3>
<ul>
<li><strong>Authors: </strong>Zixun Fang, Zhiheng Liu, Kai Zhu, Yu Liu, Ka Leong Cheng, Wei Zhai, Yang Cao, Zheng-Jun Zha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09499">https://arxiv.org/abs/2501.09499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09499">https://arxiv.org/pdf/2501.09499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09499]] VanGogh: A Unified Multimodal Diffusion-based Framework for Video Colorization(https://arxiv.org/abs/2501.09499)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video colorization aims to transform grayscale videos into vivid color representations while maintaining temporal consistency and structural integrity. Existing video colorization methods often suffer from color bleeding and lack comprehensive control, particularly under complex motion or diverse semantic cues. To this end, we introduce VanGogh, a unified multimodal diffusion-based framework for video colorization. VanGogh tackles these challenges using a Dual Qformer to align and fuse features from multiple modalities, complemented by a depth-guided generation process and an optical flow loss, which help reduce color overflow. Additionally, a color injection strategy and luma channel replacement are implemented to improve generalization and mitigate flickering artifacts. Thanks to this design, users can exercise both global and local control over the generation process, resulting in higher-quality colorized videos. Extensive qualitative and quantitative evaluations, and user studies, demonstrate that VanGogh achieves superior temporal consistency and color this http URL page: this https URL.</li>
</ul>

<h3>Title: Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling for Multimodal Emotion Analysis</h3>
<ul>
<li><strong>Authors: </strong>Qize Yang, Detao Bai, Yi-Xing Peng, Xihan Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09502">https://arxiv.org/abs/2501.09502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09502">https://arxiv.org/pdf/2501.09502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09502]] Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling for Multimodal Emotion Analysis(https://arxiv.org/abs/2501.09502)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding emotions accurately is essential for fields like human-computer interaction. Due to the complexity of emotions and their multi-modal nature (e.g., emotions are influenced by facial expressions and audio), researchers have turned to using multi-modal models to understand human emotions rather than single-modality. However, current video multi-modal large language models (MLLMs) encounter difficulties in effectively integrating audio and identifying subtle facial micro-expressions. Furthermore, the lack of detailed emotion analysis datasets also limits the development of multimodal emotion analysis. To address these issues, we introduce a self-reviewed dataset and a human-reviewed dataset, comprising 24,137 coarse-grained samples and 3,500 manually annotated samples with detailed emotion annotations, respectively. These datasets allow models to learn from diverse scenarios and better generalize to real-world applications. Moreover, in addition to the audio modeling, we propose to explicitly integrate facial encoding models into the existing advanced Video MLLM, enabling the MLLM to effectively unify audio and the subtle facial cues for emotion understanding. By aligning these features within a unified space and employing instruction tuning in our proposed datasets, our Omni-Emotion achieves state-of-the-art performance in both emotion recognition and reasoning tasks.</li>
</ul>

<h3>Title: AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Junjie He, Yuxiang Tuo, Binghui Chen, Chongyang Zhong, Yifeng Geng, Liefeng Bo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09503">https://arxiv.org/abs/2501.09503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09503">https://arxiv.org/pdf/2501.09503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09503]] AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation(https://arxiv.org/abs/2501.09503)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recently, large-scale generative models have demonstrated outstanding text-to-image generation capabilities. However, generating high-fidelity personalized images with specific subjects still presents challenges, especially in cases involving multiple subjects. In this paper, we propose AnyStory, a unified approach for personalized subject generation. AnyStory not only achieves high-fidelity personalization for single subjects, but also for multiple subjects, without sacrificing subject fidelity. Specifically, AnyStory models the subject personalization problem in an "encode-then-route" manner. In the encoding step, AnyStory utilizes a universal and powerful image encoder, i.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve high-fidelity encoding of subject features. In the routing step, AnyStory utilizes a decoupled instance-aware subject router to accurately perceive and predict the potential location of the corresponding subject in the latent space, and guide the injection of subject conditions. Detailed experimental results demonstrate the excellent performance of our method in retaining subject details, aligning text descriptions, and personalizing for multiple subjects. The project page is at this https URL .</li>
</ul>

<h3>Title: HydraMix: Multi-Image Feature Mixing for Small Data Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Christoph Reinders, Frederik Schubert, Bodo Rosenhahn</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09504">https://arxiv.org/abs/2501.09504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09504">https://arxiv.org/pdf/2501.09504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09504]] HydraMix: Multi-Image Feature Mixing for Small Data Image Classification(https://arxiv.org/abs/2501.09504)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair, segmentation</a></li>
<li><strong>Abstract: </strong>Training deep neural networks requires datasets with a large number of annotated examples. The collection and annotation of these datasets is not only extremely expensive but also faces legal and privacy problems. These factors are a significant limitation for many real-world applications. To address this, we introduce HydraMix, a novel architecture that generates new image compositions by mixing multiple different images from the same class. HydraMix learns the fusion of the content of various images guided by a segmentation-based mixing mask in feature space and is optimized via a combination of unsupervised and adversarial training. Our data augmentation scheme allows the creation of models trained from scratch on very small datasets. We conduct extensive experiments on ciFAIR-10, STL-10, and ciFAIR-100. Additionally, we introduce a novel text-image metric to assess the generality of the augmented datasets. Our results show that HydraMix outperforms existing state-of-the-art methods for image classification on small datasets.</li>
</ul>

<h3>Title: Merging Models on the Fly Without Retraining: A Sequential Approach to Scalable Continual Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Anke Tang, Enneng Yang, Li Shen, Yong Luo, Han Hu, Bo Du, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09522">https://arxiv.org/abs/2501.09522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09522">https://arxiv.org/pdf/2501.09522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09522]] Merging Models on the Fly Without Retraining: A Sequential Approach to Scalable Continual Model Merging(https://arxiv.org/abs/2501.09522)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep model merging represents an emerging research direction that combines multiple fine-tuned models to harness their specialized capabilities across different tasks and domains. Current model merging techniques focus on merging all available models simultaneously, with weight interpolation-based methods being the predominant approaches. However, these conventional approaches are not well-suited for scenarios where models become available sequentially, and they often suffer from high memory requirements and potential interference between tasks. In this study, we propose a training-free projection-based continual merging method that processes models sequentially through orthogonal projections of weight matrices and adaptive scaling mechanisms. Our method operates by projecting new parameter updates onto subspaces orthogonal to existing merged parameter updates while using an adaptive scaling mechanism to maintain stable parameter distances, enabling efficient sequential integration of task-specific knowledge. Our approach maintains constant memory complexity to the number of models, minimizes interference between tasks through orthogonal projections, and retains the performance of previously merged models through adaptive task vector scaling. Extensive experiments on CLIP-ViT models demonstrate that our method achieves a 5-8% average accuracy improvement while maintaining robust performance in different task orderings.</li>
</ul>

<h3>Title: Confidence Estimation for Error Detection in Text-to-SQL Systems</h3>
<ul>
<li><strong>Authors: </strong>Oleg Somov, Elena Tutubalina</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09527">https://arxiv.org/abs/2501.09527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09527">https://arxiv.org/pdf/2501.09527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09527]] Confidence Estimation for Error Detection in Text-to-SQL Systems(https://arxiv.org/abs/2501.09527)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Text-to-SQL enables users to interact with databases through natural language, simplifying the retrieval and synthesis of information. Despite the success of large language models (LLMs) in converting natural language questions into SQL queries, their broader adoption is limited by two main challenges: achieving robust generalization across diverse queries and ensuring interpretative confidence in their predictions. To tackle these issues, our research investigates the integration of selective classifiers into Text-to-SQL systems. We analyse the trade-off between coverage and risk using entropy based confidence estimation with selective classifiers and assess its impact on the overall performance of Text-to-SQL models. Additionally, we explore the models' initial calibration and improve it with calibration techniques for better model alignment between confidence and accuracy. Our experimental results show that encoder-decoder T5 is better calibrated than in-context-learning GPT 4 and decoder-only Llama 3, thus the designated external entropy-based selective classifier has better performance. The study also reveal that, in terms of error detection, selective classifier with a higher probability detects errors associated with irrelevant questions rather than incorrect query generations.</li>
</ul>

<h3>Title: Exploring AI-based System Design for Pixel-level Protected Health Information Detection in Medical Images</h3>
<ul>
<li><strong>Authors: </strong>Tuan Truong, Ivo M. Baltruschat, Mark Klemens, Grit Werner, Matthias Lenga</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09552">https://arxiv.org/abs/2501.09552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09552">https://arxiv.org/pdf/2501.09552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09552]] Exploring AI-based System Design for Pixel-level Protected Health Information Detection in Medical Images(https://arxiv.org/abs/2501.09552)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, extraction</a></li>
<li><strong>Abstract: </strong>De-identification of medical images is a critical step to ensure privacy during data sharing in research and clinical settings. The initial step in this process involves detecting Protected Health Information (PHI), which can be found in image metadata or imprinted within image pixels. Despite the importance of such systems, there has been limited evaluation of existing AI-based solutions, creating barriers to the development of reliable and robust tools. In this study, we present an AI-based pipeline for PHI detection, comprising three key components: text detection, text extraction, and analysis of PHI content in medical images. By experimenting with exchanging roles of vision and language models within the pipeline, we evaluate the performance and recommend the best setup for the PHI detection task.</li>
</ul>

<h3>Title: Text-driven Adaptation of Foundation Models for Few-shot Surgical Workflow Analysis</h3>
<ul>
<li><strong>Authors: </strong>Tingxuan Chen, Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09555">https://arxiv.org/abs/2501.09555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09555">https://arxiv.org/pdf/2501.09555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09555]] Text-driven Adaptation of Foundation Models for Few-shot Surgical Workflow Analysis(https://arxiv.org/abs/2501.09555)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Purpose: Surgical workflow analysis is crucial for improving surgical efficiency and safety. However, previous studies rely heavily on large-scale annotated datasets, posing challenges in cost, scalability, and reliance on expert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven Adaptation), designed to handle various surgical workflow analysis tasks with minimal paired image-label data. Methods: Our approach has two key components. First, Few-shot selection-based modality alignment selects a small subset of images and aligns their embeddings with text embeddings from the downstream task, bridging the modality gap. Second, Text-driven adaptation leverages only text data to train a decoder, eliminating the need for paired image-text data. This decoder is then applied to aligned image embeddings, enabling image-related tasks without explicit image-text pairs. Results: We evaluate our approach to generative tasks (image captioning) and discriminative tasks (triplet recognition and phase recognition). Results show that Surg-FTDA outperforms baselines and generalizes well across downstream tasks. Conclusion: We propose a text-driven adaptation approach that mitigates the modality gap and handles multiple downstream tasks in surgical workflow analysis, with minimal reliance on large annotated datasets. The code and dataset will be released in this https URL.</li>
</ul>

<h3>Title: From Scarcity to Capability: Empowering Fake News Detection in Low-Resource Languages with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hrithik Majumdar Shibu, Shrestha Datta, Md. Sumon Miah, Nasrullah Sami, Mahruba Sharmin Chowdhury, Md. Saiful Islam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09604">https://arxiv.org/abs/2501.09604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09604">https://arxiv.org/pdf/2501.09604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09604]] From Scarcity to Capability: Empowering Fake News Detection in Low-Resource Languages with LLMs(https://arxiv.org/abs/2501.09604)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>The rapid spread of fake news presents a significant global challenge, particularly in low-resource languages like Bangla, which lack adequate datasets and detection tools. Although manual fact-checking is accurate, it is expensive and slow to prevent the dissemination of fake news. Addressing this gap, we introduce BanFakeNews-2.0, a robust dataset to enhance Bangla fake news detection. This version includes 11,700 additional, meticulously curated fake news articles validated from credible sources, creating a proportional dataset of 47,000 authentic and 13,000 fake news items across 13 categories. In addition, we created a manually curated independent test set of 460 fake and 540 authentic news items for rigorous evaluation. We invest efforts in collecting fake news from credible sources and manually verified while preserving the linguistic richness. We develop a benchmark system utilizing transformer-based architectures, including fine-tuned Bidirectional Encoder Representations from Transformers variants (F1-87\%) and Large Language Models with Quantized Low-Rank Approximation (F1-89\%), that significantly outperforms traditional methods. BanFakeNews-2.0 offers a valuable resource to advance research and application in fake news detection for low-resourced languages. We publicly release our dataset and model on Github to foster research in this direction.</li>
</ul>

<h3>Title: Adversarial-Ensemble Kolmogorov Arnold Networks for Enhancing Indoor Wi-Fi Positioning: A Defensive Approach Against Spoofing and Signal Manipulation Attacks</h3>
<ul>
<li><strong>Authors: </strong>Mitul Goswami, Romit Chatterjee, Somnath Mahato, Prasant Kumar Pattnaik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09609">https://arxiv.org/abs/2501.09609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09609">https://arxiv.org/pdf/2501.09609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09609]] Adversarial-Ensemble Kolmogorov Arnold Networks for Enhancing Indoor Wi-Fi Positioning: A Defensive Approach Against Spoofing and Signal Manipulation Attacks(https://arxiv.org/abs/2501.09609)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>The research presents a study on enhancing the robustness of Wi-Fi-based indoor positioning systems against adversarial attacks. The goal is to improve the positioning accuracy and resilience of these systems under two attack scenarios: Wi-Fi Spoofing and Signal Strength Manipulation. Three models are developed and evaluated: a baseline model (M_Base), an adversarially trained robust model (M_Rob), and an ensemble model (M_Ens). All models utilize a Kolmogorov-Arnold Network (KAN) architecture. The robust model is trained with adversarially perturbed data, while the ensemble model combines predictions from both the base and robust models. Experimental results show that the robust model reduces positioning error by approximately 10% compared to the baseline, achieving 2.03 meters error under Wi-Fi spoofing and 2.00 meters under signal strength manipulation. The ensemble model further outperforms with errors of 2.01 meters and 1.975 meters for the respective attack types. This analysis highlights the effectiveness of adversarial training techniques in mitigating attack impacts. The findings underscore the importance of considering adversarial scenarios in developing indoor positioning systems, as improved resilience can significantly enhance the accuracy and reliability of such systems in mission-critical environments.</li>
</ul>

<h3>Title: WMamba: Wavelet-based Mamba for Face Forgery Detection</h3>
<ul>
<li><strong>Authors: </strong>Siran Peng, Tianshuo Zhang, Li Gao, Xiangyu Zhu, Haoyuan Zhang, Kai Pang, Zhen Lei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09617">https://arxiv.org/abs/2501.09617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09617">https://arxiv.org/pdf/2501.09617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09617]] WMamba: Wavelet-based Mamba for Face Forgery Detection(https://arxiv.org/abs/2501.09617)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of deepfake generation technologies, the demand for robust and accurate face forgery detection algorithms has become increasingly critical. Recent studies have demonstrated that wavelet analysis can uncover subtle forgery artifacts that remain imperceptible in the spatial domain. Wavelets effectively capture important facial contours, which are often slender, fine-grained, and global in nature. However, existing wavelet-based approaches fail to fully leverage these unique characteristics, resulting in sub-optimal feature extraction and limited generalizability. To address this challenge, we introduce WMamba, a novel wavelet-based feature extractor built upon the Mamba architecture. WMamba maximizes the utility of wavelet information through two key innovations. First, we propose Dynamic Contour Convolution (DCConv), which employs specially crafted deformable kernels to adaptively model slender facial contours. Second, by leveraging the Mamba architecture, our method captures long-range spatial relationships with linear computational complexity. This efficiency allows for the extraction of fine-grained, global forgery artifacts from small image patches. Extensive experimental results show that WMamba achieves state-of-the-art (SOTA) performance, highlighting its effectiveness and superiority in face forgery detection.</li>
</ul>

<h3>Title: Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Chaoqi Wang, Zhuokai Zhao, Yibo Jiang, Zhaorun Chen, Chen Zhu, Yuxin Chen, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Hao Ma, Sinong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09620">https://arxiv.org/abs/2501.09620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09620">https://arxiv.org/pdf/2501.09620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09620]] Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment(https://arxiv.org/abs/2501.09620)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have demonstrated significant progress in performing complex tasks. While Reinforcement Learning from Human Feedback (RLHF) has been effective in aligning LLMs with human preferences, it is susceptible to spurious correlations in reward modeling. Consequently, it often introduces biases-such as length bias, sycophancy, conceptual bias, and discrimination that hinder the model's ability to capture true causal relationships. To address this, we propose a novel causal reward modeling approach that integrates causal inference to mitigate these spurious correlations. Our method enforces counterfactual invariance, ensuring reward predictions remain consistent when irrelevant variables are altered. Through experiments on both synthetic and real-world datasets, we show that our approach mitigates various types of spurious correlations effectively, resulting in more reliable and fair alignment of LLMs with human preferences. As a drop-in enhancement to the existing RLHF workflow, our causal reward modeling provides a practical way to improve the trustworthiness and fairness of LLM finetuning.</li>
</ul>

<h3>Title: Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML</h3>
<ul>
<li><strong>Authors: </strong>Tehila Dahan, Kfir Y. Levy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09621">https://arxiv.org/abs/2501.09621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09621">https://arxiv.org/pdf/2501.09621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09621]] Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML(https://arxiv.org/abs/2501.09621)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We address the challenges of Byzantine-robust training in asynchronous distributed machine learning systems, aiming to enhance efficiency amid massive parallelization and heterogeneous computing resources. Asynchronous systems, marked by independently operating workers and intermittent updates, uniquely struggle with maintaining integrity against Byzantine failures, which encompass malicious or erroneous actions that disrupt learning. The inherent delays in such settings not only introduce additional bias to the system but also obscure the disruptions caused by Byzantine faults. To tackle these issues, we adapt the Byzantine framework to asynchronous dynamics by introducing a novel weighted robust aggregation framework. This allows for the extension of robust aggregators and a recent meta-aggregator to their weighted versions, mitigating the effects of delayed updates. By further incorporating a recent variance-reduction technique, we achieve an optimal convergence rate for the first time in an asynchronous Byzantine environment. Our methodology is rigorously validated through empirical and theoretical analysis, demonstrating its effectiveness in enhancing fault tolerance and optimizing performance in asynchronous ML systems.</li>
</ul>

<h3>Title: Empowering Large Language Models in Wireless Communication: A Novel Dataset and Fine-Tuning Framework</h3>
<ul>
<li><strong>Authors: </strong>Yushen Lin, Ruichen Zhang, Wenqi Huang, Kaidi Wang, Zhiguo Ding, Daniel K. C. So, Dusit Niyato</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09631">https://arxiv.org/abs/2501.09631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09631">https://arxiv.org/pdf/2501.09631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09631]] Empowering Large Language Models in Wireless Communication: A Novel Dataset and Fine-Tuning Framework(https://arxiv.org/abs/2501.09631)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>In this work, we develop a specialized dataset aimed at enhancing the evaluation and fine-tuning of large language models (LLMs) specifically for wireless communication applications. The dataset includes a diverse set of multi-hop questions, including true/false and multiple-choice types, spanning varying difficulty levels from easy to hard. By utilizing advanced language models for entity extraction and question generation, rigorous data curation processes are employed to maintain high quality and relevance. Additionally, we introduce a Pointwise V-Information (PVI) based fine-tuning method, providing a detailed theoretical analysis and justification for its use in quantifying the information content of training data with 2.24\% and 1.31\% performance boost for different models compared to baselines, respectively. To demonstrate the effectiveness of the fine-tuned models with the proposed methodologies on practical tasks, we also consider different tasks, including summarizing optimization problems from technical papers and solving the mathematical problems related to non-orthogonal multiple access (NOMA), which are generated by using the proposed multi-agent framework. Simulation results show significant performance gain in summarization tasks with 20.9\% in the ROUGE-L metrics. We also study the scaling laws of fine-tuning LLMs and the challenges LLMs face in the field of wireless communications, offering insights into their adaptation to wireless communication tasks. This dataset and fine-tuning methodology aim to enhance the training and evaluation of LLMs, contributing to advancements in LLMs for wireless communication research and applications.</li>
</ul>

<h3>Title: Unified Face Matching and Physical-Digital Spoofing Attack Detection</h3>
<ul>
<li><strong>Authors: </strong>Arun Kunwar, Ajita Rattani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09635">https://arxiv.org/abs/2501.09635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09635">https://arxiv.org/pdf/2501.09635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09635]] Unified Face Matching and Physical-Digital Spoofing Attack Detection(https://arxiv.org/abs/2501.09635)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, biometric, transformer</a></li>
<li><strong>Abstract: </strong>Face recognition technology has dramatically transformed the landscape of security, surveillance, and authentication systems, offering a user-friendly and non-invasive biometric solution. However, despite its significant advantages, face recognition systems face increasing threats from physical and digital spoofing attacks. Current research typically treats face recognition and attack detection as distinct classification challenges. This approach necessitates the implementation of separate models for each task, leading to considerable computational complexity, particularly on devices with limited resources. Such inefficiencies can stifle scalability and hinder performance. In response to these challenges, this paper introduces an innovative unified model designed for face recognition and detection of physical and digital attacks. By leveraging the advanced Swin Transformer backbone and incorporating HiLo attention in a convolutional neural network framework, we address unified face recognition and spoof attack detection more effectively. Moreover, we introduce augmentation techniques that replicate the traits of physical and digital spoofing cues, significantly enhancing our model robustness. Through comprehensive experimental evaluation across various datasets, we showcase the effectiveness of our model in unified face recognition and spoof detection. Additionally, we confirm its resilience against unseen physical and digital spoofing attacks, underscoring its potential for real-world applications.</li>
</ul>

<h3>Title: LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading</h3>
<ul>
<li><strong>Authors: </strong>Kuan-Ming Liu (1), Ming-Chih Lo (2) ((1) National Chengchi University, College of Commerce, (2) National Yang Ming Chiao Tung University, College of Computer Science)</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.TR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09636">https://arxiv.org/abs/2501.09636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09636">https://arxiv.org/pdf/2501.09636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09636]] LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading(https://arxiv.org/abs/2501.09636)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in deep learning and large language models (LLMs) have facilitated the deployment of the mixture-of-experts (MoE) mechanism in the stock investment domain. While these models have demonstrated promising trading performance, they are often unimodal, neglecting the wealth of information available in other modalities, such as textual data. Moreover, the traditional neural network-based router selection mechanism fails to consider contextual and real-world nuances, resulting in suboptimal expert selection. To address these limitations, we propose LLMoE, a novel framework that employs LLMs as the router within the MoE architecture. Specifically, we replace the conventional neural network-based router with LLMs, leveraging their extensive world knowledge and reasoning capabilities to select experts based on historical price data and stock news. This approach provides a more effective and interpretable selection mechanism. Our experiments on multimodal real-world stock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models and other deep neural network approaches. Additionally, the flexible architecture of LLMoE allows for easy adaptation to various downstream tasks.</li>
</ul>

<h3>Title: The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Katzy, Razvan Mihai Popescu, Arie van Deursen, Maliheh Izadi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09653">https://arxiv.org/abs/2501.09653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09653">https://arxiv.org/pdf/2501.09653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09653]] The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large Language Models(https://arxiv.org/abs/2501.09653)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The recent rise in the popularity of large language models has spurred the development of extensive code datasets needed to train them. This has left limited code available for collection and use in the downstream investigation of specific behaviors, or evaluation of large language models without suffering from data contamination. To address this problem, we release The Heap, a large multilingual dataset covering 57 programming languages that has been deduplicated with respect to other open datasets of code, enabling researchers to conduct fair evaluations of large language models without significant data cleaning overhead.</li>
</ul>

<h3>Title: A Survey of Research in Large Language Models for Electronic Design Automation</h3>
<ul>
<li><strong>Authors: </strong>Jingyu Pan, Guanglei Zhou, Chen-Chia Chang, Isaac Jacobson, Jiang Hu, Yiran Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09655">https://arxiv.org/abs/2501.09655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09655">https://arxiv.org/pdf/2501.09655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09655]] A Survey of Research in Large Language Models for Electronic Design Automation(https://arxiv.org/abs/2501.09655)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Within the rapidly evolving domain of Electronic Design Automation (EDA), Large Language Models (LLMs) have emerged as transformative technologies, offering unprecedented capabilities for optimizing and automating various aspects of electronic design. This survey provides a comprehensive exploration of LLM applications in EDA, focusing on advancements in model architectures, the implications of varying model sizes, and innovative customization techniques that enable tailored analytical insights. By examining the intersection of LLM capabilities and EDA requirements, the paper highlights the significant impact these models have on extracting nuanced understandings from complex datasets. Furthermore, it addresses the challenges and opportunities in integrating LLMs into EDA workflows, paving the way for future research and application in this dynamic field. Through this detailed analysis, the survey aims to offer valuable insights to professionals in the EDA industry, AI researchers, and anyone interested in the convergence of advanced AI technologies and electronic design.</li>
</ul>

<h3>Title: Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP Evaluation Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Alexis Roger, Prateek Humane, Daniel Z. Kaplan, Kshitij Gupta, Qi Sun, George Adamopoulos, Jonathan Siu Chi Lim, Quentin Anthony, Edwin Fennell, Irina Rish</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09672">https://arxiv.org/abs/2501.09672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09672">https://arxiv.org/pdf/2501.09672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09672]] Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP Evaluation Benchmark(https://arxiv.org/abs/2501.09672)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of Vision-Language Models (VLMs) in the past several years calls for rigorous and comprehensive evaluation methods and benchmarks. This work analyzes existing VLM evaluation techniques, including automated metrics, AI-based assessments, and human evaluations across diverse tasks. We first introduce Robin - a novel suite of VLMs that we built by combining Large Language Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use Robin to identify shortcomings of current evaluation approaches across scales. Next, to overcome the identified limitations, we introduce CHIRP - a new long form response benchmark we developed for more robust and complete VLM evaluation. We provide open access to the Robin training code, model suite, and CHIRP benchmark to promote reproducibility and advance VLM research.</li>
</ul>

<h3>Title: U-Fair: Uncertainty-based Multimodal Multitask Learning for Fairer Depression Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiaee Cheong, Aditya Bangar, Sinan Kalkan, Hatice Gunes</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09687">https://arxiv.org/abs/2501.09687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09687">https://arxiv.org/pdf/2501.09687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09687]] U-Fair: Uncertainty-based Multimodal Multitask Learning for Fairer Depression Detection(https://arxiv.org/abs/2501.09687)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Machine learning bias in mental health is becoming an increasingly pertinent challenge. Despite promising efforts indicating that multitask approaches often work better than unitask approaches, there is minimal work investigat- ing the impact of multitask learning on performance and fairness in depression detection nor leveraged it to achieve fairer prediction outcomes. In this work, we undertake a systematic investigation of using a multitask approach to improve performance and fairness for depression detection. We propose a novel gender-based task-reweighting method using uncertainty grounded in how the PHQ-8 questionnaire is structured. Our results indicate that, although a multitask approach improves performance and fairness compared to a unitask approach, the results are not always consistent and we see evidence of negative transfer and a reduction in the Pareto frontier, which is concerning given the high-stake healthcare setting. Our proposed approach of gender-based reweighting with uncertainty improves performance and fairness and alleviates both challenges to a certain extent. Our findings on each PHQ-8 subitem task difficulty are also in agreement with the largest study conducted on the PHQ-8 subitem discrimination capacity, thus providing the very first tangible evidence linking ML findings with large-scale empirical population studies conducted on the PHQ-8.</li>
</ul>

<h3>Title: Fine-Grained Image-Text Correspondence with Cost Aggregation for Open-Vocabulary Part Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jiho Choi, Seonho Lee, Minhyun Lee, Seungho Lee, Hyunjung Shim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09688">https://arxiv.org/abs/2501.09688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09688">https://arxiv.org/pdf/2501.09688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09688]] Fine-Grained Image-Text Correspondence with Cost Aggregation for Open-Vocabulary Part Segmentation(https://arxiv.org/abs/2501.09688)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Open-Vocabulary Part Segmentation (OVPS) is an emerging field for recognizing fine-grained parts in unseen categories. We identify two primary challenges in OVPS: (1) the difficulty in aligning part-level image-text correspondence, and (2) the lack of structural understanding in segmenting object parts. To address these issues, we propose PartCATSeg, a novel framework that integrates object-aware part-level cost aggregation, compositional loss, and structural guidance from DINO. Our approach employs a disentangled cost aggregation strategy that handles object and part-level costs separately, enhancing the precision of part-level segmentation. We also introduce a compositional loss to better capture part-object relationships, compensating for the limited part annotations. Additionally, structural guidance from DINO features improves boundary delineation and inter-part understanding. Extensive experiments on Pascal-Part-116, ADE20K-Part-234, and PartImageNet datasets demonstrate that our method significantly outperforms state-of-the-art approaches, setting a new baseline for robust generalization to unseen part categories.</li>
</ul>

<h3>Title: Cueless EEG imagined speech for subject identification: dataset and benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Ali Derakhshesh, Zahra Dehghanian, Reza Ebrahimpour, Hamid R. Rabiee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09700">https://arxiv.org/abs/2501.09700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09700">https://arxiv.org/pdf/2501.09700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09700]] Cueless EEG imagined speech for subject identification: dataset and benchmarks(https://arxiv.org/abs/2501.09700)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, biometric</a></li>
<li><strong>Abstract: </strong>Electroencephalogram (EEG) signals have emerged as a promising modality for biometric identification. While previous studies have explored the use of imagined speech with semantically meaningful words for subject identification, most have relied on additional visual or auditory cues. In this study, we introduce a cueless EEG-based imagined speech paradigm, where subjects imagine the pronunciation of semantically meaningful words without any external cues. This innovative approach addresses the limitations of prior methods by requiring subjects to select and imagine words from a predefined list naturally. The dataset comprises over 4,350 trials from 11 subjects across five sessions. We assess a variety of classification methods, including traditional machine learning techniques such as Support Vector Machines (SVM) and XGBoost, as well as time-series foundation models and deep learning architectures specifically designed for EEG classification, such as EEG Conformer and Shallow ConvNet. A session-based hold-out validation strategy was employed to ensure reliable evaluation and prevent data leakage. Our results demonstrate outstanding classification accuracy, reaching 97.93%. These findings highlight the potential of cueless EEG paradigms for secure and reliable subject identification in real-world applications, such as brain-computer interfaces (BCIs).</li>
</ul>

<h3>Title: Practical Continual Forgetting for Pre-trained Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Hongbo Zhao, Fei Zhu, Bolin Ni, Feng Zhu, Gaofeng Meng, Zhaoxiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09705">https://arxiv.org/abs/2501.09705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09705">https://arxiv.org/pdf/2501.09705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09705]] Practical Continual Forgetting for Pre-trained Vision Models(https://arxiv.org/abs/2501.09705)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, transformer</a></li>
<li><strong>Abstract: </strong>For privacy and security concerns, the need to erase unwanted information from pre-trained vision models is becoming evident nowadays. In real-world scenarios, erasure requests originate at any time from both users and model owners, and these requests usually form a sequence. Therefore, under such a setting, selective information is expected to be continuously removed from a pre-trained model while maintaining the rest. We define this problem as continual forgetting and identify three key challenges. (i) For unwanted knowledge, efficient and effective deleting is crucial. (ii) For remaining knowledge, the impact brought by the forgetting procedure should be minimal. (iii) In real-world scenarios, the training samples may be scarce or partially missing during the process of forgetting. To address them, we first propose Group Sparse LoRA (GS-LoRA). Specifically, towards (i), we introduce LoRA modules to fine-tune the FFN layers in Transformer blocks for each forgetting task independently, and towards (ii), a simple group sparse regularization is adopted, enabling automatic selection of specific LoRA groups and zeroing out the others. To further extend GS-LoRA to more practical scenarios, we incorporate prototype information as additional supervision and introduce a more practical approach, GS-LoRA++. For each forgotten class, we move the logits away from its original prototype. For the remaining classes, we pull the logits closer to their respective prototypes. We conduct extensive experiments on face recognition, object detection and image classification and demonstrate that our method manages to forget specific classes with minimal impact on other classes. Codes have been released on this https URL.</li>
</ul>

<h3>Title: Domain Adaptation of Foundation LLMs for e-Commerce</h3>
<ul>
<li><strong>Authors: </strong>Christian Herold, Michael Kozielski, Tala Bazazo, Pavel Petrushkov, Hadi Hashemi, Patrycja Cieplicka, Dominika Basaj, Shahram Khadivi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09706">https://arxiv.org/abs/2501.09706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09706">https://arxiv.org/pdf/2501.09706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09706]] Domain Adaptation of Foundation LLMs for e-Commerce(https://arxiv.org/abs/2501.09706)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present the e-Llama models: 8 billion and 70 billion parameter large language models that are adapted towards the e-commerce domain. These models are meant as foundation models with deep knowledge about e-commerce, that form a base for instruction- and fine-tuning. The e-Llama models are obtained by continuously pretraining the Llama 3.1 base models on 1 trillion tokens of domain-specific data. We discuss our approach and motivate our choice of hyperparameters with a series of ablation studies. To quantify how well the models have been adapted to the e-commerce domain, we define and implement a set of multilingual, e-commerce specific evaluation tasks. We show that, when carefully choosing the training setup, the Llama 3.1 models can be adapted towards the new domain without sacrificing significant performance on general domain tasks. We also explore the possibility of merging the adapted model and the base model for a better control of the performance trade-off between domains.</li>
</ul>

<h3>Title: FLOL: Fast Baselines for Real-World Low-Light Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Juan C. Benito, Daniel Feijoo, Alvaro Garcia, Marcos V. Conde</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09718">https://arxiv.org/abs/2501.09718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09718">https://arxiv.org/pdf/2501.09718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09718]] FLOL: Fast Baselines for Real-World Low-Light Enhancement(https://arxiv.org/abs/2501.09718)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Low-Light Image Enhancement (LLIE) is a key task in computational photography and imaging. The problem of enhancing images captured during night or in dark environments has been well-studied in the image signal processing literature. However, current deep learning-based solutions struggle with efficiency and robustness in real-world scenarios (e.g. scenes with noise, saturated pixels, bad illumination). We propose a lightweight neural network that combines image processing in the frequency and spatial domains. Our method, FLOL+, is one of the fastest models for this task, achieving state-of-the-art results on popular real scenes datasets such as LOL and LSRW. Moreover, we are able to process 1080p images under 12ms. Code and models at this https URL</li>
</ul>

<h3>Title: Comparative Insights from 12 Machine Learning Models in Extracting Economic Ideology from Political Text</h3>
<ul>
<li><strong>Authors: </strong>Jihed Ncib</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09719">https://arxiv.org/abs/2501.09719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09719">https://arxiv.org/pdf/2501.09719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09719]] Comparative Insights from 12 Machine Learning Models in Extracting Economic Ideology from Political Text(https://arxiv.org/abs/2501.09719)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study conducts a systematic assessment of the capabilities of 12 machine learning models and model variations in detecting economic ideology. As an evaluation benchmark, I use manifesto data spanning six elections in the United Kingdom and pre-annotated by expert and crowd coders. The analysis assesses the performance of several generative, fine-tuned, and zero-shot models at the granular and aggregate levels. The results show that generative models such as GPT-4o and Gemini 1.5 Flash consistently outperform other models against all benchmarks. However, they pose issues of accessibility and resource availability. Fine-tuning yielded competitive performance and offers a reliable alternative through domain-specific optimization. But its dependency on training data severely limits scalability. Zero-shot models consistently face difficulties with identifying signals of economic ideology, often resulting in negative associations with human coding. Using general knowledge for the domain-specific task of ideology scaling proved to be unreliable. Other key findings include considerable within-party variation, fine-tuning benefiting from larger training data, and zero-shot's sensitivity to prompt content. The assessments include the strengths and limitations of each model and derive best-practices for automated analyses of political content.</li>
</ul>

<h3>Title: A Simple Aerial Detection Baseline of Multimodal Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qingyun Li, Yushi Chen, Xinya Shu, Dong Chen, Xin He, Yi Yu, Xue Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09720">https://arxiv.org/abs/2501.09720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09720">https://arxiv.org/pdf/2501.09720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09720]] A Simple Aerial Detection Baseline of Multimodal Language Models(https://arxiv.org/abs/2501.09720)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer, generative</a></li>
<li><strong>Abstract: </strong>The multimodal language models (MLMs) based on generative pre-trained Transformer are considered powerful candidates for unifying various domains and tasks. MLMs developed for remote sensing (RS) have demonstrated outstanding performance in multiple tasks, such as visual question answering and visual grounding. In addition to visual grounding that detects specific objects corresponded to given instruction, aerial detection, which detects all objects of multiple categories, is also a valuable and challenging task for RS foundation models. However, aerial detection has not been explored by existing RS MLMs because the autoregressive prediction mechanism of MLMs differs significantly from the detection outputs. In this paper, we present a simple baseline for applying MLMs to aerial detection for the first time, named LMMRotate. Specifically, we first introduce a normalization method to transform detection outputs into textual outputs to be compatible with the MLM framework. Then, we propose a evaluation method, which ensures a fair comparison between MLMs and conventional object detection models. We construct the baseline by fine-tuning open-source general-purpose MLMs and achieve impressive detection performance comparable to conventional detector. We hope that this baseline will serve as a reference for future MLM development, enabling more comprehensive capabilities for understanding RS images. Code is available at this https URL.</li>
</ul>

<h3>Title: Generating particle physics Lagrangians with transformers</h3>
<ul>
<li><strong>Authors: </strong>Yong Sheng Koay, Rikard Enberg, Stefano Moretti, Eliel Camargo-Molina</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SC, hep-ph, hep-th</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09729">https://arxiv.org/abs/2501.09729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09729">https://arxiv.org/pdf/2501.09729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09729]] Generating particle physics Lagrangians with transformers(https://arxiv.org/abs/2501.09729)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In physics, Lagrangians provide a systematic way to describe laws governing physical systems. In the context of particle physics, they encode the interactions and behavior of the fundamental building blocks of our universe. By treating Lagrangians as complex, rule-based constructs similar to linguistic expressions, we trained a transformer model -- proven to be effective in natural language tasks -- to predict the Lagrangian corresponding to a given list of particles. We report on the transformer's performance in constructing Lagrangians respecting the Standard Model $\mathrm{SU}(3)\times \mathrm{SU}(2)\times \mathrm{U}(1)$ gauge symmetries. The resulting model is shown to achieve high accuracies (over 90\%) with Lagrangians up to six matter fields, with the capacity to generalize beyond the training distribution, albeit within architectural constraints. We show through an analysis of input embeddings that the model has internalized concepts such as group representations and conjugation operations as it learned to generate Lagrangians. We make the model and training datasets available to the community. An interactive demonstration can be found at: \url{this https URL}.</li>
</ul>

<h3>Title: Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps</h3>
<ul>
<li><strong>Authors: </strong>Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, Saining Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09732">https://arxiv.org/abs/2501.09732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09732">https://arxiv.org/pdf/2501.09732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09732]] Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps(https://arxiv.org/abs/2501.09732)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.</li>
</ul>

<h3>Title: Enhancing Lexicon-Based Text Embeddings with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yibin Lei, Tao Shen, Yu Cao, Andrew Yates</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09749">https://arxiv.org/abs/2501.09749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09749">https://arxiv.org/pdf/2501.09749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09749]] Enhancing Lexicon-Based Text Embeddings with Large Language Models(https://arxiv.org/abs/2501.09749)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent large language models (LLMs) have demonstrated exceptional performance on general-purpose text embedding tasks. While dense embeddings have dominated related research, we introduce the first Lexicon-based EmbeddiNgS (LENS) leveraging LLMs that achieve competitive performance on these tasks. Regarding the inherent tokenization redundancy issue and unidirectional attention limitations in traditional causal LLMs, LENS consolidates the vocabulary space through token embedding clustering, and investigates bidirectional attention and various pooling strategies. Specifically, LENS simplifies lexicon matching by assigning each dimension to a specific token cluster, where semantically similar tokens are grouped together, and unlocking the full potential of LLMs through bidirectional attention. Extensive experiments demonstrate that LENS outperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB), delivering compact feature representations that match the sizes of dense counterparts. Notably, combining LENSE with dense embeddings achieves state-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).</li>
</ul>

<h3>Title: OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking</h3>
<ul>
<li><strong>Authors: </strong>Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Jiang Yong, Pengjun Xie, Fei Huang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09751">https://arxiv.org/abs/2501.09751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09751">https://arxiv.org/pdf/2501.09751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09751]] OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking(https://arxiv.org/abs/2501.09751)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, utility, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, repetitive, and unoriginal outputs. To address these issues, we propose OmniThink, a machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they progressively deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles.</li>
</ul>

<h3>Title: Lost in Translation, Found in Context: Sign Language Translation with Contextual Cues</h3>
<ul>
<li><strong>Authors: </strong>Youngjoon Jang, Haran Raajesh, Liliane Momeni, Gül Varol, Andrew Zisserman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09754">https://arxiv.org/abs/2501.09754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09754">https://arxiv.org/pdf/2501.09754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09754]] Lost in Translation, Found in Context: Sign Language Translation with Contextual Cues(https://arxiv.org/abs/2501.09754)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Our objective is to translate continuous sign language into spoken language text. Inspired by the way human interpreters rely on context for accurate translation, we incorporate additional contextual cues together with the signing video, into a new translation framework. Specifically, besides visual sign recognition features that encode the input video, we integrate complementary textual information from (i) captions describing the background show, (ii) translation of previous sentences, as well as (iii) pseudo-glosses transcribing the signing. These are automatically extracted and inputted along with the visual features to a pre-trained large language model (LLM), which we fine-tune to generate spoken language translations in text form. Through extensive ablation studies, we show the positive contribution of each input cue to the translation performance. We train and evaluate our approach on BOBSL -- the largest British Sign Language dataset currently available. We show that our contextual approach significantly enhances the quality of the translations compared to previously reported results on BOBSL, and also to state-of-the-art methods that we implement as baselines. Furthermore, we demonstrate the generality of our approach by applying it also to How2Sign, an American Sign Language dataset, and achieve competitive results.</li>
</ul>

<h3>Title: Learnings from Scaling Visual Tokenizers for Reconstruction and Generation</h3>
<ul>
<li><strong>Authors: </strong>Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, Xinlei Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09755">https://arxiv.org/abs/2501.09755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09755">https://arxiv.org/pdf/2501.09755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09755]] Learnings from Scaling Visual Tokenizers for Reconstruction and Generation(https://arxiv.org/abs/2501.09755)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Visual tokenization via auto-encoding empowers state-of-the-art image and video generative models by compressing pixels into a latent space. Although scaling Transformer-based generators has been central to recent advances, the tokenizer component itself is rarely scaled, leaving open questions about how auto-encoder design choices influence both its objective of reconstruction and downstream generative performance. Our work aims to conduct an exploration of scaling in auto-encoders to fill in this blank. To facilitate this exploration, we replace the typical convolutional backbone with an enhanced Vision Transformer architecture for Tokenization (ViTok). We train ViTok on large-scale image and video datasets far exceeding ImageNet-1K, removing data constraints on tokenizer scaling. We first study how scaling the auto-encoder bottleneck affects both reconstruction and generation -- and find that while it is highly correlated with reconstruction, its relationship with generation is more complex. We next explored the effect of separately scaling the auto-encoders' encoder and decoder on reconstruction and generation performance. Crucially, we find that scaling the encoder yields minimal gains for either reconstruction or generation, while scaling the decoder boosts reconstruction but the benefits for generation are mixed. Building on our exploration, we design ViTok as a lightweight auto-encoder that achieves competitive performance with state-of-the-art auto-encoders on ImageNet-1K and COCO reconstruction tasks (256p and 512p) while outperforming existing auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates competitive performance on image generation for ImageNet-1K and sets new state-of-the-art benchmarks for class-conditional video generation on UCF-101.</li>
</ul>

<h3>Title: SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces</h3>
<ul>
<li><strong>Authors: </strong>Sumit Chaturvedi, Mengwei Ren, Yannick Hold-Geoffroy, Jingyuan Liu, Julie Dorsey, Zhixin Shu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09756">https://arxiv.org/abs/2501.09756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09756">https://arxiv.org/pdf/2501.09756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09756]] SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces(https://arxiv.org/abs/2501.09756)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce SynthLight, a diffusion model for portrait relighting. Our approach frames image relighting as a re-rendering problem, where pixels are transformed in response to changes in environmental lighting conditions. Using a physically-based rendering engine, we synthesize a dataset to simulate this lighting-conditioned transformation with 3D head assets under varying lighting. We propose two training and inference strategies to bridge the gap between the synthetic and real image domains: (1) multi-task training that takes advantage of real human portraits without lighting labels; (2) an inference time diffusion sampling procedure based on classifier-free guidance that leverages the input portrait to better preserve details. Our method generalizes to diverse real photographs and produces realistic illumination effects, including specular highlights and cast shadows, while preserving the subject's identity. Our quantitative experiments on Light Stage data demonstrate results comparable to state-of-the-art relighting methods. Our qualitative results on in-the-wild images showcase rich and unprecedented illumination effects. Project Page: \url{this https URL}</li>
</ul>

<h3>Title: Distilling Multi-modal Large Language Models for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Deepti Hegde, Rajeev Yasarla, Hong Cai, Shizhong Han, Apratim Bhattacharyya, Shweta Mahajan, Litian Liu, Risheek Garrepalli, Vishal M. Patel, Fatih Porikli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09757">https://arxiv.org/abs/2501.09757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09757">https://arxiv.org/pdf/2501.09757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09757]] Distilling Multi-modal Large Language Models for Autonomous Driving(https://arxiv.org/abs/2501.09757)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Autonomous driving demands safe motion planning, especially in critical "long-tail" scenarios. Recent end-to-end autonomous driving systems leverage large language models (LLMs) as planners to improve generalizability to rare events. However, using LLMs at test time introduces high computational costs. To address this, we propose DiMA, an end-to-end autonomous driving system that maintains the efficiency of an LLM-free (or vision-based) planner while leveraging the world knowledge of an LLM. DiMA distills the information from a multi-modal LLM to a vision-based end-to-end planner through a set of specially designed surrogate tasks. Under a joint training strategy, a scene encoder common to both networks produces structured representations that are semantically grounded as well as aligned to the final planning objective. Notably, the LLM is optional at inference, enabling robust planning without compromising on efficiency. Training with DiMA results in a 37% reduction in the L2 trajectory error and an 80% reduction in the collision rate of the vision-based planner, as well as a 44% trajectory error reduction in longtail scenarios. DiMA also achieves state-of-the-art performance on the nuScenes planning benchmark.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
