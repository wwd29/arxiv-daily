<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-03-28</h1>
<h3>Title: Sort & Slice: A Simple and Superior Alternative to Hash-Based Folding  for Extended-Connectivity Fingerprints</h3>
<ul>
<li><strong>Authors: </strong>Markus Dablander, Thierry Hanser, Renaud Lambiotte, Garrett M. Morris</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.chem-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17954">https://arxiv.org/abs/2403.17954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17954">https://arxiv.org/pdf/2403.17954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17954]] Sort & Slice: A Simple and Superior Alternative to Hash-Based Folding  for Extended-Connectivity Fingerprints(https://arxiv.org/abs/2403.17954)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Extended-connectivity fingerprints (ECFPs) are a ubiquitous tool in current cheminformatics and molecular machine learning, and one of the most prevalent molecular feature extraction techniques used for chemical prediction. Atom features learned by graph neural networks can be aggregated to compound-level representations using a large spectrum of graph pooling methods; in contrast, sets of detected ECFP substructures are by default transformed into bit vectors using only a simple hash-based folding procedure. We introduce a general mathematical framework for the vectorisation of structural fingerprints via a formal operation called substructure pooling that encompasses hash-based folding, algorithmic substructure-selection, and a wide variety of other potential techniques. We go on to describe Sort & Slice, an easy-to-implement and bit-collision-free alternative to hash-based folding for the pooling of ECFP substructures. Sort & Slice first sorts ECFP substructures according to their relative prevalence in a given set of training compounds and then slices away all but the $L$ most frequent substructures which are subsequently used to generate a binary fingerprint of desired length, $L$. We computationally compare the performance of hash-based folding, Sort & Slice, and two advanced supervised substructure-selection schemes (filtering and mutual-information maximisation) for ECFP-based molecular property prediction. Our results indicate that, despite its technical simplicity, Sort & Slice robustly (and at times substantially) outperforms traditional hash-based folding as well as the other investigated methods across prediction tasks, data splitting techniques, machine-learning models and ECFP hyperparameters. We thus recommend that Sort & Slice canonically replace hash-based folding as the default substructure-pooling technique to vectorise ECFPs for supervised molecular machine learning.</li>
</ul>

<h3>Title: Deep Generative Domain Adaptation with Temporal Attention for Cross-User  Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Xiaozhou Ye, Kevin I-Kai Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17958">https://arxiv.org/abs/2403.17958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17958">https://arxiv.org/pdf/2403.17958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17958]] Deep Generative Domain Adaptation with Temporal Attention for Cross-User  Activity Recognition(https://arxiv.org/abs/2403.17958)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In Human Activity Recognition (HAR), a predominant assumption is that the data utilized for training and evaluation purposes are drawn from the same distribution. It is also assumed that all data samples are independent and identically distributed ($\displaystyle i.i.d.$). Contrarily, practical implementations often challenge this notion, manifesting data distribution discrepancies, especially in scenarios such as cross-user HAR. Domain adaptation is the promising approach to address these challenges inherent in cross-user HAR tasks. However, a clear gap in domain adaptation techniques is the neglect of the temporal relation embedded within time series data during the phase of aligning data distributions. Addressing this oversight, our research presents the Deep Generative Domain Adaptation with Temporal Attention (DGDATA) method. This novel method uniquely recognises and integrates temporal relations during the domain adaptation process. By synergizing the capabilities of generative models with the Temporal Relation Attention mechanism, our method improves the classification performance in cross-user HAR. A comprehensive evaluation has been conducted on three public sensor-based HAR datasets targeting different scenarios and applications to demonstrate the efficacy of the proposed DGDATA method.</li>
</ul>

<h3>Title: EG-ConMix: An Intrusion Detection Method based on Graph Contrastive  Learning</h3>
<ul>
<li><strong>Authors: </strong>Lijin Wu, Shanshan Lei, Feilong Liao, Yuanjun Zheng, Yuxin Liu, Wentao Fu, Hao Song, Jiajun Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17980">https://arxiv.org/abs/2403.17980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17980">https://arxiv.org/pdf/2403.17980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17980]] EG-ConMix: An Intrusion Detection Method based on Graph Contrastive  Learning(https://arxiv.org/abs/2403.17980)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, extraction</a></li>
<li><strong>Abstract: </strong>As the number of IoT devices increases, security concerns become more prominent. The impact of threats can be minimized by deploying Network Intrusion Detection System (NIDS) by monitoring network traffic, detecting and discovering intrusions, and issuing security alerts promptly. Most intrusion detection research in recent years has been directed towards the pair of traffic itself without considering the interrelationships among them, thus limiting the monitoring of complex IoT network attack events. Besides, anomalous traffic in real networks accounts for only a small fraction, which leads to a severe imbalance problem in the dataset that makes algorithmic learning and prediction extremely difficult. In this paper, we propose an EG-ConMix method based on E-GraphSAGE, incorporating a data augmentation module to fix the problem of data imbalance. In addition, we incorporate contrastive learning to discern the difference between normal and malicious traffic samples, facilitating the extraction of key features. Extensive experiments on two publicly available datasets demonstrate the superior intrusion detection performance of EG-ConMix compared to state-of-the-art methods. Remarkably, it exhibits significant advantages in terms of training speed and accuracy for large-scale graphs.</li>
</ul>

<h3>Title: Is Watermarking LLM-Generated Code Robust?</h3>
<ul>
<li><strong>Authors: </strong>Tarun Suresh, Shubham Ugare, Gagandeep Singh, Sasa Misailovic</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17983">https://arxiv.org/abs/2403.17983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17983">https://arxiv.org/pdf/2403.17983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17983]] Is Watermarking LLM-Generated Code Robust?(https://arxiv.org/abs/2403.17983)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>We present the first study of the robustness of existing watermarking techniques on Python code generated by large language models. Although existing works showed that watermarking can be robust for natural language, we show that it is easy to remove these watermarks on code by semantic-preserving transformations.</li>
</ul>

<h3>Title: Mixing Artificial and Natural Intelligence: From Statistical Mechanics  to AI and Back to Turbulence</h3>
<ul>
<li><strong>Authors: </strong>Michael (Misha)Chertkov</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.stat-mech, cs.AI, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17993">https://arxiv.org/abs/2403.17993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17993">https://arxiv.org/pdf/2403.17993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17993]] Mixing Artificial and Natural Intelligence: From Statistical Mechanics  to AI and Back to Turbulence(https://arxiv.org/abs/2403.17993)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The paper reflects on the future role of AI in scientific research, with a special focus on turbulence studies, and examines the evolution of AI, particularly through Diffusion Models rooted in non-equilibrium statistical mechanics. It underscores the significant impact of AI on advancing reduced, Lagrangian models of turbulence through innovative use of deep neural networks. Additionally, the paper reviews various other AI applications in turbulence research and outlines potential challenges and opportunities in the concurrent advancement of AI and statistical hydrodynamics. This discussion sets the stage for a future where AI and turbulence research are intricately intertwined, leading to more profound insights and advancements in both fields.</li>
</ul>

<h3>Title: Solution for Point Tracking Task of ICCV 1st Perception Test Challenge  2023</h3>
<ul>
<li><strong>Authors: </strong>Hongpeng Pan, Yang Yang, Zhongtian Fu, Yuxuan Zhang, Shian Du, Yi Xu, Xiangyang Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17994">https://arxiv.org/abs/2403.17994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17994">https://arxiv.org/pdf/2403.17994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17994]] Solution for Point Tracking Task of ICCV 1st Perception Test Challenge  2023(https://arxiv.org/abs/2403.17994)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This report proposes an improved method for the Tracking Any Point (TAP) task, which tracks any physical surface through a video. Several existing approaches have explored the TAP by considering the temporal relationships to obtain smooth point motion trajectories, however, they still suffer from the cumulative error caused by temporal prediction. To address this issue, we propose a simple yet effective approach called TAP with confident static points (TAPIR+), which focuses on rectifying the tracking of the static point in the videos shot by a static camera. To clarify, our approach contains two key components: (1) Multi-granularity Camera Motion Detection, which could identify the video sequence by the static camera shot. (2) CMR-based point trajectory prediction with one moving object segmentation approach to isolate the static point from the moving object. Our approach ranked first in the final test with a score of 0.46.</li>
</ul>

<h3>Title: SpectralWaste Dataset: Multimodal Data for Waste Sorting Automation</h3>
<ul>
<li><strong>Authors: </strong>Sara Casao, Fernando Peña, Alberto Sabater, Rosa Castillón, Darío Suárez, Eduardo Montijano, Ana C. Murillo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18033">https://arxiv.org/abs/2403.18033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18033">https://arxiv.org/pdf/2403.18033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18033]] SpectralWaste Dataset: Multimodal Data for Waste Sorting Automation(https://arxiv.org/abs/2403.18033)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, segmentation</a></li>
<li><strong>Abstract: </strong>The increase in non-biodegradable waste is a worldwide concern. Recycling facilities play a crucial role, but their automation is hindered by the complex characteristics of waste recycling lines like clutter or object deformation. In addition, the lack of publicly available labeled data for these environments makes developing robust perception systems challenging. Our work explores the benefits of multimodal perception for object segmentation in real waste management scenarios. First, we present SpectralWaste, the first dataset collected from an operational plastic waste sorting facility that provides synchronized hyperspectral and conventional RGB images. This dataset contains labels for several categories of objects that commonly appear in sorting plants and need to be detected and separated from the main trash flow for several reasons, such as security in the management line or reuse. Additionally, we propose a pipeline employing different object segmentation architectures and evaluate the alternatives on our dataset, conducting an extensive analysis for both multimodal and unimodal alternatives. Our evaluation pays special attention to efficiency and suitability for real-time processing and demonstrates how HSI can bring a boost to RGB-only perception in these realistic industrial settings without much computational overhead.</li>
</ul>

<h3>Title: Bidirectional Consistency Models</h3>
<ul>
<li><strong>Authors: </strong>Liangchen Li, Jiajun He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18035">https://arxiv.org/abs/2403.18035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18035">https://arxiv.org/pdf/2403.18035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18035]] Bidirectional Consistency Models(https://arxiv.org/abs/2403.18035)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) are capable of generating remarkably high-quality samples by iteratively denoising a random vector, a process that corresponds to moving along the probability flow ordinary differential equation (PF ODE). Interestingly, DMs can also invert an input image to noise by moving backward along the PF ODE, a key operation for downstream tasks such as interpolation and image editing. However, the iterative nature of this process restricts its speed, hindering its broader application. Recently, Consistency Models (CMs) have emerged to address this challenge by approximating the integral of the PF ODE, thereby bypassing the need to iterate. Yet, the absence of an explicit ODE solver complicates the inversion process. To resolve this, we introduce the Bidirectional Consistency Model (BCM), which learns a single neural network that enables both forward and backward traversal along the PF ODE, efficiently unifying generation and inversion tasks within one framework. Notably, our proposed method enables one-step generation and inversion while also allowing the use of additional steps to enhance generation quality or reduce reconstruction error. Furthermore, by leveraging our model's bidirectional consistency, we introduce a sampling strategy that can enhance FID while preserving the generated image content. We further showcase our model's capabilities in several downstream tasks, such as interpolation and inpainting, and present demonstrations of potential applications, including blind restoration of compressed images and defending black-box adversarial attacks.</li>
</ul>

<h3>Title: Move as You Say, Interact as You Can: Language-guided Human Motion  Generation with Scene Affordance</h3>
<ul>
<li><strong>Authors: </strong>Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18036">https://arxiv.org/abs/2403.18036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18036">https://arxiv.org/pdf/2403.18036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18036]] Move as You Say, Interact as You Can: Language-guided Human Motion  Generation with Scene Affordance(https://arxiv.org/abs/2403.18036)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite significant advancements in text-to-motion synthesis, generating language-guided human motion within 3D environments poses substantial challenges. These challenges stem primarily from (i) the absence of powerful generative models capable of jointly modeling natural language, 3D scenes, and human motion, and (ii) the generative models' intensive data requirements contrasted with the scarcity of comprehensive, high-quality, language-scene-motion datasets. To tackle these issues, we introduce a novel two-stage framework that employs scene affordance as an intermediate representation, effectively linking 3D scene grounding and conditional motion generation. Our framework comprises an Affordance Diffusion Model (ADM) for predicting explicit affordance map and an Affordance-to-Motion Diffusion Model (AMDM) for generating plausible human motions. By leveraging scene affordance maps, our method overcomes the difficulty in generating human motion under multimodal condition signals, especially when training with limited data lacking extensive language-scene-motion pairs. Our extensive experiments demonstrate that our approach consistently outperforms all baselines on established benchmarks, including HumanML3D and HUMANISE. Additionally, we validate our model's exceptional generalization capabilities on a specially curated evaluation set featuring previously unseen descriptions and scenes.</li>
</ul>

<h3>Title: TGGLinesPlus: A robust topological graph-guided computer vision  algorithm for line detection from images</h3>
<ul>
<li><strong>Authors: </strong>Liping Yang, Joshua Driscol, Ming Gong, Shujie Wang, Catherine G. Potts</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18038">https://arxiv.org/abs/2403.18038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18038">https://arxiv.org/pdf/2403.18038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18038]] TGGLinesPlus: A robust topological graph-guided computer vision  algorithm for line detection from images(https://arxiv.org/abs/2403.18038)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Line detection is a classic and essential problem in image processing, computer vision and machine intelligence. Line detection has many important applications, including image vectorization (e.g., document recognition and art design), indoor mapping, and important societal challenges (e.g., sea ice fracture line extraction from satellite imagery). Many line detection algorithms and methods have been developed, but robust and intuitive methods are still lacking. In this paper, we proposed and implemented a topological graph-guided algorithm, named TGGLinesPlus, for line detection. Our experiments on images from a wide range of domains have demonstrated the flexibility of our TGGLinesPlus algorithm. We also benchmarked our algorithm with five classic and state-of-the-art line detection methods and the results demonstrate the robustness of TGGLinesPlus. We hope our open-source implementation of TGGLinesPlus will inspire and pave the way for many applications where spatial science matters.</li>
</ul>

<h3>Title: Supervisory Prompt Training</h3>
<ul>
<li><strong>Authors: </strong>Jean Ghislain Billa, Min Oh, Liang Du</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18051">https://arxiv.org/abs/2403.18051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18051">https://arxiv.org/pdf/2403.18051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18051]] Supervisory Prompt Training(https://arxiv.org/abs/2403.18051)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The performance of Large Language Models (LLMs) relies heavily on the quality of prompts, which are often manually engineered and task-specific, making them costly and non-scalable. We propose a novel approach, Supervisory Prompt Training (SPT). SPT automates the generation of highly effective prompts using a dual LLM system. In this system, one LLM, the generator, performs a task while the other, the corrector, provides feedback and generates improved prompts. In contrast to earlier techniques, both the generator and corrector collaboratively and continuously improve their prompts over time. We also introduce the concept of \textit{impact scores} to measure the sentence-level effectiveness of the prompts. Our method was tested on four benchmarks, testing the level of hallucinations in LLMs. Notably, we were able to increase the accuracy of GPT-4 on GSM8K from 65.8\% to 94.1\% (28.3\% increase). SPT advances LLMs by refining prompts to enhance performance and reduce hallucinations, offering an efficient and scalable alternative to traditional model fine-tuning.</li>
</ul>

<h3>Title: COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Yuelin Bai, Xinrun Du, Yiming Liang, Yonggang Jin, Ziqiang Liu, Junting Zhou, Tianyu Zheng, Xincheng Zhang, Nuo Ma, Zekun Wang, Ruibin Yuan, Haihong Wu, Hongquan Lin, Wenhao Huang, Jiajun Zhang, Wenhu Chen, Chenghua Lin, Jie Fu, Min Yang, Shiwen Ni, Ge Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18058">https://arxiv.org/abs/2403.18058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18058">https://arxiv.org/pdf/2403.18058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18058]] COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning(https://arxiv.org/abs/2403.18058)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Recently, there have been significant advancements in large language models (LLMs), particularly focused on the English language. These advancements have enabled these LLMs to understand and execute complex instructions with unprecedented accuracy and fluency. However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning. The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks. Existing datasets are either derived from English-centric LLMs or are ill-suited for aligning with the interaction patterns of real-world Chinese users. To bridge this gap, we introduce COIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to build a diverse, wide-ranging instruction-tuning dataset to better align model behavior with human interactions. To this end, we collect a high-quality human-written corpus from various sources on the Chinese Internet, including Q&A communities, Wikis, examinations, and existing NLP datasets. This corpus was rigorously filtered and carefully processed to form the COIG-CQIA dataset. Furthermore, we train models of various scales on different subsets of CQIA, following in-depth evaluation and analyses. The findings from our experiments offer valuable insights for selecting and developing Chinese instruction-tuning datasets. We also find that models trained on CQIA-Subset achieve competitive results in human assessment as well as knowledge and security benchmarks. Data are available at https://huggingface.co/datasets/m-a-p/COIG-CQIA</li>
</ul>

<h3>Title: Spectral Convolutional Transformer: Harmonizing Real vs. Complex  Multi-View Spectral Operators for Vision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Badri N. Patro, Vinay P. Namboodiri, Vijay S. Agneeswaran</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18063">https://arxiv.org/abs/2403.18063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18063">https://arxiv.org/pdf/2403.18063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18063]] Spectral Convolutional Transformer: Harmonizing Real vs. Complex  Multi-View Spectral Operators for Vision Transformer(https://arxiv.org/abs/2403.18063)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Transformers used in vision have been investigated through diverse architectures - ViT, PVT, and Swin. These have worked to improve the attention mechanism and make it more efficient. Differently, the need for including local information was felt, leading to incorporating convolutions in transformers such as CPVT and CvT. Global information is captured using a complex Fourier basis to achieve global token mixing through various methods, such as AFNO, GFNet, and Spectformer. We advocate combining three diverse views of data - local, global, and long-range dependence. We also investigate the simplest global representation using only the real domain spectral representation - obtained through the Hartley transform. We use a convolutional operator in the initial layers to capture local information. Through these two contributions, we are able to optimize and obtain a spectral convolution transformer (SCT) that provides improved performance over the state-of-the-art methods while reducing the number of parameters. Through extensive experiments, we show that SCT-C-small gives state-of-the-art performance on the ImageNet dataset and reaches 84.5\% top-1 accuracy, while SCT-C-Large reaches 85.9\% and SCT-C-Huge reaches 86.4\%. We evaluate SCT on transfer learning on datasets such as CIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on downstream tasks i.e. instance segmentation on the MSCOCO dataset. The project page is available on this webpage.\url{https://github.com/badripatro/sct}</li>
</ul>

<h3>Title: EgoPoseFormer: A Simple Baseline for Egocentric 3D Human Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Chenhongyi Yang, Anastasia Tkach, Shreyas Hampali, Linguang Zhang, Elliot J. Crowley, Cem Keskin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18080">https://arxiv.org/abs/2403.18080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18080">https://arxiv.org/pdf/2403.18080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18080]] EgoPoseFormer: A Simple Baseline for Egocentric 3D Human Pose Estimation(https://arxiv.org/abs/2403.18080)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present EgoPoseFormer, a simple yet effective transformer-based model for stereo egocentric human pose estimation. The main challenge in egocentric pose estimation is overcoming joint invisibility, which is caused by self-occlusion or a limited field of view (FOV) of head-mounted cameras. Our approach overcomes this challenge by incorporating a two-stage pose estimation paradigm: in the first stage, our model leverages the global information to estimate each joint's coarse location, then in the second stage, it employs a DETR style transformer to refine the coarse locations by exploiting fine-grained stereo visual features. In addition, we present a deformable stereo operation to enable our transformer to effectively process multi-view features, which enables it to accurately localize each joint in the 3D world. We evaluate our method on the stereo UnrealEgo dataset and show it significantly outperforms previous approaches while being computationally efficient: it improves MPJPE by 27.4mm (45% improvement) with only 7.9% model parameters and 13.1% FLOPs compared to the state-of-the-art. Surprisingly, with proper training techniques, we find that even our first-stage pose proposal network can achieve superior performance compared to previous arts. We also show that our method can be seamlessly extended to monocular settings, which achieves state-of-the-art performance on the SceneEgo dataset, improving MPJPE by 25.5mm (21% improvement) compared to the best existing method with only 60.7% model parameters and 36.4% FLOPs.</li>
</ul>

<h3>Title: OCAI: Improving Optical Flow Estimation by Occlusion and Consistency  Aware Interpolation</h3>
<ul>
<li><strong>Authors: </strong>Jisoo Jeong, Hong Cai, Risheek Garrepalli, Jamie Menjay Lin, Munawar Hayat, Fatih Porikli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18092">https://arxiv.org/abs/2403.18092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18092">https://arxiv.org/pdf/2403.18092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18092]] OCAI: Improving Optical Flow Estimation by Occlusion and Consistency  Aware Interpolation(https://arxiv.org/abs/2403.18092)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The scarcity of ground-truth labels poses one major challenge in developing optical flow estimation models that are both generalizable and robust. While current methods rely on data augmentation, they have yet to fully exploit the rich information available in labeled video sequences. We propose OCAI, a method that supports robust frame interpolation by generating intermediate video frames alongside optical flows in between. Utilizing a forward warping approach, OCAI employs occlusion awareness to resolve ambiguities in pixel values and fills in missing values by leveraging the forward-backward consistency of optical flows. Additionally, we introduce a teacher-student style semi-supervised learning method on top of the interpolated frames. Using a pair of unlabeled frames and the teacher model's predicted optical flow, we generate interpolated frames and flows to train a student model. The teacher's weights are maintained using Exponential Moving Averaging of the student. Our evaluations demonstrate perceptually superior interpolation quality and enhanced optical flow accuracy on established benchmarks such as Sintel and KITTI.</li>
</ul>

<h3>Title: Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hai-Long Nguyen, Duc-Minh Nguyen, Tan-Minh Nguyen, Ha-Thanh Nguyen, Thi-Hai-Yen Vuong, Ken Satoh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18093">https://arxiv.org/abs/2403.18093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18093">https://arxiv.org/pdf/2403.18093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18093]] Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large  Language Models(https://arxiv.org/abs/2403.18093)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models with billions of parameters, such as GPT-3.5, GPT-4, and LLaMA, are increasingly prevalent. Numerous studies have explored effective prompting techniques to harness the power of these LLMs for various research problems. Retrieval, specifically in the legal data domain, poses a challenging task for the direct application of Prompting techniques due to the large number and substantial length of legal articles. This research focuses on maximizing the potential of prompting by placing it as the final phase of the retrieval system, preceded by the support of two phases: BM25 Pre-ranking and BERT-based Re-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating prompting techniques on LLMs into the retrieval system significantly improves retrieval accuracy. However, error analysis reveals several existing issues in the retrieval system that still need resolution.</li>
</ul>

<h3>Title: GPTs and Language Barrier: A Cross-Lingual Legal QA Examination</h3>
<ul>
<li><strong>Authors: </strong>Ha-Thanh Nguyen, Hiroaki Yamada, Ken Satoh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18098">https://arxiv.org/abs/2403.18098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18098">https://arxiv.org/pdf/2403.18098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18098]] GPTs and Language Barrier: A Cross-Lingual Legal QA Examination(https://arxiv.org/abs/2403.18098)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we explore the application of Generative Pre-trained Transformers (GPTs) in cross-lingual legal Question-Answering (QA) systems using the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a set of related legal articles that serve as context, the objective is to determine whether the statement is legally valid, i.e., if it can be inferred from the provided contextual articles or not, which is also known as an entailment task. By benchmarking four different combinations of English and Japanese prompts and data, we provide valuable insights into GPTs' performance in multilingual legal QA scenarios, contributing to the development of more efficient and accurate cross-lingual QA solutions in the legal domain.</li>
</ul>

<h3>Title: Tutorial on Diffusion Models for Imaging and Vision</h3>
<ul>
<li><strong>Authors: </strong>Stanley H. Chan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18103">https://arxiv.org/abs/2403.18103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18103">https://arxiv.org/pdf/2403.18103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18103]] Tutorial on Diffusion Models for Imaging and Vision(https://arxiv.org/abs/2403.18103)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The astonishing growth of generative tools in recent years has empowered many exciting applications in text-to-image generation and text-to-video generation. The underlying principle behind these generative tools is the concept of diffusion, a particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the diffusion models. The target audience of this tutorial includes undergraduate and graduate students who are interested in doing research on diffusion models or applying these models to solve other problems.</li>
</ul>

<h3>Title: Mathematical Foundation and Corrections for Full Range Head Pose  Estimation</h3>
<ul>
<li><strong>Authors: </strong>Huei-Chung Hu, Xuyang Wu, Yuan Wang, Yi Fang, Hsin-Tai Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18104">https://arxiv.org/abs/2403.18104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18104">https://arxiv.org/pdf/2403.18104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18104]] Mathematical Foundation and Corrections for Full Range Head Pose  Estimation(https://arxiv.org/abs/2403.18104)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Numerous works concerning head pose estimation (HPE) offer algorithms or proposed neural network-based approaches for extracting Euler angles from either facial key points or directly from images of the head region. However, many works failed to provide clear definitions of the coordinate systems and Euler or Tait-Bryan angles orders in use. It is a well-known fact that rotation matrices depend on coordinate systems, and yaw, roll, and pitch angles are sensitive to their application order. Without precise definitions, it becomes challenging to validate the correctness of the output head pose and drawing routines employed in prior works. In this paper, we thoroughly examined the Euler angles defined in the 300W-LP dataset, head pose estimation such as 3DDFA-v2, 6D-RepNet, WHENet, etc, and the validity of their drawing routines of the Euler angles. When necessary, we infer their coordinate system and sequence of yaw, roll, pitch from provided code. This paper presents (1) code and algorithms for inferring coordinate system from provided source code, code for Euler angle application order and extracting precise rotation matrices and the Euler angles, (2) code and algorithms for converting poses from one rotation system to another, (3) novel formulae for 2D augmentations of the rotation matrices, and (4) derivations and code for the correct drawing routines for rotation matrices and poses. This paper also addresses the feasibility of defining rotations with right-handed coordinate system in Wikipedia and SciPy, which makes the Euler angle extraction much easier for full-range head pose research.</li>
</ul>

<h3>Title: Large Language Models for Education: A Survey and Outlook</h3>
<ul>
<li><strong>Authors: </strong>Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S. Yu, Qingsong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18105">https://arxiv.org/abs/2403.18105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18105">https://arxiv.org/pdf/2403.18105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18105]] Large Language Models for Education: A Survey and Outlook(https://arxiv.org/abs/2403.18105)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment.</li>
</ul>

<h3>Title: Segment Any Medical Model Extended</h3>
<ul>
<li><strong>Authors: </strong>Yihao Liu, Jiaming Zhang, Andres Diaz-Pinto, Haowei Li, Alejandro Martin-Gomez, Amir Kheradmand, Mehran Armand</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18114">https://arxiv.org/abs/2403.18114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18114">https://arxiv.org/pdf/2403.18114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18114]] Segment Any Medical Model Extended(https://arxiv.org/abs/2403.18114)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The Segment Anything Model (SAM) has drawn significant attention from researchers who work on medical image segmentation because of its generalizability. However, researchers have found that SAM may have limited performance on medical images compared to state-of-the-art non-foundation models. Regardless, the community sees potential in extending, fine-tuning, modifying, and evaluating SAM for analysis of medical imaging. An increasing number of works have been published focusing on the mentioned four directions, where variants of SAM are proposed. To this end, a unified platform helps push the boundary of the foundation model for medical images, facilitating the use, modification, and validation of SAM and its variants in medical image segmentation. In this work, we introduce SAMM Extended (SAMME), a platform that integrates new SAM variant models, adopts faster communication protocols, accommodates new interactive modes, and allows for fine-tuning of subcomponents of the models. These features can expand the potential of foundation models like SAM, and the results can be translated to applications such as image-guided therapy, mixed reality interaction, robotic navigation, and data augmentation.</li>
</ul>

<h3>Title: EgoLifter: Open-world 3D Segmentation for Egocentric Perception</h3>
<ul>
<li><strong>Authors: </strong>Qiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18118">https://arxiv.org/abs/2403.18118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18118">https://arxiv.org/pdf/2403.18118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18118]] EgoLifter: Open-world 3D Segmentation for Egocentric Perception(https://arxiv.org/abs/2403.18118)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this paper we present EgoLifter, a novel system that can automatically segment scenes captured from egocentric sensors into a complete decomposition of individual 3D objects. The system is specifically designed for egocentric data where scenes contain hundreds of objects captured from natural (non-scanning) motion. EgoLifter adopts 3D Gaussians as the underlying representation of 3D scenes and objects and uses segmentation masks from the Segment Anything Model (SAM) as weak supervision to learn flexible and promptable definitions of object instances free of any specific object taxonomy. To handle the challenge of dynamic objects in ego-centric videos, we design a transient prediction module that learns to filter out dynamic objects in the 3D reconstruction. The result is a fully automatic pipeline that is able to reconstruct 3D object instances as collections of 3D Gaussians that collectively compose the entire scene. We created a new benchmark on the Aria Digital Twin dataset that quantitatively demonstrates its state-of-the-art performance in open-world 3D segmentation from natural egocentric input. We run EgoLifter on various egocentric activity datasets which shows the promise of the method for 3D egocentric perception at scale.</li>
</ul>

<h3>Title: ChatGPT Role-play Dataset: Analysis of User Motives and Model  Naturalness</h3>
<ul>
<li><strong>Authors: </strong>Yufei Tao, Ameeta Agrawal, Judit Dombi, Tetyana Sydorenko, Jung In Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18121">https://arxiv.org/abs/2403.18121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18121">https://arxiv.org/pdf/2403.18121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18121]] ChatGPT Role-play Dataset: Analysis of User Motives and Model  Naturalness(https://arxiv.org/abs/2403.18121)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in interactive large language models like ChatGPT have revolutionized various domains; however, their behavior in natural and role-play conversation settings remains underexplored. In our study, we address this gap by deeply investigating how ChatGPT behaves during conversations in different settings by analyzing its interactions in both a normal way and a role-play setting. We introduce a novel dataset of broad range of human-AI conversations annotated with user motives and model naturalness to examine (i) how humans engage with the conversational AI model, and (ii) how natural are AI model responses. Our study highlights the diversity of user motives when interacting with ChatGPT and variable AI naturalness, showing not only the nuanced dynamics of natural conversations between humans and AI, but also providing new avenues for improving the effectiveness of human-AI communication.</li>
</ul>

<h3>Title: For those who don't know (how) to ask: Building a dataset of technology  questions for digital newcomers</h3>
<ul>
<li><strong>Authors: </strong>Evan Lucas, Kelly S. Steelman, Leo C. Ureel, Charles Wallace</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18125">https://arxiv.org/abs/2403.18125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18125">https://arxiv.org/pdf/2403.18125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18125]] For those who don't know (how) to ask: Building a dataset of technology  questions for digital newcomers(https://arxiv.org/abs/2403.18125)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While the rise of large language models (LLMs) has created rich new opportunities to learn about digital technology, many on the margins of this technology struggle to gain and maintain competency due to lexical or conceptual barriers that prevent them from asking appropriate questions. Although there have been many efforts to understand factuality of LLM-created content and ability of LLMs to answer questions, it is not well understood how unclear or nonstandard language queries affect the model outputs. We propose the creation of a dataset that captures questions of digital newcomers and outsiders, utilizing data we have compiled from a decade's worth of one-on-one tutoring. In this paper we lay out our planned efforts and some potential uses of this dataset.</li>
</ul>

<h3>Title: Recommendation of data-free class-incremental learning algorithms by  simulating future data</h3>
<ul>
<li><strong>Authors: </strong>Eva Feillet, Adrian Popescu, Céline Hudelot</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18132">https://arxiv.org/abs/2403.18132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18132">https://arxiv.org/pdf/2403.18132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18132]] Recommendation of data-free class-incremental learning algorithms by  simulating future data(https://arxiv.org/abs/2403.18132)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free, generative</a></li>
<li><strong>Abstract: </strong>Class-incremental learning deals with sequential data streams composed of batches of classes. Various algorithms have been proposed to address the challenging case where samples from past classes cannot be stored. However, selecting an appropriate algorithm for a user-defined setting is an open problem, as the relative performance of these algorithms depends on the incremental settings. To solve this problem, we introduce an algorithm recommendation method that simulates the future data stream. Given an initial set of classes, it leverages generative models to simulate future classes from the same visual domain. We evaluate recent algorithms on the simulated stream and recommend the one which performs best in the user-defined incremental setting. We illustrate the effectiveness of our method on three large datasets using six algorithms and six incremental settings. Our method outperforms competitive baselines, and performance is close to that of an oracle choosing the best algorithm in each setting. This work contributes to facilitate the practical deployment of incremental learning.</li>
</ul>

<h3>Title: Securing GNNs: Explanation-Based Identification of Backdoored Training  Graphs</h3>
<ul>
<li><strong>Authors: </strong>Jane Downer, Ren Wang, Binghui Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18136">https://arxiv.org/abs/2403.18136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18136">https://arxiv.org/pdf/2403.18136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18136]] Securing GNNs: Explanation-Based Identification of Backdoored Training  Graphs(https://arxiv.org/abs/2403.18136)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet they are vulnerable to backdoor attacks that can compromise their performance and ethical application. The detection of these attacks is crucial for maintaining the reliability and security of GNN classification tasks, but effective detection techniques are lacking. Following an initial investigation, we observed that while graph-level explanations can offer limited insights, their effectiveness in detecting backdoor triggers is inconsistent and incomplete. To bridge this gap, we extract and transform secondary outputs of GNN explanation mechanisms, designing seven novel metrics that more effectively detect backdoor attacks. Additionally, we develop an adaptive attack to rigorously evaluate our approach. We test our method on multiple benchmark datasets and examine its efficacy against various attack models. Our results show that our method can achieve high detection performance, marking a significant advancement in safeguarding GNNs against backdoor attacks.</li>
</ul>

<h3>Title: Juru: Legal Brazilian Large Language Model from Reputable Sources</h3>
<ul>
<li><strong>Authors: </strong>Roseval Malaquias Junior, Ramon Pires, Roseli Romero, Rodrigo Nogueira</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18140">https://arxiv.org/abs/2403.18140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18140">https://arxiv.org/pdf/2403.18140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18140]] Juru: Legal Brazilian Large Language Model from Reputable Sources(https://arxiv.org/abs/2403.18140)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The high computational cost associated with pretraining large language models limits their research. Two strategies have emerged to address this issue: domain specialization and pretraining with high-quality data. To explore these strategies, we specialized the Sabi\'a-2 Small model with 1.9 billion unique tokens from reputable Brazilian legal sources and conducted few-shot evaluations on legal and general knowledge exams. Our model, Juru, demonstrates the benefits of domain specialization with a reduced amount of pretraining data. However, this specialization comes at the expense of degrading performance in other knowledge areas within the same language. This study contributes to the growing body of scientific evidence showing that pretraining data selection may enhance the performance of large language models, enabling the exploration of these models at a lower cost.</li>
</ul>

<h3>Title: HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded  Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Yongyi Yang, Jiaming Yang, Wei Hu, Michał Dereziński</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18142">https://arxiv.org/abs/2403.18142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18142">https://arxiv.org/pdf/2403.18142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18142]] HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded  Graph Neural Networks(https://arxiv.org/abs/2403.18142)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>As a variant of Graph Neural Networks (GNNs), Unfolded GNNs offer enhanced interpretability and flexibility over traditional designs. Nevertheless, they still suffer from scalability challenges when it comes to the training cost. Although many methods have been proposed to address the scalability issues, they mostly focus on per-iteration efficiency, without worst-case convergence guarantees. Moreover, those methods typically add components to or modify the original model, thus possibly breaking the interpretability of Unfolded GNNs. In this paper, we propose HERTA: a High-Efficiency and Rigorous Training Algorithm for Unfolded GNNs that accelerates the whole training process, achieving a nearly-linear time worst-case training guarantee. Crucially, HERTA converges to the optimum of the original model, thus preserving the interpretability of Unfolded GNNs. Additionally, as a byproduct of HERTA, we propose a new spectral sparsification method applicable to normalized and regularized graph Laplacians that ensures tighter bounds for our algorithm than existing spectral sparsifiers do. Experiments on real-world datasets verify the superiority of HERTA as well as its adaptability to various loss functions and optimizers.</li>
</ul>

<h3>Title: Leak and Learn: An Attacker's Cookbook to Train Using Leaked Data from  Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Joshua C. Zhao, Ahaan Dabholkar, Atul Sharma, Saurabh Bagchi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18144">https://arxiv.org/abs/2403.18144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18144">https://arxiv.org/pdf/2403.18144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18144]] Leak and Learn: An Attacker's Cookbook to Train Using Leaked Data from  Federated Learning(https://arxiv.org/abs/2403.18144)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated learning is a decentralized learning paradigm introduced to preserve privacy of client data. Despite this, prior work has shown that an attacker at the server can still reconstruct the private training data using only the client updates. These attacks are known as data reconstruction attacks and fall into two major categories: gradient inversion (GI) and linear layer leakage attacks (LLL). However, despite demonstrating the effectiveness of these attacks in breaching privacy, prior work has not investigated the usefulness of the reconstructed data for downstream tasks. In this work, we explore data reconstruction attacks through the lens of training and improving models with leaked data. We demonstrate the effectiveness of both GI and LLL attacks in maliciously training models using the leaked data more accurately than a benign federated learning strategy. Counter-intuitively, this bump in training quality can occur despite limited reconstruction quality or a small total number of leaked images. Finally, we show the limitations of these attacks for downstream training, individually for GI attacks and for LLL attacks.</li>
</ul>

<h3>Title: Divide, Conquer, Combine Bayesian Decision Tree Sampling</h3>
<ul>
<li><strong>Authors: </strong>Jodie A. Cochrane, Adrian Wills, Sarah J. Johnson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18147">https://arxiv.org/abs/2403.18147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18147">https://arxiv.org/pdf/2403.18147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18147]] Divide, Conquer, Combine Bayesian Decision Tree Sampling(https://arxiv.org/abs/2403.18147)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Decision trees are commonly used predictive models due to their flexibility and interpretability. This paper is directed at quantifying the uncertainty of decision tree predictions by employing a Bayesian inference approach. This is challenging because these approaches need to explore both the tree structure space and the space of decision parameters associated with each tree structure. This has been handled by using Markov Chain Monte Carlo (MCMC) methods, where a Markov Chain is constructed to provide samples from the desired Bayesian estimate. Importantly, the structure and the decision parameters are tightly coupled; small changes in the tree structure can demand vastly different decision parameters to provide accurate predictions. A challenge for existing MCMC approaches is proposing joint changes in both the tree structure and the decision parameters that result in efficient sampling. This paper takes a different approach, where each distinct tree structure is associated with a unique set of decision parameters. The proposed approach, entitled DCC-Tree, is inspired by the work in Zhou et al. [23] for probabilistic programs and Cochrane et al. [4] for Hamiltonian Monte Carlo (HMC) based sampling for decision trees. Results show that DCC-Tree performs comparably to other HMC-based methods and better than existing Bayesian tree methods while improving on consistency and reducing the per-proposal complexity.</li>
</ul>

<h3>Title: Large Language Models Produce Responses Perceived to be Empathic</h3>
<ul>
<li><strong>Authors: </strong>Yoon Kyung Lee, Jina Suh, Hongli Zhan, Junyi Jessy Li, Desmond C. Ong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18148">https://arxiv.org/abs/2403.18148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18148">https://arxiv.org/pdf/2403.18148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18148]] Large Language Models Produce Responses Perceived to be Empathic(https://arxiv.org/abs/2403.18148)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated surprising performance on many tasks, including writing supportive messages that display empathy. Here, we had these models generate empathic messages in response to posts describing common life experiences, such as workplace situations, parenting, relationships, and other anxiety- and anger-eliciting situations. Across two studies (N=192, 202), we showed human raters a variety of responses written by several models (GPT4 Turbo, Llama2, and Mistral), and had people rate these responses on how empathic they seemed to be. We found that LLM-generated responses were consistently rated as more empathic than human-written responses. Linguistic analyses also show that these models write in distinct, predictable ``styles", in terms of their use of punctuation, emojis, and certain words. These results highlight the potential of using LLMs to enhance human peer support in contexts where empathy is important.</li>
</ul>

<h3>Title: Large Language Models as Financial Data Annotators: A Study on  Effectiveness and Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Toyin Aguda, Suchetha Siddagangappa, Elena Kochkina, Simerjot Kaur, Dongsheng Wang, Charese Smiley, Sameena Shah</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18152">https://arxiv.org/abs/2403.18152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18152">https://arxiv.org/pdf/2403.18152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18152]] Large Language Models as Financial Data Annotators: A Study on  Effectiveness and Efficiency(https://arxiv.org/abs/2403.18152)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Collecting labeled datasets in finance is challenging due to scarcity of domain experts and higher cost of employing them. While Large Language Models (LLMs) have demonstrated remarkable performance in data annotation tasks on general domain datasets, their effectiveness on domain specific datasets remains underexplored. To address this gap, we investigate the potential of LLMs as efficient data annotators for extracting relations in financial documents. We compare the annotations produced by three LLMs (GPT-4, PaLM 2, and MPT Instruct) against expert annotators and crowdworkers. We demonstrate that the current state-of-the-art LLMs can be sufficient alternatives to non-expert crowdworkers. We analyze models using various prompts and parameter settings and find that customizing the prompts for each relation group by providing specific examples belonging to those groups is paramount. Furthermore, we introduce a reliability index (LLM-RelIndex) used to identify outputs that may require expert attention. Finally, we perform an extensive time, cost and error analysis and provide recommendations for the collection and usage of automated annotations in domain-specific settings.</li>
</ul>

<h3>Title: The Effects of Short Video-Sharing Services on Video Copy Detection</h3>
<ul>
<li><strong>Authors: </strong>Rintaro Yanagi, Yamato Okamoto, Shuhei Yokoo, Shin'ichi Satoh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18158">https://arxiv.org/abs/2403.18158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18158">https://arxiv.org/pdf/2403.18158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18158]] The Effects of Short Video-Sharing Services on Video Copy Detection(https://arxiv.org/abs/2403.18158)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The short video-sharing services that allow users to post 10-30 second videos (e.g., YouTube Shorts and TikTok) have attracted a lot of attention in recent years. However, conventional video copy detection (VCD) methods mainly focus on general video-sharing services (e.g., YouTube and Bilibili), and the effects of short video-sharing services on video copy detection are still unclear. Considering that illegally copied videos in short video-sharing services have service-distinctive characteristics, especially in those time lengths, the pros and cons of VCD in those services are required to be analyzed. In this paper, we examine the effects of short video-sharing services on VCD by constructing a dataset that has short video-sharing service characteristics. Our novel dataset is automatically constructed from the publicly available dataset to have reference videos and fixed short-time-length query videos, and such automation procedures assure the reproducibility and data privacy preservation of this paper. From the experimental results focusing on segment-level and video-level situations, we can see that three effects: "Segment-level VCD in short video-sharing services is more difficult than those in general video-sharing services", "Video-level VCD in short video-sharing services is easier than those in general video-sharing services", "The video alignment component mainly suppress the detection performance in short video-sharing services".</li>
</ul>

<h3>Title: Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal  Propagation Analysis for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Kyunggeun Lee, Jun Ma, Harris Teague</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18159">https://arxiv.org/abs/2403.18159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18159">https://arxiv.org/pdf/2403.18159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18159]] Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal  Propagation Analysis for Large Language Models(https://arxiv.org/abs/2403.18159)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large generative models, such as large language models (LLMs) and diffusion models have as revolutionized the fields of NLP and computer vision respectively. However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices. In this study, we propose a light-weight quantization aware fine tuning technique using knowledge distillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs using commonly available datasets to realize a popular language use case, on device chat applications. To improve this paradigm of finetuning, as main contributions, we provide insights into stability of KD-QAT by empirically studying the gradient propagation during training to better understand the vulnerabilities of KD-QAT based approaches to low-bit quantization errors. Based on our insights, we propose ov-freeze, a simple technique to stabilize the KD-QAT process. Finally, we experiment with the popular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that ov-freeze results in near float-point precision performance, i.e., less than 0.7% loss of accuracy on Commonsense Reasoning benchmarks.</li>
</ul>

<h3>Title: Optimizing Cyber Response Time on Temporal Active Directory Networks  Using Decoys</h3>
<ul>
<li><strong>Authors: </strong>Huy Q. Ngo, Mingyu Guo, Hung Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.GT, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18162">https://arxiv.org/abs/2403.18162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18162">https://arxiv.org/pdf/2403.18162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18162]] Optimizing Cyber Response Time on Temporal Active Directory Networks  Using Decoys(https://arxiv.org/abs/2403.18162)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Microsoft Active Directory (AD) is the default security management system for Window domain network. We study the problem of placing decoys in AD network to detect potential attacks. We model the problem as a Stackelberg game between an attacker and a defender on AD attack graphs where the defender employs a set of decoys to detect the attacker on their way to Domain Admin (DA). Contrary to previous works, we consider time-varying (temporal) attack graphs. We proposed a novel metric called response time, to measure the effectiveness of our decoy placement in temporal attack graphs. Response time is defined as the duration from the moment attackers trigger the first decoy to when they compromise the DA. Our goal is to maximize the defender's response time to the worst-case attack paths. We establish the NP-hard nature of the defender's optimization problem, leading us to develop Evolutionary Diversity Optimization (EDO) algorithms. EDO algorithms identify diverse sets of high-quality solutions for the optimization problem. Despite the polynomial nature of the fitness function, it proves experimentally slow for larger graphs. To enhance scalability, we proposed an algorithm that exploits the static nature of AD infrastructure in the temporal setting. Then, we introduce tailored repair operations, ensuring the convergence to better results while maintaining scalability for larger graphs.</li>
</ul>

<h3>Title: Mechanisms of non-factual hallucinations in language models</h3>
<ul>
<li><strong>Authors: </strong>Lei Yu, Meng Cao, Jackie Chi Kit Cheung, Yue Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18167">https://arxiv.org/abs/2403.18167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18167">https://arxiv.org/pdf/2403.18167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18167]] Mechanisms of non-factual hallucinations in language models(https://arxiv.org/abs/2403.18167)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains elusive. Our study investigates the mechanistic causes of hallucination, specifically non-factual ones where the LM incorrectly predicts object attributes in response to subject-relation queries. With causal mediation analysis and embedding space projection, we identify two general mechanistic causes of hallucinations shared across LMs of various scales and designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and 2) failing to select the correct object attribute in upper layer attention heads and MLPs. These two mechanisms exhibit varying degrees of subject-object association, predictive uncertainty and perturbation robustness. Additionally, we scrutinize LM pre-training checkpoints, revealing distinct learning dynamics for the two mechanistic causes of hallucinations. We also highlight how attribution features from our causal analysis can effectively construct hallucination detectors. Our work proposes a mechanistic understanding of LM factual errors.</li>
</ul>

<h3>Title: Multi-Layer Dense Attention Decoder for Polyp Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Krushi Patel, Fengjun Li, Guanghui Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18180">https://arxiv.org/abs/2403.18180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18180">https://arxiv.org/pdf/2403.18180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18180]] Multi-Layer Dense Attention Decoder for Polyp Segmentation(https://arxiv.org/abs/2403.18180)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Detecting and segmenting polyps is crucial for expediting the diagnosis of colon cancer. This is a challenging task due to the large variations of polyps in color, texture, and lighting conditions, along with subtle differences between the polyp and its surrounding area. Recently, vision Transformers have shown robust abilities in modeling global context for polyp segmentation. However, they face two major limitations: the inability to learn local relations among multi-level layers and inadequate feature aggregation in the decoder. To address these issues, we propose a novel decoder architecture aimed at hierarchically aggregating locally enhanced multi-level dense features. Specifically, we introduce a novel module named Dense Attention Gate (DAG), which adaptively fuses all previous layers' features to establish local feature relations among all layers. Furthermore, we propose a novel nested decoder architecture that hierarchically aggregates decoder features, thereby enhancing semantic features. We incorporate our novel dense decoder with the PVT backbone network and conduct evaluations on five polyp segmentation datasets: Kvasir, CVC-300, CVC-ColonDB, CVC-ClinicDB, and ETIS. Our experiments and comparisons with nine competing segmentation models demonstrate that the proposed architecture achieves state-of-the-art performance and outperforms the previous models on four datasets. The source code is available at: https://github.com/krushi1992/Dense-Decoder.</li>
</ul>

<h3>Title: Don't Look into the Dark: Latent Codes for Pluralistic Image Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Haiwei Chen, Yajie Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18186">https://arxiv.org/abs/2403.18186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18186">https://arxiv.org/pdf/2403.18186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18186]] Don't Look into the Dark: Latent Codes for Pluralistic Image Inpainting(https://arxiv.org/abs/2403.18186)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>We present a method for large-mask pluralistic image inpainting based on the generative framework of discrete latent codes. Our method learns latent priors, discretized as tokens, by only performing computations at the visible locations of the image. This is realized by a restrictive partial encoder that predicts the token label for each visible block, a bidirectional transformer that infers the missing labels by only looking at these tokens, and a dedicated synthesis network that couples the tokens with the partial image priors to generate coherent and pluralistic complete image even under extreme mask settings. Experiments on public benchmarks validate our design choices as the proposed method outperforms strong baselines in both visual quality and diversity metrics.</li>
</ul>

<h3>Title: LayoutFlow: Flow Matching for Layout Generation</h3>
<ul>
<li><strong>Authors: </strong>Julian Jorge Andrade Guerreiro, Naoto Inoue, Kento Masui, Mayu Otani, Hideki Nakayama</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18187">https://arxiv.org/abs/2403.18187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18187">https://arxiv.org/pdf/2403.18187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18187]] LayoutFlow: Flow Matching for Layout Generation(https://arxiv.org/abs/2403.18187)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Finding a suitable layout represents a crucial task for diverse applications in graphic design. Motivated by simpler and smoother sampling trajectories, we explore the use of Flow Matching as an alternative to current diffusion-based layout generation models. Specifically, we propose LayoutFlow, an efficient flow-based model capable of generating high-quality layouts. Instead of progressively denoising the elements of a noisy layout, our method learns to gradually move, or flow, the elements of an initial sample until it reaches its final prediction. In addition, we employ a conditioning scheme that allows us to handle various generation tasks with varying degrees of conditioning with a single model. Empirically, LayoutFlow performs on par with state-of-the-art models while being significantly faster.</li>
</ul>

<h3>Title: Middle Fusion and Multi-Stage, Multi-Form Prompts for Robust RGB-T  Tracking</h3>
<ul>
<li><strong>Authors: </strong>Qiming Wang, Yongqiang Bai, Hongxing Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18193">https://arxiv.org/abs/2403.18193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18193">https://arxiv.org/pdf/2403.18193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18193]] Middle Fusion and Multi-Stage, Multi-Form Prompts for Robust RGB-T  Tracking(https://arxiv.org/abs/2403.18193)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>RGB-T tracking, a vital downstream task of object tracking, has made remarkable progress in recent years. Yet, it remains hindered by two major challenges: 1) the trade-off between performance and efficiency; 2) the scarcity of training data. To address the latter challenge, some recent methods employ prompts to fine-tune pre-trained RGB tracking models and leverage upstream knowledge in a parameter-efficient manner. However, these methods inadequately explore modality-independent patterns and disregard the dynamic reliability of different modalities in open scenarios. We propose M3PT, a novel RGB-T prompt tracking method that leverages middle fusion and multi-modal and multi-stage visual prompts to overcome these challenges. We pioneer the use of the middle fusion framework for RGB-T tracking, which achieves a balance between performance and efficiency. Furthermore, we incorporate the pre-trained RGB tracking model into the framework and utilize multiple flexible prompt strategies to adapt the pre-trained model to the comprehensive exploration of uni-modal patterns and the improved modeling of fusion-modal features, harnessing the potential of prompt learning in RGB-T tracking. Our method outperforms the state-of-the-art methods on four challenging benchmarks, while attaining 46.1 fps inference speed.</li>
</ul>

<h3>Title: Looking Beyond What You See: An Empirical Analysis on Subgroup  Intersectional Fairness for Multi-label Chest X-ray Classification Using  Social Determinants of Racial Health Inequities</h3>
<ul>
<li><strong>Authors: </strong>Dana Moukheiber, Saurabh Mahindre, Lama Moukheiber, Mira Moukheiber, Mingchen Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18196">https://arxiv.org/abs/2403.18196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18196">https://arxiv.org/pdf/2403.18196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18196]] Looking Beyond What You See: An Empirical Analysis on Subgroup  Intersectional Fairness for Multi-label Chest X-ray Classification Using  Social Determinants of Racial Health Inequities(https://arxiv.org/abs/2403.18196)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, fair</a></li>
<li><strong>Abstract: </strong>There has been significant progress in implementing deep learning models in disease diagnosis using chest X- rays. Despite these advancements, inherent biases in these models can lead to disparities in prediction accuracy across protected groups. In this study, we propose a framework to achieve accurate diagnostic outcomes and ensure fairness across intersectional groups in high-dimensional chest X- ray multi-label classification. Transcending traditional protected attributes, we consider complex interactions within social determinants, enabling a more granular benchmark and evaluation of fairness. We present a simple and robust method that involves retraining the last classification layer of pre-trained models using a balanced dataset across groups. Additionally, we account for fairness constraints and integrate class-balanced fine-tuning for multi-label settings. The evaluation of our method on the MIMIC-CXR dataset demonstrates that our framework achieves an optimal tradeoff between accuracy and fairness compared to baseline methods.</li>
</ul>

<h3>Title: Few-shot Online Anomaly Detection and Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shenxing Wei, Xing Wei, Zhiheng Ma, Songlin Dong, Shaochen Zhang, Yihong Gong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18201">https://arxiv.org/abs/2403.18201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18201">https://arxiv.org/pdf/2403.18201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18201]] Few-shot Online Anomaly Detection and Segmentation(https://arxiv.org/abs/2403.18201)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Detecting anomaly patterns from images is a crucial artificial intelligence technique in industrial applications. Recent research in this domain has emphasized the necessity of a large volume of training data, overlooking the practical scenario where, post-deployment of the model, unlabeled data containing both normal and abnormal samples can be utilized to enhance the model's performance. Consequently, this paper focuses on addressing the challenging yet practical few-shot online anomaly detection and segmentation (FOADS) task. Under the FOADS framework, models are trained on a few-shot normal dataset, followed by inspection and improvement of their capabilities by leveraging unlabeled streaming data containing both normal and abnormal samples simultaneously. To tackle this issue, we propose modeling the feature distribution of normal images using a Neural Gas network, which offers the flexibility to adapt the topology structure to identify outliers in the data flow. In order to achieve improved performance with limited training samples, we employ multi-scale feature embedding extracted from a CNN pre-trained on ImageNet to obtain a robust representation. Furthermore, we introduce an algorithm that can incrementally update parameters without the need to store previous samples. Comprehensive experimental results demonstrate that our method can achieve substantial performance under the FOADS setting, while ensuring that the time complexity remains within an acceptable range on MVTec AD and BTAD datasets.</li>
</ul>

<h3>Title: Road Obstacle Detection based on Unknown Objectness Scores</h3>
<ul>
<li><strong>Authors: </strong>Chihiro Noguchi, Toshiaki Ohgushi, Masao Yamanaka</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18207">https://arxiv.org/abs/2403.18207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18207">https://arxiv.org/pdf/2403.18207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18207]] Road Obstacle Detection based on Unknown Objectness Scores(https://arxiv.org/abs/2403.18207)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The detection of unknown traffic obstacles is vital to ensure safe autonomous driving. The standard object-detection methods cannot identify unknown objects that are not included under predefined categories. This is because object-detection methods are trained to assign a background label to pixels corresponding to the presence of unknown objects. To address this problem, the pixel-wise anomaly-detection approach has attracted increased research attention. Anomaly-detection techniques, such as uncertainty estimation and perceptual difference from reconstructed images, make it possible to identify pixels of unknown objects as out-of-distribution (OoD) samples. However, when applied to images with many unknowns and complex components, such as driving scenes, these methods often exhibit unstable performance. The purpose of this study is to achieve stable performance for detecting unknown objects by incorporating the object-detection fashions into the pixel-wise anomaly detection methods. To achieve this goal, we adopt a semantic-segmentation network with a sigmoid head that simultaneously provides pixel-wise anomaly scores and objectness scores. Our experimental results show that the objectness scores play an important role in improving the detection performance. Based on these results, we propose a novel anomaly score by integrating these two scores, which we term as unknown objectness score. Quantitative evaluations show that the proposed method outperforms state-of-the-art methods when applied to the publicly available datasets.</li>
</ul>

<h3>Title: NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual  Pretraining and Multi-level Modulation</h3>
<ul>
<li><strong>Authors: </strong>Jingyang Huo, Yikai Wang, Xuelin Qian, Yun Wang, Chong Li, Jianfeng Feng, Yanwei Fu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18211">https://arxiv.org/abs/2403.18211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18211">https://arxiv.org/pdf/2403.18211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18211]] NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual  Pretraining and Multi-level Modulation(https://arxiv.org/abs/2403.18211)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent fMRI-to-image approaches mainly focused on associating fMRI signals with specific conditions of pre-trained diffusion models. These approaches, while producing high-quality images, capture only a limited aspect of the complex information in fMRI signals and offer little detailed control over image creation. In contrast, this paper proposes to directly modulate the generation process of diffusion models using fMRI signals. Our approach, NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI calibrated-encoding, to tackle multi-individual pre-training for a shared latent space to minimize individual difference and enable the subsequent cross-subject training; ii) fMRI-to-image cross-subject pre-training, perceptually learning to guide diffusion model with high- and low-level conditions across different individuals; iii) fMRI-to-image single-subject refining, similar with step ii but focus on adapting to particular individual. NeuroPictor extracts high-level semantic features from fMRI signals that characterizing the visual stimulus and incrementally fine-tunes the diffusion model with a low-level manipulation network to provide precise structural instructions. By training with over 60,000 fMRI-image pairs from various individuals, our model enjoys superior fMRI-to-image decoding capacity, particularly in the within-subject setting, as evidenced in benchmark datasets. Project page: https://jingyanghuo.github.io/neuropictor/.</li>
</ul>

<h3>Title: A Transformer-Based Framework for Payload Malware Detection and  Classification</h3>
<ul>
<li><strong>Authors: </strong>Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18223">https://arxiv.org/abs/2403.18223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18223">https://arxiv.org/pdf/2403.18223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18223]] A Transformer-Based Framework for Payload Malware Detection and  Classification(https://arxiv.org/abs/2403.18223)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>As malicious cyber threats become more sophisticated in breaching computer networks, the need for effective intrusion detection systems (IDSs) becomes crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced to allow IDSs analyze the content of network packets, providing more context for identifying potential threats. IDSs traditionally rely on using anomaly-based and signature-based detection techniques to detect unrecognized and suspicious activity. Deep learning techniques have shown great potential in DPI for IDSs due to their efficiency in learning intricate patterns from the packet content being transmitted through the network. In this paper, we propose a revolutionary DPI algorithm based on transformers adapted for the purpose of detecting malicious traffic with a classifier head. Transformers learn the complex content of sequence data and generalize them well to similar scenarios thanks to their self-attention mechanism. Our proposed method uses the raw payload bytes that represent the packet contents and is deployed as man-in-the-middle. The payload bytes are used to detect malicious packets and classify their types. Experimental results on the UNSW-NB15 and CIC-IOT23 datasets demonstrate that our transformer-based model is effective in distinguishing malicious from benign traffic in the test dataset, attaining an average accuracy of 79\% using binary classification and 72\% on the multi-classification experiment, both using solely payload bytes.</li>
</ul>

<h3>Title: Fourier or Wavelet bases as counterpart self-attention in spikformer for  efficient visual classification</h3>
<ul>
<li><strong>Authors: </strong>Qingyu Wang, Duzhen Zhang, Tilelin Zhang, Bo Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18228">https://arxiv.org/abs/2403.18228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18228">https://arxiv.org/pdf/2403.18228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18228]] Fourier or Wavelet bases as counterpart self-attention in spikformer for  efficient visual classification(https://arxiv.org/abs/2403.18228)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Energy-efficient spikformer has been proposed by integrating the biologically plausible spiking neural network (SNN) and artificial Transformer, whereby the Spiking Self-Attention (SSA) is used to achieve both higher accuracy and lower computational cost. However, it seems that self-attention is not always necessary, especially in sparse spike-form calculation manners. In this paper, we innovatively replace vanilla SSA (using dynamic bases calculating from Query and Key) with spike-form Fourier Transform, Wavelet Transform, and their combinations (using fixed triangular or wavelets bases), based on a key hypothesis that both of them use a set of basis functions for information transformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is proposed and verified in visual classification tasks, including both static image and event-based video datasets. The FWformer can achieve comparable or even higher accuracies ($0.4\%$-$1.5\%$), higher running speed ($9\%$-$51\%$ for training and $19\%$-$70\%$ for inference), reduced theoretical energy consumption ($20\%$-$25\%$), and reduced GPU memory usage ($4\%$-$26\%$), compared to the standard spikformer. Our result indicates the continuous refinement of new Transformers, that are inspired either by biological discovery (spike-form), or information theory (Fourier or Wavelet Transform), is promising.</li>
</ul>

<h3>Title: TAFormer: A Unified Target-Aware Transformer for Video and Motion Joint  Prediction in Aerial Scenes</h3>
<ul>
<li><strong>Authors: </strong>Liangyu Xu, Wanxuan Lu, Hongfeng Yu, Yongqiang Mao, Hanbo Bi, Chenglong Liu, Xian Sun, Kun Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18238">https://arxiv.org/abs/2403.18238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18238">https://arxiv.org/pdf/2403.18238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18238]] TAFormer: A Unified Target-Aware Transformer for Video and Motion Joint  Prediction in Aerial Scenes(https://arxiv.org/abs/2403.18238)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>As drone technology advances, using unmanned aerial vehicles for aerial surveys has become the dominant trend in modern low-altitude remote sensing. The surge in aerial video data necessitates accurate prediction for future scenarios and motion states of the interested target, particularly in applications like traffic management and disaster response. Existing video prediction methods focus solely on predicting future scenes (video frames), suffering from the neglect of explicitly modeling target's motion states, which is crucial for aerial video interpretation. To address this issue, we introduce a novel task called Target-Aware Aerial Video Prediction, aiming to simultaneously predict future scenes and motion states of the target. Further, we design a model specifically for this task, named TAFormer, which provides a unified modeling approach for both video and target motion states. Specifically, we introduce Spatiotemporal Attention (STA), which decouples the learning of video dynamics into spatial static attention and temporal dynamic attention, effectively modeling the scene appearance and motion. Additionally, we design an Information Sharing Mechanism (ISM), which elegantly unifies the modeling of video and target motion by facilitating information interaction through two sets of messenger tokens. Moreover, to alleviate the difficulty of distinguishing targets in blurry predictions, we introduce Target-Sensitive Gaussian Loss (TSGL), enhancing the model's sensitivity to both target's position and content. Extensive experiments on UAV123VP and VisDroneVP (derived from single-object tracking datasets) demonstrate the exceptional performance of TAFormer in target-aware video prediction, showcasing its adaptability to the additional requirements of aerial video interpretation for target awareness.</li>
</ul>

<h3>Title: NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,  Reconstruction, and Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruikai Cui, Weizhe Liu, Weixuan Sun, Senbo Wang, Taizhang Shang, Yang Li, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, Hongdong Li, Pan Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18241">https://arxiv.org/abs/2403.18241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18241">https://arxiv.org/pdf/2403.18241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18241]] NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,  Reconstruction, and Generation(https://arxiv.org/abs/2403.18241)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling. To ensure spatial coherence and reduce memory usage, we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3D shape using orthogonal 2D planes. Additionally, we meticulously enforce spatial correspondences across distinct planes using a transformer-based autoencoder structure, promoting the preservation of spatial relationships in the generated 3D shapes. This yields an algorithm that consistently outperforms state-of-the-art 3D shape generation methods on various tasks, including unconditional shape generation, multi-modal shape completion, single-view reconstruction, and text-to-shape synthesis.</li>
</ul>

<h3>Title: An Experimentally Validated Feasible Quantum Protocol for Identity-Based  Signature with Application to Secure Email Communication</h3>
<ul>
<li><strong>Authors: </strong>Tapaswini Mohanty, Vikas Srivastava, Sumit Kumar Debnath, Debasish Roy, Kouichi Sakurai, Sourav Mukhopadhyay</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18247">https://arxiv.org/abs/2403.18247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18247">https://arxiv.org/pdf/2403.18247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18247]] An Experimentally Validated Feasible Quantum Protocol for Identity-Based  Signature with Application to Secure Email Communication(https://arxiv.org/abs/2403.18247)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Digital signatures are one of the simplest cryptographic building blocks that provide appealing security characteristics such as authenticity, unforgeability, and undeniability. In 1984, Shamir developed the first Identity-based signature (IBS) to simplify public key infrastructure and circumvent the need for certificates. It makes the process uncomplicated by enabling users to verify digital signatures using only the identifiers of signers, such as email, phone number, etc. Nearly all existing IBS protocols rely on several theoretical assumption-based hard problems. Unfortunately, these hard problems are unsafe and pose a hazard in the quantum realm. Thus, designing IBS algorithms that can withstand quantum attacks and ensure long-term security is an important direction for future research. Quantum cryptography (QC) is one such approach. In this paper, we propose an IBS based on QC. Our scheme's security is based on the laws of quantum mechanics. It thereby achieves long-term security and provides resistance against quantum attacks. We verify the proposed design's correctness and feasibility by simulating it in a prototype quantum device and the IBM Qiskit quantum simulator. The implementation code in qiskit with Jupyternotebook is provided in the Annexure. Moreover, we discuss the application of our design in secure email communication.</li>
</ul>

<h3>Title: Exploring the Deceptive Power of LLM-Generated Fake News: A Study of  Real-World Detection Challenges</h3>
<ul>
<li><strong>Authors: </strong>Yanshen Sun, Jianfeng He, Limeng Cui, Shuo Lei, Chang-Tien Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18249">https://arxiv.org/abs/2403.18249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18249">https://arxiv.org/pdf/2403.18249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18249]] Exploring the Deceptive Power of LLM-Generated Fake News: A Study of  Real-World Detection Challenges(https://arxiv.org/abs/2403.18249)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings.</li>
</ul>

<h3>Title: Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models</h3>
<ul>
<li><strong>Authors: </strong>Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18252">https://arxiv.org/abs/2403.18252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18252">https://arxiv.org/pdf/2403.18252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18252]] Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models(https://arxiv.org/abs/2403.18252)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Visual representation learning has been a cornerstone in computer vision, evolving from supervised learning with human-annotated labels to aligning image-text pairs from the Internet. Despite recent advancements in multi-modal large language models (MLLMs), the visual representations they rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual reasoning. In this work, we propose Visual Table, a novel visual representation tailored for MLLMs. It provides hierarchical text descriptions of holistic visual scenes, consisting of a scene description and multiple object-centric descriptions that encompass categories, attributes, and knowledge at instance level. We further develop a scalable generator for visual table generation and train it on small-scale annotations from GPT4V. Extensive evaluations demonstrate that, with generated visual tables as additional visual representations, our model can consistently outperform the state-of-the-art (SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone visual representations, our model can closely match or even beat the SOTA MLLMs that are built on CLIP visual embeddings. Our code is available at https://github.com/LaVi-Lab/Visual-Table.</li>
</ul>

<h3>Title: Enhancing Generative Class Incremental Learning Performance with Model  Forgetting Approach</h3>
<ul>
<li><strong>Authors: </strong>Taro Togo, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18258">https://arxiv.org/abs/2403.18258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18258">https://arxiv.org/pdf/2403.18258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18258]] Enhancing Generative Class Incremental Learning Performance with Model  Forgetting Approach(https://arxiv.org/abs/2403.18258)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study presents a novel approach to Generative Class Incremental Learning (GCIL) by introducing the forgetting mechanism, aimed at dynamically managing class information for better adaptation to streaming data. GCIL is one of the hot topics in the field of computer vision, and this is considered one of the crucial tasks in society, specifically the continual learning of generative models. The ability to forget is a crucial brain function that facilitates continual learning by selectively discarding less relevant information for humans. However, in the field of machine learning models, the concept of intentionally forgetting has not been extensively investigated. In this study we aim to bridge this gap by incorporating the forgetting mechanisms into GCIL, thereby examining their impact on the models' ability to learn in continual learning. Through our experiments, we have found that integrating the forgetting mechanisms significantly enhances the models' performance in acquiring new knowledge, underscoring the positive role that strategic forgetting plays in the process of continual learning.</li>
</ul>

<h3>Title: DSF-GAN: DownStream Feedback Generative Adversarial Network</h3>
<ul>
<li><strong>Authors: </strong>Oriel Perets, Nadav Rappoport</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18267">https://arxiv.org/abs/2403.18267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18267">https://arxiv.org/pdf/2403.18267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18267]] DSF-GAN: DownStream Feedback Generative Adversarial Network(https://arxiv.org/abs/2403.18267)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>Utility and privacy are two crucial measurements of the quality of synthetic tabular data. While significant advancements have been made in privacy measures, generating synthetic samples with high utility remains challenging. To enhance the utility of synthetic samples, we propose a novel architecture called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This approach incorporates feedback from a downstream prediction model during training to augment the generator's loss function with valuable information. Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of synthetic samples. To evaluate our method, we tested it using two popular datasets. Our experiments demonstrate improved model performance when training on synthetic samples generated by DSF-GAN, compared to those generated by the same GAN architecture without feedback. The evaluation was conducted on the same validation set comprising real samples. All code and datasets used in this research will be made openly available for ease of reproduction.</li>
</ul>

<h3>Title: Unleashing the Potential of SAM for Medical Adaptation via Hierarchical  Decoding</h3>
<ul>
<li><strong>Authors: </strong>Zhiheng Cheng, Qingyue Wei, Hongru Zhu, Yan Wang, Liangqiong Qu, Wei Shao, Yuyin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18271">https://arxiv.org/abs/2403.18271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18271">https://arxiv.org/pdf/2403.18271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18271]] Unleashing the Potential of SAM for Medical Adaptation via Hierarchical  Decoding(https://arxiv.org/abs/2403.18271)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The Segment Anything Model (SAM) has garnered significant attention for its versatile segmentation abilities and intuitive prompt-based interface. However, its application in medical imaging presents challenges, requiring either substantial training costs and extensive medical datasets for full model fine-tuning or high-quality prompts for optimal performance. This paper introduces H-SAM: a prompt-free adaptation of SAM tailored for efficient fine-tuning of medical images via a two-stage hierarchical decoding procedure. In the initial stage, H-SAM employs SAM's original decoder to generate a prior probabilistic mask, guiding a more intricate decoding process in the second stage. Specifically, we propose two key designs: 1) A class-balanced, mask-guided self-attention mechanism addressing the unbalanced label distribution, enhancing image embedding; 2) A learnable mask cross-attention mechanism spatially modulating the interplay among different image regions based on the prior mask. Moreover, the inclusion of a hierarchical pixel decoder in H-SAM enhances its proficiency in capturing fine-grained and localized details. This approach enables SAM to effectively integrate learned medical priors, facilitating enhanced adaptation for medical image segmentation with limited samples. Our H-SAM demonstrates a 4.78% improvement in average Dice compared to existing prompt-free SAM variants for multi-organ segmentation using only 10% of 2D slices. Notably, without using any unlabeled data, H-SAM even outperforms state-of-the-art semi-supervised models relying on extensive unlabeled training data across various medical datasets. Our code is available at https://github.com/Cccccczh404/H-SAM.</li>
</ul>

<h3>Title: BlendX: Complex Multi-Intent Detection with Blended Patterns</h3>
<ul>
<li><strong>Authors: </strong>Yejin Yoon, Jungyeon Lee, Kangsan Kim, Chanhee Park, Taeuk Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18277">https://arxiv.org/abs/2403.18277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18277">https://arxiv.org/pdf/2403.18277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18277]] BlendX: Complex Multi-Intent Detection with Blended Patterns(https://arxiv.org/abs/2403.18277)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Task-oriented dialogue (TOD) systems are commonly designed with the presumption that each utterance represents a single intent. However, this assumption may not accurately reflect real-world situations, where users frequently express multiple intents within a single utterance. While there is an emerging interest in multi-intent detection (MID), existing in-domain datasets such as MixATIS and MixSNIPS have limitations in their formulation. To address these issues, we present BlendX, a suite of refined datasets featuring more diverse patterns than their predecessors, elevating both its complexity and diversity. For dataset construction, we utilize both rule-based heuristics as well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a similarity-driven strategy for utterance selection. To ensure the quality of the proposed datasets, we also introduce three novel metrics that assess the statistical properties of an utterance related to word count, conjunction use, and pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art MID models struggle with the challenges posed by the new datasets, highlighting the need to reexamine the current state of the MID field. The dataset is available at https://github.com/HYU-NLP/BlendX.</li>
</ul>

<h3>Title: AIR-HLoc: Adaptive Image Retrieval for Efficient Visual Localisation</h3>
<ul>
<li><strong>Authors: </strong>Changkun Liu, Huajian Huang, Zhengyang Ma, Tristan Braud</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18281">https://arxiv.org/abs/2403.18281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18281">https://arxiv.org/pdf/2403.18281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18281]] AIR-HLoc: Adaptive Image Retrieval for Efficient Visual Localisation(https://arxiv.org/abs/2403.18281)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>State-of-the-art (SOTA) hierarchical localisation pipelines (HLoc) rely on image retrieval (IR) techniques to establish 2D-3D correspondences by selecting the $k$ most similar images from a reference image database for a given query image. Although higher values of $k$ enhance localisation robustness, the computational cost for feature matching increases linearly with $k$. In this paper, we observe that queries that are the most similar to images in the database result in a higher proportion of feature matches and, thus, more accurate positioning. Thus, a small number of images is sufficient for queries very similar to images in the reference database. We then propose a novel approach, AIR-HLoc, which divides query images into different localisation difficulty levels based on their similarity to the reference image database. We consider an image with high similarity to the reference image as an easy query and an image with low similarity as a hard query. Easy queries show a limited improvement in accuracy when increasing $k$. Conversely, higher values of $k$ significantly improve accuracy for hard queries. Given the limited improvement in accuracy when increasing $k$ for easy queries and the significant improvement for hard queries, we adapt the value of $k$ to the query's difficulty level. Therefore, AIR-HLoc optimizes processing time by adaptively assigning different values of $k$ based on the similarity between the query and reference images without losing accuracy. Our extensive experiments on the Cambridge Landmarks, 7Scenes, and Aachen Day-Night-v1.1 datasets demonstrate our algorithm's efficacy, reducing 30\%, 26\%, and 11\% in computational overhead while maintaining SOTA accuracy compared to HLoc with fixed image retrieval.</li>
</ul>

<h3>Title: Multi-scale Unified Network for Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18294">https://arxiv.org/abs/2403.18294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18294">https://arxiv.org/pdf/2403.18294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18294]] Multi-scale Unified Network for Image Classification(https://arxiv.org/abs/2403.18294)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Convolutional Neural Networks (CNNs) have advanced significantly in visual representation learning and recognition. However, they face notable challenges in performance and computational efficiency when dealing with real-world, multi-scale image inputs. Conventional methods rescale all input images into a fixed size, wherein a larger fixed size favors performance but rescaling small size images to a larger size incurs digitization noise and increased computation cost. In this work, we carry out a comprehensive, layer-wise investigation of CNN models in response to scale variation, based on Centered Kernel Alignment (CKA) analysis. The observations reveal lower layers are more sensitive to input image scale variations than high-level layers. Inspired by this insight, we propose Multi-scale Unified Network (MUSN) consisting of multi-scale subnets, a unified network, and scale-invariant constraint. Our method divides the shallow layers into multi-scale subnets to enable feature extraction from multi-scale inputs, and the low-level features are unified in deep layers for extracting high-level semantic features. A scale-invariant constraint is posed to maintain feature consistency across different scales. Extensive experiments on ImageNet and other scale-diverse datasets, demonstrate that MSUN achieves significant improvements in both model performance and computational efficiency. Particularly, MSUN yields an accuracy increase up to 44.53% and diminishes FLOPs by 7.01-16.13% in multi-scale scenarios.</li>
</ul>

<h3>Title: Dual Instruction Tuning with Large Language Models for Mathematical  Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yongwei Zhou, Tiejun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18295">https://arxiv.org/abs/2403.18295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18295">https://arxiv.org/pdf/2403.18295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18295]] Dual Instruction Tuning with Large Language Models for Mathematical  Reasoning(https://arxiv.org/abs/2403.18295)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements highlight the success of instruction tuning with large language models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical reasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions. To alleviate this problem, we propose a dual instruction tuning strategy to meticulously model mathematical reasoning from both forward and reverse directions. This involves introducing the Intermediate Reasoning State Prediction task (forward reasoning) and the Instruction Reconstruction task (reverse reasoning) to enhance the LLMs' understanding and execution of instructions. Training instances for these tasks are constructed based on existing mathematical instruction tuning datasets. Subsequently, LLMs undergo multi-task fine-tuning using both existing mathematical instructions and the newly created data. Comprehensive experiments validate the effectiveness and domain generalization of the dual instruction tuning strategy across various mathematical reasoning tasks.</li>
</ul>

<h3>Title: GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic  Communication Paradigm</h3>
<ul>
<li><strong>Authors: </strong>Chunhang Zheng, Kechao Cai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18296">https://arxiv.org/abs/2403.18296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18296">https://arxiv.org/pdf/2403.18296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18296]] GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic  Communication Paradigm(https://arxiv.org/abs/2403.18296)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Traditional approaches to semantic communication tasks rely on the knowledge of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these methods necessitate training under specific SNR conditions, entailing considerable time and computational resources. In this paper, we propose GeNet, a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at combating noise, thereby facilitating Task-Oriented Communication (TOC). We propose a novel approach where we first transform the input data image into graph structures. Then we leverage a GNN-based encoder to extract semantic information from the source data. This extracted semantic information is then transmitted through the channel. At the receiver's end, a GNN-based decoder is utilized to reconstruct the relevant semantic information from the source data for TOC. Through experimental evaluation, we show GeNet's effectiveness in anti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's performance by varying the number of nodes, revealing its versatility as a new paradigm for semantic communication. Additionally, we show GeNet's robustness to geometric transformations by testing it with different rotation angles, without resorting to data augmentation.</li>
</ul>

<h3>Title: Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives</h3>
<ul>
<li><strong>Authors: </strong>Shrinivas Ramasubramanian, Harsh Rangwani, Sho Takemori, Kunal Samanta, Yuhei Umeda, Venkatesh Babu Radhakrishnan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18301">https://arxiv.org/abs/2403.18301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18301">https://arxiv.org/pdf/2403.18301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18301]] Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives(https://arxiv.org/abs/2403.18301)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The rise in internet usage has led to the generation of massive amounts of data, resulting in the adoption of various supervised and semi-supervised machine learning algorithms, which can effectively utilize the colossal amount of data to train models. However, before deploying these models in the real world, these must be strictly evaluated on performance measures like worst-case recall and satisfy constraints such as fairness. We find that current state-of-the-art empirical techniques offer sub-optimal performance on these practical, non-decomposable performance objectives. On the other hand, the theoretical techniques necessitate training a new model from scratch for each performance objective. To bridge the gap, we propose SelMix, a selective mixup-based inexpensive fine-tuning technique for pre-trained models, to optimize for the desired objective. The core idea of our framework is to determine a sampling distribution to perform a mixup of features between samples from particular classes such that it optimizes the given objective. We comprehensively evaluate our technique against the existing empirical and theoretically principled methods on standard benchmark datasets for imbalanced classification. We find that proposed SelMix fine-tuning significantly improves the performance for various practical non-decomposable objectives across benchmarks.</li>
</ul>

<h3>Title: Bayesian Learned Models Can Detect Adversarial Malware For Free</h3>
<ul>
<li><strong>Authors: </strong>Bao Gia Doan, Dang Quang Nguyen, Paul Montague, Tamas Abraham, Olivier De Vel, Seyit Camtepe, Salil S. Kanhere, Ehsan Abbasnejad, Damith C. Ranasinghe</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18309">https://arxiv.org/abs/2403.18309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18309">https://arxiv.org/pdf/2403.18309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18309]] Bayesian Learned Models Can Detect Adversarial Malware For Free(https://arxiv.org/abs/2403.18309)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>The vulnerability of machine learning-based malware detectors to adversarial attacks has prompted the need for robust solutions. Adversarial training is an effective method but is computationally expensive to scale up to large datasets and comes at the cost of sacrificing model performance for robustness. We hypothesize that adversarial malware exploits the low-confidence regions of models and can be identified using epistemic uncertainty of ML approaches -- epistemic uncertainty in a machine learning-based malware detector is a result of a lack of similar training samples in regions of the problem space. In particular, a Bayesian formulation can capture the model parameters' distribution and quantify epistemic uncertainty without sacrificing model performance. To verify our hypothesis, we consider Bayesian learning approaches with a mutual information-based formulation to quantify uncertainty and detect adversarial malware in Android, Windows domains and PDF malware. We found, quantifying uncertainty through Bayesian learning methods can defend against adversarial malware. In particular, Bayesian models: (1) are generally capable of identifying adversarial malware in both feature and problem space, (2) can detect concept drift by measuring uncertainty, and (3) with a diversity-promoting approach (or better posterior approximations) lead to parameter instances from the posterior to significantly enhance a detectors' ability.</li>
</ul>

<h3>Title: Uncertainty-Aware SAR ATR: Defending Against Adversarial Attacks via  Bayesian Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Tian Ye, Rajgopal Kannan, Viktor Prasanna, Carl Busart</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18318">https://arxiv.org/abs/2403.18318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18318">https://arxiv.org/pdf/2403.18318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18318]] Uncertainty-Aware SAR ATR: Defending Against Adversarial Attacks via  Bayesian Neural Networks(https://arxiv.org/abs/2403.18318)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Adversarial attacks have demonstrated the vulnerability of Machine Learning (ML) image classifiers in Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR) systems. An adversarial attack can deceive the classifier into making incorrect predictions by perturbing the input SAR images, for example, with a few scatterers attached to the on-ground objects. Therefore, it is critical to develop robust SAR ATR systems that can detect potential adversarial attacks by leveraging the inherent uncertainty in ML classifiers, thereby effectively alerting human decision-makers. In this paper, we propose a novel uncertainty-aware SAR ATR for detecting adversarial attacks. Specifically, we leverage the capability of Bayesian Neural Networks (BNNs) in performing image classification with quantified epistemic uncertainty to measure the confidence for each input SAR image. By evaluating the uncertainty, our method alerts when the input SAR image is likely to be adversarially generated. Simultaneously, we also generate visual explanations that reveal the specific regions in the SAR image where the adversarial scatterers are likely to to be present, thus aiding human decision-making with hints of evidence of adversarial attacks. Experiments on the MSTAR dataset demonstrate that our approach can identify over 80% adversarial SAR images with fewer than 20% false alarms, and our visual explanations can identify up to over 90% of scatterers in an adversarial SAR image.</li>
</ul>

<h3>Title: Quantum Algorithms: A New Frontier in Financial Crime Prevention</h3>
<ul>
<li><strong>Authors: </strong>Abraham Itzhak Weinberg, Alessio Faccia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18322">https://arxiv.org/abs/2403.18322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18322">https://arxiv.org/pdf/2403.18322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18322]] Quantum Algorithms: A New Frontier in Financial Crime Prevention(https://arxiv.org/abs/2403.18322)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Financial crimes fast proliferation and sophistication require novel approaches that provide robust and effective solutions. This paper explores the potential of quantum algorithms in combating financial crimes. It highlights the advantages of quantum computing by examining traditional and Machine Learning (ML) techniques alongside quantum approaches. The study showcases advanced methodologies such as Quantum Machine Learning (QML) and Quantum Artificial Intelligence (QAI) as powerful solutions for detecting and preventing financial crimes, including money laundering, financial crime detection, cryptocurrency attacks, and market manipulation. These quantum approaches leverage the inherent computational capabilities of quantum computers to overcome limitations faced by classical methods. Furthermore, the paper illustrates how quantum computing can support enhanced financial risk management analysis. Financial institutions can improve their ability to identify and mitigate risks, leading to more robust risk management strategies by exploiting the quantum advantage. This research underscores the transformative impact of quantum algorithms on financial risk management. By embracing quantum technologies, organisations can enhance their capabilities to combat evolving threats and ensure the integrity and stability of financial systems.</li>
</ul>

<h3>Title: Privacy-Preserving Distributed Nonnegative Matrix Factorization</h3>
<ul>
<li><strong>Authors: </strong>Ehsan Lari, Reza Arablouei, Stefan Werner</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18326">https://arxiv.org/abs/2403.18326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18326">https://arxiv.org/pdf/2403.18326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18326]] Privacy-Preserving Distributed Nonnegative Matrix Factorization(https://arxiv.org/abs/2403.18326)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>Nonnegative matrix factorization (NMF) is an effective data representation tool with numerous applications in signal processing and machine learning. However, deploying NMF in a decentralized manner over ad-hoc networks introduces privacy concerns due to the conventional approach of sharing raw data among network agents. To address this, we propose a privacy-preserving algorithm for fully-distributed NMF that decomposes a distributed large data matrix into left and right matrix factors while safeguarding each agent's local data privacy. It facilitates collaborative estimation of the left matrix factor among agents and enables them to estimate their respective right factors without exposing raw data. To ensure data privacy, we secure information exchanges between neighboring agents utilizing the Paillier cryptosystem, a probabilistic asymmetric algorithm for public-key cryptography that allows computations on encrypted data without decryption. Simulation results conducted on synthetic and real-world datasets demonstrate the effectiveness of the proposed algorithm in achieving privacy-preserving distributed NMF over ad-hoc networks.</li>
</ul>

<h3>Title: Can LLMs Converse Formally? Automatically Assessing LLMs in Translating  and Interpreting Formal Specifications</h3>
<ul>
<li><strong>Authors: </strong>Rushang Karia, Daksh Dobhal, Daniel Bramblett, Pulkit Verma, Siddharth Srivastava</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18327">https://arxiv.org/abs/2403.18327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18327">https://arxiv.org/pdf/2403.18327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18327]] Can LLMs Converse Formally? Automatically Assessing LLMs in Translating  and Interpreting Formal Specifications(https://arxiv.org/abs/2403.18327)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Stakeholders often describe system requirements using natural language which are then converted to formal syntax by a domain-expert leading to increased design costs. This paper assesses the capabilities of Large Language Models (LLMs) in converting between natural language descriptions and formal specifications. Existing work has evaluated the capabilities of LLMs in generating formal syntax such as source code but such experiments are typically hand-crafted and use problems that are likely to be in the training set of LLMs, and often require human-annotated datasets. We propose an approach that can use two copies of an LLM in conjunction with an off-the-shelf verifier to automatically evaluate its translation abilities without any additional human input. Our approach generates formal syntax using language grammars to automatically generate a dataset. We conduct an empirical evaluation to measure the accuracy of this translation task and show that SOTA LLMs cannot adequately solve this task, limiting their current utility in the design of complex systems.</li>
</ul>

<h3>Title: Tracking-Assisted Object Detection with Event Cameras</h3>
<ul>
<li><strong>Authors: </strong>Ting-Kang Yen, Igor Morawski, Shusil Dangi, Kai He, Chung-Yi Lin, Jia-Fong Yeh, Hung-Ting Su, Winston Hsu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18330">https://arxiv.org/abs/2403.18330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18330">https://arxiv.org/pdf/2403.18330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18330]] Tracking-Assisted Object Detection with Event Cameras(https://arxiv.org/abs/2403.18330)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Event-based object detection has recently garnered attention in the computer vision community due to the exceptional properties of event cameras, such as high dynamic range and no motion blur. However, feature asynchronism and sparsity cause invisible objects due to no relative motion to the camera, posing a significant challenge in the task. Prior works have studied various memory mechanisms to preserve as many features as possible at the current time, guided by temporal clues. While these implicit-learned memories retain some short-term information, they still struggle to preserve long-term features effectively. In this paper, we consider those invisible objects as pseudo-occluded objects and aim to reveal their features. Firstly, we introduce visibility attribute of objects and contribute an auto-labeling algorithm to append additional visibility labels on an existing event camera dataset. Secondly, we exploit tracking strategies for pseudo-occluded objects to maintain their permanence and retain their bounding boxes, even when features have not been available for a very long time. These strategies can be treated as an explicit-learned memory guided by the tracking objective to record the displacements of objects across frames. Lastly, we propose a spatio-temporal feature aggregation module to enrich the latent features and a consistency loss to increase the robustness of the overall pipeline. We conduct comprehensive experiments to verify our method's effectiveness where still objects are retained but real occluded objects are discarded. The results demonstrate that (1) the additional visibility labels can assist in supervised training, and (2) our method outperforms state-of-the-art approaches with a significant improvement of 7.9% absolute mAP.</li>
</ul>

<h3>Title: DODA: Diffusion for Object-detection Domain Adaptation in Agriculture</h3>
<ul>
<li><strong>Authors: </strong>Shuai Xiang, Pieter M. Blok, James Burridge, Haozhou Wang, Wei Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18334">https://arxiv.org/abs/2403.18334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18334">https://arxiv.org/pdf/2403.18334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18334]] DODA: Diffusion for Object-detection Domain Adaptation in Agriculture(https://arxiv.org/abs/2403.18334)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The diverse and high-quality content generated by recent generative models demonstrates the great potential of using synthetic data to train downstream models. However, in vision, especially in objection detection, related areas are not fully explored, the synthetic images are merely used to balance the long tails of existing datasets, and the accuracy of the generated labels is low, the full potential of generative models has not been exploited. In this paper, we propose DODA, a data synthesizer that can generate high-quality object detection data for new domains in agriculture. Specifically, we improve the controllability of layout-to-image through encoding layout as an image, thereby improving the quality of labels, and use a visual encoder to provide visual clues for the diffusion model to decouple visual features from the diffusion model, and empowering the model the ability to generate data in new domains. On the Global Wheat Head Detection (GWHD) Dataset, which is the largest dataset in agriculture and contains diverse domains, using the data synthesized by DODA improves the performance of the object detector by 12.74-17.76 AP$_{50}$ in the domain that was significantly shifted from the training data.</li>
</ul>

<h3>Title: Macroscale fracture surface segmentation via semi-supervised learning  considering the structural similarity</h3>
<ul>
<li><strong>Authors: </strong>Johannes Rosenberger, Johannes Tlatlik, Sebastian Münstermann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18337">https://arxiv.org/abs/2403.18337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18337">https://arxiv.org/pdf/2403.18337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18337]] Macroscale fracture surface segmentation via semi-supervised learning  considering the structural similarity(https://arxiv.org/abs/2403.18337)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>To this date the safety assessment of materials, used for example in the nuclear power sector, commonly relies on a fracture mechanical analysis utilizing macroscopic concepts, where a global load quantity K or J is compared to the materials fracture toughness curve. Part of the experimental effort involved in these concepts is dedicated to the quantitative analysis of fracture surfaces. Within the scope of this study a methodology for the semi-supervised training of deep learning models for fracture surface segmentation on a macroscopic level was established. Therefore, three distinct and unique datasets were created to analyze the influence of structural similarity on the segmentation capability. The structural similarity differs due to the assessed materials and specimen, as well as imaging-induced variance due to fluctuations in image acquisition in different laboratories. The datasets correspond to typical isolated laboratory conditions, complex real-world circumstances, and a curated subset of the two. We implemented a weak-to-strong consistency regularization for semi-supervised learning. On the heterogeneous dataset we were able to train robust and well-generalizing models that learned feature representations from images across different domains without observing a significant drop in prediction quality. Furthermore, our approach reduced the number of labeled images required for training by a factor of 6. To demonstrate the success of our method and the benefit of our approach for the fracture mechanics assessment, we utilized the models for initial crack size measurements with the area average method. For the laboratory setting, the deep learning assisted measurements proved to have the same quality as manual measurements. For models trained on the heterogeneous dataset, very good measurement accuracies with mean deviations smaller than 1 % could be achieved...</li>
</ul>

<h3>Title: IterAlign: Iterative Constitutional Alignment of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiusi Chen, Hongzhi Wen, Sreyashi Nag, Chen Luo, Qingyu Yin, Ruirui Li, Zheng Li, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18341">https://arxiv.org/abs/2403.18341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18341">https://arxiv.org/pdf/2403.18341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18341]] IterAlign: Iterative Constitutional Alignment of Large Language Models(https://arxiv.org/abs/2403.18341)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid development of large language models (LLMs), aligning LLMs with human values and societal norms to ensure their reliability and safety has become crucial. Reinforcement learning with human feedback (RLHF) and Constitutional AI (CAI) have been proposed for LLM alignment. However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming. To overcome these drawbacks, we study constitution-based LLM alignment and propose a data-driven constitution discovery and self-alignment framework called IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM and automatically discovers new constitutions using a stronger LLM. These constitutions are then used to guide self-correction of the base LLM. Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target the alignment gaps in the current LLM. Empirical results on several safety benchmark datasets and multiple base LLMs show that IterAlign successfully improves truthfulness, helpfulness, harmlessness and honesty, improving the LLM alignment by up to $13.5\%$ in harmlessness.</li>
</ul>

<h3>Title: Quantifying and Mitigating Unimodal Biases in Multimodal Large Language  Models: A Causal Perspective</h3>
<ul>
<li><strong>Authors: </strong>Meiqi Chen, Yixin Cao, Yan Zhang, Chaochao Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18346">https://arxiv.org/abs/2403.18346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18346">https://arxiv.org/pdf/2403.18346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18346]] Quantifying and Mitigating Unimodal Biases in Multimodal Large Language  Models: A Causal Perspective(https://arxiv.org/abs/2403.18346)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have facilitated the development of Multimodal LLMs (MLLMs). Despite their impressive capabilities, MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks. To investigate this issue, we propose a causal framework to interpret the biases in Visual Question Answering (VQA) problems. Within our framework, we devise a causal graph to elucidate the predictions of MLLMs on VQA problems, and assess the causal effect of biases through an in-depth causal analysis. Motivated by the causal graph, we introduce a novel MORE dataset, consisting of 12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities, necessitating multi-hop reasoning and the surmounting of unimodal biases. Furthermore, we propose two strategies to mitigate unimodal biases and enhance MLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA) framework for limited-access MLLMs and the refinement of open-source MLLMs through fine-tuning. Extensive quantitative and qualitative experiments offer valuable insights for future research.</li>
</ul>

<h3>Title: Rejection Improves Reliability: Training LLMs to Refuse Unknown  Questions Using RL from Knowledge Feedback</h3>
<ul>
<li><strong>Authors: </strong>Hongshen Xu, Zichen Zhu, Da Ma, Situo Zhang, Shuai Fan, Lu Chen, Kai Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18349">https://arxiv.org/abs/2403.18349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18349">https://arxiv.org/pdf/2403.18349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18349]] Rejection Improves Reliability: Training LLMs to Refuse Unknown  Questions Using RL from Knowledge Feedback(https://arxiv.org/abs/2403.18349)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope. While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms. In this paper, we conduct a comprehensive examination of the role of rejection, introducing the notion of model reliability along with corresponding metrics. These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations. To improve the inherent reliability of LLMs, we present a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically determine the model's knowledge boundary and trains a reliable reward model to encourage the refusal of out-of-knowledge questions. Experimental results on mathematical questions affirm the substantial efficacy of RLKF in significantly enhancing LLM reliability.</li>
</ul>

<h3>Title: Generating Diverse Agricultural Data for Vision-Based Farming  Applications</h3>
<ul>
<li><strong>Authors: </strong>Mikolaj Cieslak, Umabharathi Govindarajan, Alejandro Garcia, Anuradha Chandrashekar, Torsten Hädrich, Aleksander Mendoza-Drosik, Dominik L. Michels, Sören Pirk, Chia-Chun Fu, Wojciech Pałubicki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18351">https://arxiv.org/abs/2403.18351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18351">https://arxiv.org/pdf/2403.18351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18351]] Generating Diverse Agricultural Data for Vision-Based Farming  Applications(https://arxiv.org/abs/2403.18351)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present a specialized procedural model for generating synthetic agricultural scenes, focusing on soybean crops, along with various weeds. This model is capable of simulating distinct growth stages of these plants, diverse soil conditions, and randomized field arrangements under varying lighting conditions. The integration of real-world textures and environmental factors into the procedural generation process enhances the photorealism and applicability of the synthetic data. Our dataset includes 12,000 images with semantic labels, offering a comprehensive resource for computer vision tasks in precision agriculture, such as semantic segmentation for autonomous weed control. We validate our model's effectiveness by comparing the synthetic data against real agricultural images, demonstrating its potential to significantly augment training data for machine learning models in agriculture. This approach not only provides a cost-effective solution for generating high-quality, diverse data but also addresses specific needs in agricultural vision tasks that are not fully covered by general-purpose models.</li>
</ul>

<h3>Title: MonoHair: High-Fidelity Hair Modeling from a Monocular Video</h3>
<ul>
<li><strong>Authors: </strong>Keyu Wu, Lingchen Yang, Zhiyi Kuang, Yao Feng, Xutao Han, Yuefan Shen, Hongbo Fu, Kun Zhou, Youyi Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18356">https://arxiv.org/abs/2403.18356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18356">https://arxiv.org/pdf/2403.18356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18356]] MonoHair: High-Fidelity Hair Modeling from a Monocular Video(https://arxiv.org/abs/2403.18356)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Undoubtedly, high-fidelity 3D hair is crucial for achieving realism, artistic expression, and immersion in computer graphics. While existing 3D hair modeling methods have achieved impressive performance, the challenge of achieving high-quality hair reconstruction persists: they either require strict capture conditions, making practical applications difficult, or heavily rely on learned prior data, obscuring fine-grained details in images. To address these challenges, we propose MonoHair,a generic framework to achieve high-fidelity hair reconstruction from a monocular video, without specific requirements for environments. Our approach bifurcates the hair modeling process into two main stages: precise exterior reconstruction and interior structure inference. The exterior is meticulously crafted using our Patch-based Multi-View Optimization (PMVO). This method strategically collects and integrates hair information from multiple views, independent of prior data, to produce a high-fidelity exterior 3D line map. This map not only captures intricate details but also facilitates the inference of the hair's inner structure. For the interior, we employ a data-driven, multi-view 3D hair reconstruction method. This method utilizes 2D structural renderings derived from the reconstructed exterior, mirroring the synthetic 2D inputs used during training. This alignment effectively bridges the domain gap between our training data and real-world data, thereby enhancing the accuracy and reliability of our interior structure inference. Lastly, we generate a strand model and resolve the directional ambiguity by our hair growth algorithm. Our experiments demonstrate that our method exhibits robustness across diverse hairstyles and achieves state-of-the-art performance. For more results, please refer to our project page https://keyuwu-cs.github.io/MonoHair/.</li>
</ul>

<h3>Title: Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific  Boundaries for Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Ba Hung Ngo, Nhat-Tuong Do-Tran, Tuan-Ngoc Nguyen, Hae-Gon Jeon, Tae Jong Choi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18360">https://arxiv.org/abs/2403.18360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18360">https://arxiv.org/pdf/2403.18360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18360]] Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific  Boundaries for Domain Adaptation(https://arxiv.org/abs/2403.18360)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Most domain adaptation (DA) methods are based on either a convolutional neural networks (CNNs) or a vision transformers (ViTs). They align the distribution differences between domains as encoders without considering their unique characteristics. For instance, ViT excels in accuracy due to its superior ability to capture global representations, while CNN has an advantage in capturing local representations. This fact has led us to design a hybrid method to fully take advantage of both ViT and CNN, called Explicitly Class-specific Boundaries (ECB). ECB learns CNN on ViT to combine their distinct strengths. In particular, we leverage ViT's properties to explicitly find class-specific decision boundaries by maximizing the discrepancy between the outputs of the two classifiers to detect target samples far from the source support. In contrast, the CNN encoder clusters target features based on the previously defined class-specific boundaries by minimizing the discrepancy between the probabilities of the two classifiers. Finally, ViT and CNN mutually exchange knowledge to improve the quality of pseudo labels and reduce the knowledge discrepancies of these models. Compared to conventional DA methods, our ECB achieves superior performance, which verifies its effectiveness in this hybrid model. The project website can be found https://dotrannhattuong.github.io/ECB/website/.</li>
</ul>

<h3>Title: ViTAR: Vision Transformer with Any Resolution</h3>
<ul>
<li><strong>Authors: </strong>Qihang Fan, Quanzeng You, Xiaotian Han, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18361">https://arxiv.org/abs/2403.18361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18361">https://arxiv.org/pdf/2403.18361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18361]] ViTAR: Vision Transformer with Any Resolution(https://arxiv.org/abs/2403.18361)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>his paper tackles a significant challenge faced by Vision Transformers (ViTs): their constrained scalability across different image resolutions. Typically, ViTs experience a performance decline when processing resolutions different from those seen during training. Our work introduces two key innovations to address this issue. Firstly, we propose a novel module for dynamic resolution adjustment, designed with a single Transformer block, specifically to achieve highly efficient incremental token integration. Secondly, we introduce fuzzy positional encoding in the Vision Transformer to provide consistent positional awareness across multiple resolutions, thereby preventing overfitting to any single training resolution. Our resulting model, ViTAR (Vision Transformer with Any Resolution), demonstrates impressive adaptability, achieving 83.3\% top-1 accuracy at a 1120x1120 resolution and 80.4\% accuracy at a 4032x4032 resolution, all while reducing computational costs. ViTAR also shows strong performance in downstream tasks such as instance and semantic segmentation and can easily combined with self-supervised learning techniques like Masked AutoEncoder. Our work provides a cost-effective solution for enhancing the resolution scalability of ViTs, paving the way for more versatile and efficient high-resolution image processing.</li>
</ul>

<h3>Title: BLADE: Enhancing Black-box Large Language Models with Small  Domain-Specific Models</h3>
<ul>
<li><strong>Authors: </strong>Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, Yiqun Liu, Chong Chen, Qi Tian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18365">https://arxiv.org/abs/2403.18365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18365">https://arxiv.org/pdf/2403.18365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18365]] BLADE: Enhancing Black-box Large Language Models with Small  Domain-Specific Models(https://arxiv.org/abs/2403.18365)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable of addressing a diverse range of tasks. However, general LLMs, which are developed on open-domain data, may lack the domain-specific knowledge essential for tasks in vertical domains, such as legal, medical, etc. To address this issue, previous approaches either conduct continuous pre-training with domain-specific data or employ retrieval augmentation to support general LLMs. Unfortunately, these strategies are either cost-intensive or unreliable in practical applications. To this end, we present a novel framework named BLADE, which enhances Black-box LArge language models with small Domain-spEcific models. BLADE consists of a black-box LLM and a small domain-specific LM. The small LM preserves domain-specific knowledge and offers specialized insights, while the general LLM contributes robust language comprehension and reasoning capabilities. Specifically, our method involves three steps: 1) pre-training the small LM with domain-specific data, 2) fine-tuning this model using knowledge instruction data, and 3) joint Bayesian optimization of the general LLM and the small LM. Extensive experiments conducted on public legal and medical benchmarks reveal that BLADE significantly outperforms existing approaches. This shows the potential of BLADE as an effective and cost-efficient solution in adapting general LLMs for vertical domains.</li>
</ul>

<h3>Title: Ship in Sight: Diffusion Models for Ship-Image Super Resolution</h3>
<ul>
<li><strong>Authors: </strong>Luigi Sigillo, Riccardo Fosco Gramaccioni, Alessandro Nicolosi, Danilo Comminiello</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18370">https://arxiv.org/abs/2403.18370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18370">https://arxiv.org/pdf/2403.18370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18370]] Ship in Sight: Diffusion Models for Ship-Image Super Resolution(https://arxiv.org/abs/2403.18370)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>In recent years, remarkable advancements have been achieved in the field of image generation, primarily driven by the escalating demand for high-quality outcomes across various image generation subtasks, such as inpainting, denoising, and super resolution. A major effort is devoted to exploring the application of super-resolution techniques to enhance the quality of low-resolution images. In this context, our method explores in depth the problem of ship image super resolution, which is crucial for coastal and port surveillance. We investigate the opportunity given by the growing interest in text-to-image diffusion models, taking advantage of the prior knowledge that such foundation models have already learned. In particular, we present a diffusion-model-based architecture that leverages text conditioning during training while being class-aware, to best preserve the crucial details of the ships during the generation of the super-resoluted image. Since the specificity of this task and the scarcity availability of off-the-shelf data, we also introduce a large labeled ship dataset scraped from online ship images, mostly from ShipSpotting\footnote{\url{www.shipspotting.com}} website. Our method achieves more robust results than other deep learning models previously employed for super resolution, as proven by the multiple experiments performed. Moreover, we investigate how this model can benefit downstream tasks, such as classification and object detection, thus emphasizing practical implementation in a real-world scenario. Experimental results show flexibility, reliability, and impressive performance of the proposed framework over state-of-the-art methods for different tasks. The code is available at: https://github.com/LuigiSigillo/ShipinSight .</li>
</ul>

<h3>Title: Stragglers-Aware Low-Latency Synchronous Federated Learning via  Layer-Wise Model Updates</h3>
<ul>
<li><strong>Authors: </strong>Natalie Lang, Alejandro Cohen, Nir Shlezinger</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18375">https://arxiv.org/abs/2403.18375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18375">https://arxiv.org/pdf/2403.18375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18375]] Stragglers-Aware Low-Latency Synchronous Federated Learning via  Layer-Wise Model Updates(https://arxiv.org/abs/2403.18375)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Synchronous federated learning (FL) is a popular paradigm for collaborative edge learning. It typically involves a set of heterogeneous devices locally training neural network (NN) models in parallel with periodic centralized aggregations. As some of the devices may have limited computational resources and varying availability, FL latency is highly sensitive to stragglers. Conventional approaches discard incomplete intra-model updates done by stragglers, alter the amount of local workload and architecture, or resort to asynchronous settings; which all affect the trained model performance under tight training latency constraints. In this work, we propose straggler-aware layer-wise federated learning (SALF) that leverages the optimization procedure of NNs via backpropagation to update the global model in a layer-wise fashion. SALF allows stragglers to synchronously convey partial gradients, having each layer of the global model be updated independently with a different contributing set of users. We provide a theoretical analysis, establishing convergence guarantees for the global model under mild assumptions on the distribution of the participating devices, revealing that SALF converges at the same asymptotic rate as FL with no timing limitations. This insight is matched with empirical observations, demonstrating the performance gains of SALF compared to alternative mechanisms mitigating the device heterogeneity gap in FL.</li>
</ul>

<h3>Title: IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining  Useful Life Prediction</h3>
<ul>
<li><strong>Authors: </strong>Guangzai Ye, Li Feng, Jianlan Guo, Yuqiang Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18379">https://arxiv.org/abs/2403.18379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18379">https://arxiv.org/pdf/2403.18379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18379]] IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining  Useful Life Prediction(https://arxiv.org/abs/2403.18379)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurately estimating the Remaining Useful Life (RUL) of lithium-ion batteries is crucial for maintaining the safe and stable operation of rechargeable battery management systems. However, this task is often challenging due to the complex temporal dynamics involved. Recently, attention-based networks, such as Transformers and Informer, have been the popular architecture in time series forecasting. Despite their effectiveness, these models with abundant parameters necessitate substantial training time to unravel temporal patterns. To tackle these challenges, we propose a simple MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which is an architecture based exclusively on multi-layer perceptrons (MLPs), extracting information by mixing operations along both intra-patch and inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer comprises parallel dual-head mixer layers: the intra-patch mixing MLP, capturing local temporal patterns in the short-term period, and the inter-patch mixing MLP, capturing global temporal patterns in the long-term period. Notably, to address the varying importance of features in RUL prediction, we introduce a weighted loss function in the MLP-Mixer-based architecture, marking the first time such an approach has been employed. Our experiments demonstrate that IIP-Mixer achieves competitive performance in battery RUL prediction, outperforming other popular time-series frameworks</li>
</ul>

<h3>Title: Improving Attributed Text Generation of Large Language Models via  Preference Learning</h3>
<ul>
<li><strong>Authors: </strong>Dongfang Li, Zetian Sun, Baotian Hu, Zhenyu Liu, Xinshuo Hu, Xuebo Liu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18381">https://arxiv.org/abs/2403.18381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18381">https://arxiv.org/pdf/2403.18381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18381]] Improving Attributed Text Generation of Large Language Models via  Preference Learning(https://arxiv.org/abs/2403.18381)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have been widely adopted in natural language processing, yet they face the challenge of generating unreliable content. Recent works aim to reduce misinformation and hallucinations by resorting to attribution as a means to provide evidence (i.e., citations). However, current attribution methods usually focus on the retrieval stage and automatic evaluation that neglect mirroring the citation mechanisms in human scholarly writing to bolster credibility. In this paper, we address these challenges by modelling the attribution task as preference learning and introducing an Automatic Preference Optimization (APO) framework. First, we create a curated collection for post-training with 6,330 examples by collecting and filtering from existing datasets. Second, considering the high cost of labelling preference data, we further propose an automatic method to synthesize attribution preference data resulting in 95,263 pairs. Moreover, inspired by the human citation process, we further propose a progressive preference optimization method by leveraging fine-grained information. Extensive experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate that APO achieves state-of-the-art citation F1 with higher answer quality.</li>
</ul>

<h3>Title: Generative Multi-modal Models are Good Class-Incremental Learners</h3>
<ul>
<li><strong>Authors: </strong>Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18383">https://arxiv.org/abs/2403.18383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18383">https://arxiv.org/pdf/2403.18383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18383]] Generative Multi-modal Models are Good Class-Incremental Learners(https://arxiv.org/abs/2403.18383)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic forgetting caused by the classifier's bias towards the current task has long posed a significant challenge. It is mainly caused by the characteristic of discriminative models. With the growing popularity of the generative multi-modal models, we would explore replacing discriminative models with generative ones for CIL. However, transitioning from discriminative to generative models requires addressing two key challenges. The primary challenge lies in transferring the generated textual information into the classification of distinct categories. Additionally, it requires formulating the task of CIL within a generative framework. To this end, we propose a novel generative multi-modal model (GMM) framework for class-incremental learning. Our approach directly generates labels for images using an adapted generative model. After obtaining the detailed text, we use a text encoder to extract text features and employ feature matching to determine the most similar label as the classification prediction. In the conventional CIL settings, we achieve significantly better results in long-sequence task scenarios. Under the Few-shot CIL setting, we have improved by at least 14\% accuracy over all the current state-of-the-art methods with significantly less forgetting. Our code is available at \url{https://github.com/DoubleClass/GMM}.</li>
</ul>

<h3>Title: Colour and Brush Stroke Pattern Recognition in Abstract Art using  Modified Deep Convolutional Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Srinitish Srinivasan, Varenya Pathak</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18397">https://arxiv.org/abs/2403.18397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18397">https://arxiv.org/pdf/2403.18397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18397]] Colour and Brush Stroke Pattern Recognition in Abstract Art using  Modified Deep Convolutional Generative Adversarial Networks(https://arxiv.org/abs/2403.18397)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Abstract Art is an immensely popular, discussed form of art that often has the ability to depict the emotions of an artist. Many researchers have made attempts to study abstract art in the form of edge detection, brush stroke and emotion recognition algorithms using machine and deep learning. This papers describes the study of a wide distribution of abstract paintings using Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and reproduce a distribution enabling researchers and scientists to effectively explore and study the generated image space. However, the challenge lies in developing an efficient GAN architecture that overcomes common training pitfalls. This paper addresses this challenge by introducing a modified-DCGAN (mDCGAN) specifically designed for high-quality artwork generation. The approach involves a thorough exploration of the modifications made, delving into the intricate workings of DCGANs, optimisation techniques, and regularisation methods aimed at improving stability and realism in art generation enabling effective study of generated patterns. The proposed mDCGAN incorporates meticulous adjustments in layer configurations and architectural choices, offering tailored solutions to the unique demands of art generation while effectively combating issues like mode collapse and gradient vanishing. Further this paper explores the generated latent space by performing random walks to understand vector relationships between brush strokes and colours in the abstract art space and a statistical analysis of unstable outputs after a certain period of GAN training and compare its significant difference. These findings validate the effectiveness of the proposed approach, emphasising its potential to revolutionise the field of digital art generation and digital art ecosystem.</li>
</ul>

<h3>Title: On Spectrogram Analysis in a Multiple Classifier Fusion Framework for  Power Grid Classification Using Electric Network Frequency</h3>
<ul>
<li><strong>Authors: </strong>Georgios Tzolopoulos, Christos Korgialas, Constantine Kotropoulos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18402">https://arxiv.org/abs/2403.18402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18402">https://arxiv.org/pdf/2403.18402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18402]] On Spectrogram Analysis in a Multiple Classifier Fusion Framework for  Power Grid Classification Using Electric Network Frequency(https://arxiv.org/abs/2403.18402)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Electric Network Frequency (ENF) serves as a unique signature inherent to power distribution systems. Here, a novel approach for power grid classification is developed, leveraging ENF. Spectrograms are generated from audio and power recordings across different grids, revealing distinctive ENF patterns that aid in grid classification through a fusion of classifiers. Four traditional machine learning classifiers plus a Convolutional Neural Network (CNN), optimized using Neural Architecture Search, are developed for One-vs-All classification. This process generates numerous predictions per sample, which are then compiled and used to train a shallow multi-label neural network specifically designed to model the fusion process, ultimately leading to the conclusive class prediction for each sample. Experimental findings reveal that both validation and testing accuracy outperform those of current state-of-the-art classifiers, underlining the effectiveness and robustness of the proposed methodology.</li>
</ul>

<h3>Title: FoC: Figure out the Cryptographic Functions in Stripped Binaries with  LLMs</h3>
<ul>
<li><strong>Authors: </strong>Guoqiang Chen, Xiuwei Shang, Shaoyin Cheng, Yanming Zhang, Weiming Zhang, Nenghai Yu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18403">https://arxiv.org/abs/2403.18403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18403">https://arxiv.org/pdf/2403.18403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18403]] FoC: Figure out the Cryptographic Functions in Stripped Binaries with  LLMs(https://arxiv.org/abs/2403.18403)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Analyzing the behavior of cryptographic functions in stripped binaries is a challenging but essential task. Cryptographic algorithms exhibit greater logical complexity compared to typical code, yet their analysis is unavoidable in areas such as virus analysis and legacy code inspection. Existing methods often rely on data or structural pattern matching, leading to suboptimal generalizability and suffering from manual work. In this paper, we propose a novel framework called FoC to Figure out the Cryptographic functions in stripped binaries. In FoC, we first build a binary large language model (FoCBinLLM) to summarize the semantics of cryptographic functions in natural language. The prediction of FoC-BinLLM is insensitive to minor changes, such as vulnerability patches. To mitigate it, we further build a binary code similarity model (FoC-Sim) upon the FoC-BinLLM to create change-sensitive representations and use it to retrieve similar implementations of unknown cryptographic functions in a database. In addition, we construct a cryptographic binary dataset for evaluation and to facilitate further research in this domain. And an automated method is devised to create semantic labels for extensive binary functions. Evaluation results demonstrate that FoC-BinLLM outperforms ChatGPT by 14.61% on the ROUGE-L score. FoC-Sim outperforms the previous best methods with a 52% higher Recall@1. Furthermore, our method also shows practical ability in virus analysis and 1-day vulnerability detection.</li>
</ul>

<h3>Title: An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering  Using a VLM</h3>
<ul>
<li><strong>Authors: </strong>Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18406">https://arxiv.org/abs/2403.18406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18406">https://arxiv.org/pdf/2403.18406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18406]] An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering  Using a VLM(https://arxiv.org/abs/2403.18406)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Stimulated by the sophisticated reasoning capabilities of recent Large Language Models (LLMs), a variety of strategies for bridging video modality have been devised. A prominent strategy involves Video Language Models (VideoLMs), which train a learnable interface with video data to connect advanced vision encoders with LLMs. Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging. In this study, we introduce a simple yet novel strategy where only a single Vision Language Model (VLM) is utilized. Our starting point is the plain insight that a video comprises a series of images, or frames, interwoven with temporal information. The essence of video comprehension lies in adeptly managing the temporal aspects along with the spatial details of each frame. Initially, we transform a video into a single composite image by arranging multiple frames in a grid layout. The resulting single image is termed as an image grid. This format, while maintaining the appearance of a solitary image, effectively retains temporal information within the grid structure. Therefore, the image grid approach enables direct application of a single high-performance VLM without necessitating any video-data training. Our extensive experimental analysis across ten zero-shot video question answering benchmarks, including five open-ended and five multiple-choice benchmarks, reveals that the proposed Image Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out of ten benchmarks.</li>
</ul>

<h3>Title: The Topos of Transformer Networks</h3>
<ul>
<li><strong>Authors: </strong>Mattia Jacopo Villani, Peter McBurney</a></li>
<li><strong>Subjects: </strong>cs.LG, math.CT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18415">https://arxiv.org/abs/2403.18415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18415">https://arxiv.org/pdf/2403.18415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18415]] The Topos of Transformer Networks(https://arxiv.org/abs/2403.18415)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The transformer neural network has significantly out-shined all other neural network architectures as the engine behind large language models. We provide a theoretical analysis of the expressivity of the transformer architecture through the lens of topos theory. From this viewpoint, we show that many common neural network architectures, such as the convolutional, recurrent and graph convolutional networks, can be embedded in a pretopos of piecewise-linear functions, but that the transformer necessarily lives in its topos completion. In particular, this suggests that the two network families instantiate different fragments of logic: the former are first order, whereas transformers are higher-order reasoners. Furthermore, we draw parallels with architecture search and gradient descent, integrating our analysis in the framework of cybernetic agents.</li>
</ul>

<h3>Title: ECNet: Effective Controllable Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Sicheng Li, Keqiang Sun, Zhixin Lai, Xiaoshi Wu, Feng Qiu, Haoran Xie, Kazunori Miyata, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18417">https://arxiv.org/abs/2403.18417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18417">https://arxiv.org/pdf/2403.18417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18417]] ECNet: Effective Controllable Text-to-Image Diffusion Models(https://arxiv.org/abs/2403.18417)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>The conditional text-to-image diffusion models have garnered significant attention in recent years. However, the precision of these models is often compromised mainly for two reasons, ambiguous condition input and inadequate condition guidance over single denoising loss. To address the challenges, we introduce two innovative solutions. Firstly, we propose a Spatial Guidance Injector (SGI) which enhances conditional detail by encoding text inputs with precise annotation information. This method directly tackles the issue of ambiguous control inputs by providing clear, annotated guidance to the model. Secondly, to overcome the issue of limited conditional supervision, we introduce Diffusion Consistency Loss (DCL), which applies supervision on the denoised latent code at any given time step. This encourages consistency between the latent code at each time step and the input signal, thereby enhancing the robustness and accuracy of the output. The combination of SGI and DCL results in our Effective Controllable Network (ECNet), which offers a more accurate controllable end-to-end text-to-image generation framework with a more precise conditioning input and stronger controllable supervision. We validate our approach through extensive experiments on generation under various conditions, such as human body skeletons, facial landmarks, and sketches of general objects. The results consistently demonstrate that our method significantly enhances the controllability and robustness of the generated images, outperforming existing state-of-the-art controllable text-to-image models.</li>
</ul>

<h3>Title: BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text</h3>
<ul>
<li><strong>Authors: </strong>Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, Christopher D. Manning</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18421">https://arxiv.org/abs/2403.18421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18421">https://arxiv.org/pdf/2403.18421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18421]] BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text(https://arxiv.org/abs/2403.18421)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance on a wide variety of biomedical NLP tasks. However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources. Can smaller, more targeted models compete? To address this question, we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive model trained exclusively on PubMed abstracts and full articles. When fine-tuned, BioMedLM can produce strong multiple-choice biomedical question-answering results competitive with much larger models, such as achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to patient questions on medical topics. This demonstrates that smaller models can potentially serve as transparent, privacy-preserving, economical and environmentally friendly foundations for particular NLP applications, such as in biomedicine. The model is available on the Hugging Face Hub: https://huggingface.co/stanford-crfm/BioMedLM.</li>
</ul>

<h3>Title: SemRoDe: Macro Adversarial Training to Learn Representations That are  Robust to Word-Level Attacks</h3>
<ul>
<li><strong>Authors: </strong>Brian Formento, Wenjie Feng, Chuan Sheng Foo, Luu Anh Tuan, See-Kiong Ng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18423">https://arxiv.org/abs/2403.18423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18423">https://arxiv.org/pdf/2403.18423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18423]] SemRoDe: Macro Adversarial Training to Learn Representations That are  Robust to Word-Level Attacks(https://arxiv.org/abs/2403.18423)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Language models (LMs) are indispensable tools for natural language processing tasks, but their vulnerability to adversarial attacks remains a concern. While current research has explored adversarial training techniques, their improvements to defend against word-level attacks have been limited. In this work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing inspiration from recent studies in the image domain, we investigate and later confirm that in a discrete data setting such as language, adversarial samples generated via word substitutions do indeed belong to an adversarial domain exhibiting a high Wasserstein distance from the base domain. Our method learns a robust representation that bridges these two domains. We hypothesize that if samples were not projected into an adversarial domain, but instead to a domain with minimal shift, it would improve attack robustness. We align the domains by incorporating a new distance-based objective. With this, our model is able to learn more generalized representations by aligning the model's high-level output features and therefore better handling unseen adversarial samples. This method can be generalized across word embeddings, even when they share minimal overlap at both vocabulary and word-substitution levels. To evaluate the effectiveness of our approach, we conduct experiments on BERT and RoBERTa models on three datasets. The results demonstrate promising state-of-the-art robustness.</li>
</ul>

<h3>Title: U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ilias Mitsouras, Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18425">https://arxiv.org/abs/2403.18425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18425">https://arxiv.org/pdf/2403.18425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18425]] U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models(https://arxiv.org/abs/2403.18425)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable performance in text-to-image synthesis, producing realistic and high resolution images that faithfully adhere to the corresponding text-prompts. Despite their great success, they still fall behind in sketch-to-image synthesis tasks, where in addition to text-prompts, the spatial layout of the generated images has to closely follow the outlines of certain reference sketches. Employing an MLP latent edge predictor to guide the spatial layout of the synthesized image by predicting edge maps at each denoising step has been recently proposed. Despite yielding promising results, the pixel-wise operation of the MLP does not take into account the spatial layout as a whole, and demands numerous denoising iterations to produce satisfactory images, leading to time inefficiency. To this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge predictor, which is capable of efficiently capturing both local and global features, as well as spatial correlations between pixels. Moreover, we propose the addition of a sketch simplification network that offers the user the choice of preprocessing and simplifying input sketches for enhanced outputs. The experimental results, corroborated by user feedback, demonstrate that our proposed U-Net latent edge predictor leads to more realistic results, that are better aligned with the spatial outlines of the reference sketches, while drastically reducing the number of required denoising steps and, consequently, the overall execution time.</li>
</ul>

<h3>Title: TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions</h3>
<ul>
<li><strong>Authors: </strong>Jamshid Mozafari, Anubhav Jangra, Adam Jatowt</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18426">https://arxiv.org/abs/2403.18426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18426">https://arxiv.org/pdf/2403.18426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18426]] TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions(https://arxiv.org/abs/2403.18426)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Nowadays, individuals tend to engage in dialogues with Large Language Models, seeking answers to their questions. In times when such answers are readily accessible to anyone, the stimulation and preservation of human's cognitive abilities, as well as the assurance of maintaining good reasoning skills by humans becomes crucial. This study addresses such needs by proposing hints (instead of final answers or before giving answers) as a viable solution. We introduce a framework for the automatic hint generation for factoid questions, employing it to construct TriviaHG, a novel large-scale dataset featuring 160,230 hints corresponding to 16,645 questions from the TriviaQA dataset. Additionally, we present an automatic evaluation method that measures the Convergence and Familiarity quality attributes of hints. To evaluate the TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals to annotate 2,791 hints and tasked 6 humans with answering questions using the provided hints. The effectiveness of hints varied, with success rates of 96%, 78%, and 36% for questions with easy, medium, and hard answers, respectively. Moreover, the proposed automatic evaluation methods showed a robust correlation with annotators' results. Conclusively, the findings highlight three key insights: the facilitative role of hints in resolving unknown questions, the dependence of hint quality on answer difficulty, and the feasibility of employing automatic evaluation methods for hint assessment.</li>
</ul>

<h3>Title: Collaborative Active Learning in Conditional Trust Environment</h3>
<ul>
<li><strong>Authors: </strong>Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18436">https://arxiv.org/abs/2403.18436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18436">https://arxiv.org/pdf/2403.18436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18436]] Collaborative Active Learning in Conditional Trust Environment(https://arxiv.org/abs/2403.18436)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate collaborative active learning, a paradigm in which multiple collaborators explore a new domain by leveraging their combined machine learning capabilities without disclosing their existing data and models. Instead, the collaborators share prediction results from the new domain and newly acquired labels. This collaboration offers several advantages: (a) it addresses privacy and security concerns by eliminating the need for direct model and data disclosure; (b) it enables the use of different data sources and insights without direct data exchange; and (c) it promotes cost-effectiveness and resource efficiency through shared labeling costs. To realize these benefits, we introduce a collaborative active learning framework designed to fulfill the aforementioned objectives. We validate the effectiveness of the proposed framework through simulations. The results demonstrate that collaboration leads to higher AUC scores compared to independent efforts, highlighting the framework's ability to overcome the limitations of individual models. These findings support the use of collaborative approaches in active learning, emphasizing their potential to enhance outcomes through collective expertise and shared resources. Our work provides a foundation for further research on collaborative active learning and its practical applications in various domains where data privacy, cost efficiency, and model performance are critical considerations.</li>
</ul>

<h3>Title: Global Vegetation Modeling with Pre-Trained Weather Transformers</h3>
<ul>
<li><strong>Authors: </strong>Pascal Janetzky, Florian Gallusser, Simon Hentschel, Andreas Hotho, Anna Krause</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18438">https://arxiv.org/abs/2403.18438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18438">https://arxiv.org/pdf/2403.18438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18438]] Global Vegetation Modeling with Pre-Trained Weather Transformers(https://arxiv.org/abs/2403.18438)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate vegetation models can produce further insights into the complex interaction between vegetation activity and ecosystem processes. Previous research has established that long-term trends and short-term variability of temperature and precipitation affect vegetation activity. Motivated by the recent success of Transformer-based Deep Learning models for medium-range weather forecasting, we adapt the publicly available pre-trained FourCastNet to model vegetation activity while accounting for the short-term dynamics of climate variability. We investigate how the learned global representation of the atmosphere's state can be transferred to model the normalized difference vegetation index (NDVI). Our model globally estimates vegetation activity at a resolution of \SI{0.25}{\degree} while relying only on meteorological data. We demonstrate that leveraging pre-trained weather models improves the NDVI estimates compared to learning an NDVI model from scratch. Additionally, we compare our results to other recent data-driven NDVI modeling approaches from machine learning and ecology literature. We further provide experimental evidence on how much data and training time is necessary to turn FourCastNet into an effective vegetation model. Code and models will be made available upon publication.</li>
</ul>

<h3>Title: Generalized Policy Learning for Smart Grids: FL TRPO Approach</h3>
<ul>
<li><strong>Authors: </strong>Yunxiang Li, Nicolas Mauricio Cuadrado, Samuel Horváth, Martin Takáč</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18439">https://arxiv.org/abs/2403.18439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18439">https://arxiv.org/pdf/2403.18439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18439]] Generalized Policy Learning for Smart Grids: FL TRPO Approach(https://arxiv.org/abs/2403.18439)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>The smart grid domain requires bolstering the capabilities of existing energy management systems; Federated Learning (FL) aligns with this goal as it demonstrates a remarkable ability to train models on heterogeneous datasets while maintaining data privacy, making it suitable for smart grid applications, which often involve disparate data distributions and interdependencies among features that hinder the suitability of linear models. This paper introduces a framework that combines FL with a Trust Region Policy Optimization (FL TRPO) aiming to reduce energy-associated emissions and costs. Our approach reveals latent interconnections and employs personalized encoding methods to capture unique insights, understanding the relationships between features and optimal strategies, allowing our model to generalize to previously unseen data. Experimental results validate the robustness of our approach, affirming its proficiency in effectively learning policy models for smart grid challenges.</li>
</ul>

<h3>Title: FRESCO: Federated Reinforcement Energy System for Cooperative  Optimization</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Mauricio Cuadrado, Roberto Alejandro Gutierrez, Martin Takáč</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18444">https://arxiv.org/abs/2403.18444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18444">https://arxiv.org/pdf/2403.18444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18444]] FRESCO: Federated Reinforcement Energy System for Cooperative  Optimization(https://arxiv.org/abs/2403.18444)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>The rise in renewable energy is creating new dynamics in the energy grid that promise to create a cleaner and more participative energy grid, where technology plays a crucial part in making the required flexibility to achieve the vision of the next-generation grid. This work presents FRESCO, a framework that aims to ease the implementation of energy markets using a hierarchical control architecture of reinforcement learning agents trained using federated learning. The core concept we are proving is that having greedy agents subject to changing conditions from a higher level agent creates a cooperative setup that will allow for fulfilling all the individual objectives. This paper presents a general overview of the framework, the current progress, and some insights we obtained from the recent results.</li>
</ul>

<h3>Title: Can Language Beat Numerical Regression? Language-Based Multimodal  Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Inhwan Bae, Junoh Lee, Hae-Gon Jeon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18447">https://arxiv.org/abs/2403.18447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18447">https://arxiv.org/pdf/2403.18447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18447]] Can Language Beat Numerical Regression? Language-Based Multimodal  Trajectory Prediction(https://arxiv.org/abs/2403.18447)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Language models have demonstrated impressive ability in context understanding and generative performance. Inspired by the recent success of language foundation models, in this paper, we propose LMTraj (Language-based Multimodal Trajectory predictor), which recasts the trajectory prediction task into a sort of question-answering problem. Departing from traditional numerical regression models, which treat the trajectory coordinate sequence as continuous signals, we consider them as discrete signals like text prompts. Specially, we first transform an input space for the trajectory coordinate into the natural language space. Here, the entire time-series trajectories of pedestrians are converted into a text prompt, and scene images are described as text information through image captioning. The transformed numerical and image data are then wrapped into the question-answering template for use in a language model. Next, to guide the language model in understanding and reasoning high-level knowledge, such as scene context and social relationships between pedestrians, we introduce an auxiliary multi-task question and answering. We then train a numerical tokenizer with the prompt data. We encourage the tokenizer to separate the integer and decimal parts well, and leverage it to capture correlations between the consecutive numbers in the language model. Lastly, we train the language model using the numerical tokenizer and all of the question-answer prompts. Here, we propose a beam-search-based most-likely prediction and a temperature-based multimodal prediction to implement both deterministic and stochastic inferences. Applying our LMTraj, we show that the language-based model can be a powerful pedestrian trajectory predictor, and outperforms existing numerical-based predictor methods. Code is publicly available at https://github.com/inhwanbae/LMTrajectory .</li>
</ul>

<h3>Title: CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in  Resource-Constrained CPS and IoT</h3>
<ul>
<li><strong>Authors: </strong>Yi Hu, Jinhang Zuo, Alanis Zhao, Bob Iannucci, Carlee Joe-Wong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18451">https://arxiv.org/abs/2403.18451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18451">https://arxiv.org/pdf/2403.18451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18451]] CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in  Resource-Constrained CPS and IoT(https://arxiv.org/abs/2403.18451)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Foundation models (FMs) emerge as a promising solution to harness distributed and diverse environmental data by leveraging prior knowledge to understand the complicated temporal and spatial correlations within heterogeneous datasets. Unlike distributed learning frameworks such as federated learning, which often struggle with multimodal data, FMs can transform diverse inputs into embeddings. This process facilitates the integration of information from various modalities and the application of prior learning to new domains. However, deploying FMs in resource-constrained edge systems poses significant challenges. To this end, we introduce CoRAST, a novel learning framework that utilizes FMs for enhanced analysis of distributed, correlated heterogeneous data. Utilizing a server-based FM, CoRAST can exploit existing environment information to extract temporal, spatial, and cross-modal correlations among sensor data. This enables CoRAST to offer context-aware insights for localized client tasks through FM-powered global representation learning. Our evaluation on real-world weather dataset demonstrates CoRAST's ability to exploit correlated heterogeneous data through environmental representation learning to reduce the forecast errors by up to 50.3% compared to the baselines.</li>
</ul>

<h3>Title: SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Inhwan Bae, Young-Jae Park, Hae-Gon Jeon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18452">https://arxiv.org/abs/2403.18452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18452">https://arxiv.org/pdf/2403.18452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18452]] SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model(https://arxiv.org/abs/2403.18452)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>There are five types of trajectory prediction tasks: deterministic, stochastic, domain adaptation, momentary observation, and few-shot. These associated tasks are defined by various factors, such as the length of input paths, data split and pre-processing methods. Interestingly, even though they commonly take sequential coordinates of observations as input and infer future paths in the same coordinates as output, designing specialized architectures for each task is still necessary. For the other task, generality issues can lead to sub-optimal performances. In this paper, we propose SingularTrajectory, a diffusion-based universal trajectory prediction framework to reduce the performance gap across the five tasks. The core of SingularTrajectory is to unify a variety of human dynamics representations on the associated tasks. To do this, we first build a Singular space to project all types of motion patterns from each task into one embedding space. We next propose an adaptive anchor working in the Singular space. Unlike traditional fixed anchor methods that sometimes yield unacceptable paths, our adaptive anchor enables correct anchors, which are put into a wrong location, based on a traversability map. Finally, we adopt a diffusion-based predictor to further enhance the prototype paths using a cascaded denoising process. Our unified framework ensures the generality across various benchmark settings such as input modality, and trajectory lengths. Extensive experiments on five public benchmarks demonstrate that SingularTrajectory substantially outperforms existing models, highlighting its effectiveness in estimating general dynamics of human movements. Code is publicly available at https://github.com/inhwanbae/SingularTrajectory .</li>
</ul>

<h3>Title: DiffStyler: Diffusion-based Localized Image Style Transfer</h3>
<ul>
<li><strong>Authors: </strong>Shaoxu Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18461">https://arxiv.org/abs/2403.18461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18461">https://arxiv.org/pdf/2403.18461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18461]] DiffStyler: Diffusion-based Localized Image Style Transfer(https://arxiv.org/abs/2403.18461)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image style transfer aims to imbue digital imagery with the distinctive attributes of style targets, such as colors, brushstrokes, shapes, whilst concurrently preserving the semantic integrity of the content. Despite the advancements in arbitrary style transfer methods, a prevalent challenge remains the delicate equilibrium between content semantics and style attributes. Recent developments in large-scale text-to-image diffusion models have heralded unprecedented synthesis capabilities, albeit at the expense of relying on extensive and often imprecise textual descriptions to delineate artistic styles. Addressing these limitations, this paper introduces DiffStyler, a novel approach that facilitates efficient and precise arbitrary image style transfer. DiffStyler lies the utilization of a text-to-image Stable Diffusion model-based LoRA to encapsulate the essence of style targets. This approach, coupled with strategic cross-LoRA feature and attention injection, guides the style transfer process. The foundation of our methodology is rooted in the observation that LoRA maintains the spatial feature consistency of UNet, a discovery that further inspired the development of a mask-wise style transfer technique. This technique employs masks extracted through a pre-trained FastSAM model, utilizing mask prompts to facilitate feature fusion during the denoising process, thereby enabling localized style transfer that preserves the original image's unaffected regions. Moreover, our approach accommodates multiple style targets through the use of corresponding masks. Through extensive experimentation, we demonstrate that DiffStyler surpasses previous methods in achieving a more harmonious balance between content preservation and style integration.</li>
</ul>

<h3>Title: Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain  Adaptive Segmentation of 3D Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Zhimin Yuan, Wankang Zeng, Yanfei Su, Weiquan Liu, Ming Cheng, Yulan Guo, Cheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18469">https://arxiv.org/abs/2403.18469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18469">https://arxiv.org/pdf/2403.18469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18469]] Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain  Adaptive Segmentation of 3D Point Clouds(https://arxiv.org/abs/2403.18469)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to annotating new domains. Self-training is a competitive approach for this task, but its performance is limited by different sensor sampling patterns (i.e., variations in point density) and incomplete training strategies. In this work, we propose a density-guided translator (DGT), which translates point density between domains, and integrates it into a two-stage self-training pipeline named DGT-ST. First, in contrast to existing works that simultaneously conduct data generation and feature/output alignment within unstable adversarial training, we employ the non-learnable DGT to bridge the domain gap at the input level. Second, to provide a well-initialized model for self-training, we propose a category-level adversarial network in stage one that utilizes the prototype to prevent negative transfer. Finally, by leveraging the designs above, a domain-mixed self-training method with source-aware consistency loss is proposed in stage two to narrow the domain gap further. Experiments on two synthetic-to-real segmentation tasks (SynLiDAR $\rightarrow$ semanticKITTI and SynLiDAR $\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms state-of-the-art methods, achieving 9.4$\%$ and 4.3$\%$ mIoU improvements, respectively. Code is available at \url{https://github.com/yuan-zm/DGT-ST}.</li>
</ul>

<h3>Title: DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face  Forgery Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zhongxi Chen, Ke Sun, Ziyin Zhou, Xianming Lin, Xiaoshuai Sun, Liujuan Cao, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18471">https://arxiv.org/abs/2403.18471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18471">https://arxiv.org/pdf/2403.18471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18471]] DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face  Forgery Analysis(https://arxiv.org/abs/2403.18471)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The rapid progress in deep learning has given rise to hyper-realistic facial forgery methods, leading to concerns related to misinformation and security risks. Existing face forgery datasets have limitations in generating high-quality facial images and addressing the challenges posed by evolving generative techniques. To combat this, we present DiffusionFace, the first diffusion-based face forgery dataset, covering various forgery categories, including unconditional and Text Guide facial image generation, Img2Img, Inpaint, and Diffusion-based facial exchange algorithms. Our DiffusionFace dataset stands out with its extensive collection of 11 diffusion models and the high-quality of the generated images, providing essential metadata and a real-world internet-sourced forgery facial image dataset for evaluation. Additionally, we provide an in-depth analysis of the data and introduce practical evaluation protocols to rigorously assess discriminative models' effectiveness in detecting counterfeit facial images, aiming to enhance security in facial image authentication processes. The dataset is available for download at \url{https://github.com/Rapisurazurite/DiffFace}.</li>
</ul>

<h3>Title: Synthesizing EEG Signals from Event-Related Potential Paradigms with  Conditional Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Guido Klein, Pierre Guetschel, Gianluigi Silvestri, Michael Tangermann</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18486">https://arxiv.org/abs/2403.18486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18486">https://arxiv.org/pdf/2403.18486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18486]] Synthesizing EEG Signals from Event-Related Potential Paradigms with  Conditional Diffusion Models(https://arxiv.org/abs/2403.18486)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Data scarcity in the brain-computer interface field can be alleviated through the use of generative models, specifically diffusion models. While diffusion models have previously been successfully applied to electroencephalogram (EEG) data, existing models lack flexibility w.r.t.~sampling or require alternative representations of the EEG data. To overcome these limitations, we introduce a novel approach to conditional diffusion models that utilizes classifier-free guidance to directly generate subject-, session-, and class-specific EEG data. In addition to commonly used metrics, domain-specific metrics are employed to evaluate the specificity of the generated samples. The results indicate that the proposed model can generate EEG data that resembles real data for each subject, session, and class.</li>
</ul>

<h3>Title: I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ayoub Karine, Thibault Napoléon, Maher Jridi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18490">https://arxiv.org/abs/2403.18490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18490">https://arxiv.org/pdf/2403.18490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18490]] I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic  Segmentation(https://arxiv.org/abs/2403.18490)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>This paper proposes a new knowledge distillation method tailored for image semantic segmentation, termed Intra- and Inter-Class Knowledge Distillation (I2CKD). The focus of this method is on capturing and transferring knowledge between the intermediate layers of teacher (cumbersome model) and student (compact model). For knowledge extraction, we exploit class prototypes derived from feature maps. To facilitate knowledge transfer, we employ a triplet loss in order to minimize intra-class variances and maximize inter-class variances between teacher and student prototypes. Consequently, I2CKD enables the student to better mimic the feature representation of the teacher for each class, thereby enhancing the segmentation performance of the compact network. Extensive experiments on three segmentation datasets, i.e., Cityscapes, Pascal VOC and CamVid, using various teacher-student network pairs demonstrate the effectiveness of the proposed method.</li>
</ul>

<h3>Title: Learning in PINNs: Phase transition, total diffusion, and generalization</h3>
<ul>
<li><strong>Authors: </strong>Sokratis J. Anagnostopoulos, Juan Diego Toscano, Nikolaos Stergiopulos, George Em Karniadakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18494">https://arxiv.org/abs/2403.18494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18494">https://arxiv.org/pdf/2403.18494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18494]] Learning in PINNs: Phase transition, total diffusion, and generalization(https://arxiv.org/abs/2403.18494)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We investigate the learning dynamics of fully-connected neural networks through the lens of gradient signal-to-noise ratio (SNR), examining the behavior of first-order optimizers like Adam in non-convex objectives. By interpreting the drift/diffusion phases in the information bottleneck theory, focusing on gradient homogeneity, we identify a third phase termed ``total diffusion", characterized by equilibrium in the learning rates and homogeneous gradients. This phase is marked by an abrupt SNR increase, uniform residuals across the sample space and the most rapid training convergence. We propose a residual-based re-weighting scheme to accelerate this diffusion in quadratic loss functions, enhancing generalization. We also explore the information compression phenomenon, pinpointing a significant saturation-induced compression of activations at the total diffusion phase, with deeper layers experiencing negligible information loss. Supported by experimental data on physics-informed neural networks (PINNs), which underscore the importance of gradient homogeneity due to their PDE-based sample inter-dependence, our findings suggest that recognizing phase transitions could refine ML optimization strategies for improved generalization.</li>
</ul>

<h3>Title: Faster Convergence for Transformer Fine-tuning with Line Search Methods</h3>
<ul>
<li><strong>Authors: </strong>Philip Kenneweg, Leonardo Galli, Tristan Kenneweg, Barbara Hammer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18506">https://arxiv.org/abs/2403.18506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18506">https://arxiv.org/pdf/2403.18506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18506]] Faster Convergence for Transformer Fine-tuning with Line Search Methods(https://arxiv.org/abs/2403.18506)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent works have shown that line search methods greatly increase performance of traditional stochastic gradient descent methods on a variety of datasets and architectures [1], [2]. In this work we succeed in extending line search methods to the novel and highly popular Transformer architecture and dataset domains in natural language processing. More specifically, we combine the Armijo line search with the Adam optimizer and extend it by subdividing the networks architecture into sensible units and perform the line search separately on these local units. Our optimization method outperforms the traditional Adam optimizer and achieves significant performance improvements for small data sets or small training budgets, while performing equal or better for other tested cases. Our work is publicly available as a python package, which provides a hyperparameter-free pytorch optimizer that is compatible with arbitrary network architectures.</li>
</ul>

<h3>Title: Efficient Algorithms for Regularized Nonnegative Scale-invariant  Low-rank Approximation Models</h3>
<ul>
<li><strong>Authors: </strong>Jeremy E. Cohen, Valentin Leplat</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18517">https://arxiv.org/abs/2403.18517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18517">https://arxiv.org/pdf/2403.18517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18517]] Efficient Algorithms for Regularized Nonnegative Scale-invariant  Low-rank Approximation Models(https://arxiv.org/abs/2403.18517)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Regularized nonnegative low-rank approximations such as sparse Nonnegative Matrix Factorization or sparse Nonnegative Tucker Decomposition are an important branch of dimensionality reduction models with enhanced interpretability. However, from a practical perspective, the choice of regularizers and regularization coefficients, as well as the design of efficient algorithms, is challenging because of the multifactor nature of these models and the lack of theory to back these choices. This paper aims at improving upon these issues. By studying a more general model called the Homogeneous Regularized Scale-Invariant, we prove that the scale-invariance inherent to low-rank approximation models causes an implicit regularization with both unexpected beneficial and detrimental effects. This observation allows to better understand the effect of regularization functions in low-rank approximation models, to guide the choice of the regularization hyperparameters, and to design balancing strategies to enhance the convergence speed of dedicated optimization algorithms. Some of these results were already known but restricted to specific instances of regularized low-rank approximations. We also derive a generic Majorization Minimization algorithm that handles many regularized nonnegative low-rank approximations, with convergence guarantees. We showcase our contributions on sparse Nonnegative Matrix Factorization, ridge-regularized Canonical Polyadic decomposition and sparse Nonnegative Tucker Decomposition.</li>
</ul>

<h3>Title: Improving Line Search Methods for Large Scale Neural Network Training</h3>
<ul>
<li><strong>Authors: </strong>Philip Kenneweg, Tristan Kenneweg, Barbara Hammer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18519">https://arxiv.org/abs/2403.18519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18519">https://arxiv.org/pdf/2403.18519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18519]] Improving Line Search Methods for Large Scale Neural Network Training(https://arxiv.org/abs/2403.18519)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In recent studies, line search methods have shown significant improvements in the performance of traditional stochastic gradient descent techniques, eliminating the need for a specific learning rate schedule. In this paper, we identify existing issues in state-of-the-art line search methods, propose enhancements, and rigorously evaluate their effectiveness. We test these methods on larger datasets and more complex data domains than before. Specifically, we improve the Armijo line search by integrating the momentum term from ADAM in its search direction, enabling efficient large-scale training, a task that was previously prone to failure using Armijo line search methods. Our optimization approach outperforms both the previous Armijo implementation and tuned learning rate schedules for Adam. Our evaluation focuses on Transformers and CNNs in the domains of NLP and image data. Our work is publicly available as a Python package, which provides a hyperparameter free Pytorch optimizer.</li>
</ul>

<h3>Title: Safe and Robust Reinforcement-Learning: Principles and Practice</h3>
<ul>
<li><strong>Authors: </strong>Taku Yamagata, Raul Santos-Rodriguez</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18539">https://arxiv.org/abs/2403.18539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18539">https://arxiv.org/pdf/2403.18539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18539]] Safe and Robust Reinforcement-Learning: Principles and Practice(https://arxiv.org/abs/2403.18539)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has shown remarkable success in solving relatively complex tasks, yet the deployment of RL systems in real-world scenarios poses significant challenges related to safety and robustness. This paper aims to identify and further understand those challenges thorough the exploration of the main dimensions of the safe and robust RL landscape, encompassing algorithmic, ethical, and practical considerations. We conduct a comprehensive review of methodologies and open problems that summarizes the efforts in recent years to address the inherent risks associated with RL applications. After discussing and proposing definitions for both safe and robust RL, the paper categorizes existing research works into different algorithmic approaches that enhance the safety and robustness of RL agents. We examine techniques such as uncertainty estimation, optimisation methodologies, exploration-exploitation trade-offs, and adversarial training. Environmental factors, including sim-to-real transfer and domain adaptation, are also scrutinized to understand how RL systems can adapt to diverse and dynamic surroundings. Moreover, human involvement is an integral ingredient of the analysis, acknowledging the broad set of roles that humans can take in this context. Importantly, to aid practitioners in navigating the complexities of safe and robust RL implementation, this paper introduces a practical checklist derived from the synthesized literature. The checklist encompasses critical aspects of algorithm design, training environment considerations, and ethical guidelines. It will serve as a resource for developers and policymakers alike to ensure the responsible deployment of RL systems in many application domains.</li>
</ul>

<h3>Title: Attention-aware semantic relevance predicting Chinese sentence reading</h3>
<ul>
<li><strong>Authors: </strong>Kun Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18542">https://arxiv.org/abs/2403.18542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18542">https://arxiv.org/pdf/2403.18542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18542]] Attention-aware semantic relevance predicting Chinese sentence reading(https://arxiv.org/abs/2403.18542)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>In recent years, several influential computational models and metrics have been proposed to predict how humans comprehend and process sentence. One particularly promising approach is contextual semantic similarity. Inspired by the attention algorithm in Transformer and human memory mechanisms, this study proposes an ``attention-aware'' approach for computing contextual semantic relevance. This new approach takes into account the different contributions of contextual parts and the expectation effect, allowing it to incorporate contextual information fully. The attention-aware approach also facilitates the simulation of existing reading models and evaluate them. The resulting ``attention-aware'' metrics of semantic relevance can more accurately predict fixation durations in Chinese reading tasks recorded in an eye-tracking corpus than those calculated by existing approaches. The study's findings further provide strong support for the presence of semantic preview benefits in Chinese naturalistic reading. Furthermore, the attention-aware metrics of semantic relevance, being memory-based, possess high interpretability from both linguistic and cognitive standpoints, making them a valuable computational tool for modeling eye-movements in reading and further gaining insight into the process of language comprehension. Our approach underscores the potential of these metrics to advance our comprehension of how humans understand and process language, ultimately leading to a better understanding of language comprehension and processing.</li>
</ul>

<h3>Title: CosalPure: Learning Concept from Group Images for Robust Co-Saliency  Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Zhu, Qing Guo, Felix Juefei-Xu, Yihao Huang, Yang Liu, Geguang Pu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18554">https://arxiv.org/abs/2403.18554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18554">https://arxiv.org/pdf/2403.18554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18554]] CosalPure: Learning Concept from Group Images for Robust Co-Saliency  Detection(https://arxiv.org/abs/2403.18554)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Co-salient object detection (CoSOD) aims to identify the common and salient (usually in the foreground) regions across a given group of images. Although achieving significant progress, state-of-the-art CoSODs could be easily affected by some adversarial perturbations, leading to substantial accuracy reduction. The adversarial perturbations can mislead CoSODs but do not change the high-level semantic information (e.g., concept) of the co-salient objects. In this paper, we propose a novel robustness enhancement framework by first learning the concept of the co-salient objects based on the input group images and then leveraging this concept to purify adversarial perturbations, which are subsequently fed to CoSODs for robustness enhancement. Specifically, we propose CosalPure containing two modules, i.e., group-image concept learning and concept-guided diffusion purification. For the first module, we adopt a pre-trained text-to-image diffusion model to learn the concept of co-salient objects within group images where the learned concept is robust to adversarial examples. For the second module, we map the adversarial image to the latent space and then perform diffusion generation by embedding the learned concept into the noise prediction function as an extra condition. Our method can effectively alleviate the influence of the SOTA adversarial attack containing different adversarial patterns, including exposure and noise. The extensive results demonstrate that our method could enhance the robustness of CoSODs significantly.</li>
</ul>

<h3>Title: Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images  with Deep Learning -- A Review</h3>
<ul>
<li><strong>Authors: </strong>Mohammadreza Amirian, Daniel Barco, Ivo Herzig, Frank-Peter Schilling</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18565">https://arxiv.org/abs/2403.18565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18565">https://arxiv.org/pdf/2403.18565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18565]] Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images  with Deep Learning -- A Review(https://arxiv.org/abs/2403.18565)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Deep learning based approaches have been used to improve image quality in cone-beam computed tomography (CBCT), a medical imaging technique often used in applications such as image-guided radiation therapy, implant dentistry or orthopaedics. In particular, while deep learning methods have been applied to reduce various types of CBCT image artifacts arising from motion, metal objects, or low-dose acquisition, a comprehensive review summarizing the successes and shortcomings of these approaches, with a primary focus on the type of artifacts rather than the architecture of neural networks, is lacking in the literature. In this review, the data generation and simulation pipelines, and artifact reduction techniques are specifically investigated for each type of artifact. We provide an overview of deep learning techniques that have successfully been shown to reduce artifacts in 3D, as well as in time-resolved (4D) CBCT through the use of projection- and/or volume-domain optimizations, or by introducing neural networks directly within the CBCT reconstruction algorithms. Research gaps are identified to suggest avenues for future exploration. One of the key findings of this work is an observed trend towards the use of generative models including GANs and score-based or diffusion models, accompanied with the need for more diverse and open training datasets and simulations.</li>
</ul>

<h3>Title: HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional  Synthesis and Sampling of Hand-Object Interactions</h3>
<ul>
<li><strong>Authors: </strong>Hao Xu, Haipeng Li, Yinqiao Wang, Shuaicheng Liu, Chi-Wing Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18575">https://arxiv.org/abs/2403.18575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18575">https://arxiv.org/pdf/2403.18575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18575]] HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional  Synthesis and Sampling of Hand-Object Interactions(https://arxiv.org/abs/2403.18575)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Reconstructing 3D hand mesh robustly from a single image is very challenging, due to the lack of diversity in existing real-world datasets. While data synthesis helps relieve the issue, the syn-to-real gap still hinders its usage. In this work, we present HandBooster, a new approach to uplift the data diversity and boost the 3D hand-mesh reconstruction performance by training a conditional generative space on hand-object interactions and purposely sampling the space to synthesize effective data samples. First, we construct versatile content-aware conditions to guide a diffusion model to produce realistic images with diverse hand appearances, poses, views, and backgrounds; favorably, accurate 3D annotations are obtained for free. Then, we design a novel condition creator based on our similarity-aware distribution sampling strategies to deliberately find novel and realistic interaction poses that are distinctive from the training set. Equipped with our method, several baselines can be significantly improved beyond the SOTA on the HO3D and DexYCB benchmarks. Our code will be released on https://github.com/hxwork/HandBooster_Pytorch.</li>
</ul>

<h3>Title: MisGUIDE : Defense Against Data-Free Deep Learning Model Extraction</h3>
<ul>
<li><strong>Authors: </strong>Mahendra Gurve, Sankar Behera, Satyadev Ahlawat, Yamuna Prasad</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18580">https://arxiv.org/abs/2403.18580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18580">https://arxiv.org/pdf/2403.18580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18580]] MisGUIDE : Defense Against Data-Free Deep Learning Model Extraction(https://arxiv.org/abs/2403.18580)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, extraction, data-free, transformer</a></li>
<li><strong>Abstract: </strong>The rise of Machine Learning as a Service (MLaaS) has led to the widespread deployment of machine learning models trained on diverse datasets. These models are employed for predictive services through APIs, raising concerns about the security and confidentiality of the models due to emerging vulnerabilities in prediction APIs. Of particular concern are model cloning attacks, where individuals with limited data and no knowledge of the training dataset manage to replicate a victim model's functionality through black-box query access. This commonly entails generating adversarial queries to query the victim model, thereby creating a labeled dataset. This paper proposes "MisGUIDE", a two-step defense framework for Deep Learning models that disrupts the adversarial sample generation process by providing a probabilistic response when the query is deemed OOD. The first step employs a Vision Transformer-based framework to identify OOD queries, while the second step perturbs the response for such queries, introducing a probabilistic loss function to MisGUIDE the attackers. The aim of the proposed defense method is to reduce the accuracy of the cloned model while maintaining accuracy on authentic queries. Extensive experiments conducted on two benchmark datasets demonstrate that the proposed framework significantly enhances the resistance against state-of-the-art data-free model extraction in black-box settings.</li>
</ul>

<h3>Title: The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency  Attacks in Computer Vision</h3>
<ul>
<li><strong>Authors: </strong>Andreas Müller, Erwin Quiring</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18587">https://arxiv.org/abs/2403.18587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18587">https://arxiv.org/pdf/2403.18587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18587]] The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency  Attacks in Computer Vision(https://arxiv.org/abs/2403.18587)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Resource efficiency plays an important role for machine learning nowadays. The energy and decision latency are two critical aspects to ensure a sustainable and practical application. Unfortunately, the energy consumption and decision latency are not robust against adversaries. Researchers have recently demonstrated that attackers can compute and submit so-called sponge examples at inference time to increase the energy consumption and decision latency of neural networks. In computer vision, the proposed strategy crafts inputs with less activation sparsity which could otherwise be used to accelerate the computation. In this paper, we analyze the mechanism how these energy-latency attacks reduce activation sparsity. In particular, we find that input uniformity is a key enabler. A uniform image, that is, an image with mostly flat, uniformly colored surfaces, triggers more activations due to a specific interplay of convolution, batch normalization, and ReLU activation. Based on these insights, we propose two new simple, yet effective strategies for crafting sponge examples: sampling images from a probability distribution and identifying dense, yet inconspicuous inputs in natural datasets. We empirically examine our findings in a comprehensive evaluation with multiple image classification models and show that our attack achieves the same sparsity effect as prior sponge-example methods, but at a fraction of computation effort. We also show that our sponge examples transfer between different neural networks. Finally, we discuss applications of our findings for the good by improving efficiency by increasing sparsity.</li>
</ul>

<h3>Title: Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote  Sensing Image Understanding</h3>
<ul>
<li><strong>Authors: </strong>Run Shao, Zhaoyang Zhang, Chao Tao, Yunsheng Zhang, Chengli Peng, Haifeng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18593">https://arxiv.org/abs/2403.18593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18593">https://arxiv.org/pdf/2403.18593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18593]] Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote  Sensing Image Understanding(https://arxiv.org/abs/2403.18593)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>The tokenizer, as one of the fundamental components of large models, has long been overlooked or even misunderstood in visual tasks. One key factor of the great comprehension power of the large language model is that natural language tokenizers utilize meaningful words or subwords as the basic elements of language. In contrast, mainstream visual tokenizers, represented by patch-based methods such as Patch Embed, rely on meaningless rectangular patches as basic elements of vision, which cannot serve as effectively as words or subwords in language. Starting from the essence of the tokenizer, we defined semantically independent regions (SIRs) for vision. We designed a simple HOmogeneous visual tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity, the OPM splits the image into 4*4 pixel seeds and then utilizes the attention mechanism to perceive SIRs. The OVM employs cross-attention to merge seeds within the same SIR. To achieve adaptability, the OVM defines a variable number of learnable vectors as cross-attention queries, allowing for the adjustment of token quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19 classification dataset, and GID5 segmentation dataset for sparse and dense tasks. The results demonstrate that the visual tokens obtained by HOOK correspond to individual objects, which demonstrates homogeneity. HOOK outperformed Patch Embed by 6\% and 10\% in the two tasks and achieved state-of-the-art performance compared to the baselines used for comparison. Compared to Patch Embed, which requires more than one hundred tokens for one image, HOOK requires only 6 and 8 tokens for sparse and dense tasks, respectively, resulting in efficiency improvements of 1.5 to 2.8 times. The code is available at https://github.com/GeoX-Lab/Hook.</li>
</ul>

<h3>Title: FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image  Editing</h3>
<ul>
<li><strong>Authors: </strong>Trong-Tung Nguyen, Duc-Anh Nguyen, Anh Tran, Cuong Pham</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18605">https://arxiv.org/abs/2403.18605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18605">https://arxiv.org/pdf/2403.18605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18605]] FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image  Editing(https://arxiv.org/abs/2403.18605)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, diffusion</a></li>
<li><strong>Abstract: </strong>Our work addresses limitations seen in previous approaches for object-centric editing problems, such as unrealistic results due to shape discrepancies and limited control in object replacement or insertion. To this end, we introduce FlexEdit, a flexible and controllable editing framework for objects where we iteratively adjust latents at each denoising step using our FlexEdit block. Initially, we optimize latents at test time to align with specified object constraints. Then, our framework employs an adaptive mask, automatically extracted during denoising, to protect the background while seamlessly blending new content into the target image. We demonstrate the versatility of FlexEdit in various object editing tasks and curate an evaluation test suite with samples from both real and synthetic images, along with novel evaluation metrics designed for object-centric editing. We conduct extensive experiments on different editing scenarios, demonstrating the superiority of our editing framework over recent advanced text-guided image editing methods. Our project page is published at https://flex-edit.github.io/.</li>
</ul>

<h3>Title: Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic  Learning over Low-power Devices</h3>
<ul>
<li><strong>Authors: </strong>Hanqing Fu, Gaolei Li, Jun Wu, Jianhua Li, Xi Lin, Kai Zhou, Yuchen Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18607">https://arxiv.org/abs/2403.18607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18607">https://arxiv.org/pdf/2403.18607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18607]] Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic  Learning over Low-power Devices(https://arxiv.org/abs/2403.18607)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, federate</a></li>
<li><strong>Abstract: </strong>Federated neuromorphic learning (FedNL) leverages event-driven spiking neural networks and federated learning frameworks to effectively execute intelligent analysis tasks over amounts of distributed low-power devices but also perform vulnerability to poisoning attacks. The threat of backdoor attacks on traditional deep neural networks typically comes from time-invariant data. However, in FedNL, unknown threats may be hidden in time-varying spike signals. In this paper, we start to explore a novel vulnerability of FedNL-based systems with the concept of time division multiplexing, termed Spikewhisper, which allows attackers to evade detection as much as possible, as multiple malicious clients can imperceptibly poison with different triggers at different timeslices. In particular, the stealthiness of Spikewhisper is derived from the time-domain divisibility of global triggers, in which each malicious client pastes only one local trigger to a certain timeslice in the neuromorphic sample, and also the polarity and motion of each local trigger can be configured by attackers. Extensive experiments based on two different neuromorphic datasets demonstrate that the attack success rate of Spikewispher is higher than the temporally centralized attacks. Besides, it is validated that the effect of Spikewispher is sensitive to the trigger duration.</li>
</ul>

<h3>Title: Scalable Lipschitz Estimation for CNNs</h3>
<ul>
<li><strong>Authors: </strong>Yusuf Sulehman, Tingting Mu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18613">https://arxiv.org/abs/2403.18613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18613">https://arxiv.org/pdf/2403.18613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18613]] Scalable Lipschitz Estimation for CNNs(https://arxiv.org/abs/2403.18613)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Estimating the Lipschitz constant of deep neural networks is of growing interest as it is useful for informing on generalisability and adversarial robustness. Convolutional neural networks (CNNs) in particular, underpin much of the recent success in computer vision related applications. However, although existing methods for estimating the Lipschitz constant can be tight, they have limited scalability when applied to CNNs. To tackle this, we propose a novel method to accelerate Lipschitz constant estimation for CNNs. The core idea is to divide a large convolutional block via a joint layer and width-wise partition, into a collection of smaller blocks. We prove an upper-bound on the Lipschitz constant of the larger block in terms of the Lipschitz constants of the smaller blocks. Through varying the partition factor, the resulting method can be adjusted to prioritise either accuracy or scalability and permits parallelisation. We demonstrate an enhanced scalability and comparable accuracy to existing baselines through a range of experiments.</li>
</ul>

<h3>Title: SDSAT: Accelerating LLM Inference through Speculative Decoding with  Semantic Adaptive Tokens</h3>
<ul>
<li><strong>Authors: </strong>Chengbo Liu, Yong Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18647">https://arxiv.org/abs/2403.18647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18647">https://arxiv.org/pdf/2403.18647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18647]] SDSAT: Accelerating LLM Inference through Speculative Decoding with  Semantic Adaptive Tokens(https://arxiv.org/abs/2403.18647)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy. The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the "two-step-draft-then-verify" generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Please refer to https://github.com/hasuoshenyun/SDSAT.</li>
</ul>

<h3>Title: Addressing Data Annotation Challenges in Multiple Sensors: A Solution  for Scania Collected Datasets</h3>
<ul>
<li><strong>Authors: </strong>Ajinkya Khoche, Aron Asefaw, Alejandro Gonzalez, Bogdan Timus, Sina Sharif Mansouri, Patric Jensfelt</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18649">https://arxiv.org/abs/2403.18649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18649">https://arxiv.org/pdf/2403.18649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18649]] Addressing Data Annotation Challenges in Multiple Sensors: A Solution  for Scania Collected Datasets(https://arxiv.org/abs/2403.18649)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Data annotation in autonomous vehicles is a critical step in the development of Deep Neural Network (DNN) based models or the performance evaluation of the perception system. This often takes the form of adding 3D bounding boxes on time-sequential and registered series of point-sets captured from active sensors like Light Detection and Ranging (LiDAR) and Radio Detection and Ranging (RADAR). When annotating multiple active sensors, there is a need to motion compensate and translate the points to a consistent coordinate frame and timestamp respectively. However, highly dynamic objects pose a unique challenge, as they can appear at different timestamps in each sensor's data. Without knowing the speed of the objects, their position appears to be different in different sensor outputs. Thus, even after motion compensation, highly dynamic objects are not matched from multiple sensors in the same frame, and human annotators struggle to add unique bounding boxes that capture all objects. This article focuses on addressing this challenge, primarily within the context of Scania collected datasets. The proposed solution takes a track of an annotated object as input and uses the Moving Horizon Estimation (MHE) to robustly estimate its speed. The estimated speed profile is utilized to correct the position of the annotated box and add boxes to object clusters missed by the original annotation.</li>
</ul>

<h3>Title: Fact Checking Beyond Training Set</h3>
<ul>
<li><strong>Authors: </strong>Payam Karisani, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18671">https://arxiv.org/abs/2403.18671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18671">https://arxiv.org/pdf/2403.18671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18671]] Fact Checking Beyond Training Set(https://arxiv.org/abs/2403.18671)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Evaluating the veracity of everyday claims is time consuming and in some cases requires domain expertise. We empirically demonstrate that the commonly used fact checking pipeline, known as the retriever-reader, suffers from performance deterioration when it is trained on the labeled data from one domain and used in another domain. Afterwards, we delve into each component of the pipeline and propose novel algorithms to address this problem. We propose an adversarial algorithm to make the retriever component robust against distribution shift. Our core idea is to initially train a bi-encoder on the labeled source data, and then, to adversarially train two separate document and claim encoders using unlabeled target data. We then focus on the reader component and propose to train it such that it is insensitive towards the order of claims and evidence documents. Our empirical evaluations support the hypothesis that such a reader shows a higher robustness against distribution shift. To our knowledge, there is no publicly available multi-topic fact checking dataset. Thus, we propose a simple automatic method to re-purpose two well-known fact checking datasets. We then construct eight fact checking scenarios from these datasets, and compare our model to a set of strong baseline models, including recent domain adaptation models that use GPT4 for generating synthetic data.</li>
</ul>

<h3>Title: Deep Learning for Robust and Explainable Models in Computer Vision</h3>
<ul>
<li><strong>Authors: </strong>Mohammadreza Amirian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18674">https://arxiv.org/abs/2403.18674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18674">https://arxiv.org/pdf/2403.18674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18674]] Deep Learning for Robust and Explainable Models in Computer Vision(https://arxiv.org/abs/2403.18674)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in machine and deep learning (ML and DL) research have provided excellent tools for leveraging enormous amounts of data and optimizing huge models with millions of parameters to obtain accurate networks for image processing. These developments open up tremendous opportunities for using artificial intelligence (AI) in the automation and human assisted AI industry. However, as more and more models are deployed and used in practice, many challenges have emerged. This thesis presents various approaches that address robustness and explainability challenges for using ML and DL in practice. Robustness and reliability are the critical components of any model before certification and deployment in practice. Deep convolutional neural networks (CNNs) exhibit vulnerability to transformations of their inputs, such as rotation and scaling, or intentional manipulations as described in the adversarial attack literature. In addition, building trust in AI-based models requires a better understanding of current models and developing methods that are more explainable and interpretable a priori. This thesis presents developments in computer vision models' robustness and explainability. Furthermore, this thesis offers an example of using vision models' feature response visualization (models' interpretations) to improve robustness despite interpretability and robustness being seemingly unrelated in the related research. Besides methodological developments for robust and explainable vision models, a key message of this thesis is introducing model interpretation techniques as a tool for understanding vision models and improving their design and robustness. In addition to the theoretical developments, this thesis demonstrates several applications of ML and DL in different contexts, such as medical imaging and affective computing.</li>
</ul>

<h3>Title: NL-ITI: Optimizing Probing and Intervention for Improvement of ITI  Method</h3>
<ul>
<li><strong>Authors: </strong>Jakub Hoscilowicz, Adam Wiacek, Jan Chojnacki, Adam Cieslak, Leszek Michon, Vitalii Urbanevych, Artur Janicki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18680">https://arxiv.org/abs/2403.18680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18680">https://arxiv.org/pdf/2403.18680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18680]] NL-ITI: Optimizing Probing and Intervention for Improvement of ITI  Method(https://arxiv.org/abs/2403.18680)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLM) are prone to returning false information. It constitutes one of major challenges in the AI field. In our work, we explore paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it identifies attention heads, which contain the highest amount of desired type of knowledge (e.g., truthful). Afterwards, during inference, LLM activations are shifted for chosen subset of attention heads. We further improved the ITI framework by introducing a nonlinear probing and multi-token intervention - Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice benchmarks, including TruthfulQA, on which we report around 14% MC1 metric improvement with respect to the baseline ITI results. NL-ITI achieves also encouraging results on other testsets - on Business Ethics subdomain of MMLU, around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI performs better while being less invasive in the behavior of LLM at the same time (as measured by Kullback-Leibler divergence).</li>
</ul>

<h3>Title: TransFusion: Contrastive Learning with Transformers</h3>
<ul>
<li><strong>Authors: </strong>Huanran Li, Daniel Pimentel-Alarcón</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18681">https://arxiv.org/abs/2403.18681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18681">https://arxiv.org/pdf/2403.18681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18681]] TransFusion: Contrastive Learning with Transformers(https://arxiv.org/abs/2403.18681)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper proposes a novel framework, TransFusion, designed to make the process of contrastive learning more analytical and explainable. TransFusion consists of attention blocks whose softmax being replaced by ReLU, and its final block's weighted-sum operation is truncated to leave the adjacency matrix as the output. The model is trained by minimizing the Jensen-Shannon Divergence between its output and the target affinity matrix, which indicates whether each pair of samples belongs to the same or different classes. The main contribution of TransFusion lies in defining a theoretical limit for answering two fundamental questions in the field: the maximum level of data augmentation and the minimum batch size required for effective contrastive learning. Furthermore, experimental results indicate that TransFusion successfully extracts features that isolate clusters from complex real-world data, leading to improved classification accuracy in downstream tasks.</li>
</ul>

<h3>Title: Annolid: Annotate, Segment, and Track Anything You Need</h3>
<ul>
<li><strong>Authors: </strong>Chen Yang, Thomas A. Cleland</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18690">https://arxiv.org/abs/2403.18690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18690">https://arxiv.org/pdf/2403.18690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18690]] Annolid: Annotate, Segment, and Track Anything You Need(https://arxiv.org/abs/2403.18690)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Annolid is a deep learning-based software package designed for the segmentation, labeling, and tracking of research targets within video files, focusing primarily on animal behavior analysis. Based on state-of-the-art instance segmentation methods, Annolid now harnesses the Cutie video object segmentation model to achieve resilient, markerless tracking of multiple animals from single annotated frames, even in environments in which they may be partially or entirely concealed by environmental features or by one another. Our integration of Segment Anything and Grounding-DINO strategies additionally enables the automatic masking and segmentation of recognizable animals and objects by text command, removing the need for manual annotation. Annolid's comprehensive approach to object segmentation flexibly accommodates a broad spectrum of behavior analysis applications, enabling the classification of diverse behavioral states such as freezing, digging, pup huddling, and social interactions in addition to the tracking of animals and their body parts.</li>
</ul>

<h3>Title: Conditional Wasserstein Distances with Applications in Bayesian OT Flow  Matching</h3>
<ul>
<li><strong>Authors: </strong>Jannis Chemseddine, Paul Hagemann, Christian Wald, Gabriele Steidl</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18705">https://arxiv.org/abs/2403.18705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18705">https://arxiv.org/pdf/2403.18705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18705]] Conditional Wasserstein Distances with Applications in Bayesian OT Flow  Matching(https://arxiv.org/abs/2403.18705)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback--Leibler divergence, this is in general not hold true for the Wasserstein distance. In this paper, we introduce a conditional Wasserstein distance via a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. Interestingly, the dual formulation of the conditional Wasserstein-1 flow resembles losses in the conditional Wasserstein GAN literature in a quite natural way. We derive theoretical properties of the conditional Wasserstein distance, characterize the corresponding geodesics and velocity fields as well as the flow ODEs. Subsequently, we propose to approximate the velocity fields by relaxing the conditional Wasserstein distance. Based on this, we propose an extension of OT Flow Matching for solving Bayesian inverse problems and demonstrate its numerical advantages on an inverse problem and class-conditional image generation.</li>
</ul>

<h3>Title: Dense Vision Transformer Compression with Few Samples</h3>
<ul>
<li><strong>Authors: </strong>Hanxiao Zhang, Yifan Zhou, Guo-Hua Wang, Jianxin Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18708">https://arxiv.org/abs/2403.18708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18708">https://arxiv.org/pdf/2403.18708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18708]] Dense Vision Transformer Compression with Few Samples(https://arxiv.org/abs/2403.18708)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Few-shot model compression aims to compress a large model into a more compact one with only a tiny training set (even without labels). Block-level pruning has recently emerged as a leading technique in achieving high accuracy and low latency in few-shot CNN compression. But, few-shot compression for Vision Transformers (ViT) remains largely unexplored, which presents a new challenge. In particular, the issue of sparse compression exists in traditional CNN few-shot methods, which can only produce very few compressed models of different model sizes. This paper proposes a novel framework for few-shot ViT compression named DC-ViT. Instead of dropping the entire block, DC-ViT selectively eliminates the attention module while retaining and reusing portions of the MLP module. DC-ViT enables dense compression, which outputs numerous compressed models that densely populate the range of model complexity. DC-ViT outperforms state-of-the-art few-shot compression methods by a significant margin of 10 percentage points, along with lower latency in the compression of ViT and its variants.</li>
</ul>

<h3>Title: Mitigating Hallucinations in Large Vision-Language Models with  Instruction Contrastive Decoding</h3>
<ul>
<li><strong>Authors: </strong>Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18715">https://arxiv.org/abs/2403.18715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18715">https://arxiv.org/pdf/2403.18715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18715]] Mitigating Hallucinations in Large Vision-Language Models with  Instruction Contrastive Decoding(https://arxiv.org/abs/2403.18715)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However, their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents. To address this issue, this paper introduces the Instruction Contrastive Decoding (ICD) method, a novel approach designed to reduce hallucinations during LVLM inference. Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules. ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution. Through comprehensive experiments on discriminative benchmarks (POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that ICD significantly mitigates both object-level and attribute-level hallucinations. Moreover, our method not only addresses hallucinations but also significantly enhances the general perception and recognition capabilities of LVLMs.</li>
</ul>

<h3>Title: Statistical testing of random number generators and their improvement  using randomness extraction</h3>
<ul>
<li><strong>Authors: </strong>Cameron Foreman, Richie Yeung, Florian J. Curchod</a></li>
<li><strong>Subjects: </strong>cs.CR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18716">https://arxiv.org/abs/2403.18716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18716">https://arxiv.org/pdf/2403.18716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18716]] Statistical testing of random number generators and their improvement  using randomness extraction(https://arxiv.org/abs/2403.18716)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Random number generators (RNGs) are notoriously hard to build and test, especially in a cryptographic setting. Although one cannot conclusively determine the quality of an RNG by testing the statistical properties of its output alone, running numerical tests is both a powerful verification tool and the only universally applicable method. In this work, we present and make available a comprehensive statistical testing environment (STE) that is based on existing statistical test suites. The STE can be parameterised to run lightweight (i.e. fast) all the way to intensive testing, which goes far beyond what is required by certification bodies. With it, we benchmark the statistical properties of several RNGs, comparing them against each other. We then present and implement a variety of post-processing methods, in the form of randomness extractors, which improve the RNG's output quality under different sets of assumptions and analyse their impact through numerical testing with the STE.</li>
</ul>

<h3>Title: Semi-Supervised Learning for Deep Causal Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18717">https://arxiv.org/abs/2403.18717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18717">https://arxiv.org/pdf/2403.18717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18717]] Semi-Supervised Learning for Deep Causal Generative Models(https://arxiv.org/abs/2403.18717)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Developing models that can answer questions of the form "How would $x$ change if $y$ had been $z$?" is fundamental for advancing medical image analysis. Training causal generative models that address such counterfactual questions, though, currently requires that all relevant variables have been observed and that corresponding labels are available in training data. However, clinical data may not have complete records for all patients and state of the art causal generative models are unable to take full advantage of this. We thus develop, for the first time, a semi-supervised deep causal generative model that exploits the causal relationships between variables to maximise the use of all available data. We explore this in the setting where each sample is either fully labelled or fully unlabelled, as well as the more clinically realistic case of having different labels missing for each sample. We leverage techniques from causal inference to infer missing values and subsequently generate realistic counterfactuals, even for samples with incomplete labels.</li>
</ul>

<h3>Title: Understanding the Learning Dynamics of Alignment with Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Shawn Im, Yixuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18742">https://arxiv.org/abs/2403.18742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18742">https://arxiv.org/pdf/2403.18742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18742]] Understanding the Learning Dynamics of Alignment with Human Feedback(https://arxiv.org/abs/2403.18742)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potentially offensive text; reader discretion is advised.</li>
</ul>

<h3>Title: CheckEval: Robust Evaluation Framework using Large Language Model via  Checklist</h3>
<ul>
<li><strong>Authors: </strong>Yukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, Pilsung Kang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18771">https://arxiv.org/abs/2403.18771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18771">https://arxiv.org/pdf/2403.18771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18771]] CheckEval: Robust Evaluation Framework using Large Language Model via  Checklist(https://arxiv.org/abs/2403.18771)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We introduce CheckEval, a novel evaluation framework using Large Language Models, addressing the challenges of ambiguity and inconsistency in current evaluation methods. CheckEval addresses these challenges by dividing evaluation criteria into detailed sub-aspects and constructing a checklist of Boolean questions for each, simplifying the evaluation. This approach not only renders the process more interpretable but also significantly enhances the robustness and reliability of results by focusing on specific evaluation dimensions. Validated through a focused case study using the SummEval benchmark, CheckEval indicates a strong correlation with human judgments. Furthermore, it demonstrates a highly consistent Inter-Annotator Agreement. These findings highlight the effectiveness of CheckEval for objective, flexible, and precise evaluations. By offering a customizable and interactive framework, CheckEval sets a new standard for the use of LLMs in evaluation, responding to the evolving needs of the field and establishing a clear method for future LLM-based evaluation.</li>
</ul>

<h3>Title: RAW: A Robust and Agile Plug-and-Play Watermark Framework for  AI-Generated Images with Provable Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Xun Xian, Ganghua Wang, Xuan Bi, Jayanth Srinivasa, Ashish Kundu, Mingyi Hong, Jie Ding</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18774">https://arxiv.org/abs/2403.18774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18774">https://arxiv.org/pdf/2403.18774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18774]] RAW: A Robust and Agile Plug-and-Play Watermark Framework for  AI-Generated Images with Provable Guarantees(https://arxiv.org/abs/2403.18774)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Safeguarding intellectual property and preventing potential misuse of AI-generated images are of paramount importance. This paper introduces a robust and agile plug-and-play watermark detection framework, dubbed as RAW. As a departure from traditional encoder-decoder methods, which incorporate fixed binary codes as watermarks within latent representations, our approach introduces learnable watermarks directly into the original image data. Subsequently, we employ a classifier that is jointly trained with the watermark to detect the presence of the watermark. The proposed framework is compatible with various generative architectures and supports on-the-fly watermark injection after training. By incorporating state-of-the-art smoothing techniques, we show that the framework provides provable guarantees regarding the false positive rate for misclassifying a watermarked image, even in the presence of certain adversarial attacks targeting watermark removal. Experiments on a diverse range of images generated by state-of-the-art diffusion models reveal substantial performance enhancements compared to existing approaches. For instance, our method demonstrates a notable increase in AUROC, from 0.48 to 0.82, when compared to state-of-the-art approaches in detecting watermarked images under adversarial attacks, while maintaining image quality, as indicated by closely aligned FID and CLIP scores.</li>
</ul>

<h3>Title: ImageNet-D: Benchmarking Neural Network Robustness on Diffusion  Synthetic Object</h3>
<ul>
<li><strong>Authors: </strong>Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18775">https://arxiv.org/abs/2403.18775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18775">https://arxiv.org/pdf/2403.18775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18775]] ImageNet-D: Benchmarking Neural Network Robustness on Diffusion  Synthetic Object(https://arxiv.org/abs/2403.18775)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models' robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\%. Our work suggests that diffusion models can be an effective source to test vision models. The code and dataset are available at https://github.com/chenshuang-zhang/imagenet_d.</li>
</ul>

<h3>Title: Object Pose Estimation via the Aggregation of Diffusion Features</h3>
<ul>
<li><strong>Authors: </strong>Tianfu Wang, Guosheng Hu, Hongguang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18791">https://arxiv.org/abs/2403.18791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18791">https://arxiv.org/pdf/2403.18791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18791]] Object Pose Estimation via the Aggregation of Diffusion Features(https://arxiv.org/abs/2403.18791)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Estimating the pose of objects from images is a crucial task of 3D scene understanding, and recent approaches have shown promising results on very large benchmarks. However, these methods experience a significant performance drop when dealing with unseen objects. We believe that it results from the limited generalizability of image features. To address this problem, we have an in-depth analysis on the features of diffusion models, e.g. Stable Diffusion, which hold substantial potential for modeling unseen objects. Based on this analysis, we then innovatively introduce these diffusion features for object pose estimation. To achieve this, we propose three distinct architectures that can effectively capture and aggregate diffusion features of different granularity, greatly improving the generalizability of object pose estimation. Our approach outperforms the state-of-the-art methods by a considerable margin on three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our method achieves higher accuracy than the previous best arts on unseen objects: 98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the strong generalizability of our method. Our code is released at https://github.com/Tianfu18/diff-feats-pose.</li>
</ul>

<h3>Title: Long-form factuality in large language models</h3>
<ul>
<li><strong>Authors: </strong>Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18802">https://arxiv.org/abs/2403.18802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18802">https://arxiv.org/pdf/2403.18802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18802]] Long-form factuality in large language models(https://arxiv.org/abs/2403.18802)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall). Empirically, we demonstrate that LLM agents can achieve superhuman rating performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.</li>
</ul>

<h3>Title: Is Modularity Transferable? A Case Study through the Lens of Knowledge  Distillation</h3>
<ul>
<li><strong>Authors: </strong>Mateusz Klimaszewski, Piotr Andruszkiewicz, Alexandra Birch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18804">https://arxiv.org/abs/2403.18804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18804">https://arxiv.org/pdf/2403.18804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18804]] Is Modularity Transferable? A Case Study through the Lens of Knowledge  Distillation(https://arxiv.org/abs/2403.18804)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rise of Modular Deep Learning showcases its potential in various Natural Language Processing applications. Parameter-efficient fine-tuning (PEFT) modularity has been shown to work for various use cases, from domain adaptation to multilingual setups. However, all this work covers the case where the modular components are trained and deployed within one single Pre-trained Language Model (PLM). This model-specific setup is a substantial limitation on the very modularity that modular architectures are trying to achieve. We ask whether current modular approaches are transferable between models and whether we can transfer the modules from more robust and larger PLMs to smaller ones. In this work, we aim to fill this gap via a lens of Knowledge Distillation, commonly used for model compression, and present an extremely straightforward approach to transferring pre-trained, task-specific PEFT modules between same-family PLMs. Moreover, we propose a method that allows the transfer of modules between incompatible PLMs without any change in the inference complexity. The experiments on Named Entity Recognition, Natural Language Inference, and Paraphrase Identification tasks over multiple languages and PEFT methods showcase the initial potential of transferable modularity.</li>
</ul>

<h3>Title: ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth  Estimation</h3>
<ul>
<li><strong>Authors: </strong>Suraj Patni, Aradhye Agarwal, Chetan Arora</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18807">https://arxiv.org/abs/2403.18807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18807">https://arxiv.org/pdf/2403.18807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18807]] ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth  Estimation(https://arxiv.org/abs/2403.18807)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embeddings. Our proposed design establishes a new state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of 0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to 0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%) over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%, 18%, 45%, 9%) by ZoeDepth. The code is available at https://github.com/Aradhye2002/EcoDepth.</li>
</ul>

<h3>Title: Mini-Gemini: Mining the Potential of Multi-modality Vision Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18814">https://arxiv.org/abs/2403.18814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18814">https://arxiv.org/pdf/2403.18814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18814]] Mini-Gemini: Mining the Potential of Multi-modality Vision Language  Models(https://arxiv.org/abs/2403.18814)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini. We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-guided generation. To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count. We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs. In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and generation simultaneously. Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance in several zero-shot benchmarks and even surpasses the developed private models. Code and models are available at https://github.com/dvlab-research/MiniGemini.</li>
</ul>

<h3>Title: Garment3DGen: 3D Garment Stylization and Texture Generation</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Sarafianos, Tuur Stuyck, Xiaoyu Xiang, Yilei Li, Jovan Popovic, Rakesh Ranjan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18816">https://arxiv.org/abs/2403.18816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18816">https://arxiv.org/pdf/2403.18816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18816]] Garment3DGen: 3D Garment Stylization and Texture Generation(https://arxiv.org/abs/2403.18816)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Garment3DGen a new method to synthesize 3D garment assets from a base mesh given a single input image as guidance. Our proposed approach allows users to generate 3D textured clothes based on both real and synthetic images, such as those generated by text prompts. The generated assets can be directly draped and simulated on human bodies. First, we leverage the recent progress of image to 3D diffusion methods to generate 3D garment geometries. However, since these geometries cannot be utilized directly for downstream tasks, we propose to use them as pseudo ground-truth and set up a mesh deformation optimization procedure that deforms a base template mesh to match the generated 3D target. Second, we introduce carefully designed losses that allow the input base mesh to freely deform towards the desired target, yet preserve mesh quality and topology such that they can be simulated. Finally, a texture estimation module generates high-fidelity texture maps that are globally and locally consistent and faithfully capture the input guidance, allowing us to render the generated 3D assets. With Garment3DGen users can generate the textured 3D garment of their choice without the need of artist intervention. One can provide a textual prompt describing the garment they desire to generate a simulation-ready 3D asset. We present a plethora of quantitative and qualitative comparisons on various assets both real and generated and provide use-cases of how one can generate simulation-ready 3D garments.</li>
</ul>

<h3>Title: ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object  Removal and Insertion</h3>
<ul>
<li><strong>Authors: </strong>Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, Yedid Hoshen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18818">https://arxiv.org/abs/2403.18818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18818">https://arxiv.org/pdf/2403.18818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18818]] ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object  Removal and Insertion(https://arxiv.org/abs/2403.18818)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have revolutionized image editing but often generate images that violate physical laws, particularly the effects of objects on the scene, e.g., occlusions, shadows, and reflections. By analyzing the limitations of self-supervised approaches, we propose a practical solution centered on a \q{counterfactual} dataset. Our method involves capturing a scene before and after removing a single object, while minimizing other changes. By fine-tuning a diffusion model on this dataset, we are able to not only remove objects but also their effects on the scene. However, we find that applying this approach for photorealistic object insertion requires an impractically large dataset. To tackle this challenge, we propose bootstrap supervision; leveraging our object removal model trained on a small counterfactual dataset, we synthetically expand this dataset considerably. Our approach significantly outperforms prior methods in photorealistic object removal and insertion, particularly at modeling the effects of objects on the scene.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
