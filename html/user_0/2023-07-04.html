<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h2>privacy</h2>
<h3>Title: Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning. (arXiv:2307.00268v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00268">http://arxiv.org/abs/2307.00268</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00268] Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning](http://arxiv.org/abs/2307.00268) #privacy</code></li>
<li>Summary: <p>Lately, differential privacy (DP) has been introduced in cooperative
multiagent reinforcement learning (CMARL) to safeguard the agents' privacy
against adversarial inference during knowledge sharing. Nevertheless, we argue
that the noise introduced by DP mechanisms may inadvertently give rise to a
novel poisoning threat, specifically in the context of private knowledge
sharing during CMARL, which remains unexplored in the literature. To address
this shortcoming, we present an adaptive, privacy-exploiting, and
evasion-resilient localized poisoning attack (PeLPA) that capitalizes on the
inherent DP-noise to circumvent anomaly detection systems and hinder the
optimal convergence of the CMARL model. We rigorously evaluate our proposed
PeLPA attack in diverse environments, encompassing both non-adversarial and
multiple-adversarial contexts. Our findings reveal that, in a medium-scale
environment, the PeLPA attack with attacker ratios of 20% and 40% can lead to
an increase in average steps to goal by 50.69% and 64.41%, respectively.
Furthermore, under similar conditions, PeLPA can result in a 1.4x and 1.6x
computational time increase in optimal reward attainment and a 1.18x and 1.38x
slower convergence for attacker ratios of 20% and 40%, respectively.
</p></li>
</ul>

<h3>Title: Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD. (arXiv:2307.00310v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00310">http://arxiv.org/abs/2307.00310</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00310] Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD](http://arxiv.org/abs/2307.00310) #privacy</code></li>
<li>Summary: <p>Differentially private stochastic gradient descent (DP-SGD) is the canonical
algorithm for private deep learning. While it is known that its privacy
analysis is tight in the worst-case, several empirical results suggest that
when training on common benchmark datasets, the models obtained leak
significantly less privacy for many datapoints. In this paper, we develop a new
analysis for DP-SGD that captures the intuition that points with similar
neighbors in the dataset enjoy better privacy than outliers. Formally, this is
done by modifying the per-step privacy analysis of DP-SGD to introduce a
dependence on the distribution of model updates computed from a training
dataset. We further develop a new composition theorem to effectively use this
new per-step analysis to reason about an entire training run. Put all together,
our evaluation shows that this novel DP-SGD analysis allows us to now formally
show that DP-SGD leaks significantly less privacy for many datapoints. In
particular, we observe that correctly classified points obtain better privacy
guarantees than misclassified points.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Adversarial Attacks and Defenses on 3D Point Cloud Classification: A Survey. (arXiv:2307.00309v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00309">http://arxiv.org/abs/2307.00309</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00309] Adversarial Attacks and Defenses on 3D Point Cloud Classification: A Survey](http://arxiv.org/abs/2307.00309) #defense</code></li>
<li>Summary: <p>Deep learning has successfully solved a wide range of tasks in 2D vision as a
dominant AI technique. Recently, deep learning on 3D point clouds is becoming
increasingly popular for addressing various tasks in this field. Despite
remarkable achievements, deep learning algorithms are vulnerable to adversarial
attacks. These attacks are imperceptible to the human eye but can easily fool
deep neural networks in the testing and deployment stage. To encourage future
research, this survey summarizes the current progress on adversarial attack and
defense techniques on point cloud classification. This paper first introduces
the principles and characteristics of adversarial attacks and summarizes and
analyzes the adversarial example generation methods in recent years. Besides,
it classifies defense strategies as input transformation, data optimization,
and deep model modification. Finally, it presents several challenging issues
and future research directions in this domain.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Common Knowledge Learning for Generating Transferable Adversarial Examples. (arXiv:2307.00274v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00274">http://arxiv.org/abs/2307.00274</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00274] Common Knowledge Learning for Generating Transferable Adversarial Examples](http://arxiv.org/abs/2307.00274) #attack</code></li>
<li>Summary: <p>This paper focuses on an important type of black-box attacks, i.e.,
transfer-based adversarial attacks, where the adversary generates adversarial
examples by a substitute (source) model and utilize them to attack an unseen
target model, without knowing its information. Existing methods tend to give
unsatisfactory adversarial transferability when the source and target models
are from different types of DNN architectures (e.g. ResNet-18 and Swin
Transformer). In this paper, we observe that the above phenomenon is induced by
the output inconsistency problem. To alleviate this problem while effectively
utilizing the existing DNN models, we propose a common knowledge learning (CKL)
framework to learn better network weights to generate adversarial examples with
better transferability, under fixed network architectures. Specifically, to
reduce the model-specific features and obtain better output distributions, we
construct a multi-teacher framework, where the knowledge is distilled from
different teacher architectures into one student network. By considering that
the gradient of input is usually utilized to generated adversarial examples, we
impose constraints on the gradients between the student and teacher models, to
further alleviate the output inconsistency problem and enhance the adversarial
transferability. Extensive experiments demonstrate that our proposed work can
significantly improve the adversarial transferability.
</p></li>
</ul>

<h3>Title: SecBeam: Securing mmWave Beam Alignment against Beam-Stealing Attacks. (arXiv:2307.00178v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00178">http://arxiv.org/abs/2307.00178</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00178] SecBeam: Securing mmWave Beam Alignment against Beam-Stealing Attacks](http://arxiv.org/abs/2307.00178) #attack</code></li>
<li>Summary: <p>Millimeter wave (mmWave) communications employ narrow-beam directional
communications to compensate for the high path loss at mmWave frequencies.
Compared to their omnidirectional counterparts, an additional step of aligning
the transmitter's and receiver's antennas is required. In current standards
such as 802.11ad, this beam alignment process is implemented via an exhaustive
search through the horizontal plane known as beam sweeping. However, the beam
sweeping process is unauthenticated. As a result, an adversary, Mallory, can
launch an active beam-stealing attack by injecting forged beacons of high
power, forcing the legitimate devices to beamform towards her direction.
Mallory is now in control of the communication link between the two devices,
thus breaking the false sense of security given by the directionality of mmWave
transmissions.
</p></li>
</ul>

<p>Prior works have added integrity protection to beam alignment messages to
prevent forgeries. In this paper, we demonstrate a new beam-stealing attack
that does not require message forging. We show that Mallory can amplify and
relay a beam sweeping frame from her direction without altering its contents.
Intuitively, cryptographic primitives cannot verify physical properties such as
the SNR used in beam selection. We propose a new beam sweeping protocol called
SecBeam that utilizes power/sector randomization and coarse angle-of-arrival
information to detect amplify-and-relay attacks. We demonstrate the security
and performance of SecBeam using an experimental mmWave platform and via
ray-tracing simulations.
</p>

<h2>robust</h2>
<h3>Title: Unsupervised Coordinate-Based Video Denoising. (arXiv:2307.00179v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00179">http://arxiv.org/abs/2307.00179</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00179] Unsupervised Coordinate-Based Video Denoising](http://arxiv.org/abs/2307.00179) #robust</code></li>
<li>Summary: <p>In this paper, we introduce a novel unsupervised video denoising deep
learning approach that can help to mitigate data scarcity issues and shows
robustness against different noise patterns, enhancing its broad applicability.
Our method comprises three modules: a Feature generator creating features maps,
a Denoise-Net generating denoised but slightly blurry reference frames, and a
Refine-Net re-introducing high-frequency details. By leveraging the
coordinate-based network, we can greatly simplify the network structure while
preserving high-frequency details in the denoised video frames. Extensive
experiments on both simulated and real-captured demonstrate that our method can
effectively denoise real-world calcium imaging video sequences without prior
knowledge of noise models and data augmentation during training.
</p></li>
</ul>

<h3>Title: More for Less: Compact Convolutional Transformers Enable Robust Medical Image Classification with Limited Data. (arXiv:2307.00213v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00213">http://arxiv.org/abs/2307.00213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00213] More for Less: Compact Convolutional Transformers Enable Robust Medical Image Classification with Limited Data](http://arxiv.org/abs/2307.00213) #robust</code></li>
<li>Summary: <p>Transformers are very powerful tools for a variety of tasks across domains,
from text generation to image captioning. However, transformers require
substantial amounts of training data, which is often a challenge in biomedical
settings, where high quality labeled data can be challenging or expensive to
obtain. This study investigates the efficacy of Compact Convolutional
Transformers (CCT) for robust medical image classification with limited data,
addressing a key issue faced by conventional Vision Transformers - their
requirement for large datasets. A hybrid of transformers and convolutional
layers, CCTs demonstrate high accuracy on modestly sized datasets. We employed
a benchmark dataset of peripheral blood cell images of eight distinct cell
types, each represented by approximately 2,000 low-resolution (28x28x3 pixel)
samples. Despite the dataset size being smaller than those typically used with
Vision Transformers, we achieved a commendable classification accuracy of
92.49% and a micro-average ROC AUC of 0.9935. The CCT also learned quickly,
exceeding 80% validation accuracy after five epochs. Analysis of per-class
precision, recall, F1, and ROC showed that performance was strong across cell
types. Our findings underscore the robustness of CCTs, indicating their
potential as a solution to data scarcity issues prevalent in biomedical
imaging. We substantiate the applicability of CCTs in data-constrained areas
and encourage further work on CCTs.
</p></li>
</ul>

<h3>Title: SysNoise: Exploring and Benchmarking Training-Deployment System Inconsistency. (arXiv:2307.00280v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00280">http://arxiv.org/abs/2307.00280</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00280] SysNoise: Exploring and Benchmarking Training-Deployment System Inconsistency](http://arxiv.org/abs/2307.00280) #robust</code></li>
<li>Summary: <p>Extensive studies have shown that deep learning models are vulnerable to
adversarial and natural noises, yet little is known about model robustness on
noises caused by different system implementations. In this paper, we for the
first time introduce SysNoise, a frequently occurred but often overlooked noise
in the deep learning training-deployment cycle. In particular, SysNoise happens
when the source training system switches to a disparate target system in
deployments, where various tiny system mismatch adds up to a non-negligible
difference. We first identify and classify SysNoise into three categories based
on the inference stage; we then build a holistic benchmark to quantitatively
measure the impact of SysNoise on 20+ models, comprehending image
classification, object detection, instance segmentation and natural language
processing tasks. Our extensive experiments revealed that SysNoise could bring
certain impacts on model robustness across different tasks and common
mitigations like data augmentation and adversarial training show limited
effects on it. Together, our findings open a new research topic and we hope
this work will raise research attention to deep learning deployment systems
accounting for model performance. We have open-sourced the benchmark and
framework at https://modeltc.github.io/systemnoise_web.
</p></li>
</ul>

<h3>Title: SyMFM6D: Symmetry-aware Multi-directional Fusion for Multi-View 6D Object Pose Estimation. (arXiv:2307.00306v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00306">http://arxiv.org/abs/2307.00306</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00306] SyMFM6D: Symmetry-aware Multi-directional Fusion for Multi-View 6D Object Pose Estimation](http://arxiv.org/abs/2307.00306) #robust</code></li>
<li>Summary: <p>Detecting objects and estimating their 6D poses is essential for automated
systems to interact safely with the environment. Most 6D pose estimators,
however, rely on a single camera frame and suffer from occlusions and
ambiguities due to object symmetries. We overcome this issue by presenting a
novel symmetry-aware multi-view 6D pose estimator called SyMFM6D. Our approach
efficiently fuses the RGB-D frames from multiple perspectives in a deep
multi-directional fusion network and predicts predefined keypoints for all
objects in the scene simultaneously. Based on the keypoints and an instance
semantic segmentation, we efficiently compute the 6D poses by least-squares
fitting. To address the ambiguity issues for symmetric objects, we propose a
novel training procedure for symmetry-aware keypoint detection including a new
objective function. Our SyMFM6D network significantly outperforms the
state-of-the-art in both single-view and multi-view 6D pose estimation. We
furthermore show the effectiveness of our symmetry-aware training procedure and
demonstrate that our approach is robust towards inaccurate camera calibration
and dynamic camera setups.
</p></li>
</ul>

<h3>Title: Detection of River Sandbank for Sand Mining with the Presence of Other High Mineral Content Regions Using Multi-spectral Images. (arXiv:2307.00314v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00314">http://arxiv.org/abs/2307.00314</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00314] Detection of River Sandbank for Sand Mining with the Presence of Other High Mineral Content Regions Using Multi-spectral Images](http://arxiv.org/abs/2307.00314) #robust</code></li>
<li>Summary: <p>Sand mining is a booming industry. The river sandbank is one of the primary
sources of sand mining. Detection of potential river sandbank regions for sand
mining directly impacts the economy, society, and environment. In the past,
semi-supervised and supervised techniques have been used to detect mining
regions including sand mining. A few techniques employ multi-modal analysis
combining different modalities such as multi-spectral imaging, synthetic
aperture radar (\emph{SAR}) imaging, aerial images, and point cloud data.
However, the distinguishing spectral characteristics of river sandbank regions
are yet to be fully explored. This paper provides a novel method to detect
river sandbank regions for sand mining using multi-spectral images without any
labeled data over the seasons. Association with a river stream and the
abundance of minerals are the most prominent features of such a region. The
proposed work uses these distinguishing features to determine the spectral
signature of a river sandbank region, which is robust to other high mineral
abundance regions. It follows a two-step approach, where first, potential high
mineral regions are detected and next, they are segregated using the presence
of a river stream. The proposed technique provides average accuracy, precision,
and recall of 90.75%, 85.47%, and 73.5%, respectively over the seasons from
Landsat 8 images without using any labeled dataset.
</p></li>
</ul>

<h3>Title: Ticket-BERT: Labeling Incident Management Tickets with Language Models. (arXiv:2307.00108v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00108">http://arxiv.org/abs/2307.00108</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00108] Ticket-BERT: Labeling Incident Management Tickets with Language Models](http://arxiv.org/abs/2307.00108) #robust</code></li>
<li>Summary: <p>An essential aspect of prioritizing incident tickets for resolution is
efficiently labeling tickets with fine-grained categories. However, ticket data
is often complex and poses several unique challenges for modern machine
learning methods: (1) tickets are created and updated either by machines with
pre-defined algorithms or by engineers with domain expertise that share
different protocols, (2) tickets receive frequent revisions that update ticket
status by modifying all or parts of ticket descriptions, and (3) ticket
labeling is time-sensitive and requires knowledge updates and new labels per
the rapid software and hardware improvement lifecycle. To handle these issues,
we introduce Ticket- BERT which trains a simple yet robust language model for
labeling tickets using our proposed ticket datasets. Experiments demonstrate
the superiority of Ticket-BERT over baselines and state-of-the-art text
classifiers on Azure Cognitive Services. We further encapsulate Ticket-BERT
with an active learning cycle and deploy it on the Microsoft IcM system, which
enables the model to quickly finetune on newly-collected tickets with a few
annotations.
</p></li>
</ul>

<h3>Title: InferTurbo: A Scalable System for Boosting Full-graph Inference of Graph Neural Network over Huge Graphs. (arXiv:2307.00228v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00228">http://arxiv.org/abs/2307.00228</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00228] InferTurbo: A Scalable System for Boosting Full-graph Inference of Graph Neural Network over Huge Graphs](http://arxiv.org/abs/2307.00228) #robust</code></li>
<li>Summary: <p>GNN inference is a non-trivial task, especially in industrial scenarios with
giant graphs, given three main challenges, i.e., scalability tailored for
full-graph inference on huge graphs, inconsistency caused by stochastic
acceleration strategies (e.g., sampling), and the serious redundant computation
issue. To address the above challenges, we propose a scalable system named
InferTurbo to boost the GNN inference tasks in industrial scenarios. Inspired
by the philosophy of ``think-like-a-vertex", a GAS-like (Gather-Apply-Scatter)
schema is proposed to describe the computation paradigm and data flow of GNN
inference. The computation of GNNs is expressed in an iteration manner, in
which a vertex would gather messages via in-edges and update its state
information by forwarding an associated layer of GNNs with those messages and
then send the updated information to other vertexes via out-edges. Following
the schema, the proposed InferTurbo can be built with alternative backends
(e.g., batch processing system or graph computing system). Moreover, InferTurbo
introduces several strategies like shadow-nodes and partial-gather to handle
nodes with large degrees for better load balancing. With InferTurbo, GNN
inference can be hierarchically conducted over the full graph without sampling
and redundant computation. Experimental results demonstrate that our system is
robust and efficient for inference tasks over graphs containing some hub nodes
with many adjacent edges. Meanwhile, the system gains a remarkable performance
compared with the traditional inference pipeline, and it can finish a GNN
inference task over a graph with tens of billions of nodes and hundreds of
billions of edges within 2 hours.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Information Extraction in Domain and Generic Documents: Findings from Heuristic-based and Data-driven Approaches. (arXiv:2307.00130v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00130">http://arxiv.org/abs/2307.00130</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00130] Information Extraction in Domain and Generic Documents: Findings from Heuristic-based and Data-driven Approaches](http://arxiv.org/abs/2307.00130) #extraction</code></li>
<li>Summary: <p>Information extraction (IE) plays very important role in natural language
processing (NLP) and is fundamental to many NLP applications that used to
extract structured information from unstructured text data. Heuristic-based
searching and data-driven learning are two main stream implementation
approaches. However, no much attention has been paid to document genre and
length influence on IE tasks. To fill the gap, in this study, we investigated
the accuracy and generalization abilities of heuristic-based searching and
data-driven to perform two IE tasks: named entity recognition (NER) and
semantic role labeling (SRL) on domain-specific and generic documents with
different length. We posited two hypotheses: first, short documents may yield
better accuracy results compared to long documents; second, generic documents
may exhibit superior extraction outcomes relative to domain-dependent documents
due to training document genre limitations. Our findings reveals that no single
method demonstrated overwhelming performance in both tasks. For named entity
extraction, data-driven approaches outperformed symbolic methods in terms of
accuracy, particularly in short texts. In the case of semantic roles
extraction, we observed that heuristic-based searching method and data-driven
based model with syntax representation surpassed the performance of pure
data-driven approach which only consider semantic information. Additionally, we
discovered that different semantic roles exhibited varying accuracy levels with
the same method. This study offers valuable insights for downstream text mining
tasks, such as NER and SRL, when addressing various document features and
genres.
</p></li>
</ul>

<h3>Title: iMETRE: Incorporating Markers of Entity Types for Relation Extraction. (arXiv:2307.00132v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00132">http://arxiv.org/abs/2307.00132</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00132] iMETRE: Incorporating Markers of Entity Types for Relation Extraction](http://arxiv.org/abs/2307.00132) #extraction</code></li>
<li>Summary: <p>Sentence-level relation extraction (RE) aims to identify the relationship
between 2 entities given a contextual sentence. While there have been many
attempts to solve this problem, the current solutions have a lot of room to
improve. In this paper, we approach the task of relationship extraction in the
financial dataset REFinD. Our approach incorporates typed entity markers
representations and various models finetuned on the dataset, which has allowed
us to achieve an F1 score of 69.65% on the validation set. Through this paper,
we discuss various approaches and possible limitations.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Hierarchical Federated Learning Incentivization for Gas Usage Estimation. (arXiv:2307.00233v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00233">http://arxiv.org/abs/2307.00233</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00233] Hierarchical Federated Learning Incentivization for Gas Usage Estimation](http://arxiv.org/abs/2307.00233) #federate</code></li>
<li>Summary: <p>Accurately estimating gas usage is essential for the efficient functioning of
gas distribution networks and saving operational costs. Traditional methods
rely on centralized data processing, which poses privacy risks. Federated
learning (FL) offers a solution to this problem by enabling local data
processing on each participant, such as gas companies and heating stations.
However, local training and communication overhead may discourage gas companies
and heating stations from actively participating in the FL training process. To
address this challenge, we propose a Hierarchical FL Incentive Mechanism for
Gas Usage Estimation (HI-GAS), which has been testbedded in the ENN Group, one
of the leading players in the natural gas and green energy industry. It is
designed to support horizontal FL among gas companies, and vertical FL among
each gas company and heating station within a hierarchical FL ecosystem,
rewarding participants based on their contributions to FL. In addition, a
hierarchical FL model aggregation approach is also proposed to improve the gas
usage estimation performance by aggregating models at different levels of the
hierarchy. The incentive scheme employs a multi-dimensional contribution-aware
reward distribution function that combines the evaluation of data quality and
model contribution to incentivize both gas companies and heating stations
within their jurisdiction while maintaining fairness. Results of extensive
experiments validate the effectiveness of the proposed mechanism.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: FFPDG: Fast, Fair and Private Data Generation. (arXiv:2307.00161v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00161">http://arxiv.org/abs/2307.00161</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00161] FFPDG: Fast, Fair and Private Data Generation](http://arxiv.org/abs/2307.00161) #fair</code></li>
<li>Summary: <p>Generative modeling has been used frequently in synthetic data generation.
Fairness and privacy are two big concerns for synthetic data. Although Recent
GAN [\cite{goodfellow2014generative}] based methods show good results in
preserving privacy, the generated data may be more biased. At the same time,
these methods require high computation resources. In this work, we design a
fast, fair, flexible and private data generation method. We show the
effectiveness of our method theoretically and empirically. We show that models
trained on data generated by the proposed method can perform well (in inference
stage) on real application scenarios.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: AE-RED: A Hyperspectral Unmixing Framework Powered by Deep Autoencoder and Regularization by Denoising. (arXiv:2307.00269v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00269">http://arxiv.org/abs/2307.00269</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00269] AE-RED: A Hyperspectral Unmixing Framework Powered by Deep Autoencoder and Regularization by Denoising](http://arxiv.org/abs/2307.00269) #interpretability</code></li>
<li>Summary: <p>Spectral unmixing has been extensively studied with a variety of methods and
used in many applications. Recently, data-driven techniques with deep learning
methods have obtained great attention to spectral unmixing for its superior
learning ability to automatically learn the structure information. In
particular, autoencoder based architectures are elaborately designed to solve
blind unmixing and model complex nonlinear mixtures. Nevertheless, these
methods perform unmixing task as blackboxes and lack of interpretability. On
the other hand, conventional unmixing methods carefully design the regularizer
to add explicit information, in which algorithms such as plug-and-play (PnP)
strategies utilize off-the-shelf denoisers to plug powerful priors. In this
paper, we propose a generic unmixing framework to integrate the autoencoder
network with regularization by denoising (RED), named AE-RED. More specially,
we decompose the unmixing optimized problem into two subproblems. The first one
is solved using deep autoencoders to implicitly regularize the estimates and
model the mixture mechanism. The second one leverages the denoiser to bring in
the explicit information. In this way, both the characteristics of the deep
autoencoder based unmixing methods and priors provided by denoisers are merged
into our well-designed framework to enhance the unmixing performance.
Experiment results on both synthetic and real data sets show the superiority of
our proposed framework compared with state-of-the-art unmixing approaches.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Seeing in Words: Learning to Classify through Language Bottlenecks. (arXiv:2307.00028v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00028">http://arxiv.org/abs/2307.00028</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00028] Seeing in Words: Learning to Classify through Language Bottlenecks](http://arxiv.org/abs/2307.00028) #explainability</code></li>
<li>Summary: <p>Neural networks for computer vision extract uninterpretable features despite
achieving high accuracy on benchmarks. In contrast, humans can explain their
predictions using succinct and intuitive descriptions. To incorporate
explainability into neural networks, we train a vision model whose feature
representations are text. We show that such a model can effectively classify
ImageNet images, and we discuss the challenges we encountered when training it.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Re-Think and Re-Design Graph Neural Networks in Spaces of Continuous Graph Diffusion Functionals. (arXiv:2307.00222v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00222">http://arxiv.org/abs/2307.00222</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00222] Re-Think and Re-Design Graph Neural Networks in Spaces of Continuous Graph Diffusion Functionals](http://arxiv.org/abs/2307.00222) #diffusion</code></li>
<li>Summary: <p>Graph neural networks (GNNs) are widely used in domains like social networks
and biological systems. However, the locality assumption of GNNs, which limits
information exchange to neighboring nodes, hampers their ability to capture
long-range dependencies and global patterns in graphs. To address this, we
propose a new inductive bias based on variational analysis, drawing inspiration
from the Brachistochrone problem. Our framework establishes a mapping between
discrete GNN models and continuous diffusion functionals. This enables the
design of application-specific objective functions in the continuous domain and
the construction of discrete deep models with mathematical guarantees. To
tackle over-smoothing in GNNs, we analyze the existing layer-by-layer graph
embedding models and identify that they are equivalent to l2-norm integral
functionals of graph gradients, which cause over-smoothing. Similar to
edge-preserving filters in image denoising, we introduce total variation (TV)
to align the graph diffusion pattern with global community topologies.
Additionally, we devise a selective mechanism to address the trade-off between
model depth and over-smoothing, which can be easily integrated into existing
GNNs. Furthermore, we propose a novel generative adversarial network (GAN) that
predicts spreading flows in graphs through a neural transport equation. To
mitigate vanishing flows, we customize the objective function to minimize
transportation within each community while maximizing inter-community flows.
Our GNN models achieve state-of-the-art (SOTA) performance on popular graph
learning benchmarks such as Cora, Citeseer, and Pubmed.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Hierarchical Neural Coding for Controllable CAD Model Generation. (arXiv:2307.00149v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00149">http://arxiv.org/abs/2307.00149</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00149] Hierarchical Neural Coding for Controllable CAD Model Generation](http://arxiv.org/abs/2307.00149) #transformer</code></li>
<li>Summary: <p>This paper presents a novel generative model for Computer Aided Design (CAD)
that 1) represents high-level design concepts of a CAD model as a three-level
hierarchical tree of neural codes, from global part arrangement down to local
curve geometry; and 2) controls the generation or completion of CAD models by
specifying the target design using a code tree. Concretely, a novel variant of
a vector quantized VAE with "masked skip connection" extracts design variations
as neural codebooks at three levels. Two-stage cascaded auto-regressive
transformers learn to generate code trees from incomplete CAD models and then
complete CAD models following the intended design. Extensive experiments
demonstrate superior performance on conventional tasks such as random
generation while enabling novel interaction capabilities on conditional
generation tasks. The code is available at
https://github.com/samxuxiang/hnc-cad.
</p></li>
</ul>

<h3>Title: Stitched ViTs are Flexible Vision Backbones. (arXiv:2307.00154v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00154">http://arxiv.org/abs/2307.00154</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00154] Stitched ViTs are Flexible Vision Backbones](http://arxiv.org/abs/2307.00154) #transformer</code></li>
<li>Summary: <p>Large pretrained plain vision Transformers (ViTs) have been the workhorse for
many downstream tasks. However, existing works utilizing off-the-shelf ViTs are
inefficient in terms of training and deployment, because adopting ViTs with
individual sizes requires separate training and is restricted by fixed
performance-efficiency trade-offs. In this paper, we are inspired by stitchable
neural networks, which is a new framework that cheaply produces a single model
that covers rich subnetworks by stitching pretrained model families, supporting
diverse performance-efficiency trade-offs at runtime. Building upon this
foundation, we introduce SN-Netv2, a systematically improved model stitching
framework to facilitate downstream task adaptation. Specifically, we first
propose a Two-way stitching scheme to enlarge the stitching space. We then
design a resource-constrained sampling strategy that takes into account the
underlying FLOPs distributions in the space for improved sampling. Finally, we
observe that learning stitching layers is a low-rank update, which plays an
essential role on downstream tasks to stabilize training and ensure a good
Pareto frontier. With extensive experiments on ImageNet-1K, ADE20K,
COCO-Stuff-10K, NYUv2 and COCO-2017, SN-Netv2 demonstrates strong ability to
serve as a flexible vision backbone, achieving great advantages in both
training efficiency and adaptation. Code will be released at
https://github.com/ziplab/SN-Netv2.
</p></li>
</ul>

<h3>Title: PM-DETR: Domain Adaptive Prompt Memory for Object Detection with Transformers. (arXiv:2307.00313v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00313">http://arxiv.org/abs/2307.00313</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00313] PM-DETR: Domain Adaptive Prompt Memory for Object Detection with Transformers](http://arxiv.org/abs/2307.00313) #transformer</code></li>
<li>Summary: <p>The Transformer-based detectors (i.e., DETR) have demonstrated impressive
performance on end-to-end object detection. However, transferring DETR to
different data distributions may lead to a significant performance degradation.
Existing adaptation techniques focus on model-based approaches, which aim to
leverage feature alignment to narrow the distribution shift between different
domains. In this study, we propose a hierarchical Prompt Domain Memory (PDM)
for adapting detection transformers to different distributions. PDM
comprehensively leverages the prompt memory to extract domain-specific
knowledge and explicitly constructs a long-term memory space for the data
distribution, which represents better domain diversity compared to existing
methods. Specifically, each prompt and its corresponding distribution value are
paired in the memory space, and we inject top M distribution-similar prompts
into the input and multi-level embeddings of DETR. Additionally, we introduce
the Prompt Memory Alignment (PMA) to reduce the discrepancy between the source
and target domains by fully leveraging the domain-specific knowledge extracted
from the prompt domain memory. Extensive experiments demonstrate that our
method outperforms state-of-the-art domain adaptive object detection methods on
three benchmarks, including scene, synthetic to real, and weather adaptation.
Codes will be released.
</p></li>
</ul>

<h3>Title: Investigating Masking-based Data Generation in Language Models. (arXiv:2307.00008v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00008">http://arxiv.org/abs/2307.00008</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00008] Investigating Masking-based Data Generation in Language Models](http://arxiv.org/abs/2307.00008) #transformer</code></li>
<li>Summary: <p>The current era of natural language processing (NLP) has been defined by the
prominence of pre-trained language models since the advent of BERT. A feature
of BERT and models with similar architecture is the objective of masked
language modeling, in which part of the input is intentionally masked and the
model is trained to predict this piece of masked information. Data augmentation
is a data-driven technique widely used in machine learning, including research
areas like computer vision and natural language processing, to improve model
performance by artificially augmenting the training data set by designated
techniques. Masked language models (MLM), an essential training feature of
BERT, have introduced a novel approach to perform effective pre-training on
Transformer based models in natural language processing tasks. Recent studies
have utilized masked language model to generate artificially augmented data for
NLP downstream tasks. The experimental results show that Mask based data
augmentation method provides a simple but efficient approach to improve the
model performance. In this paper, we explore and discuss the broader
utilization of these data augmentation methods based on MLM.
</p></li>
</ul>

<h3>Title: SMILE: Evaluation and Domain Adaptation for Social Media Language Understanding. (arXiv:2307.00135v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00135">http://arxiv.org/abs/2307.00135</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00135] SMILE: Evaluation and Domain Adaptation for Social Media Language Understanding](http://arxiv.org/abs/2307.00135) #transformer</code></li>
<li>Summary: <p>We study the ability of transformer-based language models (LMs) to understand
social media language. Social media (SM) language is distinct from standard
written language, yet existing benchmarks fall short of capturing LM
performance in this socially, economically, and politically important domain.
We quantify the degree to which social media language differs from conventional
language and conclude that the difference is significant both in terms of token
distribution and rate of linguistic shift. Next, we introduce a new benchmark
for Social MedIa Language Evaluation (SMILE) that covers four SM platforms and
eleven tasks. Finally, we show that learning a tokenizer and pretraining on a
mix of social media and conventional language yields an LM that outperforms the
best similar-sized alternative by 4.2 points on the overall SMILE score.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: DisCo: Disentangled Control for Referring Human Dance Generation in Real World. (arXiv:2307.00040v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00040">http://arxiv.org/abs/2307.00040</a></li>
<li>Code URL: <a href="https://github.com/Wangt-CN/DisCo">https://github.com/Wangt-CN/DisCo</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00040] DisCo: Disentangled Control for Referring Human Dance Generation in Real World](http://arxiv.org/abs/2307.00040) #generative</code></li>
<li>Summary: <p>Generative AI has made significant strides in computer vision, particularly
in image/video synthesis conditioned on text descriptions. Despite the
advancements, it remains challenging especially in the generation of
human-centric content such as dance synthesis. Existing dance synthesis methods
struggle with the gap between synthesized content and real-world dance
scenarios. In this paper, we define a new problem setting: Referring Human
Dance Generation, which focuses on real-world dance scenarios with three
important properties: (i) Faithfulness: the synthesis should retain the
appearance of both human subject foreground and background from the reference
image, and precisely follow the target pose; (ii) Generalizability: the model
should generalize to unseen human subjects, backgrounds, and poses; (iii)
Compositionality: it should allow for composition of seen/unseen subjects,
backgrounds, and poses from different sources. To address these challenges, we
introduce a novel approach, DISCO, which includes a novel model architecture
with disentangled control to improve the faithfulness and compositionality of
dance synthesis, and an effective human attribute pre-training for better
generalizability to unseen humans. Extensive qualitative and quantitative
results demonstrate that DISCO can generate high-quality human dance images and
videos with diverse appearances and flexible motions. Code, demo, video and
visualization are available at: https://disco-dance.github.io/.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models. (arXiv:2307.00101v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00101">http://arxiv.org/abs/2307.00101</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00101] Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models](http://arxiv.org/abs/2307.00101) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs) are trained primarily on minimally processed web
text, which exhibits the same wide range of social biases held by the humans
who created that content. Consequently, text generated by LLMs can
inadvertently perpetuate stereotypes towards marginalized groups, like the
LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs
generate text describing people with different sexual identities. Analyzing
bias in the text generated by an LLM using regard score shows measurable bias
against queer people. We then show that a post-hoc method based on
chain-of-thought prompting using SHAP analysis can increase the regard of the
sentence, representing a promising approach towards debiasing the output of
LLMs in this setting.
</p></li>
</ul>

<h3>Title: Meta-training with Demonstration Retrieval for Efficient Few-shot Learning. (arXiv:2307.00119v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00119">http://arxiv.org/abs/2307.00119</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00119] Meta-training with Demonstration Retrieval for Efficient Few-shot Learning](http://arxiv.org/abs/2307.00119) #large language model</code></li>
<li>Summary: <p>Large language models show impressive results on few-shot NLP tasks. However,
these models are memory and computation-intensive. Meta-training allows one to
leverage smaller models for few-shot generalization in a domain-general and
task-agnostic manner; however, these methods alone results in models that may
not have sufficient parameterization or knowledge to adapt quickly to a large
variety of tasks. To overcome this issue, we propose meta-training with
demonstration retrieval, where we use a dense passage retriever to retrieve
semantically similar labeled demonstrations to each example for more varied
supervision. By separating external knowledge from model parameters, we can use
meta-training to train parameter-efficient models that generalize well on a
larger variety of tasks. We construct a meta-training set from UnifiedQA and
CrossFit, and propose a demonstration bank based on UnifiedQA tasks. To our
knowledge, our work is the first to combine retrieval with meta-training, to
use DPR models to retrieve demonstrations, and to leverage demonstrations from
many tasks simultaneously, rather than randomly sampling demonstrations from
the training set of the target task. Our approach outperforms a variety of
targeted parameter-efficient and retrieval-augmented few-shot methods on QA,
NLI, and text classification tasks (including SQuAD, QNLI, and TREC). Our
approach can be meta-trained and fine-tuned quickly on a single GPU.
</p></li>
</ul>

<h3>Title: Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks. (arXiv:2307.00175v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00175">http://arxiv.org/abs/2307.00175</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00175] Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks](http://arxiv.org/abs/2307.00175) #large language model</code></li>
<li>Summary: <p>We consider the questions of whether or not large language models (LLMs) have
beliefs, and, if they do, how we might measure them. First, we evaluate two
existing approaches, one due to Azaria and Mitchell (2023) and the other to
Burns et al. (2022). We provide empirical results that show that these methods
fail to generalize in very basic ways. We then argue that, even if LLMs have
beliefs, these methods are unlikely to be successful for conceptual reasons.
Thus, there is still no lie-detector for LLMs. After describing our empirical
results we take a step back and consider whether or not we should expect LLMs
to have something like beliefs in the first place. We consider some recent
arguments aiming to show that LLMs cannot have beliefs. We show that these
arguments are misguided. We provide a more productive framing of questions
surrounding the status of beliefs in LLMs, and highlight the empirical nature
of the problem. We conclude by suggesting some concrete paths for future work.
</p></li>
</ul>

<h3>Title: Personality Traits in Large Language Models. (arXiv:2307.00184v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00184">http://arxiv.org/abs/2307.00184</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00184] Personality Traits in Large Language Models](http://arxiv.org/abs/2307.00184) #large language model</code></li>
<li>Summary: <p>The advent of large language models (LLMs) has revolutionized natural
language processing, enabling the generation of coherent and contextually
relevant text. As LLMs increasingly power conversational agents, the
synthesized personality embedded in these models by virtue of their training on
large amounts of human-generated data draws attention. Since personality is an
important factor determining the effectiveness of communication, we present a
comprehensive method for administering validated psychometric tests and
quantifying, analyzing, and shaping personality traits exhibited in text
generated from widely-used LLMs. We find that: 1) personality simulated in the
outputs of some LLMs (under specific prompting configurations) is reliable and
valid; 2) evidence of reliability and validity of LLM-simulated personality is
stronger for larger and instruction fine-tuned models; and 3) personality in
LLM outputs can be shaped along desired dimensions to mimic specific
personality profiles. We also discuss potential applications and ethical
implications of our measurement and shaping framework, especially regarding
responsible use of LLMs.
</p></li>
</ul>

<h3>Title: InstructEval: Systematic Evaluation of Instruction Selection Methods. (arXiv:2307.00259v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00259">http://arxiv.org/abs/2307.00259</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00259] InstructEval: Systematic Evaluation of Instruction Selection Methods](http://arxiv.org/abs/2307.00259) #large language model</code></li>
<li>Summary: <p>In-context learning (ICL) performs tasks by prompting a large language model
(LLM) using an instruction and a small set of annotated examples called
demonstrations. Recent work has shown that the precise details of the inputs
used in the prompt significantly impacts ICL, which has incentivized
instruction selection algorithms. The effect of instruction-choice however is
severely underexplored, with existing analyses being restricted to shallow
subsets of models and tasks, which limits the generalizability of their
insights. We develop an ICL evaluation suite to conduct a thorough assessment
of these techniques. The suite includes 13 open-sourced LLMs of varying scales
from 4 distinct model families and covers 9 different tasks, representing a
range of task types across 3 categories. In this work, we evaluate the relative
performance of 7 popular instruction selection methods using our benchmark over
five desiderata relevant to ICL. We discover that using curated
manually-written instructions and simple instructions without any task-specific
descriptions often elicits superior ICL performance than that of automatic
instruction-induction methods, pointing to a lack of generalizability among the
latter. We release our evaluation suite for benchmarking instruction selection
approaches, and call for more rigorous and generalizable methods in this space.
</p></li>
</ul>

<h3>Title: Let Me Teach You: Pedagogical Foundations of Feedback for Language Models. (arXiv:2307.00279v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00279">http://arxiv.org/abs/2307.00279</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00279] Let Me Teach You: Pedagogical Foundations of Feedback for Language Models](http://arxiv.org/abs/2307.00279) #large language model</code></li>
<li>Summary: <p>Natural Language Feedback (NLF) is an increasingly popular avenue to align
Large Language Models (LLMs) to human preferences. Despite the richness and
diversity of the information it can convey, NLF is often hand-designed and
arbitrary. In a different world, research in pedagogy has long established
several effective feedback models. In this opinion piece, we compile ideas from
pedagogy to introduce FELT, a feedback framework for LLMs that outlines the
various characteristics of the feedback space, and a feedback content taxonomy
based on these variables. Our taxonomy offers both a general mapping of the
feedback space, as well as pedagogy-established discrete categories, allowing
us to empirically demonstrate the impact of different feedback types on revised
generations. In addition to streamlining existing NLF designs, FELT also brings
out new, unexplored directions for research in NLF. We make our taxonomy
available to the community, providing guides and examples for mapping our
categorizations to future resources.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Training-free Object Counting with Prompts. (arXiv:2307.00038v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00038">http://arxiv.org/abs/2307.00038</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00038] Training-free Object Counting with Prompts](http://arxiv.org/abs/2307.00038) #segmentation</code></li>
<li>Summary: <p>This paper tackles the problem of object counting in images. Existing
approaches rely on extensive training data with point annotations for each
object, making data collection labor-intensive and time-consuming. To overcome
this, we propose a training-free object counter that treats the counting task
as a segmentation problem. Our approach leverages the Segment Anything Model
(SAM), known for its high-quality masks and zero-shot segmentation capability.
However, the vanilla mask generation method of SAM lacks class-specific
information in the masks, resulting in inferior counting accuracy. To overcome
this limitation, we introduce a prior-guided mask generation method that
incorporates three types of priors into the segmentation process, enhancing
efficiency and accuracy. Additionally, we tackle the issue of counting objects
specified through free-form text by proposing a two-stage approach that
combines reference object selection and prior-guided mask generation. Extensive
experiments on standard datasets demonstrate the competitive performance of our
training-free counter compared to learning-based approaches. This paper
presents a promising solution for counting objects in various scenarios without
the need for extensive data collection and model training. Code is available at
https://github.com/shizenglin/training-free-object-counter.
</p></li>
</ul>

<h3>Title: Prompting classes: Exploring the Power of Prompt Class Learning in Weakly Supervised Semantic Segmentation. (arXiv:2307.00097v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00097">http://arxiv.org/abs/2307.00097</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00097] Prompting classes: Exploring the Power of Prompt Class Learning in Weakly Supervised Semantic Segmentation](http://arxiv.org/abs/2307.00097) #segmentation</code></li>
<li>Summary: <p>Recently, CLIP-based approaches have exhibited remarkable performance on
generalization and few-shot learning tasks, fueled by the power of contrastive
language-vision pre-training. In particular, prompt tuning has emerged as an
effective strategy to adapt the pre-trained language-vision models to
downstream tasks by employing task-related textual tokens. Motivated by this
progress, in this work we question whether other fundamental problems, such as
weakly supervised semantic segmentation (WSSS), can benefit from prompt tuning.
Our findings reveal two interesting observations that shed light on the impact
of prompt tuning on WSSS. First, modifying only the class token of the text
prompt results in a greater impact on the Class Activation Map (CAM), compared
to arguably more complex strategies that optimize the context. And second, the
class token associated with the image ground truth does not necessarily
correspond to the category that yields the best CAM. Motivated by these
observations, we introduce a novel approach based on a PrOmpt cLass lEarning
(POLE) strategy. Through extensive experiments we demonstrate that our simple,
yet efficient approach achieves SOTA performance in a well-known WSSS
benchmark. These results highlight not only the benefits of language-vision
models in WSSS but also the potential of prompt learning for this problem. The
code is available at https://github.com/rB080/WSS_POLE.
</p></li>
</ul>

<h3>Title: Obscured Wildfire Flame Detection By Temporal Analysis of Smoke Patterns Captured by Unmanned Aerial Systems. (arXiv:2307.00104v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00104">http://arxiv.org/abs/2307.00104</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00104] Obscured Wildfire Flame Detection By Temporal Analysis of Smoke Patterns Captured by Unmanned Aerial Systems](http://arxiv.org/abs/2307.00104) #segmentation</code></li>
<li>Summary: <p>This research paper addresses the challenge of detecting obscured wildfires
(when the fire flames are covered by trees, smoke, clouds, and other natural
barriers) in real-time using drones equipped only with RGB cameras. We propose
a novel methodology that employs semantic segmentation based on the temporal
analysis of smoke patterns in video sequences. Our approach utilizes an
encoder-decoder architecture based on deep convolutional neural network
architecture with a pre-trained CNN encoder and 3D convolutions for decoding
while using sequential stacking of features to exploit temporal variations. The
predicted fire locations can assist drones in effectively combating forest
fires and pinpoint fire retardant chemical drop on exact flame locations. We
applied our method to a curated dataset derived from the FLAME2 dataset that
includes RGB video along with IR video to determine the ground truth. Our
proposed method has a unique property of detecting obscured fire and achieves a
Dice score of 85.88%, while achieving a high precision of 92.47% and
classification accuracy of 90.67% on test data showing promising results when
inspected visually. Indeed, our method outperforms other methods by a
significant margin in terms of video-level fire classification as we obtained
about 100% accuracy using MobileNet+CBAM as the encoder backbone.
</p></li>
</ul>

<h3>Title: Internal-External Boundary Attention Fusion for Glass Surface Segmentation. (arXiv:2307.00212v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00212">http://arxiv.org/abs/2307.00212</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00212] Internal-External Boundary Attention Fusion for Glass Surface Segmentation](http://arxiv.org/abs/2307.00212) #segmentation</code></li>
<li>Summary: <p>Glass surfaces of transparent objects and mirrors are not able to be uniquely
and explicitly characterized by their visual appearances because they contain
the visual appearance of other reflected or transmitted surfaces as well.
Detecting glass regions from a single-color image is a challenging task. Recent
deep-learning approaches have paid attention to the description of glass
surface boundary where the transition of visual appearances between glass and
non-glass surfaces are observed. In this work, we analytically investigate how
glass surface boundary helps to characterize glass objects. Inspired by prior
semantic segmentation approaches with challenging image types such as X-ray or
CT scans, we propose separated internal-external boundary attention modules
that individually learn and selectively integrate visual characteristics of the
inside and outside region of glass surface from a single color image. Our
proposed method is evaluated on six public benchmarks comparing with
state-of-the-art methods showing promising results.
</p></li>
</ul>

<h3>Title: Forward-Forward Algorithm for Hyperspectral Image Classification: A Preliminary Study. (arXiv:2307.00231v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00231">http://arxiv.org/abs/2307.00231</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00231] Forward-Forward Algorithm for Hyperspectral Image Classification: A Preliminary Study](http://arxiv.org/abs/2307.00231) #segmentation</code></li>
<li>Summary: <p>The back-propagation algorithm has long been the de-facto standard in
optimizing weights and biases in neural networks, particularly in cutting-edge
deep learning models. Its widespread adoption in fields like natural language
processing, computer vision, and remote sensing has revolutionized automation
in various tasks. The popularity of back-propagation stems from its ability to
achieve outstanding performance in tasks such as classification, detection, and
segmentation. Nevertheless, back-propagation is not without its limitations,
encompassing sensitivity to initial conditions, vanishing gradients,
overfitting, and computational complexity. The recent introduction of a
forward-forward algorithm (FFA), which computes local goodness functions to
optimize network parameters, alleviates the dependence on substantial
computational resources and the constant need for architectural scaling. This
study investigates the application of FFA for hyperspectral image
classification. Experimental results and comparative analysis are provided with
the use of the traditional back-propagation algorithm. Preliminary results show
the potential behind FFA and its promises.
</p></li>
</ul>

<h3>Title: VesselMorph: Domain-Generalized Retinal Vessel Segmentation via Shape-Aware Representation. (arXiv:2307.00240v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00240">http://arxiv.org/abs/2307.00240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00240] VesselMorph: Domain-Generalized Retinal Vessel Segmentation via Shape-Aware Representation](http://arxiv.org/abs/2307.00240) #segmentation</code></li>
<li>Summary: <p>Due to the absence of a single standardized imaging protocol, domain shift
between data acquired from different sites is an inherent property of medical
images and has become a major obstacle for large-scale deployment of
learning-based algorithms. For retinal vessel images, domain shift usually
presents as the variation of intensity, contrast and resolution, while the
basic tubular shape of vessels remains unaffected. Thus, taking advantage of
such domain-invariant morphological features can greatly improve the
generalizability of deep models. In this study, we propose a method named
VesselMorph which generalizes the 2D retinal vessel segmentation task by
synthesizing a shape-aware representation. Inspired by the traditional Frangi
filter and the diffusion tensor imaging literature, we introduce a
Hessian-based bipolar tensor field to depict the morphology of the vessels so
that the shape information is taken into account. We map the intensity image
and the tensor field to a latent space for feature extraction. Then we fuse the
two latent representations via a weight-balancing trick and feed the result to
a segmentation network. We evaluate on six public datasets of fundus and OCT
angiography images from diverse patient populations. VesselMorph achieves
superior generalization performance compared with competing methods in
different domain shift scenarios.
</p></li>
</ul>

<h3>Title: Efficient Subclass Segmentation in Medical Images. (arXiv:2307.00257v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00257">http://arxiv.org/abs/2307.00257</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00257] Efficient Subclass Segmentation in Medical Images](http://arxiv.org/abs/2307.00257) #segmentation</code></li>
<li>Summary: <p>As research interests in medical image analysis become increasingly
fine-grained, the cost for extensive annotation also rises. One feasible way to
reduce the cost is to annotate with coarse-grained superclass labels while
using limited fine-grained annotations as a complement. In this way,
fine-grained data learning is assisted by ample coarse annotations. Recent
studies in classification tasks have adopted this method to achieve
satisfactory results. However, there is a lack of research on efficient
learning of fine-grained subclasses in semantic segmentation tasks. In this
paper, we propose a novel approach that leverages the hierarchical structure of
categories to design network architecture. Meanwhile, a task-driven data
generation method is presented to make it easier for the network to recognize
different subclass categories. Specifically, we introduce a Prior Concatenation
module that enhances confidence in subclass segmentation by concatenating
predicted logits from the superclass classifier, a Separate Normalization
module that stretches the intra-class distance within the same superclass to
facilitate subclass segmentation, and a HierarchicalMix model that generates
high-quality pseudo labels for unlabeled samples by fusing only similar
superclass regions from labeled and unlabeled images. Our experiments on the
BraTS2021 and ACDC datasets demonstrate that our approach achieves comparable
accuracy to a model trained with full subclass annotations, with limited
subclass annotations and sufficient superclass annotations. Our approach offers
a promising solution for efficient fine-grained subclass segmentation in
medical images. Our code is publicly available here.
</p></li>
</ul>

<h3>Title: HrSegNet : Real-time High-Resolution Neural Network with Semantic Guidance for Crack Segmentation. (arXiv:2307.00270v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00270">http://arxiv.org/abs/2307.00270</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00270] HrSegNet : Real-time High-Resolution Neural Network with Semantic Guidance for Crack Segmentation](http://arxiv.org/abs/2307.00270) #segmentation</code></li>
<li>Summary: <p>Through extensive research on deep learning in recent years and its
application in construction, crack detection has evolved rapidly from rough
detection at the image-level and patch-level to fine-grained detection at the
pixel-level, which better suits the nature of this field. Despite numerous
existing studies utilizing off-the-shelf deep learning models or enhancing
them, these models are not always effective or efficient in real-world
applications. In order to bridge this gap, we propose a High-resolution model
with Semantic guidance, specifically designed for real-time crack segmentation,
referred to as HrSegNet. Our model maintains high resolution throughout the
entire process, as opposed to recovering from low-resolution features to
high-resolution ones, thereby maximizing the preservation of crack details.
Moreover, to enhance the context information, we use low-resolution semantic
features to guide the reconstruction of high-resolution features. To ensure the
efficiency of the algorithm, we design a simple yet effective method to control
the computation cost of the entire model by controlling the capacity of
high-resolution channels, while providing the model with extremely strong
scalability. Extensive quantitative and qualitative evaluations demonstrate
that our proposed HrSegNet has exceptional crack segmentation capabilities, and
that maintaining high resolution and semantic guidance are crucial to the final
prediction. Compared to state-of-the-art segmentation models, HrSegNet achieves
the best trade-off between efficiency and effectiveness. Specifically, on the
crack dataset CrackSeg9k, our fastest model HrSegNet-B16 achieves a speed of
182 FPS with 78.43% mIoU, while our most accurate model HrSegNet-B48 achieves
80.32% mIoU with an inference speed of 140.3 FPS.
</p></li>
</ul>

<h3>Title: All-in-SAM: from Weak Annotation to Pixel-wise Nuclei Segmentation with Prompt-based Finetuning. (arXiv:2307.00290v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00290">http://arxiv.org/abs/2307.00290</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00290] All-in-SAM: from Weak Annotation to Pixel-wise Nuclei Segmentation with Prompt-based Finetuning](http://arxiv.org/abs/2307.00290) #segmentation</code></li>
<li>Summary: <p>The Segment Anything Model (SAM) is a recently proposed prompt-based
segmentation model in a generic zero-shot segmentation approach. With the
zero-shot segmentation capacity, SAM achieved impressive flexibility and
precision on various segmentation tasks. However, the current pipeline requires
manual prompts during the inference stage, which is still resource intensive
for biomedical image segmentation. In this paper, instead of using prompts
during the inference stage, we introduce a pipeline that utilizes the SAM,
called all-in-SAM, through the entire AI development workflow (from annotation
generation to model finetuning) without requiring manual prompts during the
inference stage. Specifically, SAM is first employed to generate pixel-level
annotations from weak prompts (e.g., points, bounding box). Then, the
pixel-level annotations are used to finetune the SAM segmentation model rather
than training from scratch. Our experimental results reveal two key findings:
1) the proposed pipeline surpasses the state-of-the-art (SOTA) methods in a
nuclei segmentation task on the public Monuseg dataset, and 2) the utilization
of weak and few annotations for SAM finetuning achieves competitive performance
compared to using strong pixel-wise annotated data.
</p></li>
</ul>

<h3>Title: What do self-supervised speech models know about words?. (arXiv:2307.00162v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.00162">http://arxiv.org/abs/2307.00162</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.00162] What do self-supervised speech models know about words?](http://arxiv.org/abs/2307.00162) #segmentation</code></li>
<li>Summary: <p>Many self-supervised speech models (S3Ms) have been introduced over the last
few years, producing performance and data efficiency improvements for a variety
of speech tasks. Evidence is emerging that different S3Ms encode linguistic
information in different layers, and also that some S3Ms appear to learn
phone-like sub-word units. However, the extent to which these models capture
larger linguistic units, such as words, and where word-related information is
encoded, remains unclear. In this study, we conduct several analyses of word
segment representations extracted from different layers of three S3Ms:
wav2vec2, HuBERT, and WavLM. We employ canonical correlation analysis (CCA), a
lightweight analysis tool, to measure the similarity between these
representations and word-level linguistic properties. We find that the maximal
word-level linguistic content tends to be found in intermediate model layers,
while some lower-level information like pronunciation is also retained in
higher layers of HuBERT and WavLM. Syntactic and semantic word attributes have
similar layer-wise behavior. We also find that, for all of the models tested,
word identity information is concentrated near the center of each word segment.
We then test the layer-wise performance of the same models, when used directly
with no additional learned parameters, on several tasks: acoustic word
discrimination, word segmentation, and semantic sentence similarity. We find
similar layer-wise trends in performance, and furthermore, find that when using
the best-performing layer of HuBERT or WavLM, it is possible to achieve
performance on word segmentation and sentence similarity that rivals more
complex existing approaches.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
