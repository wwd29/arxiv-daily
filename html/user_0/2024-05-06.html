<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-05-06</h1>
<h3>Title: Mitigating LLM Hallucinations via Conformal Abstention</h3>
<ul>
<li><strong>Authors: </strong>Yasin Abbasi Yadkori, Ilja Kuzborskij, David Stutz, András György, Adam Fisch, Arnaud Doucet, Iuliya Beloshapka, Wei-Hung Weng, Yao-Yuan Yang, Csaba Szepesvári, Ali Taylan Cemgil, Nenad Tomasev</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01563">https://arxiv.org/abs/2405.01563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01563">https://arxiv.org/pdf/2405.01563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01563]] Mitigating LLM Hallucinations via Conformal Abstention(https://arxiv.org/abs/2405.01563)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>We develop a principled procedure for determining when a large language model (LLM) should abstain from responding (e.g., by saying "I don't know") in a general domain, instead of resorting to possibly "hallucinating" a non-sensical or incorrect answer. Building on earlier approaches that use self-consistency as a more reliable measure of model confidence, we propose using the LLM itself to self-evaluate the similarity between each of its sampled responses for a given query. We then further leverage conformal prediction techniques to develop an abstention procedure that benefits from rigorous theoretical guarantees on the hallucination rate (error rate). Experimentally, our resulting conformal abstention method reliably bounds the hallucination rate on various closed-book, open-domain generative question answering datasets, while also maintaining a significantly less conservative abstention rate on a dataset with long responses (Temporal Sequences) compared to baselines using log-probability scores to quantify uncertainty, while achieveing comparable performance on a dataset with short answers (TriviaQA). To evaluate the experiments automatically, one needs to determine if two responses are equivalent given a question. Following standard practice, we use a thresholded similarity function to determine if two responses match, but also provide a method for calibrating the threshold based on conformal prediction, with theoretical guarantees on the accuracy of the match prediction, which might be of independent interest.</li>
</ul>

<h3>Title: HateTinyLLM : Hate Speech Detection Using Tiny Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tanmay Sen, Ansuman Das, Mrinmay Sen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01577">https://arxiv.org/abs/2405.01577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01577">https://arxiv.org/pdf/2405.01577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01577]] HateTinyLLM : Hate Speech Detection Using Tiny Large Language Models(https://arxiv.org/abs/2405.01577)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hate speech encompasses verbal, written, or behavioral communication that targets derogatory or discriminatory language against individuals or groups based on sensitive characteristics. Automated hate speech detection plays a crucial role in curbing its propagation, especially across social media platforms. Various methods, including recent advancements in deep learning, have been devised to address this challenge. In this study, we introduce HateTinyLLM, a novel framework based on fine-tuned decoder-only tiny large language models (tinyLLMs) for efficient hate speech detection. Our experimental findings demonstrate that the fine-tuned HateTinyLLM outperforms the pretrained mixtral-7b model by a significant margin. We explored various tiny LLMs, including PY007/TinyLlama-1.1B-step-50K-105b, Microsoft/phi-2, and facebook/opt-1.3b, and fine-tuned them using LoRA and adapter methods. Our observations indicate that all LoRA-based fine-tuned models achieved over 80\% accuracy.</li>
</ul>

<h3>Title: The Mercurial Top-Level Ontology of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nele Köhler, Fabian Neuhaus</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01581">https://arxiv.org/abs/2405.01581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01581">https://arxiv.org/pdf/2405.01581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01581]] The Mercurial Top-Level Ontology of Large Language Models(https://arxiv.org/abs/2405.01581)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In our work, we systematize and analyze implicit ontological commitments in the responses generated by large language models (LLMs), focusing on ChatGPT 3.5 as a case study. We investigate how LLMs, despite having no explicit ontology, exhibit implicit ontological categorizations that are reflected in the texts they generate. The paper proposes an approach to understanding the ontological commitments of LLMs by defining ontology as a theory that provides a systematic account of the ontological commitments of some text. We investigate the ontological assumptions of ChatGPT and present a systematized account, i.e., GPT's top-level ontology. This includes a taxonomy, which is available as an OWL file, as well as a discussion about ontological assumptions (e.g., about its mereology or presentism). We show that in some aspects GPT's top-level ontology is quite similar to existing top-level ontologies. However, there are significant challenges arising from the flexible nature of LLM-generated texts, including ontological overload, ambiguity, and inconsistency.</li>
</ul>

<h3>Title: Transfer Learning and Transformer Architecture for Financial Sentiment  Analysis</h3>
<ul>
<li><strong>Authors: </strong>Tohida Rehman, Raghubir Bose, Samiran Chattopadhyay, Debarshi Kumar Sanyal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01586">https://arxiv.org/abs/2405.01586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01586">https://arxiv.org/pdf/2405.01586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01586]] Transfer Learning and Transformer Architecture for Financial Sentiment  Analysis(https://arxiv.org/abs/2405.01586)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Financial sentiment analysis allows financial institutions like Banks and Insurance Companies to better manage the credit scoring of their customers in a better way. Financial domain uses specialized mechanisms which makes sentiment analysis difficult. In this paper, we propose a pre-trained language model which can help to solve this problem with fewer labelled data. We extend on the principles of Transfer learning and Transformation architecture principles and also take into consideration recent outbreak of pandemics like COVID. We apply the sentiment analysis to two different sets of data. We also take smaller training set and fine tune the same as part of the model.</li>
</ul>

<h3>Title: Improve Academic Query Resolution through BERT-based Question Extraction  from Images</h3>
<ul>
<li><strong>Authors: </strong>Nidhi Kamal, Saurabh Yadav, Jorawar Singh, Aditi Avasthi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01587">https://arxiv.org/abs/2405.01587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01587">https://arxiv.org/pdf/2405.01587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01587]] Improve Academic Query Resolution through BERT-based Question Extraction  from Images(https://arxiv.org/abs/2405.01587)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Providing fast and accurate resolution to the student's query is an essential solution provided by Edtech organizations. This is generally provided with a chat-bot like interface to enable students to ask their doubts easily. One preferred format for student queries is images, as it allows students to capture and post questions without typing complex equations and information. However, this format also presents difficulties, as images may contain multiple questions or textual noise that lowers the accuracy of existing single-query answering solutions. In this paper, we propose a method for extracting questions from text or images using a BERT-based deep learning model and compare it to the other rule-based and layout-based methods. Our method aims to improve the accuracy and efficiency of student query resolution in Edtech organizations.</li>
</ul>

<h3>Title: GPT-4 passes most of the 297 written Polish Board Certification  Examinations</h3>
<ul>
<li><strong>Authors: </strong>Jakub Pokrywka, Jeremi Kaczmarek, Edward Gorzelańczyk</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01589">https://arxiv.org/abs/2405.01589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01589">https://arxiv.org/pdf/2405.01589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01589]] GPT-4 passes most of the 297 written Polish Board Certification  Examinations(https://arxiv.org/abs/2405.01589)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Introduction: Recently, the effectiveness of Large Language Models (LLMs) has increased rapidly, allowing them to be used in a great number of applications. However, the risks posed by the generation of false information through LLMs significantly limit their applications in sensitive areas such as healthcare, highlighting the necessity for rigorous validations to determine their utility and reliability. To date, no study has extensively compared the performance of LLMs on Polish medical examinations across a broad spectrum of specialties on a very large dataset. Objectives: This study evaluated the performance of three Generative Pretrained Transformer (GPT) models on the Polish Board Certification Exam (Pa\'nstwowy Egzamin Specjalizacyjny, PES) dataset, which consists of 297 tests. Methods: We developed a software program to download and process PES exams and tested the performance of GPT models using OpenAI Application Programming Interface. Results: Our findings reveal that GPT-3.5 did not pass any of the analyzed exams. In contrast, the GPT-4 models demonstrated the capability to pass the majority of the exams evaluated, with the most recent model, gpt-4-0125, successfully passing 222 (75%) of them. The performance of the GPT models varied significantly, displaying excellence in exams related to certain specialties while completely failing others. Conclusions: The significant progress and impressive performance of LLM models hold great promise for the increased application of AI in the field of medicine in Poland. For instance, this advancement could lead to the development of AI-based medical assistants for healthcare professionals, enhancing the efficiency and accuracy of medical services.</li>
</ul>

<h3>Title: 101 Billion Arabic Words Dataset</h3>
<ul>
<li><strong>Authors: </strong>Manel Aloui, Hasna Chouikhi, Ghaith Chaabane, Haithem Kchaou, Chehir Dhaouadi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01590">https://arxiv.org/abs/2405.01590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01590">https://arxiv.org/pdf/2405.01590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01590]] 101 Billion Arabic Words Dataset(https://arxiv.org/abs/2405.01590)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, Large Language Models have revolutionized the field of natural language processing, showcasing an impressive rise predominantly in English-centric domains. These advancements have set a global benchmark, inspiring significant efforts toward developing Arabic LLMs capable of understanding and generating the Arabic language with remarkable accuracy. Despite these advancements, a critical challenge persists: the potential bias in Arabic LLMs, primarily attributed to their reliance on datasets comprising English data that has been translated into Arabic. This reliance not only compromises the authenticity of the generated content but also reflects a broader issue -the scarcity of original quality Arabic linguistic data. This study aims to address the data scarcity in the Arab world and to encourage the development of Arabic Language Models that are true to both the linguistic and nuances of the region. We undertook a large-scale data mining project, extracting a substantial volume of text from the Common Crawl WET files, specifically targeting Arabic content. The extracted data underwent a rigorous cleaning and deduplication process, using innovative techniques to ensure the integrity and uniqueness of the dataset. The result is the 101 Billion Arabic Words Dataset, the largest Arabic dataset available to date, which can significantly contribute to the development of authentic Arabic LLMs. This study not only highlights the potential for creating linguistically and culturally accurate Arabic LLMs but also sets a precedent for future research in enhancing the authenticity of Arabic language models.</li>
</ul>

<h3>Title: Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in  Radiology with General-Domain Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Seonhee Cho, Choonghan Kim, Jiho Lee, Chetan Chilkunda, Sujin Choi, Joo Heung Yoon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01591">https://arxiv.org/abs/2405.01591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01591">https://arxiv.org/pdf/2405.01591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01591]] Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in  Radiology with General-Domain Large Language Model(https://arxiv.org/abs/2405.01591)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Multimodal Models (LMMs) have attracted interest in their generalization capability with only a few samples in the prompt. This progress is particularly relevant to the medical domain, where the quality and sensitivity of data pose unique challenges for model training and application. However, the dependency on high-quality data for effective in-context learning raises questions about the feasibility of these models when encountering with the inevitable variations and errors inherent in real-world medical data. In this paper, we introduce MID-M, a novel framework that leverages the in-context learning capabilities of a general-domain Large Language Model (LLM) to process multimodal data via image descriptions. MID-M achieves a comparable or superior performance to task-specific fine-tuned LMMs and other general-domain ones, without the extensive domain-specific training or pre-training on multimodal data, with significantly fewer parameters. This highlights the potential of leveraging general-domain LLMs for domain-specific tasks and offers a sustainable and cost-effective alternative to traditional LMM developments. Moreover, the robustness of MID-M against data quality issues demonstrates its practical utility in real-world medical domain applications.</li>
</ul>

<h3>Title: Large Language Model Agent for Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Li, Yongfeng Zhang, Edward C. Malthouse</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01593">https://arxiv.org/abs/2405.01593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01593">https://arxiv.org/pdf/2405.01593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01593]] Large Language Model Agent for Fake News Detection(https://arxiv.org/abs/2405.01593)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the current digital era, the rapid spread of misinformation on online platforms presents significant challenges to societal well-being, public trust, and democratic processes, influencing critical decision making and public opinion. To address these challenges, there is a growing need for automated fake news detection mechanisms. Pre-trained large language models (LLMs) have demonstrated exceptional capabilities across various natural language processing (NLP) tasks, prompting exploration into their potential for verifying news claims. Instead of employing LLMs in a non-agentic way, where LLMs generate responses based on direct prompts in a single shot, our work introduces FactAgent, an agentic approach of utilizing LLMs for fake news detection. FactAgent enables LLMs to emulate human expert behavior in verifying news claims without any model training, following a structured workflow. This workflow breaks down the complex task of news veracity checking into multiple sub-steps, where LLMs complete simple tasks using their internal knowledge or external tools. At the final step of the workflow, LLMs integrate all findings throughout the workflow to determine the news claim's veracity. Compared to manual human verification, FactAgent offers enhanced efficiency. Experimental studies demonstrate the effectiveness of FactAgent in verifying claims without the need for any training process. Moreover, FactAgent provides transparent explanations at each step of the workflow and during final decision-making, offering insights into the reasoning process of fake news detection for end users. FactAgent is highly adaptable, allowing for straightforward updates to its tools that LLMs can leverage within the workflow, as well as updates to the workflow itself using domain knowledge. This adaptability enables FactAgent's application to news verification across various domains.</li>
</ul>

<h3>Title: Efficient Sample-Specific Encoder Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Yassir Fathullah, Mark J. F. Gales</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01601">https://arxiv.org/abs/2405.01601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01601">https://arxiv.org/pdf/2405.01601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01601]] Efficient Sample-Specific Encoder Perturbations(https://arxiv.org/abs/2405.01601)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Encoder-decoder foundation models have displayed state-of-the-art performance on a range of autoregressive sequence tasks. This paper proposes a simple and lightweight modification to such systems to control the behaviour according to a specific attribute of interest. This paper proposes a novel inference-efficient approach to modifying the behaviour of an encoder-decoder system according to a specific attribute of interest. Specifically, we show that a small proxy network can be used to find a sample-by-sample perturbation of the encoder output of a frozen foundation model to trigger the decoder to generate improved decodings. This work explores a specific realization of this framework focused on improving the COMET performance of Flan-T5 on Machine Translation and the WER of Whisper foundation models on Speech Recognition. Results display consistent improvements in performance evaluated through COMET and WER respectively. Furthermore, experiments also show that the proxies are robust to the exact nature of the data used to train them and can extend to other domains.</li>
</ul>

<h3>Title: KITE: A Kernel-based Improved Transferability Estimation Method</h3>
<ul>
<li><strong>Authors: </strong>Yunhui Guo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01603">https://arxiv.org/abs/2405.01603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01603">https://arxiv.org/pdf/2405.01603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01603]] KITE: A Kernel-based Improved Transferability Estimation Method(https://arxiv.org/abs/2405.01603)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Transferability estimation has emerged as an important problem in transfer learning. A transferability estimation method takes as inputs a set of pre-trained models and decides which pre-trained model can deliver the best transfer learning performance. Existing methods tackle this problem by analyzing the output of the pre-trained model or by comparing the pre-trained model with a probe model trained on the target dataset. However, neither is sufficient to provide reliable and efficient transferability estimations. In this paper, we present a novel perspective and introduce Kite, as a Kernel-based Improved Transferability Estimation method. Kite is based on the key observations that the separability of the pre-trained features and the similarity of the pre-trained features to random features are two important factors for estimating transferability. Inspired by kernel methods, Kite adopts centered kernel alignment as an effective way to assess feature separability and feature similarity. Kite is easy to interpret, fast to compute, and robust to the target dataset size. We evaluate the performance of Kite on a recently introduced large-scale model selection benchmark. The benchmark contains 8 source dataset, 6 target datasets and 4 architectures with a total of 32 pre-trained models. Extensive results show that Kite outperforms existing methods by a large margin for transferability estimation.</li>
</ul>

<h3>Title: Wildfire Risk Prediction: A Review</h3>
<ul>
<li><strong>Authors: </strong>Zhengsen Xu, Jonathan Li, Linlin Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01607">https://arxiv.org/abs/2405.01607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01607">https://arxiv.org/pdf/2405.01607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01607]] Wildfire Risk Prediction: A Review(https://arxiv.org/abs/2405.01607)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Wildfires have significant impacts on global vegetation, wildlife, and humans. They destroy plant communities and wildlife habitats and contribute to increased emissions of carbon dioxide, nitrogen oxides, methane, and other pollutants. The prediction of wildfires relies on various independent variables combined with regression or machine learning methods. In this technical review, we describe the options for independent variables, data processing techniques, models, independent variables collinearity and importance estimation methods, and model performance evaluation metrics. First, we divide the independent variables into 4 aspects, including climate and meteorology conditions, socio-economical factors, terrain and hydrological features, and wildfire historical records. Second, preprocessing methods are described for different magnitudes, different spatial-temporal resolutions, and different formats of data. Third, the collinearity and importance evaluation methods of independent variables are also considered. Fourth, we discuss the application of statistical models, traditional machine learning models, and deep learning models in wildfire risk prediction. In this subsection, compared with other reviews, this manuscript particularly discusses the evaluation metrics and recent advancements in deep learning methods. Lastly, addressing the limitations of current research, this paper emphasizes the need for more effective deep learning time series forecasting algorithms, the utilization of three-dimensional data including ground and trunk fuel, extraction of more accurate historical fire point data, and improved model evaluation metrics.</li>
</ul>

<h3>Title: Automating the Analysis of Public Saliency and Attitudes towards  Biodiversity from Digital Media</h3>
<ul>
<li><strong>Authors: </strong>Noah Giebink, Amrita Gupta, Diogo Verìssimo, Charlotte H. Chang, Tony Chang, Angela Brennan, Brett Dickson, Alex Bowmer, Jonathan Baillie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01610">https://arxiv.org/abs/2405.01610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01610">https://arxiv.org/pdf/2405.01610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01610]] Automating the Analysis of Public Saliency and Attitudes towards  Biodiversity from Digital Media(https://arxiv.org/abs/2405.01610)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Measuring public attitudes toward wildlife provides crucial insights into our relationship with nature and helps monitor progress toward Global Biodiversity Framework targets. Yet, conducting such assessments at a global scale is challenging. Manually curating search terms for querying news and social media is tedious, costly, and can lead to biased results. Raw news and social media data returned from queries are often cluttered with irrelevant content and syndicated articles. We aim to overcome these challenges by leveraging modern Natural Language Processing (NLP) tools. We introduce a folk taxonomy approach for improved search term generation and employ cosine similarity on Term Frequency-Inverse Document Frequency vectors to filter syndicated articles. We also introduce an extensible relevance filtering pipeline which uses unsupervised learning to reveal common topics, followed by an open-source zero-shot Large Language Model (LLM) to assign topics to news article titles, which are then used to assign relevance. Finally, we conduct sentiment, topic, and volume analyses on resulting data. We illustrate our methodology with a case study of news and X (formerly Twitter) data before and during the COVID-19 pandemic for various mammal taxa, including bats, pangolins, elephants, and gorillas. During the data collection period, up to 62% of articles including keywords pertaining to bats were deemed irrelevant to biodiversity, underscoring the importance of relevance filtering. At the pandemic's onset, we observed increased volume and a significant sentiment shift toward horseshoe bats, which were implicated in the pandemic, but not for other focal taxa. The proposed methods open the door to conservation practitioners applying modern and emerging NLP tools, including LLMs "out of the box," to analyze public perceptions of biodiversity during current events or campaigns.</li>
</ul>

<h3>Title: Unifying and extending Precision Recall metrics for assessing generative  models</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Sykes, Loic Simon, Julien Rabin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01611">https://arxiv.org/abs/2405.01611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01611">https://arxiv.org/pdf/2405.01611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01611]] Unifying and extending Precision Recall metrics for assessing generative  models(https://arxiv.org/abs/2405.01611)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the recent success of generative models in image and text, the evaluation of generative models has gained a lot of attention. Whereas most generative models are compared in terms of scalar values such as Frechet Inception Distance (FID) or Inception Score (IS), in the last years (Sajjadi et al., 2018) proposed a definition of precision-recall curve to characterize the closeness of two distributions. Since then, various approaches to precision and recall have seen the light (Kynkaanniemi et al., 2019; Naeem et al., 2020; Park & Kim, 2023). They center their attention on the extreme values of precision and recall, but apart from this fact, their ties are elusive. In this paper, we unify most of these approaches under the same umbrella, relying on the work of (Simon et al., 2019). Doing so, we were able not only to recover entire curves, but also to expose the sources of the accounted pitfalls of the concerned metrics. We also provide consistency results that go well beyond the ones presented in the corresponding literature. Last, we study the different behaviors of the curves obtained experimentally.</li>
</ul>

<h3>Title: Explainable AI (XAI) in Image Segmentation in Medicine, Industry, and  Beyond: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Rokas Gipiškis, Chun-Wei Tsai, Olga Kurasova</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01636">https://arxiv.org/abs/2405.01636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01636">https://arxiv.org/pdf/2405.01636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01636]] Explainable AI (XAI) in Image Segmentation in Medicine, Industry, and  Beyond: A Survey(https://arxiv.org/abs/2405.01636)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, segmentation</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (XAI) has found numerous applications in computer vision. While image classification-based explainability techniques have garnered significant attention, their counterparts in semantic segmentation have been relatively neglected. Given the prevalent use of image segmentation, ranging from medical to industrial deployments, these techniques warrant a systematic look. In this paper, we present the first comprehensive survey on XAI in semantic image segmentation. This work focuses on techniques that were either specifically introduced for dense prediction tasks or were extended for them by modifying existing methods in classification. We analyze and categorize the literature based on application categories and domains, as well as the evaluation metrics and datasets used. We also propose a taxonomy for interpretable semantic segmentation, and discuss potential challenges and future research directions.</li>
</ul>

<h3>Title: Explaining models relating objects and privacy</h3>
<ul>
<li><strong>Authors: </strong>Alessio Xompero, Myriam Bontonou, Jean-Michel Arbona, Emmanouil Benetos, Andrea Cavallaro</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01646">https://arxiv.org/abs/2405.01646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01646">https://arxiv.org/pdf/2405.01646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01646]] Explaining models relating objects and privacy(https://arxiv.org/abs/2405.01646)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Accurately predicting whether an image is private before sharing it online is difficult due to the vast variety of content and the subjective nature of privacy itself. In this paper, we evaluate privacy models that use objects extracted from an image to determine why the image is predicted as private. To explain the decision of these models, we use feature-attribution to identify and quantify which objects (and which of their features) are more relevant to privacy classification with respect to a reference input (i.e., no objects localised in an image) predicted as public. We show that the presence of the person category and its cardinality is the main factor for the privacy decision. Therefore, these models mostly fail to identify private images depicting documents with sensitive data, vehicle ownership, and internet activity, or public images with people (e.g., an outdoor concert or people walking in a public space next to a famous landmark). As baselines for future benchmarks, we also devise two strategies that are based on the person presence and cardinality and achieve comparable classification performance of the privacy models.</li>
</ul>

<h3>Title: Improving Complex Reasoning over Knowledge Graph with Logic-Aware  Curriculum Tuning</h3>
<ul>
<li><strong>Authors: </strong>Tianle Xia, Liang Ding, Guojia Wan, Yibing Zhan, Bo Du, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01649">https://arxiv.org/abs/2405.01649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01649">https://arxiv.org/pdf/2405.01649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01649]] Improving Complex Reasoning over Knowledge Graph with Logic-Aware  Curriculum Tuning(https://arxiv.org/abs/2405.01649)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Answering complex logical queries over incomplete knowledge graphs (KGs) is challenging. Most previous works have focused on learning entity/relation embeddings and simulating first-order logic operators with various neural networks. However, they are bottlenecked by the inability to share world knowledge to improve logical reasoning, thus resulting in suboptimal performance. In this paper, we propose a complex logical reasoning schema over knowledge graphs upon large language models (LLMs), containing a curriculum-based logical-aware instruction tuning framework, named LACT. Specifically, we augment the arbitrary first-order logical queries via binary tree decomposition, to stimulate the reasoning capability of LLMs. To address the difficulty gap among different types of complex queries, we design a simple and flexible logic-aware curriculum learning framework. Experiments across widely used datasets demonstrate that LACT has substantial improvements~(brings an average +5.5% MRR score) over advanced methods, achieving the new state-of-the-art. Our code and model will be released at GitHub and huggingface soon.</li>
</ul>

<h3>Title: Key Patches Are All You Need: A Multiple Instance Learning Framework For  Robust Medical Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Diogo J. Araújo, M. Rita Verdelho, Alceu Bissoto, Jacinto C. Nascimento, Carlos Santiago, Catarina Barata</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01654">https://arxiv.org/abs/2405.01654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01654">https://arxiv.org/pdf/2405.01654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01654]] Key Patches Are All You Need: A Multiple Instance Learning Framework For  Robust Medical Diagnosis(https://arxiv.org/abs/2405.01654)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning models have revolutionized the field of medical image analysis, due to their outstanding performances. However, they are sensitive to spurious correlations, often taking advantage of dataset bias to improve results for in-domain data, but jeopardizing their generalization capabilities. In this paper, we propose to limit the amount of information these models use to reach the final classification, by using a multiple instance learning (MIL) framework. MIL forces the model to use only a (small) subset of patches in the image, identifying discriminative regions. This mimics the clinical procedures, where medical decisions are based on localized findings. We evaluate our framework on two medical applications: skin cancer diagnosis using dermoscopy and breast cancer diagnosis using mammography. Our results show that using only a subset of the patches does not compromise diagnostic performance for in-domain data, compared to the baseline approaches. However, our approach is more robust to shifts in patient demographics, while also providing more detailed explanations about which regions contributed to the decision. Code is available at: https://github.com/diogojpa99/MedicalMultiple-Instance-Learning.</li>
</ul>

<h3>Title: S4: Self-Supervised Sensing Across the Spectrum</h3>
<ul>
<li><strong>Authors: </strong>Jayanth Shenoy, Xinjian Davis Zhang, Shlok Mehrotra, Bill Tao, Rem Yang, Han Zhao, Deepak Vasisht</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01656">https://arxiv.org/abs/2405.01656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01656">https://arxiv.org/pdf/2405.01656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01656]] S4: Self-Supervised Sensing Across the Spectrum(https://arxiv.org/abs/2405.01656)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Satellite image time series (SITS) segmentation is crucial for many applications like environmental monitoring, land cover mapping and agricultural crop type classification. However, training models for SITS segmentation remains a challenging task due to the lack of abundant training data, which requires fine grained annotation. We propose S4 a new self-supervised pre-training approach that significantly reduces the requirement for labeled training data by utilizing two new insights: (a) Satellites capture images in different parts of the spectrum such as radio frequencies, and visible frequencies. (b) Satellite imagery is geo-registered allowing for fine-grained spatial alignment. We use these insights to formulate pre-training tasks in S4. We also curate m2s2-SITS, a large-scale dataset of unlabeled, spatially-aligned, multi-modal and geographic specific SITS that serves as representative pre-training data for S4. Finally, we evaluate S4 on multiple SITS segmentation datasets and demonstrate its efficacy against competing baselines while using limited labeled data.</li>
</ul>

<h3>Title: Investigating Wit, Creativity, and Detectability of Large Language  Models in Domain-Specific Writing Style Adaptation of Reddit's Showerthoughts</h3>
<ul>
<li><strong>Authors: </strong>Tolga Buz, Benjamin Frost, Nikola Genchev, Moritz Schneider, Lucie-Aimée Kaffee, Gerard de Melo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01660">https://arxiv.org/abs/2405.01660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01660">https://arxiv.org/pdf/2405.01660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01660]] Investigating Wit, Creativity, and Detectability of Large Language  Models in Domain-Specific Writing Style Adaptation of Reddit's Showerthoughts(https://arxiv.org/abs/2405.01660)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent Large Language Models (LLMs) have shown the ability to generate content that is difficult or impossible to distinguish from human writing. We investigate the ability of differently-sized LLMs to replicate human writing style in short, creative texts in the domain of Showerthoughts, thoughts that may occur during mundane activities. We compare GPT-2 and GPT-Neo fine-tuned on Reddit data as well as GPT-3.5 invoked in a zero-shot manner, against human-authored texts. We measure human preference on the texts across the specific dimensions that account for the quality of creative, witty texts. Additionally, we compare the ability of humans versus fine-tuned RoBERTa classifiers to detect AI-generated texts. We conclude that human evaluators rate the generated texts slightly worse on average regarding their creative quality, but they are unable to reliably distinguish between human-written and AI-generated texts. We further provide a dataset for creative, witty text generation based on Reddit Showerthoughts posts.</li>
</ul>

<h3>Title: WitheredLeaf: Finding Entity-Inconsistency Bugs with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hongbo Chen, Yifan Zhang, Xing Han, Huanyao Rong, Yuheng Zhang, Tianhao Mao, Hang Zhang, XiaoFeng Wang, Luyi Xing, Xun Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01668">https://arxiv.org/abs/2405.01668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01668">https://arxiv.org/pdf/2405.01668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01668]] WitheredLeaf: Finding Entity-Inconsistency Bugs with LLMs(https://arxiv.org/abs/2405.01668)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Originating from semantic bugs, Entity-Inconsistency Bugs (EIBs) involve misuse of syntactically valid yet incorrect program entities, such as variable identifiers and function names, which often have security implications. Unlike straightforward syntactic vulnerabilities, EIBs are subtle and can remain undetected for years. Traditional detection methods, such as static analysis and dynamic testing, often fall short due to the versatile and context-dependent nature of EIBs. However, with advancements in Large Language Models (LLMs) like GPT-4, we believe LLM-powered automatic EIB detection becomes increasingly feasible through these models' semantics understanding abilities. This research first undertakes a systematic measurement of LLMs' capabilities in detecting EIBs, revealing that GPT-4, while promising, shows limited recall and precision that hinder its practical application. The primary problem lies in the model's tendency to focus on irrelevant code snippets devoid of EIBs. To address this, we introduce a novel, cascaded EIB detection system named WitheredLeaf, which leverages smaller, code-specific language models to filter out most negative cases and mitigate the problem, thereby significantly enhancing the overall precision and recall. We evaluated WitheredLeaf on 154 Python and C GitHub repositories, each with over 1,000 stars, identifying 123 new flaws, 45% of which can be exploited to disrupt the program's normal operations. Out of 69 submitted fixes, 27 have been successfully merged.</li>
</ul>

<h3>Title: Generative AI in Cybersecurity</h3>
<ul>
<li><strong>Authors: </strong>Shivani Metta, Isaac Chang, Jack Parker, Michael P. Roman, Arturo F. Ehuan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01674">https://arxiv.org/abs/2405.01674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01674">https://arxiv.org/pdf/2405.01674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01674]] Generative AI in Cybersecurity(https://arxiv.org/abs/2405.01674)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>The dawn of Generative Artificial Intelligence (GAI), characterized by advanced models such as Generative Pre-trained Transformers (GPT) and other Large Language Models (LLMs), has been pivotal in reshaping the field of data analysis, pattern recognition, and decision-making processes. This surge in GAI technology has ushered in not only innovative opportunities for data processing and automation but has also introduced significant cybersecurity challenges. As GAI rapidly progresses, it outstrips the current pace of cybersecurity protocols and regulatory frameworks, leading to a paradox wherein the same innovations meant to safeguard digital infrastructures also enhance the arsenal available to cyber criminals. These adversaries, adept at swiftly integrating and exploiting emerging technologies, may utilize GAI to develop malware that is both more covert and adaptable, thus complicating traditional cybersecurity efforts. The acceleration of GAI presents an ambiguous frontier for cybersecurity experts, offering potent tools for threat detection and response, while concurrently providing cyber attackers with the means to engineer more intricate and potent malware. Through the joint efforts of Duke Pratt School of Engineering, Coalfire, and Safebreach, this research undertakes a meticulous analysis of how malicious agents are exploiting GAI to augment their attack strategies, emphasizing a critical issue for the integrity of future cybersecurity initiatives. The study highlights the critical need for organizations to proactively identify and develop more complex defensive strategies to counter the sophisticated employment of GAI in malware creation.</li>
</ul>

<h3>Title: 1-Diffractor: Efficient and Utility-Preserving Text Obfuscation  Leveraging Word-Level Metric Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Stephen Meisenbacher, Maulik Chevli, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01678">https://arxiv.org/abs/2405.01678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01678">https://arxiv.org/pdf/2405.01678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01678]] 1-Diffractor: Efficient and Utility-Preserving Text Obfuscation  Leveraging Word-Level Metric Differential Privacy(https://arxiv.org/abs/2405.01678)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The study of privacy-preserving Natural Language Processing (NLP) has gained rising attention in recent years. One promising avenue studies the integration of Differential Privacy in NLP, which has brought about innovative methods in a variety of application settings. Of particular note are $\textit{word-level Metric Local Differential Privacy (MLDP)}$ mechanisms, which work to obfuscate potentially sensitive input text by performing word-by-word $\textit{perturbations}$. Although these methods have shown promising results in empirical tests, there are two major drawbacks: (1) the inevitable loss of utility due to addition of noise, and (2) the computational expensiveness of running these mechanisms on high-dimensional word embeddings. In this work, we aim to address these challenges by proposing $\texttt{1-Diffractor}$, a new mechanism that boasts high speedups in comparison to previous mechanisms, while still demonstrating strong utility- and privacy-preserving capabilities. We evaluate $\texttt{1-Diffractor}$ for utility on several NLP tasks, for theoretical and task-based privacy, and for efficiency in terms of speed and memory. $\texttt{1-Diffractor}$ shows significant improvements in efficiency, while still maintaining competitive utility and privacy scores across all conducted comparative tests against previous MLDP mechanisms. Our code is made available at: https://github.com/sjmeis/Diffractor.</li>
</ul>

<h3>Title: Leveraging Prompt-Learning for Structured Information Extraction from  Crohn's Disease Radiology Reports in a Low-Resource Language</h3>
<ul>
<li><strong>Authors: </strong>Liam Hazan, Gili Focht, Naama Gavrielov, Roi Reichart, Talar Hagopian, Mary-Louise C. Greer, Ruth Cytter Kuint, Dan Turner, Moti Freiman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01682">https://arxiv.org/abs/2405.01682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01682">https://arxiv.org/pdf/2405.01682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01682]] Leveraging Prompt-Learning for Structured Information Extraction from  Crohn's Disease Radiology Reports in a Low-Resource Language(https://arxiv.org/abs/2405.01682)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Automatic conversion of free-text radiology reports into structured data using Natural Language Processing (NLP) techniques is crucial for analyzing diseases on a large scale. While effective for tasks in widely spoken languages like English, generative large language models (LLMs) typically underperform with less common languages and can pose potential risks to patient privacy. Fine-tuning local NLP models is hindered by the skewed nature of real-world medical datasets, where rare findings represent a significant data imbalance. We introduce SMP-BERT, a novel prompt learning method that leverages the structured nature of reports to overcome these challenges. In our studies involving a substantial collection of Crohn's disease radiology reports in Hebrew (over 8,000 patients and 10,000 reports), SMP-BERT greatly surpassed traditional fine-tuning methods in performance, notably in detecting infrequent conditions (AUC: 0.99 vs 0.94, F1: 0.84 vs 0.34). SMP-BERT empowers more accurate AI diagnostics available for low-resource languages.</li>
</ul>

<h3>Title: Automatically Extracting Numerical Results from Randomized Controlled  Trials with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hye Sun Yun, David Pogrebitskiy, Iain J. Marshall, Byron C. Wallace</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01686">https://arxiv.org/abs/2405.01686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01686">https://arxiv.org/pdf/2405.01686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01686]] Automatically Extracting Numerical Results from Randomized Controlled  Trials with Large Language Models(https://arxiv.org/abs/2405.01686)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Meta-analyses statistically aggregate the findings of different randomized controlled trials (RCTs) to assess treatment effectiveness. Because this yields robust estimates of treatment effectiveness, results from meta-analyses are considered the strongest form of evidence. However, rigorous evidence syntheses are time-consuming and labor-intensive, requiring manual extraction of data from individual trials to be synthesized. Ideally, language technologies would permit fully automatic meta-analysis, on demand. This requires accurately extracting numerical results from individual trials, which has been beyond the capabilities of natural language processing (NLP) models to date. In this work, we evaluate whether modern large language models (LLMs) can reliably perform this task. We annotate (and release) a modest but granular evaluation dataset of clinical trial reports with numerical findings attached to interventions, comparators, and outcomes. Using this dataset, we evaluate the performance of seven LLMs applied zero-shot for the task of conditionally extracting numerical findings from trial reports. We find that massive LLMs that can accommodate lengthy inputs are tantalizingly close to realizing fully automatic meta-analysis, especially for dichotomous (binary) outcomes (e.g., mortality). However, LLMs -- including ones trained on biomedical texts -- perform poorly when the outcome measures are complex and tallying the results requires inference. This work charts a path toward fully automatic meta-analysis of RCTs via LLMs, while also highlighting the limitations of existing models for this aim.</li>
</ul>

<h3>Title: Adversarial Attacks on Reinforcement Learning Agents for Command and  Control</h3>
<ul>
<li><strong>Authors: </strong>Ahaan Dabholkar, James Z. Hare, Mark Mittrick, John Richardson, Nicholas Waytowich, Priya Narayanan, Saurabh Bagchi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01693">https://arxiv.org/abs/2405.01693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01693">https://arxiv.org/pdf/2405.01693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01693]] Adversarial Attacks on Reinforcement Learning Agents for Command and  Control(https://arxiv.org/abs/2405.01693)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Given the recent impact of Deep Reinforcement Learning in training agents to win complex games like StarCraft and DoTA(Defense Of The Ancients) - there has been a surge in research for exploiting learning based techniques for professional wargaming, battlefield simulation and modeling. Real time strategy games and simulators have become a valuable resource for operational planning and military research. However, recent work has shown that such learning based approaches are highly susceptible to adversarial perturbations. In this paper, we investigate the robustness of an agent trained for a Command and Control task in an environment that is controlled by an active adversary. The C2 agent is trained on custom StarCraft II maps using the state of the art RL algorithms - A3C and PPO. We empirically show that an agent trained using these algorithms is highly susceptible to noise injected by the adversary and investigate the effects these perturbations have on the performance of the trained agent. Our work highlights the urgent need to develop more robust training algorithms especially for critical arenas like the battlefield.</li>
</ul>

<h3>Title: SOAR: Advancements in Small Body Object Detection for Aerial Imagery  Using State Space Models and Programmable Gradients</h3>
<ul>
<li><strong>Authors: </strong>Tushar Verma, Jyotsna Singh, Yash Bhartari, Rishi Jarwal, Suraj Singh, Shubhkarman Singh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01699">https://arxiv.org/abs/2405.01699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01699">https://arxiv.org/pdf/2405.01699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01699]] SOAR: Advancements in Small Body Object Detection for Aerial Imagery  Using State Space Models and Programmable Gradients(https://arxiv.org/abs/2405.01699)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Small object detection in aerial imagery presents significant challenges in computer vision due to the minimal data inherent in small-sized objects and their propensity to be obscured by larger objects and background noise. Traditional methods using transformer-based models often face limitations stemming from the lack of specialized databases, which adversely affect their performance with objects of varying orientations and scales. This underscores the need for more adaptable, lightweight models. In response, this paper introduces two innovative approaches that significantly enhance detection and segmentation capabilities for small aerial objects. Firstly, we explore the use of the SAHI framework on the newly introduced lightweight YOLO v9 architecture, which utilizes Programmable Gradient Information (PGI) to reduce the substantial information loss typically encountered in sequential feature extraction processes. The paper employs the Vision Mamba model, which incorporates position embeddings to facilitate precise location-aware visual understanding, combined with a novel bidirectional State Space Model (SSM) for effective visual context modeling. This State Space Model adeptly harnesses the linear complexity of CNNs and the global receptive field of Transformers, making it particularly effective in remote sensing image classification. Our experimental results demonstrate substantial improvements in detection accuracy and processing efficiency, validating the applicability of these approaches for real-time small object detection across diverse aerial scenarios. This paper also discusses how these methodologies could serve as foundational models for future advancements in aerial object recognition technologies. The source code will be made accessible here.</li>
</ul>

<h3>Title: Active Learning Enabled Low-cost Cell Image Segmentation Using Bounding  Box Annotation</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhu, Qiang Yang, Li Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01701">https://arxiv.org/abs/2405.01701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01701">https://arxiv.org/pdf/2405.01701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01701]] Active Learning Enabled Low-cost Cell Image Segmentation Using Bounding  Box Annotation(https://arxiv.org/abs/2405.01701)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Cell image segmentation is usually implemented using fully supervised deep learning methods, which heavily rely on extensive annotated training data. Yet, due to the complexity of cell morphology and the requirement for specialized knowledge, pixel-level annotation of cell images has become a highly labor-intensive task. To address the above problems, we propose an active learning framework for cell segmentation using bounding box annotations, which greatly reduces the data annotation cost of cell segmentation algorithms. First, we generate a box-supervised learning method (denoted as YOLO-SAM) by combining the YOLOv8 detector with the Segment Anything Model (SAM), which effectively reduces the complexity of data annotation. Furthermore, it is integrated into an active learning framework that employs the MC DropBlock method to train the segmentation model with fewer box-annotated samples. Extensive experiments demonstrate that our model saves more than ninety percent of data annotation time compared to mask-supervised deep learning methods.</li>
</ul>

<h3>Title: Privacy-aware Berrut Approximated Coded Computing for Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Xavier Martínez Luaña, Rebeca P. Díaz Redondo, Manuel Fernández Veiga</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CC, cs.DC, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01704">https://arxiv.org/abs/2405.01704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01704">https://arxiv.org/pdf/2405.01704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01704]] Privacy-aware Berrut Approximated Coded Computing for Federated Learning(https://arxiv.org/abs/2405.01704)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is an interesting strategy that enables the collaborative training of an AI model among different data owners without revealing their private datasets. Even so, FL has some privacy vulnerabilities that have been tried to be overcome by applying some techniques like Differential Privacy (DP), Homomorphic Encryption (HE), or Secure Multi-Party Computation (SMPC). However, these techniques have some important drawbacks that might narrow their range of application: problems to work with non-linear functions and to operate large matrix multiplications and high communication and computational costs to manage semi-honest nodes. In this context, we propose a solution to guarantee privacy in FL schemes that simultaneously solves the previously mentioned problems. Our proposal is based on the Berrut Approximated Coded Computing, a technique from the Coded Distributed Computing paradigm, adapted to a Secret Sharing configuration, to provide input privacy to FL in a scalable way. It can be applied for computing non-linear functions and treats the special case of distributed matrix multiplication, a key primitive at the core of many automated learning tasks. Because of these characteristics, it could be applied in a wide range of FL scenarios, since it is independent of the machine learning models or aggregation algorithms used in the FL scheme. We provide analysis of the achieve privacy and complexity of our solution and, due to the extensive numerical results performed, it can be observed a good trade-off between privacy and precision.</li>
</ul>

<h3>Title: Long Tail Image Generation Through Feature Space Augmentation and  Iterated Learning</h3>
<ul>
<li><strong>Authors: </strong>Rafael Elberg, Denis Parra, Mircea Petrache</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01705">https://arxiv.org/abs/2405.01705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01705">https://arxiv.org/pdf/2405.01705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01705]] Long Tail Image Generation Through Feature Space Augmentation and  Iterated Learning(https://arxiv.org/abs/2405.01705)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>Image and multimodal machine learning tasks are very challenging to solve in the case of poorly distributed data. In particular, data availability and privacy restrictions exacerbate these hurdles in the medical domain. The state of the art in image generation quality is held by Latent Diffusion models, making them prime candidates for tackling this problem. However, a few key issues still need to be solved, such as the difficulty in generating data from under-represented classes and a slow inference process. To mitigate these issues, we propose a new method for image augmentation in long-tailed data based on leveraging the rich latent space of pre-trained Stable Diffusion Models. We create a modified separable latent space to mix head and tail class examples. We build this space via Iterated Learning of underlying sparsified embeddings, which we apply to task-specific saliency maps via a K-NN approach. Code is available at https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</li>
</ul>

<h3>Title: A deep causal inference model for fully-interpretable travel behaviour  analysis</h3>
<ul>
<li><strong>Authors: </strong>Kimia Kamal, Bilal Farooq</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01708">https://arxiv.org/abs/2405.01708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01708">https://arxiv.org/pdf/2405.01708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01708]] A deep causal inference model for fully-interpretable travel behaviour  analysis(https://arxiv.org/abs/2405.01708)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Transport policy assessment often involves causal questions, yet the causal inference capabilities of traditional travel behavioural models are at best limited. We present the deep CAusal infeRence mOdel for traveL behavIour aNAlysis (CAROLINA), a framework that explicitly models causality in travel behaviour, enhances predictive accuracy, and maintains interpretability by leveraging causal inference, deep learning, and traditional discrete choice modelling. Within this framework, we introduce a Generative Counterfactual model for forecasting human behaviour by adapting the Normalizing Flow method. Through the case studies of virtual reality-based pedestrian crossing behaviour, revealed preference travel behaviour from London, and synthetic data, we demonstrate the effectiveness of our proposed models in uncovering causal relationships, prediction accuracy, and assessing policy interventions. Our results show that intervention mechanisms that can reduce pedestrian stress levels lead to a 38.5% increase in individuals experiencing shorter waiting times. Reducing the travel distances in London results in a 47% increase in sustainable travel modes.</li>
</ul>

<h3>Title: Individual Fairness Through Reweighting and Tuning</h3>
<ul>
<li><strong>Authors: </strong>Abdoul Jalil Djiberou Mahamadou, Lea Goetz, Russ Altman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01711">https://arxiv.org/abs/2405.01711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01711">https://arxiv.org/pdf/2405.01711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01711]] Individual Fairness Through Reweighting and Tuning(https://arxiv.org/abs/2405.01711)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Inherent bias within society can be amplified and perpetuated by artificial intelligence (AI) systems. To address this issue, a wide range of solutions have been proposed to identify and mitigate bias and enforce fairness for individuals and groups. Recently, Graph Laplacian Regularizer (GLR), a regularization technique from the semi-supervised learning literature has been used as a substitute for the common Lipschitz condition to enhance individual fairness (IF). Notable prior work has shown that enforcing IF through a GLR can improve the transfer learning accuracy of AI models under covariate shifts. However, the prior work defines a GLR on the source and target data combined, implicitly assuming that the target data are available at train time, which might not hold in practice. In this work, we investigated whether defining a GLR independently on the train and target data could maintain similar accuracy compared to the prior work model. Furthermore, we introduced the Normalized Fairness Gain score (FGN) to measure IF for in-processing algorithmic fairness techniques. FGN quantifies the amount of gained fairness when a GLR is used versus not. We evaluated the new and original methods under FGN, the Prediction Consistency (PC), and traditional classification metrics on the German Credit Approval dataset. The results showed that the two models achieved similar statistical mean performances over five-fold cross-validation. Furthermore, the proposed metric showed that PC scores can be misleading as the scores can be high and statistically similar to fairness-enhanced models while FGN scores are small. This work therefore provides new insights into when a GLR effectively enhances IF and the pitfalls of PC.</li>
</ul>

<h3>Title: Interpretable Vital Sign Forecasting with Model Agnostic Attention Maps</h3>
<ul>
<li><strong>Authors: </strong>Yuwei Liu, Chen Dan, Anubhav Bhatti, Bingjie Shen, Divij Gupta, Suraj Parmar, San Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01714">https://arxiv.org/abs/2405.01714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01714">https://arxiv.org/pdf/2405.01714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01714]] Interpretable Vital Sign Forecasting with Model Agnostic Attention Maps(https://arxiv.org/abs/2405.01714)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Sepsis is a leading cause of mortality in intensive care units (ICUs), representing a substantial medical challenge. The complexity of analyzing diverse vital signs to predict sepsis further aggravates this issue. While deep learning techniques have been advanced for early sepsis prediction, their 'black-box' nature obscures the internal logic, impairing interpretability in critical settings like ICUs. This paper introduces a framework that combines a deep learning model with an attention mechanism that highlights the critical time steps in the forecasting process, thus improving model interpretability and supporting clinical decision-making. We show that the attention mechanism could be adapted to various black box time series forecasting models such as N-HiTS and N-BEATS. Our method preserves the accuracy of conventional deep learning models while enhancing interpretability through attention-weight-generated heatmaps. We evaluated our model on the eICU-CRD dataset, focusing on forecasting vital signs for sepsis patients. We assessed its performance using mean squared error (MSE) and dynamic time warping (DTW) metrics. We explored the attention maps of N-HiTS and N-BEATS, examining the differences in their performance and identifying crucial factors influencing vital sign forecasting.</li>
</ul>

<h3>Title: ATTAXONOMY: Unpacking Differential Privacy Guarantees Against Practical  Adversaries</h3>
<ul>
<li><strong>Authors: </strong>Rachel Cummings, Shlomi Hod, Jayshree Sarathy, Marika Swanberg</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01716">https://arxiv.org/abs/2405.01716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01716">https://arxiv.org/pdf/2405.01716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01716]] ATTAXONOMY: Unpacking Differential Privacy Guarantees Against Practical  Adversaries(https://arxiv.org/abs/2405.01716)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, robust, membership infer</a></li>
<li><strong>Abstract: </strong>Differential Privacy (DP) is a mathematical framework that is increasingly deployed to mitigate privacy risks associated with machine learning and statistical analyses. Despite the growing adoption of DP, its technical privacy parameters do not lend themselves to an intelligible description of the real-world privacy risks associated with that deployment: the guarantee that most naturally follows from the DP definition is protection against membership inference by an adversary who knows all but one data record and has unlimited auxiliary knowledge. In many settings, this adversary is far too strong to inform how to set real-world privacy parameters. One approach for contextualizing privacy parameters is via defining and measuring the success of technical attacks, but doing so requires a systematic categorization of the relevant attack space. In this work, we offer a detailed taxonomy of attacks, showing the various dimensions of attacks and highlighting that many real-world settings have been understudied. Our taxonomy provides a roadmap for analyzing real-world deployments and developing theoretical bounds for more informative privacy attacks. We operationalize our taxonomy by using it to analyze a real-world case study, the Israeli Ministry of Health's recent release of a birth dataset using DP, showing how the taxonomy enables fine-grained threat modeling and provides insight towards making informed privacy parameter choices. Finally, we leverage the taxonomy towards defining a more realistic attack than previously considered in the literature, namely a distributional reconstruction attack: we generalize Balle et al.'s notion of reconstruction robustness to a less-informed adversary with distributional uncertainty, and extend the worst-case guarantees of DP to this average-case setting.</li>
</ul>

<h3>Title: Robust Risk-Sensitive Reinforcement Learning with Conditional  Value-at-Risk</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Ni, Lifeng Lai</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01718">https://arxiv.org/abs/2405.01718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01718">https://arxiv.org/pdf/2405.01718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01718]] Robust Risk-Sensitive Reinforcement Learning with Conditional  Value-at-Risk(https://arxiv.org/abs/2405.01718)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust Markov Decision Processes (RMDPs) have received significant research interest, offering an alternative to standard Markov Decision Processes (MDPs) that often assume fixed transition probabilities. RMDPs address this by optimizing for the worst-case scenarios within ambiguity sets. While earlier studies on RMDPs have largely centered on risk-neutral reinforcement learning (RL), with the goal of minimizing expected total discounted costs, in this paper, we analyze the robustness of CVaR-based risk-sensitive RL under RMDP. Firstly, we consider predetermined ambiguity sets. Based on the coherency of CVaR, we establish a connection between robustness and risk sensitivity, thus, techniques in risk-sensitive RL can be adopted to solve the proposed problem. Furthermore, motivated by the existence of decision-dependent uncertainty in real-world problems, we study problems with state-action-dependent ambiguity sets. To solve this, we define a new risk measure named NCVaR and build the equivalence of NCVaR optimization and robust CVaR optimization. We further propose value iteration algorithms and validate our approach in simulation experiments.</li>
</ul>

<h3>Title: Zero-Shot Monocular Motion Segmentation in the Wild by Combining Deep  Learning with Geometric Motion Model Fusion</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Huang, Yuhao Chen, John Zelek</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01723">https://arxiv.org/abs/2405.01723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01723">https://arxiv.org/pdf/2405.01723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01723]] Zero-Shot Monocular Motion Segmentation in the Wild by Combining Deep  Learning with Geometric Motion Model Fusion(https://arxiv.org/abs/2405.01723)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Detecting and segmenting moving objects from a moving monocular camera is challenging in the presence of unknown camera motion, diverse object motions and complex scene structures. Most existing methods rely on a single motion cue to perform motion segmentation, which is usually insufficient when facing different complex environments. While a few recent deep learning based methods are able to combine multiple motion cues to achieve improved accuracy, they depend heavily on vast datasets and extensive annotations, making them less adaptable to new scenarios. To address these limitations, we propose a novel monocular dense segmentation method that achieves state-of-the-art motion segmentation results in a zero-shot manner. The proposed method synergestically combines the strengths of deep learning and geometric model fusion methods by performing geometric model fusion on object proposals. Experiments show that our method achieves competitive results on several motion segmentation datasets and even surpasses some state-of-the-art supervised methods on certain benchmarks, while not being trained on any data. We also present an ablation study to show the effectiveness of combining different geometric models together for motion segmentation, highlighting the value of our geometric model fusion strategy.</li>
</ul>

<h3>Title: Large Language Models are Inconsistent and Biased Evaluators</h3>
<ul>
<li><strong>Authors: </strong>Rickard Stureborg, Dimitris Alikaniotis, Yoshi Suhara</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01724">https://arxiv.org/abs/2405.01724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01724">https://arxiv.org/pdf/2405.01724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01724]] Large Language Models are Inconsistent and Biased Evaluators(https://arxiv.org/abs/2405.01724)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The zero-shot capability of Large Language Models (LLMs) has enabled highly flexible, reference-free metrics for various tasks, making LLM evaluators common tools in NLP. However, the robustness of these LLM evaluators remains relatively understudied; existing work mainly pursued optimal performance in terms of correlating LLM scores with human expert scores. In this paper, we conduct a series of analyses using the SummEval dataset and confirm that LLMs are biased evaluators as they: (1) exhibit familiarity bias-a preference for text with lower perplexity, (2) show skewed and biased distributions of ratings, and (3) experience anchoring effects for multi-attribute judgments. We also found that LLMs are inconsistent evaluators, showing low "inter-sample" agreement and sensitivity to prompt differences that are insignificant to human understanding of text quality. Furthermore, we share recipes for configuring LLM evaluators to mitigate these limitations. Experimental results on the RoSE dataset demonstrate improvements over the state-of-the-art LLM evaluators.</li>
</ul>

<h3>Title: Explainability Guided Adversarial Evasion Attacks on Malware Detectors</h3>
<ul>
<li><strong>Authors: </strong>Kshitiz Aryal, Maanak Gupta, Mahmoud Abdelsalam, Moustafa Saleh</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01728">https://arxiv.org/abs/2405.01728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01728">https://arxiv.org/pdf/2405.01728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01728]] Explainability Guided Adversarial Evasion Attacks on Malware Detectors(https://arxiv.org/abs/2405.01728)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, explainability</a></li>
<li><strong>Abstract: </strong>As the focus on security of Artificial Intelligence (AI) is becoming paramount, research on crafting and inserting optimal adversarial perturbations has become increasingly critical. In the malware domain, this adversarial sample generation relies heavily on the accuracy and placement of crafted perturbation with the goal of evading a trained classifier. This work focuses on applying explainability techniques to enhance the adversarial evasion attack on a machine-learning-based Windows PE malware detector. The explainable tool identifies the regions of PE malware files that have the most significant impact on the decision-making process of a given malware detector, and therefore, the same regions can be leveraged to inject the adversarial perturbation for maximum efficiency. Profiling all the PE malware file regions based on their impact on the malware detector's decision enables the derivation of an efficient strategy for identifying the optimal location for perturbation injection. The strategy should incorporate the region's significance in influencing the malware detector's decision and the sensitivity of the PE malware file's integrity towards modifying that region. To assess the utility of explainable AI in crafting an adversarial sample of Windows PE malware, we utilize the DeepExplainer module of SHAP for determining the contribution of each region of PE malware to its detection by a CNN-based malware detector, MalConv. Furthermore, we analyzed the significance of SHAP values at a more granular level by subdividing each section of Windows PE into small subsections. We then performed an adversarial evasion attack on the subsections based on the corresponding SHAP values of the byte sequences.</li>
</ul>

<h3>Title: Diabetic Retinopathy Detection Using Quantum Transfer Learning</h3>
<ul>
<li><strong>Authors: </strong>Ankush Jain, Rinav Gupta, Jai Singhal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01734">https://arxiv.org/abs/2405.01734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01734">https://arxiv.org/pdf/2405.01734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01734]] Diabetic Retinopathy Detection Using Quantum Transfer Learning(https://arxiv.org/abs/2405.01734)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Diabetic Retinopathy (DR), a prevalent complication in diabetes patients, can lead to vision impairment due to lesions formed on the retina. Detecting DR at an advanced stage often results in irreversible blindness. The traditional process of diagnosing DR through retina fundus images by ophthalmologists is not only time-intensive but also expensive. While classical transfer learning models have been widely adopted for computer-aided detection of DR, their high maintenance costs can hinder their detection efficiency. In contrast, Quantum Transfer Learning offers a more effective solution to this challenge. This approach is notably advantageous because it operates on heuristic principles, making it highly optimized for the task. Our proposed methodology leverages this hybrid quantum transfer learning technique to detect DR. To construct our model, we utilize the APTOS 2019 Blindness Detection dataset, available on Kaggle. We employ the ResNet-18, ResNet34, ResNet50, ResNet101, ResNet152 and Inception V3, pre-trained classical neural networks, for the initial feature extraction. For the classification stage, we use a Variational Quantum Classifier. Our hybrid quantum model has shown remarkable results, achieving an accuracy of 97% for ResNet-18. This demonstrates that quantum computing, when integrated with quantum machine learning, can perform tasks with a level of power and efficiency unattainable by classical computers alone. By harnessing these advanced technologies, we can significantly improve the detection and diagnosis of Diabetic Retinopathy, potentially saving many from the risk of blindness. Keywords: Diabetic Retinopathy, Quantum Transfer Learning, Deep Learning</li>
</ul>

<h3>Title: Question Suggestion for Conversational Shopping Assistants Using Product  Metadata</h3>
<ul>
<li><strong>Authors: </strong>Nikhita Vedula, Oleg Rokhlenko, Shervin Malmasi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01738">https://arxiv.org/abs/2405.01738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01738">https://arxiv.org/pdf/2405.01738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01738]] Question Suggestion for Conversational Shopping Assistants Using Product  Metadata(https://arxiv.org/abs/2405.01738)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Digital assistants have become ubiquitous in e-commerce applications, following the recent advancements in Information Retrieval (IR), Natural Language Processing (NLP) and Generative Artificial Intelligence (AI). However, customers are often unsure or unaware of how to effectively converse with these assistants to meet their shopping needs. In this work, we emphasize the importance of providing customers a fast, easy to use, and natural way to interact with conversational shopping assistants. We propose a framework that employs Large Language Models (LLMs) to automatically generate contextual, useful, answerable, fluent and diverse questions about products, via in-context learning and supervised fine-tuning. Recommending these questions to customers as helpful suggestions or hints to both start and continue a conversation can result in a smoother and faster shopping experience with reduced conversation overhead and friction. We perform extensive offline evaluations, and discuss in detail about potential customer impact, and the type, length and latency of our generated product questions if incorporated into a real-world shopping assistant.</li>
</ul>

<h3>Title: Enhancing User Experience in On-Device Machine Learning with Gated  Compression Layers</h3>
<ul>
<li><strong>Authors: </strong>Haiguang Li, Usama Pervaiz, Joseph Antognini, Michał Matuszak, Lawrence Au, Gilles Roux, Trausti Thormundsso</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01739">https://arxiv.org/abs/2405.01739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01739">https://arxiv.org/pdf/2405.01739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01739]] Enhancing User Experience in On-Device Machine Learning with Gated  Compression Layers(https://arxiv.org/abs/2405.01739)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>On-device machine learning (ODML) enables powerful edge applications, but power consumption remains a key challenge for resource-constrained devices. To address this, developers often face a trade-off between model accuracy and power consumption, employing either computationally intensive models on high-power cores or pared-down models on low-power cores. Both approaches typically lead to a compromise in user experience (UX). This work focuses on the use of Gated Compression (GC) layer to enhance ODML model performance while conserving power and maximizing cost-efficiency, especially for always-on use cases. GC layers dynamically regulate data flow by selectively gating activations of neurons within the neural network and effectively filtering out non-essential inputs, which reduces power needs without compromising accuracy, and enables more efficient execution on heterogeneous compute cores. These improvements enhance UX through prolonged battery life, improved device responsiveness, and greater user comfort. In this work, we have integrated GC layers into vision and speech domain models including the transformer-based ViT model. Our experiments demonstrate theoretical power efficiency gains ranging from 158x to 30,000x for always-on scenarios. This substantial improvement empowers ODML applications with enhanced UX benefits.</li>
</ul>

<h3>Title: The Psychosocial Impacts of Generative AI Harms</h3>
<ul>
<li><strong>Authors: </strong>Faye-Marie Vassel, Evan Shieh, Cassidy R. Sugimoto, Thema Monroe-White</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01740">https://arxiv.org/abs/2405.01740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01740">https://arxiv.org/pdf/2405.01740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01740]] The Psychosocial Impacts of Generative AI Harms(https://arxiv.org/abs/2405.01740)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid emergence of generative Language Models (LMs) has led to growing concern about the impacts that their unexamined adoption may have on the social well-being of diverse user groups. Meanwhile, LMs are increasingly being adopted in K-20 schools and one-on-one student settings with minimal investigation of potential harms associated with their deployment. Motivated in part by real-world/everyday use cases (e.g., an AI writing assistant) this paper explores the potential psychosocial harms of stories generated by five leading LMs in response to open-ended prompting. We extend findings of stereotyping harms analyzing a total of 150K 100-word stories related to student classroom interactions. Examining patterns in LM-generated character demographics and representational harms (i.e., erasure, subordination, and stereotyping) we highlight particularly egregious vignettes, illustrating the ways LM-generated outputs may influence the experiences of users with marginalized and minoritized identities, and emphasizing the need for a critical understanding of the psychosocial impacts of generative AI tools when deployed and utilized in diverse social contexts.</li>
</ul>

<h3>Title: PVF (Parameter Vulnerability Factor): A Quantitative Metric Measuring AI  Vulnerability and Resilience Against Parameter Corruptions</h3>
<ul>
<li><strong>Authors: </strong>Xun Jiao, Fred Lin, Harish D. Dixit, Joel Coburn, Abhinav Pandey, Han Wang, Jianyu Huang, Venkat Ramesh, Wang Xu, Daniel Moore, Sriram Sankar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.AR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01741">https://arxiv.org/abs/2405.01741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01741">https://arxiv.org/pdf/2405.01741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01741]] PVF (Parameter Vulnerability Factor): A Quantitative Metric Measuring AI  Vulnerability and Resilience Against Parameter Corruptions(https://arxiv.org/abs/2405.01741)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Reliability of AI systems is a fundamental concern for the successful deployment and widespread adoption of AI technologies. Unfortunately, the escalating complexity and heterogeneity of AI hardware systems make them inevitably and increasingly susceptible to hardware faults (e.g., bit flips) that can potentially corrupt model parameters. Given this challenge, this paper aims to answer a critical question: How likely is a parameter corruption to result in an incorrect model output? To systematically answer this question, we propose a novel quantitative metric, Parameter Vulnerability Factor (PVF), inspired by architectural vulnerability factor (AVF) in computer architecture community, aiming to standardize the quantification of AI model resilience/vulnerability against parameter corruptions. We define a model parameter's PVF as the probability that a corruption in that particular model parameter will result in an incorrect output. Similar to AVF, this statistical concept can be derived from statistically extensive and meaningful fault injection (FI) experiments. In this paper, we present several use cases on applying PVF to three types of tasks/models during inference -- recommendation (DLRM), vision classification (CNN), and text classification (BERT). PVF can provide pivotal insights to AI hardware designers in balancing the tradeoff between fault protection and performance/efficiency such as mapping vulnerable AI parameter components to well-protected hardware modules. PVF metric is applicable to any AI model and has a potential to help unify and standardize AI vulnerability/resilience evaluation practice.</li>
</ul>

<h3>Title: ALCM: Autonomous LLM-Augmented Causal Discovery Framework</h3>
<ul>
<li><strong>Authors: </strong>Elahe Khatibi, Mahyar Abbasian, Zhongqi Yang, Iman Azimi, Amir M. Rahmani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01744">https://arxiv.org/abs/2405.01744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01744">https://arxiv.org/pdf/2405.01744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01744]] ALCM: Autonomous LLM-Augmented Causal Discovery Framework(https://arxiv.org/abs/2405.01744)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>To perform effective causal inference in high-dimensional datasets, initiating the process with causal discovery is imperative, wherein a causal graph is generated based on observational data. However, obtaining a complete and accurate causal graph poses a formidable challenge, recognized as an NP-hard problem. Recently, the advent of Large Language Models (LLMs) has ushered in a new era, indicating their emergent capabilities and widespread applicability in facilitating causal reasoning across diverse domains, such as medicine, finance, and science. The expansive knowledge base of LLMs holds the potential to elevate the field of causal reasoning by offering interpretability, making inferences, generalizability, and uncovering novel causal structures. In this paper, we introduce a new framework, named Autonomous LLM-Augmented Causal Discovery Framework (ALCM), to synergize data-driven causal discovery algorithms and LLMs, automating the generation of a more resilient, accurate, and explicable causal graph. The ALCM consists of three integral components: causal structure learning, causal wrapper, and LLM-driven causal refiner. These components autonomously collaborate within a dynamic environment to address causal discovery questions and deliver plausible causal graphs. We evaluate the ALCM framework by implementing two demonstrations on seven well-known datasets. Experimental results demonstrate that ALCM outperforms existing LLM methods and conventional data-driven causal reasoning mechanisms. This study not only shows the effectiveness of the ALCM but also underscores new research directions in leveraging the causal reasoning capabilities of LLMs.</li>
</ul>

<h3>Title: CoS: Enhancing Personalization and Mitigating Bias with Context Steering</h3>
<ul>
<li><strong>Authors: </strong>Jerry Zhi-Yang He, Sashrika Pandey, Mariah L. Schrum, Anca Dragan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01768">https://arxiv.org/abs/2405.01768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01768">https://arxiv.org/pdf/2405.01768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01768]] CoS: Enhancing Personalization and Mitigating Bias with Context Steering(https://arxiv.org/abs/2405.01768)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>When querying a large language model (LLM), the context, i.e. personal, demographic, and cultural information specific to an end-user, can significantly shape the response of the LLM. For example, asking the model to explain Newton's second law with the context "I am a toddler" yields a different answer compared to the context "I am a physics professor." Proper usage of the context enables the LLM to generate personalized responses, whereas inappropriate contextual influence can lead to stereotypical and potentially harmful generations (e.g. associating "female" with "housekeeper"). In practice, striking the right balance when leveraging context is a nuanced and challenging problem that is often situation-dependent. One common approach to address this challenge is to fine-tune LLMs on contextually appropriate responses. However, this approach is expensive, time-consuming, and not controllable for end-users in different situations. In this work, we propose Context Steering (CoS) - a simple training-free method that can be easily applied to autoregressive LLMs at inference time. By measuring the contextual influence in terms of token prediction likelihood and modulating it, our method enables practitioners to determine the appropriate level of contextual influence based on their specific use case and end-user base. We showcase a variety of applications of CoS including amplifying the contextual influence to achieve better personalization and mitigating unwanted influence for reducing model bias. In addition, we show that we can combine CoS with Bayesian Inference to quantify the extent of hate speech on the internet. We demonstrate the effectiveness of CoS on state-of-the-art LLMs and benchmarks.</li>
</ul>

<h3>Title: A Survey on Large Language Models for Critical Societal Domains:  Finance, Healthcare, and Law</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Zoey Chen, Jing Ma, Xinlu Zhang, Nan Hao, An Yan, Armineh Nourbakhsh, Xianjun Yang, Julian McAuley, Linda Petzold, William Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01769">https://arxiv.org/abs/2405.01769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01769">https://arxiv.org/pdf/2405.01769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01769]] A Survey on Large Language Models for Critical Societal Domains:  Finance, Healthcare, and Law(https://arxiv.org/abs/2405.01769)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>In the fast-evolving domain of artificial intelligence, large language models (LLMs) such as GPT-3 and GPT-4 are revolutionizing the landscapes of finance, healthcare, and law: domains characterized by their reliance on professional expertise, challenging data acquisition, high-stakes, and stringent regulatory compliance. This survey offers a detailed exploration of the methodologies, applications, challenges, and forward-looking opportunities of LLMs within these high-stakes sectors. We highlight the instrumental role of LLMs in enhancing diagnostic and treatment methodologies in healthcare, innovating financial analytics, and refining legal interpretation and compliance strategies. Moreover, we critically examine the ethics for LLM applications in these fields, pointing out the existing ethical concerns and the need for transparent, fair, and robust AI systems that respect regulatory norms. By presenting a thorough review of current literature and practical applications, we showcase the transformative impact of LLMs, and outline the imperative for interdisciplinary cooperation, methodological advancements, and ethical vigilance. Through this lens, we aim to spark dialogue and inspire future research dedicated to maximizing the benefits of LLMs while mitigating their risks in these precision-dependent sectors. To facilitate future research on LLMs in these critical societal domains, we also initiate a reading list that tracks the latest advancements under this topic, which will be continually updated: \url{https://github.com/czyssrs/LLM_X_papers}.</li>
</ul>

<h3>Title: Understanding Position Bias Effects on Fairness in Social Multi-Document  Summarization</h3>
<ul>
<li><strong>Authors: </strong>Olubusayo Olabisi, Ameeta Agrawal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01790">https://arxiv.org/abs/2405.01790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01790">https://arxiv.org/pdf/2405.01790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01790]] Understanding Position Bias Effects on Fairness in Social Multi-Document  Summarization(https://arxiv.org/abs/2405.01790)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Text summarization models have typically focused on optimizing aspects of quality such as fluency, relevance, and coherence, particularly in the context of news articles. However, summarization models are increasingly being used to summarize diverse sources of text, such as social media data, that encompass a wide demographic user base. It is thus crucial to assess not only the quality of the generated summaries, but also the extent to which they can fairly represent the opinions of diverse social groups. Position bias, a long-known issue in news summarization, has received limited attention in the context of social multi-document summarization. We deeply investigate this phenomenon by analyzing the effect of group ordering in input documents when summarizing tweets from three distinct linguistic communities: African-American English, Hispanic-aligned Language, and White-aligned Language. Our empirical analysis shows that although the textual quality of the summaries remains consistent regardless of the input document order, in terms of fairness, the results vary significantly depending on how the dialect groups are presented in the input data. Our results suggest that position bias manifests differently in social multi-document summarization, severely impacting the fairness of summarization models.</li>
</ul>

<h3>Title: Exploiting ChatGPT for Diagnosing Autism-Associated Language Disorders  and Identifying Distinct Features</h3>
<ul>
<li><strong>Authors: </strong>Chuanbo Hu, Wenqi Li, Mindi Ruan, Xiangxu Yu, Lynn K. Paul, Shuo Wang, Xin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01799">https://arxiv.org/abs/2405.01799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01799">https://arxiv.org/pdf/2405.01799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01799]] Exploiting ChatGPT for Diagnosing Autism-Associated Language Disorders  and Identifying Distinct Features(https://arxiv.org/abs/2405.01799)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Diagnosing language disorders associated with autism is a complex and nuanced challenge, often hindered by the subjective nature and variability of traditional assessment methods. Traditional diagnostic methods not only require intensive human effort but also often result in delayed interventions due to their lack of speed and specificity. In this study, we explored the application of ChatGPT, a state of the art large language model, to overcome these obstacles by enhancing diagnostic accuracy and profiling specific linguistic features indicative of autism. Leveraging ChatGPT advanced natural language processing capabilities, this research aims to streamline and refine the diagnostic process. Specifically, we compared ChatGPT's performance with that of conventional supervised learning models, including BERT, a model acclaimed for its effectiveness in various natural language processing tasks. We showed that ChatGPT substantially outperformed these models, achieving over 13% improvement in both accuracy and F1 score in a zero shot learning configuration. This marked enhancement highlights the model potential as a superior tool for neurological diagnostics. Additionally, we identified ten distinct features of autism associated language disorders that vary significantly across different experimental scenarios. These features, which included echolalia, pronoun reversal, and atypical language usage, were crucial for accurately diagnosing ASD and customizing treatment plans. Together, our findings advocate for adopting sophisticated AI tools like ChatGPT in clinical settings to assess and diagnose developmental disorders. Our approach not only promises greater diagnostic precision but also aligns with the goals of personalized medicine, potentially transforming the evaluation landscape for autism and similar neurological conditions.</li>
</ul>

<h3>Title: Efficient and Economic Large Language Model Inference with Attention  Offloading</h3>
<ul>
<li><strong>Authors: </strong>Shaoyuan Chen, Yutong Lin, Mingxing Zhang, Yongwei Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01814">https://arxiv.org/abs/2405.01814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01814">https://arxiv.org/pdf/2405.01814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01814]] Efficient and Economic Large Language Model Inference with Attention  Offloading(https://arxiv.org/abs/2405.01814)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based large language models (LLMs) exhibit impressive performance in generative tasks but introduce significant challenges in real-world serving due to inefficient use of the expensive, computation-optimized accelerators. This mismatch arises from the autoregressive nature of LLMs, where the generation phase comprises operators with varying resource demands. Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators, especially as context length increases. To enhance the efficiency and cost-effectiveness of LLM serving, we introduce the concept of attention offloading. This approach leverages a collection of cheap, memory-optimized devices for the attention operator while still utilizing high-end accelerators for other parts of the model. This heterogeneous setup ensures that each component is tailored to its specific workload, maximizing overall performance and cost efficiency. Our comprehensive analysis and experiments confirm the viability of splitting the attention computation over multiple devices. Also, the communication bandwidth required between heterogeneous devices proves to be manageable with prevalent networking technologies. To further validate our theory, we develop Lamina, an LLM inference system that incorporates attention offloading. Experimental results indicate that Lamina can provide 1.48x-12.1x higher estimated throughput per dollar than homogeneous solutions.</li>
</ul>

<h3>Title: Uniformly Stable Algorithms for Adversarial Training and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Jiancong Xiao, Jiawei Zhang, Zhi-Quan Luo, Asuman Ozdaglar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01817">https://arxiv.org/abs/2405.01817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01817">https://arxiv.org/pdf/2405.01817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01817]] Uniformly Stable Algorithms for Adversarial Training and Beyond(https://arxiv.org/abs/2405.01817)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In adversarial machine learning, neural networks suffer from a significant issue known as robust overfitting, where the robust test accuracy decreases over epochs (Rice et al., 2020). Recent research conducted by Xing et al.,2021; Xiao et al., 2022 has focused on studying the uniform stability of adversarial training. Their investigations revealed that SGD-based adversarial training fails to exhibit uniform stability, and the derived stability bounds align with the observed phenomenon of robust overfitting in experiments. This motivates us to develop uniformly stable algorithms specifically tailored for adversarial training. To this aim, we introduce Moreau envelope-$\mathcal{A}$, a variant of the Moreau Envelope-type algorithm. We employ a Moreau envelope function to reframe the original problem as a min-min problem, separating the non-strong convexity and non-smoothness of the adversarial loss. Then, this approach alternates between solving the inner and outer minimization problems to achieve uniform stability without incurring additional computational overhead. In practical scenarios, we show the efficacy of ME-$\mathcal{A}$ in mitigating the issue of robust overfitting. Beyond its application in adversarial training, this represents a fundamental result in uniform stability analysis, as ME-$\mathcal{A}$ is the first algorithm to exhibit uniform stability for weakly-convex, non-smooth problems.</li>
</ul>

<h3>Title: Sequencer Level Security</h3>
<ul>
<li><strong>Authors: </strong>Martin Derka, Jan Gorzny, Diego Siqueira, Donato Pellegrino, Marius Guggenmos, Zhiyang Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01819">https://arxiv.org/abs/2405.01819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01819">https://arxiv.org/pdf/2405.01819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01819]] Sequencer Level Security(https://arxiv.org/abs/2405.01819)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Current blockchains do not provide any security guarantees to the smart contracts and their users as far as the content of the transactions is concerned. In the spirit of decentralization and censorship resistance, they follow the paradigm of including valid transactions in blocks without any further scrutiny. Rollups are a special kind of blockchains whose primary purpose is to scale the transaction throughput. Many of the existing rollups operate through a centrally operated sequencing protocol. In this paper, we introduce the Sequencer Level Security (SLS) protocol, an enhancement to sequencing protocols of rollups. This pioneering contribution explores the concept of the sequencer's capability to identify and temporarily quarantine malicious transactions instead of including them in blocks immediately. We describe the mechanics of the protocol for both the transactions submitted to the rollup mempool, as well as transactions originating from Layer one. We comment on topics such as trust and decentralization, and consider the security impact on the protocol itself. We implement a prototype of the SLS protocol, Zircuit, which is built on top of Geth and the OP stack. The SLS protocol described can be easily generalized to other rollup designs, and can be used for purposes other than security.</li>
</ul>

<h3>Title: Improving Concept Alignment in Vision-Language Concept Bottleneck Models</h3>
<ul>
<li><strong>Authors: </strong>Nithish Muthuchamy Selvaraj, Xiaobao Guo, Bingquan Shen, Adams Wai-Kin Kong, Alex Kot</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01825">https://arxiv.org/abs/2405.01825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01825">https://arxiv.org/pdf/2405.01825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01825]] Improving Concept Alignment in Vision-Language Concept Bottleneck Models(https://arxiv.org/abs/2405.01825)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Concept Bottleneck Models (CBM) map the input image to a high-level human-understandable concept space and then make class predictions based on these concepts. Recent approaches automate the construction of CBM by prompting Large Language Models (LLM) to generate text concepts and then use Vision Language Models (VLM) to obtain concept scores to train a CBM. However, it is desired to build CBMs with concepts defined by human experts instead of LLM generated concepts to make them more trustworthy. In this work, we take a closer inspection on the faithfulness of VLM concept scores for such expert-defined concepts in domains like fine-grain bird species classification and animal classification. Our investigations reveal that frozen VLMs, like CLIP, struggle to correctly associate a concept to the corresponding visual input despite achieving a high classification performance. To address this, we propose a novel Contrastive Semi-Supervised (CSS) learning method which uses a few labeled concept examples to improve concept alignment (activate truthful visual concepts) in CLIP model. Extensive experiments on three benchmark datasets show that our approach substantially increases the concept accuracy and classification accuracy, yet requires only a fraction of the human-annotated concept labels. To further improve the classification performance, we also introduce a new class-level intervention procedure for fine-grain classification problems that identifies the confounding classes and intervenes their concept space to reduce errors.</li>
</ul>

<h3>Title: FER-YOLO-Mamba: Facial Expression Detection and Classification Based on  Selective State Space</h3>
<ul>
<li><strong>Authors: </strong>Hui Ma, Sen Lei, Turgay Celik, Heng-Chao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01828">https://arxiv.org/abs/2405.01828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01828">https://arxiv.org/pdf/2405.01828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01828]] FER-YOLO-Mamba: Facial Expression Detection and Classification Based on  Selective State Space(https://arxiv.org/abs/2405.01828)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Facial Expression Recognition (FER) plays a pivotal role in understanding human emotional cues. However, traditional FER methods based on visual information have some limitations, such as preprocessing, feature extraction, and multi-stage classification procedures. These not only increase computational complexity but also require a significant amount of computing resources. Considering Convolutional Neural Network (CNN)-based FER schemes frequently prove inadequate in identifying the deep, long-distance dependencies embedded within facial expression images, and the Transformer's inherent quadratic computational complexity, this paper presents the FER-YOLO-Mamba model, which integrates the principles of Mamba and YOLO technologies to facilitate efficient coordination in facial expression image recognition and localization. Within the FER-YOLO-Mamba model, we further devise a FER-YOLO-VSS dual-branch module, which combines the inherent strengths of convolutional layers in local feature extraction with the exceptional capability of State Space Models (SSMs) in revealing long-distance dependencies. To the best of our knowledge, this is the first Vision Mamba model designed for facial expression detection and classification. To evaluate the performance of the proposed FER-YOLO-Mamba model, we conducted experiments on two benchmark datasets, RAF-DB and SFEW. The experimental results indicate that the FER-YOLO-Mamba model achieved better results compared to other models. The code is available from https://github.com/SwjtuMa/FER-YOLO-Mamba.</li>
</ul>

<h3>Title: A Novel Approach to Guard from Adversarial Attacks using Stable  Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Trinath Sai Subhash Reddy Pittala, Uma Maheswara Rao Meleti, Geethakrishna Puligundla</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01838">https://arxiv.org/abs/2405.01838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01838">https://arxiv.org/pdf/2405.01838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01838]] A Novel Approach to Guard from Adversarial Attacks using Stable  Diffusion(https://arxiv.org/abs/2405.01838)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Recent developments in adversarial machine learning have highlighted the importance of building robust AI systems to protect against increasingly sophisticated attacks. While frameworks like AI Guardian are designed to defend against these threats, they often rely on assumptions that can limit their effectiveness. For example, they may assume attacks only come from one direction or include adversarial images in their training data. Our proposal suggests a different approach to the AI Guardian framework. Instead of including adversarial examples in the training process, we propose training the AI system without them. This aims to create a system that is inherently resilient to a wider range of attacks. Our method focuses on a dynamic defense strategy using stable diffusion that learns continuously and models threats comprehensively. We believe this approach can lead to a more generalized and robust defense against adversarial attacks. In this paper, we outline our proposed approach, including the theoretical basis, experimental design, and expected impact on improving AI security against adversarial threats.</li>
</ul>

<h3>Title: SGHateCheck: Functional Tests for Detecting Hate Speech in Low-Resource  Languages of Singapore</h3>
<ul>
<li><strong>Authors: </strong>Ri Chi Ng, Nirmalendu Prakash, Ming Shan Hee, Kenny Tsu Wei Choo, Roy Ka-Wei Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01842">https://arxiv.org/abs/2405.01842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01842">https://arxiv.org/pdf/2405.01842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01842]] SGHateCheck: Functional Tests for Detecting Hate Speech in Low-Resource  Languages of Singapore(https://arxiv.org/abs/2405.01842)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To address the limitations of current hate speech detection models, we introduce \textsf{SGHateCheck}, a novel framework designed for the linguistic and cultural context of Singapore and Southeast Asia. It extends the functional testing approach of HateCheck and MHC, employing large language models for translation and paraphrasing into Singapore's main languages, and refining these with native annotators. \textsf{SGHateCheck} reveals critical flaws in state-of-the-art models, highlighting their inadequacy in sensitive content moderation. This work aims to foster the development of more effective hate speech detection tools for diverse linguistic environments, particularly for Singapore and Southeast Asia contexts.</li>
</ul>

<h3>Title: SUKHSANDESH: An Avatar Therapeutic Question Answering Platform for  Sexual Education in Rural India</h3>
<ul>
<li><strong>Authors: </strong>Salam Michael Singh, Shubhmoy Kumar Garg, Amitesh Misra, Aaditeshwar Seth, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01858">https://arxiv.org/abs/2405.01858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01858">https://arxiv.org/pdf/2405.01858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01858]] SUKHSANDESH: An Avatar Therapeutic Question Answering Platform for  Sexual Education in Rural India(https://arxiv.org/abs/2405.01858)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sexual education aims to foster a healthy lifestyle in terms of emotional, mental and social well-being. In countries like India, where adolescents form the largest demographic group, they face significant vulnerabilities concerning sexual health. Unfortunately, sexual education is often stigmatized, creating barriers to providing essential counseling and information to this at-risk population. Consequently, issues such as early pregnancy, unsafe abortions, sexually transmitted infections, and sexual violence become prevalent. Our current proposal aims to provide a safe and trustworthy platform for sexual education to the vulnerable rural Indian population, thereby fostering the healthy and overall growth of the nation. In this regard, we strive towards designing SUKHSANDESH, a multi-staged AI-based Question Answering platform for sexual education tailored to rural India, adhering to safety guardrails and regional language support. By utilizing information retrieval techniques and large language models, SUKHSANDESH will deliver effective responses to user queries. We also propose to anonymise the dataset to mitigate safety measures and set AI guardrails against any harmful or unwanted response generation. Moreover, an innovative feature of our proposal involves integrating ``avatar therapy'' with SUKHSANDESH. This feature will convert AI-generated responses into real-time audio delivered by an animated avatar speaking regional Indian languages. This approach aims to foster empathy and connection, which is particularly beneficial for individuals with limited literacy skills. Partnering with Gram Vaani, an industry leader, we will deploy SUKHSANDESH to address sexual education needs in rural India.</li>
</ul>

<h3>Title: Cyber Security in Energy Informatics: A Non-technical Perspective</h3>
<ul>
<li><strong>Authors: </strong>Duong Dang, Tero Vartiainen, Mike Mekkanen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01867">https://arxiv.org/abs/2405.01867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01867">https://arxiv.org/pdf/2405.01867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01867]] Cyber Security in Energy Informatics: A Non-technical Perspective(https://arxiv.org/abs/2405.01867)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Literature in cyber security including cyber security in energy informatics are tecnocentric focuses that may miss the chances of understanding a bigger picture of cyber security measures. This research thus aims to conduct a literature review focusing on non-technical issues in cyber security in the energy informatics field. The findings show that there are seven non-technical issues have been discussed in literature, including education, awareness, policy, standards, human, and risks, challenges, and solutions. These findings can be valuable for not only researchers, but also managers, policy makers, and educators.</li>
</ul>

<h3>Title: Incorporating External Knowledge and Goal Guidance for LLM-based  Conversational Recommender Systems</h3>
<ul>
<li><strong>Authors: </strong>Chuang Li, Yang Deng, Hengchang Hu, Min-Yen Kan, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01868">https://arxiv.org/abs/2405.01868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01868">https://arxiv.org/pdf/2405.01868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01868]] Incorporating External Knowledge and Goal Guidance for LLM-based  Conversational Recommender Systems(https://arxiv.org/abs/2405.01868)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper aims to efficiently enable large language models (LLMs) to use external knowledge and goal guidance in conversational recommender system (CRS) tasks. Advanced LLMs (e.g., ChatGPT) are limited in domain-specific CRS tasks for 1) generating grounded responses with recommendation-oriented knowledge, or 2) proactively leading the conversations through different dialogue goals. In this work, we first analyze those limitations through a comprehensive evaluation, showing the necessity of external knowledge and goal guidance which contribute significantly to the recommendation accuracy and language quality. In light of this finding, we propose a novel ChatCRS framework to decompose the complex CRS task into several sub-tasks through the implementation of 1) a knowledge retrieval agent using a tool-augmented approach to reason over external Knowledge Bases and 2) a goal-planning agent for dialogue goal prediction. Experimental results on two multi-goal CRS datasets reveal that ChatCRS sets new state-of-the-art benchmarks, improving language quality of informativeness by 17% and proactivity by 27%, and achieving a tenfold enhancement in recommendation accuracy.</li>
</ul>

<h3>Title: Defect Image Sample Generation With Diffusion Prior for Steel Surface  Defect Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yichun Tai, Kun Yang, Tao Peng, Zhenzhen Huang, Zhijiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01872">https://arxiv.org/abs/2405.01872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01872">https://arxiv.org/pdf/2405.01872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01872]] Defect Image Sample Generation With Diffusion Prior for Steel Surface  Defect Recognition(https://arxiv.org/abs/2405.01872)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The task of steel surface defect recognition is an industrial problem with great industry values. The data insufficiency is the major challenge in training a robust defect recognition network. Existing methods have investigated to enlarge the dataset by generating samples with generative models. However, their generation quality is still limited by the insufficiency of defect image samples. To this end, we propose Stable Surface Defect Generation (StableSDG), which transfers the vast generation distribution embedded in Stable Diffusion model for steel surface defect image generation. To tackle with the distinctive distribution gap between steel surface images and generated images of the diffusion model, we propose two processes. First, we align the distribution by adapting parameters of the diffusion model, adopted both in the token embedding space and network parameter space. Besides, in the generation process, we propose image-oriented generation rather than from pure Gaussian noises. We conduct extensive experiments on steel surface defect dataset, demonstrating state-of-the-art performance on generating high-quality samples and training recognition models, and both designed processes are significant for the performance.</li>
</ul>

<h3>Title: DALLMi: Domain Adaption for LLM-based Multi-label Classifier</h3>
<ul>
<li><strong>Authors: </strong>Miruna Beţianu, Abele Mălan, Marco Aldinucci, Robert Birke, Lydia Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01883">https://arxiv.org/abs/2405.01883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01883">https://arxiv.org/pdf/2405.01883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01883]] DALLMi: Domain Adaption for LLM-based Multi-label Classifier(https://arxiv.org/abs/2405.01883)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) increasingly serve as the backbone for classifying text associated with distinct domains and simultaneously several labels (classes). When encountering domain shifts, e.g., classifier of movie reviews from IMDb to Rotten Tomatoes, adapting such an LLM-based multi-label classifier is challenging due to incomplete label sets at the target domain and daunting training overhead. The existing domain adaptation methods address either image multi-label classifiers or text binary classifiers. In this paper, we design DALLMi, Domain Adaptation Large Language Model interpolator, a first-of-its-kind semi-supervised domain adaptation method for text data models based on LLMs, specifically BERT. The core of DALLMi is the novel variation loss and MixUp regularization, which jointly leverage the limited positively labeled and large quantity of unlabeled text and, importantly, their interpolation from the BERT word embeddings. DALLMi also introduces a label-balanced sampling strategy to overcome the imbalance between labeled and unlabeled data. We evaluate DALLMi against the partial-supervised and unsupervised approach on three datasets under different scenarios of label availability for the target domain. Our results show that DALLMi achieves higher mAP than unsupervised and partially-supervised approaches by 19.9% and 52.2%, respectively.</li>
</ul>

<h3>Title: Beyond Single-Event Extraction: Towards Efficient Document-Level  Multi-Event Argument Extraction</h3>
<ul>
<li><strong>Authors: </strong>Wanlong Liu, Li Zhou, Dingyi Zeng, Yichen Xiao, Shaohuan Cheng, Chen Zhang, Grandee Lee, Malu Zhang, Wenyu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01884">https://arxiv.org/abs/2405.01884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01884">https://arxiv.org/pdf/2405.01884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01884]] Beyond Single-Event Extraction: Towards Efficient Document-Level  Multi-Event Argument Extraction(https://arxiv.org/abs/2405.01884)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Recent mainstream event argument extraction methods process each event in isolation, resulting in inefficient inference and ignoring the correlations among multiple events. To address these limitations, here we propose a multiple-event argument extraction model DEEIA (Dependency-guided Encoding and Event-specific Information Aggregation), capable of extracting arguments from all events within a document simultaneouslyThe proposed DEEIA model employs a multi-event prompt mechanism, comprising DE and EIA modules. The DE module is designed to improve the correlation between prompts and their corresponding event contexts, whereas the EIA module provides event-specific information to improve contextual understanding. Extensive experiments show that our method achieves new state-of-the-art performance on four public datasets (RAMS, WikiEvents, MLEE, and ACE05), while significantly saving the inference time compared to the baselines. Further analyses demonstrate the effectiveness of the proposed modules.</li>
</ul>

<h3>Title: Aloe: A Family of Fine-tuned Open Healthcare LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Jordi Bayarri-Planas, Adrian Tormos, Daniel Hinjos, Pablo Bernabeu-Perez, Anna Arias-Duart, Pablo Agustin Martin-Torres, Lucia Urcelay-Ganzabal, Marta Gonzalez-Mallo, Sergio Alvarez-Napagao, Eduard Ayguadé-Parra, Ulises Cortés Dario Garcia-Gasulla</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01886">https://arxiv.org/abs/2405.01886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01886">https://arxiv.org/pdf/2405.01886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01886]] Aloe: A Family of Fine-tuned Open Healthcare LLMs(https://arxiv.org/abs/2405.01886)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As the capabilities of Large Language Models (LLMs) in healthcare and medicine continue to advance, there is a growing need for competitive open-source models that can safeguard public interest. With the increasing availability of highly competitive open base models, the impact of continued pre-training is increasingly uncertain. In this work, we explore the role of instruct tuning, model merging, alignment, red teaming and advanced inference schemes, as means to improve current open models. To that end, we introduce the Aloe family, a set of open medical LLMs highly competitive within its scale range. Aloe models are trained on the current best base models (Mistral, LLaMA 3), using a new custom dataset which combines public data sources improved with synthetic Chain of Thought (CoT). Aloe models undergo an alignment phase, becoming one of the first few policy-aligned open healthcare LLM using Direct Preference Optimization, setting a new standard for ethical performance in healthcare LLMs. Model evaluation expands to include various bias and toxicity datasets, a dedicated red teaming effort, and a much-needed risk assessment for healthcare LLMs. Finally, to explore the limits of current LLMs in inference, we study several advanced prompt engineering strategies to boost performance across benchmarks, yielding state-of-the-art results for open healthcare 7B LLMs, unprecedented at this scale.</li>
</ul>

<h3>Title: Securing the Open RAN Infrastructure: Exploring Vulnerabilities in  Kubernetes Deployments</h3>
<ul>
<li><strong>Authors: </strong>Felix Klement, Alessandro Brighente, Michele Polese, Mauro Conti, Stefan Katzenbeisser</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01888">https://arxiv.org/abs/2405.01888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01888">https://arxiv.org/pdf/2405.01888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01888]] Securing the Open RAN Infrastructure: Exploring Vulnerabilities in  Kubernetes Deployments(https://arxiv.org/abs/2405.01888)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate the security implications of virtualized and software-based Open Radio Access Network (RAN) systems, specifically focusing on the architecture proposed by the O-RAN ALLIANCE and O-Cloud deployments based on the O-RAN Software Community (OSC) stack and infrastructure. Our key findings are based on a thorough security assessment and static scanning of the OSC Near Real-Time RAN Intelligent Controller (RIC) cluster. We highlight the presence of potential vulnerabilities and misconfigurations in the Kubernetes infrastructure supporting the RIC, also due to the usage of outdated versions of software packages, and provide an estimation of their criticality using various deployment auditing frameworks (e.g., MITRE ATT&CK and the NSA CISA). In addition, we propose methodologies to minimize these issues and harden the Open RAN virtualization infrastructure. These encompass the integration of security evaluation methods into the deployment process, implementing deployment hardening measures, and employing policy-based control for RAN components. We emphasize the need to address the problems found in order to improve the overall security of virtualized Open RAN systems.</li>
</ul>

<h3>Title: OARelatedWork: A Large-Scale Dataset of Related Work Sections with  Full-texts from Open Access Sources</h3>
<ul>
<li><strong>Authors: </strong>Martin Docekal, Martin Fajcik, Pavel Smrz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01930">https://arxiv.org/abs/2405.01930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01930">https://arxiv.org/pdf/2405.01930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01930]] OARelatedWork: A Large-Scale Dataset of Related Work Sections with  Full-texts from Open Access Sources(https://arxiv.org/abs/2405.01930)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper introduces OARelatedWork, the first large-scale multi-document summarization dataset for related work generation containing whole related work sections and full-texts of cited papers. The dataset includes 94 450 papers and 5 824 689 unique referenced papers. It was designed for the task of automatically generating related work to shift the field toward generating entire related work sections from all available content instead of generating parts of related work sections from abstracts only, which is the current mainstream in this field for abstractive approaches. We show that the estimated upper bound for extractive summarization increases by 217% in the ROUGE-2 score, when using full content instead of abstracts. Furthermore, we show the benefits of full content data on naive, oracle, traditional, and transformer-based baselines. Long outputs, such as related work sections, pose challenges for automatic evaluation metrics like BERTScore due to their limited input length. We tackle this issue by proposing and evaluating a meta-metric using BERTScore. Despite operating on smaller blocks, we show this meta-metric correlates with human judgment, comparably to the original BERTScore.</li>
</ul>

<h3>Title: Impact of Architectural Modifications on Deep Learning Adversarial  Robustness</h3>
<ul>
<li><strong>Authors: </strong>Firuz Juraev, Mohammed Abuhamad, Simon S. Woo, George K Thiruvathukal, Tamer Abuhmed</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01934">https://arxiv.org/abs/2405.01934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01934">https://arxiv.org/pdf/2405.01934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01934]] Impact of Architectural Modifications on Deep Learning Adversarial  Robustness(https://arxiv.org/abs/2405.01934)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Rapid advancements of deep learning are accelerating adoption in a wide variety of applications, including safety-critical applications such as self-driving vehicles, drones, robots, and surveillance systems. These advancements include applying variations of sophisticated techniques that improve the performance of models. However, such models are not immune to adversarial manipulations, which can cause the system to misbehave and remain unnoticed by experts. The frequency of modifications to existing deep learning models necessitates thorough analysis to determine the impact on models' robustness. In this work, we present an experimental evaluation of the effects of model modifications on deep learning model robustness using adversarial attacks. Our methodology involves examining the robustness of variations of models against various adversarial attacks. By conducting our experiments, we aim to shed light on the critical issue of maintaining the reliability and safety of deep learning models in safety- and security-critical applications. Our results indicate the pressing demand for an in-depth assessment of the effects of model changes on the robustness of models.</li>
</ul>

<h3>Title: An Attention Based Pipeline for Identifying Pre-Cancer Lesions in Head  and Neck Clinical Images</h3>
<ul>
<li><strong>Authors: </strong>Abdullah Alsalemi, Anza Shakeel, Mollie Clark, Syed Ali Khurram, Shan E Ahmed Raza</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01937">https://arxiv.org/abs/2405.01937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01937">https://arxiv.org/pdf/2405.01937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01937]] An Attention Based Pipeline for Identifying Pre-Cancer Lesions in Head  and Neck Clinical Images(https://arxiv.org/abs/2405.01937)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Early detection of cancer can help improve patient prognosis by early intervention. Head and neck cancer is diagnosed in specialist centres after a surgical biopsy, however, there is a potential for these to be missed leading to delayed diagnosis. To overcome these challenges, we present an attention based pipeline that identifies suspected lesions, segments, and classifies them as non-dysplastic, dysplastic and cancerous lesions. We propose (a) a vision transformer based Mask R-CNN network for lesion detection and segmentation of clinical images, and (b) Multiple Instance Learning (MIL) based scheme for classification. Current results show that the segmentation model produces segmentation masks and bounding boxes with up to 82% overlap accuracy score on unseen external test data and surpassing reviewed segmentation benchmarks. Next, a classification F1-score of 85% on the internal cohort test set. An app has been developed to perform lesion segmentation taken via a smart device. Future work involves employing endoscopic video data for precise early detection and prognosis.</li>
</ul>

<h3>Title: Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Guo, Hidetaka Kamigaito, Taro Wanatnabe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01943">https://arxiv.org/abs/2405.01943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01943">https://arxiv.org/pdf/2405.01943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01943]] Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large  Language Models(https://arxiv.org/abs/2405.01943)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement in Large Language Models (LLMs) has markedly enhanced the capabilities of language understanding and generation. However, the substantial model size poses hardware challenges, affecting both memory size for serving and inference latency for token generation. To address those challenges, we propose Dependency-aware Semi-structured Sparsity (DaSS), a novel method for the recent prevalent SwiGLU-based LLMs pruning. Our approach incorporates structural dependency into the weight magnitude-based unstructured pruning. We introduce an MLP-specific pruning metric that evaluates the importance of each weight by jointly considering its magnitude and its corresponding MLP intermediate activation norms. DaSS facilitates a balance between the adaptability offered by unstructured pruning and the structural consistency inherent in dependency-based structured pruning. Empirical evaluations on Mistral and LLaMA2 model families demonstrate that DaSS not only outperforms both SparseGPT and Wanda in achieving hardware-friendly N:M sparsity patterns but also maintains the computational efficiency of Wanda.</li>
</ul>

<h3>Title: From Attack to Defense: Insights into Deep Learning Security Measures in  Black-Box Settings</h3>
<ul>
<li><strong>Authors: </strong>Firuz Juraev, Mohammed Abuhamad, Eric Chan-Tin, George K. Thiruvathukal, Tamer Abuhmed</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01963">https://arxiv.org/abs/2405.01963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01963">https://arxiv.org/pdf/2405.01963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01963]] From Attack to Defense: Insights into Deep Learning Security Measures in  Black-Box Settings(https://arxiv.org/abs/2405.01963)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep Learning (DL) is rapidly maturing to the point that it can be used in safety- and security-crucial applications. However, adversarial samples, which are undetectable to the human eye, pose a serious threat that can cause the model to misbehave and compromise the performance of such applications. Addressing the robustness of DL models has become crucial to understanding and defending against adversarial attacks. In this study, we perform comprehensive experiments to examine the effect of adversarial attacks and defenses on various model architectures across well-known datasets. Our research focuses on black-box attacks such as SimBA, HopSkipJump, MGAAttack, and boundary attacks, as well as preprocessor-based defensive mechanisms, including bits squeezing, median smoothing, and JPEG filter. Experimenting with various models, our results demonstrate that the level of noise needed for the attack increases as the number of layers increases. Moreover, the attack success rate decreases as the number of layers increases. This indicates that model complexity and robustness have a significant relationship. Investigating the diversity and robustness relationship, our experiments with diverse models show that having a large number of parameters does not imply higher robustness. Our experiments extend to show the effects of the training dataset on model robustness. Using various datasets such as ImageNet-1000, CIFAR-100, and CIFAR-10 are used to evaluate the black-box attacks. Considering the multiple dimensions of our analysis, e.g., model complexity and training dataset, we examined the behavior of black-box attacks when models apply defenses. Our results show that applying defense strategies can significantly reduce attack effectiveness. This research provides in-depth analysis and insight into the robustness of DL models against various attacks, and defenses.</li>
</ul>

<h3>Title: Conformal Prediction for Natural Language Processing: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Margarida M. Campos, António Farinhas, Chrysoula Zerva, Mário A.T. Figueiredo, André F.T. Martins</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01976">https://arxiv.org/abs/2405.01976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01976">https://arxiv.org/pdf/2405.01976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01976]] Conformal Prediction for Natural Language Processing: A Survey(https://arxiv.org/abs/2405.01976)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid proliferation of large language models and natural language processing (NLP) applications creates a crucial need for uncertainty quantification to mitigate risks such as hallucinations and to enhance decision-making reliability in critical applications. Conformal prediction is emerging as a theoretically sound and practically useful framework, combining flexibility with strong statistical guarantees. Its model-agnostic and distribution-free nature makes it particularly promising to address the current shortcomings of NLP systems that stem from the absence of uncertainty quantification. This paper provides a comprehensive survey of conformal prediction techniques, their guarantees, and existing applications in NLP, pointing to directions for future research and open challenges.</li>
</ul>

<h3>Title: Quantifying Distribution Shifts and Uncertainties for Enhanced Model  Robustness in Machine Learning Applications</h3>
<ul>
<li><strong>Authors: </strong>Vegard Flovik</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01978">https://arxiv.org/abs/2405.01978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01978">https://arxiv.org/pdf/2405.01978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01978]] Quantifying Distribution Shifts and Uncertainties for Enhanced Model  Robustness in Machine Learning Applications(https://arxiv.org/abs/2405.01978)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Distribution shifts, where statistical properties differ between training and test datasets, present a significant challenge in real-world machine learning applications where they directly impact model generalization and robustness. In this study, we explore model adaptation and generalization by utilizing synthetic data to systematically address distributional disparities. Our investigation aims to identify the prerequisites for successful model adaptation across diverse data distributions, while quantifying the associated uncertainties. Specifically, we generate synthetic data using the Van der Waals equation for gases and employ quantitative measures such as Kullback-Leibler divergence, Jensen-Shannon distance, and Mahalanobis distance to assess data similarity. These metrics en able us to evaluate both model accuracy and quantify the associated uncertainty in predictions arising from data distribution shifts. Our findings suggest that utilizing statistical measures, such as the Mahalanobis distance, to determine whether model predictions fall within the low-error "interpolation regime" or the high-error "extrapolation regime" provides a complementary method for assessing distribution shift and model uncertainty. These insights hold significant value for enhancing model robustness and generalization, essential for the successful deployment of machine learning applications in real-world scenarios.</li>
</ul>

<h3>Title: SFFNet: A Wavelet-Based Spatial and Frequency Domain Fusion Network for  Remote Sensing Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yunsong Yang, Genji Yuan, Jinjiang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01992">https://arxiv.org/abs/2405.01992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01992">https://arxiv.org/pdf/2405.01992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01992]] SFFNet: A Wavelet-Based Spatial and Frequency Domain Fusion Network for  Remote Sensing Segmentation(https://arxiv.org/abs/2405.01992)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In order to fully utilize spatial information for segmentation and address the challenge of handling areas with significant grayscale variations in remote sensing segmentation, we propose the SFFNet (Spatial and Frequency Domain Fusion Network) framework. This framework employs a two-stage network design: the first stage extracts features using spatial methods to obtain features with sufficient spatial details and semantic information; the second stage maps these features in both spatial and frequency domains. In the frequency domain mapping, we introduce the Wavelet Transform Feature Decomposer (WTFD) structure, which decomposes features into low-frequency and high-frequency components using the Haar wavelet transform and integrates them with spatial features. To bridge the semantic gap between frequency and spatial features, and facilitate significant feature selection to promote the combination of features from different representation domains, we design the Multiscale Dual-Representation Alignment Filter (MDAF). This structure utilizes multiscale convolutions and dual-cross attentions. Comprehensive experimental results demonstrate that, compared to existing methods, SFFNet achieves superior performance in terms of mIoU, reaching 84.80% and 87.73% respectively.The code is located at https://github.com/yysdck/SFFNet.</li>
</ul>

<h3>Title: Cooperation and Federation in Distributed Radar Point Cloud Processing</h3>
<ul>
<li><strong>Authors: </strong>S. Savazzi, V. Rampa, S. Kianoush, A. Minora, L. Costa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01995">https://arxiv.org/abs/2405.01995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01995">https://arxiv.org/pdf/2405.01995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01995]] Cooperation and Federation in Distributed Radar Point Cloud Processing(https://arxiv.org/abs/2405.01995)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>The paper considers the problem of human-scale RF sensing utilizing a network of resource-constrained MIMO radars with low range-azimuth resolution. The radars operate in the mmWave band and obtain time-varying 3D point cloud (PC) information that is sensitive to body movements. They also observe the same scene from different views and cooperate while sensing the environment using a sidelink communication channel. Conventional cooperation setups allow the radars to mutually exchange raw PC information to improve ego sensing. The paper proposes a federation mechanism where the radars exchange the parameters of a Bayesian posterior measure of the observed PCs, rather than raw data. The radars act as distributed parameter servers to reconstruct a global posterior (i.e., federated posterior) using Bayesian tools. The paper quantifies and compares the benefits of radar federation with respect to cooperation mechanisms. Both approaches are validated by experiments with a real-time demonstration platform. Federation makes minimal use of the sidelink communication channel (20 {\div} 25 times lower bandwidth use) and is less sensitive to unresolved targets. On the other hand, cooperation reduces the mean absolute target estimation error of about 20%.</li>
</ul>

<h3>Title: Exploring Combinatorial Problem Solving with Large Language Models: A  Case Study on the Travelling Salesman Problem Using GPT-3.5 Turbo</h3>
<ul>
<li><strong>Authors: </strong>Mahmoud Masoud, Ahmed Abdelhay, Mohammed Elhenawy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01997">https://arxiv.org/abs/2405.01997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01997">https://arxiv.org/pdf/2405.01997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01997]] Exploring Combinatorial Problem Solving with Large Language Models: A  Case Study on the Travelling Salesman Problem Using GPT-3.5 Turbo(https://arxiv.org/abs/2405.01997)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are deep learning models designed to generate text based on textual input. Although researchers have been developing these models for more complex tasks such as code generation and general reasoning, few efforts have explored how LLMs can be applied to combinatorial problems. In this research, we investigate the potential of LLMs to solve the Travelling Salesman Problem (TSP). Utilizing GPT-3.5 Turbo, we conducted experiments employing various approaches, including zero-shot in-context learning, few-shot in-context learning, and chain-of-thoughts (CoT). Consequently, we fine-tuned GPT-3.5 Turbo to solve a specific problem size and tested it using a set of various instance sizes. The fine-tuned models demonstrated promising performance on problems identical in size to the training instances and generalized well to larger problems. Furthermore, to improve the performance of the fine-tuned model without incurring additional training costs, we adopted a self-ensemble approach to improve the quality of the solutions.</li>
</ul>

<h3>Title: HoloGS: Instant Depth-based 3D Gaussian Splatting with Microsoft  HoloLens 2</h3>
<ul>
<li><strong>Authors: </strong>Miriam Jäger, Theodor Kapler, Michael Feßenbecker, Felix Birkelbach, Markus Hillemann, Boris Jutzi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02005">https://arxiv.org/abs/2405.02005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02005">https://arxiv.org/pdf/2405.02005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02005]] HoloGS: Instant Depth-based 3D Gaussian Splatting with Microsoft  HoloLens 2(https://arxiv.org/abs/2405.02005)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In the fields of photogrammetry, computer vision and computer graphics, the task of neural 3D scene reconstruction has led to the exploration of various techniques. Among these, 3D Gaussian Splatting stands out for its explicit representation of scenes using 3D Gaussians, making it appealing for tasks like 3D point cloud extraction and surface reconstruction. Motivated by its potential, we address the domain of 3D scene reconstruction, aiming to leverage the capabilities of the Microsoft HoloLens 2 for instant 3D Gaussian Splatting. We present HoloGS, a novel workflow utilizing HoloLens sensor data, which bypasses the need for pre-processing steps like Structure from Motion by instantly accessing the required input data i.e. the images, camera poses and the point cloud from depth sensing. We provide comprehensive investigations, including the training process and the rendering quality, assessed through the Peak Signal-to-Noise Ratio, and the geometric 3D accuracy of the densified point cloud from Gaussian centers, measured by Chamfer Distance. We evaluate our approach on two self-captured scenes: An outdoor scene of a cultural heritage statue and an indoor scene of a fine-structured plant. Our results show that the HoloLens data, including RGB images, corresponding camera poses, and depth sensing based point clouds to initialize the Gaussians, are suitable as input for 3D Gaussian Splatting.</li>
</ul>

<h3>Title: DiffMap: Enhancing Map Segmentation with Map Prior Using Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Peijin Jia, Tuopu Wen, Ziang Luo, Mengmeng Yang, Kun Jiang, Zhiquan Lei, Xuewei Tang, Ziyuan Liu, Le Cui, Kehua Sheng, Bo Zhang, Diange Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02008">https://arxiv.org/abs/2405.02008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02008">https://arxiv.org/pdf/2405.02008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02008]] DiffMap: Enhancing Map Segmentation with Map Prior Using Diffusion Model(https://arxiv.org/abs/2405.02008)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Constructing high-definition (HD) maps is a crucial requirement for enabling autonomous driving. In recent years, several map segmentation algorithms have been developed to address this need, leveraging advancements in Bird's-Eye View (BEV) perception. However, existing models still encounter challenges in producing realistic and consistent semantic map layouts. One prominent issue is the limited utilization of structured priors inherent in map segmentation masks. In light of this, we propose DiffMap, a novel approach specifically designed to model the structured priors of map segmentation masks using latent diffusion model. By incorporating this technique, the performance of existing semantic segmentation methods can be significantly enhanced and certain structural errors present in the segmentation outputs can be effectively rectified. Notably, the proposed module can be seamlessly integrated into any map segmentation model, thereby augmenting its capability to accurately delineate semantic information. Furthermore, through extensive visualization analysis, our model demonstrates superior proficiency in generating results that more accurately reflect real-world map layouts, further validating its efficacy in improving the quality of the generated maps.</li>
</ul>

<h3>Title: The Trade-off between Performance, Efficiency, and Fairness in Adapter  Modules for Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Minh Duc Bui, Katharina von der Wense</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02010">https://arxiv.org/abs/2405.02010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02010">https://arxiv.org/pdf/2405.02010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02010]] The Trade-off between Performance, Efficiency, and Fairness in Adapter  Modules for Text Classification(https://arxiv.org/abs/2405.02010)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair</a></li>
<li><strong>Abstract: </strong>Current natural language processing (NLP) research tends to focus on only one or, less frequently, two dimensions - e.g., performance, privacy, fairness, or efficiency - at a time, which may lead to suboptimal conclusions and often overlooking the broader goal of achieving trustworthy NLP. Work on adapter modules (Houlsby et al., 2019; Hu et al., 2021) focuses on improving performance and efficiency, with no investigation of unintended consequences on other aspects such as fairness. To address this gap, we conduct experiments on three text classification datasets by either (1) finetuning all parameters or (2) using adapter modules. Regarding performance and efficiency, we confirm prior findings that the accuracy of adapter-enhanced models is roughly on par with that of fully finetuned models, while training time is substantially reduced. Regarding fairness, we show that adapter modules result in mixed fairness across sensitive groups. Further investigation reveals that, when the standard fine-tuned model exhibits limited biases, adapter modules typically do not introduce extra bias. On the other hand, when the finetuned model exhibits increased bias, the impact of adapter modules on bias becomes more unpredictable, introducing the risk of significantly magnifying these biases for certain groups. Our findings highlight the need for a case-by-case evaluation rather than a one-size-fits-all judgment.</li>
</ul>

<h3>Title: IFNet: Deep Imaging and Focusing for Handheld SAR with Millimeter-wave  Signals</h3>
<ul>
<li><strong>Authors: </strong>Li Yadong, Zhang Dongheng, Geng Ruixu, Wu Jincheng, Hu Yang, Sun Qibin, Chen Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02023">https://arxiv.org/abs/2405.02023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02023">https://arxiv.org/pdf/2405.02023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02023]] IFNet: Deep Imaging and Focusing for Handheld SAR with Millimeter-wave  Signals(https://arxiv.org/abs/2405.02023)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advancements have showcased the potential of handheld millimeter-wave (mmWave) imaging, which applies synthetic aperture radar (SAR) principles in portable settings. However, existing studies addressing handheld motion errors either rely on costly tracking devices or employ simplified imaging models, leading to impractical deployment or limited performance. In this paper, we present IFNet, a novel deep unfolding network that combines the strengths of signal processing models and deep neural networks to achieve robust imaging and focusing for handheld mmWave systems. We first formulate the handheld imaging model by integrating multiple priors about mmWave images and handheld phase errors. Furthermore, we transform the optimization processes into an iterative network structure for improved and efficient imaging performance. Extensive experiments demonstrate that IFNet effectively compensates for handheld phase errors and recovers high-fidelity images from severely distorted signals. In comparison with existing methods, IFNet can achieve at least 11.89 dB improvement in average peak signal-to-noise ratio (PSNR) and 64.91% improvement in average structural similarity index measure (SSIM) on a real-world dataset.</li>
</ul>

<h3>Title: Analyzing Narrative Processing in Large Language Models (LLMs): Using  GPT4 to test BERT</h3>
<ul>
<li><strong>Authors: </strong>Patrick Krauss, Jannik Hösch, Claus Metzner, Andreas Maier, Peter Uhrig, Achim Schilling</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02024">https://arxiv.org/abs/2405.02024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02024">https://arxiv.org/pdf/2405.02024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02024]] Analyzing Narrative Processing in Large Language Models (LLMs): Using  GPT4 to test BERT(https://arxiv.org/abs/2405.02024)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The ability to transmit and receive complex information via language is unique to humans and is the basis of traditions, culture and versatile social interactions. Through the disruptive introduction of transformer based large language models (LLMs) humans are not the only entity to "understand" and produce language any more. In the present study, we have performed the first steps to use LLMs as a model to understand fundamental mechanisms of language processing in neural networks, in order to make predictions and generate hypotheses on how the human brain does language processing. Thus, we have used ChatGPT to generate seven different stylistic variations of ten different narratives (Aesop's fables). We used these stories as input for the open source LLM BERT and have analyzed the activation patterns of the hidden units of BERT using multi-dimensional scaling and cluster analysis. We found that the activation vectors of the hidden units cluster according to stylistic variations in earlier layers of BERT (1) than narrative content (4-5). Despite the fact that BERT consists of 12 identical building blocks that are stacked and trained on large text corpora, the different layers perform different tasks. This is a very useful model of the human brain, where self-similar structures, i.e. different areas of the cerebral cortex, can have different functions and are therefore well suited to processing language in a very efficient way. The proposed approach has the potential to open the black box of LLMs on the one hand, and might be a further step to unravel the neural processes underlying human language processing and cognition in general.</li>
</ul>

<h3>Title: Large Multimodal Model based Standardisation of Pathology Reports with  Confidence and their Prognostic Significance</h3>
<ul>
<li><strong>Authors: </strong>Ethar Alzaid, Gabriele Pergola, Harriet Evans, David Snead, Fayyaz Minhas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02040">https://arxiv.org/abs/2405.02040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02040">https://arxiv.org/pdf/2405.02040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02040]] Large Multimodal Model based Standardisation of Pathology Reports with  Confidence and their Prognostic Significance(https://arxiv.org/abs/2405.02040)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Pathology reports are rich in clinical and pathological details but are often presented in free-text format. The unstructured nature of these reports presents a significant challenge limiting the accessibility of their content. In this work, we present a practical approach based on the use of large multimodal models (LMMs) for automatically extracting information from scanned images of pathology reports with the goal of generating a standardised report specifying the value of different fields along with estimated confidence about the accuracy of the extracted fields. The proposed approach overcomes limitations of existing methods which do not assign confidence scores to extracted fields limiting their practical use. The proposed framework uses two stages of prompting a Large Multimodal Model (LMM) for information extraction and validation. The framework generalises to textual reports from multiple medical centres as well as scanned images of legacy pathology reports. We show that the estimated confidence is an effective indicator of the accuracy of the extracted information that can be used to select only accurately extracted fields. We also show the prognostic significance of structured and unstructured data from pathology reports and show that the automatically extracted field values significant prognostic value for patient stratification. The framework is available for evaluation via the URL: https://labieb.dcs.warwick.ac.uk/.</li>
</ul>

<h3>Title: On human-centred security: A new systems model based on modes and mode  transitions</h3>
<ul>
<li><strong>Authors: </strong>Edwin J Beggs, John V Tucker, Victoria Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02043">https://arxiv.org/abs/2405.02043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02043">https://arxiv.org/pdf/2405.02043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02043]] On human-centred security: A new systems model based on modes and mode  transitions(https://arxiv.org/abs/2405.02043)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>We propose an abstract conceptual framework for analysing complex security systems using a new notion of modes and mode transitions. A mode is an independent component of a system with its own objectives, monitoring data, algorithms, and scope and limits. The behaviour of a mode, including its transitions to other modes, is determined by interpretations of the mode's monitoring data in the light of its objectives and capabilities -- these interpretations we call beliefs. We formalise the conceptual framework mathematically and, by quantifying and visualising beliefs in higher-dimensional geometric spaces, we argue our models may help both design, analyse and explain systems. The mathematical models are based on simplicial complexes.</li>
</ul>

<h3>Title: Zero-Sum Positional Differential Games as a Framework for Robust  Reinforcement Learning: Deep Q-Learning Approach</h3>
<ul>
<li><strong>Authors: </strong>Anton Plaksin, Vitaly Kalev</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GT, eess.SY, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02044">https://arxiv.org/abs/2405.02044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02044">https://arxiv.org/pdf/2405.02044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02044]] Zero-Sum Positional Differential Games as a Framework for Robust  Reinforcement Learning: Deep Q-Learning Approach(https://arxiv.org/abs/2405.02044)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust Reinforcement Learning (RRL) is a promising Reinforcement Learning (RL) paradigm aimed at training robust to uncertainty or disturbances models, making them more efficient for real-world applications. Following this paradigm, uncertainty or disturbances are interpreted as actions of a second adversarial agent, and thus, the problem is reduced to seeking the agents' policies robust to any opponent's actions. This paper is the first to propose considering the RRL problems within the positional differential game theory, which helps us to obtain theoretically justified intuition to develop a centralized Q-learning approach. Namely, we prove that under Isaacs's condition (sufficiently general for real-world dynamical systems), the same Q-function can be utilized as an approximate solution of both minimax and maximin Bellman equations. Based on these results, we present the Isaacs Deep Q-Network algorithms and demonstrate their superiority compared to other baseline RRL and Multi-Agent RL algorithms in various environments.</li>
</ul>

<h3>Title: Federated Learning for Tabular Data using TabNet: A Vehicular Use-Case</h3>
<ul>
<li><strong>Authors: </strong>William Lindskog, Christian Prehofer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02060">https://arxiv.org/abs/2405.02060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02060">https://arxiv.org/pdf/2405.02060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02060]] Federated Learning for Tabular Data using TabNet: A Vehicular Use-Case(https://arxiv.org/abs/2405.02060)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>In this paper, we show how Federated Learning (FL) can be applied to vehicular use-cases in which we seek to classify obstacles, irregularities and pavement types on roads. Our proposed framework utilizes FL and TabNet, a state-of-the-art neural network for tabular data. We are the first to demonstrate how TabNet can be integrated with FL. Moreover, we achieve a maximum test accuracy of 93.6%. Finally, we reason why FL is a suitable concept for this data set.</li>
</ul>

<h3>Title: Towards general deep-learning-based tree instance segmentation models</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Henrich, Jan van Delden</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02061">https://arxiv.org/abs/2405.02061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02061">https://arxiv.org/pdf/2405.02061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02061]] Towards general deep-learning-based tree instance segmentation models(https://arxiv.org/abs/2405.02061)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The segmentation of individual trees from forest point clouds is a crucial task for downstream analyses such as carbon sequestration estimation. Recently, deep-learning-based methods have been proposed which show the potential of learning to segment trees. Since these methods are trained in a supervised way, the question arises how general models can be obtained that are applicable across a wide range of settings. So far, training has been mainly conducted with data from one specific laser scanning type and for specific types of forests. In this work, we train one segmentation model under various conditions, using seven diverse datasets found in literature, to gain insights into the generalization capabilities under domain-shift. Our results suggest that a generalization from coniferous dominated sparse point clouds to deciduous dominated high-resolution point clouds is possible. Conversely, qualitative evidence suggests that generalization from high-resolution to low-resolution point clouds is challenging. This emphasizes the need for forest point clouds with diverse data characteristics for model development. To enrich the available data basis, labeled trees from two previous works were propagated to the complete forest point cloud and are made publicly available at https://doi.org/10.25625/QUTUWU.</li>
</ul>

<h3>Title: WateRF: Robust Watermarks in Radiance Fields for Protection of  Copyrights</h3>
<ul>
<li><strong>Authors: </strong>Youngdong Jang, Dong In Lee, MinHyuk Jang, Jong Wook Kim, Feng Yang, Sangpil Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02066">https://arxiv.org/abs/2405.02066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02066">https://arxiv.org/pdf/2405.02066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02066]] WateRF: Robust Watermarks in Radiance Fields for Protection of  Copyrights(https://arxiv.org/abs/2405.02066)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, watermark</a></li>
<li><strong>Abstract: </strong>The advances in the Neural Radiance Fields (NeRF) research offer extensive applications in diverse domains, but protecting their copyrights has not yet been researched in depth. Recently, NeRF watermarking has been considered one of the pivotal solutions for safely deploying NeRF-based 3D representations. However, existing methods are designed to apply only to implicit or explicit NeRF representations. In this work, we introduce an innovative watermarking method that can be employed in both representations of NeRF. This is achieved by fine-tuning NeRF to embed binary messages in the rendering process. In detail, we propose utilizing the discrete wavelet transform in the NeRF space for watermarking. Furthermore, we adopt a deferred back-propagation technique and introduce a combination with the patch-wise loss to improve rendering quality and bit accuracy with minimum trade-offs. We evaluate our method in three different aspects: capacity, invisibility, and robustness of the embedded watermarks in the 2D-rendered images. Our method achieves state-of-the-art performance with faster training speed over the compared state-of-the-art methods.</li>
</ul>

<h3>Title: Histogram-Based Federated XGBoost using Minimal Variance Sampling for  Federated Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>William Lindskog, Christian Prehofer, Sarandeep Singh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02067">https://arxiv.org/abs/2405.02067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02067">https://arxiv.org/pdf/2405.02067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02067]] Histogram-Based Federated XGBoost using Minimal Variance Sampling for  Federated Tabular Data(https://arxiv.org/abs/2405.02067)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has gained considerable traction, yet, for tabular data, FL has received less attention. Most FL research has focused on Neural Networks while Tree-Based Models (TBMs) such as XGBoost have historically performed better on tabular data. It has been shown that subsampling of training data when building trees can improve performance but it is an open problem whether such subsampling can improve performance in FL. In this paper, we evaluate a histogram-based federated XGBoost that uses Minimal Variance Sampling (MVS). We demonstrate the underlying algorithm and show that our model using MVS can improve performance in terms of accuracy and regression error in a federated setting. In our evaluation, our model using MVS performs better than uniform (random) sampling and no sampling at all. It achieves both outstanding local and global performance on a new set of federated tabular datasets. Federated XGBoost using MVS also outperforms centralized XGBoost in half of the studied cases.</li>
</ul>

<h3>Title: Advancing Pre-trained Teacher: Towards Robust Feature Discrepancy for  Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Canhui Tang, Sanping Zhou, Yizhe Li, Yonghao Dong, Le Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02068">https://arxiv.org/abs/2405.02068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02068">https://arxiv.org/pdf/2405.02068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02068]] Advancing Pre-trained Teacher: Towards Robust Feature Discrepancy for  Anomaly Detection(https://arxiv.org/abs/2405.02068)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the wide application of knowledge distillation between an ImageNet pre-trained teacher model and a learnable student model, industrial anomaly detection has witnessed a significant achievement in the past few years. The success of knowledge distillation mainly relies on how to keep the feature discrepancy between the teacher and student model, in which it assumes that: (1) the teacher model can jointly represent two different distributions for the normal and abnormal patterns, while (2) the student model can only reconstruct the normal distribution. However, it still remains a challenging issue to maintain these ideal assumptions in practice. In this paper, we propose a simple yet effective two-stage industrial anomaly detection framework, termed as AAND, which sequentially performs Anomaly Amplification and Normality Distillation to obtain robust feature discrepancy. In the first anomaly amplification stage, we propose a novel Residual Anomaly Amplification (RAA) module to advance the pre-trained teacher encoder. With the exposure of synthetic anomalies, it amplifies anomalies via residual generation while maintaining the integrity of pre-trained model. It mainly comprises a Matching-guided Residual Gate and an Attribute-scaling Residual Generator, which can determine the residuals' proportion and characteristic, respectively. In the second normality distillation stage, we further employ a reverse distillation paradigm to train a student decoder, in which a novel Hard Knowledge Distillation (HKD) loss is built to better facilitate the reconstruction of normal patterns. Comprehensive experiments on the MvTecAD, VisA, and MvTec3D-RGB datasets show that our method achieves state-of-the-art performance.</li>
</ul>

<h3>Title: A Federated Learning Benchmark on Tabular Data: Comparing Tree-Based  Models and Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>William Lindskog, Christian Prehofer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02074">https://arxiv.org/abs/2405.02074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02074">https://arxiv.org/pdf/2405.02074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02074]] A Federated Learning Benchmark on Tabular Data: Comparing Tree-Based  Models and Neural Networks(https://arxiv.org/abs/2405.02074)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has lately gained traction as it addresses how machine learning models train on distributed datasets. FL was designed for parametric models, namely Deep Neural Networks (DNNs).Thus, it has shown promise on image and text tasks. However, FL for tabular data has received little attention. Tree-Based Models (TBMs) have been considered to perform better on tabular data and they are starting to see FL integrations. In this study, we benchmark federated TBMs and DNNs for horizontal FL, with varying data partitions, on 10 well-known tabular datasets. Our novel benchmark results indicates that current federated boosted TBMs perform better than federated DNNs in different data partitions. Furthermore, a federated XGBoost outperforms all other models. Lastly, we find that federated TBMs perform better than federated parametric models, even when increasing the number of clients significantly.</li>
</ul>

<h3>Title: Argumentative Large Language Models for Explainable and Contestable  Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Freedman, Adam Dejl, Deniz Gorur, Xiang Yin, Antonio Rago, Francesca Toni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02079">https://arxiv.org/abs/2405.02079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02079">https://arxiv.org/pdf/2405.02079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02079]] Argumentative Large Language Models for Explainable and Contestable  Decision-Making(https://arxiv.org/abs/2405.02079)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The diversity of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them a promising candidate for use in decision-making. However, they are currently limited by their inability to reliably provide outputs which are explainable and contestable. In this paper, we attempt to reconcile these strengths and weaknesses by introducing a method for supplementing LLMs with argumentative reasoning. Concretely, we introduce argumentative LLMs, a method utilising LLMs to construct argumentation frameworks, which then serve as the basis for formal reasoning in decision-making. The interpretable nature of these argumentation frameworks and formal reasoning means that any decision made by the supplemented LLM may be naturally explained to, and contested by, humans. We demonstrate the effectiveness of argumentative LLMs experimentally in the decision-making task of claim verification. We obtain results that are competitive with, and in some cases surpass, comparable state-of-the-art techniques.</li>
</ul>

<h3>Title: A Mutual Information Perspective on Federated Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Christos Louizos, Matthias Reisser, Denis Korzhenkov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02081">https://arxiv.org/abs/2405.02081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02081">https://arxiv.org/pdf/2405.02081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02081]] A Mutual Information Perspective on Federated Contrastive Learning(https://arxiv.org/abs/2405.02081)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>We investigate contrastive learning in the federated setting through the lens of SimCLR and multi-view mutual information maximization. In doing so, we uncover a connection between contrastive representation learning and user verification; by adding a user verification loss to each client's local SimCLR loss we recover a lower bound to the global multi-view mutual information. To accommodate for the case of when some labelled data are available at the clients, we extend our SimCLR variant to the federated semi-supervised setting. We see that a supervised SimCLR objective can be obtained with two changes: a) the contrastive loss is computed between datapoints that share the same label and b) we require an additional auxiliary head that predicts the correct labels from either of the two views. Along with the proposed SimCLR extensions, we also study how different sources of non-i.i.d.-ness can impact the performance of federated unsupervised learning through global mutual information maximization; we find that a global objective is beneficial for some sources of non-i.i.d.-ness but can be detrimental for others. We empirically evaluate our proposed extensions in various tasks to validate our claims and furthermore demonstrate that our proposed modifications generalize to other pretraining methods.</li>
</ul>

<h3>Title: Got Root? A Linux Priv-Esc Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Andreas Happe, Jürgen Cito</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02106">https://arxiv.org/abs/2405.02106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02106">https://arxiv.org/pdf/2405.02106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02106]] Got Root? A Linux Priv-Esc Benchmark(https://arxiv.org/abs/2405.02106)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, robust</a></li>
<li><strong>Abstract: </strong>Linux systems are integral to the infrastructure of modern computing environments, necessitating robust security measures to prevent unauthorized access. Privilege escalation attacks represent a significant threat, typically allowing attackers to elevate their privileges from an initial low-privilege account to the all-powerful root account. A benchmark set of vulnerable systems is of high importance to evaluate the effectiveness of privilege-escalation techniques performed by both humans and automated tooling. Analyzing their behavior allows defenders to better fortify their entrusted Linux systems and thus protect their infrastructure from potentially devastating attacks. To address this gap, we developed a comprehensive benchmark for Linux privilege escalation. It provides a standardized platform to evaluate and compare the performance of human and synthetic actors, e.g., hacking scripts or automated tooling.</li>
</ul>

<h3>Title: Probablistic Restoration with Adaptive Noise Sampling for 3D Human Pose  Estimation</h3>
<ul>
<li><strong>Authors: </strong>Xianzhou Zeng, Hao Qin, Ming Kong, Luyuan Chen, Qiang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02114">https://arxiv.org/abs/2405.02114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02114">https://arxiv.org/pdf/2405.02114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02114]] Probablistic Restoration with Adaptive Noise Sampling for 3D Human Pose  Estimation(https://arxiv.org/abs/2405.02114)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>The accuracy and robustness of 3D human pose estimation (HPE) are limited by 2D pose detection errors and 2D to 3D ill-posed challenges, which have drawn great attention to Multi-Hypothesis HPE research. Most existing MH-HPE methods are based on generative models, which are computationally expensive and difficult to train. In this study, we propose a Probabilistic Restoration 3D Human Pose Estimation framework (PRPose) that can be integrated with any lightweight single-hypothesis model. Specifically, PRPose employs a weakly supervised approach to fit the hidden probability distribution of the 2D-to-3D lifting process in the Single-Hypothesis HPE model and then reverse-map the distribution to the 2D pose input through an adaptive noise sampling strategy to generate reasonable multi-hypothesis samples effectively. Extensive experiments on 3D HPE benchmarks (Human3.6M and MPI-INF-3DHP) highlight the effectiveness and efficiency of PRPose. Code is available at: https://github.com/xzhouzeng/PRPose.</li>
</ul>

<h3>Title: Single and Multi-Hop Question-Answering Datasets for Reticular Chemistry  with GPT-4-Turbo</h3>
<ul>
<li><strong>Authors: </strong>Nakul Rampal, Kaiyu Wang, Matthew Burigana, Lingxiang Hou, Juri Al-Johani, Anna Sackmann, Hanan S. Murayshid, Walaa Abdullah Al-Sumari, Arwa M. Al-Abdulkarim, Nahla Eid Al-Hazmi, Majed O. Al-Awad, Christian Borgs, Jennifer T. Chayes, Omar M. Yaghi</a></li>
<li><strong>Subjects: </strong>cs.CL, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02128">https://arxiv.org/abs/2405.02128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02128">https://arxiv.org/pdf/2405.02128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02128]] Single and Multi-Hop Question-Answering Datasets for Reticular Chemistry  with GPT-4-Turbo(https://arxiv.org/abs/2405.02128)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rapid advancement in artificial intelligence and natural language processing has led to the development of large-scale datasets aimed at benchmarking the performance of machine learning models. Herein, we introduce 'RetChemQA,' a comprehensive benchmark dataset designed to evaluate the capabilities of such models in the domain of reticular chemistry. This dataset includes both single-hop and multi-hop question-answer pairs, encompassing approximately 45,000 Q&As for each type. The questions have been extracted from an extensive corpus of literature containing about 2,530 research papers from publishers including NAS, ACS, RSC, Elsevier, and Nature Publishing Group, among others. The dataset has been generated using OpenAI's GPT-4 Turbo, a cutting-edge model known for its exceptional language understanding and generation capabilities. In addition to the Q&A dataset, we also release a dataset of synthesis conditions extracted from the corpus of literature used in this study. The aim of RetChemQA is to provide a robust platform for the development and evaluation of advanced machine learning algorithms, particularly for the reticular chemistry community. The dataset is structured to reflect the complexities and nuances of real-world scientific discourse, thereby enabling nuanced performance assessments across a variety of tasks. The dataset is available at the following link: https://github.com/nakulrampal/RetChemQA</li>
</ul>

<h3>Title: Optimising Calls to Large Language Models with Uncertainty-Based  Two-Tier Selection</h3>
<ul>
<li><strong>Authors: </strong>Guillem Ramírez, Alexandra Birch, Ivan Titov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02134">https://arxiv.org/abs/2405.02134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02134">https://arxiv.org/pdf/2405.02134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02134]] Optimising Calls to Large Language Models with Uncertainty-Based  Two-Tier Selection(https://arxiv.org/abs/2405.02134)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Researchers and practitioners operating on a limited budget face the cost-performance trade-off dilemma. The challenging decision often centers on whether to use a large LLM with better performance or a smaller one with reduced costs. This has motivated recent research in the optimisation of LLM calls. Either a cascading strategy is used, where a smaller LLM or both are called sequentially, or a routing strategy is used, where only one model is ever called. Both scenarios are dependent on a decision criterion which is typically implemented by an extra neural model. In this work, we propose a simpler solution; we use only the uncertainty of the generations of the small LLM as the decision criterion. We compare our approach with both cascading and routing strategies using three different pairs of pre-trained small and large LLMs, on nine different tasks and against approaches that require an additional neural model. Our experiments reveal this simple solution optimally balances cost and performance, outperforming existing methods on 25 out of 27 experimental setups.</li>
</ul>

<h3>Title: An Information Theoretic Perspective on Conformal Prediction</h3>
<ul>
<li><strong>Authors: </strong>Alvaro H.C. Correia, Fabio Valerio Massoli, Christos Louizos, Arash Behboodi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02140">https://arxiv.org/abs/2405.02140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02140">https://arxiv.org/pdf/2405.02140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02140]] An Information Theoretic Perspective on Conformal Prediction(https://arxiv.org/abs/2405.02140)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Conformal Prediction (CP) is a distribution-free uncertainty estimation framework that constructs prediction sets guaranteed to contain the true answer with a user-specified probability. Intuitively, the size of the prediction set encodes a general notion of uncertainty, with larger sets associated with higher degrees of uncertainty. In this work, we leverage information theory to connect conformal prediction to other notions of uncertainty. More precisely, we prove three different ways to upper bound the intrinsic uncertainty, as described by the conditional entropy of the target variable given the inputs, by combining CP with information theoretical inequalities. Moreover, we demonstrate two direct and useful applications of such connection between conformal prediction and information theory: (i) more principled and effective conformal training objectives that generalize previous approaches and enable end-to-end training of machine learning models from scratch, and (ii) a natural mechanism to incorporate side information into conformal prediction. We empirically validate both applications in centralized and federated learning settings, showing our theoretical results translate to lower inefficiency (average prediction set size) for popular CP methods.</li>
</ul>

<h3>Title: MedReadMe: A Systematic Study for Fine-grained Sentence Readability in  Medical Domain</h3>
<ul>
<li><strong>Authors: </strong>Chao Jiang, Wei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02144">https://arxiv.org/abs/2405.02144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02144">https://arxiv.org/pdf/2405.02144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02144]] MedReadMe: A Systematic Study for Fine-grained Sentence Readability in  Medical Domain(https://arxiv.org/abs/2405.02144)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Medical texts are notoriously challenging to read. Properly measuring their readability is the first step towards making them more accessible. In this paper, we present a systematic study on fine-grained readability measurements in the medical domain at both sentence-level and span-level. We introduce a new dataset MedReadMe, which consists of manually annotated readability ratings and fine-grained complex span annotation for 4,520 sentences, featuring two novel "Google-Easy" and "Google-Hard" categories. It supports our quantitative analysis, which covers 650 linguistic features and automatic complex word and jargon identification. Enabled by our high-quality annotation, we benchmark and improve several state-of-the-art sentence-level readability metrics for the medical domain specifically, which include unsupervised, supervised, and prompting-based methods using recently developed large language models (LLMs). Informed by our fine-grained complex span annotation, we find that adding a single feature, capturing the number of jargon spans, into existing readability formulas can significantly improve their correlation with human judgments. We will publicly release the dataset and code.</li>
</ul>

<h3>Title: Payout Races and Congested Channels: A Formal Analysis of Security in  the Lightning Network</h3>
<ul>
<li><strong>Authors: </strong>Ben Weintraub, Satwik Prabhu Kumble, Cristina Nita-Rotaru, Stefanie Roos</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02147">https://arxiv.org/abs/2405.02147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02147">https://arxiv.org/pdf/2405.02147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02147]] Payout Races and Congested Channels: A Formal Analysis of Security in  the Lightning Network(https://arxiv.org/abs/2405.02147)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The Lightning Network, a payment channel network with a market cap of over 192M USD, is designed to resolve Bitcoin's scalability issues through fast off-chain transactions. There are multiple Lightning Network client implementations, all of which conform to the same textual specifications known as BOLTs. Several vulnerabilities have been manually discovered, but to-date there have been few works systematically analyzing the security of the Lightning Network. In this work, we take a foundational approach to analyzing the security of the Lightning Network with the help of formal methods. Based on the BOLTs' specifications, we build a detailed formal model of the Lightning Network's single-hop payment protocol and verify it using the Spin model checker. Our model captures both concurrency and error semantics of the payment protocol. We then define several security properties which capture the correct intermediate operation of the protocol, ensuring that the outcome is always certain to both channel peers, and using them we re-discover a known attack previously reported in the literature along with a novel attack, referred to as a Payout Race. A Payout Race consists of a particular sequence of events that can lead to an ambiguity in the protocol in which innocent users can unwittingly lose funds. We confirm the practicality of this attack by reproducing it in a local testbed environment.</li>
</ul>

<h3>Title: Simulating the economic impact of rationality through reinforcement  learning and agent-based modelling</h3>
<ul>
<li><strong>Authors: </strong>Simone Brusatin, Tommaso Padoan, Andrea Coletta, Domenico Delli Gatti, Aldo Glielmo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, cs.MA, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02161">https://arxiv.org/abs/2405.02161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02161">https://arxiv.org/pdf/2405.02161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02161]] Simulating the economic impact of rationality through reinforcement  learning and agent-based modelling(https://arxiv.org/abs/2405.02161)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Agent-based models (ABMs) are simulation models used in economics to overcome some of the limitations of traditional frameworks based on general equilibrium assumptions. However, agents within an ABM follow predetermined, not fully rational, behavioural rules which can be cumbersome to design and difficult to justify. Here we leverage multi-agent reinforcement learning (RL) to expand the capabilities of ABMs with the introduction of fully rational agents that learn their policy by interacting with the environment and maximising a reward function. Specifically, we propose a 'Rational macro ABM' (R-MABM) framework by extending a paradigmatic macro ABM from the economic literature. We show that gradually substituting ABM firms in the model with RL agents, trained to maximise profits, allows for a thorough study of the impact of rationality on the economy. We find that RL agents spontaneously learn three distinct strategies for maximising profits, with the optimal strategy depending on the level of market competition and rationality. We also find that RL agents with independent policies, and without the ability to communicate with each other, spontaneously learn to segregate into different strategic groups, thus increasing market power and overall profits. Finally, we find that a higher degree of rationality in the economy always improves the macroeconomic environment as measured by total output, depending on the specific rational policy, this can come at the cost of higher instability. Our R-MABM framework is general, it allows for stable multi-agent learning, and represents a principled and robust direction to extend existing economic simulators.</li>
</ul>

<h3>Title: EEG2TEXT: Open Vocabulary EEG-to-Text Decoding with EEG Pre-Training and  Multi-View Transformer</h3>
<ul>
<li><strong>Authors: </strong>Hanwen Liu, Daniel Hajialigol, Benny Antony, Aiguo Han, Xuan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02165">https://arxiv.org/abs/2405.02165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02165">https://arxiv.org/pdf/2405.02165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02165]] EEG2TEXT: Open Vocabulary EEG-to-Text Decoding with EEG Pre-Training and  Multi-View Transformer(https://arxiv.org/abs/2405.02165)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deciphering the intricacies of the human brain has captivated curiosity for centuries. Recent strides in Brain-Computer Interface (BCI) technology, particularly using motor imagery, have restored motor functions such as reaching, grasping, and walking in paralyzed individuals. However, unraveling natural language from brain signals remains a formidable challenge. Electroencephalography (EEG) is a non-invasive technique used to record electrical activity in the brain by placing electrodes on the scalp. Previous studies of EEG-to-text decoding have achieved high accuracy on small closed vocabularies, but still fall short of high accuracy when dealing with large open vocabularies. We propose a novel method, EEG2TEXT, to improve the accuracy of open vocabulary EEG-to-text decoding. Specifically, EEG2TEXT leverages EEG pre-training to enhance the learning of semantics from EEG signals and proposes a multi-view transformer to model the EEG signal processing by different spatial regions of the brain. Experiments show that EEG2TEXT has superior performance, outperforming the state-of-the-art baseline methods by a large margin of up to 5% in absolute BLEU and ROUGE scores. EEG2TEXT shows great potential for a high-performance open-vocabulary brain-to-text system to facilitate communication.</li>
</ul>

<h3>Title: Assessing and Verifying Task Utility in LLM-Powered Applications</h3>
<ul>
<li><strong>Authors: </strong>Negar Arabzadeh, Siging Huo, Nikhil Mehta, Qinqyun Wu, Chi Wang, Ahmed Awadallah, Charles L. A. Clarke, Julia Kiseleva</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02178">https://arxiv.org/abs/2405.02178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02178">https://arxiv.org/pdf/2405.02178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02178]] Assessing and Verifying Task Utility in LLM-Powered Applications(https://arxiv.org/abs/2405.02178)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid development of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents, assisting humans in their daily tasks. However, a significant gap remains in assessing to what extent LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the need to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the effectiveness and robustness of AgentEval for two open source datasets including Math Problem solving and ALFWorld House-hold related tasks. For reproducibility purposes, we make the data, code and all the logs publicly available at https://bit.ly/3w3yKcS .</li>
</ul>

<h3>Title: A Flow-Based Model for Conditional and Probabilistic Electricity  Consumption Profile Generation and Prediction</h3>
<ul>
<li><strong>Authors: </strong>Weijie Xia, Chenguang Wang, Peter Palensky, Pedro P. Vergara</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02180">https://arxiv.org/abs/2405.02180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02180">https://arxiv.org/pdf/2405.02180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02180]] A Flow-Based Model for Conditional and Probabilistic Electricity  Consumption Profile Generation and Prediction(https://arxiv.org/abs/2405.02180)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Residential Load Profile (RLP) generation and prediction are critical for the operation and planning of distribution networks, particularly as diverse low-carbon technologies are increasingly integrated. This paper introduces a novel flow-based generative model, termed Full Convolutional Profile Flow (FCPFlow), which is uniquely designed for both conditional and unconditional RLP generation, and for probabilistic load forecasting. By introducing two new layers--the invertible linear layer and the invertible normalization layer--the proposed FCPFlow architecture shows three main advantages compared to traditional statistical and contemporary deep generative models: 1) it is well-suited for RLP generation under continuous conditions, such as varying weather and annual electricity consumption, 2) it shows superior scalability in different datasets compared to traditional statistical, and 3) it also demonstrates better modeling capabilities in capturing the complex correlation of RLPs compared with deep generative models.</li>
</ul>

<h3>Title: Non-Destructive Peat Analysis using Hyperspectral Imaging and Machine  Learning</h3>
<ul>
<li><strong>Authors: </strong>Yijun Yan, Jinchang Ren, Barry Harrison, Oliver Lewis, Yinhe Li, Ping Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02191">https://arxiv.org/abs/2405.02191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02191">https://arxiv.org/pdf/2405.02191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02191]] Non-Destructive Peat Analysis using Hyperspectral Imaging and Machine  Learning(https://arxiv.org/abs/2405.02191)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Peat, a crucial component in whisky production, imparts distinctive and irreplaceable flavours to the final product. However, the extraction of peat disrupts ancient ecosystems and releases significant amounts of carbon, contributing to climate change. This paper aims to address this issue by conducting a feasibility study on enhancing peat use efficiency in whisky manufacturing through non-destructive analysis using hyperspectral imaging. Results show that shot-wave infrared (SWIR) data is more effective for analyzing peat samples and predicting total phenol levels, with accuracies up to 99.81%.</li>
</ul>

<h3>Title: Multispectral Fine-Grained Classification of Blackgrass in Wheat and  Barley Crops</h3>
<ul>
<li><strong>Authors: </strong>Madeleine Darbyshire, Shaun Coutts, Eleanor Hammond, Fazilet Gokbudak, Cengiz Oztireli, Petra Bosilj, Junfeng Gao, Elizabeth Sklar, Simon Parsons</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02218">https://arxiv.org/abs/2405.02218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02218">https://arxiv.org/pdf/2405.02218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02218]] Multispectral Fine-Grained Classification of Blackgrass in Wheat and  Barley Crops(https://arxiv.org/abs/2405.02218)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, fair, transformer</a></li>
<li><strong>Abstract: </strong>As the burden of herbicide resistance grows and the environmental repercussions of excessive herbicide use become clear, new ways of managing weed populations are needed. This is particularly true for cereal crops, like wheat and barley, that are staple food crops and occupy a globally significant portion of agricultural land. Even small improvements in weed management practices across these major food crops worldwide would yield considerable benefits for both the environment and global food security. Blackgrass is a major grass weed which causes particular problems in cereal crops in north-west Europe, a major cereal production area, because it has high levels of of herbicide resistance and is well adapted to agronomic practice in this region. With the use of machine vision and multispectral imaging, we investigate the effectiveness of state-of-the-art methods to identify blackgrass in wheat and barley crops. As part of this work, we provide a large dataset with which we evaluate several key aspects of blackgrass weed recognition. Firstly, we determine the performance of different CNN and transformer-based architectures on images from unseen fields. Secondly, we demonstrate the role that different spectral bands have on the performance of weed classification. Lastly, we evaluate the role of dataset size in classification performance for each of the models trialled. We find that even with a fairly modest quantity of training data an accuracy of almost 90% can be achieved on images from unseen fields.</li>
</ul>

<h3>Title: REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific  Sentences using Public and Proprietary LLMs</h3>
<ul>
<li><strong>Authors: </strong>Deepa Tilwani, Yash Saxena, Ali Mohammadi, Edward Raff, Amit Sheth, Srinivasan Parthasarathy, Manas Gaur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02228">https://arxiv.org/abs/2405.02228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02228">https://arxiv.org/pdf/2405.02228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02228]] REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific  Sentences using Public and Proprietary LLMs(https://arxiv.org/abs/2405.02228)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Automatic citation generation for sentences in a document or report is paramount for intelligence analysts, cybersecurity, news agencies, and education personnel. In this research, we investigate whether large language models (LLMs) are capable of generating references based on two forms of sentence queries: (a) Direct Queries, LLMs are asked to provide author names of the given research article, and (b) Indirect Queries, LLMs are asked to provide the title of a mentioned article when given a sentence from a different article. To demonstrate where LLM stands in this task, we introduce a large dataset called REASONS comprising abstracts of the 12 most popular domains of scientific research on arXiv. From around 20K research articles, we make the following deductions on public and proprietary LLMs: (a) State-of-the-art, often called anthropomorphic GPT-4 and GPT-3.5, suffers from high pass percentage (PP) to minimize the hallucination rate (HR). When tested with Perplexity.ai (7B), they unexpectedly made more errors; (b) Augmenting relevant metadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmented generation (RAG) using Mistral demonstrates consistent and robust citation support on indirect queries and matched performance to GPT-3.5 and GPT-4. The HR across all domains and models decreased by an average of 41.93% and the PP was reduced to 0% in most cases. In terms of generation quality, the average F1 Score and BLEU were 68.09% and 57.51%, respectively; (d) Testing with adversarial samples showed that LLMs, including the Advance RAG Mistral, struggle to understand context, but the extent of this issue was small in Mistral and GPT-4-Preview. Our study con tributes valuable insights into the reliability of RAG for automated citation generation tasks.</li>
</ul>

<h3>Title: Learning Optimal Deterministic Policies with Stochastic Policy Gradients</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Montenegro, Marco Mussi, Alberto Maria Metelli, Matteo Papini</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02235">https://arxiv.org/abs/2405.02235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02235">https://arxiv.org/pdf/2405.02235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02235]] Learning Optimal Deterministic Policies with Stochastic Policy Gradients(https://arxiv.org/abs/2405.02235)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Policy gradient (PG) methods are successful approaches to deal with continuous reinforcement learning (RL) problems. They learn stochastic parametric (hyper)policies by either exploring in the space of actions or in the space of parameters. Stochastic controllers, however, are often undesirable from a practical perspective because of their lack of robustness, safety, and traceability. In common practice, stochastic (hyper)policies are learned only to deploy their deterministic version. In this paper, we make a step towards the theoretical understanding of this practice. After introducing a novel framework for modeling this scenario, we study the global convergence to the best deterministic policy, under (weak) gradient domination assumptions. Then, we illustrate how to tune the exploration level used for learning to optimize the trade-off between the sample complexity and the performance of the deployed deterministic policy. Finally, we quantitatively compare action-based and parameter-based exploration, giving a formal guise to intuitive results.</li>
</ul>

<h3>Title: Secure and Efficient General Matrix Multiplication On Cloud Using  Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Yang Gao, Gang Quan, Soamar Homsi, Wujie Wen, Liqiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02238">https://arxiv.org/abs/2405.02238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02238">https://arxiv.org/pdf/2405.02238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02238]] Secure and Efficient General Matrix Multiplication On Cloud Using  Homomorphic Encryption(https://arxiv.org/abs/2405.02238)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>Despite the cloud enormous technical and financial advantages, security and privacy have always been the primary concern for adopting cloud computing facility, especially for government agencies and commercial sectors with high-security requirements. Homomorphic Encryption (HE) has recently emerged as an effective tool in assuring privacy and security for sensitive applications by allowing computing on encrypted data. One major obstacle to employing HE-based computation, however, is its excessive computational cost, which is multiple magnitudes higher than its counterpart based on the plaintext. In this paper, we study the problem of how to reduce the HE-based computational cost for general Matrix Multiplication (MM), i.e., a fundamental building block for numerous practical applications, by taking advantage of the Single Instruction Multiple Data (SIMD) operation supported by HE schemes. Specifically, we develop a novel element-wise algorithm for general matrix multiplication, based on which we propose two HE-based General Matrix Multiplication (HEGMM) algorithms to reduce the HE computation cost. Our experimental results show that our algorithms can significantly outperform the state-of-the-art approaches of HE-based matrix multiplication.</li>
</ul>

<h3>Title: What matters when building vision-language models?</h3>
<ul>
<li><strong>Authors: </strong>Hugo Laurençon, Léo Tronchon, Matthieu Cord, Victor Sanh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02246">https://arxiv.org/abs/2405.02246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02246">https://arxiv.org/pdf/2405.02246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02246]] What matters when building vision-language models?(https://arxiv.org/abs/2405.02246)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods. Our consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. We release the model (base, instructed, and chat) along with the datasets created for its training.</li>
</ul>

<h3>Title: On the test-time zero-shot generalization of vision-language models: Do  we really need prompt learning?</h3>
<ul>
<li><strong>Authors: </strong>Maxime Zanella, Ismail Ben Ayed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02266">https://arxiv.org/abs/2405.02266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02266">https://arxiv.org/pdf/2405.02266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02266]] On the test-time zero-shot generalization of vision-language models: Do  we really need prompt learning?(https://arxiv.org/abs/2405.02266)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The development of large vision-language models, notably CLIP, has catalyzed research into effective adaptation techniques, with a particular focus on soft prompt tuning. Conjointly, test-time augmentation, which utilizes multiple augmented views of a single image to enhance zero-shot generalization, is emerging as a significant area of interest. This has predominantly directed research efforts toward test-time prompt tuning. In contrast, we introduce a robust MeanShift for Test-time Augmentation (MTA), which surpasses prompt-based methods without requiring this intensive training procedure. This positions MTA as an ideal solution for both standalone and API-based applications. Additionally, our method does not rely on ad hoc rules (e.g., confidence threshold) used in some previous test-time augmentation techniques to filter the augmented views. Instead, MTA incorporates a quality assessment variable for each view directly into its optimization process, termed as the inlierness score. This score is jointly optimized with a density mode seeking process, leading to an efficient training- and hyperparameter-free approach. We extensively benchmark our method on 15 datasets and demonstrate MTA's superiority and computational efficiency. Deployed easily as plug-and-play module on top of zero-shot models and state-of-the-art few-shot methods, MTA shows systematic and consistent improvements.</li>
</ul>

<h3>Title: DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular  Videos</h3>
<ul>
<li><strong>Authors: </strong>Wen-Hsuan Chu, Lei Ke, Katerina Fragkiadaki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02280">https://arxiv.org/abs/2405.02280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02280">https://arxiv.org/pdf/2405.02280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02280]] DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular  Videos(https://arxiv.org/abs/2405.02280)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Existing VLMs can track in-the-wild 2D video objects while current generative models provide powerful visual priors for synthesizing novel views for the highly under-constrained 2D-to-3D object lifting. Building upon this exciting progress, we present DreamScene4D, the first approach that can generate three-dimensional dynamic scenes of multiple objects from monocular in-the-wild videos with large object motion across occlusions and novel viewpoints. Our key insight is to design a "decompose-then-recompose" scheme to factorize both the whole video scene and each object's 3D motion. We first decompose the video scene by using open-vocabulary mask trackers and an adapted image diffusion model to segment, track, and amodally complete the objects and background in the video. Each object track is mapped to a set of 3D Gaussians that deform and move in space and time. We also factorize the observed motion into multiple components to handle fast motion. The camera motion can be inferred by re-rendering the background to match the video frames. For the object motion, we first model the object-centric deformation of the objects by leveraging rendering losses and multi-view generative priors in an object-centric frame, then optimize object-centric to world-frame transformations by comparing the rendered outputs against the perceived pixel and optical flow. Finally, we recompose the background and objects and optimize for relative object scales using monocular depth prediction guidance. We show extensive results on the challenging DAVIS, Kubric, and self-captured videos, detail some limitations, and provide future directions. Besides 4D scene generation, our results show that DreamScene4D enables accurate 2D point motion tracking by projecting the inferred 3D trajectories to 2D, while never explicitly trained to do so.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
