<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Exploiting Sparsity in Automotive Radar Object Detection Networks. (arXiv:2308.07748v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07748">http://arxiv.org/abs/2308.07748</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07748]] Exploiting Sparsity in Automotive Radar Object Detection Networks(http://arxiv.org/abs/2308.07748)</code></li>
<li>Summary: <p>Having precise perception of the environment is crucial for ensuring the
secure and reliable functioning of autonomous driving systems. Radar object
detection networks are one fundamental part of such systems. CNN-based object
detectors showed good performance in this context, but they require large
compute resources. This paper investigates sparse convolutional object
detection networks, which combine powerful grid-based detection with low
compute resources. We investigate radar specific challenges and propose sparse
kernel point pillars (SKPP) and dual voxel point convolutions (DVPC) as
remedies for the grid rendering and sparse backbone architectures. We evaluate
our SKPP-DPVCN architecture on nuScenes, which outperforms the baseline by
5.89% and the previous state of the art by 4.19% in Car AP4.0. Moreover,
SKPP-DPVCN reduces the average scale error (ASE) by 21.41% over the baseline.
</p></li>
</ul>

<h3>Title: Quantum secure non-malleable randomness encoder and its applications. (arXiv:2308.07340v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07340">http://arxiv.org/abs/2308.07340</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07340]] Quantum secure non-malleable randomness encoder and its applications(http://arxiv.org/abs/2308.07340)</code></li>
<li>Summary: <p>"Non-Malleable Randomness Encoder"(NMRE) was introduced by Kanukurthi,
Obbattu, and Sekar~[KOS18] as a useful cryptographic primitive helpful in the
construction of non-malleable codes. To the best of our knowledge, their
construction is not known to be quantum secure.
</p>
<p>We provide a construction of a first rate-$1/2$, $2$-split, quantum secure
NMRE and use this in a black-box manner, to construct for the first time the
following:
</p>
<p>1) rate $1/11$, $3$-split, quantum non-malleable code,
</p>
<p>2) rate $1/3$, $3$-split, quantum secure non-malleable code,
</p>
<p>3) rate $1/5$, $2$-split, average case quantum secure non-malleable code.
</p></li>
</ul>

<h3>Title: A Scalable Formal Verification Methodology for Data-Oblivious Hardware. (arXiv:2308.07757v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07757">http://arxiv.org/abs/2308.07757</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07757]] A Scalable Formal Verification Methodology for Data-Oblivious Hardware(http://arxiv.org/abs/2308.07757)</code></li>
<li>Summary: <p>The importance of preventing microarchitectural timing side channels in
security-critical applications has surged in recent years. Constant-time
programming has emerged as a best-practice technique for preventing the leakage
of secret information through timing. It is based on the assumption that the
timing of certain basic machine instructions is independent of their respective
input data. However, whether or not an instruction satisfies this
data-independent timing criterion varies between individual processor
microarchitectures. In this paper, we propose a novel methodology to formally
verify data-oblivious behavior in hardware using standard property checking
techniques. The proposed methodology is based on an inductive property that
enables scalability even to complex out-of-order cores. We show that proving
this inductive property is sufficient to exhaustively verify data-obliviousness
at the microarchitectural level. In addition, the paper discusses several
techniques that can be used to make the verification process easier and faster.
We demonstrate the feasibility of the proposed methodology through case studies
on several open-source designs. One case study uncovered a data-dependent
timing violation in the extensively verified and highly secure IBEX RISC-V
core. In addition to several hardware accelerators and in-order processors, our
experiments also include RISC-V BOOM, a complex out-of-order processor,
highlighting the scalability of the approach.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: ADD: An Automatic Desensitization Fisheye Dataset for Autonomous Driving. (arXiv:2308.07590v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07590">http://arxiv.org/abs/2308.07590</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07590]] ADD: An Automatic Desensitization Fisheye Dataset for Autonomous Driving(http://arxiv.org/abs/2308.07590)</code></li>
<li>Summary: <p>Autonomous driving systems require many images for analyzing the surrounding
environment. However, there is fewer data protection for private information
among these captured images, such as pedestrian faces or vehicle license
plates, which has become a significant issue. In this paper, in response to the
call for data security laws and regulations and based on the advantages of
large Field of View(FoV) of the fisheye camera, we build the first Autopilot
Desensitization Dataset, called ADD, and formulate the first
deep-learning-based image desensitization framework, to promote the study of
image desensitization in autonomous driving scenarios. The compiled dataset
consists of 650K images, including different face and vehicle license plate
information captured by the surround-view fisheye camera. It covers various
autonomous driving scenarios, including diverse facial characteristics and
license plate colors. Then, we propose an efficient multitask desensitization
network called DesCenterNet as a benchmark on the ADD dataset, which can
perform face and vehicle license plate detection and desensitization tasks.
Based on ADD, we further provide an evaluation criterion for desensitization
performance, and extensive comparison experiments have verified the
effectiveness and superiority of our method on image desensitization.
</p></li>
</ul>

<h3>Title: Using Text Injection to Improve Recognition of Personal Identifiers in Speech. (arXiv:2308.07393v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07393">http://arxiv.org/abs/2308.07393</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07393]] Using Text Injection to Improve Recognition of Personal Identifiers in Speech(http://arxiv.org/abs/2308.07393)</code></li>
<li>Summary: <p>Accurate recognition of specific categories, such as persons' names, dates or
other identifiers is critical in many Automatic Speech Recognition (ASR)
applications. As these categories represent personal information, ethical use
of this data including collection, transcription, training and evaluation
demands special care. One way of ensuring the security and privacy of
individuals is to redact or eliminate Personally Identifiable Information (PII)
from collection altogether. However, this results in ASR models that tend to
have lower recognition accuracy of these categories. We use text-injection to
improve the recognition of PII categories by including fake textual substitutes
of PII categories in the training data using a text injection method. We
demonstrate substantial improvement to Recall of Names and Dates in medical
notes while improving overall WER. For alphanumeric digit sequences we show
improvements to Character Error Rate and Sentence Accuracy.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Fairness and Privacy in Federated Learning and Their Implications in Healthcare. (arXiv:2308.07805v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07805">http://arxiv.org/abs/2308.07805</a></li>
<li>Code URL: https://github.com/UVA-MLSys/DS7406</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07805]] Fairness and Privacy in Federated Learning and Their Implications in Healthcare(http://arxiv.org/abs/2308.07805)</code></li>
<li>Summary: <p>Currently, many contexts exist where distributed learning is difficult or
otherwise constrained by security and communication limitations. One common
domain where this is a consideration is in Healthcare where data is often
governed by data-use-ordinances like HIPAA. On the other hand, larger sample
sizes and shared data models are necessary to allow models to better generalize
on account of the potential for more variability and balancing underrepresented
classes. Federated learning is a type of distributed learning model that allows
data to be trained in a decentralized manner. This, in turn, addresses data
security, privacy, and vulnerability considerations as data itself is not
shared across a given learning network nodes. Three main challenges to
federated learning include node data is not independent and identically
distributed (iid), clients requiring high levels of communication overhead
between peers, and there is the heterogeneity of different clients within a
network with respect to dataset bias and size. As the field has grown, the
notion of fairness in federated learning has also been introduced through novel
implementations. Fairness approaches differ from the standard form of federated
learning and also have distinct challenges and considerations for the
healthcare domain. This paper endeavors to outline the typical lifecycle of
fair federated learning in research as well as provide an updated taxonomy to
account for the current state of fairness in implementations. Lastly, this
paper provides added insight into the implications and challenges of
implementing and supporting fairness in federated learning in the healthcare
domain.
</p></li>
</ul>

<h3>Title: Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening. (arXiv:2308.07707v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07707">http://arxiv.org/abs/2308.07707</a></li>
<li>Code URL: https://github.com/if-loops/selective-synaptic-dampening</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07707]] Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening(http://arxiv.org/abs/2308.07707)</code></li>
<li>Summary: <p>Machine unlearning, the ability for a machine learning model to forget, is
becoming increasingly important to comply with data privacy regulations, as
well as to remove harmful, manipulated, or outdated information. The key
challenge lies in forgetting specific information while protecting model
performance on the remaining data. While current state-of-the-art methods
perform well, they typically require some level of retraining over the retained
data, in order to protect or restore model performance. This adds computational
overhead and mandates that the training data remain available and accessible,
which may not be feasible. In contrast, other methods employ a retrain-free
paradigm, however, these approaches are prohibitively computationally expensive
and do not perform on par with their retrain-based counterparts. We present
Selective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-free
approach to machine unlearning which is fast, performant, and does not require
long-term storage of the training data. First, SSD uses the Fisher information
matrix of the training and forgetting data to select parameters that are
disproportionately important to the forget set. Second, SSD induces forgetting
by dampening these parameters proportional to their relative importance to the
forget set with respect to the wider training data. We evaluate our method
against several existing unlearning methods in a range of experiments using
ResNet18 and Vision Transformer. Results show that the performance of SSD is
competitive with retrain-based post hoc methods, demonstrating the viability of
retrain-free post hoc unlearning approaches.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Backpropagation Path Search On Adversarial Transferability. (arXiv:2308.07625v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07625">http://arxiv.org/abs/2308.07625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07625]] Backpropagation Path Search On Adversarial Transferability(http://arxiv.org/abs/2308.07625)</code></li>
<li>Summary: <p>Deep neural networks are vulnerable to adversarial examples, dictating the
imperativeness to test the model's robustness before deployment. Transfer-based
attackers craft adversarial examples against surrogate models and transfer them
to victim models deployed in the black-box situation. To enhance the
adversarial transferability, structure-based attackers adjust the
backpropagation path to avoid the attack from overfitting the surrogate model.
However, existing structure-based attackers fail to explore the convolution
module in CNNs and modify the backpropagation graph heuristically, leading to
limited effectiveness. In this paper, we propose backPropagation pAth Search
(PAS), solving the aforementioned two problems. We first propose SkipConv to
adjust the backpropagation path of convolution by structural
reparameterization. To overcome the drawback of heuristically designed
backpropagation paths, we further construct a DAG-based search space, utilize
one-step approximation for path evaluation and employ Bayesian Optimization to
search for the optimal path. We conduct comprehensive experiments in a wide
range of transfer settings, showing that PAS improves the attack success rate
by a huge margin for both normally trained and defense models.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: DISBELIEVE: Distance Between Client Models is Very Essential for Effective Local Model Poisoning Attacks. (arXiv:2308.07387v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07387">http://arxiv.org/abs/2308.07387</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07387]] DISBELIEVE: Distance Between Client Models is Very Essential for Effective Local Model Poisoning Attacks(http://arxiv.org/abs/2308.07387)</code></li>
<li>Summary: <p>Federated learning is a promising direction to tackle the privacy issues
related to sharing patients' sensitive data. Often, federated systems in the
medical image analysis domain assume that the participating local clients are
\textit{honest}. Several studies report mechanisms through which a set of
malicious clients can be introduced that can poison the federated setup,
hampering the performance of the global model. To overcome this, robust
aggregation methods have been proposed that defend against those attacks. We
observe that most of the state-of-the-art robust aggregation methods are
heavily dependent on the distance between the parameters or gradients of
malicious clients and benign clients, which makes them prone to local model
poisoning attacks when the parameters or gradients of malicious and benign
clients are close. Leveraging this, we introduce DISBELIEVE, a local model
poisoning attack that creates malicious parameters or gradients such that their
distance to benign clients' parameters or gradients is low respectively but at
the same time their adverse effect on the global model's performance is high.
Experiments on three publicly available medical image datasets demonstrate the
efficacy of the proposed DISBELIEVE attack as it significantly lowers the
performance of the state-of-the-art \textit{robust aggregation} methods for
medical image analysis. Furthermore, compared to state-of-the-art local model
poisoning attacks, DISBELIEVE attack is also effective on natural images where
we observe a severe drop in classification performance of the global model for
multi-class classification on benchmark dataset CIFAR-10.
</p></li>
</ul>

<h3>Title: 3DHacker: Spectrum-based Decision Boundary Generation for Hard-label 3D Point Cloud Attack. (arXiv:2308.07546v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07546">http://arxiv.org/abs/2308.07546</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07546]] 3DHacker: Spectrum-based Decision Boundary Generation for Hard-label 3D Point Cloud Attack(http://arxiv.org/abs/2308.07546)</code></li>
<li>Summary: <p>With the maturity of depth sensors, the vulnerability of 3D point cloud
models has received increasing attention in various applications such as
autonomous driving and robot navigation. Previous 3D adversarial attackers
either follow the white-box setting to iteratively update the coordinate
perturbations based on gradients, or utilize the output model logits to
estimate noisy gradients in the black-box setting. However, these attack
methods are hard to be deployed in real-world scenarios since realistic 3D
applications will not share any model details to users. Therefore, we explore a
more challenging yet practical 3D attack setting, \textit{i.e.}, attacking
point clouds with black-box hard labels, in which the attacker can only have
access to the prediction label of the input. To tackle this setting, we propose
a novel 3D attack method, termed \textbf{3D} \textbf{H}ard-label
att\textbf{acker} (\textbf{3DHacker}), based on the developed decision boundary
algorithm to generate adversarial samples solely with the knowledge of class
labels. Specifically, to construct the class-aware model decision boundary,
3DHacker first randomly fuses two point clouds of different classes in the
spectral domain to craft their intermediate sample with high imperceptibility,
then projects it onto the decision boundary via binary search. To restrict the
final perturbation size, 3DHacker further introduces an iterative optimization
strategy to move the intermediate sample along the decision boundary for
generating adversarial point clouds with smallest trivial perturbations.
Extensive evaluations show that, even in the challenging hard-label setting,
3DHacker still competitively outperforms existing 3D attacks regarding the
attack performance as well as adversary quality.
</p></li>
</ul>

<h3>Title: A Review of Adversarial Attacks in Computer Vision. (arXiv:2308.07673v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07673">http://arxiv.org/abs/2308.07673</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07673]] A Review of Adversarial Attacks in Computer Vision(http://arxiv.org/abs/2308.07673)</code></li>
<li>Summary: <p>Deep neural networks have been widely used in various downstream tasks,
especially those safety-critical scenario such as autonomous driving, but deep
networks are often threatened by adversarial samples. Such adversarial attacks
can be invisible to human eyes, but can lead to DNN misclassification, and
often exhibits transferability between deep learning and machine learning
models and real-world achievability. Adversarial attacks can be divided into
white-box attacks, for which the attacker knows the parameters and gradient of
the model, and black-box attacks, for the latter, the attacker can only obtain
the input and output of the model. In terms of the attacker's purpose, it can
be divided into targeted attacks and non-targeted attacks, which means that the
attacker wants the model to misclassify the original sample into the specified
class, which is more practical, while the non-targeted attack just needs to
make the model misclassify the sample. The black box setting is a scenario we
will encounter in practice.
</p></li>
</ul>

<h3>Title: White-Box Adversarial Attacks on Deep Learning-Based Radio Frequency Fingerprint Identification. (arXiv:2308.07433v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07433">http://arxiv.org/abs/2308.07433</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07433]] White-Box Adversarial Attacks on Deep Learning-Based Radio Frequency Fingerprint Identification(http://arxiv.org/abs/2308.07433)</code></li>
<li>Summary: <p>Radio frequency fingerprint identification (RFFI) is an emerging technique
for the lightweight authentication of wireless Internet of things (IoT)
devices. RFFI exploits unique hardware impairments as device identifiers, and
deep learning is widely deployed as the feature extractor and classifier for
RFFI. However, deep learning is vulnerable to adversarial attacks, where
adversarial examples are generated by adding perturbation to clean data for
causing the classifier to make wrong predictions. Deep learning-based RFFI has
been shown to be vulnerable to such attacks, however, there is currently no
exploration of effective adversarial attacks against a diversity of RFFI
classifiers. In this paper, we report on investigations into white-box attacks
(non-targeted and targeted) using two approaches, namely the fast gradient sign
method (FGSM) and projected gradient descent (PGD). A LoRa testbed was built
and real datasets were collected. These adversarial examples have been
experimentally demonstrated to be effective against convolutional neural
networks (CNNs), long short-term memory (LSTM) networks, and gated recurrent
units (GRU).
</p></li>
</ul>

<h3>Title: Enhancing the Antidote: Improved Pointwise Certifications against Poisoning Attacks. (arXiv:2308.07553v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07553">http://arxiv.org/abs/2308.07553</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07553]] Enhancing the Antidote: Improved Pointwise Certifications against Poisoning Attacks(http://arxiv.org/abs/2308.07553)</code></li>
<li>Summary: <p>Poisoning attacks can disproportionately influence model behaviour by making
small changes to the training corpus. While defences against specific poisoning
attacks do exist, they in general do not provide any guarantees, leaving them
potentially countered by novel attacks. In contrast, by examining worst-case
behaviours Certified Defences make it possible to provide guarantees of the
robustness of a sample against adversarial attacks modifying a finite number of
training samples, known as pointwise certification. We achieve this by
exploiting both Differential Privacy and the Sampled Gaussian Mechanism to
ensure the invariance of prediction for each testing instance against finite
numbers of poisoned examples. In doing so, our model provides guarantees of
adversarial robustness that are more than twice as large as those provided by
prior certifications.
</p></li>
</ul>

<h3>Title: Simple and Efficient Partial Graph Adversarial Attack: A New Perspective. (arXiv:2308.07834v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07834">http://arxiv.org/abs/2308.07834</a></li>
<li>Code URL: https://github.com/pasalab/pga</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07834]] Simple and Efficient Partial Graph Adversarial Attack: A New Perspective(http://arxiv.org/abs/2308.07834)</code></li>
<li>Summary: <p>As the study of graph neural networks becomes more intensive and
comprehensive, their robustness and security have received great research
interest. The existing global attack methods treat all nodes in the graph as
their attack targets. Although existing methods have achieved excellent
results, there is still considerable space for improvement. The key problem is
that the current approaches rigidly follow the definition of global attacks.
They ignore an important issue, i.e., different nodes have different robustness
and are not equally resilient to attacks. From a global attacker's view, we
should arrange the attack budget wisely, rather than wasting them on highly
robust nodes. To this end, we propose a totally new method named partial graph
attack (PGA), which selects the vulnerable nodes as attack targets. First, to
select the vulnerable items, we propose a hierarchical target selection policy,
which allows attackers to only focus on easy-to-attack nodes. Then, we propose
a cost-effective anchor-picking policy to pick the most promising anchors for
adding or removing edges, and a more aggressive iterative greedy-based attack
method to perform more efficient attacks. Extensive experimental results
demonstrate that PGA can achieve significant improvements in both attack effect
and attack efficiency compared to other existing graph global attack methods.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-wise Regression. (arXiv:2308.07477v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07477">http://arxiv.org/abs/2308.07477</a></li>
<li>Code URL: https://github.com/antonbaumann/mimo-unet</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07477]] Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-wise Regression(http://arxiv.org/abs/2308.07477)</code></li>
<li>Summary: <p>Uncertainty estimation in machine learning is paramount for enhancing the
reliability and interpretability of predictive models, especially in
high-stakes real-world scenarios. Despite the availability of numerous methods,
they often pose a trade-off between the quality of uncertainty estimation and
computational efficiency. Addressing this challenge, we present an adaptation
of the Multiple-Input Multiple-Output (MIMO) framework -- an approach
exploiting the overparameterization of deep neural networks -- for pixel-wise
regression tasks. Our MIMO variant expands the applicability of the approach
from simple image classification to broader computer vision domains. For that
purpose, we adapted the U-Net architecture to train multiple subnetworks within
a single model, harnessing the overparameterization in deep neural networks.
Additionally, we introduce a novel procedure for synchronizing subnetwork
performance within the MIMO framework. Our comprehensive evaluations of the
resulting MIMO U-Net on two orthogonal datasets demonstrate comparable accuracy
to existing models, superior calibration on in-distribution data, robust
out-of-distribution detection capabilities, and considerable improvements in
parameter size and inference time. Code available at
github.com/antonbaumann/MIMO-Unet
</p></li>
</ul>

<h3>Title: Improved Region Proposal Network for Enhanced Few-Shot Object Detection. (arXiv:2308.07535v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07535">http://arxiv.org/abs/2308.07535</a></li>
<li>Code URL: https://github.com/zshanggu/htrpn</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07535]] Improved Region Proposal Network for Enhanced Few-Shot Object Detection(http://arxiv.org/abs/2308.07535)</code></li>
<li>Summary: <p>Despite significant success of deep learning in object detection tasks, the
standard training of deep neural networks requires access to a substantial
quantity of annotated images across all classes. Data annotation is an arduous
and time-consuming endeavor, particularly when dealing with infrequent objects.
Few-shot object detection (FSOD) methods have emerged as a solution to the
limitations of classic object detection approaches based on deep learning. FSOD
methods demonstrate remarkable performance by achieving robust object detection
using a significantly smaller amount of training data. A challenge for FSOD is
that instances from novel classes that do not belong to the fixed set of
training classes appear in the background and the base model may pick them up
as potential objects. These objects behave similarly to label noise because
they are classified as one of the training dataset classes, leading to FSOD
performance degradation. We develop a semi-supervised algorithm to detect and
then utilize these unlabeled novel objects as positive samples during the FSOD
training stage to improve FSOD performance. Specifically, we develop a
hierarchical ternary classification region proposal network (HTRPN) to localize
the potential unlabeled novel objects and assign them new objectness labels to
distinguish these objects from the base training dataset classes. Our improved
hierarchical sampling strategy for the region proposal network (RPN) also
boosts the perception ability of the object detection model for large objects.
We test our approach and COCO and PASCAL VOC baselines that are commonly used
in FSOD literature. Our experimental results indicate that our method is
effective and outperforms the existing state-of-the-art (SOTA) FSOD methods.
Our implementation is provided as a supplement to support reproducibility of
the results.
</p></li>
</ul>

<h3>Title: Self-supervised Hypergraphs for Learning Multiple World Interpretations. (arXiv:2308.07615v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07615">http://arxiv.org/abs/2308.07615</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07615]] Self-supervised Hypergraphs for Learning Multiple World Interpretations(http://arxiv.org/abs/2308.07615)</code></li>
<li>Summary: <p>We present a method for learning multiple scene representations given a small
labeled set, by exploiting the relationships between such representations in
the form of a multi-task hypergraph. We also show how we can use the hypergraph
to improve a powerful pretrained VisTransformer model without any additional
labeled data. In our hypergraph, each node is an interpretation layer (e.g.,
depth or segmentation) of the scene. Within each hyperedge, one or several
input nodes predict the layer at the output node. Thus, each node could be an
input node in some hyperedges and an output node in others. In this way,
multiple paths can reach the same node, to form ensembles from which we obtain
robust pseudolabels, which allow self-supervised learning in the hypergraph. We
test different ensemble models and different types of hyperedges and show
superior performance to other multi-task graph models in the field. We also
introduce Dronescapes, a large video dataset captured with UAVs in different
complex real-world scenes, with multiple representations, suitable for
multi-task learning.
</p></li>
</ul>

<h3>Title: EQ-Net: Elastic Quantization Neural Networks. (arXiv:2308.07650v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07650">http://arxiv.org/abs/2308.07650</a></li>
<li>Code URL: https://github.com/xuke225/eq-net</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07650]] EQ-Net: Elastic Quantization Neural Networks(http://arxiv.org/abs/2308.07650)</code></li>
<li>Summary: <p>Current model quantization methods have shown their promising capability in
reducing storage space and computation complexity. However, due to the
diversity of quantization forms supported by different hardware, one limitation
of existing solutions is that usually require repeated optimization for
different scenarios. How to construct a model with flexible quantization forms
has been less studied. In this paper, we explore a one-shot network
quantization regime, named Elastic Quantization Neural Networks (EQ-Net), which
aims to train a robust weight-sharing quantization supernet. First of all, we
propose an elastic quantization space (including elastic bit-width,
granularity, and symmetry) to adapt to various mainstream quantitative forms.
Secondly, we propose the Weight Distribution Regularization Loss (WDR-Loss) and
Group Progressive Guidance Loss (GPG-Loss) to bridge the inconsistency of the
distribution for weights and output logits in the elastic quantization space
gap. Lastly, we incorporate genetic algorithms and the proposed Conditional
Quantization-Aware Accuracy Predictor (CQAP) as an estimator to quickly search
mixed-precision quantized neural networks in supernet. Extensive experiments
demonstrate that our EQ-Net is close to or even better than its static
counterparts as well as state-of-the-art robust bit-width methods. Code can be
available at
\href{https://github.com/xuke225/EQ-Net.git}{https://github.com/xuke225/EQ-Net}.
</p></li>
</ul>

<h3>Title: Gradient-Based Post-Training Quantization: Challenging the Status Quo. (arXiv:2308.07662v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07662">http://arxiv.org/abs/2308.07662</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07662]] Gradient-Based Post-Training Quantization: Challenging the Status Quo(http://arxiv.org/abs/2308.07662)</code></li>
<li>Summary: <p>Quantization has become a crucial step for the efficient deployment of deep
neural networks, where floating point operations are converted to simpler fixed
point operations. In its most naive form, it simply consists in a combination
of scaling and rounding transformations, leading to either a limited
compression rate or a significant accuracy drop. Recently, Gradient-based
post-training quantization (GPTQ) methods appears to be constitute a suitable
trade-off between such simple methods and more powerful, yet expensive
Quantization-Aware Training (QAT) approaches, particularly when attempting to
quantize LLMs, where scalability of the quantization process is of paramount
importance. GPTQ essentially consists in learning the rounding operation using
a small calibration set. In this work, we challenge common choices in GPTQ
methods. In particular, we show that the process is, to a certain extent,
robust to a number of variables (weight selection, feature augmentation, choice
of calibration set). More importantly, we derive a number of best practices for
designing more efficient and scalable GPTQ methods, regarding the problem
formulation (loss, degrees of freedom, use of non-uniform quantization schemes)
or optimization process (choice of variable and optimizer). Lastly, we propose
a novel importance-based mixed-precision technique. Those guidelines lead to
significant performance improvements on all the tested state-of-the-art GPTQ
methods and networks (e.g. +6.819 points on ViT for 4-bit quantization), paving
the way for the design of scalable, yet effective quantization methods.
</p></li>
</ul>

<h3>Title: Multi-scale Promoted Self-adjusting Correlation Learning for Facial Action Unit Detection. (arXiv:2308.07770v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07770">http://arxiv.org/abs/2308.07770</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07770]] Multi-scale Promoted Self-adjusting Correlation Learning for Facial Action Unit Detection(http://arxiv.org/abs/2308.07770)</code></li>
<li>Summary: <p>Facial Action Unit (AU) detection is a crucial task in affective computing
and social robotics as it helps to identify emotions expressed through facial
expressions. Anatomically, there are innumerable correlations between AUs,
which contain rich information and are vital for AU detection. Previous methods
used fixed AU correlations based on expert experience or statistical rules on
specific benchmarks, but it is challenging to comprehensively reflect complex
correlations between AUs via hand-crafted settings. There are alternative
methods that employ a fully connected graph to learn these dependencies
exhaustively. However, these approaches can result in a computational explosion
and high dependency with a large dataset. To address these challenges, this
paper proposes a novel self-adjusting AU-correlation learning (SACL) method
with less computation for AU detection. This method adaptively learns and
updates AU correlation graphs by efficiently leveraging the characteristics of
different levels of AU motion and emotion representation information extracted
in different stages of the network. Moreover, this paper explores the role of
multi-scale learning in correlation information extraction, and design a simple
yet effective multi-scale feature learning (MSFL) method to promote better
performance in AU detection. By integrating AU correlation information with
multi-scale features, the proposed method obtains a more robust feature
representation for the final AU detection. Extensive experiments show that the
proposed method outperforms the state-of-the-art methods on widely used AU
detection benchmark datasets, with only 28.7\% and 12.0\% of the parameters and
FLOPs of the best method, respectively. The code for this method is available
at \url{https://github.com/linuxsino/Self-adjusting-AU}.
</p></li>
</ul>

<h3>Title: SEDA: Self-Ensembling ViT with Defensive Distillation and Adversarial Training for robust Chest X-rays Classification. (arXiv:2308.07874v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07874">http://arxiv.org/abs/2308.07874</a></li>
<li>Code URL: https://github.com/razaimam45/seda</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07874]] SEDA: Self-Ensembling ViT with Defensive Distillation and Adversarial Training for robust Chest X-rays Classification(http://arxiv.org/abs/2308.07874)</code></li>
<li>Summary: <p>Deep Learning methods have recently seen increased adoption in medical
imaging applications. However, elevated vulnerabilities have been explored in
recent Deep Learning solutions, which can hinder future adoption. Particularly,
the vulnerability of Vision Transformer (ViT) to adversarial, privacy, and
confidentiality attacks raise serious concerns about their reliability in
medical settings. This work aims to enhance the robustness of self-ensembling
ViTs for the tuberculosis chest x-ray classification task. We propose
Self-Ensembling ViT with defensive Distillation and Adversarial training
(SEDA). SEDA utilizes efficient CNN blocks to learn spatial features with
various levels of abstraction from feature representations extracted from
intermediate ViT blocks, that are largely unaffected by adversarial
perturbations. Furthermore, SEDA leverages adversarial training in combination
with defensive distillation for improved robustness against adversaries.
Training using adversarial examples leads to better model generalizability and
improves its ability to handle perturbations. Distillation using soft
probabilities introduces uncertainty and variation into the output
probabilities, making it more difficult for adversarial and privacy attacks.
Extensive experiments performed with the proposed architecture and training
paradigm on publicly available Tuberculosis x-ray dataset shows SOTA efficacy
of SEDA compared to SEViT in terms of computational efficiency with 70x times
lighter framework and enhanced robustness of +9%.
</p></li>
</ul>

<h3>Title: Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models. (arXiv:2308.07847v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07847">http://arxiv.org/abs/2308.07847</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07847]] Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models(http://arxiv.org/abs/2308.07847)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have led to significant improvements in many
tasks across various domains, such as code interpretation, response generation,
and ambiguity handling. These LLMs, however, when upgrading, primarily
prioritize enhancing user experience while neglecting security, privacy, and
safety implications. Consequently, unintended vulnerabilities or biases can be
introduced. Previous studies have predominantly focused on specific versions of
the models and disregard the potential emergence of new attack vectors
targeting the updated versions. Through the lens of adversarial examples within
the in-context learning framework, this longitudinal study addresses this gap
by conducting a comprehensive assessment of the robustness of successive
versions of LLMs, vis-\`a-vis GPT-3.5. We conduct extensive experiments to
analyze and understand the impact of the robustness in two distinct learning
categories: zero-shot learning and few-shot learning. Our findings indicate
that, in comparison to earlier versions of LLMs, the updated versions do not
exhibit the anticipated level of robustness against adversarial attacks. In
addition, our study emphasizes the increased effectiveness of synergized
adversarial queries in most zero-shot learning and few-shot learning cases. We
hope that our study can lead to a more refined assessment of the robustness of
LLMs over time and provide valuable insights of these models for both
developers and users.
</p></li>
</ul>

<h3>Title: Domain-Adaptive Device Fingerprints for Network Access Authentication Through Multifractal Dimension Representation. (arXiv:2308.07925v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07925">http://arxiv.org/abs/2308.07925</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07925]] Domain-Adaptive Device Fingerprints for Network Access Authentication Through Multifractal Dimension Representation(http://arxiv.org/abs/2308.07925)</code></li>
<li>Summary: <p>RF data-driven device fingerprinting through the use of deep learning has
recently surfaced as a potential solution for automated network access
authentication. Traditional approaches are commonly susceptible to the domain
adaptation problem where a model trained on data from one domain performs badly
when tested on data from a different domain. Some examples of a domain change
include varying the device location or environment and varying the time or day
of data collection. In this work, we propose using multifractal analysis and
the variance fractal dimension trajectory (VFDT) as a data representation input
to the deep neural network to extract device fingerprints that are domain
generalizable. We analyze the effectiveness of the proposed VFDT representation
in detecting device-specific signatures from hardware-impaired IQ signals, and
evaluate its robustness in real-world settings, using an experimental testbed
of 30 WiFi-enabled Pycom devices under different locations and at different
scales. Our results show that the VFDT representation improves the scalability,
robustness and generalizability of the deep learning models significantly
compared to when using raw IQ data.
</p></li>
</ul>

<h3>Title: Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides. (arXiv:2308.07441v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07441">http://arxiv.org/abs/2308.07441</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07441]] Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides(http://arxiv.org/abs/2308.07441)</code></li>
<li>Summary: <p>Atmospheric nitrogen oxides (NOx) primarily from fuel combustion have
recognized acute and chronic health and environmental effects. Machine learning
(ML) methods have significantly enhanced our capacity to predict NOx
concentrations at ground-level with high spatiotemporal resolution but may
suffer from high estimation bias since they lack physical and chemical
knowledge about air pollution dynamics. Chemical transport models (CTMs)
leverage this knowledge; however, accurate predictions of ground-level
concentrations typically necessitate extensive post-calibration. Here, we
present a physics-informed deep learning framework that encodes
advection-diffusion mechanisms and fluid dynamics constraints to jointly
predict NO2 and NOx and reduce ML model bias by 21-42%. Our approach captures
fine-scale transport of NO2 and NOx, generates robust spatial extrapolation,
and provides explicit uncertainty estimation. The framework fuses
knowledge-driven physicochemical principles of CTMs with the predictive power
of ML for air quality exposure, health, and policy applications. Our approach
offers significant improvements over purely data-driven ML methods and has
unprecedented bias reduction in joint NO2 and NOx prediction.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Reducing Training Demands for 3D Gait Recognition with Deep Koopman Operator Constraints. (arXiv:2308.07468v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07468">http://arxiv.org/abs/2308.07468</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07468]] Reducing Training Demands for 3D Gait Recognition with Deep Koopman Operator Constraints(http://arxiv.org/abs/2308.07468)</code></li>
<li>Summary: <p>Deep learning research has made many biometric recognition solution viable,
but it requires vast training data to achieve real-world generalization. Unlike
other biometric traits, such as face and ear, gait samples cannot be easily
crawled from the web to form massive unconstrained datasets. As the human body
has been extensively studied for different digital applications, one can rely
on prior shape knowledge to overcome data scarcity. This work follows the
recent trend of fitting a 3D deformable body model into gait videos using deep
neural networks to obtain disentangled shape and pose representations for each
frame. To enforce temporal consistency in the network, we introduce a new
Linear Dynamical Systems (LDS) module and loss based on Koopman operator
theory, which provides an unsupervised motion regularization for the periodic
nature of gait, as well as a predictive capacity for extending gait sequences.
We compare LDS to the traditional adversarial training approach and use the USF
HumanID and CASIA-B datasets to show that LDS can obtain better accuracy with
less training data. Finally, we also show that our 3D modeling approach is much
better than other 3D gait approaches in overcoming viewpoint variation under
normal, bag-carrying and clothing change conditions.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: There Is a Digital Art History. (arXiv:2308.07464v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07464">http://arxiv.org/abs/2308.07464</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07464]] There Is a Digital Art History(http://arxiv.org/abs/2308.07464)</code></li>
<li>Summary: <p>In this paper, we revisit Johanna Drucker's question, "Is there a digital art
history?" -- posed exactly a decade ago -- in the light of the emergence of
large-scale, transformer-based vision models. While more traditional types of
neural networks have long been part of digital art history, and digital
humanities projects have recently begun to use transformer models, their
epistemic implications and methodological affordances have not yet been
systematically analyzed. We focus our analysis on two main aspects that,
together, seem to suggest a coming paradigm shift towards a "digital" art
history in Drucker's sense. On the one hand, the visual-cultural repertoire
newly encoded in large-scale vision models has an outsized effect on digital
art history. The inclusion of significant numbers of non-photographic images
allows for the extraction and automation of different forms of visual logics.
Large-scale vision models have "seen" large parts of the Western visual canon
mediated by Net visual culture, and they continuously solidify and concretize
this canon through their already widespread application in all aspects of
digital life. On the other hand, based on two technical case studies of
utilizing a contemporary large-scale visual model to investigate basic
questions from the fields of art history and urbanism, we suggest that such
systems require a new critical methodology that takes into account the
epistemic entanglement of a model and its applications. This new methodology
reads its corpora through a neural model's training data, and vice versa: the
visual ideologies of research datasets and training datasets become entangled.
</p></li>
</ul>

<h3>Title: FeatGeNN: Improving Model Performance for Tabular Data with Correlation-based Feature Extraction. (arXiv:2308.07527v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07527">http://arxiv.org/abs/2308.07527</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07527]] FeatGeNN: Improving Model Performance for Tabular Data with Correlation-based Feature Extraction(http://arxiv.org/abs/2308.07527)</code></li>
<li>Summary: <p>Automated Feature Engineering (AutoFE) has become an important task for any
machine learning project, as it can help improve model performance and gain
more information for statistical analysis. However, most current approaches for
AutoFE rely on manual feature creation or use methods that can generate a large
number of features, which can be computationally intensive and lead to
overfitting. To address these challenges, we propose a novel convolutional
method called FeatGeNN that extracts and creates new features using correlation
as a pooling function. Unlike traditional pooling functions like max-pooling,
correlation-based pooling considers the linear relationship between the
features in the data matrix, making it more suitable for tabular data. We
evaluate our method on various benchmark datasets and demonstrate that FeatGeNN
outperforms existing AutoFE approaches regarding model performance. Our results
suggest that correlation-based pooling can be a promising alternative to
max-pooling for AutoFE in tabular data applications.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: NeFL: Nested Federated Learning for Heterogeneous Clients. (arXiv:2308.07761v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07761">http://arxiv.org/abs/2308.07761</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07761]] NeFL: Nested Federated Learning for Heterogeneous Clients(http://arxiv.org/abs/2308.07761)</code></li>
<li>Summary: <p>Federated learning (FL) is a promising approach in distributed learning
keeping privacy. However, during the training pipeline of FL, slow or incapable
clients (i.e., stragglers) slow down the total training time and degrade
performance. System heterogeneity, including heterogeneous computing and
network bandwidth, has been addressed to mitigate the impact of stragglers.
Previous studies split models to tackle the issue, but with less
degree-of-freedom in terms of model architecture. We propose nested federated
learning (NeFL), a generalized framework that efficiently divides a model into
submodels using both depthwise and widthwise scaling. NeFL is implemented by
interpreting models as solving ordinary differential equations (ODEs) with
adaptive step sizes. To address the inconsistency that arises when training
multiple submodels with different architecture, we decouple a few parameters.
NeFL enables resource-constrained clients to effectively join the FL pipeline
and the model to be trained with a larger amount of data. Through a series of
experiments, we demonstrate that NeFL leads to significant gains, especially
for the worst-case submodel (e.g., 8.33 improvement on CIFAR-10). Furthermore,
we demonstrate NeFL aligns with recent studies in FL.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: AttMOT: Improving Multiple-Object Tracking by Introducing Auxiliary Pedestrian Attributes. (arXiv:2308.07537v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07537">http://arxiv.org/abs/2308.07537</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07537]] AttMOT: Improving Multiple-Object Tracking by Introducing Auxiliary Pedestrian Attributes(http://arxiv.org/abs/2308.07537)</code></li>
<li>Summary: <p>Multi-object tracking (MOT) is a fundamental problem in computer vision with
numerous applications, such as intelligent surveillance and automated driving.
Despite the significant progress made in MOT, pedestrian attributes, such as
gender, hairstyle, body shape, and clothing features, which contain rich and
high-level information, have been less explored. To address this gap, we
propose a simple, effective, and generic method to predict pedestrian
attributes to support general Re-ID embedding. We first introduce AttMOT, a
large, highly enriched synthetic dataset for pedestrian tracking, containing
over 80k frames and 6 million pedestrian IDs with different time, weather
conditions, and scenarios. To the best of our knowledge, AttMOT is the first
MOT dataset with semantic attributes. Subsequently, we explore different
approaches to fuse Re-ID embedding and pedestrian attributes, including
attention mechanisms, which we hope will stimulate the development of
attribute-assisted MOT. The proposed method AAM demonstrates its effectiveness
and generality on several representative pedestrian multi-object tracking
benchmarks, including MOT17 and MOT20, through experiments on the AttMOT
dataset. When applied to state-of-the-art trackers, AAM achieves consistent
improvements in MOTA, HOTA, AssA, IDs, and IDF1 scores. For instance, on MOT17,
the proposed method yields a +1.1 MOTA, +1.7 HOTA, and +1.8 IDF1 improvement
when used with FairMOT. To encourage further research on attribute-assisted
MOT, we will release the AttMOT dataset.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Emotion Embeddings $\unicode{x2014}$ Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets. (arXiv:2308.07871v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07871">http://arxiv.org/abs/2308.07871</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07871]] Emotion Embeddings $\unicode{x2014}$ Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets(http://arxiv.org/abs/2308.07871)</code></li>
<li>Summary: <p>Human emotion is expressed in many communication modalities and media formats
and so their computational study is equally diversified into natural language
processing, audio signal analysis, computer vision, etc. Similarly, the large
variety of representation formats used in previous research to describe
emotions (polarity scales, basic emotion categories, dimensional approaches,
appraisal theory, etc.) have led to an ever proliferating diversity of
datasets, predictive models, and software tools for emotion analysis. Because
of these two distinct types of heterogeneity, at the expressional and
representational level, there is a dire need to unify previous work on
increasingly diverging data and label types. This article presents such a
unifying computational model. We propose a training procedure that learns a
shared latent representation for emotions, so-called emotion embeddings,
independent of different natural languages, communication modalities, media or
representation label formats, and even disparate model architectures.
Experiments on a wide range of heterogeneous affective datasets indicate that
this approach yields the desired interoperability for the sake of reusability,
interpretability and flexibility, without penalizing prediction quality. Code
and data are archived under https://doi.org/10.5281/zenodo.7405327 .
</p></li>
</ul>

<h3>Title: Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07876">http://arxiv.org/abs/2308.07876</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07876]] Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT(http://arxiv.org/abs/2308.07876)</code></li>
<li>Summary: <p>Recent supervised models for event coding vastly outperform pattern-matching
methods. However, their reliance solely on new annotations disregards the vast
knowledge within expert databases, hindering their applicability to
fine-grained classification. To address these limitations, we explore zero-shot
approaches for political event ontology relation classification, by leveraging
knowledge from established annotation codebooks. Our study encompasses both
ChatGPT and a novel natural language inference (NLI) based approach named ZSP.
ZSP adopts a tree-query framework that deconstructs the task into context,
modality, and class disambiguation levels. This framework improves
interpretability, efficiency, and adaptability to schema changes. By conducting
extensive experiments on our newly curated datasets, we pinpoint the
instability issues within ChatGPT and highlight the superior performance of
ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained
Rootcode classification. ZSP demonstrates competitive performance compared to
supervised BERT models, positioning it as a valuable tool for event record
validation and ontology development. Our work underscores the potential of
leveraging transfer learning and existing expertise to enhance the efficiency
and scalability of research in the field.
</p></li>
</ul>

<h3>Title: A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection. (arXiv:2308.07774v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07774">http://arxiv.org/abs/2308.07774</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07774]] A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection(http://arxiv.org/abs/2308.07774)</code></li>
<li>Summary: <p>A key component of many graph neural networks (GNNs) is the pooling
operation, which seeks to reduce the size of a graph while preserving important
structural information. However, most existing graph pooling strategies rely on
an assignment matrix obtained by employing a GNN layer, which is characterized
by trainable parameters, often leading to significant computational complexity
and a lack of interpretability in the pooling process. In this paper, we
propose an unsupervised graph encoder-decoder model to detect abnormal nodes
from graphs by learning an anomaly scoring function to rank nodes based on
their degree of abnormality. In the encoding stage, we design a novel pooling
mechanism, named LCPool, which leverages locality-constrained linear coding for
feature encoding to find a cluster assignment matrix by solving a least-squares
optimization problem with a locality regularization term. By enforcing locality
constraints during the coding process, LCPool is designed to be free from
learnable parameters, capable of efficiently handling large graphs, and can
effectively generate a coarser graph representation while retaining the most
significant structural characteristics of the graph. In the decoding stage, we
propose an unpooling operation, called LCUnpool, to reconstruct both the
structure and nodal features of the original graph. We conduct empirical
evaluations of our method on six benchmark datasets using several evaluation
metrics, and the results demonstrate its superiority over state-of-the-art
anomaly detection approaches.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: BSED: Baseline Shapley-Based Explainable Detector. (arXiv:2308.07490v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07490">http://arxiv.org/abs/2308.07490</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07490]] BSED: Baseline Shapley-Based Explainable Detector(http://arxiv.org/abs/2308.07490)</code></li>
<li>Summary: <p>Explainable artificial intelligence (XAI) has witnessed significant advances
in the field of object recognition, with saliency maps being used to highlight
image features relevant to the predictions of learned models. Although these
advances have made AI-based technology more interpretable to humans, several
issues have come to light. Some approaches present explanations irrelevant to
predictions, and cannot guarantee the validity of XAI (axioms). In this study,
we propose the Baseline Shapley-based Explainable Detector (BSED), which
extends the Shapley value to object detection, thereby enhancing the validity
of interpretation. The Shapley value can attribute the prediction of a learned
model to a baseline feature while satisfying the explainability axioms. The
processing cost for the BSED is within the reasonable range, while the original
Shapley value is prohibitively computationally expensive. Furthermore, BSED is
a generalizable method that can be applied to various detectors in a
model-agnostic manner, and interpret various detection targets without
fine-grained parameter tuning. These strengths can enable the practical
applicability of XAI. We present quantitative and qualitative comparisons with
existing methods to demonstrate the superior performance of our method in terms
of explanation validity. Moreover, we present some applications, such as
correcting detection based on explanations from our method.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: U-Turn Diffusion. (arXiv:2308.07421v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07421">http://arxiv.org/abs/2308.07421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07421]] U-Turn Diffusion(http://arxiv.org/abs/2308.07421)</code></li>
<li>Summary: <p>We present a comprehensive examination of score-based diffusion models of AI
for generating synthetic images. These models hinge upon a dynamic auxiliary
time mechanism driven by stochastic differential equations, wherein the score
function is acquired from input images. Our investigation unveils a criterion
for evaluating efficiency of the score-based diffusion models: the power of the
generative process depends on the ability to de-construct fast correlations
during the reverse/de-noising phase. To improve the quality of the produced
synthetic images, we introduce an approach coined "U-Turn Diffusion". The
U-Turn Diffusion technique starts with the standard forward diffusion process,
albeit with a condensed duration compared to conventional settings.
Subsequently, we execute the standard reverse dynamics, initialized with the
concluding configuration from the forward process. This U-Turn Diffusion
procedure, combining forward, U-turn, and reverse processes, creates a
synthetic image approximating an independent and identically distributed
(i.i.d.) sample from the probability distribution implicitly described via
input samples. To analyze relevant time scales we employ various analytical
tools, including auto-correlation analysis, weighted norm of the score-function
analysis, and Kolmogorov-Smirnov Gaussianity test. The tools guide us to
establishing that the Kernel Intersection Distance, a metric comparing the
quality of synthetic samples with real data samples, is minimized at the
optimal U-turn time.
</p></li>
</ul>

<h3>Title: UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity. (arXiv:2308.07428v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07428">http://arxiv.org/abs/2308.07428</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07428]] UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity(http://arxiv.org/abs/2308.07428)</code></li>
<li>Summary: <p>Image reconstruction and captioning from brain activity evoked by visual
stimuli allow researchers to further understand the connection between the
human brain and the visual perception system. While deep generative models have
recently been employed in this field, reconstructing realistic captions and
images with both low-level details and high semantic fidelity is still a
challenging problem. In this work, we propose UniBrain: Unify Image
Reconstruction and Captioning All in One Diffusion Model from Human Brain
Activity. For the first time, we unify image reconstruction and captioning from
visual-evoked functional magnetic resonance imaging (fMRI) through a latent
diffusion model termed Versatile Diffusion. Specifically, we transform fMRI
voxels into text and image latent for low-level information and guide the
backward diffusion process through fMRI-based image and text conditions derived
from CLIP to generate realistic captions and images. UniBrain outperforms
current methods both qualitatively and quantitatively in terms of image
reconstruction and reports image captioning results for the first time on the
Natural Scenes Dataset (NSD) dataset. Moreover, the ablation experiments and
functional region-of-interest (ROI) analysis further exhibit the superiority of
UniBrain and provide comprehensive insight for visual-evoked brain decoding.
</p></li>
</ul>

<h3>Title: SGDiff: A Style Guided Diffusion Model for Fashion Synthesis. (arXiv:2308.07605v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07605">http://arxiv.org/abs/2308.07605</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07605]] SGDiff: A Style Guided Diffusion Model for Fashion Synthesis(http://arxiv.org/abs/2308.07605)</code></li>
<li>Summary: <p>This paper reports on the development of \textbf{a novel style guided
diffusion model (SGDiff)} which overcomes certain weaknesses inherent in
existing models for image synthesis. The proposed SGDiff combines image
modality with a pretrained text-to-image diffusion model to facilitate creative
fashion image synthesis. It addresses the limitations of text-to-image
diffusion models by incorporating supplementary style guidance, substantially
reducing training costs, and overcoming the difficulties of controlling
synthesized styles with text-only inputs. This paper also introduces a new
dataset -- SG-Fashion, specifically designed for fashion image synthesis
applications, offering high-resolution images and an extensive range of garment
categories. By means of comprehensive ablation study, we examine the
application of classifier-free guidance to a variety of conditions and validate
the effectiveness of the proposed model for generating fashion images of the
desired categories, product attributes, and styles. The contributions of this
paper include a novel classifier-free guidance method for multi-modal feature
fusion, a comprehensive dataset for fashion image synthesis application, a
thorough investigation on conditioned text-to-image synthesis, and valuable
insights for future research in the text-to-image synthesis domain. The code
and dataset are available at: \url{https://github.com/taited/SGDiff}.
</p></li>
</ul>

<h3>Title: Geometry of the Visual Cortex with Applications to Image Inpainting and Enhancement. (arXiv:2308.07652v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07652">http://arxiv.org/abs/2308.07652</a></li>
<li>Code URL: https://github.com/ballerin/v1diffusion</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07652]] Geometry of the Visual Cortex with Applications to Image Inpainting and Enhancement(http://arxiv.org/abs/2308.07652)</code></li>
<li>Summary: <p>Equipping the rototranslation group $SE(2)$ with a sub-Riemannian structure
inspired by the visual cortex V1, we propose algorithms for image inpainting
and enhancement based on hypoelliptic diffusion. We innovate on previous
implementations of the methods by Citti, Sarti and Boscain et al., by proposing
an alternative that prevents fading and capable of producing sharper results in
a procedure that we call WaxOn-WaxOff. We also exploit the sub-Riemannian
structure to define a completely new unsharp using $SE(2)$, analogous of the
classical unsharp filter for 2D image processing, with applications to image
enhancement. We demonstrate our method on blood vessels enhancement in retinal
scans.
</p></li>
</ul>

<h3>Title: Inversion-by-Inversion: Exemplar-based Sketch-to-Photo Synthesis via Stochastic Differential Equations without Training. (arXiv:2308.07665v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07665">http://arxiv.org/abs/2308.07665</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07665]] Inversion-by-Inversion: Exemplar-based Sketch-to-Photo Synthesis via Stochastic Differential Equations without Training(http://arxiv.org/abs/2308.07665)</code></li>
<li>Summary: <p>Exemplar-based sketch-to-photo synthesis allows users to generate
photo-realistic images based on sketches. Recently, diffusion-based methods
have achieved impressive performance on image generation tasks, enabling
highly-flexible control through text-driven generation or energy functions.
However, generating photo-realistic images with color and texture from sketch
images remains challenging for diffusion models. Sketches typically consist of
only a few strokes, with most regions left blank, making it difficult for
diffusion-based methods to produce photo-realistic images. In this work, we
propose a two-stage method named ``Inversion-by-Inversion" for exemplar-based
sketch-to-photo synthesis. This approach includes shape-enhancing inversion and
full-control inversion. During the shape-enhancing inversion process, an
uncolored photo is generated with the guidance of a shape-energy function. This
step is essential to ensure control over the shape of the generated photo. In
the full-control inversion process, we propose an appearance-energy function to
control the color and texture of the final generated photo.Importantly, our
Inversion-by-Inversion pipeline is training-free and can accept different types
of exemplars for color and texture control. We conducted extensive experiments
to evaluate our proposed method, and the results demonstrate its effectiveness.
</p></li>
</ul>

<h3>Title: DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models. (arXiv:2308.07687v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07687">http://arxiv.org/abs/2308.07687</a></li>
<li>Code URL: https://github.com/cure-lab/diffguard</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07687]] DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models(http://arxiv.org/abs/2308.07687)</code></li>
<li>Summary: <p>Given a classifier, the inherent property of semantic Out-of-Distribution
(OOD) samples is that their contents differ from all legal classes in terms of
semantics, namely semantic mismatch. There is a recent work that directly
applies it to OOD detection, which employs a conditional Generative Adversarial
Network (cGAN) to enlarge semantic mismatch in the image space. While achieving
remarkable OOD detection performance on small datasets, it is not applicable to
ImageNet-scale datasets due to the difficulty in training cGANs with both input
images and labels as conditions. As diffusion models are much easier to train
and amenable to various conditions compared to cGANs, in this work, we propose
to directly use pre-trained diffusion models for semantic mismatch-guided OOD
detection, named DiffGuard. Specifically, given an OOD input image and the
predicted label from the classifier, we try to enlarge the semantic difference
between the reconstructed OOD image under these conditions and the original
input image. We also present several test-time techniques to further strengthen
such differences. Experimental results show that DiffGuard is effective on both
Cifar-10 and hard cases of the large-scale ImageNet, and it can be easily
combined with existing OOD detection techniques to achieve state-of-the-art OOD
detection results.
</p></li>
</ul>

<h3>Title: Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model. (arXiv:2308.07749v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07749">http://arxiv.org/abs/2308.07749</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07749]] Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model(http://arxiv.org/abs/2308.07749)</code></li>
<li>Summary: <p>The rising demand for creating lifelike avatars in the digital realm has led
to an increased need for generating high-quality human videos guided by textual
descriptions and poses. We propose Dancing Avatar, designed to fabricate human
motion videos driven by poses and textual cues. Our approach employs a
pretrained T2I diffusion model to generate each video frame in an
autoregressive fashion. The crux of innovation lies in our adept utilization of
the T2I diffusion model for producing video frames successively while
preserving contextual relevance. We surmount the hurdles posed by maintaining
human character and clothing consistency across varying poses, along with
upholding the background's continuity amidst diverse human movements. To ensure
consistent human appearances across the entire video, we devise an intra-frame
alignment module. This module assimilates text-guided synthesized human
character knowledge into the pretrained T2I diffusion model, synergizing
insights from ChatGPT. For preserving background continuity, we put forth a
background alignment pipeline, amalgamating insights from segment anything and
image inpainting techniques. Furthermore, we propose an inter-frame alignment
module that draws inspiration from an auto-regressive pipeline to augment
temporal consistency between adjacent frames, where the preceding frame guides
the synthesis process of the current frame. Comparisons with state-of-the-art
methods demonstrate that Dancing Avatar exhibits the capacity to generate human
videos with markedly superior quality, both in terms of human and background
fidelity, as well as temporal coherence compared to existing state-of-the-art
approaches.
</p></li>
</ul>

<h3>Title: CCD-3DR: Consistent Conditioning in Diffusion for Single-Image 3D Reconstruction. (arXiv:2308.07837v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07837">http://arxiv.org/abs/2308.07837</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07837]] CCD-3DR: Consistent Conditioning in Diffusion for Single-Image 3D Reconstruction(http://arxiv.org/abs/2308.07837)</code></li>
<li>Summary: <p>In this paper, we present a novel shape reconstruction method leveraging
diffusion model to generate 3D sparse point cloud for the object captured in a
single RGB image. Recent methods typically leverage global embedding or local
projection-based features as the condition to guide the diffusion model.
However, such strategies fail to consistently align the denoised point cloud
with the given image, leading to unstable conditioning and inferior
performance. In this paper, we present CCD-3DR, which exploits a novel centered
diffusion probabilistic model for consistent local feature conditioning. We
constrain the noise and sampled point cloud from the diffusion model into a
subspace where the point cloud center remains unchanged during the forward
diffusion process and reverse process. The stable point cloud center further
serves as an anchor to align each point with its corresponding local
projection-based features. Extensive experiments on synthetic benchmark
ShapeNet-R2N2 demonstrate that CCD-3DR outperforms all competitors by a large
margin, with over 40% improvement. We also provide results on real-world
dataset Pix3D to thoroughly demonstrate the potential of CCD-3DR in real-world
applications. Codes will be released soon
</p></li>
</ul>

<h3>Title: StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models. (arXiv:2308.07863v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07863">http://arxiv.org/abs/2308.07863</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07863]] StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models(http://arxiv.org/abs/2308.07863)</code></li>
<li>Summary: <p>Content and style (C-S) disentanglement is a fundamental problem and critical
challenge of style transfer. Existing approaches based on explicit definitions
(e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretable
nor easy to control, resulting in entangled representations and less satisfying
results. In this paper, we propose a new C-S disentangled framework for style
transfer without using previous assumptions. The key insight is to explicitly
extract the content information and implicitly learn the complementary style
information, yielding interpretable and controllable C-S disentanglement and
style transfer. A simple yet effective CLIP-based style disentanglement loss
coordinated with a style reconstruction prior is introduced to disentangle C-S
in the CLIP image space. By further leveraging the powerful style removal and
generative ability of diffusion models, our framework achieves superior results
than state of the art and flexible C-S disentanglement and trade-off control.
Our work provides new insights into the C-S disentanglement in style transfer
and demonstrates the potential of diffusion models for learning
well-disentangled C-S characteristics.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: ICAFusion: Iterative Cross-Attention Guided Feature Fusion for Multispectral Object Detection. (arXiv:2308.07504v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07504">http://arxiv.org/abs/2308.07504</a></li>
<li>Code URL: https://github.com/chanchanchan97/icafusion</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07504]] ICAFusion: Iterative Cross-Attention Guided Feature Fusion for Multispectral Object Detection(http://arxiv.org/abs/2308.07504)</code></li>
<li>Summary: <p>Effective feature fusion of multispectral images plays a crucial role in
multi-spectral object detection. Previous studies have demonstrated the
effectiveness of feature fusion using convolutional neural networks, but these
methods are sensitive to image misalignment due to the inherent deffciency in
local-range feature interaction resulting in the performance degradation. To
address this issue, a novel feature fusion framework of dual cross-attention
transformers is proposed to model global feature interaction and capture
complementary information across modalities simultaneously. This framework
enhances the discriminability of object features through the query-guided
cross-attention mechanism, leading to improved performance. However, stacking
multiple transformer blocks for feature enhancement incurs a large number of
parameters and high spatial complexity. To handle this, inspired by the human
process of reviewing knowledge, an iterative interaction mechanism is proposed
to share parameters among block-wise multimodal transformers, reducing model
complexity and computation cost. The proposed method is general and effective
to be integrated into different detection frameworks and used with different
backbones. Experimental results on KAIST, FLIR, and VEDAI datasets show that
the proposed method achieves superior performance and faster inference, making
it suitable for various practical scenarios. Code will be available at
https://github.com/chanchanchan97/ICAFusion.
</p></li>
</ul>

<h3>Title: SST: A Simplified Swin Transformer-based Model for Taxi Destination Prediction based on Existing Trajectory. (arXiv:2308.07555v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07555">http://arxiv.org/abs/2308.07555</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07555]] SST: A Simplified Swin Transformer-based Model for Taxi Destination Prediction based on Existing Trajectory(http://arxiv.org/abs/2308.07555)</code></li>
<li>Summary: <p>Accurately predicting the destination of taxi trajectories can have various
benefits for intelligent location-based services. One potential method to
accomplish this prediction is by converting the taxi trajectory into a
two-dimensional grid and using computer vision techniques. While the Swin
Transformer is an innovative computer vision architecture with demonstrated
success in vision downstream tasks, it is not commonly used to solve real-world
trajectory problems. In this paper, we propose a simplified Swin Transformer
(SST) structure that does not use the shifted window idea in the traditional
Swin Transformer, as trajectory data is consecutive in nature. Our
comprehensive experiments, based on real trajectory data, demonstrate that SST
can achieve higher accuracy compared to state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Story Visualization by Online Text Augmentation with Context Memory. (arXiv:2308.07575v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07575">http://arxiv.org/abs/2308.07575</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07575]] Story Visualization by Online Text Augmentation with Context Memory(http://arxiv.org/abs/2308.07575)</code></li>
<li>Summary: <p>Story visualization (SV) is a challenging text-to-image generation task for
the difficulty of not only rendering visual details from the text descriptions
but also encoding a long-term context across multiple sentences. While prior
efforts mostly focus on generating a semantically relevant image for each
sentence, encoding a context spread across the given paragraph to generate
contextually convincing images (e.g., with a correct character or with a proper
background of the scene) remains a challenge. To this end, we propose a novel
memory architecture for the Bi-directional Transformers with an online text
augmentation that generates multiple pseudo-descriptions as supplementary
supervision during training, for better generalization to the language
variation at inference. In extensive experiments on the two popular SV
benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method
significantly outperforms the state of the arts in various evaluation metrics
including FID, character F1, frame accuracy, BLEU-2/3, and R-precision with
similar or less computational complexity.
</p></li>
</ul>

<h3>Title: Graph-Segmenter: Graph Transformer with Boundary-aware Attention for Semantic Segmentation. (arXiv:2308.07592v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07592">http://arxiv.org/abs/2308.07592</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07592]] Graph-Segmenter: Graph Transformer with Boundary-aware Attention for Semantic Segmentation(http://arxiv.org/abs/2308.07592)</code></li>
<li>Summary: <p>The transformer-based semantic segmentation approaches, which divide the
image into different regions by sliding windows and model the relation inside
each window, have achieved outstanding success. However, since the relation
modeling between windows was not the primary emphasis of previous work, it was
not fully utilized. To address this issue, we propose a Graph-Segmenter,
including a Graph Transformer and a Boundary-aware Attention module, which is
an effective network for simultaneously modeling the more profound relation
between windows in a global view and various pixels inside each window as a
local one, and for substantial low-cost boundary adjustment. Specifically, we
treat every window and pixel inside the window as nodes to construct graphs for
both views and devise the Graph Transformer. The introduced boundary-aware
attention module optimizes the edge information of the target objects by
modeling the relationship between the pixel on the object's edge. Extensive
experiments on three widely used semantic segmentation datasets (Cityscapes,
ADE-20k and PASCAL Context) demonstrate that our proposed network, a Graph
Transformer with Boundary-aware Attention, can achieve state-of-the-art
segmentation performance.
</p></li>
</ul>

<h3>Title: UniTR: A Unified and Efficient Multi-Modal Transformer for Bird's-Eye-View Representation. (arXiv:2308.07732v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07732">http://arxiv.org/abs/2308.07732</a></li>
<li>Code URL: https://github.com/haiyang-w/unitr</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07732]] UniTR: A Unified and Efficient Multi-Modal Transformer for Bird's-Eye-View Representation(http://arxiv.org/abs/2308.07732)</code></li>
<li>Summary: <p>Jointly processing information from multiple sensors is crucial to achieving
accurate and robust perception for reliable autonomous driving systems.
However, current 3D perception research follows a modality-specific paradigm,
leading to additional computation overheads and inefficient collaboration
between different sensor data. In this paper, we present an efficient
multi-modal backbone for outdoor 3D perception named UniTR, which processes a
variety of modalities with unified modeling and shared parameters. Unlike
previous works, UniTR introduces a modality-agnostic transformer encoder to
handle these view-discrepant sensor data for parallel modal-wise representation
learning and automatic cross-modal interaction without additional fusion steps.
More importantly, to make full use of these complementary sensor types, we
present a novel multi-modal integration strategy by both considering
semantic-abundant 2D perspective and geometry-aware 3D sparse neighborhood
relations. UniTR is also a fundamentally task-agnostic backbone that naturally
supports different 3D perception tasks. It sets a new state-of-the-art
performance on the nuScenes benchmark, achieving +1.1 NDS higher for 3D object
detection and +12.0 higher mIoU for BEV map segmentation with lower inference
latency. Code will be available at https://github.com/Haiyang-W/UniTR .
</p></li>
</ul>

<h3>Title: ChartDETR: A Multi-shape Detection Network for Visual Chart Recognition. (arXiv:2308.07743v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07743">http://arxiv.org/abs/2308.07743</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07743]] ChartDETR: A Multi-shape Detection Network for Visual Chart Recognition(http://arxiv.org/abs/2308.07743)</code></li>
<li>Summary: <p>Visual chart recognition systems are gaining increasing attention due to the
growing demand for automatically identifying table headers and values from
chart images. Current methods rely on keypoint detection to estimate data
element shapes in charts but suffer from grouping errors in post-processing. To
address this issue, we propose ChartDETR, a transformer-based multi-shape
detector that localizes keypoints at the corners of regular shapes to
reconstruct multiple data elements in a single chart image. Our method predicts
all data element shapes at once by introducing query groups in set prediction,
eliminating the need for further postprocessing. This property allows ChartDETR
to serve as a unified framework capable of representing various chart types
without altering the network architecture, effectively detecting data elements
of diverse shapes. We evaluated ChartDETR on three datasets, achieving
competitive results across all chart types without any additional enhancements.
For example, ChartDETR achieved an F1 score of 0.98 on Adobe Synthetic,
significantly outperforming the previous best model with a 0.71 F1 score.
Additionally, we obtained a new state-of-the-art result of 0.97 on
ExcelChart400k. The code will be made publicly available.
</p></li>
</ul>

<h3>Title: Dual-path TokenLearner for Remote Photoplethysmography-based Physiological Measurement with Facial Videos. (arXiv:2308.07771v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07771">http://arxiv.org/abs/2308.07771</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07771]] Dual-path TokenLearner for Remote Photoplethysmography-based Physiological Measurement with Facial Videos(http://arxiv.org/abs/2308.07771)</code></li>
<li>Summary: <p>Remote photoplethysmography (rPPG) based physiological measurement is an
emerging yet crucial vision task, whose challenge lies in exploring accurate
rPPG prediction from facial videos accompanied by noises of illumination
variations, facial occlusions, head movements, \etc, in a non-contact manner.
Existing mainstream CNN-based models make efforts to detect physiological
signals by capturing subtle color changes in facial regions of interest (ROI)
caused by heartbeats. However, such models are constrained by the limited local
spatial or temporal receptive fields in the neural units. Unlike them, a native
Transformer-based framework called Dual-path TokenLearner (Dual-TL) is proposed
in this paper, which utilizes the concept of learnable tokens to integrate both
spatial and temporal informative contexts from the global perspective of the
video. Specifically, the proposed Dual-TL uses a Spatial TokenLearner (S-TL) to
explore associations in different facial ROIs, which promises the rPPG
prediction far away from noisy ROI disturbances. Complementarily, a Temporal
TokenLearner (T-TL) is designed to infer the quasi-periodic pattern of
heartbeats, which eliminates temporal disturbances such as head movements. The
two TokenLearners, S-TL and T-TL, are executed in a dual-path mode. This
enables the model to reduce noise disturbances for final rPPG signal
prediction. Extensive experiments on four physiological measurement benchmark
datasets are conducted. The Dual-TL achieves state-of-the-art performances in
both intra- and cross-dataset testings, demonstrating its immense potential as
a basic backbone for rPPG measurement. The source code is available at
\href{https://github.com/VUT-HFUT/Dual-TL}{https://github.com/VUT-HFUT/Dual-TL}
</p></li>
</ul>

<h3>Title: Learning Image Deraining Transformer Network with Dynamic Dual Self-Attention. (arXiv:2308.07781v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07781">http://arxiv.org/abs/2308.07781</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07781]] Learning Image Deraining Transformer Network with Dynamic Dual Self-Attention(http://arxiv.org/abs/2308.07781)</code></li>
<li>Summary: <p>Recently, Transformer-based architecture has been introduced into single
image deraining task due to its advantage in modeling non-local information.
However, existing approaches tend to integrate global features based on a dense
self-attention strategy since it tend to uses all similarities of the tokens
between the queries and keys. In fact, this strategy leads to ignoring the most
relevant information and inducing blurry effect by the irrelevant
representations during the feature aggregation. To this end, this paper
proposes an effective image deraining Transformer with dynamic dual
self-attention (DDSA), which combines both dense and sparse attention
strategies to better facilitate clear image reconstruction. Specifically, we
only select the most useful similarity values based on top-k approximate
calculation to achieve sparse attention. In addition, we also develop a novel
spatial-enhanced feed-forward network (SEFN) to further obtain a more accurate
representation for achieving high-quality derained results. Extensive
experiments on benchmark datasets demonstrate the effectiveness of our proposed
method.
</p></li>
</ul>

<h3>Title: Memory-and-Anticipation Transformer for Online Action Understanding. (arXiv:2308.07893v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07893">http://arxiv.org/abs/2308.07893</a></li>
<li>Code URL: https://github.com/echo0125/memory-and-anticipation-transformer</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07893]] Memory-and-Anticipation Transformer for Online Action Understanding(http://arxiv.org/abs/2308.07893)</code></li>
<li>Summary: <p>Most existing forecasting systems are memory-based methods, which attempt to
mimic human forecasting ability by employing various memory mechanisms and have
progressed in temporal modeling for memory dependency. Nevertheless, an obvious
weakness of this paradigm is that it can only model limited historical
dependence and can not transcend the past. In this paper, we rethink the
temporal dependence of event evolution and propose a novel
memory-anticipation-based paradigm to model an entire temporal structure,
including the past, present, and future. Based on this idea, we present
Memory-and-Anticipation Transformer (MAT), a memory-anticipation-based
approach, to address the online action detection and anticipation tasks. In
addition, owing to the inherent superiority of MAT, it can process online
action detection and anticipation tasks in a unified manner. The proposed MAT
model is tested on four challenging benchmarks TVSeries, THUMOS'14, HDD, and
EPIC-Kitchens-100, for online action detection and anticipation tasks, and it
significantly outperforms all existing methods. Code is available at
https://github.com/Echo0125/Memory-and-Anticipation-Transformer.
</p></li>
</ul>

<h3>Title: Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans. (arXiv:2308.07462v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07462">http://arxiv.org/abs/2308.07462</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07462]] Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans(http://arxiv.org/abs/2308.07462)</code></li>
<li>Summary: <p>The introduction of Artificial Intelligence (AI) generative language models
such as GPT (Generative Pre-trained Transformer) and tools such as ChatGPT has
triggered a revolution that can transform how text is generated. This has many
implications, for example, as AI-generated text becomes a significant fraction
of the text in many disciplines, would this have an effect on the language
capabilities of readers and also on the training of newer AI tools? Would it
affect the evolution of languages? Focusing on one specific aspect of the
language: words; will the use of tools such as ChatGPT increase or reduce the
vocabulary used or the lexical richness (understood as the number of different
words used in a written or oral production) when writing a given text? This has
implications for words, as those not included in AI-generated content will tend
to be less and less popular and may eventually be lost. In this work, we
perform an initial comparison of the vocabulary and lexical richness of ChatGPT
and humans when performing the same tasks. In more detail, two datasets
containing the answers to different types of questions answered by ChatGPT and
humans are used, and the analysis shows that ChatGPT tends to use fewer
distinct words and lower lexical richness than humans. These results are very
preliminary and additional datasets and ChatGPT configurations have to be
evaluated to extract more general conclusions. Therefore, further research is
needed to understand how the use of ChatGPT and more broadly generative AI
tools will affect the vocabulary and lexical richness in different types of
text and languages.
</p></li>
</ul>

<h3>Title: VBD-MT Chinese-Vietnamese Translation Systems for VLSP 2022. (arXiv:2308.07601v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07601">http://arxiv.org/abs/2308.07601</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07601]] VBD-MT Chinese-Vietnamese Translation Systems for VLSP 2022(http://arxiv.org/abs/2308.07601)</code></li>
<li>Summary: <p>We present our systems participated in the VLSP 2022 machine translation
shared task. In the shared task this year, we participated in both translation
tasks, i.e., Chinese-Vietnamese and Vietnamese-Chinese translations. We build
our systems based on the neural-based Transformer model with the powerful
multilingual denoising pre-trained model mBART. The systems are enhanced by a
sampling method for backtranslation, which leverage large scale available
monolingual data. Additionally, several other methods are applied to improve
the translation quality including ensembling and postprocessing. We achieve
38.9 BLEU on ChineseVietnamese and 38.0 BLEU on VietnameseChinese on the public
test sets, which outperform several strong baselines.
</p></li>
</ul>

<h3>Title: Attention Is Not All You Need Anymore. (arXiv:2308.07661v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07661">http://arxiv.org/abs/2308.07661</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07661]] Attention Is Not All You Need Anymore(http://arxiv.org/abs/2308.07661)</code></li>
<li>Summary: <p>In recent years, the popular Transformer architecture has achieved great
success in many application areas, including natural language processing and
computer vision. Many existing works aim to reduce the computational and memory
complexity of the self-attention mechanism in the Transformer by trading off
performance. However, performance is key for the continuing success of the
Transformer. In this paper, a drop-in replacement for the self-attention
mechanism in the Transformer, called the Extractor, is proposed. Experimental
results show that replacing the self-attention mechanism with the Extractor
improves the performance of the Transformer. Furthermore, the proposed
Extractor has the potential to run faster than the self-attention since it has
a much shorter critical path of computation. Additionally, the sequence
prediction problem in the context of text generation is formulated using
variable-length discrete-time Markov chains, and the Transformer is reviewed
based on our understanding.
</p></li>
</ul>

<h3>Title: Enhancing Visually-Rich Document Understanding via Layout Structure Modeling. (arXiv:2308.07777v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07777">http://arxiv.org/abs/2308.07777</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07777]] Enhancing Visually-Rich Document Understanding via Layout Structure Modeling(http://arxiv.org/abs/2308.07777)</code></li>
<li>Summary: <p>In recent years, the use of multi-modal pre-trained Transformers has led to
significant advancements in visually-rich document understanding. However,
existing models have mainly focused on features such as text and vision while
neglecting the importance of layout relationship between text nodes. In this
paper, we propose GraphLayoutLM, a novel document understanding model that
leverages the modeling of layout structure graph to inject document layout
knowledge into the model. GraphLayoutLM utilizes a graph reordering algorithm
to adjust the text sequence based on the graph structure. Additionally, our
model uses a layout-aware multi-head self-attention layer to learn document
layout knowledge. The proposed model enables the understanding of the spatial
arrangement of text elements, improving document comprehension. We evaluate our
model on various benchmarks, including FUNSD, XFUND and CORD, and achieve
state-of-the-art results among these datasets. Our experimental results
demonstrate that our proposed method provides a significant improvement over
existing approaches and showcases the importance of incorporating layout
information into document understanding models. We also conduct an ablation
study to investigate the contribution of each component of our model. The
results show that both the graph reordering algorithm and the layout-aware
multi-head self-attention layer play a crucial role in achieving the best
performance.
</p></li>
</ul>

<h3>Title: Block-Wise Encryption for Reliable Vision Transformer models. (arXiv:2308.07612v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07612">http://arxiv.org/abs/2308.07612</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07612]] Block-Wise Encryption for Reliable Vision Transformer models(http://arxiv.org/abs/2308.07612)</code></li>
<li>Summary: <p>This article presents block-wise image encryption for the vision transformer
and its applications. Perceptual image encryption for deep learning enables us
not only to protect the visual information of plain images but to also embed
unique features controlled with a key into images and models. However, when
using conventional perceptual encryption methods, the performance of models is
degraded due to the influence of encryption. In this paper, we focus on
block-wise encryption for the vision transformer, and we introduce three
applications: privacy-preserving image classification, access control, and the
combined use of federated learning and encrypted images. Our scheme can have
the same performance as models without any encryption, and it does not require
any network modification. It also allows us to easily update the secret key. In
experiments, the effectiveness of the scheme is demonstrated in terms of
performance degradation and access control on the CIFAR10 and CIFAR-100
datasets.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Development and Evaluation of Three Chatbots for Postpartum Mood and Anxiety Disorders. (arXiv:2308.07407v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07407">http://arxiv.org/abs/2308.07407</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07407]] Development and Evaluation of Three Chatbots for Postpartum Mood and Anxiety Disorders(http://arxiv.org/abs/2308.07407)</code></li>
<li>Summary: <p>In collaboration with Postpartum Support International (PSI), a non-profit
organization dedicated to supporting caregivers with postpartum mood and
anxiety disorders, we developed three chatbots to provide context-specific
empathetic support to postpartum caregivers, leveraging both rule-based and
generative models. We present and evaluate the performance of our chatbots
using both machine-based metrics and human-based questionnaires. Overall, our
rule-based model achieves the best performance, with outputs that are close to
ground truth reference and contain the highest levels of empathy. Human users
prefer the rule-based chatbot over the generative chatbot for its
context-specific and human-like replies. Our generative chatbot also produced
empathetic responses and was described by human users as engaging. However,
limitations in the training dataset often result in confusing or nonsensical
responses. We conclude by discussing practical benefits of rule-based vs.
generative models for supporting individuals with mental health challenges. In
light of the recent surge of ChatGPT and BARD, we also discuss the
possibilities and pitfalls of large language models for digital mental
healthcare.
</p></li>
</ul>

<h3>Title: Informed Named Entity Recognition Decoding for Generative Language Models. (arXiv:2308.07791v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07791">http://arxiv.org/abs/2308.07791</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07791]] Informed Named Entity Recognition Decoding for Generative Language Models(http://arxiv.org/abs/2308.07791)</code></li>
<li>Summary: <p>Ever-larger language models with ever-increasing capabilities are by now
well-established text processing tools. Alas, information extraction tasks such
as named entity recognition are still largely unaffected by this progress as
they are primarily based on the previous generation of encoder-only transformer
models. Here, we propose a simple yet effective approach, Informed Named Entity
Recognition Decoding (iNERD), which treats named entity recognition as a
generative process. It leverages the language understanding capabilities of
recent generative models in a future-proof manner and employs an informed
decoding scheme incorporating the restricted nature of information extraction
into open-ended text generation, improving performance and eliminating any risk
of hallucinations. We coarse-tune our model on a merged named entity corpus to
strengthen its performance, evaluate five generative language models on eight
named entity recognition datasets, and achieve remarkable results, especially
in an environment with an unknown entity class set, demonstrating the
adaptability of the approach.
</p></li>
</ul>

<h3>Title: Generating Personas for Games with Multimodal Adversarial Imitation Learning. (arXiv:2308.07598v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07598">http://arxiv.org/abs/2308.07598</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07598]] Generating Personas for Games with Multimodal Adversarial Imitation Learning(http://arxiv.org/abs/2308.07598)</code></li>
<li>Summary: <p>Reinforcement learning has been widely successful in producing agents capable
of playing games at a human level. However, this requires complex reward
engineering, and the agent's resulting policy is often unpredictable. Going
beyond reinforcement learning is necessary to model a wide range of human
playstyles, which can be difficult to represent with a reward function. This
paper presents a novel imitation learning approach to generate multiple persona
policies for playtesting. Multimodal Generative Adversarial Imitation Learning
(MultiGAIL) uses an auxiliary input parameter to learn distinct personas using
a single-agent model. MultiGAIL is based on generative adversarial imitation
learning and uses multiple discriminators as reward models, inferring the
environment reward by comparing the agent and distinct expert policies. The
reward from each discriminator is weighted according to the auxiliary input.
Our experimental analysis demonstrates the effectiveness of our technique in
two environments with continuous and discrete action spaces.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Link-Context Learning for Multimodal LLMs. (arXiv:2308.07891v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07891">http://arxiv.org/abs/2308.07891</a></li>
<li>Code URL: https://github.com/isekai-portal/Link-Context-Learning</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07891]] Link-Context Learning for Multimodal LLMs(http://arxiv.org/abs/2308.07891)</code></li>
<li>Summary: <p>The ability to learn from context with novel concepts, and deliver
appropriate responses are essential in human conversations. Despite current
Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being
trained on mega-scale datasets, recognizing unseen images or understanding
novel concepts in a training-free manner remains a challenge. In-Context
Learning (ICL) explores training-free few-shot learning, where models are
encouraged to ``learn to learn" from limited tasks and generalize to unseen
tasks. In this work, we propose link-context learning (LCL), which emphasizes
"reasoning from cause and effect" to augment the learning capabilities of
MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal
relationship between the support set and the query set. By providing
demonstrations with causal links, LCL guides the model to discern not only the
analogy but also the underlying causal associations between data points, which
empowers MLLMs to recognize unseen images and understand novel concepts more
effectively. To facilitate the evaluation of this novel approach, we introduce
the ISEKAI dataset, comprising exclusively of unseen generated image-label
pairs designed for link-context learning. Extensive experiments show that our
LCL-MLLM exhibits strong link-context learning capabilities to novel concepts
over vanilla MLLMs. Code and data will be released at
https://github.com/isekai-portal/Link-Context-Learning.
</p></li>
</ul>

<h3>Title: Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification. (arXiv:2308.07921v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07921">http://arxiv.org/abs/2308.07921</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07921]] Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification(http://arxiv.org/abs/2308.07921)</code></li>
<li>Summary: <p>Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has
brought significant advancements in addressing math reasoning problems. In
particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter,
shows remarkable performance on challenging math datasets. In this paper, we
explore the effect of code on enhancing LLMs' reasoning capability by
introducing different constraints on the \textit{Code Usage Frequency} of GPT-4
Code Interpreter. We found that its success can be largely attributed to its
powerful skills in generating and executing code, evaluating the output of code
execution, and rectifying its solution when receiving unreasonable outputs.
Based on this insight, we propose a novel and effective prompting method,
explicit \uline{c}ode-based \uline{s}elf-\uline{v}erification~(CSV), to further
boost the mathematical reasoning potential of GPT-4 Code Interpreter. This
method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to
use code to self-verify its answers. In instances where the verification state
registers as ``False'', the model shall automatically amend its solution,
analogous to our approach of rectifying errors during a mathematics
examination. Furthermore, we recognize that the states of the verification
result indicate the confidence of a solution, which can improve the
effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we
achieve an impressive zero-shot accuracy on MATH dataset \textbf{(53.9\% $\to$
84.3\%)}.
</p></li>
</ul>

<h3>Title: Data Race Detection Using Large Language Models. (arXiv:2308.07505v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07505">http://arxiv.org/abs/2308.07505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07505]] Data Race Detection Using Large Language Models(http://arxiv.org/abs/2308.07505)</code></li>
<li>Summary: <p>Large language models (LLMs) are demonstrating significant promise as an
alternate strategy to facilitate analyses and optimizations of high-performance
computing programs, circumventing the need for resource-intensive manual tool
creation. In this paper, we explore a novel LLM-based data race detection
approach combining prompting engineering and fine-tuning techniques. We create
a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with
fine-grain labels showing the presence of data race pairs and their associated
variables, line numbers, and read/write information. DRB-ML is then used to
evaluate representative LLMs and fine-tune open-source ones. Our experiment
shows that LLMs can be a viable approach to data race detection. However, they
still cannot compete with traditional data race detection tools when we need
detailed information about variable pairs causing data races.
</p></li>
</ul>

<h3>Title: CALYPSO: LLMs as Dungeon Masters' Assistants. (arXiv:2308.07540v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07540">http://arxiv.org/abs/2308.07540</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07540]] CALYPSO: LLMs as Dungeon Masters' Assistants(http://arxiv.org/abs/2308.07540)</code></li>
<li>Summary: <p>The role of a Dungeon Master, or DM, in the game Dungeons &amp; Dragons is to
perform multiple tasks simultaneously. The DM must digest information about the
game setting and monsters, synthesize scenes to present to other players, and
respond to the players' interactions with the scene. Doing all of these tasks
while maintaining consistency within the narrative and story world is no small
feat of human cognition, making the task tiring and unapproachable to new
players. Large language models (LLMs) like GPT-3 and ChatGPT have shown
remarkable abilities to generate coherent natural language text. In this paper,
we conduct a formative evaluation with DMs to establish the use cases of LLMs
in D&amp;D and tabletop gaming generally. We introduce CALYPSO, a system of
LLM-powered interfaces that support DMs with information and inspiration
specific to their own scenario. CALYPSO distills game context into bite-sized
prose and helps brainstorm ideas without distracting the DM from the game. When
given access to CALYPSO, DMs reported that it generated high-fidelity text
suitable for direct presentation to players, and low-fidelity ideas that the DM
could develop further while maintaining their creative agency. We see CALYPSO
as exemplifying a paradigm of AI-augmented tools that provide synchronous
creative assistance within established game worlds, and tabletop gaming more
broadly.
</p></li>
</ul>

<h3>Title: A Survey on Model Compression for Large Language Models. (arXiv:2308.07633v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07633">http://arxiv.org/abs/2308.07633</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07633]] A Survey on Model Compression for Large Language Models(http://arxiv.org/abs/2308.07633)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have revolutionized natural language processing
tasks with remarkable success. However, their formidable size and computational
demands present significant challenges for practical deployment, especially in
resource-constrained environments. As these challenges become increasingly
pertinent, the field of model compression has emerged as a pivotal research
area to alleviate these limitations. This paper presents a comprehensive survey
that navigates the landscape of model compression techniques tailored
specifically for LLMs. Addressing the imperative need for efficient deployment,
we delve into various methodologies, encompassing quantization, pruning,
knowledge distillation, and more. Within each of these techniques, we highlight
recent advancements and innovative approaches that contribute to the evolving
landscape of LLM research. Furthermore, we explore benchmarking strategies and
evaluation metrics that are essential for assessing the effectiveness of
compressed LLMs. By providing insights into the latest developments and
practical implications, this survey serves as an invaluable resource for both
researchers and practitioners. As LLMs continue to evolve, this survey aims to
facilitate enhanced efficiency and real-world applicability, establishing a
foundation for future advancements in the field.
</p></li>
</ul>

<h3>Title: LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation. (arXiv:2308.07635v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07635">http://arxiv.org/abs/2308.07635</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07635]] LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation(http://arxiv.org/abs/2308.07635)</code></li>
<li>Summary: <p>There is an increasing interest in developing LLMs for medical diagnosis to
improve diagnosis efficiency. Despite their alluring technological potential,
there is no unified and comprehensive evaluation criterion, leading to the
inability to evaluate the quality and potential risks of medical LLMs, further
hindering the application of LLMs in medical treatment scenarios. Besides,
current evaluations heavily rely on labor-intensive interactions with LLMs to
obtain diagnostic dialogues and human evaluation on the quality of diagnosis
dialogue. To tackle the lack of unified and comprehensive evaluation criterion,
we first initially establish an evaluation criterion, termed LLM-specific
Mini-CEX to assess the diagnostic capabilities of LLMs effectively, based on
original Mini-CEX. To address the labor-intensive interaction problem, we
develop a patient simulator to engage in automatic conversations with LLMs, and
utilize ChatGPT for evaluating diagnosis dialogues automatically. Experimental
results show that the LLM-specific Mini-CEX is adequate and necessary to
evaluate medical diagnosis dialogue. Besides, ChatGPT can replace manual
evaluation on the metrics of humanistic qualities and provides reproducible and
automated comparisons between different LLMs.
</p></li>
</ul>

<h3>Title: Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation. (arXiv:2308.07645v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07645">http://arxiv.org/abs/2308.07645</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07645]] Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation(http://arxiv.org/abs/2308.07645)</code></li>
<li>Summary: <p>Large Language Models (LLMs) hold immense potential to generate synthetic
data of high quality and utility, which has numerous applications from
downstream model training to practical data utilisation. However, contemporary
models, despite their impressive capacities, consistently struggle to produce
both coherent and diverse data. To address the coherency issue, we introduce
contrastive expert guidance, where the difference between the logit
distributions of fine-tuned and base language models is emphasised to ensure
domain adherence. In order to ensure diversity, we utilise existing real and
synthetic examples as negative prompts to the model. We deem this dual-pronged
approach to logit reshaping as STEER: Semantic Text Enhancement via Embedding
Repositioning. STEER operates at inference-time and systematically guides the
LLMs to strike a balance between adherence to the data distribution (ensuring
semantic fidelity) and deviation from prior synthetic examples or existing real
datasets (ensuring diversity and authenticity). This delicate balancing act is
achieved by dynamically moving towards or away from chosen representations in
the latent space. STEER demonstrates improved performance over previous
synthetic data generation techniques, exhibiting better balance between data
diversity and coherency across three distinct tasks: hypothesis generation,
toxic and non-toxic comment generation, and commonsense reasoning task
generation. We demonstrate how STEER allows for fine-tuned control over the
diversity-coherency trade-off via its hyperparameters, highlighting its
versatility.
</p></li>
</ul>

<h3>Title: Better Zero-Shot Reasoning with Role-Play Prompting. (arXiv:2308.07702v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07702">http://arxiv.org/abs/2308.07702</a></li>
<li>Code URL: https://github.com/HLT-NLP/Role-Play-Prompting</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07702]] Better Zero-Shot Reasoning with Role-Play Prompting(http://arxiv.org/abs/2308.07702)</code></li>
<li>Summary: <p>Modern large language models (LLMs), such as ChatGPT, exhibit a remarkable
capacity for role-playing, enabling them to embody not only human characters
but also non-human entities like a Linux terminal. This versatility allows them
to simulate complex human-like interactions and behaviors within various
contexts, as well as to emulate specific objects or systems. While these
capabilities have enhanced user engagement and introduced novel modes of
interaction, the influence of role-playing on LLMs' reasoning abilities remains
underexplored. In this study, we introduce a strategically designed role-play
prompting methodology and assess its performance under the zero-shot setting
across twelve diverse reasoning benchmarks, encompassing arithmetic,
commonsense reasoning, symbolic reasoning, and more. Leveraging models such as
ChatGPT and Llama 2, our empirical results illustrate that role-play prompting
consistently surpasses the standard zero-shot approach across most datasets.
Notably, accuracy on AQuA rises from 53.5% to 63.8%, and on Last Letter from
23.8% to 84.2%. Beyond enhancing contextual understanding, we posit that
role-play prompting serves as an implicit Chain-of-Thought (CoT) trigger,
thereby improving the quality of reasoning. By comparing our approach with the
Zero-Shot-CoT technique, which prompts the model to "think step by step", we
further demonstrate that role-play prompting can generate a more effective CoT.
This highlights its potential to augment the reasoning capabilities of LLMs.
</p></li>
</ul>

<h3>Title: Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07758">http://arxiv.org/abs/2308.07758</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07758]] Backward Reasoning in Large Language Models for Verification(http://arxiv.org/abs/2308.07758)</code></li>
<li>Summary: <p>Chain-of-Though (CoT) prompting has shown promising performance in various
reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency}
proposes to sample a diverse set of reasoning chains which may lead to
different answers while the answer that receives the most votes is selected. In
this paper, we propose a novel method to use backward reasoning in verifying
candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM
to predict the masked token when a candidate answer is provided by \textit{a
simple template}, i.e., ``\textit{\textbf{If we know the answer of the above
question is \{a candidate answer\}, what is the value of unknown variable ${\bf
x}$?}}'' Intuitively, the LLM is expected to predict the masked token
successfully if the provided candidate answer is correct. We further propose
FOBAR to combine forward and backward reasoning for estimating the probability
of candidate answers. We conduct extensive experiments on six data sets and
three LLMs. Experimental results demonstrate that FOBAR achieves
state-of-the-art performance on various reasoning benchmarks.
</p></li>
</ul>

<h3>Title: Through the Lens of Core Competency: Survey on Evaluation of Large Language Models. (arXiv:2308.07902v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07902">http://arxiv.org/abs/2308.07902</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07902]] Through the Lens of Core Competency: Survey on Evaluation of Large Language Models(http://arxiv.org/abs/2308.07902)</code></li>
<li>Summary: <p>From pre-trained language model (PLM) to large language model (LLM), the
field of natural language processing (NLP) has witnessed steep performance
gains and wide practical uses. The evaluation of a research field guides its
direction of improvement. However, LLMs are extremely hard to thoroughly
evaluate for two reasons. First of all, traditional NLP tasks become inadequate
due to the excellent performance of LLM. Secondly, existing evaluation tasks
are difficult to keep up with the wide range of applications in real-world
scenarios. To tackle these problems, existing works proposed various benchmarks
to better evaluate LLMs. To clarify the numerous evaluation tasks in both
academia and industry, we investigate multiple papers concerning LLM
evaluations. We summarize 4 core competencies of LLM, including reasoning,
knowledge, reliability, and safety. For every competency, we introduce its
definition, corresponding benchmarks, and metrics. Under this competency
architecture, similar tasks are combined to reflect corresponding ability,
while new tasks can also be easily added into the system. Finally, we give our
suggestions on the future direction of LLM's evaluation.
</p></li>
</ul>

<h3>Title: Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping. (arXiv:2308.07641v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07641">http://arxiv.org/abs/2308.07641</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07641]] Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping(http://arxiv.org/abs/2308.07641)</code></li>
<li>Summary: <p>We present a simple yet novel parameterized form of linear mapping to
achieves remarkable network compression performance: a pseudo SVD called
Ternary SVD (TSVD).
</p>
<p>Unlike vanilla SVD, TSVD limits the $U$ and $V$ matrices in SVD to ternary
matrices form in $\{\pm 1, 0\}$. This means that instead of using the expensive
multiplication instructions, TSVD only requires addition instructions when
computing $U(\cdot)$ and $V(\cdot)$.
</p>
<p>We provide direct and training transition algorithms for TSVD like Post
Training Quantization and Quantization Aware Training respectively.
Additionally, we analyze the convergence of the direct transition algorithms in
theory.
</p>
<p>In experiments, we demonstrate that TSVD can achieve state-of-the-art network
compression performance in various types of networks and tasks, including
current baseline models such as ConvNext, Swim, BERT, and large language model
like OPT.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: A Unified Query-based Paradigm for Camouflaged Instance Segmentation. (arXiv:2308.07392v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07392">http://arxiv.org/abs/2308.07392</a></li>
<li>Code URL: https://github.com/dongbo811/uqformer</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07392]] A Unified Query-based Paradigm for Camouflaged Instance Segmentation(http://arxiv.org/abs/2308.07392)</code></li>
<li>Summary: <p>Due to the high similarity between camouflaged instances and the background,
the recently proposed camouflaged instance segmentation (CIS) faces challenges
in accurate localization and instance segmentation. To this end, inspired by
query-based transformers, we propose a unified query-based multi-task learning
framework for camouflaged instance segmentation, termed UQFormer, which builds
a set of mask queries and a set of boundary queries to learn a shared composed
query representation and efficiently integrates global camouflaged object
region and boundary cues, for simultaneous instance segmentation and instance
boundary detection in camouflaged scenarios. Specifically, we design a composed
query learning paradigm that learns a shared representation to capture object
region and boundary features by the cross-attention interaction of mask queries
and boundary queries in the designed multi-scale unified learning transformer
decoder. Then, we present a transformer-based multi-task learning framework for
simultaneous camouflaged instance segmentation and camouflaged instance
boundary detection based on the learned composed query representation, which
also forces the model to learn a strong instance-level query representation.
Notably, our model views the instance segmentation as a query-based direct set
prediction problem, without other post-processing such as non-maximal
suppression. Compared with 14 state-of-the-art approaches, our UQFormer
significantly improves the performance of camouflaged instance segmentation.
Our code will be available at https://github.com/dongbo811/UQFormer.
</p></li>
</ul>

<h3>Title: Confidence Contours: Uncertainty-Aware Annotation for Medical Semantic Segmentation. (arXiv:2308.07528v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07528">http://arxiv.org/abs/2308.07528</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07528]] Confidence Contours: Uncertainty-Aware Annotation for Medical Semantic Segmentation(http://arxiv.org/abs/2308.07528)</code></li>
<li>Summary: <p>Medical image segmentation modeling is a high-stakes task where understanding
of uncertainty is crucial for addressing visual ambiguity. Prior work has
developed segmentation models utilizing probabilistic or generative mechanisms
to infer uncertainty from labels where annotators draw a singular boundary.
However, as these annotations cannot represent an individual annotator's
uncertainty, models trained on them produce uncertainty maps that are difficult
to interpret. We propose a novel segmentation representation, Confidence
Contours, which uses high- and low-confidence ``contours'' to capture
uncertainty directly, and develop a novel annotation system for collecting
contours. We conduct an evaluation on the Lung Image Dataset Consortium (LIDC)
and a synthetic dataset. From an annotation study with 30 participants, results
show that Confidence Contours provide high representative capacity without
considerably higher annotator effort. We also find that general-purpose
segmentation models can learn Confidence Contours at the same performance level
as standard singular annotations. Finally, from interviews with 5 medical
experts, we find that Confidence Contour maps are more interpretable than
Bayesian maps due to representation of structural uncertainty.
</p></li>
</ul>

<h3>Title: Visual and Textual Prior Guided Mask Assemble for Few-Shot Segmentation and Beyond. (arXiv:2308.07539v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07539">http://arxiv.org/abs/2308.07539</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07539]] Visual and Textual Prior Guided Mask Assemble for Few-Shot Segmentation and Beyond(http://arxiv.org/abs/2308.07539)</code></li>
<li>Summary: <p>Few-shot segmentation (FSS) aims to segment the novel classes with a few
annotated images. Due to CLIP's advantages of aligning visual and textual
information, the integration of CLIP can enhance the generalization ability of
FSS model. However, even with the CLIP model, the existing CLIP-based FSS
methods are still subject to the biased prediction towards base classes, which
is caused by the class-specific feature level interactions. To solve this
issue, we propose a visual and textual Prior Guided Mask Assemble Network
(PGMA-Net). It employs a class-agnostic mask assembly process to alleviate the
bias, and formulates diverse tasks into a unified manner by assembling the
prior through affinity. Specifically, the class-relevant textual and visual
features are first transformed to class-agnostic prior in the form of
probability map. Then, a Prior-Guided Mask Assemble Module (PGMAM) including
multiple General Assemble Units (GAUs) is introduced. It considers diverse and
plug-and-play interactions, such as visual-textual, inter- and intra-image,
training-free, and high-order ones. Lastly, to ensure the class-agnostic
ability, a Hierarchical Decoder with Channel-Drop Mechanism (HDCDM) is proposed
to flexibly exploit the assembled masks and low-level features, without relying
on any class-specific information. It achieves new state-of-the-art results in
the FSS task, with mIoU of $77.6$ on $\text{PASCAL-}5^i$ and $59.4$ on
$\text{COCO-}20^i$ in 1-shot scenario. Beyond this, we show that without extra
re-training, the proposed PGMA-Net can solve bbox-level and cross-domain FSS,
co-segmentation, zero-shot segmentation (ZSS) tasks, leading an any-shot
segmentation framework.
</p></li>
</ul>

<h3>Title: Self-Prompting Large Vision Models for Few-Shot Medical Image Segmentation. (arXiv:2308.07624v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07624">http://arxiv.org/abs/2308.07624</a></li>
<li>Code URL: https://github.com/peteryyzhang/few-shot-self-prompt-sam</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07624]] Self-Prompting Large Vision Models for Few-Shot Medical Image Segmentation(http://arxiv.org/abs/2308.07624)</code></li>
<li>Summary: <p>Recent advancements in large foundation models have shown promising potential
in the medical industry due to their flexible prompting capability. One such
model, the Segment Anything Model (SAM), a prompt-driven segmentation model,
has shown remarkable performance improvements, surpassing state-of-the-art
approaches in medical image segmentation. However, existing methods primarily
rely on tuning strategies that require extensive data or prior prompts tailored
to the specific task, making it particularly challenging when only a limited
number of data samples are available. In this paper, we propose a novel
perspective on self-prompting in medical vision applications. Specifically, we
harness the embedding space of SAM to prompt itself through a simple yet
effective linear pixel-wise classifier. By preserving the encoding capabilities
of the large model, the contextual information from its decoder, and leveraging
its interactive promptability, we achieve competitive results on multiple
datasets (i.e. improvement of more than 15% compared to fine-tuning the mask
decoder using a few images).
</p></li>
</ul>

<h3>Title: Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07706">http://arxiv.org/abs/2308.07706</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07706]] Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models(http://arxiv.org/abs/2308.07706)</code></li>
<li>Summary: <p>Medical Image Segmentation is crucial in various clinical applications within
the medical domain. While state-of-the-art segmentation models have proven
effective, integrating textual guidance to enhance visual features for this
task remains an area with limited progress. Existing segmentation models that
utilize textual guidance are primarily trained on open-domain images, raising
concerns about their direct applicability in the medical domain without manual
intervention or fine-tuning.
</p>
<p>To address these challenges, we propose using multimodal vision-language
models for capturing semantic information from image descriptions and images,
enabling the segmentation of diverse medical images. This study comprehensively
evaluates existing vision language models across multiple datasets to assess
their transferability from the open domain to the medical field. Furthermore,
we introduce variations of image descriptions for previously unseen images in
the dataset, revealing notable variations in model performance based on the
generated prompts.
</p>
<p>Our findings highlight the distribution shift between the open-domain images
and the medical domain and show that the segmentation models trained on
open-domain images are not directly transferrable to the medical field. But
their performance can be increased by finetuning them in the medical datasets.
We report the zero-shot and finetuned segmentation performance of 4 Vision
Language Models (VLMs) on 11 medical datasets using 9 types of prompts derived
from 14 attributes.
</p></li>
</ul>

<h3>Title: Real-time Automatic M-mode Echocardiography Measurement with Panel Attention from Local-to-Global Pixels. (arXiv:2308.07717v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07717">http://arxiv.org/abs/2308.07717</a></li>
<li>Code URL: https://github.com/hanktseng131415go/ramem</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07717]] Real-time Automatic M-mode Echocardiography Measurement with Panel Attention from Local-to-Global Pixels(http://arxiv.org/abs/2308.07717)</code></li>
<li>Summary: <p>Motion mode (M-mode) recording is an essential part of echocardiography to
measure cardiac dimension and function. However, the current diagnosis cannot
build an automatic scheme, as there are three fundamental obstructs: Firstly,
there is no open dataset available to build the automation for ensuring
constant results and bridging M-mode echocardiography with real-time instance
segmentation (RIS); Secondly, the examination is involving the time-consuming
manual labelling upon M-mode echocardiograms; Thirdly, as objects in
echocardiograms occupy a significant portion of pixels, the limited receptive
field in existing backbones (e.g., ResNet) composed from multiple convolution
layers are inefficient to cover the period of a valve movement. Existing
non-local attentions (NL) compromise being unable real-time with a high
computation overhead or losing information from a simplified version of the
non-local block. Therefore, we proposed RAMEM, a real-time automatic M-mode
echocardiography measurement scheme, contributes three aspects to answer the
problems: 1) provide MEIS, a dataset of M-mode echocardiograms for instance
segmentation, to enable consistent results and support the development of an
automatic scheme; 2) propose panel attention, local-to-global efficient
attention by pixel-unshuffling, embedding with updated UPANets V2 in a RIS
scheme toward big object detection with global receptive field; 3) develop and
implement AMEM, an efficient algorithm of automatic M-mode echocardiography
measurement enabling fast and accurate automatic labelling among diagnosis. The
experimental results show that RAMEM surpasses existing RIS backbones (with
non-local attention) in PASCAL 2012 SBD and human performances in real-time
MEIS tested. The code of MEIS and dataset are available at
https://github.com/hanktseng131415go/RAME.
</p></li>
</ul>

<h3>Title: Context-Aware Pseudo-Label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation. (arXiv:2308.07731v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07731">http://arxiv.org/abs/2308.07731</a></li>
<li>Code URL: https://github.com/xmed-lab/cpr</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07731]] Context-Aware Pseudo-Label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation(http://arxiv.org/abs/2308.07731)</code></li>
<li>Summary: <p>In the domain adaptation problem, source data may be unavailable to the
target client side due to privacy or intellectual property issues. Source-free
unsupervised domain adaptation (SF-UDA) aims at adapting a model trained on the
source side to align the target distribution with only the source model and
unlabeled target data. The source model usually produces noisy and
context-inconsistent pseudo-labels on the target domain, i.e., neighbouring
regions that have a similar visual appearance are annotated with different
pseudo-labels. This observation motivates us to refine pseudo-labels with
context relations. Another observation is that features of the same class tend
to form a cluster despite the domain gap, which implies context relations can
be readily calculated from feature distances. To this end, we propose a
context-aware pseudo-label refinement method for SF-UDA. Specifically, a
context-similarity learning module is developed to learn context relations.
Next, pseudo-label revision is designed utilizing the learned context
relations. Further, we propose calibrating the revised pseudo-labels to
compensate for wrong revision caused by inaccurate context relations.
Additionally, we adopt a pixel-level and class-level denoising scheme to select
reliable pseudo-labels for domain adaptation. Experiments on cross-domain
fundus images indicate that our approach yields the state-of-the-art results.
Code is available at https://github.com/xmed-lab/CPR.
</p></li>
</ul>

<h3>Title: CASPNet++: Joint Multi-Agent Motion Prediction. (arXiv:2308.07751v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07751">http://arxiv.org/abs/2308.07751</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07751]] CASPNet++: Joint Multi-Agent Motion Prediction(http://arxiv.org/abs/2308.07751)</code></li>
<li>Summary: <p>The prediction of road users' future motion is a critical task in supporting
advanced driver-assistance systems (ADAS). It plays an even more crucial role
for autonomous driving (AD) in enabling the planning and execution of safe
driving maneuvers. Based on our previous work, Context-Aware Scene Prediction
Network (CASPNet), an improved system, CASPNet++, is proposed. In this work, we
focus on further enhancing the interaction modeling and scene understanding to
support the joint prediction of all road users in a scene using spatiotemporal
grids to model future occupancy. Moreover, an instance-based output head is
introduced to provide multi-modal trajectories for agents of interest. In
extensive quantitative and qualitative analysis, we demonstrate the scalability
of CASPNet++ in utilizing and fusing diverse environmental input sources such
as HD maps, Radar detection, and Lidar segmentation. Tested on the
urban-focused prediction dataset nuScenes, CASPNet++ reaches state-of-the-art
performance. The model has been deployed in a testing vehicle, running in
real-time with moderate computational resources.
</p></li>
</ul>

<h3>Title: Future Video Prediction from a Single Frame for Video Anomaly Detection. (arXiv:2308.07783v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07783">http://arxiv.org/abs/2308.07783</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07783]] Future Video Prediction from a Single Frame for Video Anomaly Detection(http://arxiv.org/abs/2308.07783)</code></li>
<li>Summary: <p>Video anomaly detection (VAD) is an important but challenging task in
computer vision. The main challenge rises due to the rarity of training samples
to model all anomaly cases. Hence, semi-supervised anomaly detection methods
have gotten more attention, since they focus on modeling normals and they
detect anomalies by measuring the deviations from normal patterns. Despite
impressive advances of these methods in modeling normal motion and appearance,
long-term motion modeling has not been effectively explored so far. Inspired by
the abilities of the future frame prediction proxy-task, we introduce the task
of future video prediction from a single frame, as a novel proxy-task for video
anomaly detection. This proxy-task alleviates the challenges of previous
methods in learning longer motion patterns. Moreover, we replace the initial
and future raw frames with their corresponding semantic segmentation map, which
not only makes the method aware of object class but also makes the prediction
task less complex for the model. Extensive experiments on the benchmark
datasets (ShanghaiTech, UCSD-Ped1, and UCSD-Ped2) show the effectiveness of the
method and the superiority of its performance compared to SOTA prediction-based
VAD methods.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
