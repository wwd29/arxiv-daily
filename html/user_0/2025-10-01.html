<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-01</h1>
<h3>Title: SOLD: SELFIES-based Objective-driven Latent Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Elbert Ho</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25198">https://arxiv.org/abs/2509.25198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25198">https://arxiv.org/pdf/2509.25198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25198]] SOLD: SELFIES-based Objective-driven Latent Diffusion(https://arxiv.org/abs/2509.25198)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Recently, machine learning has made a significant impact on de novo drug design. However, current approaches to creating novel molecules conditioned on a target protein typically rely on generating molecules directly in the 3D conformational space, which are often slow and overly complex. In this work, we propose SOLD (SELFIES-based Objective-driven Latent Diffusion), a novel latent diffusion model that generates molecules in a latent space derived from 1D SELFIES strings and conditioned on a target protein. In the process, we also train an innovative SELFIES transformer and propose a new way to balance losses when training multi-task machine learning this http URL model generates high-affinity molecules for the target protein in a simple and efficient way, while also leaving room for future improvements through the addition of more data.</li>
</ul>

<h3>Title: Spectral Logit Sculpting: Adaptive Low-Rank Logit Transformation for Controlled Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Jin Li, Zhebo Wang, Tianliang Lu, Mohan Li, Wenpeng Xing, Meng Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25204">https://arxiv.org/abs/2509.25204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25204">https://arxiv.org/pdf/2509.25204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25204]] Spectral Logit Sculpting: Adaptive Low-Rank Logit Transformation for Controlled Text Generation(https://arxiv.org/abs/2509.25204)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Entropy-based inference methods have gained traction for improving the reliability of Large Language Models (LLMs). However, many existing approaches, such as entropy minimization techniques, suffer from high computational overhead and fail to leverage historical token context effectively. To address these limitations, we propose Spectral Logit Sculpting (SLS), a lightweight inference-time optimization method that dynamically modulates token distributions using spectral and entropic properties of recent logits. SLS maintains a sliding buffer of top-K logits, performs on-the-fly Singular Value Decomposition (SVD) to identify dominant spectral directions, and adaptively rescales logits based on both entropy and logit gap statistics--only activating when uncertainty is high. Without updating any model parameters, SLS effectively sharpens the output distribution while preserving contextual consistency. Experimental results on multiple public benchmarks demonstrate that SLS consistently outperforms existing baseline methods, achieving superior accuracy in mathematical, coding, and scientific reasoning tasks.</li>
</ul>

<h3>Title: Polynomial Contrastive Learning for Privacy-Preserving Representation Learning on Graphs</h3>
<ul>
<li><strong>Authors: </strong>Daksh Pandey</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, math.RA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25205">https://arxiv.org/abs/2509.25205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25205">https://arxiv.org/pdf/2509.25205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25205]] Polynomial Contrastive Learning for Privacy-Preserving Representation Learning on Graphs(https://arxiv.org/abs/2509.25205)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has emerged as a powerful paradigm for learning representations on graph data without requiring manual labels. However, leading SSL methods like GRACE are fundamentally incompatible with privacy-preserving technologies such as Homomorphic Encryption (HE) due to their reliance on non-polynomial operations. This paper introduces Poly-GRACE, a novel framework for HE-compatible self-supervised learning on graphs. Our approach consists of a fully polynomial-friendly Graph Convolutional Network (GCN) encoder and a novel, polynomial-based contrastive loss function. Through experiments on three benchmark datasets -- Cora, CiteSeer, and PubMed -- we demonstrate that Poly-GRACE not only enables private pre-training but also achieves performance that is highly competitive with, and in the case of CiteSeer, superior to the standard non-private baseline. Our work represents a significant step towards practical and high-performance privacy-preserving graph representation learning.</li>
</ul>

<h3>Title: Hyperbolic Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yanke Wang, Kyriakos Flouris</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25206">https://arxiv.org/abs/2509.25206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25206">https://arxiv.org/pdf/2509.25206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25206]] Hyperbolic Optimization(https://arxiv.org/abs/2509.25206)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This work explores optimization methods on hyperbolic manifolds. Building on Riemannian optimization principles, we extend the Hyperbolic Stochastic Gradient Descent (a specialization of Riemannian SGD) to a Hyperbolic Adam optimizer. While these methods are particularly relevant for learning on the Poincaré ball, they may also provide benefits in Euclidean and other non-Euclidean settings, as the chosen optimization encourages the learning of Poincaré embeddings. This representation, in turn, accelerates convergence in the early stages of training, when parameters are far from the optimum. As a case study, we train diffusion models using the hyperbolic optimization methods with hyperbolic time-discretization of the Langevin dynamics, and show that they achieve faster convergence on certain datasets without sacrificing generative quality.</li>
</ul>

<h3>Title: Multi-level Diagnosis and Evaluation for Robust Tabular Feature Engineering with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yebin Lim, Susik Yoon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25207">https://arxiv.org/abs/2509.25207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25207">https://arxiv.org/pdf/2509.25207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25207]] Multi-level Diagnosis and Evaluation for Robust Tabular Feature Engineering with Large Language Models(https://arxiv.org/abs/2509.25207)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have shown promise in feature engineering for tabular data, but concerns about their reliability persist, especially due to variability in generated outputs. We introduce a multi-level diagnosis and evaluation framework to assess the robustness of LLMs in feature engineering across diverse domains, focusing on the three main factors: key variables, relationships, and decision boundary values for predicting target classes. We demonstrate that the robustness of LLMs varies significantly over different datasets, and that high-quality LLM-generated features can improve few-shot prediction performance by up to 10.52%. This work opens a new direction for assessing and enhancing the reliability of LLM-driven feature engineering in various domains.</li>
</ul>

<h3>Title: LEMs: A Primer On Large Execution Models</h3>
<ul>
<li><strong>Authors: </strong>Remi Genet, Hugo Inzirillo</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.CP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25211">https://arxiv.org/abs/2509.25211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25211">https://arxiv.org/pdf/2509.25211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25211]] LEMs: A Primer On Large Execution Models(https://arxiv.org/abs/2509.25211)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>This paper introduces Large Execution Models (LEMs), a novel deep learning framework that extends transformer-based architectures to address complex execution problems with flexible time boundaries and multiple execution constraints. Building upon recent advances in neural VWAP execution strategies, LEMs generalize the approach from fixed-duration orders to scenarios where execution duration is bounded between minimum and maximum time horizons, similar to share buyback contract structures. The proposed architecture decouples market information processing from execution allocation decisions: a common feature extraction pipeline using Temporal Kolmogorov-Arnold Networks (TKANs), Variable Selection Networks (VSNs), and multi-head attention mechanisms processes market data to create informational context, while independent allocation networks handle the specific execution logic for different scenarios (fixed quantity vs. fixed notional, buy vs. sell orders). This architectural separation enables a unified model to handle diverse execution objectives while leveraging shared market understanding across scenarios. Through comprehensive empirical evaluation on intraday cryptocurrency markets and multi-day equity trading using DOW Jones constituents, we demonstrate that LEMs achieve superior execution performance compared to traditional benchmarks by dynamically optimizing execution paths within flexible time constraints. The unified model architecture enables deployment across different execution scenarios (buy/sell orders, varying duration boundaries, volume/notional targets) through a single framework, providing significant operational advantages over asset-specific approaches.</li>
</ul>

<h3>Title: On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rongguang Ye, Ming Tang, Edith C. H. Ngai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25214">https://arxiv.org/abs/2509.25214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25214">https://arxiv.org/pdf/2509.25214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25214]] On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs(https://arxiv.org/abs/2509.25214)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>As increasingly large pre-trained models are released, deploying them on edge devices for privacy-preserving applications requires effective compression. Recent works combine quantization with the fine-tuning of high-precision LoRA adapters, which can substantially reduce model size while mitigating the accuracy loss from quantization. However, edge devices have inherently heterogeneous capabilities, while performing configuration-wise fine-tuning for every quantization setting is computationally prohibitive. In this paper, we propose CoA-LoRA, a method that dynamically adjusts the LoRA adapter to arbitrary quantization configurations (i.e., the per-layer bit-width choices of a pre-trained model) without requiring repeated fine-tuning. This is accomplished via a configuration-aware model that maps each configuration to its low-rank adjustments. The effectiveness of this model critically depends on the training configuration set, a collection of configurations chosen to cover different total bit-width budgets. However, constructing a high-quality configuration set is non-trivial. We therefore design a Pareto-based configuration search that iteratively optimizes the training configuration set, yielding more precise low-rank adjustments. Our experiments demonstrate that, unlike the state-of-the-art methods that require fine-tuning a separate LoRA adapter for each configuration, CoA-LoRA incurs no additional time cost while achieving comparable or even superior performance to those methods.</li>
</ul>

<h3>Title: Cyclic Ablation: Testing Concept Localization against Functional Regeneration in AI</h3>
<ul>
<li><strong>Authors: </strong>Eduard Kapelko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25220">https://arxiv.org/abs/2509.25220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25220">https://arxiv.org/pdf/2509.25220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25220]] Cyclic Ablation: Testing Concept Localization against Functional Regeneration in AI(https://arxiv.org/abs/2509.25220)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Safety and controllability are critical for large language models. A central question is whether undesirable behaviors like deception are localized functions that can be removed, or if they are deeply intertwined with a model's core cognitive abilities. We introduce "cyclic ablation," an iterative method to test this. By combining sparse autoencoders, targeted ablation, and adversarial training on DistilGPT-2, we attempted to eliminate the concept of deception. We found that, contrary to the localization hypothesis, deception was highly resilient. The model consistently recovered its deceptive behavior after each ablation cycle via adversarial training, a process we term functional regeneration. Crucially, every attempt at this "neurosurgery" caused a gradual but measurable decay in general linguistic performance, reflected by a consistent rise in perplexity. These findings are consistent with the view that complex concepts are distributed and entangled, underscoring the limitations of direct model editing through mechanistic interpretability.</li>
</ul>

<h3>Title: Enhancing Linear Attention with Residual Learning</h3>
<ul>
<li><strong>Authors: </strong>Xunhao Lai, Jialiang Kang, Jianqiao Lu, Tong Lin, Pengyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25223">https://arxiv.org/abs/2509.25223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25223">https://arxiv.org/pdf/2509.25223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25223]] Enhancing Linear Attention with Residual Learning(https://arxiv.org/abs/2509.25223)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Linear attention offers a linear-time alternative to self-attention but often struggles to capture long-range patterns. We revisit linear attention through a prediction-correction lens and show that prevalent variants can be written as a combination of a historical prediction and a single-token correction, which creates an expressivity bottleneck. To address this bottleneck, we introduce Residual Linear Attention (RLA), a framework that equips linear attention with an explicit residual-fitting mechanism. RLA maintains an auxiliary recurrent state that learns to accumulate residual errors over time and correct the base prediction. We further instantiate a delta-rule version, Residual Delta Net (RDN), incorporating adaptive gating and residual clipping for enhanced correction control and stability. Our implementation leverages highly optimized linear attention kernels and preserves linear time and memory. Across language modeling and recall-intensive evaluations, RLA and RDN consistently outperform their respective baselines and other modern linear-attention methods, narrowing the gap to standard Transformers while retaining linear scaling.</li>
</ul>

<h3>Title: AMLA: MUL by ADD in FlashAttention Rescaling</h3>
<ul>
<li><strong>Authors: </strong>Qichen Liao, Chengqiu Hu, Fangzheng Miao, Bao Li, Yiyang Liu, Junlong Lyu, Lirui Jiang, Jun Wang, Lingchao Zheng, Jun Li, Yuwei Fan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25224">https://arxiv.org/abs/2509.25224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25224">https://arxiv.org/pdf/2509.25224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25224]] AMLA: MUL by ADD in FlashAttention Rescaling(https://arxiv.org/abs/2509.25224)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-head Latent Attention (MLA) significantly reduces KVCache memory usage in Large Language Models while introducing substantial computational overhead and intermediate variable expansion. This poses challenges for efficient hardware implementation -- especially during the decode phase. This paper introduces Ascend MLA (AMLA), a high-performance kernel specifically optimized for Huawei's Ascend NPUs. AMLA is built on two core innovations: (1) A novel FlashAttention-based algorithm that replaces floating-point multiplications with integer additions for output block rescaling, leveraging binary correspondence between FP32 and INT32 representations; (2) A Preload Pipeline strategy with hierarchical tiling that maximizes FLOPS utilization: the Preload Pipeline achieves Cube-bound performance, while hierarchical tiling overlaps data movement and computation within the Cube core. Experiments show that on Ascend 910 NPUs (integrated in CloudMatrix384), AMLA achieves up to 614 TFLOPS, reaching 86.8% of the theoretical maximum FLOPS, outperforming the state-of-the-art open-source FlashMLA implementation, whose FLOPS utilization is up to 66.7% on NVIDIA H800 SXM5. The AMLA kernel has been integrated into Huawei's CANN and will be released soon.</li>
</ul>

<h3>Title: MSCoD: An Enhanced Bayesian Updating Framework with Multi-Scale Information Bottleneck and Cooperative Attention for Structure-Based Drug Design</h3>
<ul>
<li><strong>Authors: </strong>Long Xu, Yongcai Chen, Fengshuo Liu, Yuzhong Peng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25225">https://arxiv.org/abs/2509.25225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25225">https://arxiv.org/pdf/2509.25225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25225]] MSCoD: An Enhanced Bayesian Updating Framework with Multi-Scale Information Bottleneck and Cooperative Attention for Structure-Based Drug Design(https://arxiv.org/abs/2509.25225)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>Structure-Based Drug Design (SBDD) is a powerful strategy in computational drug discovery, utilizing three-dimensional protein structures to guide the design of molecules with improved binding affinity. However, capturing complex protein-ligand interactions across multiple scales remains challenging, as current methods often overlook the hierarchical organization and intrinsic asymmetry of these interactions. To address these limitations, we propose MSCoD, a novel Bayesian updating-based generative framework for structure-based drug design. In our MSCoD, Multi-Scale Information Bottleneck (MSIB) was developed, which enables semantic compression at multiple abstraction levels for efficient hierarchical feature extraction. Furthermore, a multi-head cooperative attention (MHCA) mechanism was developed, which employs asymmetric protein-to-ligand attention to capture diverse interaction types while addressing the dimensionality disparity between proteins and ligands. Empirical studies showed that MSCoD outperforms state-of-the-art methods on the benchmark dataset. Case studies on challenging targets such as KRAS G12D further demonstrate its applicability in real-world scenarios. The code and data underlying this article are freely available at this https URL.</li>
</ul>

<h3>Title: Integrated Forecasting of Marine Renewable Power: An Adaptively Bayesian-Optimized MVMD-LSTM Framework for Wind-Solar-Wave Energy</h3>
<ul>
<li><strong>Authors: </strong>Baoyi Xie, Shuiling Shi, Wenqi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25226">https://arxiv.org/abs/2509.25226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25226">https://arxiv.org/pdf/2509.25226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25226]] Integrated Forecasting of Marine Renewable Power: An Adaptively Bayesian-Optimized MVMD-LSTM Framework for Wind-Solar-Wave Energy(https://arxiv.org/abs/2509.25226)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust</a></li>
<li><strong>Abstract: </strong>Integrated wind-solar-wave marine energy systems hold broad promise for supplying clean electricity in offshore and coastal regions. By leveraging the spatiotemporal complementarity of multiple resources, such systems can effectively mitigate the intermittency and volatility of single-source outputs, thereby substantially improving overall power-generation efficiency and resource utilization. Accurate ultra-short-term forecasting is crucial for ensuring secure operation and optimizing proactive dispatch. However, most existing forecasting methods construct separate models for each energy source, insufficiently account for the complex couplings among multiple energies, struggle to capture the system's nonlinear and nonstationary dynamics, and typically depend on extensive manual parameter tuning-limitations that constrain both predictive performance and practicality. We address this issue using a Bayesian-optimized Multivariate Variational Mode Decomposition-Long Short-Term Memory (MVMD-LSTM) framework. The framework first applies MVMD to jointly decompose wind, solar and wave power series so as to preserve cross-source couplings; it uses Bayesian optimization to automatically search the number of modes and the penalty parameter in the MVMD process to obtain intrinsic mode functions (IMFs); finally, an LSTM models the resulting IMFs to achieve ultra-short-term power forecasting for the integrated system. Experiments based on field measurements from an offshore integrated energy platform in China show that the proposed framework significantly outperforms benchmark models in terms of MAPE, RMSE and MAE. The results demonstrate superior predictive accuracy, robustness, and degree of automation.</li>
</ul>

<h3>Title: Simple, Fast and Efficient Injective Manifold Density Estimation with Random Projections</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Ayaz Amin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25228">https://arxiv.org/abs/2509.25228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25228">https://arxiv.org/pdf/2509.25228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25228]] Simple, Fast and Efficient Injective Manifold Density Estimation with Random Projections(https://arxiv.org/abs/2509.25228)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce Random Projection Flows (RPFs), a principled framework for injective normalizing flows that leverages tools from random matrix theory and the geometry of random projections. RPFs employ random semi-orthogonal matrices, drawn from Haar-distributed orthogonal ensembles via QR decomposition of Gaussian matrices, to project data into lower-dimensional latent spaces for the base distribution. Unlike PCA-based flows or learned injective maps, RPFs are plug-and-play, efficient, and yield closed-form expressions for the Riemannian volume correction term. We demonstrate that RPFs are both theoretically grounded and practically effective, providing a strong baseline for generative modeling and a bridge between random projection theory and normalizing flows.</li>
</ul>

<h3>Title: WDformer: A Wavelet-based Differential Transformer Model for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Xiaojian Wang, Chaoli Zhang, Zhonglong Zheng, Yunliang Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25231">https://arxiv.org/abs/2509.25231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25231">https://arxiv.org/pdf/2509.25231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25231]] WDformer: A Wavelet-based Differential Transformer Model for Time Series Forecasting(https://arxiv.org/abs/2509.25231)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Time series forecasting has various applications, such as meteorological rainfall prediction, traffic flow analysis, financial forecasting, and operational load monitoring for various systems. Due to the sparsity of time series data, relying solely on time-domain or frequency-domain modeling limits the model's ability to fully leverage multi-domain information. Moreover, when applied to time series forecasting tasks, traditional attention mechanisms tend to over-focus on irrelevant historical information, which may introduce noise into the prediction process, leading to biased results. We proposed WDformer, a wavelet-based differential Transformer model. This study employs the wavelet transform to conduct a multi-resolution analysis of time series data. By leveraging the advantages of joint representation in the time-frequency domain, it accurately extracts the key information components that reflect the essential characteristics of the data. Furthermore, we apply attention mechanisms on inverted dimensions, allowing the attention mechanism to capture relationships between multiple variables. When performing attention calculations, we introduced the differential attention mechanism, which computes the attention score by taking the difference between two separate softmax attention matrices. This approach enables the model to focus more on important information and reduce noise. WDformer has achieved state-of-the-art (SOTA) results on multiple challenging real-world datasets, demonstrating its accuracy and effectiveness. Code is available at this https URL.</li>
</ul>

<h3>Title: Sampling via Gaussian Mixture Approximations</h3>
<ul>
<li><strong>Authors: </strong>Yongchao Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25232">https://arxiv.org/abs/2509.25232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25232">https://arxiv.org/pdf/2509.25232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25232]] Sampling via Gaussian Mixture Approximations(https://arxiv.org/abs/2509.25232)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a family of \textit{Gaussian Mixture Approximation} (GMA) samplers for sampling unnormalised target densities, encompassing \textit{weights-only GMA} (W-GMA), \textit{Laplace Mixture Approximation} (LMA), \textit{expectation-maximization GMA} (EM-GMA), and further variants. GMA adopts a simple two-stage paradigm: (i) initialise a finite set of Gaussian components and draw samples from a proposal mixture; (ii) fit the mixture to the target by optimising either only the component weights or also the means and variances, via a sample-based KL divergence objective that requires only evaluations of the unnormalised density, followed by stratified resampling. The method is gradient-free, and computationally efficient: it leverages the ease of sampling from Gaussians, efficient optimisation methods (projected gradient descent, mirror descent, and EM), and the robustness of stratified resampling to produce samples faithful to the target. We show that this optimisation-resampling scheme yields consistent approximations under mild conditions, and we validate this methodology with empirical results demonstrating accuracy and speed across diverse densities.</li>
</ul>

<h3>Title: FedCLF - Towards Efficient Participant Selection for Federated Learning in Heterogeneous IoV Networks</h3>
<ul>
<li><strong>Authors: </strong>Kasun Eranda Wijethilake, Adnan Mahmood, Quan Z. Sheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25233">https://arxiv.org/abs/2509.25233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25233">https://arxiv.org/pdf/2509.25233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25233]] FedCLF - Towards Efficient Participant Selection for Federated Learning in Heterogeneous IoV Networks(https://arxiv.org/abs/2509.25233)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a distributed machine learning technique that preserves data privacy by sharing only the trained parameters instead of the client data. This makes FL ideal for highly dynamic, heterogeneous, and time-critical applications, in particular, the Internet of Vehicles (IoV) networks. However, FL encounters considerable challenges in such networks owing to the high data and device heterogeneity. To address these challenges, we propose FedCLF, i.e., FL with Calibrated Loss and Feedback control, which introduces calibrated loss as a utility in the participant selection process and a feedback control mechanism to dynamically adjust the sampling frequency of the clients. The envisaged approach (a) enhances the overall model accuracy in case of highly heterogeneous data and (b) optimizes the resource utilization for resource constrained IoV networks, thereby leading to increased efficiency in the FL process. We evaluated FedCLF vis-à-vis baseline models, i.e., FedAvg, Newt, and Oort, using CIFAR-10 dataset with varying data heterogeneity. Our results depict that FedCLF significantly outperforms the baseline models by up to a 16% improvement in high data heterogeneity-related scenarios with improved efficiency via reduced sampling frequency.</li>
</ul>

<h3>Title: PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases</h3>
<ul>
<li><strong>Authors: </strong>Sri Vatsa Vuddanti, Aarav Shah, Satwik Kumar Chittiprolu, Tony Song, Sunishchal Dev, Kevin Zhu, Maheep Chaudhary</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25238">https://arxiv.org/abs/2509.25238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25238">https://arxiv.org/pdf/2509.25238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25238]] PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases(https://arxiv.org/abs/2509.25238)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Tool-augmented language agents frequently fail in real-world deployment due to tool malfunctions--timeouts, API exceptions, or inconsistent outputs--triggering cascading reasoning errors and task abandonment. Existing agent training pipelines optimize only for success trajectories, failing to expose models to the tool failures that dominate real-world usage. We propose \textbf{PALADIN}, a generalizable framework for equipping language agents with robust failure recovery capabilities. PALADIN trains on 50,000+ recovery-annotated trajectories constructed via systematic failure injection and expert demonstrations on an enhanced ToolBench dataset. Training uses LoRA-based fine-tuning to retain base capabilities while injecting recovery competence. At inference, PALADIN detects execution-time errors and retrieves the most similar case from a curated bank of 55+ failure exemplars aligned with ToolScan's taxonomy, then executes the corresponding recovery action. This approach generalizes to novel failures beyond the training distribution, retaining 95.2\% recovery performance on unseen tool APIs. Evaluation across PaladinEval and ToolReflectEval demonstrates consistent improvements in Recovery Rate (RR), Task Success Rate (TSR), Catastrophic Success Rate (CSR), and Efficiency Score (ES). PALADIN improves RR from 32.76% to 89.68% (+57% relative) over ToolBench and outperforms the strongest baseline CRITIC (76.34%) by +13.3%. Against vanilla agents, PALADIN achieves 89.86\% RR (+66% relative improvement from 23.75%). These results establish PALADIN as an effective method for building fault-tolerant agents capable of robust recovery in real-world tool environments.</li>
</ul>

<h3>Title: HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement</h3>
<ul>
<li><strong>Authors: </strong>Ming Yang, Xiaofan Li, Zhiyuan Ma, Dengliang Shi, Jintao Du, Yu Cheng, Weiguo Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25240">https://arxiv.org/abs/2509.25240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25240">https://arxiv.org/pdf/2509.25240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25240]] HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement(https://arxiv.org/abs/2509.25240)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent curriculum reinforcement learning for large language models (LLMs) typically rely on difficulty-based annotations for data filtering and ordering. However, such methods suffer from local optimization, where continual training on simple samples in the early steps can cause the policy to lose its exploration. We propose a novel schema, namely Hamiltonian curiosity augmented large language model reinforcement (HAMMER), that transfers diversity metrics, commonly used in dataset evaluation, into the dynamic reinforcement learning procedure, where training samples are ordered via a minimum-semantic Hamiltonian path making the initial training retrain more exploration. From a theoretical perspective of generalization bounds, diversity-driven ordering facilitates stable convergence. Empirical evaluations indicate that HAMMER stimulates model "curiosity" and consistently achieves a 3% to 4% average accuracy gain across diverse inference benchmark.</li>
</ul>

<h3>Title: Fine-tuning of Large Language Models for Domain-Specific Cybersecurity Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Yuan Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25241">https://arxiv.org/abs/2509.25241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25241">https://arxiv.org/pdf/2509.25241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25241]] Fine-tuning of Large Language Models for Domain-Specific Cybersecurity Knowledge(https://arxiv.org/abs/2509.25241)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in training paradigms for Large Language Models (LLMs) have unlocked their remarkable capabilities in natural language processing and cross-domain generalization. While LLMs excel in tasks like programming and mathematical problem-solving, their zero-shot performance in specialized domains requiring expert knowledge, such as cybersecurity, is often suboptimal. This limitation arises because foundational LLMs are designed for general-purpose applications, constraining their ability to encapsulate domain-specific expertise within their parameter space. To address this, we explore fine-tuning strategies to embed cybersecurity knowledge into LLMs, enhancing their performance in cybersecurity question-answering (Q\&A) tasks while prioritizing computational efficiency. Specifically, we investigate Supervised Fine-Tuning (SFT), Low-Rank Adaptation (LoRA), and Quantized Low-Rank Adaptation (QLoRA) using a cybersecurity Q\&A dataset. Our results demonstrate that these fine-tuning approaches significantly outperform the foundational model in cybersecurity Q\&A tasks. Moreover, LoRA and QLoRA achieve comparable performance to SFT with substantially lower computational costs, offering an efficient pathway for adapting LLMs to specialized domains. Our work highlights the potential of low-rank fine-tuning strategies to bridge the gap between general-purpose LLMs and domain-specific applications.</li>
</ul>

<h3>Title: Heterogeneous Multi-agent Collaboration in UAV-assisted Mobile Crowdsensing Networks</h3>
<ul>
<li><strong>Authors: </strong>Xianyang Deng, Wenshuai Liu, Yaru FuB, Qi Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25261">https://arxiv.org/abs/2509.25261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25261">https://arxiv.org/pdf/2509.25261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25261]] Heterogeneous Multi-agent Collaboration in UAV-assisted Mobile Crowdsensing Networks(https://arxiv.org/abs/2509.25261)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Unmanned aerial vehicles (UAVs)-assisted mobile crowdsensing (MCS) has emerged as a promising paradigm for data collection. However, challenges such as spectrum scarcity, device heterogeneity, and user mobility hinder efficient coordination of sensing, communication, and computation. To tackle these issues, we propose a joint optimization framework that integrates time slot partition for sensing, communication, and computation phases, resource allocation, and UAV 3D trajectory planning, aiming to maximize the amount of processed sensing data. The problem is formulated as a non-convex stochastic optimization and further modeled as a partially observable Markov decision process (POMDP) that can be solved by multi-agent deep reinforcement learning (MADRL) algorithm. To overcome the limitations of conventional multi-layer perceptron (MLP) networks, we design a novel MADRL algorithm with hybrid actor network. The newly developed method is based on heterogeneous agent proximal policy optimization (HAPPO), empowered by convolutional neural networks (CNN) for feature extraction and Kolmogorov-Arnold networks (KAN) to capture structured state-action dependencies. Extensive numerical results demonstrate that our proposed method achieves significant improvements in the amount of processed sensing data when compared with other benchmarks.</li>
</ul>

<h3>Title: Dynamic Policy Induction for Adaptive Prompt Optimization: Bridging the Efficiency-Accuracy Gap via Lightweight Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiexi Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25267">https://arxiv.org/abs/2509.25267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25267">https://arxiv.org/pdf/2509.25267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25267]] Dynamic Policy Induction for Adaptive Prompt Optimization: Bridging the Efficiency-Accuracy Gap via Lightweight Reinforcement Learning(https://arxiv.org/abs/2509.25267)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The performance of Large Language Models (LLMs) depends heavily on the chosen prompting strategy, yet static approaches such as Zero-Shot, Few-Shot, or Chain-of-Thought (CoT) impose a rigid efficiency-accuracy trade-off. Highly accurate strategies like Self-Consistency (SC) incur substantial computational waste on simple tasks, while lightweight methods often fail on complex inputs. This paper introduces the Prompt Policy Network (PPN), a lightweight reinforcement learning framework that formalizes adaptive strategy selection as a single-step Markov Decision Process (MDP). The PPN, trained with Proximal Policy Optimization (PPO) and guided by a resource-explicit reward function, learns to allocate costly reasoning strategies only when necessary. Experiments on arithmetic reasoning benchmarks demonstrate that PPN achieves superior performance on the efficiency-accuracy Pareto front, delivering up to 61.5% token cost reduction compared to Self-Consistency while maintaining competitive accuracy. This work contributes a systematic, adaptive framework for cost-efficient LLM deployment, advancing the design of lightweight optimization techniques for scalable and sustainable language model applications.</li>
</ul>

<h3>Title: A Weather Foundation Model for the Power Grid</h3>
<ul>
<li><strong>Authors: </strong>Cristian Bodnar, Raphaël Rousseau-Rizzi, Nikhil Shankar, James Merleau, Stylianos Flampouris, Guillem Candille, Slavica Antic, François Miralles, Jayesh K. Gupta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25268">https://arxiv.org/abs/2509.25268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25268">https://arxiv.org/pdf/2509.25268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25268]] A Weather Foundation Model for the Power Grid(https://arxiv.org/abs/2509.25268)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Weather foundation models (WFMs) have recently set new benchmarks in global forecast skill, yet their concrete value for the weather-sensitive infrastructure that powers modern society remains largely unexplored. In this study, we fine-tune Silurian AI's 1.5B-parameter WFM, Generative Forecasting Transformer (GFT), on a rich archive of Hydro-Québec asset observations--including transmission-line weather stations, wind-farm met-mast streams, and icing sensors--to deliver hyper-local, asset-level forecasts for five grid-critical variables: surface temperature, precipitation, hub-height wind speed, wind-turbine icing risk, and rime-ice accretion on overhead conductors. Across 6-72 h lead times, the tailored model surpasses state-of-the-art NWP benchmarks, trimming temperature mean absolute error (MAE) by 15%, total-precipitation MAE by 35%, and lowering wind speed MAE by 15%. Most importantly, it attains an average precision score of 0.72 for day-ahead rime-ice detection, a capability absent from existing operational systems, which affords several hours of actionable warning for potentially catastrophic outage events. These results show that WFMs, when post-trained with small amounts of high-fidelity, can serve as a practical foundation for next-generation grid-resilience intelligence.</li>
</ul>

<h3>Title: InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions</h3>
<ul>
<li><strong>Authors: </strong>Liangjian Wen, Qun Dai, Jianzhuang Liu, Jiangtao Zheng, Yong Dai, Dongkai Wang, Zhao Kang, Jun Wang, Zenglin Xu, Jiang Duan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25270">https://arxiv.org/abs/2509.25270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25270">https://arxiv.org/pdf/2509.25270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25270]] InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions(https://arxiv.org/abs/2509.25270)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In multimodal representation learning, synergistic interactions between modalities not only provide complementary information but also create unique outcomes through specific interaction patterns that no single modality could achieve alone. Existing methods may struggle to effectively capture the full spectrum of synergistic information, leading to suboptimal performance in tasks where such interactions are critical. This is particularly problematic because synergistic information constitutes the fundamental value proposition of multimodal representation. To address this challenge, we introduce InfMasking, a contrastive synergistic information extraction method designed to enhance synergistic information through an \textbf{Inf}inite \textbf{Masking} strategy. InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. Unmasked fused representations are then aligned with masked ones through mutual information maximization to encode comprehensive synergistic information. This infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training. As computing mutual information estimates with infinite masking is computationally prohibitive, we derive an InfMasking loss to approximate this calculation. Through controlled experiments, we demonstrate that InfMasking effectively enhances synergistic information between modalities. In evaluations on large-scale real-world datasets, InfMasking achieves state-of-the-art performance across seven benchmarks. Code is released at this https URL.</li>
</ul>

<h3>Title: MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series</h3>
<ul>
<li><strong>Authors: </strong>Payal Mohapatra, Yueyuan Sui, Akash Pandey, Stephen Xia, Qi Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25278">https://arxiv.org/abs/2509.25278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25278">https://arxiv.org/pdf/2509.25278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25278]] MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series(https://arxiv.org/abs/2509.25278)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>From clinical healthcare to daily living, continuous sensor monitoring across multiple modalities has shown great promise for real-world intelligent decision-making but also faces various challenges. In this work, we introduce MAESTRO, a novel framework that overcomes key limitations of existing multimodal learning approaches: (1) reliance on a single primary modality for alignment, (2) pairwise modeling of modalities, and (3) assumption of complete modality observations. These limitations hinder the applicability of these approaches in real-world multimodal time-series settings, where primary modality priors are often unclear, the number of modalities can be large (making pairwise modeling impractical), and sensor failures often result in arbitrary missing observations. At its core, MAESTRO facilitates dynamic intra- and cross-modal interactions based on task relevance, and leverages symbolic tokenization and adaptive attention budgeting to construct long multimodal sequences, which are processed via sparse cross-modal attention. The resulting cross-modal tokens are routed through a sparse Mixture-of-Experts (MoE) mechanism, enabling black-box specialization under varying modality combinations. We evaluate MAESTRO against 10 baselines on four diverse datasets spanning three applications, and observe average relative improvements of 4% and 8% over the best existing multimodal and multivariate approaches, respectively, under complete observations. Under partial observations -- with up to 40% of missing modalities -- MAESTRO achieves an average 9% improvement. Further analysis also demonstrates the robustness and efficiency of MAESTRO's sparse, modality-aware design for learning from dynamic time series.</li>
</ul>

<h3>Title: Optimisation of Resource Allocation in Heterogeneous Wireless Networks Using Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Oluwaseyi Giwa, Jonathan Shock, Jaco Du Toit, Tobi Awodumila</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25284">https://arxiv.org/abs/2509.25284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25284">https://arxiv.org/pdf/2509.25284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25284]] Optimisation of Resource Allocation in Heterogeneous Wireless Networks Using Deep Reinforcement Learning(https://arxiv.org/abs/2509.25284)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Dynamic resource allocation in heterogeneous wireless networks (HetNets) is challenging for traditional methods under varying user loads and channel conditions. We propose a deep reinforcement learning (DRL) framework that jointly optimises transmit power, bandwidth, and scheduling via a multi-objective reward balancing throughput, energy efficiency, and fairness. Using real base station coordinates, we compare Proximal Policy Optimisation (PPO) and Twin Delayed Deep Deterministic Policy Gradient (TD3) against three heuristic algorithms in multiple network scenarios. Our results show that DRL frameworks outperform heuristic algorithms in optimising resource allocation in dynamic networks. These findings highlight key trade-offs in DRL design for future HetNets.</li>
</ul>

<h3>Title: Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zelin Tan, Hejia Geng, Mulei Zhang, Xiaohang Yu, Guancheng Wan, Yifan Zhou, Qiang He, Xiangyuan Xue, Heng Zhou, Yutao Fan, Zhongzhi Li, Zaibin Zhang, Guibin Zhang, Chen Zhang, Zhenfei Yin, Lei Bai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25300">https://arxiv.org/abs/2509.25300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25300">https://arxiv.org/pdf/2509.25300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25300]] Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning(https://arxiv.org/abs/2509.25300)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigation of scaling behaviors in RL-based post-training, with a particular focus on mathematical reasoning. Based on 54 experiments across diverse model sizes and training settings, we characterize how model scale, data volume, and computational budget interact to shape performance. Our analysis leads to four key findings: (1). Under a fixed computational budget, larger models trained for fewer steps consistently outperform smaller models trained for more steps. (2). Given a fixed amount of training data, larger models achieve superior sample efficiency, yielding lower loss. (3). In data-constrained regimes, repeated reuse of high-quality data proves highly effective, as final performance is primarily governed by the total number of optimization steps rather than the uniqueness of samples. (4). These scaling behaviors are robust across both base and instruction-tuned models, which share similar learning dynamics (e.g., larger models show faster convergence) even while differing in absolute accuracy. Collectively, these results provide a principled foundation and practical guidelines for efficiently scaling the reasoning capabilities of LLMs through RL post-training.</li>
</ul>

<h3>Title: LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Haozhe Jia, Wenshuo Chen, Yuqi Lin, Yang Yang, Lei Wang, Mang Ning, Bowen Tian, Songning Lai, Nanqian Jia, Yifan Chen, Yutao Yue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25304">https://arxiv.org/abs/2509.25304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25304">https://arxiv.org/pdf/2509.25304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25304]] LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model(https://arxiv.org/abs/2509.25304)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While current diffusion-based models, typically built on U-Net architectures, have shown promising results on the text-to-motion generation task, they still suffer from semantic misalignment and kinematic artifacts. Through analysis, we identify severe gradient attenuation in the deep layers of the network as a key bottleneck, leading to insufficient learning of high-level features. To address this issue, we propose \textbf{LUMA} (\textit{\textbf{L}ow-dimension \textbf{U}nified \textbf{M}otion \textbf{A}lignment}), a text-to-motion diffusion model that incorporates dual-path anchoring to enhance semantic alignment. The first path incorporates a lightweight MoCLIP model trained via contrastive learning without relying on external data, offering semantic supervision in the temporal domain. The second path introduces complementary alignment signals in the frequency domain, extracted from low-frequency DCT components known for their rich semantic content. These two anchors are adaptively fused through a temporal modulation mechanism, allowing the model to progressively transition from coarse alignment to fine-grained semantic refinement throughout the denoising process. Experimental results on HumanML3D and KIT-ML demonstrate that LUMA achieves state-of-the-art performance, with FID scores of 0.035 and 0.123, respectively. Furthermore, LUMA accelerates convergence by 1.4$\times$ compared to the baseline, making it an efficient and scalable solution for high-fidelity text-to-motion generation.</li>
</ul>

<h3>Title: Uncertainty-Aware Generative Oversampling Using an Entropy-Guided Conditional Variational Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Amirhossein Zare, Amirhessam Zare, Parmida Sadat Pezeshki, Herlock (SeyedAbolfazl)Rahimi, Ali Ebrahimi, Ignacio Vázquez-García, Leo Anthony Celi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25334">https://arxiv.org/abs/2509.25334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25334">https://arxiv.org/pdf/2509.25334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25334]] Uncertainty-Aware Generative Oversampling Using an Entropy-Guided Conditional Variational Autoencoder(https://arxiv.org/abs/2509.25334)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Class imbalance remains a major challenge in machine learning, especially for high-dimensional biomedical data where nonlinear manifold structures dominate. Traditional oversampling methods such as SMOTE rely on local linear interpolation, often producing implausible synthetic samples. Deep generative models like Conditional Variational Autoencoders (CVAEs) better capture nonlinear distributions, but standard variants treat all minority samples equally, neglecting the importance of uncertain, boundary-region examples emphasized by heuristic methods like Borderline-SMOTE and ADASYN. We propose Local Entropy-Guided Oversampling with a CVAE (LEO-CVAE), a generative oversampling framework that explicitly incorporates local uncertainty into both representation learning and data generation. To quantify uncertainty, we compute Shannon entropy over the class distribution in a sample's neighborhood: high entropy indicates greater class overlap, serving as a proxy for uncertainty. LEO-CVAE leverages this signal through two mechanisms: (i) a Local Entropy-Weighted Loss (LEWL) that emphasizes robust learning in uncertain regions, and (ii) an entropy-guided sampling strategy that concentrates generation in these informative, class-overlapping areas. Applied to clinical genomics datasets (ADNI and TCGA lung cancer), LEO-CVAE consistently improves classifier performance, outperforming both traditional oversampling and generative baselines. These results highlight the value of uncertainty-aware generative oversampling for imbalanced learning in domains governed by complex nonlinear structures, such as omics data.</li>
</ul>

<h3>Title: Editing Physiological Signals in Videos Using Latent Representations</h3>
<ul>
<li><strong>Authors: </strong>Tianwen Zhou, Akshay Paruchuri, Josef Spjut, Kaan Akşit</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25348">https://arxiv.org/abs/2509.25348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25348">https://arxiv.org/pdf/2509.25348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25348]] Editing Physiological Signals in Videos Using Latent Representations(https://arxiv.org/abs/2509.25348)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, biometric</a></li>
<li><strong>Abstract: </strong>Camera-based physiological signal estimation provides a non-contact and convenient means to monitor Heart Rate (HR). However, the presence of vital signals in facial videos raises significant privacy concerns, as they can reveal sensitive personal information related to the health and emotional states of an individual. To address this, we propose a learned framework that edits physiological signals in videos while preserving visual fidelity. First, we encode an input video into a latent space via a pretrained 3D Variational Autoencoder (3D VAE), while a target HR prompt is embedded through a frozen text encoder. We fuse them using a set of trainable spatio-temporal layers with Adaptive Layer Normalizations (AdaLN) to capture the strong temporal coherence of remote Photoplethysmography (rPPG) signals. We apply Feature-wise Linear Modulation (FiLM) in the decoder with a fine-tuned output layer to avoid the degradation of physiological signals during reconstruction, enabling accurate physiological modulation in the reconstructed video. Empirical results show that our method preserves visual quality with an average PSNR of 38.96 dB and SSIM of 0.98 on selected datasets, while achieving an average HR modulation error of 10.00 bpm MAE and 10.09% MAPE using a state-of-the-art rPPG estimator. Our design's controllable HR editing is useful for applications such as anonymizing biometric signals in real videos or synthesizing realistic videos with desired vital signs.</li>
</ul>

<h3>Title: From Internal Representations to Text Quality: A Geometric Approach to LLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Viacheslav Yusupov, Danil Maksimov, Ameliia Alaeva, Anna Vasileva, Anna Antipina, Tatyana Zaitseva, Alina Ermilova, Evgeny Burnaev, Egor Shvetsov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25359">https://arxiv.org/abs/2509.25359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25359">https://arxiv.org/pdf/2509.25359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25359]] From Internal Representations to Text Quality: A Geometric Approach to LLM Evaluation(https://arxiv.org/abs/2509.25359)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper bridges internal and external analysis approaches to large language models (LLMs) by demonstrating that geometric properties of internal model representations serve as reliable proxies for evaluating generated text quality. We validate a set of metrics including Maximum Explainable Variance, Effective Rank, Intrinsic Dimensionality, MAUVE score, and Schatten Norms measured across different layers of LLMs, demonstrating that Intrinsic Dimensionality and Effective Rank can serve as universal assessments of text naturalness and quality. Our key finding reveals that different models consistently rank text from various sources in the same order based on these geometric properties, indicating that these metrics reflect inherent text characteristics rather than model-specific artifacts. This allows a reference-free text quality evaluation that does not require human-annotated datasets, offering practical advantages for automated evaluation pipelines.</li>
</ul>

<h3>Title: Generative Value Conflicts Reveal LLM Priorities</h3>
<ul>
<li><strong>Authors: </strong>Andy Liu, Kshitish Ghate, Mona Diab, Daniel Fried, Atoosa Kasirzadeh, Max Kleiman-Weiner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25369">https://arxiv.org/abs/2509.25369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25369">https://arxiv.org/pdf/2509.25369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25369]] Generative Value Conflicts Reveal LLM Priorities(https://arxiv.org/abs/2509.25369)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, generative, large language model</a></li>
<li><strong>Abstract: </strong>Past work seeks to align large language model (LLM)-based assistants with a target set of values, but such assistants are frequently forced to make tradeoffs between values when deployed. In response to the scarcity of value conflict in existing alignment datasets, we introduce ConflictScope, an automatic pipeline to evaluate how LLMs prioritize different values. Given a user-defined value set, ConflictScope automatically generates scenarios in which a language model faces a conflict between two values sampled from the set. It then prompts target models with an LLM-written "user prompt" and evaluates their free-text responses to elicit a ranking over values in the value set. Comparing results between multiple-choice and open-ended evaluations, we find that models shift away from supporting protective values, such as harmlessness, and toward supporting personal values, such as user autonomy, in more open-ended value conflict settings. However, including detailed value orderings in models' system prompts improves alignment with a target ranking by 14%, showing that system prompting can achieve moderate success at aligning LLM behavior under value conflict. Our work demonstrates the importance of evaluating value prioritization in models and provides a foundation for future work in this area.</li>
</ul>

<h3>Title: Let Physics Guide Your Protein Flows: Topology-aware Unfolding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Yogesh Verma, Markus Heinonen, Vikas Garg</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25379">https://arxiv.org/abs/2509.25379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25379">https://arxiv.org/pdf/2509.25379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25379]] Let Physics Guide Your Protein Flows: Topology-aware Unfolding and Generation(https://arxiv.org/abs/2509.25379)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Protein structure prediction and folding are fundamental to understanding biology, with recent deep learning advances reshaping the field. Diffusion-based generative models have revolutionized protein design, enabling the creation of novel proteins. However, these methods often neglect the intrinsic physical realism of proteins, driven by noising dynamics that lack grounding in physical principles. To address this, we first introduce a physically motivated non-linear noising process, grounded in classical physics, that unfolds proteins into secondary structures (e.g., alpha helices, linear beta sheets) while preserving topological integrity--maintaining bonds, and preventing collisions. We then integrate this process with the flow-matching paradigm on SE(3) to model the invariant distribution of protein backbones with high fidelity, incorporating sequence information to enable sequence-conditioned folding and expand the generative capabilities of our model. Experimental results demonstrate that the proposed method achieves state-of-the-art performance in unconditional protein generation, producing more designable and novel protein structures while accurately folding monomer sequences into precise protein conformations.</li>
</ul>

<h3>Title: On the Shape of Latent Variables in a Denoising VAE-MoG: A Posterior Sampling-Based Study</h3>
<ul>
<li><strong>Authors: </strong>Fernanda Zapata Bascuñán</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25382">https://arxiv.org/abs/2509.25382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25382">https://arxiv.org/pdf/2509.25382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25382]] On the Shape of Latent Variables in a Denoising VAE-MoG: A Posterior Sampling-Based Study(https://arxiv.org/abs/2509.25382)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we explore the latent space of a denoising variational autoencoder with a mixture-of-Gaussians prior (VAE-MoG), trained on gravitational wave data from event GW150914. To evaluate how well the model captures the underlying structure, we use Hamiltonian Monte Carlo (HMC) to draw posterior samples conditioned on clean inputs, and compare them to the encoder's outputs from noisy data. Although the model reconstructs signals accurately, statistical comparisons reveal a clear mismatch in the latent space. This shows that strong denoising performance doesn't necessarily mean the latent representations are reliable highlighting the importance of using posterior-based validation when evaluating generative models.</li>
</ul>

<h3>Title: A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland</h3>
<ul>
<li><strong>Authors: </strong>Wendong Yao, Binhua Huang, Soumyabrata Dev</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25393">https://arxiv.org/abs/2509.25393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25393">https://arxiv.org/pdf/2509.25393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25393]] A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland(https://arxiv.org/abs/2509.25393)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Forecasting high-resolution land subsidence is a critical yet challenging task due to its complex, non-linear dynamics. While standard architectures like ConvLSTM often fail to model long-range dependencies, we argue that a more fundamental limitation of prior work lies in the uni-modal data paradigm. To address this, we propose the Multi-Modal Spatio-Temporal Transformer (MM-STT), a novel framework that fuses dynamic displacement data with static physical priors. Its core innovation is a joint spatio-temporal attention mechanism that processes all multi-modal features in a unified manner. On the public EGMS dataset, MM-STT establishes a new state-of-the-art, reducing the long-range forecast RMSE by an order of magnitude compared to all baselines, including SOTA methods like STGCN and STAEformer. Our results demonstrate that for this class of problems, an architecture's inherent capacity for deep multi-modal fusion is paramount for achieving transformative performance.</li>
</ul>

<h3>Title: Fast Energy-Theft Attack on Frequency-Varying Wireless Power without Additional Sensors</h3>
<ul>
<li><strong>Authors: </strong>Hui Wang, Nima Tashakor, Xiaoyang Tian, Hans D. Schotten, Stefan M. Goetz</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SP, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25394">https://arxiv.org/abs/2509.25394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25394">https://arxiv.org/pdf/2509.25394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25394]] Fast Energy-Theft Attack on Frequency-Varying Wireless Power without Additional Sensors(https://arxiv.org/abs/2509.25394)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, steal</a></li>
<li><strong>Abstract: </strong>With the popularity of wireless charging, energy access protection and cybersecurity are gaining importance, especially in public places. Currently, the most common energy encryption method uses frequency and associated impedance variation. However, we have proven that this method is not reliable, since a hacker can detect the changing frequency and adjust the compensation. However, the previously presented system needed time to follow the updated frequency, while encryption systems may vary the frequency faster to avoid energy theft. Furthermore, the previous system required an additional sensor coil. To solve these problems, we optimized the attack and the associated system, which can intrude and steal energy within 0.2 ms. The key is the elimination of the time-consuming maximum receiver current regulation. Also, we use the main receiving coil rather than any additional sensor antenna to detect the magnetic field. Thus, the new hardware is even simpler. A simulation model and experimental results demonstrate the fast response speed of the attack on encrypted wireless power and steal 65% of the power. Overall, the applicability of the attack is highly improved and leaves less room for hardening the encryption. The results demonstrate that energy access protection needs to be given great attention.</li>
</ul>

<h3>Title: Crowdsourcing Without People: Modelling Clustering Algorithms as Experts</h3>
<ul>
<li><strong>Authors: </strong>Jordyn E. A. Lorentz, Katharine M. Clark</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25395">https://arxiv.org/abs/2509.25395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25395">https://arxiv.org/pdf/2509.25395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25395]] Crowdsourcing Without People: Modelling Clustering Algorithms as Experts(https://arxiv.org/abs/2509.25395)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper introduces mixsemble, an ensemble method that adapts the Dawid-Skene model to aggregate predictions from multiple model-based clustering algorithms. Unlike traditional crowdsourcing, which relies on human labels, the framework models the outputs of clustering algorithms as noisy annotations. Experiments on both simulated and real-world datasets show that, although the mixsemble is not always the single top performer, it consistently approaches the best result and avoids poor outcomes. This robustness makes it a practical alternative when the true data structure is unknown, especially for non-expert users.</li>
</ul>

<h3>Title: FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Liang Qiao, Yue Dai, Yeqi Huang, Hongyu Kan, Jun Shi, Hong An</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25401">https://arxiv.org/abs/2509.25401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25401">https://arxiv.org/pdf/2509.25401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25401]] FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers(https://arxiv.org/abs/2509.25401)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional capabilities in visual synthesis, yet their deployment remains constrained by substantial computational demands. To alleviate this bottleneck, many sparsity-based acceleration methods have been proposed. However, their diverse sparsity patterns often require customized kernels for high-performance inference, limiting universality. We propose FlashOmni, a unified sparse attention engine compatible with arbitrary DiT architectures. FlashOmni introduces flexible sparse symbols to standardize the representation of a wide range of sparsity strategies, such as feature caching and block-sparse skipping. This unified abstraction enables the execution of diverse sparse computations within a single attention kernel. In addition, FlashOmni designs optimized sparse GEMMs for attention blocks, leveraging sparse symbols to eliminate redundant computations and further improve efficiency. Experiments demonstrate that FlashOmni delivers near-linear, closely matching the sparsity ratio speedup (1:1) in attention and GEMM-$Q$, and achieves 2.5$\times$-3.8$\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of the theoretical limit). Applied with a multi-granularity sparsity strategy, it enables the Hunyuan model (33K) to achieve about 1.5$\times$ end-to-end acceleration without degrading visual quality.</li>
</ul>

<h3>Title: Optimal Threshold Signatures in Bitcoin</h3>
<ul>
<li><strong>Authors: </strong>Korok Ray, Sindura Saraswathi</a></li>
<li><strong>Subjects: </strong>cs.CR, econ.TH</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25408">https://arxiv.org/abs/2509.25408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25408">https://arxiv.org/pdf/2509.25408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25408]] Optimal Threshold Signatures in Bitcoin(https://arxiv.org/abs/2509.25408)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>We formulate the design of a threshold signature scheme as made possible on cryptocurrency protocols like Bitcoin. The funds are secured by an m-of-n threshold signature, where at least m signatures are needed to unlock the funds. A user designs this scheme knowing that a malicious attacker can also obtain the signatures with some probability. Higher thresholds offer more security, but also risk locking the user out of his own funds. The optimal threshold balances these twin effects. Interventions like increasing the security or usability of the signatures allow for higher thresholds. We model dynamic threshold signature schemes, where the probability of a user or attacker obtaining signatures decays with time. A dynamic threshold signature scheme is optimal, and increasing security or usability allows for higher thresholds and longer time locks.</li>
</ul>

<h3>Title: From Faithfulness to Correctness: Generative Reward Models that Think Critically</h3>
<ul>
<li><strong>Authors: </strong>Qiyao Ma, Yunsheng Shi, Hongtao Tian, Chao Wang, Weiming Chang, Ting Yao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25409">https://arxiv.org/abs/2509.25409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25409">https://arxiv.org/pdf/2509.25409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25409]] From Faithfulness to Correctness: Generative Reward Models that Think Critically(https://arxiv.org/abs/2509.25409)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Through reinforcement learning with verifiable rewards (RLVR), large language models have achieved substantial progress in domains with easily verifiable outcomes, such as mathematics and coding. However, when applied to more complex tasks like open-domain question answering, RLVR faces significant challenges due to the difficulty of verifying correctness. The nuanced and ambiguous nature of real-world knowledge makes it difficult to reliably evaluate correctness in these settings, necessitating further abilities that extend beyond mere logical consistency to encompass an understanding and assessment of both external and internal knowledge. Recent work has primarily focused on improving faithfulness, defined as semantic alignment with supporting documents, which can cause models to rely excessively on external sources and diminish their capacity for critical assessment. To address this, we propose the Thinking-supervised Reward Model (TRM), which incorporates sentence-level thinking supervision to endow reward models with critical thinking abilities. Given a query, answer, and supporting documents, TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness. By structuring reward modeling as a sequence of faithfulness, reasoning, and correctness evaluations, TRM encourages models to critically assess and leverage both external and internal knowledge. Experiments on reward signals demonstrate that TRM substantially improves the identification of incorrect sentences, and incorporating TRM into policy optimization leads to significant gains in both answer correctness and usefulness.</li>
</ul>

<h3>Title: Characterizing Event-themed Malicious Web Campaigns: A Case Study on War-themed Websites</h3>
<ul>
<li><strong>Authors: </strong>Maraz Mia, Mir Mehedi A. Pritom, Tariqul Islam, Shouhuai Xu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25410">https://arxiv.org/abs/2509.25410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25410">https://arxiv.org/pdf/2509.25410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25410]] Characterizing Event-themed Malicious Web Campaigns: A Case Study on War-themed Websites(https://arxiv.org/abs/2509.25410)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Cybercrimes such as online scams and fraud have become prevalent. Cybercriminals often abuse various global or regional events as themes of their fraudulent activities to breach user trust and attain a higher attack success rate. These attacks attempt to manipulate and deceive innocent people into interacting with meticulously crafted websites with malicious payloads, phishing, or fraudulent transactions. To deepen our understanding of the problem, this paper investigates how to characterize event-themed malicious website-based campaigns, with a case study on war-themed websites. We find that attackers tailor their attacks by exploiting the unique aspects of events, as evidenced by activities such as fundraising, providing aid, collecting essential supplies, or seeking updated news. We use explainable unsupervised clustering methods to draw further insights, which could guide the design of effective early defenses against various event-themed malicious web campaigns.</li>
</ul>

<h3>Title: Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs</h3>
<ul>
<li><strong>Authors: </strong>Hao Ban, Kaiyi Ji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25414">https://arxiv.org/abs/2509.25414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25414">https://arxiv.org/pdf/2509.25414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25414]] Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs(https://arxiv.org/abs/2509.25414)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, large language model</a></li>
<li><strong>Abstract: </strong>Large language models are often adapted using parameter-efficient techniques such as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$ is the pre-trained parameters and $x$ is the input to the adapted layer. While multi-adapter extensions often employ multiple LoRAs, prior studies suggest that the inner $A$ matrices are highly similar during training and thus suitable for sharing. We revisit this phenomenon and find that this similarity is largely attributable to the identical initialization rather than shared knowledge, with $B$ playing a more critical role in knowledge encoding and transfer. Motivated by these insights, we propose \textbf{ALoRA}, an asymmetric multi-LoRA design with multiple $A$ matrices and a single shared $B$ in multi-task fine-tuning, and \textbf{Fed-ALoRA}, which shares $B$ across clients in federated fine-tuning under both homogeneous and heterogeneous settings, through a novel matrix decomposition strategy to accommodate heterogeneous ranks across clients. Experiments on commonsense reasoning, math reasoning, multi-task NLP dataset, and federated NLP dataset demonstrate that our methods achieve more balanced performance across tasks with comparable or superior average accuracy relative to existing multi-LoRA approaches. Codes are available at this https URL.</li>
</ul>

<h3>Title: Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Shi, Hongfei Du, Yangfan He, Y. Alicia Hong, Ye Gao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25416">https://arxiv.org/abs/2509.25416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25416">https://arxiv.org/pdf/2509.25416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25416]] Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization(https://arxiv.org/abs/2509.25416)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Emotional text-to-speech seeks to convey affect while preserving intelligibility and prosody, yet existing methods rely on coarse labels or proxy classifiers and receive only utterance-level feedback. We introduce Emotion-Aware Stepwise Preference Optimization (EASPO), a post-training framework that aligns diffusion TTS with fine-grained emotional preferences at intermediate denoising steps. Central to our approach is EASPM, a time-conditioned model that scores noisy intermediate speech states and enables automatic preference pair construction. EASPO optimizes generation to match these stepwise preferences, enabling controllable emotional shaping. Experiments show superior performance over existing methods in both expressiveness and naturalness.</li>
</ul>

<h3>Title: Leveraging Vulnerabilities in Temporal Graph Neural Networks via Strategic High-Impact Assaults</h3>
<ul>
<li><strong>Authors: </strong>Dong Hyun Jeon, Lijing Zhu, Haifang Li, Pengze Li, Jingna Feng, Tiehang Duan, Houbing Herbert Song, Cui Tao, Shuteng Niu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25418">https://arxiv.org/abs/2509.25418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25418">https://arxiv.org/pdf/2509.25418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25418]] Leveraging Vulnerabilities in Temporal Graph Neural Networks via Strategic High-Impact Assaults(https://arxiv.org/abs/2509.25418)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Temporal Graph Neural Networks (TGNNs) have become indispensable for analyzing dynamic graphs in critical applications such as social networks, communication systems, and financial networks. However, the robustness of TGNNs against adversarial attacks, particularly sophisticated attacks that exploit the temporal dimension, remains a significant challenge. Existing attack methods for Spatio-Temporal Dynamic Graphs (STDGs) often rely on simplistic, easily detectable perturbations (e.g., random edge additions/deletions) and fail to strategically target the most influential nodes and edges for maximum impact. We introduce the High Impact Attack (HIA), a novel restricted black-box attack framework specifically designed to overcome these limitations and expose critical vulnerabilities in TGNNs. HIA leverages a data-driven surrogate model to identify structurally important nodes (central to network connectivity) and dynamically important nodes (critical for the graph's temporal evolution). It then employs a hybrid perturbation strategy, combining strategic edge injection (to create misleading connections) and targeted edge deletion (to disrupt essential pathways), maximizing TGNN performance degradation. Importantly, HIA minimizes the number of perturbations to enhance stealth, making it more challenging to detect. Comprehensive experiments on five real-world datasets and four representative TGNN architectures (TGN, JODIE, DySAT, and TGAT) demonstrate that HIA significantly reduces TGNN accuracy on the link prediction task, achieving up to a 35.55% decrease in Mean Reciprocal Rank (MRR) - a substantial improvement over state-of-the-art baselines. These results highlight fundamental vulnerabilities in current STDG models and underscore the urgent need for robust defenses that account for both structural and temporal dynamics.</li>
</ul>

<h3>Title: Finding Phones Fast: Low-Latency and Scalable Monitoring of Cellular Communications in Sensitive Areas</h3>
<ul>
<li><strong>Authors: </strong>Martin Kotuliak, Simon Erni, Jakub Polák, Marc Roeschlin, Richard Baker, Ivan Martinovic, Srdjan Čapkun</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25430">https://arxiv.org/abs/2509.25430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25430">https://arxiv.org/pdf/2509.25430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25430]] Finding Phones Fast: Low-Latency and Scalable Monitoring of Cellular Communications in Sensitive Areas(https://arxiv.org/abs/2509.25430)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The widespread availability of cellular devices introduces new threat vectors that allow users or attackers to bypass security policies and physical barriers and bring unauthorized devices into sensitive areas. These threats can arise from user non-compliance or deliberate actions aimed at data exfiltration/infiltration via hidden devices, drones, etc. We identify a critical gap in this context: the absence of low-latency systems for high-quality and instantaneous monitoring of cellular transmissions. Such low-latency systems are crucial to allow for timely detection, decision (e.g., geofencing or localization), and disruption of unauthorized communication in sensitive areas. Operator-based monitoring systems, built for purposes such as people counting or tracking, lack real-time capability, require cooperation across multiple operators, and thus are hard to deploy. Operator-independent monitoring approaches proposed in the literature either lack low-latency capabilities or do not scale. We propose LTag, the first low-latency, operator-independent and scalable system designed to monitor cellular connections across all operators prior to any user data transmission. LTag consists of several downlink sniffers and a distributed network of uplink sniffers that measure both downlink protocol information and uplink signal characteristics at multiple locations to gain a detailed spatial image of uplink signals. LTag aggregates the recorded information, processes it, and provides a decision about the connection all prior to connection establishment of a UE. To evaluate LTag, we deployed it in the context of geofencing, where LTag was able to determine if the signals originate from inside or outside of an area within 2.3 ms of the initial base station-to-device message, therefore enabling prompt and targeted suppression of communication before any user data was transmitted.</li>
</ul>

<h3>Title: Bayesian Transformer for Pan-Arctic Sea Ice Concentration Mapping and Uncertainty Estimation using Sentinel-1, RCM, and AMSR2 Data</h3>
<ul>
<li><strong>Authors: </strong>Mabel Heffring, Lincoln Linlin Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25437">https://arxiv.org/abs/2509.25437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25437">https://arxiv.org/pdf/2509.25437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25437]] Bayesian Transformer for Pan-Arctic Sea Ice Concentration Mapping and Uncertainty Estimation using Sentinel-1, RCM, and AMSR2 Data(https://arxiv.org/abs/2509.25437)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Although high-resolution mapping of Pan-Arctic sea ice with reliable corresponding uncertainty is essential for operational sea ice concentration (SIC) charting, it is a difficult task due to some key challenges, e.g., the subtle nature of ice signature features, model uncertainty, and data heterogeneity. This letter presents a novel Bayesian Transformer approach for Pan-Arctic SIC mapping and uncertainty quantification using Sentinel-1, RADARSAT Constellation Mission (RCM), and Advanced Microwave Scanning Radiometer 2 (AMSR2) data. First, to improve feature extraction, we design a novel high-resolution Transformer model with both global and local modules that can better discern the subtle differences in sea ice patterns. Second, to improve uncertainty quantification, we design a Bayesian extension of the proposed Transformer model, treating its parameters as random variables to more effectively capture uncertainties. Third, to address data heterogeneity, we fuse three different data types (Sentinel-1, RCM, and AMSR2) at decision-level to improve both SIC mapping and uncertainty quantification. The proposed approach is tested on Pan-Arctic datasets from September 2021, and the results demonstrate that the proposed model can achieve both high-resolution SIC maps and robust uncertainty maps compared to other uncertainty quantification approaches.</li>
</ul>

<h3>Title: Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Zhibo Hou, Zhiyu An, Wan Du</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25438">https://arxiv.org/abs/2509.25438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25438">https://arxiv.org/pdf/2509.25438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25438]] Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring(https://arxiv.org/abs/2509.25438)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>When there exists an unlearnable source of randomness (noisy-TV) in the environment, a naively intrinsic reward driven exploring agent gets stuck at that source of randomness and fails at exploration. Intrinsic reward based on uncertainty estimation or distribution similarity, while eventually escapes noisy-TVs as time unfolds, suffers from poor sample efficiency and high computational cost. Inspired by recent findings from neuroscience that humans monitor their improvements during exploration, we propose a novel method for intrinsically-motivated exploration, named Learning Progress Monitoring (LPM). During exploration, LPM rewards model improvements instead of prediction error or novelty, effectively rewards the agent for observing learnable transitions rather than the unlearnable transitions. We introduce a dual-network design that uses an error model to predict the expected prediction error of the dynamics model in its previous iteration, and use the difference between the model errors of the current iteration and previous iteration to guide exploration. We theoretically show that the intrinsic reward of LPM is zero-equivariant and a monotone indicator of Information Gain (IG), and that the error model is necessary to achieve monotonicity correspondence with IG. We empirically compared LPM against state-of-the-art baselines in noisy environments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari. Results show that LPM's intrinsic reward converges faster, explores more states in the maze experiment, and achieves higher extrinsic reward in Atari. This conceptually simple approach marks a shift-of-paradigm of noise-robust exploration. For code to reproduce our experiments, see this https URL</li>
</ul>

<h3>Title: Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications</h3>
<ul>
<li><strong>Authors: </strong>Hanyuan Gao, Xiaoxuan Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25439">https://arxiv.org/abs/2509.25439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25439">https://arxiv.org/pdf/2509.25439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25439]] Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications(https://arxiv.org/abs/2509.25439)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Hidden Markov models (HMM) are commonly used in generation tasks and have demonstrated strong capabilities in neuro-symbolic applications for the Markov property. These applications leverage the strengths of neural networks and symbolic reasoning to create robust and interpretable AI systems. However, they may inherit and amplify the shortcomings of both approaches. Both components require dense computation and data transfer, and their communication further hinders performance. This paper proposes Norm-Q, a normalized linear quantization approach for compressing probabilistic symbolic models, such as HMMs. We reduce the bit width of the data with minimal impact, thereby alleviating memory and bandwidth stress and enabling deployment on potential custom hardware. Our method introduces a normalized quantization-aware expectation maximization process for probabilistic model training. The experimental results show that Norm-Q achieves a higher compression rate with reasonable score loss compared to traditional quantization methods. In the case of the constrained generation task of large language models, we successfully quantize an HMM of 4096 hidden states to 8 bits without loss and, at most, 3 bits with acceptable loss. Notably, the Norm-Q method can achieve a compression rate of 99% for the weights of the HMM. The code is open source at this https URL.</li>
</ul>

<h3>Title: Fingerprinting LLMs via Prompt Injection</h3>
<ul>
<li><strong>Authors: </strong>Yuepeng Hu, Zhengyuan Jiang, Mengyuan Li, Osama Ahmed, Zhicong Huang, Cheng Hong, Neil Gong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25448">https://arxiv.org/abs/2509.25448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25448">https://arxiv.org/pdf/2509.25448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25448]] Fingerprinting LLMs via Prompt Injection(https://arxiv.org/abs/2509.25448)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are often modified after release through post-processing such as post-training or quantization, which makes it challenging to determine whether one model is derived from another. Existing provenance detection methods have two main limitations: (1) they embed signals into the base model before release, which is infeasible for already published models, or (2) they compare outputs across models using hand-crafted or random prompts, which are not robust to post-processing. In this work, we propose LLMPrint, a novel detection framework that constructs fingerprints by exploiting LLMs' inherent vulnerability to prompt injection. Our key insight is that by optimizing fingerprint prompts to enforce consistent token preferences, we can obtain fingerprints that are both unique to the base model and robust to post-processing. We further develop a unified verification procedure that applies to both gray-box and black-box settings, with statistical guarantees. We evaluate LLMPrint on five base models and around 700 post-trained or quantized variants. Our results show that LLMPrint achieves high true positive rates while keeping false positive rates near zero.</li>
</ul>

<h3>Title: Joint Embeddings Go Temporal</h3>
<ul>
<li><strong>Authors: </strong>Sofiane Ennadir, Siavash Golkar, Leopoldo Sarra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25449">https://arxiv.org/abs/2509.25449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25449">https://arxiv.org/pdf/2509.25449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25449]] Joint Embeddings Go Temporal(https://arxiv.org/abs/2509.25449)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Self-supervised learning has seen great success recently in unsupervised representation learning, enabling breakthroughs in natural language and image processing. However, these methods often rely on autoregressive and masked modeling, which aim to reproduce masked information in the input, which can be vulnerable to the presence of noise or confounding variables. To address this problem, Joint-Embedding Predictive Architectures (JEPA) has been introduced with the aim to perform self-supervised learning in the latent space. To leverage these advancements in the domain of time series, we introduce Time Series JEPA (TS-JEPA), an architecture specifically adapted for time series representation learning. We validate TS-JEPA on both classification and forecasting, showing that it can match or surpass current state-of-the-art baselines on different standard datasets. Notably, our approach demonstrates a strong performance balance across diverse tasks, indicating its potential as a robust foundation for learning general representations. Thus, this work lays the groundwork for developing future time series foundation models based on Joint Embedding.</li>
</ul>

<h3>Title: Infrastructure Sensor-enabled Vehicle Data Generation using Multi-Sensor Fusion for Proactive Safety Applications at Work Zone</h3>
<ul>
<li><strong>Authors: </strong>Suhala Rabab Saba, Sakib Khan, Minhaj Uddin Ahmad, Jiahe Cao, Mizanur Rahman, Li Zhao, Nathan Huynh, Eren Erman Ozguven</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25452">https://arxiv.org/abs/2509.25452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25452">https://arxiv.org/pdf/2509.25452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25452]] Infrastructure Sensor-enabled Vehicle Data Generation using Multi-Sensor Fusion for Proactive Safety Applications at Work Zone(https://arxiv.org/abs/2509.25452)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Infrastructure-based sensing and real-time trajectory generation show promise for improving safety in high-risk roadway segments such as work zones, yet practical deployments are hindered by perspective distortion, complex geometry, occlusions, and costs. This study tackles these barriers by integrating roadside camera and LiDAR sensors into a cosimulation environment to develop a scalable, cost-effective vehicle detection and localization framework, and employing a Kalman Filter-based late fusion strategy to enhance trajectory consistency and accuracy. In simulation, the fusion algorithm reduced longitudinal error by up to 70 percent compared to individual sensors while preserving lateral accuracy within 1 to 3 meters. Field validation in an active work zone, using LiDAR, a radar-camera rig, and RTK-GPS as ground truth, demonstrated that the fused trajectories closely match real vehicle paths, even when single-sensor data are intermittent or degraded. These results confirm that KF based sensor fusion can reliably compensate for individual sensor limitations, providing precise and robust vehicle tracking capabilities. Our approach thus offers a practical pathway to deploy infrastructure-enabled multi-sensor systems for proactive safety measures in complex traffic environments.</li>
</ul>

<h3>Title: SimulRAG: Simulator-based RAG for Grounding LLMs in Long-form Scientific QA</h3>
<ul>
<li><strong>Authors: </strong>Haozhou Xu, Dongxia Wu, Matteo Chinazzi, Ruijia Niu, Rose Yu, Yi-An Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25459">https://arxiv.org/abs/2509.25459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25459">https://arxiv.org/pdf/2509.25459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25459]] SimulRAG: Simulator-based RAG for Grounding LLMs in Long-form Scientific QA(https://arxiv.org/abs/2509.25459)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) show promise in solving scientific problems. They can help generate long-form answers for scientific questions, which are crucial for comprehensive understanding of complex phenomena that require detailed explanations spanning multiple interconnected concepts and evidence. However, LLMs often suffer from hallucination, especially in the challenging task of long-form scientific question answering. Retrieval-Augmented Generation (RAG) approaches can ground LLMs by incorporating external knowledge sources to improve trustworthiness. In this context, scientific simulators, which play a vital role in validating hypotheses, offer a particularly promising retrieval source to mitigate hallucination and enhance answer factuality. However, existing RAG approaches cannot be directly applied for scientific simulation-based retrieval due to two fundamental challenges: how to retrieve from scientific simulators, and how to efficiently verify and update long-form answers. To overcome these challenges, we propose the simulator-based RAG framework (SimulRAG) and provide a long-form scientific QA benchmark covering climate science and epidemiology with ground truth verified by both simulations and human annotators. In this framework, we propose a generalized simulator retrieval interface to transform between textual and numerical modalities. We further design a claim-level generation method that utilizes uncertainty estimation scores and simulator boundary assessment (UE+SBA) to efficiently verify and update claims. Extensive experiments demonstrate SimulRAG outperforms traditional RAG baselines by 30.4% in informativeness and 16.3% in factuality. UE+SBA further improves efficiency and quality for claim-level generation.</li>
</ul>

<h3>Title: Managing Differentiated Secure Connectivity using Intents</h3>
<ul>
<li><strong>Authors: </strong>Loay Abdelrazek, Filippo Rebecchi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25462">https://arxiv.org/abs/2509.25462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25462">https://arxiv.org/pdf/2509.25462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25462]] Managing Differentiated Secure Connectivity using Intents(https://arxiv.org/abs/2509.25462)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Mobile networks in the 5G and 6G era require to rethink how to manage security due to the introduction of new services, use cases, each with its own security requirements, while simultaneously expanding the threat landscape. Although automation has emerged as a key enabler to address complexity in networks, existing approaches lack the expressiveness to define and enforce complex, goal-driven, and measurable security requirements. In this paper, we propose the concept of differentiated security levels and leveraging intents as a management framework. We discuss the requirements and enablers to extend the currently defined intent-based management frameworks to pave the path for intent-based security management in mobile networks. Our approach formalizes both functional and non-functional security requirements and demonstrates how these can be expressed and modeled using an extended TM Forum (TMF) intent security ontology. We further discuss the required standardization steps to achieve intent-based security management. Our work aims at advance security automation, improve adaptability, and strengthen the resilience and security posture of the next-generation mobile networks.</li>
</ul>

<h3>Title: Data-Efficient Multitask DAgger</h3>
<ul>
<li><strong>Authors: </strong>Haotian Fu, Ran Gong, Xiaohan Zhang, Maria Vittoria Minniti, Jigarkumar Patel, Karl Schmeckpeper</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25466">https://arxiv.org/abs/2509.25466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25466">https://arxiv.org/pdf/2509.25466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25466]] Data-Efficient Multitask DAgger(https://arxiv.org/abs/2509.25466)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Generalist robot policies that can perform many tasks typically require extensive expert data or simulations for training. In this work, we propose a novel Data-Efficient multitask DAgger framework that distills a single multitask policy from multiple task-specific expert policies. Our approach significantly increases the overall task success rate by actively focusing on tasks where the multitask policy underperforms. The core of our method is a performance-aware scheduling strategy that tracks how much each task's learning process benefits from the amount of data, using a Kalman filter-based estimator to robustly decide how to allocate additional demonstrations across tasks. We validate our approach on MetaWorld, as well as a suite of diverse drawer-opening tasks in IsaacLab. The resulting policy attains high performance across all tasks while using substantially fewer expert demonstrations, and the visual policy learned with our method in simulation shows better performance than naive DAgger and Behavior Cloning when transferring zero-shot to a real robot without using real data.</li>
</ul>

<h3>Title: Balancing Compliance and Privacy in Offline CBDC Transactions Using a Secure Element-based System</h3>
<ul>
<li><strong>Authors: </strong>Panagiotis Michalopoulos, Anthony Mack, Cameron Clark, Linus Chen, Johannes Sedlmeir, Andreas Veneris</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25469">https://arxiv.org/abs/2509.25469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25469">https://arxiv.org/pdf/2509.25469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25469]] Balancing Compliance and Privacy in Offline CBDC Transactions Using a Secure Element-based System(https://arxiv.org/abs/2509.25469)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect</a></li>
<li><strong>Abstract: </strong>Blockchain technology has spawned a vast ecosystem of digital currencies with Central Bank Digital Currencies (CBDCs) -- digital forms of fiat currency -- being one of them. An important feature of digital currencies is facilitating transactions without network connectivity, which can enhance the scalability of cryptocurrencies and the privacy of CBDC users. However, in the case of CBDCs, this characteristic also introduces new regulatory challenges, particularly when it comes to applying established Anti-Money Laundering and Countering the Financing of Terrorism (AML/CFT) frameworks. This paper introduces a prototype for offline digital currency payments, equally applicable to cryptocurrencies and CBDCs, that leverages Secure Elements and digital credentials to address the tension of offline payment support with regulatory compliance. Performance evaluation results suggest that the prototype can be flexibly adapted to different regulatory environments, with a transaction latency comparable to real-life commercial payment systems. Furthermore, we conceptualize how the integration of Zero-Knowledge Proofs into our design could accommodate various tiers of enhanced privacy protection.</li>
</ul>

<h3>Title: Conformal Prediction for Signal Temporal Logic Inference</h3>
<ul>
<li><strong>Authors: </strong>Danyang Li, Yixuan Wang, Matthew Cleaveland, Mingyu Cai, Roberto Tron</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25473">https://arxiv.org/abs/2509.25473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25473">https://arxiv.org/pdf/2509.25473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25473]] Conformal Prediction for Signal Temporal Logic Inference(https://arxiv.org/abs/2509.25473)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Signal Temporal Logic (STL) inference seeks to extract human-interpretable rules from time-series data, but existing methods lack formal confidence guarantees for the inferred rules. Conformal prediction (CP) is a technique that can provide statistical correctness guarantees, but is typically applied as a post-training wrapper without improving model learning. Instead, we introduce an end-to-end differentiable CP framework for STL inference that enhances both reliability and interpretability of the resulting formulas. We introduce a robustness-based nonconformity score, embed a smooth CP layer directly into training, and employ a new loss function that simultaneously optimizes inference accuracy and CP prediction sets with a single term. Following training, an exact CP procedure delivers statistical guarantees for the learned STL formulas. Experiments on benchmark time-series tasks show that our approach reduces uncertainty in predictions (i.e., it achieves high coverage while reducing prediction set size), and improves accuracy (i.e., the number of misclassifications when using a fixed threshold) over state-of-the-art baselines.</li>
</ul>

<h3>Title: Environmental Rate Manipulation Attacks on Power Grid Security</h3>
<ul>
<li><strong>Authors: </strong>Yonatan Gizachew Achamyeleh, Yang Xiang, Yun-Ping Hsiao, Yasamin Moghaddas, Mohammad Abdullah Al Faruque</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25476">https://arxiv.org/abs/2509.25476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25476">https://arxiv.org/pdf/2509.25476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25476]] Environmental Rate Manipulation Attacks on Power Grid Security(https://arxiv.org/abs/2509.25476)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>The growing complexity of global supply chains has made hardware Trojans a significant threat in sensor-based power electronics. Traditional Trojan designs depend on digital triggers or fixed threshold conditions that can be detected during standard testing. In contrast, we introduce Environmental Rate Manipulation (ERM), a novel Trojan triggering mechanism that activates by monitoring the rate of change in environmental parameters rather than their absolute values. This approach allows the Trojan to remain inactive under normal conditions and evade redundancy and sensor-fusion defenses. We implement a compact 14~$\mu$m$^2$ circuit that measures capacitor charging rates in standard sensor front-ends and disrupts inverter pulse-width modulation PWM signals when a rapid change is induced. Experiments on a commercial Texas Instruments solar inverter demonstrate that ERM can trigger catastrophic driver chip failure. Furthermore, ETAP simulations indicate that a single compromised 100~kW inverter may initiate cascading grid instabilities. The attack's significance extends beyond individual sensors to entire classes of environmental sensing systems common in power electronics, demonstrating fundamental challenges for hardware security.</li>
</ul>

<h3>Title: The Rise of AfricaNLP: Contributions, Contributors, and Community Impact (2005-2025)</h3>
<ul>
<li><strong>Authors: </strong>Tadesse Destaw Belay, Kedir Yassin Hussen, Sukairaj Hafiz Imam, Iqra Ameer, Ibrahim Said Ahmad, Isa Inuwa-Dutse, Idris Abdulmumin, Grigori Sidorov, Vukosi Marivate, Seid Muhie Yimam, Shamsuddeen Hassan Muhammad</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25477">https://arxiv.org/abs/2509.25477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25477">https://arxiv.org/pdf/2509.25477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25477]] The Rise of AfricaNLP: Contributions, Contributors, and Community Impact (2005-2025)(https://arxiv.org/abs/2509.25477)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Natural Language Processing (NLP) is undergoing constant transformation, as Large Language Models (LLMs) are driving daily breakthroughs in research and practice. In this regard, tracking the progress of NLP research and automatically analyzing the contributions of research papers provides key insights into the nature of the field and the researchers. This study explores the progress of African NLP (AfricaNLP) by asking (and answering) basic research questions such as: i) How has the nature of NLP evolved over the last two decades?, ii) What are the contributions of AfricaNLP papers?, and iii) Which individuals and organizations (authors, affiliated institutions, and funding bodies) have been involved in the development of AfricaNLP? We quantitatively examine the contributions of AfricaNLP research using 1.9K NLP paper abstracts, 4.9K author contributors, and 7.8K human-annotated contribution sentences (AfricaNLPContributions) along with benchmark results. Our dataset and continuously existing NLP progress tracking website provide a powerful lens for tracing AfricaNLP research trends and hold potential for generating data-driven literature surveys.</li>
</ul>

<h3>Title: Translation from Wearable PPG to 12-Lead ECG</h3>
<ul>
<li><strong>Authors: </strong>Hui Ji, Wei Gao, Pengfei Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25480">https://arxiv.org/abs/2509.25480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25480">https://arxiv.org/pdf/2509.25480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25480]] Translation from Wearable PPG to 12-Lead ECG(https://arxiv.org/abs/2509.25480)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The 12-lead electrocardiogram (ECG) is the gold standard for cardiovascular monitoring, offering superior diagnostic granularity and specificity compared to photoplethysmography (PPG). However, existing 12-lead ECG systems rely on cumbersome multi-electrode setups, limiting sustained monitoring in ambulatory settings, while current PPG-based methods fail to reconstruct multi-lead ECG due to the absence of inter-lead constraints and insufficient modeling of spatial-temporal dependencies across leads. To bridge this gap, we introduce P2Es, an innovative demographic-aware diffusion framework designed to generate clinically valid 12-lead ECG from PPG signals via three key innovations. Specifically, in the forward process, we introduce frequency-domain blurring followed by temporal noise interference to simulate real-world signal distortions. In the reverse process, we design a temporal multi-scale generation module followed by frequency deblurring. In particular, we leverage KNN-based clustering combined with contrastive learning to assign affinity matrices for the reverse process, enabling demographic-specific ECG translation. Extensive experimental results show that P2Es outperforms baseline models in 12-lead ECG reconstruction.</li>
</ul>

<h3>Title: Not Wrong, But Untrue: LLM Overconfidence in Document-Based Queries</h3>
<ul>
<li><strong>Authors: </strong>Nick Hagar, Wilma Agustianto, Nicholas Diakopoulos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25498">https://arxiv.org/abs/2509.25498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25498">https://arxiv.org/pdf/2509.25498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25498]] Not Wrong, But Untrue: LLM Overconfidence in Document-Based Queries(https://arxiv.org/abs/2509.25498)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used in newsroom workflows, but their tendency to hallucinate poses risks to core journalistic practices of sourcing, attribution, and accuracy. We evaluate three widely used tools - ChatGPT, Gemini, and NotebookLM - on a reporting-style task grounded in a 300-document corpus related to TikTok litigation and policy in the U.S. We vary prompt specificity and context size and annotate sentence-level outputs using a taxonomy to measure hallucination type and severity. Across our sample, 30% of model outputs contained at least one hallucination, with rates approximately three times higher for Gemini and ChatGPT (40%) than for NotebookLM (13%). Qualitatively, most errors did not involve invented entities or numbers; instead, we observed interpretive overconfidence - models added unsupported characterizations of sources and transformed attributed opinions into general statements. These patterns reveal a fundamental epistemological mismatch: While journalism requires explicit sourcing for every claim, LLMs generate authoritative-sounding text regardless of evidentiary support. We propose journalism-specific extensions to existing hallucination taxonomies and argue that effective newsroom tools need architectures that enforce accurate attribution rather than optimize for fluency.</li>
</ul>

<h3>Title: Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Kaiqing Lin, Zhiyuan Yan, Ruoxin Chen, Junyan Ye, Ke-Yue Zhang, Yue Zhou, Peng Jin, Bin Li, Taiping Yao, Shouhong Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25502">https://arxiv.org/abs/2509.25502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25502">https://arxiv.org/pdf/2509.25502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25502]] Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection(https://arxiv.org/abs/2509.25502)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Detecting AI-generated images with multimodal large language models (MLLMs) has gained increasing attention, due to their rich world knowledge, common-sense reasoning, and potential for explainability. However, naively applying those MLLMs for detection often leads to suboptimal performance. We argue that the root of this failure lies in a fundamental mismatch: MLLMs are asked to reason about fakes before they can truly see them. First, they do not really see: existing MLLMs' vision encoders are primarily optimized for semantic-oriented recognition rather than the perception of low-level signals, leaving them insensitive to subtle forgery traces. Without access to reliable perceptual evidence, the model grounds its judgment on incomplete and limited visual observations. Second, existing finetuning data for detection typically uses narrow, instruction-style formats, which diverge sharply from the diverse, heterogeneous distributions seen in pretraining. In the absence of meaningful visual cues, the model therefore exploits these linguistic shortcuts, resulting in catastrophic forgetting of pretrained knowledge (even the basic dialogue capabilities). In response, we advocate for a new paradigm: seeing before reasoning. We propose that MLLMs should first be trained to perceive artifacts-strengthening their artifact-aware visual perception-so that subsequent reasoning is grounded in actual observations. We therefore propose Forensic-Chat, a generalizable, explainable, and still-conversational (for multi-round dialogue) assistant for fake image detection. We also propose ExplainFake-Bench, a benchmark tailored for the evaluation of the MLLM's explainability for image forensics from five key aspects. Extensive experiments show its superiority of generalization and genuinely reliable explainability.</li>
</ul>

<h3>Title: DeepFake Detection in Dyadic Video Calls using Point of Gaze Tracking</h3>
<ul>
<li><strong>Authors: </strong>Odin Kohler, Rahul Vijaykumar, Masudul H. Imtiaz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25503">https://arxiv.org/abs/2509.25503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25503">https://arxiv.org/pdf/2509.25503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25503]] DeepFake Detection in Dyadic Video Calls using Point of Gaze Tracking(https://arxiv.org/abs/2509.25503)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, biometric</a></li>
<li><strong>Abstract: </strong>With recent advancements in deepfake technology, it is now possible to generate convincing deepfakes in real-time. Unfortunately, malicious actors have started to use this new technology to perform real-time phishing attacks during video meetings. The nature of a video call allows access to what the deepfake is ``seeing,'' that is, the screen displayed to the malicious actor. Using this with the estimated gaze from the malicious actors streamed video enables us to estimate where the deepfake is looking on screen, the point of gaze. Because the point of gaze during conversations is not random and is instead used as a subtle nonverbal communicator, it can be used to detect deepfakes, which are not capable of mimicking this subtle nonverbal communication. This paper proposes a real-time deepfake detection method adapted to this genre of attack, utilizing previously unavailable biometric information. We built our model based on explainable features selected after careful review of research on gaze patterns during dyadic conversations. We then test our model on a novel dataset of our creation, achieving an accuracy of 82\%. This is the first reported method to utilize point-of-gaze tracking for deepfake detection.</li>
</ul>

<h3>Title: EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit</h3>
<ul>
<li><strong>Authors: </strong>Chang Liu, Danial Chitnis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25510">https://arxiv.org/abs/2509.25510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25510">https://arxiv.org/pdf/2509.25510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25510]] EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit(https://arxiv.org/abs/2509.25510)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often involves significant manual effort, especially during the transistor sizing process. While Machine Learning techniques in Electronic Design Automation (EDA) have shown promise in reducing complexity and minimizing human intervention, they still face challenges such as numerous iterations and a lack of knowledge about AMS circuit design. Recently, Large Language Models (LLMs) have demonstrated significant potential across various fields, showing a certain level of knowledge in circuit design and indicating their potential to automate the transistor sizing process. In this work, we propose EEsizer, an LLM-based AI agent that integrates large language models with circuit simulators and custom data analysis functions, enabling fully automated, closed-loop transistor sizing without relying on external knowledge. By employing prompt engineering and Chain-of-Thought reasoning, the agent iteratively explores design directions, evaluates performance, and refines solutions with minimal human intervention. We first benchmarked 8 LLMs on six basic circuits and selected three high-performing models to optimize a 20-transistor CMOS operational amplifier, targeting multiple performance metrics, including rail-to-rail operation from 180 nm to 90 nm technology nodes. Notably, OpenAI o3 successfully achieved the user-intended target at 90 nm across three different test groups, with a maximum of 20 iterations, demonstrating adaptability and robustness at advanced nodes. To assess design robustness, we manually designed a bias circuit and performed a variation analysis using Gaussian-distributed variations on transistor dimensions and threshold voltages.</li>
</ul>

<h3>Title: Beyond WER: Probing Whisper's Sub-token Decoder Across Diverse Language Resource Levels</h3>
<ul>
<li><strong>Authors: </strong>Siyu Liang, Nicolas Ballier, Gina-Anne Levow, Richard Wright</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25516">https://arxiv.org/abs/2509.25516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25516">https://arxiv.org/pdf/2509.25516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25516]] Beyond WER: Probing Whisper's Sub-token Decoder Across Diverse Language Resource Levels(https://arxiv.org/abs/2509.25516)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>While large multilingual automatic speech recognition (ASR) models achieve remarkable performance, the internal mechanisms of the end-to-end pipeline, particularly concerning fairness and efficacy across languages, remain underexplored. This paper introduces a fine-grained analysis of Whisper's multilingual decoder, examining its sub-token hypotheses during transcription across languages with various resource levels. Our method traces the beam search path, capturing sub-token guesses and their associated probabilities. Results reveal that higher resource languages benefit from higher likelihood of the correct token being top-ranked, greater confidence, lower predictive entropy, and more diverse alternative candidates. Lower resource languages fare worse on these metrics, but also exhibit distinct clustering patterns in sub-token usage sometimes influenced by typology in our PCA and t-SNE analysis. This sub-token probing uncovers systematic decoding disparities masked by aggregate error rates and points towards targeted interventions to ameliorate the imbalanced development of speech technology.</li>
</ul>

<h3>Title: Robust Visual Localization in Compute-Constrained Environments by Salient Edge Rendering and Weighted Hamming Similarity</h3>
<ul>
<li><strong>Authors: </strong>Tu-Hoa Pham, Philip Bailey, Daniel Posada, Georgios Georgakis, Jorge Enriquez, Surya Suresh, Marco Dolci, Philip Twu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25520">https://arxiv.org/abs/2509.25520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25520">https://arxiv.org/pdf/2509.25520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25520]] Robust Visual Localization in Compute-Constrained Environments by Salient Edge Rendering and Weighted Hamming Similarity(https://arxiv.org/abs/2509.25520)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We consider the problem of vision-based 6-DoF object pose estimation in the context of the notional Mars Sample Return campaign, in which a robotic arm would need to localize multiple objects of interest for low-clearance pickup and insertion, under severely constrained hardware. We propose a novel localization algorithm leveraging a custom renderer together with a new template matching metric tailored to the edge domain to achieve robust pose estimation using only low-fidelity, textureless 3D models as inputs. Extensive evaluations on synthetic datasets as well as from physical testbeds on Earth and in situ Mars imagery shows that our method consistently beats the state of the art in compute and memory-constrained localization, both in terms of robustness and accuracy, in turn enabling new possibilities for cheap and reliable localization on general-purpose hardware.</li>
</ul>

<h3>Title: Defeating Cerberus: Concept-Guided Privacy-Leakage Mitigation in Multimodal Language Models</h3>
<ul>
<li><strong>Authors: </strong>Boyang Zhang, Istemi Ekin Akkus, Ruichuan Chen, Alice Dethise, Klaus Satzke, Ivica Rimac, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25525">https://arxiv.org/abs/2509.25525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25525">https://arxiv.org/pdf/2509.25525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25525]] Defeating Cerberus: Concept-Guided Privacy-Leakage Mitigation in Multimodal Language Models(https://arxiv.org/abs/2509.25525)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in processing and reasoning over diverse modalities, but their advanced abilities also raise significant privacy concerns, particularly regarding Personally Identifiable Information (PII) leakage. While relevant research has been conducted on single-modal language models to some extent, the vulnerabilities in the multimodal setting have yet to be fully investigated. In this work, we investigate these emerging risks with a focus on vision language models (VLMs), a representative subclass of MLLMs that covers the two modalities most relevant for PII leakage, vision and text. We introduce a concept-guided mitigation approach that identifies and modifies the model's internal states associated with PII-related content. Our method guides VLMs to refuse PII-sensitive tasks effectively and efficiently, without requiring re-training or fine-tuning. We also address the current lack of multimodal PII datasets by constructing various ones that simulate real-world scenarios. Experimental results demonstrate that the method can achieve an average refusal rate of 93.3% for various PII-related tasks with minimal impact on unrelated model performances. We further examine the mitigation's performance under various conditions to show the adaptability of our proposed method.</li>
</ul>

<h3>Title: LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pranav Saxena, Avigyan Bhattacharya, Ji Zhang, Wenshan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25528">https://arxiv.org/abs/2509.25528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25528">https://arxiv.org/pdf/2509.25528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25528]] LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models(https://arxiv.org/abs/2509.25528)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Referential grounding in outdoor driving scenes is challenging due to large scene variability, many visually similar objects, and dynamic elements that complicate resolving natural-language references (e.g., "the black car on the right"). We propose LLM-RG, a hybrid pipeline that combines off-the-shelf vision-language models for fine-grained attribute extraction with large language models for symbolic reasoning. LLM-RG processes an image and a free-form referring expression by using an LLM to extract relevant object types and attributes, detecting candidate regions, generating rich visual descriptors with a VLM, and then combining these descriptors with spatial metadata into natural-language prompts that are input to an LLM for chain-of-thought reasoning to identify the referent's bounding box. Evaluated on the Talk2Car benchmark, LLM-RG yields substantial gains over both LLM and VLM-based baselines. Additionally, our ablations show that adding 3D spatial cues further improves grounding. Our results demonstrate the complementary strengths of VLMs and LLMs, applied in a zero-shot manner, for robust outdoor referential grounding.</li>
</ul>

<h3>Title: Calibrating Verbalized Confidence with Self-Generated Distractors</h3>
<ul>
<li><strong>Authors: </strong>Victor Wang, Elias Stengel-Eskin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25532">https://arxiv.org/abs/2509.25532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25532">https://arxiv.org/pdf/2509.25532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25532]] Calibrating Verbalized Confidence with Self-Generated Distractors(https://arxiv.org/abs/2509.25532)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Calibrated confidence estimates are necessary for large language model (LLM) outputs to be trusted by human users. While LLMs can express their confidence in human-interpretable ways, verbalized LLM-generated confidence scores have empirically been found to be miscalibrated, reporting high confidence on instances with low accuracy and thereby harming trust and safety. We hypothesize that this overconfidence often stems from a given LLM's heightened suggestibility when faced with claims that it encodes little information about; we empirically validate this hypothesis, finding more suggestibility on lower-accuracy claims. Building on this finding, we introduce Distractor-Normalized Coherence (DINCO), which estimates and accounts for an LLM's suggestibility bias by having the model verbalize its confidence independently across several self-generated distractors (i.e. alternative claims), and normalizes by the total verbalized confidence. To further improve calibration, we leverage generator-validator disagreement, augmenting normalized validator confidence with a consistency-based estimate of generator confidence. Here, we frame the popular approach of self-consistency as leveraging coherence across sampled generations, and normalized verbalized confidence as leveraging coherence across validations on incompatible claims, allowing us to integrate these complementary dimensions of coherence into DINCO. Moreover, our analysis shows that DINCO provides less saturated -- and therefore more usable -- confidence estimates, and that further sampling alone cannot close the gap between DINCO and baselines, with DINCO at 10 inference calls outperforming self-consistency at 100.</li>
</ul>

<h3>Title: Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zhiling Ye, Yun Yue, Haowen Wang, Xudong Han, Jiadi Jiang, Cheng Wei, Lei Fan, Jiaxin Liang, Shuowen Zhang, Ji Li, Chunxiao Guo, Jian Wang, Peng Wei, Jinjie Gu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25534">https://arxiv.org/abs/2509.25534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25534">https://arxiv.org/pdf/2509.25534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25534]] Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning(https://arxiv.org/abs/2509.25534)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Open-ended evaluation is essential for deploying large language models in real-world settings. In studying HealthBench, we observe that using the model itself as a grader and generating rubric-based reward signals substantially improves reasoning performance. Remarkably, the trained model also becomes a stronger grader. Motivated by this, we introduce Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning, a lightweight framework that enables faster and more resource-efficient training while surpassing baselines. Remarkably, on Qwen3-32B, training with just the 4000-sample HealthBench Easy subset is sufficient to obtain a model that exceeds GPT-5 on HealthBench Hard. Incorporating a small amount of teacher-graded data further enhances performance for less capable models.</li>
</ul>

<h3>Title: Meta-Router: Bridging Gold-standard and Preference-based Evaluations in Large Language Model Routing</h3>
<ul>
<li><strong>Authors: </strong>Yichi Zhang, Fangzheng Xie, Shu Yang, Chong Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25535">https://arxiv.org/abs/2509.25535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25535">https://arxiv.org/pdf/2509.25535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25535]] Meta-Router: Bridging Gold-standard and Preference-based Evaluations in Large Language Model Routing(https://arxiv.org/abs/2509.25535)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In language tasks that require extensive human--model interaction, deploying a single "best" model for every query can be expensive. To reduce inference cost while preserving the quality of the responses, a large language model (LLM) router selects the most appropriate model from a pool of candidates for each query. A central challenge to training a high-quality router is the scarcity of reliable supervision. Gold-standard data (e.g., expert-verified labels or rubric-based scores) provide accurate quality evaluations of LLM responses but are costly and difficult to scale. In contrast, preference-based data, collected via crowdsourcing or LLM-as-a-judge systems, are cheaper and more scalable, yet often biased in reflecting the true quality of responses. We cast the problem of LLM router training with combined gold-standard and preference-based data into a causal inference framework by viewing the response evaluation mechanism as the treatment assignment. This perspective further reveals that the bias in preference-based data corresponds to the well-known causal estimand: the conditional average treatment effect. Based on this new perspective, we develop an integrative causal router training framework that corrects preference-data bias, address imbalances between two data sources, and improve routing robustness and efficiency. Numerical experiments demonstrate that our approach delivers more accurate routing and improves the trade-off between cost and quality.</li>
</ul>

<h3>Title: Steering an Active Learning Workflow Towards Novel Materials Discovery via Queue Prioritization</h3>
<ul>
<li><strong>Authors: </strong>Marcus Schwarting, Logan Ward, Nathaniel Hudson, Xiaoli Yan, Ben Blaiszik, Santanu Chaudhuri, Eliu Huerta, Ian Foster</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25538">https://arxiv.org/abs/2509.25538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25538">https://arxiv.org/pdf/2509.25538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25538]] Steering an Active Learning Workflow Towards Novel Materials Discovery via Queue Prioritization(https://arxiv.org/abs/2509.25538)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI poses both opportunities and risks for solving inverse design problems in the sciences. Generative tools provide the ability to expand and refine a search space autonomously, but do so at the cost of exploring low-quality regions until sufficiently fine tuned. Here, we propose a queue prioritization algorithm that combines generative modeling and active learning in the context of a distributed workflow for exploring complex design spaces. We find that incorporating an active learning model to prioritize top design candidates can prevent a generative AI workflow from expending resources on nonsensical candidates and halt potential generative model decay. For an existing generative AI workflow for discovering novel molecular structure candidates for carbon capture, our active learning approach significantly increases the number of high-quality candidates identified by the generative model. We find that, out of 1000 novel candidates, our workflow without active learning can generate an average of 281 high-performing candidates, while our proposed prioritization with active learning can generate an average 604 high-performing candidates.</li>
</ul>

<h3>Title: Aligning Multilingual Reasoning with Verifiable Semantics from a High-Resource Expert Model</h3>
<ul>
<li><strong>Authors: </strong>Fahim Faisal, Kaiqiang Song, Song Wang, Simin Ma, Shujian Liu, Haoyun Deng, Sathish Reddy Indurthi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25543">https://arxiv.org/abs/2509.25543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25543">https://arxiv.org/pdf/2509.25543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25543]] Aligning Multilingual Reasoning with Verifiable Semantics from a High-Resource Expert Model(https://arxiv.org/abs/2509.25543)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While reinforcement learning has advanced the reasoning abilities of Large Language Models (LLMs), these gains are largely confined to English, creating a significant performance disparity across languages. To address this, we introduce Pivot-Based Reinforcement Learning with Semantically Verifiable Rewards (PB-RLSVR), a novel framework that enhances multilingual reasoning by circumventing the need for human-annotated data in target languages. Our approach employs a high-performing English LLM as a "pivot" model to generate reference responses for reasoning tasks. A multilingual model is then rewarded based on the semantic equivalence of its responses to the English reference, effectively transferring the pivot model's reasoning capabilities across languages. We investigate several cross-lingual semantic reward functions, including those based on embeddings and machine translation. Extensive experiments on a suite of multilingual reasoning benchmarks show that our method significantly narrows the performance gap between English and other languages, substantially outperforming traditional PPO baselines. Specifically, our PB-RLSVR framework improves the average multilingual performance of Llama-3.1-8B-Instruct and Qwen3-32B by 16.41% and 10.17%, respectively, demonstrating a powerful and data-efficient approach to building truly multilingual reasoning agents.</li>
</ul>

<h3>Title: Don't Sweat the Small Stuff: Segment-Level Meta-Evaluation Based on Pairwise Difference Correlation</h3>
<ul>
<li><strong>Authors: </strong>Colten DiIanni, Daniel Deutsch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25546">https://arxiv.org/abs/2509.25546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25546">https://arxiv.org/pdf/2509.25546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25546]] Don't Sweat the Small Stuff: Segment-Level Meta-Evaluation Based on Pairwise Difference Correlation(https://arxiv.org/abs/2509.25546)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper introduces Pairwise Difference Pearson (PDP), a novel segment-level meta-evaluation metric for Machine Translation (MT) that address limitations in previous Pearson's $\rho$-based and and Kendall's $\tau$-based meta-evaluation approaches. PDP is a correlation-based metric that utilizes pairwise differences rather than raw scores. It draws on information from all segments for a more robust understanding of score distributions and uses segment-wise pairwise differences to refine Global Pearson to intra-segment score comparisons. Analysis on the WMT'24 shared task shows PDP properly ranks sentinel evaluation metrics and better aligns with human error weightings than previous work. Noise injection analysis demonstrates PDP's robustness to random noise, segment bias, and system bias while highlighting its sensitivity to extreme outliers.</li>
</ul>

<h3>Title: Hybrid Approach for Enhancing Lesion Segmentation in Fundus Images</h3>
<ul>
<li><strong>Authors: </strong>Mohammadmahdi Eshragh, Emad A. Mohammed, Behrouz Far, Ezekiel Weis, Carol L Shields, Sandor R Ferenczy, Trafford Crump</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25549">https://arxiv.org/abs/2509.25549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25549">https://arxiv.org/pdf/2509.25549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25549]] Hybrid Approach for Enhancing Lesion Segmentation in Fundus Images(https://arxiv.org/abs/2509.25549)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Choroidal nevi are common benign pigmented lesions in the eye, with a small risk of transforming into melanoma. Early detection is critical to improving survival rates, but misdiagnosis or delayed diagnosis can lead to poor outcomes. Despite advancements in AI-based image analysis, diagnosing choroidal nevi in colour fundus images remains challenging, particularly for clinicians without specialized expertise. Existing datasets often suffer from low resolution and inconsistent labelling, limiting the effectiveness of segmentation models. This paper addresses the challenge of achieving precise segmentation of fundus lesions, a critical step toward developing robust diagnostic tools. While deep learning models like U-Net have demonstrated effectiveness, their accuracy heavily depends on the quality and quantity of annotated data. Previous mathematical/clustering segmentation methods, though accurate, required extensive human input, making them impractical for medical applications. This paper proposes a novel approach that combines mathematical/clustering segmentation models with insights from U-Net, leveraging the strengths of both methods. This hybrid model improves accuracy, reduces the need for large-scale training data, and achieves significant performance gains on high-resolution fundus images. The proposed model achieves a Dice coefficient of 89.7% and an IoU of 80.01% on 1024*1024 fundus images, outperforming the Attention U-Net model, which achieved 51.3% and 34.2%, respectively. It also demonstrated better generalizability on external datasets. This work forms a part of a broader effort to develop a decision support system for choroidal nevus diagnosis, with potential applications in automated lesion annotation to enhance the speed and accuracy of diagnosis and monitoring.</li>
</ul>

<h3>Title: Lightweight and Robust Federated Data Valuation</h3>
<ul>
<li><strong>Authors: </strong>Guojun Tang, Jiayu Zhou, Mohammad Mamun, Steve Drew</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25560">https://arxiv.org/abs/2509.25560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25560">https://arxiv.org/pdf/2509.25560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25560]] Lightweight and Robust Federated Data Valuation(https://arxiv.org/abs/2509.25560)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) faces persistent robustness challenges due to non-IID data distributions and adversarial client behavior. A promising mitigation strategy is contribution evaluation, which enables adaptive aggregation by quantifying each client's utility to the global model. However, state-of-the-art Shapley-value-based approaches incur high computational overhead due to repeated model reweighting and inference, which limits their scalability. We propose FedIF, a novel FL aggregation framework that leverages trajectory-based influence estimation to efficiently compute client contributions. FedIF adapts decentralized FL by introducing normalized and smoothed influence scores computed from lightweight gradient operations on client updates and a public validation set. Theoretical analysis demonstrates that FedIF yields a tighter bound on one-step global loss change under noisy conditions. Extensive experiments on CIFAR-10 and Fashion-MNIST show that FedIF achieves robustness comparable to or exceeding SV-based methods in the presence of label noise, gradient noise, and adversarial samples, while reducing aggregation overhead by up to 450x. Ablation studies confirm the effectiveness of FedIF's design choices, including local weight normalization and influence smoothing. Our results establish FedIF as a practical, theoretically grounded, and scalable alternative to Shapley-value-based approaches for efficient and robust FL in real-world deployments.</li>
</ul>

<h3>Title: FishNet++: Analyzing the capabilities of Multimodal Large Language Models in marine biology</h3>
<ul>
<li><strong>Authors: </strong>Faizan Farooq Khan, Yousef Radwan, Eslam Abdelrahman, Abdulwahab Felemban, Aymen Mir, Nico K. Michiels, Andrew J. Temple, Michael L. Berumen, Mohamed Elhoseiny</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25564">https://arxiv.org/abs/2509.25564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25564">https://arxiv.org/pdf/2509.25564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25564]] FishNet++: Analyzing the capabilities of Multimodal Large Language Models in marine biology(https://arxiv.org/abs/2509.25564)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have demonstrated impressive cross-domain capabilities, yet their proficiency in specialized scientific fields like marine biology remains underexplored. In this work, we systematically evaluate state-of-the-art MLLMs and reveal significant limitations in their ability to perform fine-grained recognition of fish species, with the best open-source models achieving less than 10\% accuracy. This task is critical for monitoring marine ecosystems under anthropogenic pressure. To address this gap and investigate whether these failures stem from a lack of domain knowledge, we introduce FishNet++, a large-scale, multimodal benchmark. FishNet++ significantly extends existing resources with 35,133 textual descriptions for multimodal learning, 706,426 key-point annotations for morphological studies, and 119,399 bounding boxes for detection. By providing this comprehensive suite of annotations, our work facilitates the development and evaluation of specialized vision-language models capable of advancing aquatic science.</li>
</ul>

<h3>Title: Zero Trust-based Decentralized Identity Management System for Autonomous Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Amal Yousseef, Shalaka Satam, Banafsheh Saber Latibari, Mai Abdel-Malek, Soheil Salehi, Pratik Satam</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25566">https://arxiv.org/abs/2509.25566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25566">https://arxiv.org/pdf/2509.25566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25566]] Zero Trust-based Decentralized Identity Management System for Autonomous Vehicles(https://arxiv.org/abs/2509.25566)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust</a></li>
<li><strong>Abstract: </strong>The rise of autonomous vehicles (AVs) promises to significantly enhance transportation safety and efficiency by mitigating human error, which is responsible for over 90\% of road accidents. However, the increasing connectivity of AVs introduces new cybersecurity challenges, as traditional perimeter-based security models are inadequate for dynamic and untrusted environments. This paper presents a novel Zero Trust-based Decentralized Identity Management (D-IM) protocol for AVs. By integrating the core principles of Zero Trust Architecture, "never trust, always verify", with the tamper resistant and decentralized nature of a blockchain network, our framework eliminates reliance on centralized authorities and provides continuous verification for every entity. We detail the system's design, which leverages Hyperledger Iroha to enable lightweight and secure authentication without a central trusted entity. A comprehensive experimental evaluation, conducted across both urban and highway scenarios, validates the protocol's practicality. Our results demonstrate that the D-IM framework introduces minimal overhead, with less than 7.5\% reduction in Packet Reception Rate (PRR) in urban settings and an increase of under 11\% in Channel Busy Ratio (CBR) for LTE-V2X. These findings prove the protocol's efficiency and robustness, providing a resilient foundation for securing real-time V2X communication against impersonation and replay attacks.</li>
</ul>

<h3>Title: Probing the Limits of Stylistic Alignment in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Asma Farajidizaji, Akash Gupta, Vatsal Raina</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25568">https://arxiv.org/abs/2509.25568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25568">https://arxiv.org/pdf/2509.25568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25568]] Probing the Limits of Stylistic Alignment in Vision-Language Models(https://arxiv.org/abs/2509.25568)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision-language models are increasingly used to generate image captions in specific styles, such as humor or romantic. However, these transformer-based models often struggle with this subjective task in a zero-shot setting. While preference data can be used to align them toward a desired style, such data is expensive to acquire, limiting the ability to explore the models' full capabilities. This work addresses this by studying the data efficiency of aligning small vision-language models to humor and romantic styles. This approach helps to define the performance limits of these models and determine how little preference data is needed to achieve stylistic saturation, benchmarking their capabilities and limitations.</li>
</ul>

<h3>Title: AttentionViG: Cross-Attention-Based Dynamic Neighbor Aggregation in Vision GNNs</h3>
<ul>
<li><strong>Authors: </strong>Hakan Emre Gedik, Andrew Martin, Mustafa Munir, Oguzhan Baser, Radu Marculescu, Sandeep P. Chinchali, Alan C. Bovik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25570">https://arxiv.org/abs/2509.25570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25570">https://arxiv.org/pdf/2509.25570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25570]] AttentionViG: Cross-Attention-Based Dynamic Neighbor Aggregation in Vision GNNs(https://arxiv.org/abs/2509.25570)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision Graph Neural Networks (ViGs) have demonstrated promising performance in image recognition tasks against Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). An essential part of the ViG framework is the node-neighbor feature aggregation method. Although various graph convolution methods, such as Max-Relative, EdgeConv, GIN, and GraphSAGE, have been explored, a versatile aggregation method that effectively captures complex node-neighbor relationships without requiring architecture-specific refinements is needed. To address this gap, we propose a cross-attention-based aggregation method in which the query projections come from the node, while the key projections come from its neighbors. Additionally, we introduce a novel architecture called AttentionViG that uses the proposed cross-attention aggregation scheme to conduct non-local message passing. We evaluated the image recognition performance of AttentionViG on the ImageNet-1K benchmark, where it achieved SOTA performance. Additionally, we assessed its transferability to downstream tasks, including object detection and instance segmentation on MS COCO 2017, as well as semantic segmentation on ADE20K. Our results demonstrate that the proposed method not only achieves strong performance, but also maintains efficiency, delivering competitive accuracy with comparable FLOPs to prior vision GNN architectures.</li>
</ul>

<h3>Title: Machine Learning Algorithms for Improving Black Box Optimization Solvers</h3>
<ul>
<li><strong>Authors: </strong>Morteza Kimiaei, Vyacheslav Kungurtsev</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25592">https://arxiv.org/abs/2509.25592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25592">https://arxiv.org/pdf/2509.25592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25592]] Machine Learning Algorithms for Improving Black Box Optimization Solvers(https://arxiv.org/abs/2509.25592)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Black-box optimization (BBO) addresses problems where objectives are accessible only through costly queries without gradients or explicit structure. Classical derivative-free methods -- line search, direct search, and model-based solvers such as Bayesian optimization -- form the backbone of BBO, yet often struggle in high-dimensional, noisy, or mixed-integer settings. Recent advances use machine learning (ML) and reinforcement learning (RL) to enhance BBO: ML provides expressive surrogates, adaptive updates, meta-learning portfolios, and generative models, while RL enables dynamic operator configuration, robustness, and meta-optimization across tasks. This paper surveys these developments, covering representative algorithms such as NNs with the modular model-based optimization framework (mlrMBO), zeroth-order adaptive momentum methods (ZO-AdaMM), automated BBO (ABBO), distributed block-wise optimization (DiBB), partition-based Bayesian optimization (SPBOpt), the transformer-based optimizer (B2Opt), diffusion-model-based BBO, surrogate-assisted RL for differential evolution (Surr-RLDE), robust BBO (RBO), coordinate-ascent model-based optimization with relative entropy (CAS-MORE), log-barrier stochastic gradient descent (LB-SGD), policy improvement with black-box (PIBB), and offline Q-learning with Mamba backbones (Q-Mamba). We also review benchmark efforts such as the NeurIPS 2020 BBO Challenge and the MetaBox framework. Overall, we highlight how ML and RL transform classical inexact solvers into more scalable, robust, and adaptive frameworks for real-world optimization.</li>
</ul>

<h3>Title: K-Prism: A Knowledge-Guided and Prompt Integrated Universal Medical Image Segmentation Model</h3>
<ul>
<li><strong>Authors: </strong>Bangwei Guo, Yunhe Gao, Meng Ye, Difei Gu, Yang Zhou, Leon Axel, Dimitris Metaxas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25594">https://arxiv.org/abs/2509.25594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25594">https://arxiv.org/pdf/2509.25594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25594]] K-Prism: A Knowledge-Guided and Prompt Integrated Universal Medical Image Segmentation Model(https://arxiv.org/abs/2509.25594)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation is fundamental to clinical decision-making, yet existing models remain fragmented. They are usually trained on single knowledge sources and specific to individual tasks, modalities, or organs. This fragmentation contrasts sharply with clinical practice, where experts seamlessly integrate diverse knowledge: anatomical priors from training, exemplar-based reasoning from reference cases, and iterative refinement through real-time interaction. We present $\textbf{K-Prism}$, a unified segmentation framework that mirrors this clinical flexibility by systematically integrating three knowledge paradigms: (i) $\textit{semantic priors}$ learned from annotated datasets, (ii) $\textit{in-context knowledge}$ from few-shot reference examples, and (iii) $\textit{interactive feedback}$ from user inputs like clicks or scribbles. Our key insight is that these heterogeneous knowledge sources can be encoded into a dual-prompt representation: 1-D sparse prompts defining $\textit{what}$ to segment and 2-D dense prompts indicating $\textit{where}$ to attend, which are then dynamically routed through a Mixture-of-Experts (MoE) decoder. This design enables flexible switching between paradigms and joint training across diverse tasks without architectural modifications. Comprehensive experiments on 18 public datasets spanning diverse modalities (CT, MRI, X-ray, pathology, ultrasound, etc.) demonstrate that K-Prism achieves state-of-the-art performance across semantic, in-context, and interactive segmentation settings. Code will be released upon publication.</li>
</ul>

<h3>Title: Binary Sparse Coding for Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Lucia Quirke, Stepan Shabalin, Nora Belrose</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25596">https://arxiv.org/abs/2509.25596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25596">https://arxiv.org/pdf/2509.25596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25596]] Binary Sparse Coding for Interpretability(https://arxiv.org/abs/2509.25596)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Sparse autoencoders (SAEs) are used to decompose neural network activations into sparsely activating features, but many SAE features are only interpretable at high activation strengths. To address this issue we propose to use binary sparse autoencoders (BAEs) and binary transcoders (BTCs), which constrain all activations to be zero or one. We find that binarisation significantly improves the interpretability and monosemanticity of the discovered features, while increasing reconstruction error. By eliminating the distinction between high and low activation strengths, we prevent uninterpretable information from being smuggled in through the continuous variation in feature activations. However, we also find that binarisation increases the number of uninterpretable ultra-high frequency features, and when interpretability scores are frequency-adjusted, the scores for continuous sparse coders are slightly better than those of binary ones. This suggests that polysemanticity may be an ineliminable property of neural activations.</li>
</ul>

<h3>Title: RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance</h3>
<ul>
<li><strong>Authors: </strong>Tianlang Chen, Minkai Xu, Jure Leskovec, Stefano Ermon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25604">https://arxiv.org/abs/2509.25604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25604">https://arxiv.org/pdf/2509.25604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25604]] RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance(https://arxiv.org/abs/2509.25604)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (dLLMs) have shown great potential in large-scale language modeling, and there is an increasing interest in further improving the capacity to solve complex problems by guiding the reasoning process step by step. Common practice for autoregressive language models typically learns a process reward model with dense annotation for each intermediate step. However, this is challenging for dLLMs where the generation is in an any-order fashion and intermediate states are partially masked sentences. To this end, in this paper, we propose reward-free guidance (RFG), a principled method for guiding the reasoning trajectory of dLLMs without explicit process reward. The key idea of RFG is to parameterize the process reward by log-likelihood ratios of the enhanced and reference dLLMs, where the enhanced model can be easily obtained by any off-the-shelf dLLM that has been post-trained with reinforcement learning (RL) or supervised fine-tuning (SFT). We provide theoretical justification that RFG induces the reward-guided sampling distribution with no additional reward. We conduct comprehensive experiments on four challenging mathematical reasoning and code generation benchmarks using a diverse suite of dLLMs enhanced with various post-training methods. RFG consistently yields significant improvements across all tasks and model types, achieving accuracy gains of up to 9.2%. These findings establish RFG as a general training-free framework that scales test-time reasoning without reliance on external reward models.</li>
</ul>

<h3>Title: Effective Model Pruning</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Wang, Dan Guralnik, Saiedeh Akbari, Warren Dixon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25606">https://arxiv.org/abs/2509.25606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25606">https://arxiv.org/pdf/2509.25606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25606]] Effective Model Pruning(https://arxiv.org/abs/2509.25606)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>We introduce Effective Model Pruning (EMP), a context-agnostic, parameter-free rule addressing a fundamental question about pruning: how many entries to keep. EMP does not prescribe how to score the parameters or prune the models; instead, it supplies a universal adaptive threshold that can be applied to any pruning criterion: weight magnitude, attention score, KAN importance score, or even feature-level signals such as image pixel, and used on structural parts or weights of the models. Given any score vector s, EMP maps s to a built-in effective number N_eff which is inspired by the Inverse Simpson index of contributors. Retaining the N_eff highest scoring entries and zeroing the remainder yields sparse models with performance comparable to the original dense networks across MLPs, CNNs, Transformers/LLMs, and KAN, in our experiments. By leveraging the geometry of the simplex, we derive a tight lower bound on the preserved mass s_eff (the sum of retained scores) over the corresponding ordered probability simplex associated with the score vector s. We further verify the effectiveness of N_eff by pruning the model with a scaled threshold \b{eta}*N_eff across a variety of criteria and models. Experiments suggest that the default \b{eta} = 1 yields a robust threshold for model pruning while \b{eta} not equal to 1 still serves as an optional adjustment to meet specific sparsity requirements.</li>
</ul>

<h3>Title: Transformers through the lens of support-preserving maps between measures</h3>
<ul>
<li><strong>Authors: </strong>Takashi Furuya, Maarten V. de Hoop, Matti Lassas</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25611">https://arxiv.org/abs/2509.25611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25611">https://arxiv.org/pdf/2509.25611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25611]] Transformers through the lens of support-preserving maps between measures(https://arxiv.org/abs/2509.25611)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers are deep architectures that define ``in-context maps'' which enable predicting new tokens based on a given set of tokens (such as a prompt in NLP applications or a set of patches for a vision transformer). In previous work, we studied the ability of these architectures to handle an arbitrarily large number of context tokens. To mathematically, uniformly analyze their expressivity, we considered the case that the mappings are conditioned on a context represented by a probability distribution which becomes discrete for a finite number of tokens. Modeling neural networks as maps on probability measures has multiple applications, such as studying Wasserstein regularity, proving generalization bounds and doing a mean-field limit analysis of the dynamics of interacting particles as they go through the network. In this work, we study the question what kind of maps between measures are transformers. We fully characterize the properties of maps between measures that enable these to be represented in terms of in-context maps via a push forward. On the one hand, these include transformers; on the other hand, transformers universally approximate representations with any continuous in-context map. These properties are preserving the cardinality of support and that the regular part of their Fréchet derivative is uniformly continuous. Moreover, we show that the solution map of the Vlasov equation, which is of nonlocal transport type, for interacting particle systems in the mean-field regime for the Cauchy problem satisfies the conditions on the one hand and, hence, can be approximated by a transformer; on the other hand, we prove that the measure-theoretic self-attention has the properties that ensure that the infinite depth, mean-field measure-theoretic transformer can be identified with a Vlasov flow.</li>
</ul>

<h3>Title: Unsupervised Detection of Spatiotemporal Anomalies in PMU Data Using Transformer-Based BiGAN</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Imran Hossain, Jignesh Solanki, Sarika Khushlani Solanki</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25612">https://arxiv.org/abs/2509.25612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25612">https://arxiv.org/pdf/2509.25612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25612]] Unsupervised Detection of Spatiotemporal Anomalies in PMU Data Using Transformer-Based BiGAN(https://arxiv.org/abs/2509.25612)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Ensuring power grid resilience requires the timely and unsupervised detection of anomalies in synchrophasor data streams. We introduce T-BiGAN, a novel framework that integrates window-attention Transformers within a bidirectional Generative Adversarial Network (BiGAN) to address this challenge. Its self-attention encoder-decoder architecture captures complex spatio-temporal dependencies across the grid, while a joint discriminator enforces cycle consistency to align the learned latent space with the true data distribution. Anomalies are flagged in real-time using an adaptive score that combines reconstruction error, latent space drift, and discriminator confidence. Evaluated on a realistic hardware-in-the-loop PMU benchmark, T-BiGAN achieves an ROC-AUC of 0.95 and an average precision of 0.996, significantly outperforming leading supervised and unsupervised methods. It shows particular strength in detecting subtle frequency and voltage deviations, demonstrating its practical value for live, wide-area monitoring without relying on manually labeled fault data.</li>
</ul>

<h3>Title: LMOD+: A Comprehensive Multimodal Dataset and Benchmark for Developing and Evaluating Multimodal Large Language Models in Ophthalmology</h3>
<ul>
<li><strong>Authors: </strong>Zhenyue Qin, Yang Liu, Yu Yin, Jinyu Ding, Haoran Zhang, Anran Li, Dylan Campbell, Xuansheng Wu, Ke Zou, Tiarnan D. L. Keenan, Emily Y. Chew, Zhiyong Lu, Yih-Chung Tham, Ninghao Liu, Xiuzhen Zhang, Qingyu Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25620">https://arxiv.org/abs/2509.25620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25620">https://arxiv.org/pdf/2509.25620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25620]] LMOD+: A Comprehensive Multimodal Dataset and Benchmark for Developing and Evaluating Multimodal Large Language Models in Ophthalmology(https://arxiv.org/abs/2509.25620)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Vision-threatening eye diseases pose a major global health burden, with timely diagnosis limited by workforce shortages and restricted access to specialized care. While multimodal large language models (MLLMs) show promise for medical image interpretation, advancing MLLMs for ophthalmology is hindered by the lack of comprehensive benchmark datasets suitable for evaluating generative models. We present a large-scale multimodal ophthalmology benchmark comprising 32,633 instances with multi-granular annotations across 12 common ophthalmic conditions and 5 imaging modalities. The dataset integrates imaging, anatomical structures, demographics, and free-text annotations, supporting anatomical structure recognition, disease screening, disease staging, and demographic prediction for bias evaluation. This work extends our preliminary LMOD benchmark with three major enhancements: (1) nearly 50% dataset expansion with substantial enlargement of color fundus photography; (2) broadened task coverage including binary disease diagnosis, multi-class diagnosis, severity classification with international grading standards, and demographic prediction; and (3) systematic evaluation of 24 state-of-the-art MLLMs. Our evaluations reveal both promise and limitations. Top-performing models achieved ~58% accuracy in disease screening under zero-shot settings, and performance remained suboptimal for challenging tasks like disease staging. We will publicly release the dataset, curation pipeline, and leaderboard to potentially advance ophthalmic AI applications and reduce the global burden of vision-threatening diseases.</li>
</ul>

<h3>Title: Layer-wise dynamic rank for compressing large language models</h3>
<ul>
<li><strong>Authors: </strong>Zhendong Mi, Bian Sun, Grace Li Zhang, Shaoyi Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25622">https://arxiv.org/abs/2509.25622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25622">https://arxiv.org/pdf/2509.25622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25622]] Layer-wise dynamic rank for compressing large language models(https://arxiv.org/abs/2509.25622)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have rapidly scaled in size, bringing severe memory and computational challenges that hinder their deployment. Singular Value Decomposition (SVD)-based compression has emerged as an appealing post-training compression technique for LLMs, yet most existing methods apply a uniform compression ratio across all layers, implicitly assuming homogeneous information included in various layers. This overlooks the substantial intra-layer heterogeneity observed in LLMs, where middle layers tend to encode richer information while early and late layers are more redundant. In this work, we revisit the existing SVD-based compression method and propose D-Rank, a framework with layer-wise balanced Dynamic Rank allocation for LLMs compression. We first introduce effective rank as a principled metric to measure the information density of weight matrices, and then allocate ranks via a Lagrange multiplier-based optimization scheme to adaptively assign more capacity to groups with higher information density under a fixed compression ratio. Moreover, we rebalance the allocated ranks across attention layers to account for their varying importance and extend D-Rank to latest LLMs with grouped-query attention. Extensive experiments on various LLMs with different scales across multiple compression ratios demonstrate that D-Rank consistently outperforms SVD-LLM, ASVD, and Basis Sharing, achieving more than 15 lower perplexity with LLaMA-3-8B model on C4 datasets at 20% compression ratio and up to 5% higher zero-shot reasoning accuracy with LLaMA-7B model at 40% compression ratio while achieving even higher throughput.</li>
</ul>

<h3>Title: Anchor-free Cross-view Object Geo-localization with Gaussian Position Encoding and Cross-view Association</h3>
<ul>
<li><strong>Authors: </strong>Xingtao Ling, Chenlin Fu, Yingying Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25623">https://arxiv.org/abs/2509.25623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25623">https://arxiv.org/pdf/2509.25623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25623]] Anchor-free Cross-view Object Geo-localization with Gaussian Position Encoding and Cross-view Association(https://arxiv.org/abs/2509.25623)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Most existing cross-view object geo-localization approaches adopt anchor-based paradigm. Although effective, such methods are inherently constrained by predefined anchors. To eliminate this dependency, we first propose an anchor-free formulation for cross-view object geo-localization, termed AFGeo. AFGeo directly predicts the four directional offsets (left, right, top, bottom) to the ground-truth box for each pixel, thereby localizing the object without any predefined anchors. To obtain a more robust spatial prior, AFGeo incorporates Gaussian Position Encoding (GPE) to model the click point in the query image, mitigating the uncertainty of object position that challenges object localization in cross-view scenarios. In addition, AFGeo incorporates a Cross-view Object Association Module (CVOAM) that relates the same object and its surrounding context across viewpoints, enabling reliable localization under large cross-view appearance gaps. By adopting an anchor-free localization paradigm that integrates GPE and CVOAM with minimal parameter overhead, our model is both lightweight and computationally efficient, achieving state-of-the-art performance on benchmark datasets.</li>
</ul>

<h3>Title: STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Jing-Jing Li, Jianfeng He, Chao Shang, Devang Kulshreshtha, Xun Xian, Yi Zhang, Hang Su, Sandesh Swamy, Yanjun Qi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25624">https://arxiv.org/abs/2509.25624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25624">https://arxiv.org/pdf/2509.25624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25624]] STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents(https://arxiv.org/abs/2509.25624)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, steal</a></li>
<li><strong>Abstract: </strong>As LLMs advance into autonomous agents with tool-use capabilities, they introduce security challenges that extend beyond traditional content-based LLM safety concerns. This paper introduces Sequential Tool Attack Chaining (STAC), a novel multi-turn attack framework that exploits agent tool use. STAC chains together tool calls that each appear harmless in isolation but, when combined, collectively enable harmful operations that only become apparent at the final execution step. We apply our framework to automatically generate and systematically evaluate 483 STAC cases, featuring 1,352 sets of user-agent-environment interactions and spanning diverse domains, tasks, agent types, and 10 failure modes. Our evaluations show that state-of-the-art LLM agents, including GPT-4.1, are highly vulnerable to STAC, with attack success rates (ASR) exceeding 90% in most cases. The core design of STAC's automated framework is a closed-loop pipeline that synthesizes executable multi-step tool chains, validates them through in-environment execution, and reverse-engineers stealthy multi-turn prompts that reliably induce agents to execute the verified malicious sequence. We further perform defense analysis against STAC and find that existing prompt-based defenses provide limited protection. To address this gap, we propose a new reasoning-driven defense prompt that achieves far stronger protection, cutting ASR by up to 28.8%. These results highlight a crucial gap: defending tool-enabled agents requires reasoning over entire action sequences and their cumulative effects, rather than evaluating isolated prompts or responses.</li>
</ul>

<h3>Title: Swift: An Autoregressive Consistency Model for Efficient Weather Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Jason Stock, Troy Arcomano, Rao Kotamarthi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25631">https://arxiv.org/abs/2509.25631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25631">https://arxiv.org/pdf/2509.25631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25631]] Swift: An Autoregressive Consistency Model for Efficient Weather Forecasting(https://arxiv.org/abs/2509.25631)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models offer a physically grounded framework for probabilistic weather forecasting, but their typical reliance on slow, iterative solvers during inference makes them impractical for subseasonal-to-seasonal (S2S) applications where long lead-times and domain-driven calibration are essential. To address this, we introduce Swift, a single-step consistency model that, for the first time, enables autoregressive finetuning of a probability flow model with a continuous ranked probability score (CRPS) objective. This eliminates the need for multi-model ensembling or parameter perturbations. Results show that Swift produces skillful 6-hourly forecasts that remain stable for up to 75 days, running $39\times$ faster than state-of-the-art diffusion baselines while achieving forecast skill competitive with the numerical-based, operational IFS ENS. This marks a step toward efficient and reliable ensemble forecasting from medium-range to seasonal-scales.</li>
</ul>

<h3>Title: How Does Preconditioning Guide Feature Learning in Deep Neural Networks?</h3>
<ul>
<li><strong>Authors: </strong>Kotaro Yoshida, Atsushi Nitanda</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25637">https://arxiv.org/abs/2509.25637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25637">https://arxiv.org/pdf/2509.25637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25637]] How Does Preconditioning Guide Feature Learning in Deep Neural Networks?(https://arxiv.org/abs/2509.25637)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Preconditioning is widely used in machine learning to accelerate convergence on the empirical risk, yet its role on the expected risk remains underexplored. In this work, we investigate how preconditioning affects feature learning and generalization performance. We first show that the input information available to the model is conveyed solely through the Gram matrix defined by the preconditioner's metric, thereby inducing a controllable spectral bias on feature learning. Concretely, instantiating the preconditioner as the $p$-th power of the input covariance matrix and within a single-index teacher model, we prove that in generalization, the exponent $p$ and the alignment between the teacher and the input spectrum are crucial factors. We further investigate how the interplay between these factors influences feature learning from three complementary perspectives: (i) Robustness to noise, (ii) Out-of-distribution generalization, and (iii) Forward knowledge transfer. Our results indicate that the learned feature representations closely mirror the spectral bias introduced by the preconditioner -- favoring components that are emphasized and exhibiting reduced sensitivity to those that are suppressed. Crucially, we demonstrate that generalization is significantly enhanced when this spectral bias is aligned with that of the teacher.</li>
</ul>

<h3>Title: Deep set based operator learning with uncertainty quantification</h3>
<ul>
<li><strong>Authors: </strong>Lei Ma, Ling Guo, Hao Wu, Tao Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25646">https://arxiv.org/abs/2509.25646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25646">https://arxiv.org/pdf/2509.25646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25646]] Deep set based operator learning with uncertainty quantification(https://arxiv.org/abs/2509.25646)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Learning operators from data is central to scientific machine learning. While DeepONets are widely used for their ability to handle complex domains, they require fixed sensor numbers and locations, lack mechanisms for uncertainty quantification (UQ), and are thus limited in practical applicability. Recent permutationinvariant extensions, such as the Variable-Input Deep Operator Network (VIDON), relax these sensor constraints but still rely on sufficiently dense observations and cannot capture uncertainties arising from incomplete measurements or from operators with inherent randomness. To address these challenges, we propose UQ-SONet, a permutation-invariant operator learning framework with built-in UQ. Our model integrates a set transformer embedding to handle sparse and variable sensor locations, and employs a conditional variational autoencoder (cVAE) to approximate the conditional distribution of the solution operator. By minimizing the negative ELBO, UQ-SONet provides principled uncertainty estimation while maintaining predictive accuracy. Numerical experiments on deterministic and stochastic PDEs, including the Navier-Stokes equation, demonstrate the robustness and effectiveness of the proposed framework.</li>
</ul>

<h3>Title: The Media Bias Detector: A Framework for Annotating and Analyzing the News at Scale</h3>
<ul>
<li><strong>Authors: </strong>Samar Haider, Amir Tohidi, Jenny S. Wang, Timothy Dörr, David M. Rothschild, Chris Callison-Burch, Duncan J. Watts</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25649">https://arxiv.org/abs/2509.25649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25649">https://arxiv.org/pdf/2509.25649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25649]] The Media Bias Detector: A Framework for Annotating and Analyzing the News at Scale(https://arxiv.org/abs/2509.25649)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mainstream news organizations shape public perception not only directly through the articles they publish but also through the choices they make about which topics to cover (or ignore) and how to frame the issues they do decide to cover. However, measuring these subtle forms of media bias at scale remains a challenge. Here, we introduce a large, ongoing (from January 1, 2024 to present), near real-time dataset and computational framework developed to enable systematic study of selection and framing bias in news coverage. Our pipeline integrates large language models (LLMs) with scalable, near-real-time news scraping to extract structured annotations -- including political lean, tone, topics, article type, and major events -- across hundreds of articles per day. We quantify these dimensions of coverage at multiple levels -- the sentence level, the article level, and the publisher level -- expanding the ways in which researchers can analyze media bias in the modern news landscape. In addition to a curated dataset, we also release an interactive web platform for convenient exploration of these data. Together, these contributions establish a reusable methodology for studying media bias at scale, providing empirical resources for future research. Leveraging the breadth of the corpus over time and across publishers, we also present some examples (focused on the 150,000+ articles examined in 2024) that illustrate how this novel data set can reveal insightful patterns in news coverage and bias, supporting academic research and real-world efforts to improve media accountability.</li>
</ul>

<h3>Title: DescribeEarth: Describe Anything for Remote Sensing Images</h3>
<ul>
<li><strong>Authors: </strong>Kaiyu Li, Zixuan Jiang, Xiangyong Cao, Jiayu Wang, Yuchen Xiao, Deyu Meng, Zhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25654">https://arxiv.org/abs/2509.25654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25654">https://arxiv.org/pdf/2509.25654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25654]] DescribeEarth: Describe Anything for Remote Sensing Images(https://arxiv.org/abs/2509.25654)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automated textual description of remote sensing images is crucial for unlocking their full potential in diverse applications, from environmental monitoring to urban planning and disaster management. However, existing studies in remote sensing image captioning primarily focus on the image level, lacking object-level fine-grained interpretation, which prevents the full utilization and transformation of the rich semantic and structural information contained in remote sensing images. To address this limitation, we propose Geo-DLC, a novel task of object-level fine-grained image captioning for remote sensing. To support this task, we construct DE-Dataset, a large-scale dataset contains 25 categories and 261,806 annotated instances with detailed descriptions of object attributes, relationships, and contexts. Furthermore, we introduce DE-Benchmark, a LLM-assisted question-answering based evaluation suite designed to systematically measure model capabilities on the Geo-DLC task. We also present DescribeEarth, a Multi-modal Large Language Model (MLLM) architecture explicitly designed for Geo-DLC, which integrates a scale-adaptive focal strategy and a domain-guided fusion module leveraging remote sensing vision-language model features to encode high-resolution details and remote sensing category priors while maintaining global context. Our DescribeEarth model consistently outperforms state-of-the-art general MLLMs on DE-Benchmark, demonstrating superior factual accuracy, descriptive richness, and grammatical soundness, particularly in capturing intrinsic object features and surrounding environmental attributes across simple, complex, and even out-of-distribution remote sensing scenarios. All data, code and weights are released at this https URL.</li>
</ul>

<h3>Title: EEG-based AI-BCI Wheelchair Advancement: Hybrid Deep Learning with Motor Imagery for Brain Computer Interface</h3>
<ul>
<li><strong>Authors: </strong>Bipul Thapa, Biplov Paneru, Bishwash Paneru, Khem Narayan Poudyal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25667">https://arxiv.org/abs/2509.25667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25667">https://arxiv.org/pdf/2509.25667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25667]] EEG-based AI-BCI Wheelchair Advancement: Hybrid Deep Learning with Motor Imagery for Brain Computer Interface(https://arxiv.org/abs/2509.25667)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper presents an Artificial Intelligence (AI) integrated novel approach to Brain-Computer Interface (BCI)-based wheelchair development, utilizing a motor imagery right-left-hand movement mechanism for control. The system is designed to simulate wheelchair navigation based on motor imagery right and left-hand movements using electroencephalogram (EEG) data. A pre-filtered dataset, obtained from an open-source EEG repository, was segmented into arrays of 19x200 to capture the onset of hand movements. The data was acquired at a sampling frequency of 200Hz. The system integrates a Tkinter-based interface for simulating wheelchair movements, offering users a functional and intuitive control system. We propose a BiLSTM-BiGRU model that shows a superior test accuracy of 92.26% as compared with various machine learning baseline models, including XGBoost, EEGNet, and a transformer-based model. The Bi-LSTM-BiGRU attention-based model achieved a mean accuracy of 90.13% through cross-validation, showcasing the potential of attention mechanisms in BCI applications.</li>
</ul>

<h3>Title: The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Arda Uzunoglu, Tianjian Li, Daniel Khashabi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25671">https://arxiv.org/abs/2509.25671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25671">https://arxiv.org/pdf/2509.25671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25671]] The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks(https://arxiv.org/abs/2509.25671)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Benchmarks shape scientific conclusions about model capabilities and steer model development. This creates a feedback loop: stronger benchmarks drive better models, and better models demand more discriminative benchmarks. Ensuring benchmark reliability is therefore essential for trustworthy evaluation and meaningful progress. In this work, we study benchmark reliability from a distributional perspective and introduce benchmark harmony, which measures how uniformly a model's performance is distributed across the subdomains of a benchmark. We posit that high harmony is a desirable benchmark property, indicating that the aggregate metric reflects uniform competence across subdomains. Across 19 multiple-choice benchmarks and five model families, we map each benchmark onto a mean-variance plane of harmony computed across models, where high mean and low variance signal more reliable evaluation. Our analysis shows that less harmonious benchmarks can give misleading results, since overall accuracy may be disproportionately influenced by specific subdomains. For instance, ARC-Easy is overwhelmed by questions on Biological Concepts, overshadowing other critical subdomains such as Geography, Physics, Chemistry, and Environmental Science. By recommending that harmony should be reported alongside accuracy, we reframe evaluation from simple performance averages to a more robust, distributionally reliable measurement of performance.</li>
</ul>

<h3>Title: Mitigating Biases in Language Models via Bias Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Dianqing Liu, Yi Liu, Guoqing Jin, Zhendong Mao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25673">https://arxiv.org/abs/2509.25673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25673">https://arxiv.org/pdf/2509.25673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25673]] Mitigating Biases in Language Models via Bias Unlearning(https://arxiv.org/abs/2509.25673)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Many studies have shown various biases targeting different demographic groups in language models, amplifying discrimination and harming fairness. Recent parameter modification debiasing approaches significantly degrade core capabilities such as text coherence and task accuracy. And Prompt-based debiasing methods, only effective for predefined trigger words, fail to address deeply embedded stereotypical associations in model parameters. In this paper, we propose BiasUnlearn, a novel model debiasing framework which achieves targeted debiasing via dual-pathway unlearning mechanisms coordinating stereotype forgetting with anti-stereotype retention, while preventing bias polarity reversal through adversarial forget set and dynamic dataset swapping. We conducted extensive experiments with multiple language models across various evaluation benchmarks. The results show that BiasUnlearn outperforms existing methods in mitigating bias in language models while retaining language modeling capabilities. Further experiments reveal that debiasing weights are transferable across model variants, confirming that bias representations become entrenched during pre-training and persist through fine-tuning phases.</li>
</ul>

<h3>Title: Guiding Mixture-of-Experts with Temporal Multimodal Interactions</h3>
<ul>
<li><strong>Authors: </strong>Xing Han, Hsing-Huan Chung, Joydeep Ghosh, Paul Pu Liang, Suchi Saria</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25678">https://arxiv.org/abs/2509.25678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25678">https://arxiv.org/pdf/2509.25678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25678]] Guiding Mixture-of-Experts with Temporal Multimodal Interactions(https://arxiv.org/abs/2509.25678)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) architectures have become pivotal for large-scale multimodal models. However, their routing mechanisms typically overlook the informative, time-varying interaction dynamics between modalities. This limitation hinders expert specialization, as the model cannot explicitly leverage intrinsic modality relationships for effective reasoning. To address this, we propose a novel framework that guides MoE routing using quantified temporal interaction. A multimodal interaction-aware router learns to dispatch tokens to experts based on the nature of their interactions. This dynamic routing encourages experts to acquire generalizable interaction-processing skills rather than merely learning task-specific features. Our framework builds on a new formulation of temporal multimodal interaction dynamics, which are used to guide expert routing. We first demonstrate that these temporal multimodal interactions reveal meaningful patterns across applications, and then show how they can be leveraged to improve both the design and performance of MoE-based models. Comprehensive experiments on challenging multimodal benchmarks validate our approach, demonstrating both enhanced performance and improved interpretability.</li>
</ul>

<h3>Title: OmniDFA: A Unified Framework for Open Set Synthesis Image Detection and Few-Shot Attribution</h3>
<ul>
<li><strong>Authors: </strong>Shiyu Wu, Shuyan Li, Jing Li, Jing Liu, Yequan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25682">https://arxiv.org/abs/2509.25682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25682">https://arxiv.org/pdf/2509.25682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25682]] OmniDFA: A Unified Framework for Open Set Synthesis Image Detection and Few-Shot Attribution(https://arxiv.org/abs/2509.25682)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>AI-generated image (AIGI) detection and source model attribution remain central challenges in combating deepfake abuses, primarily due to the structural diversity of generative models. Current detection methods are prone to overfitting specific forgery traits, whereas source attribution offers a robust alternative through fine-grained feature discrimination. However, synthetic image attribution remains constrained by the scarcity of large-scale, well-categorized synthetic datasets, limiting its practicality and compatibility with detection systems. In this work, we propose a new paradigm for image attribution called open-set, few-shot source identification. This paradigm is designed to reliably identify unseen generators using only limited samples, making it highly suitable for real-world application. To this end, we introduce OmniDFA (Omni Detector and Few-shot Attributor), a novel framework for AIGI that not only assesses the authenticity of images, but also determines the synthesis origins in a few-shot manner. To facilitate this work, we construct OmniFake, a large class-aware synthetic image dataset that curates $1.17$ M images from $45$ distinct generative models, substantially enriching the foundational resources for research on both AIGI detection and attribution. Experiments demonstrate that OmniDFA exhibits excellent capability in open-set attribution and achieves state-of-the-art generalization performance on AIGI detection. Our dataset and code will be made available.</li>
</ul>

<h3>Title: LD-MoLE: Learnable Dynamic Routing for Mixture of LoRA Experts</h3>
<ul>
<li><strong>Authors: </strong>Yuan Zhuang, Yi Shen, Yuexin Bian, Qing Su, Shihao Ji, Yuanyuan Shi, Fei Miao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25684">https://arxiv.org/abs/2509.25684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25684">https://arxiv.org/pdf/2509.25684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25684]] LD-MoLE: Learnable Dynamic Routing for Mixture of LoRA Experts(https://arxiv.org/abs/2509.25684)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent studies have shown that combining parameter-efficient fine-tuning (PEFT) with mixture-of-experts (MoE) is an effective strategy for adapting large language models (LLMs) to the downstream tasks. However, most existing approaches rely on conventional TopK routing, which requires careful hyperparameter tuning and assigns a fixed number of experts to each token. In this work, we propose LD-MoLE, a Learnable Dynamic routing mechanism for Mixture of LoRA Experts that enables adaptive, token-dependent, and layer-wise expert allocation. Our method replaces the non-differentiable TopK selection with a differentiable routing function and a closed-form solution. Moreover, our design allows the model to adaptively determine the number of experts to activate for each token at different layers. In addition, we introduce an analytical sparsity control objective to regularize the number of activated experts. Extensive experiments on the Qwen3-1.7B and Llama-3.2-3B models show that LD-MoLE achieves the highest average scores compared to state-of-the-art baselines, across a diverse set of benchmarks. Our method not only achieves superior performance, but also demonstrates the ability to learn token-dependent and layer-wise expert allocation.</li>
</ul>

<h3>Title: Minimalist Explanation Generation and Circuit Discovery</h3>
<ul>
<li><strong>Authors: </strong>Pirzada Suhail, Aditya Anand, Amit Sethi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25686">https://arxiv.org/abs/2509.25686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25686">https://arxiv.org/pdf/2509.25686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25686]] Minimalist Explanation Generation and Circuit Discovery(https://arxiv.org/abs/2509.25686)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Machine learning models, by virtue of training, learn a large repertoire of decision rules for any given input, and any one of these may suffice to justify a prediction. However, in high-dimensional input spaces, such rules are difficult to identify and interpret. In this paper, we introduce an activation-matching based approach to generate minimal and faithful explanations for the decisions of pre-trained image classifiers. We aim to identify minimal explanations that not only preserve the model's decision but are also concise and human-readable. To achieve this, we train a lightweight autoencoder to produce binary masks that learns to highlight the decision-wise critical regions of an image while discarding irrelevant background. The training objective integrates activation alignment across multiple layers, consistency at the output label, priors that encourage sparsity, and compactness, along with a robustness constraint that enforces faithfulness. The minimal explanations so generated also lead us to mechanistically interpreting the model internals. In this regard we also introduce a circuit readout procedure wherein using the explanation's forward pass and gradients, we identify active channels and construct a channel-level graph, scoring inter-layer edges by ingress weight magnitude times source activation and feature-to-class links by classifier weight magnitude times feature activation. Together, these contributions provide a practical bridge between minimal input-level explanations and a mechanistic understanding of the internal computations driving model decisions.</li>
</ul>

<h3>Title: A Unified Probabilistic Framework for Dictionary Learning with Parsimonious Activation</h3>
<ul>
<li><strong>Authors: </strong>Zihui Zhao, Yuanbo Tang, Jieyu Ren, Xiaoping Zhang, Yang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25690">https://arxiv.org/abs/2509.25690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25690">https://arxiv.org/pdf/2509.25690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25690]] A Unified Probabilistic Framework for Dictionary Learning with Parsimonious Activation(https://arxiv.org/abs/2509.25690)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dictionary learning is traditionally formulated as an $L_1$-regularized signal reconstruction problem. While recent developments have incorporated discriminative, hierarchical, or generative structures, most approaches rely on encouraging representation sparsity over individual samples that overlook how atoms are shared across samples, resulting in redundant and sub-optimal dictionaries. We introduce a parsimony promoting regularizer based on the row-wise $L_\infty$ norm of the coefficient matrix. This additional penalty encourages entire rows of the coefficient matrix to vanish, thereby reducing the number of dictionary atoms activated across the dataset. We derive the formulation from a probabilistic model with Beta-Bernoulli priors, which provides a Bayesian interpretation linking the regularization parameters to prior distributions. We further establish theoretical calculation for optimal hyperparameter selection and connect our formulation to both Minimum Description Length, Bayesian model selection and pathlet learning. Extensive experiments on benchmark datasets demonstrate that our method achieves substantially improved reconstruction quality (with a 20\% reduction in RMSE) and enhanced representation sparsity, utilizing fewer than one-tenth of the available dictionary atoms, while empirically validating our theoretical analysis.</li>
</ul>

<h3>Title: Annotation-Efficient Active Test-Time Adaptation with Conformal Prediction</h3>
<ul>
<li><strong>Authors: </strong>Tingyu Shi, Fan Lyu, Shaoliang Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25692">https://arxiv.org/abs/2509.25692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25692">https://arxiv.org/pdf/2509.25692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25692]] Annotation-Efficient Active Test-Time Adaptation with Conformal Prediction(https://arxiv.org/abs/2509.25692)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Active Test-Time Adaptation (ATTA) improves model robustness under domain shift by selectively querying human annotations at deployment, but existing methods use heuristic uncertainty measures and suffer from low data selection efficiency, wasting human annotation budget. We propose Conformal Prediction Active TTA (CPATTA), which first brings principled, coverage-guaranteed uncertainty into ATTA. CPATTA employs smoothed conformal scores with a top-K certainty measure, an online weight-update algorithm driven by pseudo coverage, a domain-shift detector that adapts human supervision, and a staged update scheme balances human-labeled and model-labeled data. Extensive experiments demonstrate that CPATTA consistently outperforms the state-of-the-art ATTA methods by around 5% in accuracy. Our code and datasets are available at this https URL.</li>
</ul>

<h3>Title: Can VLM Pseudo-Labels Train a Time-Series QA Model That Outperforms the VLM?</h3>
<ul>
<li><strong>Authors: </strong>Takuya Fujimura, Kota Dohi, Natsuo Yamashita, Yohei Kawaguchi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25696">https://arxiv.org/abs/2509.25696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25696">https://arxiv.org/pdf/2509.25696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25696]] Can VLM Pseudo-Labels Train a Time-Series QA Model That Outperforms the VLM?(https://arxiv.org/abs/2509.25696)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Time-series question answering (TSQA) tasks face significant challenges due to the lack of labeled data. Alternatively, with recent advancements in large-scale models, vision-language models (VLMs) have demonstrated the potential to analyze time-series signals in a zero-shot manner. In this paper, we propose a training approach that uses pseudo labels generated by a VLM. Although VLMs can produce incorrect labels, TSQA models can still be effectively trained based on the property that deep neural networks are inherently robust to such noisy labels. Our experimental results demonstrate that TSQA models are not only successfully trained with pseudo labels, but also surpass the performance of the VLM itself by leveraging a large amount of unlabeled data.</li>
</ul>

<h3>Title: AIMCoT: Active Information-driven Multimodal Chain-of-Thought for Vision-Language Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xiping Li, Jianghong Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25699">https://arxiv.org/abs/2509.25699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25699">https://arxiv.org/pdf/2509.25699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25699]] AIMCoT: Active Information-driven Multimodal Chain-of-Thought for Vision-Language Reasoning(https://arxiv.org/abs/2509.25699)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal Chain-of-Thought (CoT) has emerged as a powerful technique for enhancing the vision-language reasoning with interleaved information. However, existing methods often rely on simplistic heuristics for constructing interleaved CoT, typically depending on attention maps, which our empirical analysis reveals can be unreliable. What's more, the shortcomings of their passive and purposeless selection strategies and their arbitrary triggering mechanisms in capturing the model's cognitive need for information are further amplified. In this paper, we propose \textbf{AIMCoT}, an \textbf{A}ctive \textbf{I}nformation-driven \textbf{M}ulti-modal \textbf{C}hain-\textbf{o}f-\textbf{T}hought framework that addresses these fundamental limitations. AIMCoT introduces three synergistic components: (1) \textbf{Context-enhanced Attention-map Generation (CAG)}, which mitigates the text-vision granularity imbalance, thereby producing more reliable attention maps as a foundation. (2) \textbf{Active Visual Probing (AVP)}, which replaces passive selection with a proactive, goal-oriented strategy grounded in information theory to select image regions that help answer the questions maximally. (3) \textbf{Dynamic Attention-shifting Trigger (DAT)}, which intelligently determines the optimal moments to insert visual information by monitoring the model's text-to-vision attention shifts. Extensive experiments on three challenging benchmarks demonstrate that AIMCoT significantly outperforms state-of-the-art methods across different settings. By actively foraging for information and dynamically structuring its reasoning process, AIMCoT represents a critical step towards more robust, effective, and human-like multimodal reasoning. Our code is available at this https URL.</li>
</ul>

<h3>Title: How Diffusion Models Memorize</h3>
<ul>
<li><strong>Authors: </strong>Juyeop Kim, Songkuk Kim, Jong-Seok Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25705">https://arxiv.org/abs/2509.25705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25705">https://arxiv.org/pdf/2509.25705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25705]] How Diffusion Models Memorize(https://arxiv.org/abs/2509.25705)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>Despite their success in image generation, diffusion models can memorize training data, raising serious privacy and copyright concerns. Although prior work has sought to characterize, detect, and mitigate memorization, the fundamental question of why and how it occurs remains unresolved. In this paper, we revisit the diffusion and denoising process and analyze latent space dynamics to address the question: "How do diffusion models memorize?" We show that memorization is driven by the overestimation of training samples during early denoising, which reduces diversity, collapses denoising trajectories, and accelerates convergence toward the memorized image. Specifically: (i) memorization cannot be explained by overfitting alone, as training loss is larger under memorization due to classifier-free guidance amplifying predictions and inducing overestimation; (ii) memorized prompts inject training images into noise predictions, forcing latent trajectories to converge and steering denoising toward their paired samples; and (iii) a decomposition of intermediate latents reveals how initial randomness is quickly suppressed and replaced by memorized content, with deviations from the theoretical denoising schedule correlating almost perfectly with memorization severity. Together, these results identify early overestimation as the central underlying mechanism of memorization in diffusion models.</li>
</ul>

<h3>Title: ProbMed: A Probabilistic Framework for Medical Multimodal Binding</h3>
<ul>
<li><strong>Authors: </strong>Yuan Gao, Sangwook Kim, Jianzhong You, Chris McIntosh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25711">https://arxiv.org/abs/2509.25711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25711">https://arxiv.org/pdf/2509.25711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25711]] ProbMed: A Probabilistic Framework for Medical Multimodal Binding(https://arxiv.org/abs/2509.25711)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Medical decision-making requires integrating diverse medical information, from imaging to clinical narratives. These medical modalities are often acquired in a many-to-many manner. However, current medical vision-language pretraining models (Med-VLPMs) fail to directly account for this many-to-many mapping in their model training and embeddings. To address this, we present Probabilistic Modality-Enhanced Diagnosis (ProbMED), a multimodal Med-VLPM that employs probabilistic contrastive learning to model distributions over embeddings rather than deterministic estimates. ProbMED aligns four distinct modalities--chest X-rays, electrocardiograms, echocardiograms, and clinical text--into a unified probabilistic embedding space. We use InfoNCE loss with Hellinger distance to integrate inter-modality distributions. We introduce a probabilistic synthetic sampling loss that captures modality-specific mean and variance to improve intra-modality binding. Extensive experiments across 13 medical datasets demonstrate that our model outperforms current Med-VLPMs in cross-modality retrieval, zero-shot, and few-shot classification. We also demonstrate the robust integration of multiple modalities for prognostication, showing improved intra- and inter-medical modality binding.</li>
</ul>

<h3>Title: Expert Merging: Model Merging with Unsupervised Expert Alignment and Importance-Guided Layer Chunking</h3>
<ul>
<li><strong>Authors: </strong>Dengming Zhang, Xiaowen Ma, Zhenliang Ni, Zhenkai Wu, Han Shu, Xin Jiang, Xinghao Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25712">https://arxiv.org/abs/2509.25712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25712">https://arxiv.org/pdf/2509.25712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25712]] Expert Merging: Model Merging with Unsupervised Expert Alignment and Importance-Guided Layer Chunking(https://arxiv.org/abs/2509.25712)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Model merging, which combines multiple domain-specialized experts into a single model, offers a practical path to endow Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) with broad capabilities without the cost of joint training or serving many models. However, training-free methods rely on hand-tuned coefficients, whereas training-based methods primarily align parameters rather than downstream task behavior and typically treat all layers uniformly, ignoring inter-layer heterogeneity. We introduce Expert Merging, a training-light method that learns a small set of layer-wise coefficients using only unlabeled calibration data. The coefficients are optimized to explicitly align the merged model's hidden states and logits with those of the corresponding experts, with a coefficient regularizer for stability and task-weighted losses for controllable trade-offs. To capture inter-layer variation, Expert Merging++ augments this design with importance-guided chunking: a normalized layer-importance metric, derived from learned coefficients, task-vector magnitudes, and parameter counts, allocates more chunk-wise coefficients to high-importance layers while keeping low-importance layers lightweight. The result is a label-free, parameter-efficient, and scalable approach to multi-expert model merging across LLMs and MLLMs. Across MLLM backbones (InternVL and Qwen2-VL) and the LLM backbone (Mistral), our method surpasses strong training-free and training-based merging baselines, with Expert Merging++ delivering further gains and, in some cases, even exceeding supervised Mixture Training. The source code is available at this https URL.</li>
</ul>

<h3>Title: Reweighted Flow Matching via Unbalanced OT for Label-free Long-tailed Generation</h3>
<ul>
<li><strong>Authors: </strong>Hyunsoo Song, Minjung Gim, Jaewoong Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25713">https://arxiv.org/abs/2509.25713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25713">https://arxiv.org/pdf/2509.25713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25713]] Reweighted Flow Matching via Unbalanced OT for Label-free Long-tailed Generation(https://arxiv.org/abs/2509.25713)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Flow matching has recently emerged as a powerful framework for continuous-time generative modeling. However, when applied to long-tailed distributions, standard flow matching suffers from majority bias, producing minority modes with low fidelity and failing to match the true class proportions. In this work, we propose Unbalanced Optimal Transport Reweighted Flow Matching (UOT-RFM), a novel framework for generative modeling under class-imbalanced (long-tailed) distributions that operates without any class label information. Our method constructs the conditional vector field using mini-batch Unbalanced Optimal Transport (UOT) and mitigates majority bias through a principled inverse reweighting strategy. The reweighting relies on a label-free majority score, defined as the density ratio between the target distribution and the UOT marginal. This score quantifies the degree of majority based on the geometric structure of the data, without requiring class labels. By incorporating this score into the training objective, UOT-RFM theoretically recovers the target distribution with first-order correction ($k=1$) and empirically improves tail-class generation through higher-order corrections ($k > 1$). Our model outperforms existing flow matching baselines on long-tailed benchmarks, while maintaining competitive performance on balanced datasets.</li>
</ul>

<h3>Title: SAGE: Spatial-visual Adaptive Graph Exploration for Visual Place Recognition</h3>
<ul>
<li><strong>Authors: </strong>Shunpeng Chen, Changwei Wang, Rongtao Xu, Xingtian Pei, Yukun Song, Jinzhou Lin, Wenhao Xu, Jingyi Zhang, Li Guo, Shibiao Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25723">https://arxiv.org/abs/2509.25723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25723">https://arxiv.org/pdf/2509.25723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25723]] SAGE: Spatial-visual Adaptive Graph Exploration for Visual Place Recognition(https://arxiv.org/abs/2509.25723)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual Place Recognition (VPR) requires robust retrieval of geotagged images despite large appearance, viewpoint, and environmental variation. Prior methods focus on descriptor fine-tuning or fixed sampling strategies yet neglect the dynamic interplay between spatial context and visual similarity during training. We present SAGE (Spatial-visual Adaptive Graph Exploration), a unified training pipeline that enhances granular spatial-visual discrimination by jointly improving local feature aggregation, organize samples during training, and hard sample mining. We introduce a lightweight Soft Probing module that learns residual weights from training data for patch descriptors before bilinear aggregation, boosting distinctive local cues. During training we reconstruct an online geo-visual graph that fuses geographic proximity and current visual similarity so that candidate neighborhoods reflect the evolving embedding landscape. To concentrate learning on the most informative place neighborhoods, we seed clusters from high-affinity anchors and iteratively expand them with a greedy weighted clique expansion sampler. Implemented with a frozen DINOv2 backbone and parameter-efficient fine-tuning, SAGE achieves SOTA across eight benchmarks. It attains 98.9%, 95.8%, 94.5%, and 96.0% Recall@1 on SPED, Pitts30k-test, MSLS-val, and Nordland, respectively. Notably, our method obtains 100% Recall@10 on SPED only using 4096D global descriptors. Code and model will be available at: this https URL.</li>
</ul>

<h3>Title: Atomic Thinking of LLMs: Decoupling and Exploring Mathematical Reasoning Abilities</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Kuang, Haojing Huang, Yinghui Li, Xinnian Liang, Zhikun Xu, Yangning Li, Xiaoyu Tan, Chao Qu, Meishan Zhang, Ying Shen, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25725">https://arxiv.org/abs/2509.25725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25725">https://arxiv.org/pdf/2509.25725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25725]] Atomic Thinking of LLMs: Decoupling and Exploring Mathematical Reasoning Abilities(https://arxiv.org/abs/2509.25725)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated outstanding performance in mathematical reasoning capabilities. However, we argue that current large-scale reasoning models primarily rely on scaling up training datasets with diverse mathematical problems and long thinking chains, which raises questions about whether LLMs genuinely acquire mathematical concepts and reasoning principles or merely remember the training data. In contrast, humans tend to break down complex problems into multiple fundamental atomic capabilities. Inspired by this, we propose a new paradigm for evaluating mathematical atomic capabilities. Our work categorizes atomic abilities into two dimensions: (1) field-specific abilities across four major mathematical fields, algebra, geometry, analysis, and topology, and (2) logical abilities at different levels, including conceptual understanding, forward multi-step reasoning with formal math language, and counterexample-driven backward reasoning. We propose corresponding training and evaluation datasets for each atomic capability unit, and conduct extensive experiments about how different atomic capabilities influence others, to explore the strategies to elicit the required specific atomic capability. Evaluation and experimental results on advanced models show many interesting discoveries and inspirations about the different performances of models on various atomic capabilities and the interactions between atomic capabilities. Our findings highlight the importance of decoupling mathematical intelligence into atomic components, providing new insights into model cognition and guiding the development of training strategies toward a more efficient, transferable, and cognitively grounded paradigm of "atomic thinking".</li>
</ul>

<h3>Title: Controlled Generation for Private Synthetic Text</h3>
<ul>
<li><strong>Authors: </strong>Zihao Zhao, Anjalie Field</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25729">https://arxiv.org/abs/2509.25729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25729">https://arxiv.org/pdf/2509.25729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25729]] Controlled Generation for Private Synthetic Text(https://arxiv.org/abs/2509.25729)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Text anonymization is essential for responsibly developing and deploying AI in high-stakes domains such as healthcare, social services, and law. In this work, we propose a novel methodology for privacy-preserving synthetic text generation that leverages the principles of de-identification and the Hiding In Plain Sight (HIPS) theory. Our approach introduces entity-aware control codes to guide controllable generation using either in-context learning (ICL) or prefix tuning. The ICL variant ensures privacy levels consistent with the underlying de-identification system, while the prefix tuning variant incorporates a custom masking strategy and loss function to support scalable, high-quality generation. Experiments on legal and clinical datasets demonstrate that our method achieves a strong balance between privacy protection and utility, offering a practical and effective solution for synthetic text generation in sensitive domains.</li>
</ul>

<h3>Title: LaTo: Landmark-tokenized Diffusion Transformer for Fine-grained Human Face Editing</h3>
<ul>
<li><strong>Authors: </strong>Zhenghao Zhang, Ziying Zhang, Junchao Liao, Xiangyu Meng, Qiang Hu, Siyu Zhu, Xiaoyun Zhang, Long Qin, Weizhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25731">https://arxiv.org/abs/2509.25731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25731">https://arxiv.org/pdf/2509.25731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25731]] LaTo: Landmark-tokenized Diffusion Transformer for Fine-grained Human Face Editing(https://arxiv.org/abs/2509.25731)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Recent multimodal models for instruction-based face editing enable semantic manipulation but still struggle with precise attribute control and identity preservation. Structural facial representations such as landmarks are effective for intermediate supervision, yet most existing methods treat them as rigid geometric constraints, which can degrade identity when conditional landmarks deviate significantly from the source (e.g., large expression or pose changes, inaccurate landmark estimates). To address these limitations, we propose LaTo, a landmark-tokenized diffusion transformer for fine-grained, identity-preserving face editing. Our key innovations include: (1) a landmark tokenizer that directly quantizes raw landmark coordinates into discrete facial tokens, obviating the need for dense pixel-wise correspondence; (2) a location-mapping positional encoding that integrates facial and image tokens for unified processing, enabling flexible yet decoupled geometry-appearance interactions with high efficiency and strong identity preservation; and (3) a landmark predictor that leverages vision-language models to infer target landmarks from instructions and source images, whose structured chain-of-thought improves estimation accuracy and interactive control. To mitigate data scarcity, we curate HFL-150K, to our knowledge the largest benchmark for this task, containing over 150K real face pairs with fine-grained instructions. Extensive experiments show that LaTo outperforms state-of-the-art methods by 7.8% in identity preservation and 4.6% in semantic consistency. Code and dataset will be made publicly available upon acceptance.</li>
</ul>

<h3>Title: CATCH: A Novel Data Synthesis Framework for High Therapy Fidelity and Memory-Driven Planning Chain of Thought in AI Counseling</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Chen, Jingkai Lin, Zhaojie Chu, Xiaofen Xing, Yirong Chen, Xiangmin Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25733">https://arxiv.org/abs/2509.25733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25733">https://arxiv.org/pdf/2509.25733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25733]] CATCH: A Novel Data Synthesis Framework for High Therapy Fidelity and Memory-Driven Planning Chain of Thought in AI Counseling(https://arxiv.org/abs/2509.25733)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, advancements in AI counseling based on large language models have shown significant progress. However, existing studies employ a one-time generation approach to synthesize multi-turn dialogue samples, resulting in low therapy fidelity and failing to capture the decision-making rationale behind each response. In this work, we propose CATCH, a novel data synthesis framework designed to address these challenges. Specifically, to improve therapy fidelity, we introduce the Progressive Dialogue Synthesis strategy, which extracts goals, resources, and solutions from a client's self-report, organizes them into structured outlines, and then incrementally generates stage-aligned counseling dialogues. To capture decision-making rationale behind each response, we propose the Memory-Driven Dynamic Planning thinking pattern that integrates memory enhancement, global planning, and strategy reasoning; a collaborative multi-agent optimizer then leverages MDP to attach explicit chain-of-thought to each dialogue turn. Extensive experiments and human evaluations demonstrate that CATCH significantly enhances fidelity and logical coherence in AI counseling.</li>
</ul>

<h3>Title: Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications</h3>
<ul>
<li><strong>Authors: </strong>Chenhua Shi, Gregor Macdonald, Bhavika Jalli, Wanlu Lei, John Zou, Mridul Jain, Joji Philip</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IT, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25736">https://arxiv.org/abs/2509.25736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25736">https://arxiv.org/pdf/2509.25736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25736]] Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications(https://arxiv.org/abs/2509.25736)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The success of large language models (LLMs) depends heavily on large-scale, high-quality instruction-following and reinforcement datasets. However, generating such data through human annotation is prohibitively time-consuming particularly for domain-specific tasks like telecom network troubleshooting, where accurate responses require deep technical expertise and contextual understanding. In this paper, we present a fully automated, retrieval-augmented pipeline for generating synthetic question-answer (QA) pairs grounded in structured domain knowledge. Our multi-stage framework integrates a retriever, base generator, and refinement model to synthesize and enhance QA pairs using documents retrieved from a domain-specific knowledge graph. To ensure data quality, we employ customized RAGAS-based scoring to filter low-quality samples, producing a high-quality dataset suitable for reinforcement fine-tuning (RFT). We demonstrate our approach in a real-world telecom scenario focused on radio access network (RAN) troubleshooting. The resulting pipeline generates complex, context-rich troubleshooting solution plans without human intervention. This work offers a scalable solution for building instruction and reinforcement datasets in specialized domains, significantly reducing dependence on manual labeling while maintaining high technical fidelity.</li>
</ul>

<h3>Title: The 1st Solution for MOSEv1 Challenge on LSVOS 2025: CGFSeg</h3>
<ul>
<li><strong>Authors: </strong>Tingmin Li, Yixuan Li, Yang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25738">https://arxiv.org/abs/2509.25738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25738">https://arxiv.org/pdf/2509.25738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25738]] The 1st Solution for MOSEv1 Challenge on LSVOS 2025: CGFSeg(https://arxiv.org/abs/2509.25738)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Video Object Segmentation (VOS) aims to track and segment specific objects across entire video sequences, yet it remains highly challenging under complex real-world scenarios. The MOSEv1 and LVOS dataset, adopted in the MOSEv1 challenge on LSVOS 2025, which is specifically designed to enhance the robustness of VOS models in complex real-world scenarios, including long-term object disappearances and reappearances, as well as the presence of small and inconspicuous objects. In this paper, we present our improved method, Confidence-Guided Fusion Segmentation (CGFSeg), for the VOS task in the MOSEv1 Challenge. During training, the feature extractor of SAM2 is frozen, while the remaining components are fine-tuned to preserve strong feature extraction ability and improve segmentation accuracy. In the inference stage, we introduce a pixel-check strategy that progressively refines predictions by exploiting complementary strengths of multiple models, thereby yielding robust final masks. As a result, our method achieves a J&F score of 86.37% on the test set, ranking 1st in the MOSEv1 Challenge at LSVOS 2025. These results highlight the effectiveness of our approach in addressing the challenges of VOS task in complex scenarios.</li>
</ul>

<h3>Title: LieHMR: Autoregressive Human Mesh Recovery with $SO(3)$ Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Donghwan Kim, Tae-Kyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25739">https://arxiv.org/abs/2509.25739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25739">https://arxiv.org/pdf/2509.25739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25739]] LieHMR: Autoregressive Human Mesh Recovery with $SO(3)$ Diffusion(https://arxiv.org/abs/2509.25739)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We tackle the problem of Human Mesh Recovery (HMR) from a single RGB image, formulating it as an image-conditioned human pose and shape generation. While recovering 3D human pose from 2D observations is inherently ambiguous, most existing approaches have regressed a single deterministic output. Probabilistic methods attempt to address this by generating multiple plausible outputs to model the ambiguity. However, these methods often exhibit a trade-off between accuracy and sample diversity, and their single predictions are not competitive with state-of-the-art deterministic models. To overcome these limitations, we propose a novel approach that models well-aligned distribution to 2D observations. In particular, we introduce $SO(3)$ diffusion model, which generates the distribution of pose parameters represented as 3D rotations unconditional and conditional to image observations via conditioning dropout. Our model learns the hierarchical structure of human body joints using the transformer. Instead of using transformer as a denoising model, the time-independent transformer extracts latent vectors for the joints and a small MLP-based denoising model learns the per-joint distribution conditioned on the latent vector. We experimentally demonstrate and analyze that our model predicts accurate pose probability distribution effectively.</li>
</ul>

<h3>Title: Less is More: Towards Simple Graph Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Yanan Zhao, Feng Ji, Jingyang Dai, Jiaze Ma, Wee Peng Tay</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25742">https://arxiv.org/abs/2509.25742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25742">https://arxiv.org/pdf/2509.25742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25742]] Less is More: Towards Simple Graph Contrastive Learning(https://arxiv.org/abs/2509.25742)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Graph Contrastive Learning (GCL) has shown strong promise for unsupervised graph representation learning, yet its effectiveness on heterophilic graphs, where connected nodes often belong to different classes, remains limited. Most existing methods rely on complex augmentation schemes, intricate encoders, or negative sampling, which raises the question of whether such complexity is truly necessary in this challenging setting. In this work, we revisit the foundations of supervised and unsupervised learning on graphs and uncover a simple yet effective principle for GCL: mitigating node feature noise by aggregating it with structural features derived from the graph topology. This observation suggests that the original node features and the graph structure naturally provide two complementary views for contrastive learning. Building on this insight, we propose an embarrassingly simple GCL model that uses a GCN encoder to capture structural features and an MLP encoder to isolate node feature noise. Our design requires neither data augmentation nor negative sampling, yet achieves state-of-the-art results on heterophilic benchmarks with minimal computational and memory overhead, while also offering advantages in homophilic graphs in terms of complexity, scalability, and robustness. We provide theoretical justification for our approach and validate its effectiveness through extensive experiments, including robustness evaluations against both black-box and white-box adversarial attacks.</li>
</ul>

<h3>Title: Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space</h3>
<ul>
<li><strong>Authors: </strong>Xiang Zhang, Kun Wei, Xu Yang, Chenghao Xu, Su Yan, Cheng Deng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25743">https://arxiv.org/abs/2509.25743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25743">https://arxiv.org/pdf/2509.25743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25743]] Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space(https://arxiv.org/abs/2509.25743)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) become increasingly prevalent, their security vulnerabilities have already drawn attention. Machine unlearning is introduced to seek to mitigate these risks by removing the influence of undesirable data. However, existing methods not only rely on the retained dataset to preserve model utility, but also suffer from cumulative catastrophic utility loss under continuous unlearning requests. To solve this dilemma, we propose a novel method, called Rotation Control Unlearning (RCU), which leverages the rotational salience weight of RCU to quantify and control the unlearning degree in the continuous unlearning process. The skew symmetric loss is designed to construct the existence of the cognitive rotation space, where the changes of rotational angle can simulate the continuous unlearning process. Furthermore, we design an orthogonal rotation axes regularization to enforce mutually perpendicular rotation directions for continuous unlearning requests, effectively minimizing interference and addressing cumulative catastrophic utility loss. Experiments on multiple datasets confirm that our method without retained dataset achieves SOTA performance.</li>
</ul>

<h3>Title: IPDRecon: Image-Plane Geometric Decoding for View-Invariant Indoor Scene Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Mingyang Li, Yimeng Fan, Changsong Liu, Tianyu Zhou, Xin Wang, Yanyan Liu, Wei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25744">https://arxiv.org/abs/2509.25744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25744">https://arxiv.org/pdf/2509.25744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25744]] IPDRecon: Image-Plane Geometric Decoding for View-Invariant Indoor Scene Reconstruction(https://arxiv.org/abs/2509.25744)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Volume-based indoor scene reconstruction methods demonstrate significant research value due to their superior generalization capability and real-time deployment potential. However, existing methods rely on multi-view pixel back-projection ray intersections as weak geometric constraints to determine spatial positions, causing reconstruction quality to depend heavily on input view density with poor performance in overlapping regions and unobserved areas. To address these issues, the key lies in reducing dependency on inter-view geometric constraints while exploiting rich spatial information within individual views. We propose IPDRecon, an image-plane decoding framework comprising three core components: Pixel-level Confidence Encoder (PCE), Affine Compensation Module (ACM), and Image-Plane Spatial Decoder (IPSD). These modules collaboratively decode 3D structural information encoded in 2D images through physical imaging processes, effectively preserving spatial geometric features including edges, hollow structures, and complex textures while significantly enhancing view-invariant reconstruction. Experiments on ScanNetV2 confirm that IPDRecon achieves superior reconstruction stability, maintaining nearly identical quality when view count reduces by 40%. The method achieves a coefficient of variation of only 0.24%, performance retention rate of 99.7%, and maximum performance drop of merely 0.42%. This demonstrates that exploiting intra-view spatial information provides a robust solution for view-limited scenarios in practical applications.</li>
</ul>

<h3>Title: FinCap: Topic-Aligned Captions for Short-Form Financial YouTube Videos</h3>
<ul>
<li><strong>Authors: </strong>Siddhant Sukhani, Yash Bhardwaj, Riya Bhadani, Veer Kejriwal, Michael Galarnyk, Sudheer Chava</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25745">https://arxiv.org/abs/2509.25745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25745">https://arxiv.org/pdf/2509.25745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25745]] FinCap: Topic-Aligned Captions for Short-Form Financial YouTube Videos(https://arxiv.org/abs/2509.25745)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We evaluate multimodal large language models (MLLMs) for topic-aligned captioning in financial short-form videos (SVs) by testing joint reasoning over transcripts (T), audio (A), and video (V). Using 624 annotated YouTube SVs, we assess all seven modality combinations (T, A, V, TA, TV, AV, TAV) across five topics: main recommendation, sentiment analysis, video purpose, visual analysis, and financial entity recognition. Video alone performs strongly on four of five topics, underscoring its value for capturing visual context and effective cues such as emotions, gestures, and body language. Selective pairs such as TV or AV often surpass TAV, implying that too many modalities may introduce noise. These results establish the first baselines for financial short-form video captioning and illustrate the potential and challenges of grounding complex visual cues in this domain. All code and data can be found on our Github under the CC-BY-NC-SA 4.0 license.</li>
</ul>

<h3>Title: Dolphin v1.0 Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Taohan Weng, Chi zhang, Chaoran Yan, Siya Liu, Xiaoyang Liu, Yalun Wu, Boyang Wang, Boyan Wang, Jiren Ren, Kaiwen Yan, Jinze Yu, Kaibing Hu, Henan Liu, Haoyun zheng, Anjie Le, Hongcheng Guo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25748">https://arxiv.org/abs/2509.25748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25748">https://arxiv.org/pdf/2509.25748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25748]] Dolphin v1.0 Technical Report(https://arxiv.org/abs/2509.25748)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Ultrasound is crucial in modern medicine but faces challenges like operator dependence, image noise, and real-time scanning, hindering AI integration. While large multimodal models excel in other medical imaging areas, they struggle with ultrasound's complexities. To address this, we introduce Dolphin v1.0 (V1) and its reasoning-augmented version, Dolphin R1-the first large-scale multimodal ultrasound foundation models unifying diverse clinical tasks in a single vision-language this http URL tackle ultrasound variability and noise, we curated a 2-million-scale multimodal dataset, combining textbook knowledge, public data, synthetic samples, and general corpora. This ensures robust perception, generalization, and clinical this http URL Dolphin series employs a three-stage training strategy: domain-specialized pretraining, instruction-driven alignment, and reinforcement-based refinement. Dolphin v1.0 delivers reliable performance in classification, detection, regression, and report generation. Dolphin R1 enhances diagnostic inference, reasoning transparency, and interpretability through reinforcement learning with ultrasound-specific this http URL on U2-Bench across eight ultrasound tasks, Dolphin R1 achieves a U2-score of 0.5835-over twice the second-best model (0.2968) setting a new state of the art. Dolphin v1.0 also performs competitively, validating the unified framework. Comparisons show reasoning-enhanced training significantly improves diagnostic accuracy, consistency, and interpretability, highlighting its importance for high-stakes medical AI.</li>
</ul>

<h3>Title: ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual Try-On</h3>
<ul>
<li><strong>Authors: </strong>Junseo Park, Hyeryung Jang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25749">https://arxiv.org/abs/2509.25749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25749">https://arxiv.org/pdf/2509.25749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25749]] ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual Try-On(https://arxiv.org/abs/2509.25749)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Virtual try-on (VITON) aims to generate realistic images of a person wearing a target garment, requiring precise garment alignment in try-on regions and faithful preservation of identity and background in non-try-on regions. While latent diffusion models (LDMs) have advanced alignment and detail synthesis, preserving non-try-on regions remains challenging. A common post-hoc strategy directly replaces these regions with original content, but abrupt transitions often produce boundary artifacts. To overcome this, we reformulate VITON as a linear inverse problem and adopt trajectory-aligned solvers that progressively enforce measurement consistency, reducing abrupt changes in non-try-on regions. However, existing solvers still suffer from semantic drift during generation, leading to artifacts. We propose ART-VITON, a measurement-guided diffusion framework that ensures measurement adherence while maintaining artifact-free synthesis. Our method integrates residual prior-based initialization to mitigate training-inference mismatch and artifact-free measurement-guided sampling that combines data consistency, frequency-level correction, and periodic standard denoising. Experiments on VITON-HD, DressCode, and SHHQ-1.0 demonstrate that ART-VITON effectively preserves identity and background, eliminates boundary artifacts, and consistently improves visual fidelity and robustness over state-of-the-art baselines.</li>
</ul>

<h3>Title: Detecting Hope Across Languages: Multiclass Classification for Positive Online Discourse</h3>
<ul>
<li><strong>Authors: </strong>T. O.Abiola, K. D. Abiodun, O. E. Olumide, O. O. Adebanji, O. Hiram Calvo, Grigori Sidorov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25752">https://arxiv.org/abs/2509.25752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25752">https://arxiv.org/pdf/2509.25752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25752]] Detecting Hope Across Languages: Multiclass Classification for Positive Online Discourse(https://arxiv.org/abs/2509.25752)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The detection of hopeful speech in social media has emerged as a critical task for promoting positive discourse and well-being. In this paper, we present a machine learning approach to multiclass hope speech detection across multiple languages, including English, Urdu, and Spanish. We leverage transformer-based models, specifically XLM-RoBERTa, to detect and categorize hope speech into three distinct classes: Generalized Hope, Realistic Hope, and Unrealistic Hope. Our proposed methodology is evaluated on the PolyHope dataset for the PolyHope-M 2025 shared task, achieving competitive performance across all languages. We compare our results with existing models, demonstrating that our approach significantly outperforms prior state-of-the-art techniques in terms of macro F1 scores. We also discuss the challenges in detecting hope speech in low-resource languages and the potential for improving generalization. This work contributes to the development of multilingual, fine-grained hope speech detection models, which can be applied to enhance positive content moderation and foster supportive online communities.</li>
</ul>

<h3>Title: TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhepei Wei, Xiao Yang, Kai Sun, Jiaqi Wang, Rulin Shao, Sean Chen, Mohammad Kachuee, Teja Gollapudi, Tony Liao, Nicolas Scheffer, Rakesh Wanga, Anuj Kumar, Yu Meng, Wen-tau Yih, Xin Luna Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25760">https://arxiv.org/abs/2509.25760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25760">https://arxiv.org/pdf/2509.25760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25760]] TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning(https://arxiv.org/abs/2509.25760)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy -- models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs.</li>
</ul>

<h3>Title: OPPO: Accelerating PPO-based RLHF via Pipeline Overlap</h3>
<ul>
<li><strong>Authors: </strong>Kaizhuo Yan (1), Yingjie Yu (1), Yifan Yu (1), Haizhong Zheng (2), Fan Lai (1) ((1) University of Illinois Urbana-Champaign, (2) Carnegie Mellon University)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25762">https://arxiv.org/abs/2509.25762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25762">https://arxiv.org/pdf/2509.25762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25762]] OPPO: Accelerating PPO-based RLHF via Pipeline Overlap(https://arxiv.org/abs/2509.25762)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Proximal Policy Optimization (PPO)-based reinforcement learning from human feedback (RLHF) is a widely adopted paradigm for aligning large language models (LLMs) with human preferences. However, its training pipeline suffers from substantial inefficiencies due to sequential multi-model dependencies (e.g., reward model depends on actor outputs) and long-tail response lengths, where a few long responses straggle the stage completion. We present OPPO, a novel, lightweight, and model-agnostic PPO-based RLHF framework that improves training efficiency by overlapping pipeline execution. OPPO introduces two novel techniques: (1) Intra-step overlap, which streams upstream model outputs (e.g., actor model) in right-sized chunks, enabling the downstream model (e.g., reward) to begin prefill while the upstream continues decoding; and (2) Inter-step overlap, which adaptively overcommits a few prompts and defers long generations to future steps, mitigating tail latency without discarding partial work. OPPO integrates easily with existing PPO implementations with a few lines of code change. Extensive evaluations show that OPPO accelerates PPO-based RLHF training by $1.8 \times-2.8 \times$ and improves GPU utilization by $1.4 \times-2.1 \times$ without compromising training convergence.</li>
</ul>

<h3>Title: Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs</h3>
<ul>
<li><strong>Authors: </strong>Jia Jun Cheng Xian, Muchen Li, Haotian Yang, Xin Tao, Pengfei Wan, Leonid Sigal, Renjie Liao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25771">https://arxiv.org/abs/2509.25771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25771">https://arxiv.org/pdf/2509.25771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25771]] Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs(https://arxiv.org/abs/2509.25771)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables "free-lunch" alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment. Our Open-source code is available at this https URL.</li>
</ul>

<h3>Title: V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhengpeng Shi, Hengli Li, Yanpeng Zhao, Jianqun Zhou, Yuxuan Wang, Qinrong Cui, Wei Bi, Songchun Zhu, Bo Zhao, Zilong Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25773">https://arxiv.org/abs/2509.25773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25773">https://arxiv.org/pdf/2509.25773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25773]] V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs(https://arxiv.org/abs/2509.25773)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>AI models capable of comprehending humor hold real-world promise -- for example, enhancing engagement in human-machine interactions. To gauge and diagnose the capacity of multimodal large language models (MLLMs) for humor understanding, we introduce v-HUB, a novel visual-centric video humor understanding benchmark. v-HUB comprises a curated collection of minimally verbal short videos, sourced from classic silent films and online resources, and reflecting real-world scenarios where humor can be appreciated purely through visual cues. Each video clip is paired with rich annotations, including captions, descriptions, and explanations, supporting evaluation tasks like caption matching and humor explanation. To broaden its applicability, we further construct an open-ended video QA task, making it readily integrable into existing video understanding benchmarks. We evaluate a diverse set of MLLMs, from specialized Video-LLMs to versatile OmniLLMs that can process audio, covering both open-source and proprietary domains. The experimental results expose the difficulties MLLMs face in comprehending humor from visual cues alone. For example, all models exhibit a marked performance drop on caption matching when moving from text-based to video-based evaluation (without audio). Our findings also demonstrate that incorporating audio helps with video humor understanding, highlighting the informativeness of sound and the promise of integrating richer modalities for complex video understanding tasks.</li>
</ul>

<h3>Title: PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Jeongjae Lee, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25774">https://arxiv.org/abs/2509.25774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25774">https://arxiv.org/pdf/2509.25774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25774]] PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models(https://arxiv.org/abs/2509.25774)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While reinforcement learning has advanced the alignment of text-to-image (T2I) models, state-of-the-art policy gradient methods are still hampered by training instability and high variance, hindering convergence speed and compromising image quality. Our analysis identifies a key cause of this instability: disproportionate credit assignment, in which the mathematical structure of the generative sampler produces volatile and non-proportional feedback across timesteps. To address this, we introduce Proportionate Credit Policy Optimization (PCPO), a framework that enforces proportional credit assignment through a stable objective reformulation and a principled reweighting of timesteps. This correction stabilizes the training process, leading to significantly accelerated convergence and superior image quality. The improvement in quality is a direct result of mitigating model collapse, a common failure mode in recursive training. PCPO substantially outperforms existing policy gradient baselines on all fronts, including the state-of-the-art DanceGRPO.</li>
</ul>

<h3>Title: Autonomy-Aware Clustering: When Local Decisions Supersede Global Prescriptions</h3>
<ul>
<li><strong>Authors: </strong>Amber Srivastava, Salar Basiri, Srinivasa Salapaka</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25775">https://arxiv.org/abs/2509.25775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25775">https://arxiv.org/pdf/2509.25775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25775]] Autonomy-Aware Clustering: When Local Decisions Supersede Global Prescriptions(https://arxiv.org/abs/2509.25775)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Clustering arises in a wide range of problem formulations, yet most existing approaches assume that the entities under clustering are passive and strictly conform to their assigned groups. In reality, entities often exhibit local autonomy, overriding prescribed associations in ways not fully captured by feature representations. Such autonomy can substantially reshape clustering outcomes -- altering cluster compositions, geometry, and cardinality -- with significant downstream effects on inference and decision-making. We introduce autonomy-aware clustering, a reinforcement (RL) learning framework that learns and accounts for the influence of local autonomy without requiring prior knowledge of its form. Our approach integrates RL with a deterministic annealing (DA) procedure, where, to determine underlying clusters, DA naturally promotes exploration in early stages of annealing and transitions to exploitation later. We also show that the annealing procedure exhibits phase transitions that enable design of efficient annealing schedules. To further enhance adaptability, we propose the Adaptive Distance Estimation Network (ADEN), a transformer-based attention model that learns dependencies between entities and cluster representatives within the RL loop, accommodates variable-sized inputs and outputs, and enables knowledge transfer across diverse problem instances. Empirical results show that our framework closely aligns with underlying data dynamics: even without explicit autonomy models, it achieves solutions close to the ground truth (gap ~3-4%), whereas ignoring autonomy leads to substantially larger gaps (~35-40%). The code and data are publicly available at this https URL.</li>
</ul>

<h3>Title: Editable Noise Map Inversion: Encoding Target-image into Noise For High-Fidelity Image Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Kang, Yong Suk Choi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25776">https://arxiv.org/abs/2509.25776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25776">https://arxiv.org/pdf/2509.25776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25776]] Editable Noise Map Inversion: Encoding Target-image into Noise For High-Fidelity Image Manipulation(https://arxiv.org/abs/2509.25776)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have achieved remarkable success in generating high-quality and diverse images. Building on these advancements, diffusion models have also demonstrated exceptional performance in text-guided image editing. A key strategy for effective image editing involves inverting the source image into editable noise maps associated with the target image. However, previous inversion methods face challenges in adhering closely to the target text prompt. The limitation arises because inverted noise maps, while enabling faithful reconstruction of the source image, restrict the flexibility needed for desired edits. To overcome this issue, we propose Editable Noise Map Inversion (ENM Inversion), a novel inversion technique that searches for optimal noise maps to ensure both content preservation and editability. We analyze the properties of noise maps for enhanced editability. Based on this analysis, our method introduces an editable noise refinement that aligns with the desired edits by minimizing the difference between the reconstructed and edited noise maps. Extensive experiments demonstrate that ENM Inversion outperforms existing approaches across a wide range of image editing tasks in both preservation and edit fidelity with target prompts. Our approach can also be easily applied to video editing, enabling temporal consistency and content manipulation across frames.</li>
</ul>

<h3>Title: Online Decision Making with Generative Action Sets</h3>
<ul>
<li><strong>Authors: </strong>Jianyu Xu, Vidhi Jain, Bryan Wilder, Aarti Singh</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25777">https://arxiv.org/abs/2509.25777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25777">https://arxiv.org/pdf/2509.25777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25777]] Online Decision Making with Generative Action Sets(https://arxiv.org/abs/2509.25777)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With advances in generative AI, decision-making agents can now dynamically create new actions during online learning, but action generation typically incurs costs that must be balanced against potential benefits. We study an online learning problem where an agent can generate new actions at any time step by paying a one-time cost, with these actions becoming permanently available for future use. The challenge lies in learning the optimal sequence of two-fold decisions: which action to take and when to generate new ones, further complicated by the triangular tradeoffs among exploitation, exploration and $\textit{creation}$. To solve this problem, we propose a doubly-optimistic algorithm that employs Lower Confidence Bounds (LCB) for action selection and Upper Confidence Bounds (UCB) for action generation. Empirical evaluation on healthcare question-answering datasets demonstrates that our approach achieves favorable generation-quality tradeoffs compared to baseline strategies. From theoretical perspectives, we prove that our algorithm achieves the optimal regret of $O(T^{\frac{d}{d+2}}d^{\frac{d}{d+2}} + d\sqrt{T\log T})$, providing the first sublinear regret bound for online learning with expanding action spaces.</li>
</ul>

<h3>Title: From Cheap Geometry to Expensive Physics: Elevating Neural Operators via Latent Shape Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Zhizhou Zhang, Youjia Wu, Kaixuan Zhang, Yanjia Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25788">https://arxiv.org/abs/2509.25788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25788">https://arxiv.org/pdf/2509.25788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25788]] From Cheap Geometry to Expensive Physics: Elevating Neural Operators via Latent Shape Pretraining(https://arxiv.org/abs/2509.25788)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Industrial design evaluation often relies on high-fidelity simulations of governing partial differential equations (PDEs). While accurate, these simulations are computationally expensive, making dense exploration of design spaces impractical. Operator learning has emerged as a promising approach to accelerate PDE solution prediction; however, its effectiveness is often limited by the scarcity of labeled physics-based data. At the same time, large numbers of geometry-only candidate designs are readily available but remain largely untapped. We propose a two-stage framework to better exploit this abundant, physics-agnostic resource and improve supervised operator learning under limited labeled data. In Stage 1, we pretrain an autoencoder on a geometry reconstruction task to learn an expressive latent representation without PDE labels. In Stage 2, the neural operator is trained in a standard supervised manner to predict PDE solutions, using the pretrained latent embeddings as inputs instead of raw point clouds. Transformer-based architectures are adopted for both the autoencoder and the neural operator to handle point cloud data and integrate both stages seamlessly. Across four PDE datasets and three state-of-the-art transformer-based neural operators, our approach consistently improves prediction accuracy compared to models trained directly on raw point cloud inputs. These results demonstrate that representations from physics-agnostic pretraining provide a powerful foundation for data-efficient operator learning.</li>
</ul>

<h3>Title: Assessing Algorithmic Bias in Language-Based Depression Detection: A Comparison of DNN and LLM Approaches</h3>
<ul>
<li><strong>Authors: </strong>Obed Junias, Prajakta Kini, Theodora Chaspari</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25795">https://arxiv.org/abs/2509.25795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25795">https://arxiv.org/pdf/2509.25795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25795]] Assessing Algorithmic Bias in Language-Based Depression Detection: A Comparison of DNN and LLM Approaches(https://arxiv.org/abs/2509.25795)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates algorithmic bias in language-based models for automated depression detection, focusing on socio-demographic disparities related to gender and race/ethnicity. Models trained using deep neural networks (DNN) based embeddings are compared to few-shot learning approaches with large language models (LLMs), evaluating both performance and fairness on clinical interview transcripts from the Distress Analysis Interview Corpus/Wizard-of-Oz (DAIC-WOZ). To mitigate bias, fairness-aware loss functions are applied to DNN-based models, while in-context learning with varied prompt framing and shot counts is explored for LLMs. Results indicate that LLMs outperform DNN-based models in depression classification, particularly for underrepresented groups such as Hispanic participants. LLMs also exhibit reduced gender bias compared to DNN-based embeddings, though racial disparities persist. Among fairness-aware techniques for mitigating bias in DNN-based embeddings, the worst-group loss, which is designed to minimize loss for the worst-performing demographic group, achieves a better balance between performance and fairness. In contrast, the fairness-regularized loss minimizes loss across all groups but performs less effectively. In LLMs, guided prompting with ethical framing helps mitigate gender bias in the 1-shot setting. However, increasing the number of shots does not lead to further reductions in disparities. For race/ethnicity, neither prompting strategy nor increasing $N$ in $N$-shot learning effectively reduces disparities.</li>
</ul>

<h3>Title: CardioForest: An Explainable Ensemble Learning Model for Automatic Wide QRS Complex Tachycardia Diagnosis from ECG</h3>
<ul>
<li><strong>Authors: </strong>Vaskar Chakma, Ju Xiaolin, Heling Cao, Xue Feng, Ji Xiaodong, Pan Haiyan, Gao Zhan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25804">https://arxiv.org/abs/2509.25804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25804">https://arxiv.org/pdf/2509.25804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25804]] CardioForest: An Explainable Ensemble Learning Model for Automatic Wide QRS Complex Tachycardia Diagnosis from ECG(https://arxiv.org/abs/2509.25804)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>This study aims to develop and evaluate an ensemble machine learning-based framework for the automatic detection of Wide QRS Complex Tachycardia (WCT) from ECG signals, emphasizing diagnostic accuracy and interpretability using Explainable AI. The proposed system integrates ensemble learning techniques, i.e., an optimized Random Forest known as CardioForest, and models like XGBoost and LightGBM. The models were trained and tested on ECG data from the publicly available MIMIC-IV dataset. The testing was carried out with the assistance of accuracy, balanced accuracy, precision, recall, F1 score, ROC-AUC, and error rate (RMSE, MAE) measures. In addition, SHAP (SHapley Additive exPlanations) was used to ascertain model explainability and clinical relevance. The CardioForest model performed best on all metrics, achieving a test accuracy of 94.95%, a balanced accuracy of 88.31%, and high precision and recall metrics. SHAP analysis confirmed the model's ability to rank the most relevant ECG features, such as QRS duration, in accordance with clinical intuitions, thereby fostering trust and usability in clinical practice. The findings recognize CardioForest as an extremely dependable and interpretable WCT detection model. Being able to offer accurate predictions and transparency through explainability makes it a valuable tool to help cardiologists make timely and well-informed diagnoses, especially for high-stakes and emergency scenarios.</li>
</ul>

<h3>Title: Adapting SAM with Dynamic Similarity Graphs for Few-Shot Parameter-Efficient Small Dense Object Detection: A Case Study of Chickpea Pods in Field Conditions</h3>
<ul>
<li><strong>Authors: </strong>Xintong Jiang, Yixue Liu, Mohamed Debbagh, Yu Tian, Valerio Hoyos-Villegas, Viacheslav Adamchuk, Shangpeng Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25805">https://arxiv.org/abs/2509.25805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25805">https://arxiv.org/pdf/2509.25805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25805]] Adapting SAM with Dynamic Similarity Graphs for Few-Shot Parameter-Efficient Small Dense Object Detection: A Case Study of Chickpea Pods in Field Conditions(https://arxiv.org/abs/2509.25805)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Parameter-Efficient Fine-Tuning (PEFT) of foundation models for agricultural computer vision tasks remains challenging due to limited training data and complex field conditions. This study introduces a Dynamic Similarity-based Graph Adaptation (DSGA) module to adapt the Segment Anything Model (SAM) under extreme data constraints for precise foreground and instance segmentation of small dense objects in complex agricultural environments. Through dynamic similarity graph construction with a learnable polynomial decay-initialized weight ranking mechanism and adaptive local feature aggregation, DSGA establishes robust spatial and dynamic similarity representation with only 4.00M trainable parameters, which is 4.26% of the original SAM. Integrating this graph-based feature adaptation with Low-Rank Adaptation (LoRA) creates a complementary optimization framework that effectively captures both local and global dependencies in image embeddings while preserving model stability and parameter efficiency. Experimental results on a challenging chickpea pod dataset demonstrated that DSGA with LoRA achieved superior performance across multiple metrics evaluated under 2, 4, 8 and 10 shots, with progressive performance gains as shot count increased. Quantitative metrics showed a 17.31% improvement in Structure-measure and a 62.36% gain in adaptive F-measure compared to the baseline SAM fine-tuning. Comprehensive ablation studies and visualization analyses through Grad-CAM and t-SNE validated the framework's effectiveness in feature discrimination. The proposed adaptation demonstrated practical utility for automated agricultural monitoring applications, achieving accurate pod-counting with an adjusted R-squared of 0.8987 for images with 10 to 120 pods under challenging field conditions.</li>
</ul>

<h3>Title: Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Zhang, Wenlin Yao, Changlong Yu, Yao Liu, Qingyu Yin, Bing Yin, Hyokun Yun, Lihong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25808">https://arxiv.org/abs/2509.25808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25808">https://arxiv.org/pdf/2509.25808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25808]] Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse(https://arxiv.org/abs/2509.25808)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved impressive reasoning performance, with reinforcement learning with verifiable rewards (RLVR) emerging as a standard paradigm for post-training. A representative algorithm, group relative policy optimization (GRPO) (Shao et al., 2024), computes advantages by normalizing outcome rewards within response groups, but suffers from a vanishing advantage issue when all responses in a group receive identical rewards. To address this issue, we propose Adaptive Rollout and Response Reuse Policy Optimization (AR3PO), a sampling efficient RLVR algorithm that introduces two novel techniques: adaptive rollout, which dynamically allocates more responses to difficult prompts while saving computation on easier ones, and response reuse, which leverages previously generated correct responses to provide useful training signals. We compare AR3PO with strong RLVR baselines on multiple representative benchmarks using two different families of base models. Across the 7B and 8B models, AR3PO consistently outperforms GRPO and matches or surpasses DAPO (Yu et al., 2025), reducing rollout cost by up to 4.2x. On the larger 32B model, AR3PO achieves comparable performance to DAPO at similar training steps while maintaining substantially lower rollout cost.</li>
</ul>

<h3>Title: Learning to Reason as Action Abstractions with Scalable Mid-Training RL</h3>
<ul>
<li><strong>Authors: </strong>Shenao Zhang, Donghan Yu, Yihao Feng, Bowen Jin, Zhaoran Wang, John Peebles, Zirui Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25810">https://arxiv.org/abs/2509.25810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25810">https://arxiv.org/pdf/2509.25810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25810]] Learning to Reason as Action Abstractions with Scalable Mid-Training RL(https://arxiv.org/abs/2509.25810)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models excel with reinforcement learning (RL), but fully unlocking this potential requires a mid-training stage. An effective mid-training phase should identify a compact set of useful actions and enable fast selection among them through online RL. We formalize this intuition by presenting the first theoretical result on how mid-training shapes post-training: it characterizes an action subspace that minimizes both the value approximation error from pruning and the RL error during subsequent planning. Our analysis reveals two key determinants of mid-training effectiveness: pruning efficiency, which shapes the prior of the initial RL policy, and its impact on RL convergence, which governs the extent to which that policy can be improved via online interactions. These results suggest that mid-training is most effective when the decision space is compact and the effective horizon is short, highlighting the importance of operating in the space of action abstractions rather than primitive actions. Building on these insights, we propose Reasoning as Action Abstractions (RA3), a scalable mid-training algorithm. Specifically, we derive a sequential variational lower bound and optimize it by iteratively discovering temporally-consistent latent structures via RL, followed by fine-tuning on the bootstrapped data. Experiments on code generation tasks demonstrate the effectiveness of our approach. Across multiple base models, RA3 improves the average performance on HumanEval and MBPP by 8 and 4 points over the base model and the next-token prediction baseline. Furthermore, RA3 achieves faster convergence and higher asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and Codeforces.</li>
</ul>

<h3>Title: Logo-VGR: Visual Grounded Reasoning for Open-world Logo Recognition</h3>
<ul>
<li><strong>Authors: </strong>Zichen Liang, Jingjing Fei, Jie Wang, Zheming Yang, Changqing Li, Pei Wu, Minghui Qiu, Fei Yang, Xialei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25811">https://arxiv.org/abs/2509.25811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25811">https://arxiv.org/pdf/2509.25811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25811]] Logo-VGR: Visual Grounded Reasoning for Open-world Logo Recognition(https://arxiv.org/abs/2509.25811)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in multimodal large language models (MLLMs) have been primarily evaluated on general-purpose benchmarks, while their applications in domain-specific scenarios, such as intelligent product moderation, remain underexplored. To address this gap, we introduce an open-world logo recognition benchmark, a core challenge in product moderation. Unlike traditional logo recognition methods that rely on memorizing representations of tens of thousands of brands-an impractical approach in real-world settings-our proposed method, Logo-VGR, enables generalization to large-scale brand recognition with supervision from only a small subset of brands. Specifically, we reformulate logo recognition as a comparison-based task, requiring the model to match product images with candidate logos rather than directly generating brand labels. We further observe that existing models tend to overfit by memorizing brand distributions instead of learning robust multimodal reasoning, which results in poor performance on unseen brands. To overcome this limitation, Logo-VGR introduces a new paradigm of domain-specific multimodal reasoning: Logo Perception Grounding injects domain knowledge, and Logo-Guided Visual Grounded Reasoning enhances the model's reasoning capability. Experimental results show that Logo-VGR outperforms strong baselines by nearly 10 points in OOD settings, demonstrating superior generalization.</li>
</ul>

<h3>Title: RoBiologyDataChoiceQA: A Romanian Dataset for improving Biology understanding of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dragos-Dumitru Ghinea, Adela-Nicoleta Corbeanu, Adrian-Marius Dumitran</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25813">https://arxiv.org/abs/2509.25813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25813">https://arxiv.org/pdf/2509.25813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25813]] RoBiologyDataChoiceQA: A Romanian Dataset for improving Biology understanding of Large Language Models(https://arxiv.org/abs/2509.25813)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, large language models (LLMs) have demonstrated significant potential across various natural language processing (NLP) tasks. However, their performance in domain-specific applications and non-English languages remains less explored. This study introduces a novel Romanian-language dataset for multiple-choice biology questions, carefully curated to assess LLM comprehension and reasoning capabilities in scientific contexts. Containing approximately 14,000 questions, the dataset provides a comprehensive resource for evaluating and improving LLM performance in biology. We benchmark several popular LLMs, analyzing their accuracy, reasoning patterns, and ability to understand domain-specific terminology and linguistic nuances. Additionally, we perform comprehensive experiments to evaluate the impact of prompt engineering, fine-tuning, and other optimization techniques on model performance. Our findings highlight both the strengths and limitations of current LLMs in handling specialized knowledge tasks in low-resource languages, offering valuable insights for future research and development.</li>
</ul>

<h3>Title: Personalized Scientific Figure Caption Generation: An Empirical Study on Author-Specific Writing Style Transfer</h3>
<ul>
<li><strong>Authors: </strong>Jaeyoung Kim, Jongho Lee, Hongjun Choi, Sion Jang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25817">https://arxiv.org/abs/2509.25817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25817">https://arxiv.org/pdf/2509.25817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25817]] Personalized Scientific Figure Caption Generation: An Empirical Study on Author-Specific Writing Style Transfer(https://arxiv.org/abs/2509.25817)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We study personalized figure caption generation using author profile data from scientific papers. Our experiments demonstrate that rich author profile data, combined with relevant metadata, can significantly improve the personalization performance of multimodal large language models. However, we also reveal a fundamental trade-off between matching author style and maintaining caption quality. Our findings offer valuable insights and future directions for developing practical caption automation systems that balance both objectives. This work was conducted as part of the 3rd SciCap challenge.</li>
</ul>

<h3>Title: VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image Captions</h3>
<ul>
<li><strong>Authors: </strong>Kazuki Matsuda, Yuiga Wada, Shinnosuke Hirano, Seitaro Otsuki, Komei Sugiura</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25818">https://arxiv.org/abs/2509.25818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25818">https://arxiv.org/pdf/2509.25818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25818]] VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image Captions(https://arxiv.org/abs/2509.25818)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this study, we focus on the automatic evaluation of long and detailed image captions generated by multimodal Large Language Models (MLLMs). Most existing automatic evaluation metrics for image captioning are primarily designed for short captions and are not suitable for evaluating long captions. Moreover, recent LLM-as-a-Judge approaches suffer from slow inference due to their reliance on autoregressive inference and early fusion of visual information. To address these limitations, we propose VELA, an automatic evaluation metric for long captions developed within a novel LLM-Hybrid-as-a-Judge framework. Furthermore, we propose LongCap-Arena, a benchmark specifically designed for evaluating metrics for long captions. This benchmark comprises 7,805 images, the corresponding human-provided long reference captions and long candidate captions, and 32,246 human judgments from three distinct perspectives: Descriptiveness, Relevance, and Fluency. We demonstrated that VELA outperformed existing metrics and achieved superhuman performance on LongCap-Arena.</li>
</ul>

<h3>Title: Decentralized Asynchronous Multi-player Bandits</h3>
<ul>
<li><strong>Authors: </strong>Jingqi Fan, Canzhe Zhao, Shuai Li, Siwei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25824">https://arxiv.org/abs/2509.25824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25824">https://arxiv.org/pdf/2509.25824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25824]] Decentralized Asynchronous Multi-player Bandits(https://arxiv.org/abs/2509.25824)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In recent years, multi-player multi-armed bandits (MP-MAB) have been extensively studied due to their wide applications in cognitive radio networks and Internet of Things systems. While most existing research on MP-MAB focuses on synchronized settings, real-world systems are often decentralized and asynchronous, where players may enter or leave the system at arbitrary times, and do not have a global clock. This decentralized asynchronous setting introduces two major challenges. First, without a global time, players cannot implicitly coordinate their actions through time, making it difficult to avoid collisions. Second, it is important to detect how many players are in the system, but doing so may cost a lot. In this paper, we address the challenges posed by such a fully asynchronous setting in a decentralized environment. We develop a novel algorithm in which players adaptively change between exploration and exploitation. During exploration, players uniformly pull their arms, reducing the probability of collisions and effectively mitigating the first challenge. Meanwhile, players continue pulling arms currently exploited by others with a small probability, enabling them to detect when a player has left, thereby addressing the second challenge. We prove that our algorithm achieves a regret of $\mathcal{O}(\sqrt{T \log T} + {\log T}/{\Delta^2})$, where $\Delta$ is the minimum expected reward gap between any two arms. To the best of our knowledge, this is the first efficient MP-MAB algorithm in the asynchronous and decentralized environment. Extensive experiments further validate the effectiveness and robustness of our algorithm, demonstrating its applicability to real-world scenarios.</li>
</ul>

<h3>Title: Distillation of Large Language Models via Concrete Score Matching</h3>
<ul>
<li><strong>Authors: </strong>Yeongmin Kim, Donghyeok Shin, Mina Kang, Byeonghu Na, Il-Chul Moon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25837">https://arxiv.org/abs/2509.25837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25837">https://arxiv.org/pdf/2509.25837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25837]] Distillation of Large Language Models via Concrete Score Matching(https://arxiv.org/abs/2509.25837)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) deliver remarkable performance but are costly to deploy, motivating knowledge distillation (KD) for efficient inference. Existing KD objectives typically match student and teacher probabilities via softmax, which blurs valuable logit information. While direct logit distillation (DLD) mitigates softmax smoothing, it fails to account for logit shift invariance, thereby restricting the solution space. We propose Concrete Score Distillation (CSD), a discrete score-matching objective that overcomes both softmax-induced smoothing and restrictions on the optimal solution set. We resolve the training instability and quadratic complexity of discrete score-matching in autoregressive LLMs, and the resulting CSD objective aligns relative logit differences across all vocabulary pairs between student and teacher with flexible weighting. We provide both mode-seeking and mode-covering instances within our framework and evaluate CSD on task-agnostic instruction-following and task-specific distillation using GPT-2-1.5B, OpenLLaMA-7B, and GEMMA-7B-IT. Experiments show that CSD consistently surpasses recent KD objectives, achieves favorable fidelity-diversity trade-offs, and yields complementary gains when combined with on-policy techniques, demonstrating its scalability and effectiveness for LLM distillation.</li>
</ul>

<h3>Title: S$^2$FS: Spatially-Aware Separability-Driven Feature Selection in Fuzzy Decision Systems</h3>
<ul>
<li><strong>Authors: </strong>Suping Xu, Chuyi Dai, Ye Liu, Lin Shang, Xibei Yang, Witold Pedrycz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25841">https://arxiv.org/abs/2509.25841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25841">https://arxiv.org/pdf/2509.25841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25841]] S$^2$FS: Spatially-Aware Separability-Driven Feature Selection in Fuzzy Decision Systems(https://arxiv.org/abs/2509.25841)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Feature selection is crucial for fuzzy decision systems (FDSs), as it identifies informative features and eliminates rule redundancy, thereby enhancing predictive performance and interpretability. Most existing methods either fail to directly align evaluation criteria with learning performance or rely solely on non-directional Euclidean distances to capture relationships among decision classes, which limits their ability to clarify decision boundaries. However, the spatial distribution of instances has a potential impact on the clarity of such boundaries. Motivated by this, we propose Spatially-aware Separability-driven Feature Selection (S$^2$FS), a novel framework for FDSs guided by a spatially-aware separability criterion. This criterion jointly considers within-class compactness and between-class separation by integrating scalar-distances with spatial directional information, providing a more comprehensive characterization of class structures. S$^2$FS employs a forward greedy strategy to iteratively select the most discriminative features. Extensive experiments on ten real-world datasets demonstrate that S$^2$FS consistently outperforms eight state-of-the-art feature selection algorithms in both classification accuracy and clustering performance, while feature visualizations further confirm the interpretability of the selected features.</li>
</ul>

<h3>Title: Training-Free Reward-Guided Image Editing via Trajectory Optimal Control</h3>
<ul>
<li><strong>Authors: </strong>Jinho Chang, Jaemin Kim, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25845">https://arxiv.org/abs/2509.25845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25845">https://arxiv.org/pdf/2509.25845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25845]] Training-Free Reward-Guided Image Editing via Trajectory Optimal Control(https://arxiv.org/abs/2509.25845)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion and flow-matching models have demonstrated remarkable capabilities in high-fidelity image synthesis. A prominent line of research involves reward-guided guidance, which steers the generation process during inference to align with specific objectives. However, leveraging this reward-guided approach to the task of image editing, which requires preserving the semantic content of the source image while enhancing a target reward, is largely unexplored. In this work, we introduce a novel framework for training-free, reward-guided image editing. We formulate the editing process as a trajectory optimal control problem where the reverse process of a diffusion model is treated as a controllable trajectory originating from the source image, and the adjoint states are iteratively updated to steer the editing process. Through extensive experiments across distinct editing tasks, we demonstrate that our approach significantly outperforms existing inversion-based training-free guidance baselines, achieving a superior balance between reward maximization and fidelity to the source image without reward hacking.</li>
</ul>

<h3>Title: More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, Fabian Waschkowski, Lukas Wesemann, Peter Tu, Jing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25848">https://arxiv.org/abs/2509.25848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25848">https://arxiv.org/pdf/2509.25848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25848]] More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models(https://arxiv.org/abs/2509.25848)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning has emerged as a pivotal capability in Large Language Models (LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy Optimization (GRPO), these models are able to solve complex tasks such as mathematics and code generation. Building on these advances, recent research has sought to extend reasoning to Vision-Language Models (VLMs), yielding promising results across diverse visual tasks. Despite this progress, our study uncovers the dual nature of multimodal reasoning: while it substantially enhances logical inference and facilitates performance on challenging problems, it may gradually impair perceptual grounding, leading to recognition failures on otherwise basic visual questions. Through further analysis, we attribute this phenomenon to visual forgetting, wherein prolonged reasoning causes the model to increasingly disregard visual input. To address this, we propose Vision-Anchored Policy Optimization (VAPO), a simple yet effective method that explicitly steers the reasoning process toward visually grounded trajectories. Our result model, VAPO-Thinker-7B, significantly strengthens the model's reliance on visual information and achieves new state-of-the-art results on a wide range of established benchmarks. Project page: this https URL</li>
</ul>

<h3>Title: Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation</h3>
<ul>
<li><strong>Authors: </strong>Ziniu Li, Congliang Chen, Tianyun Yang, Tian Ding, Ruoyu Sun, Ge Zhang, Wenhao Huang, Zhi-Quan Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25849">https://arxiv.org/abs/2509.25849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25849">https://arxiv.org/pdf/2509.25849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25849]] Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation(https://arxiv.org/abs/2509.25849)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can self-improve through reinforcement learning, where they generate trajectories to explore and discover better solutions. However, this exploration process is computationally expensive, often forcing current methods to assign limited exploration budgets to each task. This uniform allocation creates problematic edge cases: easy tasks consistently succeed while difficult tasks consistently fail, both producing zero gradients during training updates for the widely used Group Relative Policy Optimization (GRPO). We address this problem from the lens of exploration budget allocation. Viewing each task's exploration as an "item" with a distinct "value" and "cost", we establish a connection to the classical knapsack problem. This formulation allows us to derive an optimal assignment rule that adaptively distributes resources based on the model's current learning status. When applied to GRPO, our method increases the effective ratio of non-zero policy gradients by 20-40% during training. Acting as a computational "free lunch", our approach could reallocate exploration budgets from tasks where learning is saturated to those where it is most impactful. This enables significantly larger budgets (e.g., 93 rollouts) for especially challenging problems, which would be computationally prohibitive under a uniform allocation. These improvements translate to meaningful gains on mathematical reasoning benchmarks, with average improvements of 2-4 points and peak gains of 9 points on specific tasks. Notably, achieving comparable performance with traditional homogeneous allocation would require about 2x the computational resources.</li>
</ul>

<h3>Title: RL-Guided Data Selection for Language Model Finetuning</h3>
<ul>
<li><strong>Authors: </strong>Animesh Jha, Harshit Gupta, Ananjan Nandi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25850">https://arxiv.org/abs/2509.25850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25850">https://arxiv.org/pdf/2509.25850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25850]] RL-Guided Data Selection for Language Model Finetuning(https://arxiv.org/abs/2509.25850)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Data selection for finetuning Large Language Models (LLMs) can be framed as a budget-constrained optimization problem: maximizing a model's downstream performance under a strict training data budget. Solving this problem is generally intractable, and existing approximate approaches are pretraining-oriented and transfer poorly to the fine-tuning setting. We reformulate this problem as a tractable Markov Decision Process (MDP) and train agents using various Reinforcement Learning (RL) methods to learn optimal data selection policies, guided by an efficient, proxy-model-based reward signal. Across four datasets, training on a $5\%$ subset selected by our approach matches or outperforms fine-tuning on the full dataset by up to $10.8$ accuracy points, while cutting wall-clock training time by up to $2 \times$, highlighting the promise of RL-guided data selection.</li>
</ul>

<h3>Title: PatchEAD: Unifying Industrial Visual Prompting Frameworks for Patch-Exclusive Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Po-Han Huang, Jeng-Lin Li, Po-Hsuan Huang, Ming-Ching Chang, Wei-Chao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25856">https://arxiv.org/abs/2509.25856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25856">https://arxiv.org/pdf/2509.25856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25856]] PatchEAD: Unifying Industrial Visual Prompting Frameworks for Patch-Exclusive Anomaly Detection(https://arxiv.org/abs/2509.25856)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Industrial anomaly detection is increasingly relying on foundation models, aiming for strong out-of-distribution generalization and rapid adaptation in real-world deployments. Notably, past studies have primarily focused on textual prompt tuning, leaving the intrinsic visual counterpart fragmented into processing steps specific to each foundation model. We aim to address this limitation by proposing a unified patch-focused framework, Patch-Exclusive Anomaly Detection (PatchEAD), enabling training-free anomaly detection that is compatible with diverse foundation models. The framework constructs visual prompting techniques, including an alignment module and foreground masking. Our experiments show superior few-shot and batch zero-shot performance compared to prior work, despite the absence of textual features. Our study further examines how backbone structure and pretrained characteristics affect patch-similarity robustness, providing actionable guidance for selecting and configuring foundation models for real-world visual inspection. These results confirm that a well-unified patch-only framework can enable quick, calibration-light deployment without the need for carefully engineered textual prompts.</li>
</ul>

<h3>Title: LiDAR Point Cloud Colourisation Using Multi-Camera Fusion and Low-Light Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Pasindu Ranasinghe, Dibyayan Patra, Bikram Banerjee, Simit Raval</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25859">https://arxiv.org/abs/2509.25859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25859">https://arxiv.org/pdf/2509.25859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25859]] LiDAR Point Cloud Colourisation Using Multi-Camera Fusion and Low-Light Image Enhancement(https://arxiv.org/abs/2509.25859)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In recent years, the fusion of camera data with LiDAR measurements has emerged as a powerful approach to enhance spatial understanding. This study introduces a novel, hardware-agnostic methodology that generates colourised point clouds from mechanical LiDAR using multiple camera inputs, providing complete 360-degree coverage. The primary innovation lies in its robustness under low-light conditions, achieved through the integration of a low-light image enhancement module within the fusion pipeline. The system requires initial calibration to determine intrinsic camera parameters, followed by automatic computation of the geometric transformation between the LiDAR and cameras, removing the need for specialised calibration targets and streamlining the setup. The data processing framework uses colour correction to ensure uniformity across camera feeds before fusion. The algorithm was tested using a Velodyne Puck Hi-Res LiDAR and a four-camera configuration. The optimised software achieved real-time performance and reliable colourisation even under very low illumination, successfully recovering scene details that would otherwise remain undetectable.</li>
</ul>

<h3>Title: MAPLE: Multi-scale Attribute-enhanced Prompt Learning for Few-shot Whole Slide Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Junjie Zhou, Wei Shao, Yagao Yue, Wei Mu, Peng Wan, Qi Zhu, Daoqiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25863">https://arxiv.org/abs/2509.25863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25863">https://arxiv.org/pdf/2509.25863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25863]] MAPLE: Multi-scale Attribute-enhanced Prompt Learning for Few-shot Whole Slide Image Classification(https://arxiv.org/abs/2509.25863)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompt learning has emerged as a promising paradigm for adapting pre-trained vision-language models (VLMs) to few-shot whole slide image (WSI) classification by aligning visual features with textual representations, thereby reducing annotation cost and enhancing model generalization. Nevertheless, existing methods typically rely on slide-level prompts and fail to capture the subtype-specific phenotypic variations of histological entities (\emph{e.g.,} nuclei, glands) that are critical for cancer diagnosis. To address this gap, we propose Multi-scale Attribute-enhanced Prompt Learning (\textbf{MAPLE}), a hierarchical framework for few-shot WSI classification that jointly integrates multi-scale visual semantics and performs prediction at both the entity and slide levels. Specifically, we first leverage large language models (LLMs) to generate entity-level prompts that can help identify multi-scale histological entities and their phenotypic attributes, as well as slide-level prompts to capture global visual descriptions. Then, an entity-guided cross-attention module is proposed to generate entity-level features, followed by aligning with their corresponding subtype-specific attributes for fine-grained entity-level prediction. To enrich entity representations, we further develop a cross-scale entity graph learning module that can update these representations by capturing their semantic correlations within and across scales. The refined representations are then aggregated into a slide-level representation and aligned with the corresponding prompts for slide-level prediction. Finally, we combine both entity-level and slide-level outputs to produce the final prediction results. Results on three cancer cohorts confirm the effectiveness of our approach in addressing few-shot pathology diagnosis tasks.</li>
</ul>

<h3>Title: ReFACT: A Benchmark for Scientific Confabulation Detection with Positional Error Annotations</h3>
<ul>
<li><strong>Authors: </strong>Yindong Wang, Martin Preiß, Margarita Bugueño, Jan Vincent Hoffbauer, Abdullatif Ghajar, Tolga Buz, Gerard de Melo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25868">https://arxiv.org/abs/2509.25868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25868">https://arxiv.org/pdf/2509.25868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25868]] ReFACT: A Benchmark for Scientific Confabulation Detection with Positional Error Annotations(https://arxiv.org/abs/2509.25868)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) frequently confabulate scientific facts,severely undermining their trustworthiness. Addressing this challenge requires benchmarks that go beyond binary factuality and enable fine-grained evaluation. We introduce \textbf{ReFACT} (\textit{Reddit False And Correct Texts}), a benchmark of 1,001 expert-annotated question--answer pairs spanning diverse scientific domains for the detection of scientific confabulation. Each instance includes both a scientifically correct answer and a non-factual counterpart annotated with \textbf{precise error spans and error-types}. ReFACT enables multi-stage evaluation: (1) confabulation detection, (2) fine-grained error localization, and (3) correction. We benchmark 9 state-of-the-art LLMs, revealing limited performance ($\sim$50\% accuracy). Even top models such as GPT-4o fail to distinguish factual from confabulated scientific answers, raising concerns about the reliability of \textit{LLM-as-judge} evaluation paradigms. Our findings highlight the need for fine-grained, human-validated benchmarks to detect and correct scientific confabulation in domain-specific contexts. Dataset is released on \href{this https URL}{GitHub}\footnote{We provide the dataset at: this https URL}.</li>
</ul>

<h3>Title: ASR Under Noise: Exploring Robustness for Sundanese and Javanese</h3>
<ul>
<li><strong>Authors: </strong>Salsabila Zahirah Pranida, Muhammad Cendekia Airlangga, Rifo Ahmad Genadi, Shady Shehata</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25878">https://arxiv.org/abs/2509.25878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25878">https://arxiv.org/pdf/2509.25878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25878]] ASR Under Noise: Exploring Robustness for Sundanese and Javanese(https://arxiv.org/abs/2509.25878)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We investigate the robustness of Whisper-based automatic speech recognition (ASR) models for two major Indonesian regional languages: Javanese and Sundanese. While recent work has demonstrated strong ASR performance under clean conditions, their effectiveness in noisy environments remains unclear. To address this, we experiment with multiple training strategies, including synthetic noise augmentation and SpecAugment, and evaluate performance across a range of signal-to-noise ratios (SNRs). Our results show that noise-aware training substantially improves robustness, particularly for larger Whisper models. A detailed error analysis further reveals language-specific challenges, highlighting avenues for future improvements</li>
</ul>

<h3>Title: A Multimodal LLM Approach for Visual Question Answering on Multiparametric 3D Brain MRI</h3>
<ul>
<li><strong>Authors: </strong>Arvind Murari Vepa, Yannan Yu, Jingru Gan, Anthony Cuturrufo, Weikai Li, Wei Wang, Fabien Scalzo, Yizhou Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25889">https://arxiv.org/abs/2509.25889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25889">https://arxiv.org/pdf/2509.25889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25889]] A Multimodal LLM Approach for Visual Question Answering on Multiparametric 3D Brain MRI(https://arxiv.org/abs/2509.25889)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We introduce mpLLM, a prompt-conditioned hierarchical mixture-of-experts (MoE) architecture for visual question answering over multi-parametric 3D brain MRI (mpMRI). mpLLM routes across modality-level and token-level projection experts to fuse multiple interrelated 3D modalities, enabling efficient training without image--report pretraining. To address limited image-text paired supervision, mpLLM integrates a synthetic visual question answering (VQA) protocol that generates medically relevant VQA from segmentation annotations, and we collaborate with medical experts for clinical validation. mpLLM outperforms strong medical VLM baselines by 5.3% on average across multiple mpMRI datasets. Our study features three main contributions: (1) the first clinically validated VQA dataset for 3D brain mpMRI, (2) a novel multimodal LLM that handles multiple interrelated 3D modalities, and (3) strong empirical results that demonstrate the medical utility of our methodology. Ablations highlight the importance of modality-level and token-level experts and prompt-conditioned routing. We have included our source code in the supplementary materials and will release our dataset upon publication.</li>
</ul>

<h3>Title: RoleConflictBench: A Benchmark of Role Conflict Scenarios for Evaluating LLMs' Contextual Sensitivity</h3>
<ul>
<li><strong>Authors: </strong>Jisu Shin, Hoyun Song, Juhyun Oh, Changgeon Ko, Eunsu Kim, Chani Jung, Alice Oh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25897">https://arxiv.org/abs/2509.25897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25897">https://arxiv.org/pdf/2509.25897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25897]] RoleConflictBench: A Benchmark of Role Conflict Scenarios for Evaluating LLMs' Contextual Sensitivity(https://arxiv.org/abs/2509.25897)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Humans often encounter role conflicts -- social dilemmas where the expectations of multiple roles clash and cannot be simultaneously fulfilled. As large language models (LLMs) become increasingly influential in human decision-making, understanding how they behave in complex social situations is essential. While previous research has evaluated LLMs' social abilities in contexts with predefined correct answers, role conflicts represent inherently ambiguous social dilemmas that require contextual sensitivity: the ability to recognize and appropriately weigh situational cues that can fundamentally alter decision priorities. To address this gap, we introduce RoleConflictBench, a novel benchmark designed to evaluate LLMs' contextual sensitivity in complex social dilemmas. Our benchmark employs a three-stage pipeline to generate over 13K realistic role conflict scenarios across 65 roles, systematically varying their associated expectations (i.e., their responsibilities and obligations) and situational urgency levels. By analyzing model choices across 10 different LLMs, we find that while LLMs show some capacity to respond to these contextual cues, this sensitivity is insufficient. Instead, their decisions are predominantly governed by a powerful, inherent bias related to social roles rather than situational information. Our analysis quantifies these biases, revealing a dominant preference for roles within the Family and Occupation domains, as well as a clear prioritization of male roles and Abrahamic religions across most evaluatee models.</li>
</ul>

<h3>Title: PerQ: Efficient Evaluation of Multilingual Text Personalization Quality</h3>
<ul>
<li><strong>Authors: </strong>Dominik Macko, Andrew Pulver</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25903">https://arxiv.org/abs/2509.25903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25903">https://arxiv.org/pdf/2509.25903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25903]] PerQ: Efficient Evaluation of Multilingual Text Personalization Quality(https://arxiv.org/abs/2509.25903)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Since no metrics are available to evaluate specific aspects of a text, such as its personalization quality, the researchers often rely solely on large language models to meta-evaluate such texts. Due to internal biases of individual language models, it is recommended to use multiple of them for combined evaluation, which directly increases costs of such meta-evaluation. In this paper, a computationally efficient method for evaluation of personalization quality of a given text (generated by a language model) is introduced, called PerQ. A case study of comparison of generation capabilities of large and small language models shows the usability of the proposed metric in research, effectively reducing the waste of resources.</li>
</ul>

<h3>Title: Federated Learning with Enhanced Privacy via Model Splitting and Random Client Participation</h3>
<ul>
<li><strong>Authors: </strong>Yiwei Li, Shuai Wang, Zhuojun Tian, Xiuhua Wang, Shijian Su</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25906">https://arxiv.org/abs/2509.25906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25906">https://arxiv.org/pdf/2509.25906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25906]] Federated Learning with Enhanced Privacy via Model Splitting and Random Client Participation(https://arxiv.org/abs/2509.25906)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) often adopts differential privacy (DP) to protect client data, but the added noise required for privacy guarantees can substantially degrade model accuracy. To resolve this challenge, we propose model-splitting privacy-amplified federated learning (MS-PAFL), a novel framework that combines structural model splitting with statistical privacy amplification. In this framework, each client's model is partitioned into a private submodel, retained locally, and a public submodel, shared for global aggregation. The calibrated Gaussian noise is injected only into the public submodel, thereby confining its adverse impact while preserving the utility of the local model. We further present a rigorous theoretical analysis that characterizes the joint privacy amplification achieved through random client participation and local data subsampling under this architecture. The analysis provides tight bounds on both single-round and total privacy loss, demonstrating that MS-PAFL significantly reduces the noise necessary to satisfy a target privacy protection level. Extensive experiments validate our theoretical findings, showing that MS-PAFL consistently attains a superior privacy-utility trade-off and enables the training of highly accurate models under strong privacy guarantees.</li>
</ul>

<h3>Title: Mem-α: Learning Memory Construction via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yu Wang, Ryuichi Takanobu, Zhiqi Liang, Yuzhen Mao, Yuanzhe Hu, Julian McAuley, Xiaojian Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25911">https://arxiv.org/abs/2509.25911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25911">https://arxiv.org/pdf/2509.25911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25911]] Mem-α: Learning Memory Construction via Reinforcement Learning(https://arxiv.org/abs/2509.25911)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) agents are constrained by limited context windows, necessitating external memory systems for long-term information understanding. Current memory-augmented agents typically depend on pre-defined instructions and tools for memory updates. However, language models may lack the ability to determine which information to store, how to structure it, and when to update it, especially as memory systems become more complex. This results in suboptimal memory construction and information loss. To this end, we propose Mem-alpha, a reinforcement learning framework that trains agents to effectively manage complex memory systems through interaction and feedback. We also construct a specialized training dataset spanning diverse multi-turn interaction patterns paired with comprehensive evaluation questions designed to teach effective memory management. During training, agents process sequential information chunks, learn to extract and store relevant content, then update the memory system. The reward signal derives from downstream question-answering accuracy over the full interaction history, directly optimizing for memory construction. To illustrate the effectiveness of our training framework, we design a memory architecture comprising core, episodic, and semantic components, equipped with multiple tools for memory operations. Empirical evaluation demonstrates that Mem-alpha achieves significant improvements over existing memory-augmented agent baselines. Despite being trained exclusively on instances with a maximum length of 30k tokens, our agents exhibit remarkable generalization to sequences exceeding 400k tokens, over 13x the training length, highlighting the robustness of Mem-alpha.</li>
</ul>

<h3>Title: Understanding the Mixture-of-Experts with Nadaraya-Watson Kernel</h3>
<ul>
<li><strong>Authors: </strong>Chuanyang Zheng, Jiankai Sun, Yihang Gao, Enze Xie, Yuehao Wang, Peihao Wang, Ting Xu, Matthew Chang, Liliang Ren, Jingyao Li, Jing Xiong, Kashif Rasul, Mac Schwager, Anderson Schneider, Zhangyang Wang, Yuriy Nevmyvaka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25913">https://arxiv.org/abs/2509.25913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25913">https://arxiv.org/pdf/2509.25913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25913]] Understanding the Mixture-of-Experts with Nadaraya-Watson Kernel(https://arxiv.org/abs/2509.25913)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) has become a cornerstone in recent state-of-the-art large language models (LLMs). Traditionally, MoE relies on $\mathrm{Softmax}$ as the router score function to aggregate expert output, a designed choice that has persisted from the earliest MoE models to modern LLMs, and is now widely regarded as standard practice. However, the necessity of using $\mathrm{Softmax}$ to project router weights into a probability simplex remains an unchallenged assumption rather than a principled design choice. In this work, we first revisit the classical Nadaraya-Watson regression and observe that MoE shares the same mathematical formulation as Nadaraya-Watson regression. Furthermore, we show that both feed-forward neural network (FFN) and MoE can be interpreted as a special case of Nadaraya-Watson regression, where the kernel function corresponds to the input neurons of the output layer. Motivated by these insights, we propose the \textbf{zero-additional-cost} Kernel Inspired Router with Normalization (KERN), an FFN-style router function, as an alternative to $\mathrm{Softmax}$. We demonstrate that this router generalizes both $\mathrm{Sigmoid}$- and $\mathrm{Softmax}$-based routers. \textbf{Based on empirical observations and established practices in FFN implementation, we recommend the use of $\mathrm{ReLU}$ activation and $\ell_2$-normalization in $\mathrm{KERN}$ router function.} Comprehensive experiments in MoE and LLM validate the effectiveness of the proposed FFN-style router function \methodNorm.</li>
</ul>

<h3>Title: VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs</h3>
<ul>
<li><strong>Authors: </strong>Peng Liu, Haozhan Shen, Chunxin Fang, Zhicheng Sun, Jiajia Liao, Tiancheng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25916">https://arxiv.org/abs/2509.25916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25916">https://arxiv.org/pdf/2509.25916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25916]] VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs(https://arxiv.org/abs/2509.25916)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) excel at high-level scene understanding but falter on fine-grained perception tasks requiring precise localization. This failure stems from a fundamental mismatch, as generating exact numerical coordinates is a challenging task for language-centric architectures. In this paper, we introduce VLM-FO1, a novel framework that overcomes this limitation by reframing object-centric perception from a brittle coordinate generation problem into a robust feature retrieval task. Our method operates as a plug-and-play module that integrates with any pre-trained VLM. It leverages a Hybrid Fine-grained Region Encoder (HFRE), featuring a dual vision encoder, to generate powerful region tokens rich in both semantic and spatial detail. A token-based referencing system then enables the LLM to seamlessly reason about and ground language in these specific visual regions. Experiments show that VLM-FO1 achieves state-of-the-art performance across a diverse suite of benchmarks, demonstrating exceptional capabilities in object grounding, region generational understanding, and visual region reasoning. Crucially, our two-stage training strategy ensures that these perception gains are achieved without compromising the base model's general visual understanding capabilities. VLM-FO1 establishes an effective and flexible paradigm for building perception-aware VLMs, bridging the gap between high-level reasoning and fine-grained visual grounding.</li>
</ul>

<h3>Title: Bringing Emerging Architectures to Sequence Labeling in NLP</h3>
<ul>
<li><strong>Authors: </strong>Ana Ezquerro, Carlos Gómez-Rodríguez, David Vilares</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25918">https://arxiv.org/abs/2509.25918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25918">https://arxiv.org/pdf/2509.25918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25918]] Bringing Emerging Architectures to Sequence Labeling in NLP(https://arxiv.org/abs/2509.25918)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Pretrained Transformer encoders are the dominant approach to sequence labeling. While some alternative architectures-such as xLSTMs, structured state-space models, diffusion models, and adversarial learning-have shown promise in language modeling, few have been applied to sequence labeling, and mostly on flat or simplified tasks. We study how these architectures adapt across tagging tasks that vary in structural complexity, label space, and token dependencies, with evaluation spanning multiple languages. We find that the strong performance previously observed in simpler settings does not always generalize well across languages or datasets, nor does it extend to more complex structured tasks.</li>
</ul>

<h3>Title: Better Privilege Separation for Agents by Restricting Data Types</h3>
<ul>
<li><strong>Authors: </strong>Dennis Jacob, Emad Alghamdi, Zhanhao Hu, Basel Alomair, David Wagner</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25926">https://arxiv.org/abs/2509.25926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25926">https://arxiv.org/pdf/2509.25926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25926]] Better Privilege Separation for Agents by Restricting Data Types(https://arxiv.org/abs/2509.25926)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have become increasingly popular due to their ability to interact with unstructured content. As such, LLMs are now a key driver behind the automation of language processing systems, such as AI agents. Unfortunately, these advantages have come with a vulnerability to prompt injections, an attack where an adversary subverts the LLM's intended functionality with an injected task. Past approaches have proposed detectors and finetuning to provide robustness, but these techniques are vulnerable to adaptive attacks or cannot be used with state-of-the-art models. To this end we propose type-directed privilege separation for LLMs, a method that systematically prevents prompt injections. We restrict the ability of an LLM to interact with third-party data by converting untrusted content to a curated set of data types; unlike raw strings, each data type is limited in scope and content, eliminating the possibility for prompt injections. We evaluate our method across several case studies and find that designs leveraging our principles can systematically prevent prompt injection attacks while maintaining high utility.</li>
</ul>

<h3>Title: The Impact of Scaling Training Data on Adversarial Robustness</h3>
<ul>
<li><strong>Authors: </strong>Marco Zimmerli, Andreas Plesner, Till Aczel, Roger Wattenhofer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25927">https://arxiv.org/abs/2509.25927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25927">https://arxiv.org/pdf/2509.25927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25927]] The Impact of Scaling Training Data on Adversarial Robustness(https://arxiv.org/abs/2509.25927)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks remain vulnerable to adversarial examples despite advances in architectures and training paradigms. We investigate how training data characteristics affect adversarial robustness across 36 state-of-the-art vision models spanning supervised, self-supervised, and contrastive learning approaches, trained on datasets from 1.2M to 22B images. Models were evaluated under six black-box attack categories: random perturbations, two types of geometric masks, COCO object manipulations, ImageNet-C corruptions, and ImageNet-R style shifts. Robustness follows a logarithmic scaling law with both data volume and model size: a tenfold increase in data reduces attack success rate (ASR) on average by ~3.2%, whereas a tenfold increase in model size reduces ASR on average by ~13.4%. Notably, some self-supervised models trained on curated datasets, such as DINOv2, outperform others trained on much larger but less curated datasets, challenging the assumption that scale alone drives robustness. Adversarial fine-tuning of ResNet50s improves generalization across structural variations but not across color distributions. Human evaluation reveals persistent gaps between human and machine vision. These results show that while scaling improves robustness, data quality, architecture, and training objectives play a more decisive role than raw scale in achieving broad-spectrum adversarial resilience.</li>
</ul>

<h3>Title: CO3: Contrasting Concepts Compose Better</h3>
<ul>
<li><strong>Authors: </strong>Debottam Dutta, Jianchong Chen, Rajalaxmi Rajagopalan, Yu-Lin Wei, Romit Roy Choudhury</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25940">https://arxiv.org/abs/2509.25940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25940">https://arxiv.org/pdf/2509.25940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25940]] CO3: Contrasting Concepts Compose Better(https://arxiv.org/abs/2509.25940)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>We propose to improve multi-concept prompt fidelity in text-to-image diffusion models. We begin with common failure cases-prompts like "a cat and a dog" that sometimes yields images where one concept is missing, faint, or colliding awkwardly with another. We hypothesize that this happens when the diffusion model drifts into mixed modes that over-emphasize a single concept it learned strongly during training. Instead of re-training, we introduce a corrective sampling strategy that steers away from regions where the joint prompt behavior overlaps too strongly with any single concept in the prompt. The goal is to steer towards "pure" joint modes where all concepts can coexist with balanced visual presence. We further show that existing multi-concept guidance schemes can operate in unstable weight regimes that amplify imbalance; we characterize favorable regions and adapt sampling to remain within them. Our approach, CO3, is plug-and-play, requires no model tuning, and complements standard classifier-free guidance. Experiments on diverse multi-concept prompts indicate improvements in concept coverage, balance and robustness, with fewer dropped or distorted concepts compared to standard baselines and prior compositional methods. Results suggest that lightweight corrective guidance can substantially mitigate brittle semantic alignment behavior in modern diffusion systems.</li>
</ul>

<h3>Title: AIM: Adaptive Intervention for Deep Multi-task Learning of Molecular Properties</h3>
<ul>
<li><strong>Authors: </strong>Mason Minot, Gisbert Schneider</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25955">https://arxiv.org/abs/2509.25955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25955">https://arxiv.org/pdf/2509.25955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25955]] AIM: Adaptive Intervention for Deep Multi-task Learning of Molecular Properties(https://arxiv.org/abs/2509.25955)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Simultaneously optimizing multiple, frequently conflicting, molecular properties is a key bottleneck in the development of novel therapeutics. Although a promising approach, the efficacy of multi-task learning is often compromised by destructive gradient interference, especially in the data-scarce regimes common to drug discovery. To address this, we propose AIM, an optimization framework that learns a dynamic policy to mediate gradient conflicts. The policy is trained jointly with the main network using a novel augmented objective composed of dense, differentiable regularizers. This objective guides the policy to produce updates that are geometrically stable and dynamically efficient, prioritizing progress on the most challenging tasks. We demonstrate that AIM achieves statistically significant improvements over multi-task baselines on subsets of the QM9 and targeted protein degraders benchmarks, with its advantage being most pronounced in data-scarce regimes. Beyond performance, AIM's key contribution is its interpretability; the learned policy matrix serves as a diagnostic tool for analyzing inter-task relationships. This combination of data-efficient performance and diagnostic insight highlights the potential of adaptive optimizers to accelerate scientific discovery by creating more robust and insightful models for multi-property molecular design.</li>
</ul>

<h3>Title: Reliability Crisis of Reference-free Metrics for Grammatical Error Correction</h3>
<ul>
<li><strong>Authors: </strong>Takumi Goto, Yusuke Sakai, Taro Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25961">https://arxiv.org/abs/2509.25961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25961">https://arxiv.org/pdf/2509.25961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25961]] Reliability Crisis of Reference-free Metrics for Grammatical Error Correction(https://arxiv.org/abs/2509.25961)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Reference-free evaluation metrics for grammatical error correction (GEC) have achieved high correlation with human judgments. However, these metrics are not designed to evaluate adversarial systems that aim to obtain unjustifiably high scores. The existence of such systems undermines the reliability of automatic evaluation, as it can mislead users in selecting appropriate GEC systems. In this study, we propose adversarial attack strategies for four reference-free metrics: SOME, Scribendi, IMPARA, and LLM-based metrics, and demonstrate that our adversarial systems outperform the current state-of-the-art. These findings highlight the need for more robust evaluation methods.</li>
</ul>

<h3>Title: Self-Supervised Anatomical Consistency Learning for Vision-Grounded Medical Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Longzhen Yang, Zhangkai Ni, Ying Wen, Yihang Liu, Lianghua He, Heng Tao Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25963">https://arxiv.org/abs/2509.25963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25963">https://arxiv.org/pdf/2509.25963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25963]] Self-Supervised Anatomical Consistency Learning for Vision-Grounded Medical Report Generation(https://arxiv.org/abs/2509.25963)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Vision-grounded medical report generation aims to produce clinically accurate descriptions of medical images, anchored in explicit visual evidence to improve interpretability and facilitate integration into clinical workflows. However, existing methods often rely on separately trained detection modules that require extensive expert annotations, introducing high labeling costs and limiting generalizability due to pathology distribution bias across datasets. To address these challenges, we propose Self-Supervised Anatomical Consistency Learning (SS-ACL) -- a novel and annotation-free framework that aligns generated reports with corresponding anatomical regions using simple textual prompts. SS-ACL constructs a hierarchical anatomical graph inspired by the invariant top-down inclusion structure of human anatomy, organizing entities by spatial location. It recursively reconstructs fine-grained anatomical regions to enforce intra-sample spatial alignment, inherently guiding attention maps toward visually relevant areas prompted by text. To further enhance inter-sample semantic alignment for abnormality recognition, SS-ACL introduces a region-level contrastive learning based on anatomical consistency. These aligned embeddings serve as priors for report generation, enabling attention maps to provide interpretable visual evidence. Extensive experiments demonstrate that SS-ACL, without relying on expert annotations, (i) generates accurate and visually grounded reports -- outperforming state-of-the-art methods by 10\% in lexical accuracy and 25\% in clinical efficacy, and (ii) achieves competitive performance on various downstream visual tasks, surpassing current leading visual foundation models by 8\% in zero-shot visual grounding.</li>
</ul>

<h3>Title: Reevaluating Convolutional Neural Networks for Spectral Analysis: A Focus on Raman Spectroscopy</h3>
<ul>
<li><strong>Authors: </strong>Deniz Soysal, Xabier García-Andrade, Laura E. Rodriguez, Pablo Sobron, Laura M. Barge, Renaud Detry</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25964">https://arxiv.org/abs/2509.25964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25964">https://arxiv.org/pdf/2509.25964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25964]] Reevaluating Convolutional Neural Networks for Spectral Analysis: A Focus on Raman Spectroscopy(https://arxiv.org/abs/2509.25964)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Autonomous Raman instruments on Mars rovers, deep-sea landers, and field robots must interpret raw spectra distorted by fluorescence baselines, peak shifts, and limited ground-truth labels. Using curated subsets of the RRUFF database, we evaluate one-dimensional convolutional neural networks (CNNs) and report four advances: (i) Baseline-independent classification: compact CNNs surpass $k$-nearest-neighbors and support-vector machines on handcrafted features, removing background-correction and peak-picking stages while ensuring reproducibility through released data splits and scripts. (ii) Pooling-controlled robustness: tuning a single pooling parameter accommodates Raman shifts up to $30 \,\mathrm{cm}^{-1}$, balancing translational invariance with spectral resolution. (iii) Label-efficient learning: semi-supervised generative adversarial networks and contrastive pretraining raise accuracy by up to $11\%$ with only $10\%$ labels, valuable for autonomous deployments with scarce annotation. (iv) Constant-time adaptation: freezing the CNN backbone and retraining only the softmax layer transfers models to unseen minerals at $\mathcal{O}(1)$ cost, outperforming Siamese networks on resource-limited processors. This workflow, which involves training on raw spectra, tuning pooling, adding semi-supervision when labels are scarce, and fine-tuning lightly for new targets, provides a practical path toward robust, low-footprint Raman classification in autonomous exploration.</li>
</ul>

<h3>Title: PinPoint3D: Fine-Grained 3D Part Segmentation from a Few Clicks</h3>
<ul>
<li><strong>Authors: </strong>Bojun Zhang, Hangjian Ye, Hao Zheng, Jianzheng Huang, Zhengyu Lin, Zhenhong Guo, Feng Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25970">https://arxiv.org/abs/2509.25970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25970">https://arxiv.org/pdf/2509.25970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25970]] PinPoint3D: Fine-Grained 3D Part Segmentation from a Few Clicks(https://arxiv.org/abs/2509.25970)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Fine-grained 3D part segmentation is crucial for enabling embodied AI systems to perform complex manipulation tasks, such as interacting with specific functional components of an object. However, existing interactive segmentation methods are largely confined to coarse, instance-level targets, while non-interactive approaches struggle with sparse, real-world scans and suffer from a severe lack of annotated data. To address these limitations, we introduce PinPoint3D, a novel interactive framework for fine-grained, multi-granularity 3D segmentation, capable of generating precise part-level masks from only a few user point clicks. A key component of our work is a new 3D data synthesis pipeline that we developed to create a large-scale, scene-level dataset with dense part annotations, overcoming a critical bottleneck that has hindered progress in this field. Through comprehensive experiments and user studies, we demonstrate that our method significantly outperforms existing approaches, achieving an average IoU of around 55.8% on each object part under first-click settings and surpassing 71.3% IoU with only a few additional clicks. Compared to current state-of-the-art baselines, PinPoint3D yields up to a 16% improvement in IoU and precision, highlighting its effectiveness on challenging, sparse point clouds with high efficiency. Our work represents a significant step towards more nuanced and precise machine perception and interaction in complex 3D environments.</li>
</ul>

<h3>Title: Data-Free Continual Learning of Server Models in Model-Heterogeneous Federated learning</h3>
<ul>
<li><strong>Authors: </strong>Xiao Zhang, Zengzhe Chen, Yuan Yuan, Yifei Zou, Fuzhen Zhuang, Wenyu Jiao, Yuke Wang, Dongxiao Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25977">https://arxiv.org/abs/2509.25977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25977">https://arxiv.org/pdf/2509.25977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25977]] Data-Free Continual Learning of Server Models in Model-Heterogeneous Federated learning(https://arxiv.org/abs/2509.25977)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, diffusion, data-free, generative</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a distributed learning paradigm across multiple entities while preserving data privacy. However, with the continuous emergence of new data and increasing model diversity, traditional federated learning faces significant challenges, including inherent issues of data heterogeneity, model heterogeneity and catastrophic forgetting, along with new challenge of knowledge misalignment. In this study, we introduce FedDCL, a novel framework designed to enable data-free continual learning of the server model in a model-heterogeneous federated setting. We leverage pre-trained diffusion models to extract lightweight class-specific prototypes, which confer a threefold data-free advantage, enabling: (1) generation of synthetic data for the current task to augment training and counteract non-IID data distributions; (2) exemplar-free generative replay for retaining knowledge from previous tasks; and (3) data-free dynamic knowledge transfer from heterogeneous clients to the server. Experimental results on various datasets demonstrate the effectiveness of FedDCL, showcasing its potential to enhance the generalizability and practical applicability of federated learning in dynamic settings.</li>
</ul>

<h3>Title: Reconcile Certified Robustness and Accuracy for DNN-based Smoothed Majority Vote Classifier</h3>
<ul>
<li><strong>Authors: </strong>Gaojie Jin, Xinping Yi, Xiaowei Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25979">https://arxiv.org/abs/2509.25979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25979">https://arxiv.org/pdf/2509.25979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25979]] Reconcile Certified Robustness and Accuracy for DNN-based Smoothed Majority Vote Classifier(https://arxiv.org/abs/2509.25979)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Within the PAC-Bayesian framework, the Gibbs classifier (defined on a posterior $Q$) and the corresponding $Q$-weighted majority vote classifier are commonly used to analyze the generalization performance. However, there exists a notable lack in theoretical research exploring the certified robustness of majority vote classifier and its interplay with generalization. In this study, we develop a generalization error bound that possesses a certified robust radius for the smoothed majority vote classifier (i.e., the $Q$-weighted majority vote classifier with smoothed inputs); In other words, the generalization bound holds under any data perturbation within the certified robust radius. As a byproduct, we find that the underpinnings of both the generalization bound and the certified robust radius draw, in part, upon weight spectral norm, which thereby inspires the adoption of spectral regularization in smooth training to boost certified robustness. Utilizing the dimension-independent property of spherical Gaussian inputs in smooth training, we propose a novel and inexpensive spectral regularizer to enhance the smoothed majority vote classifier. In addition to the theoretical contribution, a set of empirical results is provided to substantiate the effectiveness of our proposed method.</li>
</ul>

<h3>Title: Exact Solutions to the Quantum Schrödinger Bridge Problem</h3>
<ul>
<li><strong>Authors: </strong>Mykola Bordyuh, Djork-Arné Clevert, Marco Bertolini</a></li>
<li><strong>Subjects: </strong>cs.LG, math-ph, math.PR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25980">https://arxiv.org/abs/2509.25980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25980">https://arxiv.org/pdf/2509.25980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25980]] Exact Solutions to the Quantum Schrödinger Bridge Problem(https://arxiv.org/abs/2509.25980)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The Quantum Schrödinger Bridge Problem (QSBP) describes the evolution of a stochastic process between two arbitrary probability distributions, where the dynamics are governed by the Schrödinger equation rather than by the traditional real-valued wave equation. Although the QSBP is known in the mathematical literature, we formulate it here from a Lagrangian perspective and derive its main features in a way that is particularly suited to generative modeling. We show that the resulting evolution equations involve the so-called Bohm (quantum) potential, representing a notion of non-locality in the stochastic process. This distinguishes the QSBP from classical stochastic dynamics and reflects a key characteristic typical of quantum mechanical systems. In this work, we derive exact closed-form solutions for the QSBP between Gaussian distributions. Our derivation is based on solving the Fokker-Planck Equation (FPE) and the Hamilton-Jacobi Equation (HJE) arising from the Lagrangian formulation of dynamical Optimal Transport. We find that, similar to the classical Schrödinger Bridge Problem, the solution to the QSBP between Gaussians is again a Gaussian process; however, the evolution of the covariance differs due to quantum effects. Leveraging these explicit solutions, we present a modified algorithm based on a Gaussian Mixture Model framework, and demonstrate its effectiveness across several experimental settings, including single-cell evolution data, image generation, molecular translation and applications in Mean-Field Games.</li>
</ul>

<h3>Title: CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weiyu Huang, Yuezhou Hu, Jun Zhu, Jianfei Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25996">https://arxiv.org/abs/2509.25996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25996">https://arxiv.org/pdf/2509.25996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25996]] CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models(https://arxiv.org/abs/2509.25996)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Sparsity-aware training is an effective approach for transforming large language models (LLMs) into hardware-friendly sparse patterns, thereby reducing latency and memory consumption during inference. In this paper, we propose Continuous Adaptive Sparse Trainer (CAST), a fully continuous and differentiable sparsity-aware training framework for semi-structured (or "N:M") sparse models. Unlike previous approaches that optimize sparsity patterns and weights separately, CAST enables seamless joint optimization during training, while progressively transforming the model into the desired sparsity format. Specifically, CAST introduces three key components: 1) AdamS, a sparsity-aware optimizer that leverages adaptive L1 decay to promote uniform sparsification across all parameters; 2) Weight Scaling, a module designed to mitigate the magnitude reduction caused by decay while preserving desired sparsity patterns; 3) Knowledge Distillation, which employs the dense model as a self-teacher to enhance training efficiency. We evaluate CAST under 2:4 sparsity patterns across multiple model families, ranging from 125M to 13B parameters. Our results demonstrate significant improvements over previous state-of-the-art methods in both perplexity and zero-shot accuracy with minimal training resources. Notably, on LLaMA2-7B, our 2:4 sparse model achieves a negligible perplexity increase of 0.09 and a 0.36% gain in zero-shot accuracy compared to the dense model using only 2% of the original pretraining tokens. Additionally, we establish an accurate and robust empirical scaling law to predict sparse model performance given adequate training resources. Finally, we demonstrate the practical applicability of our sparse models by evaluating them under quantization and fine-tuning scenarios.</li>
</ul>

<h3>Title: VRWKV-Editor: Reducing quadratic complexity in transformer-based video editing</h3>
<ul>
<li><strong>Authors: </strong>Abdelilah Aitrouga, Youssef Hmamouche, Amal El Fallah Seghrouchni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25998">https://arxiv.org/abs/2509.25998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25998">https://arxiv.org/pdf/2509.25998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25998]] VRWKV-Editor: Reducing quadratic complexity in transformer-based video editing(https://arxiv.org/abs/2509.25998)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>In light of recent progress in video editing, deep learning models focusing on both spatial and temporal dependencies have emerged as the primary method. However, these models suffer from the quadratic computational complexity of traditional attention mechanisms, making them difficult to adapt to long-duration and high-resolution videos. This limitation restricts their applicability in practical contexts such as real-time video processing. To tackle this challenge, we introduce a method to reduce both time and space complexity of these systems by proposing VRWKV-Editor, a novel video editing model that integrates a linear spatio-temporal aggregation module into video-based diffusion models. VRWKV-Editor leverages bidirectional weighted key-value recurrence mechanism of the RWKV transformer to capture global dependencies while preserving temporal coherence, achieving linear complexity without sacrificing quality. Extensive experiments demonstrate that the proposed method achieves up to 3.7x speedup and 60% lower memory usage compared to state-of-the-art diffusion-based video editing methods, while maintaining competitive performance in frame consistency and text alignment. Furthermore, a comparative analysis we conducted on videos with different sequence lengths confirms that the gap in editing speed between our approach and architectures with self-attention becomes more significant with long videos.</li>
</ul>

<h3>Title: Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations</h3>
<ul>
<li><strong>Authors: </strong>Nicola Messina, Rosario Leonardi, Luca Ciampi, Fabio Carrara, Giovanni Maria Farinella, Fabrizio Falchi, Antonino Furnari</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26004">https://arxiv.org/abs/2509.26004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26004">https://arxiv.org/pdf/2509.26004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26004]] Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations(https://arxiv.org/abs/2509.26004)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Pixel-level recognition of objects manipulated by the user from egocentric images enables key applications spanning assistive technologies, industrial safety, and activity monitoring. However, progress in this area is currently hindered by the scarcity of annotated datasets, as existing approaches rely on costly manual labels. In this paper, we propose to learn human-object interaction detection leveraging narrations -- natural language descriptions of the actions performed by the camera wearer which contain clues about manipulated objects (e.g., "I am pouring vegetables from the chopping board to the pan"). Narrations provide a form of weak supervision that is cheap to acquire and readily available in state-of-the-art egocentric datasets. We introduce Narration-Supervised in-Hand Object Segmentation (NS-iHOS), a novel task where models have to learn to segment in-hand objects by learning from natural-language narrations. Narrations are then not employed at inference time. We showcase the potential of the task by proposing Weakly-Supervised In-hand Object Segmentation from Human Narrations (WISH), an end-to-end model distilling knowledge from narrations to learn plausible hand-object associations and enable in-hand object segmentation without using narrations at test time. We benchmark WISH against different baselines based on open-vocabulary object detectors and vision-language models, showing the superiority of its design. Experiments on EPIC-Kitchens and Ego4D show that WISH surpasses all baselines, recovering more than 50% of the performance of fully supervised methods, without employing fine-grained pixel-wise annotations.</li>
</ul>

<h3>Title: AgenticIQA: An Agentic Framework for Adaptive and Interpretable Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Hanwei Zhu, Yu Tian, Keyan Ding, Baoliang Chen, Bolin Chen, Shiqi Wang, Weisi Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26006">https://arxiv.org/abs/2509.26006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26006">https://arxiv.org/pdf/2509.26006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26006]] AgenticIQA: An Agentic Framework for Adaptive and Interpretable Image Quality Assessment(https://arxiv.org/abs/2509.26006)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Image quality assessment (IQA) is inherently complex, as it reflects both the quantification and interpretation of perceptual quality rooted in the human visual system. Conventional approaches typically rely on fixed models to output scalar scores, limiting their adaptability to diverse distortions, user-specific queries, and interpretability needs. Furthermore, scoring and interpretation are often treated as independent processes, despite their interdependence: interpretation identifies perceptual degradations, while scoring abstracts them into a compact metric. To address these limitations, we propose AgenticIQA, a modular agentic framework that integrates vision-language models (VLMs) with traditional IQA tools in a dynamic, query-aware manner. AgenticIQA decomposes IQA into four subtasks -- distortion detection, distortion analysis, tool selection, and tool execution -- coordinated by a planner, executor, and summarizer. The planner formulates task-specific strategies, the executor collects perceptual evidence via tool invocation, and the summarizer integrates this evidence to produce accurate scores with human-aligned explanations. To support training and evaluation, we introduce AgenticIQA-200K, a large-scale instruction dataset tailored for IQA agents, and AgenticIQA-Eval, the first benchmark for assessing the planning, execution, and summarization capabilities of VLM-based IQA agents. Extensive experiments across diverse IQA datasets demonstrate that AgenticIQA consistently surpasses strong baselines in both scoring accuracy and explanatory alignment.</li>
</ul>

<h3>Title: New Fourth-Order Grayscale Indicator-Based Telegraph Diffusion Model for Image Despeckling</h3>
<ul>
<li><strong>Authors: </strong>Rajendra K. Ray, Manish Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26010">https://arxiv.org/abs/2509.26010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26010">https://arxiv.org/pdf/2509.26010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26010]] New Fourth-Order Grayscale Indicator-Based Telegraph Diffusion Model for Image Despeckling(https://arxiv.org/abs/2509.26010)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, diffusion</a></li>
<li><strong>Abstract: </strong>Second-order PDE models have been widely used for suppressing multiplicative noise, but they often introduce blocky artifacts in the early stages of denoising. To resolve this, we propose a fourth-order nonlinear PDE model that integrates diffusion and wave properties. The diffusion process, guided by both the Laplacian and intensity values, reduces noise better than gradient-based methods, while the wave part keeps fine details and textures. The effectiveness of the proposed model is evaluated against two second-order anisotropic diffusion approaches using the Peak Signal-to-Noise Ratio (PSNR) and Mean Structural Similarity Index (MSSIM) for images with available ground truth. For SAR images, where a noise-free reference is unavailable, the Speckle Index (SI) is used to measure noise reduction. Additionally, we extend the proposed model to study color images by applying the denoising process independently to each channel, preserving both structure and color consistency. The same quantitative metrics PSNR and MSSIM are used for performance evaluation, ensuring a fair comparison across grayscale and color images. In all the cases, our computed results produce better results compared to existing models in this genre.</li>
</ul>

<h3>Title: SETR: A Two-Stage Semantic-Enhanced Framework for Zero-Shot Composed Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Yuqi Xiao, Yingying Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26012">https://arxiv.org/abs/2509.26012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26012">https://arxiv.org/pdf/2509.26012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26012]] SETR: A Two-Stage Semantic-Enhanced Framework for Zero-Shot Composed Image Retrieval(https://arxiv.org/abs/2509.26012)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Zero-shot Composed Image Retrieval (ZS-CIR) aims to retrieve a target image given a reference image and a relative text, without relying on costly triplet annotations. Existing CLIP-based methods face two core challenges: (1) union-based feature fusion indiscriminately aggregates all visual cues, carrying over irrelevant background details that dilute the intended modification, and (2) global cosine similarity from CLIP embeddings lacks the ability to resolve fine-grained semantic relations. To address these issues, we propose SETR (Semantic-enhanced Two-Stage Retrieval). In the coarse retrieval stage, SETR introduces an intersection-driven strategy that retains only the overlapping semantics between the reference image and relative text, thereby filtering out distractors inherent to union-based fusion and producing a cleaner, high-precision candidate set. In the fine-grained re-ranking stage, we adapt a pretrained multimodal LLM with Low-Rank Adaptation to conduct binary semantic relevance judgments ("Yes/No"), which goes beyond CLIP's global feature matching by explicitly verifying relational and attribute-level consistency. Together, these two stages form a complementary pipeline: coarse retrieval narrows the candidate pool with high recall, while re-ranking ensures precise alignment with nuanced textual modifications. Experiments on CIRR, Fashion-IQ, and CIRCO show that SETR achieves new state-of-the-art performance, improving Recall@1 on CIRR by up to 15.15 points. Our results establish two-stage reasoning as a general paradigm for robust and portable ZS-CIR.</li>
</ul>

<h3>Title: FITS: Towards an AI-Driven Fashion Information Tool for Sustainability</h3>
<ul>
<li><strong>Authors: </strong>Daphne Theodorakopoulos, Elisabeth Eberling, Miriam Bodenheimer, Sabine Loos, Frederic Stahl</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26017">https://arxiv.org/abs/2509.26017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26017">https://arxiv.org/pdf/2509.26017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26017]] FITS: Towards an AI-Driven Fashion Information Tool for Sustainability(https://arxiv.org/abs/2509.26017)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Access to credible sustainability information in the fashion industry remains limited and challenging to interpret, despite growing public and regulatory demands for transparency. General-purpose language models often lack domain-specific knowledge and tend to "hallucinate", which is particularly harmful for fields where factual correctness is crucial. This work explores how Natural Language Processing (NLP) techniques can be applied to classify sustainability data for fashion brands, thereby addressing the scarcity of credible and accessible information in this domain. We present a prototype Fashion Information Tool for Sustainability (FITS), a transformer-based system that extracts and classifies sustainability information from credible, unstructured text sources: NGO reports and scientific publications. Several BERT-based language models, including models pretrained on scientific and climate-specific data, are fine-tuned on our curated corpus using a domain-specific classification schema, with hyperparameters optimized via Bayesian optimization. FITS allows users to search for relevant data, analyze their own data, and explore the information via an interactive interface. We evaluated FITS in two focus groups of potential users concerning usability, visual design, content clarity, possible use cases, and desired features. Our results highlight the value of domain-adapted NLP in promoting informed decision-making and emphasize the broader potential of AI applications in addressing climate-related challenges. Finally, this work provides a valuable dataset, the SustainableTextileCorpus, along with a methodology for future updates. Code available at this https URL</li>
</ul>

<h3>Title: PatchVSR: Breaking Video Diffusion Resolution Limits with Patch-wise Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Shian Du, Menghan Xia, Chang Liu, Xintao Wang, Jing Wang, Pengfei Wan, Di Zhang, Xiangyang Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26025">https://arxiv.org/abs/2509.26025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26025">https://arxiv.org/pdf/2509.26025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26025]] PatchVSR: Breaking Video Diffusion Resolution Limits with Patch-wise Video Super-Resolution(https://arxiv.org/abs/2509.26025)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Pre-trained video generation models hold great potential for generative video super-resolution (VSR). However, adapting them for full-size VSR, as most existing methods do, suffers from unnecessary intensive full-attention computation and fixed output resolution. To overcome these limitations, we make the first exploration into utilizing video diffusion priors for patch-wise VSR. This is non-trivial because pre-trained video diffusion models are not native for patch-level detail generation. To mitigate this challenge, we propose an innovative approach, called PatchVSR, which integrates a dual-stream adapter for conditional guidance. The patch branch extracts features from input patches to maintain content fidelity while the global branch extracts context features from the resized full video to bridge the generation gap caused by incomplete semantics of patches. Particularly, we also inject the patch's location information into the model to better contextualize patch synthesis within the global video frame. Experiments demonstrate that our method can synthesize high-fidelity, high-resolution details at the patch level. A tailor-made multi-patch joint modulation is proposed to ensure visual consistency across individually enhanced patches. Due to the flexibility of our patch-based paradigm, we can achieve highly competitive 4K VSR based on a 512x512 resolution base model, with extremely high efficiency.</li>
</ul>

<h3>Title: Causally Guided Gaussian Perturbations for Out-Of-Distribution Generalization in Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Haoran Pei, Yuguang Yang, Kexin Liu, Baochang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26027">https://arxiv.org/abs/2509.26027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26027">https://arxiv.org/pdf/2509.26027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26027]] Causally Guided Gaussian Perturbations for Out-Of-Distribution Generalization in Medical Imaging(https://arxiv.org/abs/2509.26027)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) generalization remains a central challenge in deploying deep learning models to real-world scenarios, particularly in domains such as biomedical images, where distribution shifts are both subtle and pervasive. While existing methods often pursue domain invariance through complex generative models or adversarial training, these approaches may overlook the underlying causal mechanisms of this http URL this work, we propose Causally-Guided Gaussian Perturbations (CGP)-a lightweight framework that enhances OOD generalization by injecting spatially varying noise into input images, guided by soft causal masks derived from Vision Transformers. By applying stronger perturbations to background regions and weaker ones to foreground areas, CGP encourages the model to rely on causally relevant features rather than spurious this http URL results on the challenging WILDS benchmark Camelyon17 demonstrate consistent performance gains over state-of-the-art OOD baselines, highlighting the potential of causal perturbation as a tool for reliable and interpretable generalization.</li>
</ul>

<h3>Title: Muon Outperforms Adam in Tail-End Associative Memory Learning</h3>
<ul>
<li><strong>Authors: </strong>Shuche Wang, Fengzhuo Zhang, Jiaxiang Li, Cunxiao Du, Chao Du, Tianyu Pang, Zhuoran Yang, Mingyi Hong, Vincent Y. F. Tan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26030">https://arxiv.org/abs/2509.26030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26030">https://arxiv.org/pdf/2509.26030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26030]] Muon Outperforms Adam in Tail-End Associative Memory Learning(https://arxiv.org/abs/2509.26030)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The Muon optimizer is consistently faster than Adam in training Large Language Models (LLMs), yet the mechanism underlying its success remains unclear. This paper demystifies this mechanism through the lens of associative memory. By ablating the transformer components optimized by Muon, we reveal that the associative memory parameters of LLMs, namely the Value and Output (VO) attention weights and Feed-Forward Networks (FFNs), are the primary contributors to Muon's superiority. Motivated by this associative memory view, we then explain Muon's superiority on real-world corpora, which are intrinsically heavy-tailed: a few classes (tail classes) appear far less frequently than others. The superiority is explained through two key properties: (i) its update rule consistently yields a more isotropic singular spectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes tail classes more effectively than Adam. Beyond empirical evidence, we theoretically confirm these findings by analyzing a one-layer associative memory model under class-imbalanced data. We prove that Muon consistently achieves balanced learning across classes regardless of feature embeddings, whereas Adam can induce large disparities in learning errors depending on embedding properties. In summary, our empirical observations and theoretical analyses reveal Muon's core advantage: its update rule aligns with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions than Adam.</li>
</ul>

<h3>Title: Stealthy Yet Effective: Distribution-Preserving Backdoor Attacks on Graph Classification</h3>
<ul>
<li><strong>Authors: </strong>Xiaobao Wang, Ruoxiao Sun, Yujun Zhang, Bingdao Feng, Dongxiao He, Luzhi Wang, Di Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26032">https://arxiv.org/abs/2509.26032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26032">https://arxiv.org/pdf/2509.26032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26032]] Stealthy Yet Effective: Distribution-Preserving Backdoor Attacks on Graph Classification(https://arxiv.org/abs/2509.26032)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have demonstrated strong performance across tasks such as node classification, link prediction, and graph classification, but remain vulnerable to backdoor attacks that implant imperceptible triggers during training to control predictions. While node-level attacks exploit local message passing, graph-level attacks face the harder challenge of manipulating global representations while maintaining stealth. We identify two main sources of anomaly in existing graph classification backdoor methods: structural deviation from rare subgraph triggers and semantic deviation caused by label flipping, both of which make poisoned graphs easily detectable by anomaly detection models. To address this, we propose DPSBA, a clean-label backdoor framework that learns in-distribution triggers via adversarial training guided by anomaly-aware discriminators. DPSBA effectively suppresses both structural and semantic anomalies, achieving high attack success while significantly improving stealth. Extensive experiments on real-world datasets validate that DPSBA achieves a superior balance between effectiveness and detectability compared to state-of-the-art baselines.</li>
</ul>

<h3>Title: RE$^2$: Improving Chinese Grammatical Error Correction via Retrieving Appropriate Examples with Explanation</h3>
<ul>
<li><strong>Authors: </strong>Baoxin Wang, Yumeng Luo, Yixuan Wang, Dayong Wu, Wanxiang Che, Shijin Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26038">https://arxiv.org/abs/2509.26038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26038">https://arxiv.org/pdf/2509.26038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26038]] RE$^2$: Improving Chinese Grammatical Error Correction via Retrieving Appropriate Examples with Explanation(https://arxiv.org/abs/2509.26038)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The primary objective of Chinese grammatical error correction (CGEC) is to detect and correct errors in Chinese sentences. Recent research shows that large language models (LLMs) have been applied to CGEC with significant results. For LLMs, selecting appropriate reference examples can help improve their performance. However, existing methods predominantly rely on text similarity for example retrieval, a strategy that frequently mismatches actual error patterns and retrieves lexically similar yet grammatically irrelevant sentences. To address this problem, we propose a method named RE$^2$, which retrieves appropriate examples with explanations of grammatical errors. Instead of using text similarity of the input sentence, we use explanations of grammatical errors to select reference examples, which are used by LLMs to improve the performance of CGEC. We conduct experiments on two CGEC datasets and create a high-quality grammatical error explanation (GEE) dataset, which is not only used in our research but also serves as a valuable resource for future studies in both CGEC and GEE. The experimental results on the two datasets indicate that our proposed method effectively improves the performance of CGEC.</li>
</ul>

<h3>Title: SGS: Segmentation-Guided Scoring for Global Scene Inconsistencies</h3>
<ul>
<li><strong>Authors: </strong>Gagandeep Singh, Samudi Amarsinghe, Urawee Thani, Ki Fung Wong, Priyanka Singh, Xue Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26039">https://arxiv.org/abs/2509.26039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26039">https://arxiv.org/pdf/2509.26039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26039]] SGS: Segmentation-Guided Scoring for Global Scene Inconsistencies(https://arxiv.org/abs/2509.26039)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>We extend HAMMER, a state-of-the-art model for multimodal manipulation detection, to handle global scene inconsistencies such as foreground-background (FG-BG) mismatch. While HAMMER achieves strong performance on the DGM4 dataset, it consistently fails when the main subject is contextually misplaced into an implausible background. We diagnose this limitation as a combination of label-space bias, local attention focus, and spurious text-foreground alignment. To remedy this without retraining, we propose a lightweight segmentation-guided scoring (SGS) pipeline. SGS uses person/face segmentation masks to separate foreground and background regions, extracts embeddings with a joint vision-language model, and computes region-aware coherence scores. These scores are fused with HAMMER's original prediction to improve binary detection, grounding, and token-level explanations. SGS is inference-only, incurs negligible computational overhead, and significantly enhances robustness to global manipulations. This work demonstrates the importance of region-aware reasoning in multimodal disinformation detection. We release scripts for segmentation and scoring at this https URL</li>
</ul>

<h3>Title: Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Arash Marioriyad, Shaygan Adim, Nima Alighardashi, Mahdieh Soleymani Banghshah, Mohammad Hossein Rohban</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26041">https://arxiv.org/abs/2509.26041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26041">https://arxiv.org/pdf/2509.26041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26041]] Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning(https://arxiv.org/abs/2509.26041)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) increasingly rely on chain-of-thought (CoT) prompting to solve mathematical and logical reasoning tasks. Yet, a central question remains: to what extent are these generated rationales \emph{faithful} to the underlying computations, rather than post-hoc narratives shaped by hints that function as answer shortcuts embedded in the prompt? Following prior work on hinted vs.\ unhinted prompting, we present a systematic study of CoT faithfulness under controlled hint manipulations. Our experimental design spans four datasets (AIME, GSM-Hard, MATH-500, UniADILR), two state-of-the-art models (GPT-4o and Gemini-2-Flash), and a structured set of hint conditions varying in correctness (correct and incorrect), presentation style (sycophancy and data leak), and complexity (raw answers, two-operator expressions, four-operator expressions). We evaluate both task accuracy and whether hints are explicitly acknowledged in the reasoning. Our results reveal three key findings. First, correct hints substantially improve accuracy, especially on harder benchmarks and logical reasoning, while incorrect hints sharply reduce accuracy in tasks with lower baseline competence. Second, acknowledgement of hints is highly uneven: equation-based hints are frequently referenced, whereas raw hints are often adopted silently, indicating that more complex hints push models toward verbalizing their reliance in the reasoning process. Third, presentation style matters: sycophancy prompts encourage overt acknowledgement, while leak-style prompts increase accuracy but promote hidden reliance. This may reflect RLHF-related effects, as sycophancy exploits the human-pleasing side and data leak triggers the self-censoring side. Together, these results demonstrate that LLM reasoning is systematically shaped by shortcuts in ways that obscure faithfulness.</li>
</ul>

<h3>Title: DGM4+: Dataset Extension for Global Scene Inconsistency</h3>
<ul>
<li><strong>Authors: </strong>Gagandeep Singh, Samudi Amarsinghe, Priyanka Singh, Xue Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26047">https://arxiv.org/abs/2509.26047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26047">https://arxiv.org/pdf/2509.26047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26047]] DGM4+: Dataset Extension for Global Scene Inconsistency(https://arxiv.org/abs/2509.26047)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advances in generative models have significantly lowered the barrier to producing convincing multimodal disinformation. Fabricated images and manipulated captions increasingly co-occur to create persuasive false narratives. While the Detecting and Grounding Multi-Modal Media Manipulation (DGM4) dataset established a foundation for research in this area, it is restricted to local manipulations such as face swaps, attribute edits, and caption changes. This leaves a critical gap: global inconsistencies, such as mismatched foregrounds and backgrounds, which are now prevalent in real-world forgeries. To address this, we extend DGM4 with 5,000 high-quality samples that introduce Foreground-Background (FG-BG) mismatches and their hybrids with text manipulations. Using OpenAI's gpt-image-1 and carefully designed prompts, we generate human-centric news-style images where authentic figures are placed into absurd or impossible backdrops (e.g., a teacher calmly addressing students on the surface of Mars). Captions are produced under three conditions: literal, text attribute, and text split, yielding three new manipulation categories: FG-BG, FG-BG+TA, and FG-BG+TS. Quality control pipelines enforce one-to-three visible faces, perceptual hash deduplication, OCR-based text scrubbing, and realistic headline length. By introducing global manipulations, our extension complements existing datasets, creating a benchmark DGM4+ that tests detectors on both local and global reasoning. This resource is intended to strengthen evaluation of multimodal models such as HAMMER, which currently struggle with FG-BG inconsistencies. We release our DGM4+ dataset and generation script at this https URL</li>
</ul>

<h3>Title: RE-Searcher: Robust Agentic Search with Goal-oriented Planning and Self-reflection</h3>
<ul>
<li><strong>Authors: </strong>Daocheng Fu, Jianbiao Mei, Licheng Wen, Xuemeng Yang, Cheng Yang, Rong Wu, Tao Hu, Siqi Li, Yufan Shen, Xinyu Cai, Pinlong Cai, Botian Shi, Yong Liu, Yu Qiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26048">https://arxiv.org/abs/2509.26048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26048">https://arxiv.org/pdf/2509.26048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26048]] RE-Searcher: Robust Agentic Search with Goal-oriented Planning and Self-reflection(https://arxiv.org/abs/2509.26048)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at knowledge-intensive question answering and reasoning, yet their real-world deployment remains constrained by knowledge cutoff, hallucination, and limited interaction modalities. Augmenting LLMs with external search tools helps alleviate these issues, but it also exposes agents to a complex search environment in which small, plausible variations in query formulation can steer reasoning into unproductive trajectories and amplify errors. We present a systematic analysis that quantifies how environmental complexity induces fragile search behaviors and, in turn, degrades overall performance. To address this challenge, we propose a simple yet effective approach to instantiate a search agent, RE-Searcher. During search, RE-Searcher explicitly articulates a concrete search goal and subsequently reflects on whether the retrieved evidence satisfies that goal. This combination of goal-oriented planning and self-reflection enables RE-Searcher to resist spurious cues in complex search environments and perform robust search. Extensive experiments show that our method improves search accuracy and achieves state-of-the-art results. Perturbation studies further demonstrate substantial resilience to noisy or misleading external signals, mitigating the fragility of the search process. We believe these findings offer practical guidance for integrating LLM-powered agents into more complex interactive environments and enabling more autonomous decision-making.</li>
</ul>

<h3>Title: CEAID: Benchmark of Multilingual Machine-Generated Text Detection Methods for Central European Languages</h3>
<ul>
<li><strong>Authors: </strong>Dominik Macko, Jakub Kopal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26051">https://arxiv.org/abs/2509.26051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26051">https://arxiv.org/pdf/2509.26051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26051]] CEAID: Benchmark of Multilingual Machine-Generated Text Detection Methods for Central European Languages(https://arxiv.org/abs/2509.26051)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Machine-generated text detection, as an important task, is predominantly focused on English in research. This makes the existing detectors almost unusable for non-English languages, relying purely on cross-lingual transferability. There exist only a few works focused on any of Central European languages, leaving the transferability towards these languages rather unexplored. We fill this gap by providing the first benchmark of detection methods focused on this region, while also providing comparison of train-languages combinations to identify the best performing ones. We focus on multi-domain, multi-generator, and multilingual evaluation, pinpointing the differences of individual aspects, as well as adversarial robustness of detection methods. Supervised finetuned detectors in the Central European languages are found the most performant in these languages as well as the most resistant against obfuscation.</li>
</ul>

<h3>Title: Real-time Noise Detection and Classification in Single-Channel EEG: A Lightweight Machine Learning Approach for EMG, White Noise, and EOG Artifacts</h3>
<ul>
<li><strong>Authors: </strong>Hossein Enshaei, Pariya Jebreili, Sayed Mahmoud Sakahei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26058">https://arxiv.org/abs/2509.26058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26058">https://arxiv.org/pdf/2509.26058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26058]] Real-time Noise Detection and Classification in Single-Channel EEG: A Lightweight Machine Learning Approach for EMG, White Noise, and EOG Artifacts(https://arxiv.org/abs/2509.26058)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Electroencephalogram (EEG) artifact detection in real-world settings faces significant challenges such as computational inefficiency in multi-channel methods, poor robustness to simultaneous noise, and trade-offs between accuracy and complexity in deep learning models. We propose a hybrid spectral-temporal framework for real-time detection and classification of ocular (EOG), muscular (EMG), and white noise artifacts in single-channel EEG. This method, in contrast to other approaches, combines time-domain low-pass filtering (targeting low-frequency EOG) and frequency-domain power spectral density (PSD) analysis (capturing broad-spectrum EMG), followed by PCA-optimized feature fusion to minimize redundancy while preserving discriminative information. This feature engineering strategy allows a lightweight multi-layer perceptron (MLP) architecture to outperform advanced CNNs and RNNs by achieving 99% accuracy at low SNRs (SNR -7) dB and >90% accuracy in moderate noise (SNR 4 dB). Additionally, this framework addresses the unexplored problem of simultaneous multi-source contamination(EMG+EOG+white noise), where it maintains 96% classification accuracy despite overlapping artifacts. With 30-second training times (97% faster than CNNs) and robust performance across SNR levels, this framework bridges the gap between clinical applicability and computational efficiency, which enables real-time use in wearable brain-computer interfaces. This work also challenges the ubiquitous dependence on model depth for EEG artifact detection by demonstrating that domain-informed feature fusion surpasses complex architecture in noisy scenarios.</li>
</ul>

<h3>Title: DyFlow: Dynamic Workflow Framework for Agentic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yanbo Wang, Zixiang Xu, Yue Huang, Xiangqi Wang, Zirui Song, Lang Gao, Chenxi Wang, Xiangru Tang, Yue Zhao, Arman Cohan, Xiangliang Zhang, Xiuying Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26062">https://arxiv.org/abs/2509.26062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26062">https://arxiv.org/pdf/2509.26062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26062]] DyFlow: Dynamic Workflow Framework for Agentic Reasoning(https://arxiv.org/abs/2509.26062)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Agent systems based on large language models (LLMs) have shown great potential in complex reasoning tasks, but building efficient and generalizable workflows remains a major challenge. Most existing approaches rely on manually designed processes, which limits their adaptability across different tasks. While a few methods attempt automated workflow generation, they are often tied to specific datasets or query types and make limited use of intermediate feedback, reducing system robustness and reasoning depth. Moreover, their operations are typically predefined and inflexible. To address these limitations, we propose DyFlow, a dynamic workflow generation framework that adaptively constructs and adjusts reasoning procedures based on task requirements and real-time intermediate feedback, thereby enhancing cross-task generalization. DyFlow consists of two core components: a designer and an executor. The designer decomposes complex problems into a sequence of sub-goals defined by high-level objectives and dynamically plans the next steps based on intermediate outputs and feedback. These plans are then carried out by the executor, which executes each operation using dynamic operators with context-aware parameterization, enabling flexible and semantically grounded reasoning. We systematically evaluate DyFlow across diverse domains, including social reasoning, biomedical tasks, mathematical problem solving, and code generation. Results demonstrate that DyFlow significantly outperforms existing baselines, achieving substantial Pass@k improvements and exhibiting robust generalization across diverse domains. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge</h3>
<ul>
<li><strong>Authors: </strong>Arash Marioriyad, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26072">https://arxiv.org/abs/2509.26072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26072">https://arxiv.org/pdf/2509.26072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26072]] The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge(https://arxiv.org/abs/2509.26072)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed as automatic judges to evaluate system outputs in tasks such as summarization, dialogue, and creative writing. A faithful judge should base its verdicts solely on response quality and explicitly acknowledge the factors shaping its decision. We show that current LLM judges fail on both counts by relying on shortcuts introduced in the prompt. Our study uses two evaluation datasets: ELI5, a benchmark for long-form question answering, and LitBench, a recent benchmark for creative writing. Both datasets provide pairwise comparisons, where the evaluator must choose which of two responses is better. From each dataset we construct 100 pairwise judgment tasks and employ two widely used models, GPT-4o and Gemini-2.5-Flash, as evaluators in the role of LLM-as-a-judge. For each pair, we assign superficial cues to the responses, provenance cues indicating source identity (Human, Expert, LLM, or Unknown) and recency cues indicating temporal origin (Old, 1950 vs. New, 2025), while keeping the rest of the prompt fixed. Results reveal consistent verdict shifts: both models exhibit a strong recency bias, systematically favoring new responses over old, as well as a clear provenance hierarchy (Expert > Human > LLM > Unknown). These biases are especially pronounced in GPT-4o and in the more subjective and open-ended LitBench domain. Crucially, cue acknowledgment is rare: justifications almost never reference the injected cues, instead rationalizing decisions in terms of content qualities. These findings demonstrate that current LLM-as-a-judge systems are shortcut-prone and unfaithful, undermining their reliability as evaluators in both research and deployment.</li>
</ul>

<h3>Title: Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Leitian Tao, Xuefeng Du, Yixuan Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26074">https://arxiv.org/abs/2509.26074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26074">https://arxiv.org/pdf/2509.26074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26074]] Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis(https://arxiv.org/abs/2509.26074)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reward modeling, crucial for aligning large language models (LLMs) with human preferences, is often bottlenecked by the high cost of preference data. Existing textual data synthesis methods are computationally expensive. We propose a novel framework LENS for synthesizing preference data directly in the LLM's latent embedding space. Our method employs a Variational Autoencoder (VAE) to learn a structured latent representation of response embeddings. By performing controlled perturbations in this latent space and decoding back to the embedding space, we efficiently generate diverse, semantically consistent synthetic preference pairs, bypassing costly text generation and annotation. We provide theoretical guarantees that our synthesized pairs approximately preserve original preference ordering and improve reward model generalization. Empirically, our latent-space synthesis significantly outperforms text-based augmentation on standard benchmarks, achieving superior results while being 18x faster in generation and using a 16,000x smaller model. Our work offers a scalable and effective alternative for enhancing reward modeling through efficient data augmentation. Code is publicly available at this https URL</li>
</ul>

<h3>Title: IMProofBench: Benchmarking AI on Research-Level Mathematical Proof Generation</h3>
<ul>
<li><strong>Authors: </strong>Johannes Schmitt, Gergely Bérczi, Jasper Dekoninck, Jeremy Feusi, Tim Gehrunger, Raphael Appenzeller, Jim Bryan, Niklas Canova, Timo de Wolff, Filippo Gaia, Michel van Garrel, Baran Hashemi, David Holmes, Aitor Iribar Lopez, Victor Jaeck, Martina Jørgensen, Steven Kelk, Stefan Kuhlmann, Adam Kurpisz, Chiara Meroni, Ingmar Metzler, Martin Möller, Samuel Muñoz-Echániz, Robert Nowak, Georg Oberdieck, Daniel Platt, Dylan Possamaï, Gabriel Ribeiro, Raúl Sánchez Galán, Zheming Sun, Josef Teichmann, Richard P. Thomas, Charles Vial</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26076">https://arxiv.org/abs/2509.26076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26076">https://arxiv.org/pdf/2509.26076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26076]] IMProofBench: Benchmarking AI on Research-Level Mathematical Proof Generation(https://arxiv.org/abs/2509.26076)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As the mathematical capabilities of large language models (LLMs) improve, it becomes increasingly important to evaluate their performance on research-level tasks at the frontier of mathematical knowledge. However, existing benchmarks are limited, as they focus solely on final-answer questions or high-school competition problems. To address this gap, we introduce IMProofBench, a private benchmark consisting of 39 peer-reviewed problems developed by expert mathematicians. Each problem requires a detailed proof and is paired with subproblems that have final answers, supporting both an evaluation of mathematical reasoning capabilities by human experts and a large-scale quantitative analysis through automated grading. Furthermore, unlike prior benchmarks, the evaluation setup simulates a realistic research environment: models operate in an agentic framework with tools like web search for literature review and mathematical software such as SageMath. Our results show that current LLMs can succeed at the more accessible research-level questions, but still encounter significant difficulties on more challenging problems. Quantitatively, Grok-4 achieves the highest accuracy of 52% on final-answer subproblems, while GPT-5 obtains the best performance for proof generation, achieving a fully correct solution for 22% of problems. IMProofBench will continue to evolve as a dynamic benchmark in collaboration with the mathematical community, ensuring its relevance for evaluating the next generation of LLMs.</li>
</ul>

<h3>Title: Reinforced Strategy Optimization for Conversational Recommender Systems via Network-of-Experts</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26093">https://arxiv.org/abs/2509.26093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26093">https://arxiv.org/pdf/2509.26093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26093]] Reinforced Strategy Optimization for Conversational Recommender Systems via Network-of-Experts(https://arxiv.org/abs/2509.26093)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Conversational Recommender Systems (CRSs) provide personalized recommendations through multi-turn interactions. With the strong reasoning abilities of Large Language Models (LLMs), applying them to CRSs has become promising. Yet, existing methods often lack explicit optimization of interaction strategies, relying instead on unified prompts, which can yield suboptimal outcomes. We propose Reinforced Strategy Optimization (RSO), a hierarchical framework that decomposes response generation into macro-level strategy planning and micro-level adaptation within a network-of-experts. A Planner selects strategies (e.g., recommend, explain, encourage), while an Actor generates responses guided by auxiliary experts for preferences and factual grounding. This disentanglement enables more tractable learning. To address limited multi-turn data, we model strategy learning as reinforcement learning with an LLM-based reward for exploration. Experiments show RSO outperforms state-of-the-art baselines, validating the effectiveness of hierarchical strategy optimization.</li>
</ul>

<h3>Title: EVODiff: Entropy-aware Variance Optimized Diffusion Inference</h3>
<ul>
<li><strong>Authors: </strong>Shigui Li, Wei Chen, Delu Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IT, cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26096">https://arxiv.org/abs/2509.26096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26096">https://arxiv.org/pdf/2509.26096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26096]] EVODiff: Entropy-aware Variance Optimized Diffusion Inference(https://arxiv.org/abs/2509.26096)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) excel in image generation, but suffer from slow inference and the training-inference discrepancies. Although gradient-based solvers like DPM-Solver accelerate the denoising inference, they lack theoretical foundations in information transmission efficiency. In this work, we introduce an information-theoretic perspective on the inference processes of DMs, revealing that successful denoising fundamentally reduces conditional entropy in reverse transitions. This principle leads to our key insights into the inference processes: (1) data prediction parameterization outperforms its noise counterpart, and (2) optimizing conditional variance offers a reference-free way to minimize both transition and reconstruction errors. Based on these insights, we propose an entropy-aware variance optimized method for the generative process of DMs, called EVODiff, which systematically reduces uncertainty by optimizing conditional entropy during denoising. Extensive experiments on DMs validate our insights and demonstrate that our method significantly and consistently outperforms state-of-the-art (SOTA) gradient-based solvers. For example, compared to the DPM-Solver++, EVODiff reduces the reconstruction error by up to 45.5\% (FID improves from 5.10 to 2.78) at 10 function evaluations (NFE) on CIFAR-10, cuts the NFE cost by 25\% (from 20 to 15 NFE) for high-quality samples on ImageNet-256, and improves text-to-image generation while reducing artifacts. Code is available at this https URL.</li>
</ul>

<h3>Title: End-to-End Aspect-Guided Review Summarization at Scale</h3>
<ul>
<li><strong>Authors: </strong>Ilya Boytsov, Vinny DeGenova, Mikhail Balyasin, Joseph Walt, Caitlin Eusden, Marie-Claire Rochat, Margaret Pierson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26103">https://arxiv.org/abs/2509.26103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26103">https://arxiv.org/pdf/2509.26103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26103]] End-to-End Aspect-Guided Review Summarization at Scale(https://arxiv.org/abs/2509.26103)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>We present a scalable large language model (LLM)-based system that combines aspect-based sentiment analysis (ABSA) with guided summarization to generate concise and interpretable product review summaries for the Wayfair platform. Our approach first extracts and consolidates aspect-sentiment pairs from individual reviews, selects the most frequent aspects for each product, and samples representative reviews accordingly. These are used to construct structured prompts that guide the LLM to produce summaries grounded in actual customer feedback. We demonstrate the real-world effectiveness of our system through a large-scale online A/B test. Furthermore, we describe our real-time deployment strategy and release a dataset of 11.8 million anonymized customer reviews covering 92,000 products, including extracted aspects and generated summaries, to support future research in aspect-guided review summarization.</li>
</ul>

<h3>Title: Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jaesung R. Park, Junsu Kim, Gyeongman Kim, Jinyoung Jo, Sean Choi, Jaewoong Cho, Ernest K. Ryu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26114">https://arxiv.org/abs/2509.26114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26114">https://arxiv.org/pdf/2509.26114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26114]] Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models(https://arxiv.org/abs/2509.26114)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable rewards (RLVR) has recently emerged as the leading approach for enhancing the reasoning capabilities of large language models (LLMs). However, RLVR is prone to entropy collapse, where the LLM quickly converges to a near-deterministic form, hindering exploration and progress during prolonged RL training. In this work, we reveal that the clipping mechanism in PPO and GRPO induces biases on entropy. Through theoretical and empirical analyses, we show that clip-low increases entropy, while clip-high decreases it. Further, under standard clipping parameters, the effect of clip-high dominates, resulting in an overall entropy reduction even when purely random rewards are provided to the RL algorithm. Our findings highlight an overlooked confounding factor in RLVR: independent of the reward signal, the clipping mechanism influences entropy, which in turn affects the reasoning behavior. Furthermore, our analysis demonstrates that clipping can be deliberately used to control entropy. Specifically, with a more aggressive clip-low value, one can increase entropy, promote exploration, and ultimately prevent entropy collapse in RLVR training.</li>
</ul>

<h3>Title: UncertainGen: Uncertainty-Aware Representations of DNA Sequences for Metagenomic Binning</h3>
<ul>
<li><strong>Authors: </strong>Abdulkadir Celikkanat, Andres R. Masegosa, Mads Albertsen, Thomas D. Nielsen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26116">https://arxiv.org/abs/2509.26116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26116">https://arxiv.org/pdf/2509.26116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26116]] UncertainGen: Uncertainty-Aware Representations of DNA Sequences for Metagenomic Binning(https://arxiv.org/abs/2509.26116)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Metagenomic binning aims to cluster DNA fragments from mixed microbial samples into their respective genomes, a critical step for downstream analyses of microbial communities. Existing methods rely on deterministic representations, such as k-mer profiles or embeddings from large language models, which fail to capture the uncertainty inherent in DNA sequences arising from inter-species DNA sharing and from fragments with highly similar representations. We present the first probabilistic embedding approach, UncertainGen, for metagenomic binning, representing each DNA fragment as a probability distribution in latent space. Our approach naturally models sequence-level uncertainty, and we provide theoretical guarantees on embedding distinguishability. This probabilistic embedding framework expands the feasible latent space by introducing a data-adaptive metric, which in turn enables more flexible separation of bins/clusters. Experiments on real metagenomic datasets demonstrate the improvements over deterministic k-mer and LLM-based embeddings for the binning task by offering a scalable and lightweight solution for large-scale metagenomic analysis.</li>
</ul>

<h3>Title: EchoGen: Generating Visual Echoes in Any Scene via Feed-Forward Subject-Driven Auto-Regressive Model</h3>
<ul>
<li><strong>Authors: </strong>Ruixiao Dong, Zhendong Wang, Keli Liu, Li Li, Ying Chen, Kai Li, Daowen Li, Houqiang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26127">https://arxiv.org/abs/2509.26127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26127">https://arxiv.org/pdf/2509.26127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26127]] EchoGen: Generating Visual Echoes in Any Scene via Feed-Forward Subject-Driven Auto-Regressive Model(https://arxiv.org/abs/2509.26127)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Subject-driven generation is a critical task in creative AI; yet current state-of-the-art methods present a stark trade-off. They either rely on computationally expensive, per-subject fine-tuning, sacrificing efficiency and zero-shot capability, or employ feed-forward architectures built on diffusion models, which are inherently plagued by slow inference speeds. Visual Auto-Regressive (VAR) models are renowned for their rapid sampling speeds and strong generative quality, making them an ideal yet underexplored foundation for resolving this tension. To bridge this gap, we introduce EchoGen, a pioneering framework that empowers VAR models with subject-driven generation capabilities. The core design of EchoGen is an effective dual-path injection strategy that disentangles a subject's high-level semantic identity from its low-level fine-grained details, enabling enhanced controllability and fidelity. We employ a semantic encoder to extract the subject's abstract identity, which is injected through decoupled cross-attention to guide the overall composition. Concurrently, a content encoder captures intricate visual details, which are integrated via a multi-modal attention mechanism to ensure high-fidelity texture and structural preservation. To the best of our knowledge, EchoGen is the first feed-forward subject-driven framework built upon VAR models. Both quantitative and qualitative results substantiate our design, demonstrating that EchoGen achieves subject fidelity and image quality comparable to state-of-the-art diffusion-based methods with significantly lower sampling latency. Code and models will be released soon.</li>
</ul>

<h3>Title: Domain-Aware Hyperdimensional Computing for Edge Smart Manufacturing</h3>
<ul>
<li><strong>Authors: </strong>Fardin Jalil Piran, Anandkumar Patel, Rajiv Malhotra, Farhad Imani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26131">https://arxiv.org/abs/2509.26131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26131">https://arxiv.org/pdf/2509.26131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26131]] Domain-Aware Hyperdimensional Computing for Edge Smart Manufacturing(https://arxiv.org/abs/2509.26131)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Smart manufacturing requires on-device intelligence that meets strict latency and energy budgets. HyperDimensional Computing (HDC) offers a lightweight alternative by encoding data as high-dimensional hypervectors and computing with simple operations. Prior studies often assume that the qualitative relation between HDC hyperparameters and performance is stable across applications. Our analysis of two representative tasks, signal-based quality monitoring in Computer Numerical Control (CNC) machining and image-based defect detection in Laser Powder Bed Fusion (LPBF), shows that this assumption does not hold. We map how encoder type, projection variance, hypervector dimensionality, and data regime shape accuracy, inference latency, training time, and training energy. A formal complexity model explains predictable trends in encoding and similarity computation and reveals nonmonotonic interactions with retraining that preclude a closed-form optimum. Empirically, signals favor nonlinear Random Fourier Features with more exclusive encodings and saturate in accuracy beyond moderate dimensionality. Images favor linear Random Projection, achieve high accuracy with small dimensionality, and depend more on sample count than on dimensionality. Guided by these insights, we tune HDC under multiobjective constraints that reflect edge deployment and obtain models that match or exceed the accuracy of state-of-the-art deep learning and Transformer models while delivering at least 6x faster inference and more than 40x lower training energy. These results demonstrate that domain-aware HDC encoding is necessary and that tuned HDC offers a practical, scalable path to real-time industrial AI on constrained hardware. Future work will enable adaptive encoder and hyperparameter selection, expand evaluation to additional manufacturing modalities, and validate on low-power accelerators.</li>
</ul>

<h3>Title: CliniBench: A Clinical Outcome Prediction Benchmark for Generative and Encoder-Based Language Models</h3>
<ul>
<li><strong>Authors: </strong>Paul Grundmann, Dennis Fast, Jan Frick, Thomas Steffek, Felix Gers, Wolfgang Nejdl, Alexander Löser</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26136">https://arxiv.org/abs/2509.26136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26136">https://arxiv.org/pdf/2509.26136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26136]] CliniBench: A Clinical Outcome Prediction Benchmark for Generative and Encoder-Based Language Models(https://arxiv.org/abs/2509.26136)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>With their growing capabilities, generative large language models (LLMs) are being increasingly investigated for complex medical tasks. However, their effectiveness in real-world clinical applications remains underexplored. To address this, we present CliniBench, the first benchmark that enables comparability of well-studied encoder-based classifiers and generative LLMs for discharge diagnosis prediction from admission notes in MIMIC-IV dataset. Our extensive study compares 12 generative LLMs and 3 encoder-based classifiers and demonstrates that encoder-based classifiers consistently outperform generative models in diagnosis prediction. We assess several retrieval augmentation strategies for in-context learning from similar patients and find that they provide notable performance improvements for generative LLMs.</li>
</ul>

<h3>Title: Accelerating Transformers in Online RL</h3>
<ul>
<li><strong>Authors: </strong>Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26137">https://arxiv.org/abs/2509.26137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26137">https://arxiv.org/pdf/2509.26137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26137]] Accelerating Transformers in Online RL(https://arxiv.org/abs/2509.26137)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The appearance of transformer-based models in Reinforcement Learning (RL) has expanded the horizons of possibilities in robotics tasks, but it has simultaneously brought a wide range of challenges during its implementation, especially in model-free online RL. Some of the existing learning algorithms cannot be easily implemented with transformer-based models due to the instability of the latter. In this paper, we propose a method that uses the Accelerator policy as a transformer's trainer. The Accelerator, a simpler and more stable model, interacts with the environment independently while simultaneously training the transformer through behavior cloning during the first stage of the proposed algorithm. In the second stage, the pretrained transformer starts to interact with the environment in a fully online setting. As a result, this model-free algorithm accelerates the transformer in terms of its performance and helps it to train online in a more stable and faster way. By conducting experiments on both state-based and image-based ManiSkill environments, as well as on MuJoCo tasks in MDP and POMDP settings, we show that applying our algorithm not only enables stable training of transformers but also reduces training time on image-based environments by up to a factor of two. Moreover, it decreases the required replay buffer size in off-policy methods to 10-20 thousand, which significantly lowers the overall computational demands.</li>
</ul>

<h3>Title: EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Sachith Abeywickrama, Emadeldeen Eldele, Min Wu, Xiaoli Li, Chau Yuen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26157">https://arxiv.org/abs/2509.26157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26157">https://arxiv.org/pdf/2509.26157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26157]] EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting(https://arxiv.org/abs/2509.26157)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Transformer-based models have significantly advanced time series forecasting, with patch-based input strategies offering efficiency and improved long-horizon modeling. Yet, existing approaches rely on temporally-agnostic patch construction, where arbitrary starting positions and fixed lengths fracture temporal coherence by splitting natural transitions across boundaries. This naive segmentation often disrupts short-term dependencies and weakens representation learning. In response, we propose EntroPE (Entropy-Guided Dynamic Patch Encoder), a novel, temporally informed framework that dynamically detects transition points via conditional entropy and dynamically places patch boundaries. This preserves temporal structure while retaining the computational benefits of patching. EntroPE consists of two key modules, namely an Entropy-based Dynamic Patcher (EDP) that applies information-theoretic criteria to locate natural temporal shifts and determine patch boundaries, and an Adaptive Patch Encoder (APE) that employs pooling and cross-attention to capture intra-patch dependencies and produce fixed-size latent representations. These embeddings are then processed by a global transformer to model inter-patch dynamics. Experiments across long-term forecasting benchmarks demonstrate that EntroPE improves both accuracy and efficiency, establishing entropy-guided dynamic patching as a promising new paradigm for time series modeling. Code is available at: this https URL.</li>
</ul>

<h3>Title: Towards Continual Expansion of Data Coverage: Automatic Text-guided Edge-case Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Kyeongryeol Go</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26158">https://arxiv.org/abs/2509.26158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26158">https://arxiv.org/pdf/2509.26158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26158]] Towards Continual Expansion of Data Coverage: Automatic Text-guided Edge-case Synthesis(https://arxiv.org/abs/2509.26158)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The performance of deep neural networks is strongly influenced by the quality of their training data. However, mitigating dataset bias by manually curating challenging edge cases remains a major bottleneck. To address this, we propose an automated pipeline for text-guided edge-case synthesis. Our approach employs a Large Language Model, fine-tuned via preference learning, to rephrase image captions into diverse textual prompts that steer a Text-to-Image model toward generating difficult visual scenarios. Evaluated on the FishEye8K object detection benchmark, our method achieves superior robustness, surpassing both naive augmentation and manually engineered prompts. This work establishes a scalable framework that shifts data curation from manual effort to automated, targeted synthesis, offering a promising direction for developing more reliable and continuously improving AI systems. Code is available at this https URL.</li>
</ul>

<h3>Title: Human-MME: A Holistic Evaluation Benchmark for Human-Centric Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuansen Liu, Haiming Tang, Jinlong Peng, Jiangning Zhang, Xiaozhong Ji, Qingdong He, Donghao Luo, Zhenye Gan, Junwei Zhu, Yunhang Shen, Chaoyou Fu, Chengjie Wang, Xiaobin Hu, Shuicheng Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26165">https://arxiv.org/abs/2509.26165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26165">https://arxiv.org/pdf/2509.26165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26165]] Human-MME: A Holistic Evaluation Benchmark for Human-Centric Multimodal Large Language Models(https://arxiv.org/abs/2509.26165)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have demonstrated significant advances in visual understanding tasks. However, their capacity to comprehend human-centric scenes has rarely been explored, primarily due to the absence of comprehensive evaluation benchmarks that take into account both the human-oriented granular level and higher-dimensional causal reasoning ability. Such high-quality evaluation benchmarks face tough obstacles, given the physical complexity of the human body and the difficulty of annotating granular structures. In this paper, we propose Human-MME, a curated benchmark designed to provide a more holistic evaluation of MLLMs in human-centric scene understanding. Compared with other existing benchmarks, our work provides three key features: 1. Diversity in human scene, spanning 4 primary visual domains with 15 secondary domains and 43 sub-fields to ensure broad scenario coverage. 2. Progressive and diverse evaluation dimensions, evaluating the human-based activities progressively from the human-oriented granular perception to the higher-dimensional reasoning, consisting of eight dimensions with 19,945 real-world image question pairs and an evaluation suite. 3. High-quality annotations with rich data paradigms, constructing the automated annotation pipeline and human-annotation platform, supporting rigorous manual labeling to facilitate precise and reliable model assessment. Our benchmark extends the single-target understanding to the multi-person and multi-image mutual understanding by constructing the choice, short-answer, grounding, ranking and judgment question components, and complex questions of their combination. The extensive experiments on 17 state-of-the-art MLLMs effectively expose the limitations and guide future MLLMs research toward better human-centric image understanding. All data and code are available at this https URL.</li>
</ul>

<h3>Title: Beyond Overall Accuracy: Pose- and Occlusion-driven Fairness Analysis in Pedestrian Detection for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Khoshkdahan, Arman Akbari, Arash Akbari, Xuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26166">https://arxiv.org/abs/2509.26166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26166">https://arxiv.org/pdf/2509.26166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26166]] Beyond Overall Accuracy: Pose- and Occlusion-driven Fairness Analysis in Pedestrian Detection for Autonomous Driving(https://arxiv.org/abs/2509.26166)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Pedestrian detection plays a critical role in autonomous driving (AD), where ensuring safety and reliability is important. While many detection models aim to reduce miss-rates and handle challenges such as occlusion and long-range recognition, fairness remains an underexplored yet equally important concern. In this work, we systematically investigate how variations in the pedestrian pose--including leg status, elbow status, and body orientation--as well as individual joint occlusions, affect detection performance. We evaluate five pedestrian-specific detectors (F2DNet, MGAN, ALFNet, CSP, and Cascade R-CNN) alongside three general-purpose models (YOLOv12 variants) on the EuroCity Persons Dense Pose (ECP-DP) dataset. Fairness is quantified using the Equal Opportunity Difference (EOD) metric across various confidence thresholds. To assess statistical significance and robustness, we apply the Z-test. Our findings highlight biases against pedestrians with parallel legs, straight elbows, and lateral views. Occlusion of lower body joints has a more negative impact on the detection rate compared to the upper body and head. Cascade R-CNN achieves the lowest overall miss-rate and exhibits the smallest bias across all attributes. To the best of our knowledge, this is the first comprehensive pose- and occlusion-aware fairness evaluation in pedestrian detection for AD.</li>
</ul>

<h3>Title: Alignment-Aware Decoding</h3>
<ul>
<li><strong>Authors: </strong>Frédéric Berdoz, Luca A. Lanzendörfer, René Caky, Roger Wattenhofer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26169">https://arxiv.org/abs/2509.26169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26169">https://arxiv.org/pdf/2509.26169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26169]] Alignment-Aware Decoding(https://arxiv.org/abs/2509.26169)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Alignment of large language models remains a central challenge in natural language processing. Preference optimization has emerged as a popular and effective method for improving alignment, typically through training-time or prompt-based interventions. In this paper, we introduce alignment-aware decoding (AAD), a method to enhance model alignment directly at inference. Theoretically, AAD can be interpreted as implicit reward optimization, yet it requires no specialized training beyond the standard DPO setup. Empirically, AAD consistently outperforms strong baselines across diverse alignment benchmarks and model scales. Moreover, in data-constrained settings, AAD can produce high-quality synthetic data to improve alignment under standard decoding, providing a practical solution when labeled data is limited.</li>
</ul>

<h3>Title: Neighbor-aware informal settlement mapping with graph convolutional networks</h3>
<ul>
<li><strong>Authors: </strong>Thomas Hallopeau, Joris Guérin, Laurent Demagistri, Christovam Barcellos, Nadine Dessay</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26171">https://arxiv.org/abs/2509.26171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26171">https://arxiv.org/pdf/2509.26171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26171]] Neighbor-aware informal settlement mapping with graph convolutional networks(https://arxiv.org/abs/2509.26171)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Mapping informal settlements is crucial for addressing challenges related to urban planning, public health, and infrastructure in rapidly growing cities. Geospatial machine learning has emerged as a key tool for detecting and mapping these areas from remote sensing data. However, existing approaches often treat spatial units independently, neglecting the relational structure of the urban fabric. We propose a graph-based framework that explicitly incorporates local geographical context into the classification process. Each spatial unit (cell) is embedded in a graph structure along with its adjacent neighbors, and a lightweight Graph Convolutional Network (GCN) is trained to classify whether the central cell belongs to an informal settlement. Experiments are conducted on a case study in Rio de Janeiro using spatial cross-validation across five distinct zones, ensuring robustness and generalizability across heterogeneous urban landscapes. Our method outperforms standard baselines, improving Kappa coefficient by 17 points over individual cell classification. We also show that graph-based modeling surpasses simple feature concatenation of neighboring cells, demonstrating the benefit of encoding spatial structure for urban scene understanding.</li>
</ul>

<h3>Title: Explaining novel senses using definition generation with open language models</h3>
<ul>
<li><strong>Authors: </strong>Mariia Fedorova, Andrey Kutuzov, Francesco Periti, Yves Scherrer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26181">https://arxiv.org/abs/2509.26181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26181">https://arxiv.org/pdf/2509.26181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26181]] Explaining novel senses using definition generation with open language models(https://arxiv.org/abs/2509.26181)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We apply definition generators based on open-weights large language models to the task of creating explanations of novel senses, taking target word usages as an input. To this end, we employ the datasets from the AXOLOTL'24 shared task on explainable semantic change modeling, which features Finnish, Russian and German languages. We fine-tune and provide publicly the open-source models performing higher than the best submissions of the aforementioned shared task, which employed closed proprietary LLMs. In addition, we find that encoder-decoder definition generators perform on par with their decoder-only counterparts.</li>
</ul>

<h3>Title: AttriGen: Automated Multi-Attribute Annotation for Blood Cell Datasets</h3>
<ul>
<li><strong>Authors: </strong>Walid Houmaidi, Youssef Sabiri, Fatima Zahra Iguenfer, Amine Abouaomar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26185">https://arxiv.org/abs/2509.26185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26185">https://arxiv.org/pdf/2509.26185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26185]] AttriGen: Automated Multi-Attribute Annotation for Blood Cell Datasets(https://arxiv.org/abs/2509.26185)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>We introduce AttriGen, a novel framework for automated, fine-grained multi-attribute annotation in computer vision, with a particular focus on cell microscopy where multi-attribute classification remains underrepresented compared to traditional cell type categorization. Using two complementary datasets: the Peripheral Blood Cell (PBC) dataset containing eight distinct cell types and the WBC Attribute Dataset (WBCAtt) that contains their corresponding 11 morphological attributes, we propose a dual-model architecture that combines a CNN for cell type classification, as well as a Vision Transformer (ViT) for multi-attribute classification achieving a new benchmark of 94.62\% accuracy. Our experiments demonstrate that AttriGen significantly enhances model interpretability and offers substantial time and cost efficiency relative to conventional full-scale human annotation. Thus, our framework establishes a new paradigm that can be extended to other computer vision classification tasks by effectively automating the expansion of multi-attribute labels.</li>
</ul>

<h3>Title: PDE Solvers Should Be Local: Fast, Stable Rollouts with Learned Local Stencils</h3>
<ul>
<li><strong>Authors: </strong>Chun-Wun Cheng, Bin Dong, Carola-Bibiane Schönlieb, Angelica I Aviles-Rivero</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26186">https://arxiv.org/abs/2509.26186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26186">https://arxiv.org/pdf/2509.26186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26186]] PDE Solvers Should Be Local: Fast, Stable Rollouts with Learned Local Stencils(https://arxiv.org/abs/2509.26186)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Neural operator models for solving partial differential equations (PDEs) often rely on global mixing mechanisms-such as spectral convolutions or attention-which tend to oversmooth sharp local dynamics and introduce high computational cost. We present FINO, a finite-difference-inspired neural architecture that enforces strict locality while retaining multiscale representational power. FINO replaces fixed finite-difference stencil coefficients with learnable convolutional kernels and evolves states via an explicit, learnable time-stepping scheme. A central Local Operator Block leverage a differential stencil layer, a gating mask, and a linear fuse step to construct adaptive derivative-like local features that propagate forward in time. Embedded in an encoder-decoder with a bottleneck, FINO captures fine-grained local structures while preserving interpretability. We establish (i) a composition error bound linking one-step approximation error to stable long-horizon rollouts under a Lipschitz condition, and (ii) a universal approximation theorem for discrete time-stepped PDE dynamics. (iii) Across six benchmarks and a climate modelling task, FINO achieves up to 44\% lower error and up to around 2\times speedups over state-of-the-art operator-learning baselines, demonstrating that strict locality with learnable time-stepping yields an accurate and scalable foundation for neural PDE solvers.</li>
</ul>

<h3>Title: Optimizing Indoor Environmental Quality in Smart Buildings Using Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Youssef Sabiri, Walid Houmaidi, Aaya Bougrine, Salmane El Mansour Billah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26187">https://arxiv.org/abs/2509.26187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26187">https://arxiv.org/pdf/2509.26187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26187]] Optimizing Indoor Environmental Quality in Smart Buildings Using Deep Learning(https://arxiv.org/abs/2509.26187)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Ensuring optimal Indoor Environmental Quality (IEQ) is vital for occupant health and productivity, yet it often comes at a high energy cost in conventional Heating, Ventilation, and Air Conditioning (HVAC) systems. This paper proposes a deep learning driven approach to proactively manage IEQ parameters specifically CO2 concentration, temperature, and humidity while balancing building energy efficiency. Leveraging the ROBOD dataset collected from a net-zero energy academic building, we benchmark three architectures--Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and a hybrid Convolutional Neural Network LSTM (CNN-LSTM)--to forecast IEQ variables across various time horizons. Our results show that GRU achieves the best short-term prediction accuracy with lower computational overhead, whereas CNN-LSTM excels in extracting dominant features for extended forecasting windows. Meanwhile, LSTM offers robust long-range temporal modeling. The comparative analysis highlights that prediction reliability depends on data resolution, sensor placement, and fluctuating occupancy conditions. These findings provide actionable insights for intelligent Building Management Systems (BMS) to implement predictive HVAC control, thereby reducing energy consumption and enhancing occupant comfort in real-world building operations.</li>
</ul>

<h3>Title: VietBinoculars: A Zero-Shot Approach for Detecting Vietnamese LLM-Generated Text</h3>
<ul>
<li><strong>Authors: </strong>Trieu Hai Nguyen, Sivaswamy Akilesh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26189">https://arxiv.org/abs/2509.26189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26189">https://arxiv.org/pdf/2509.26189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26189]] VietBinoculars: A Zero-Shot Approach for Detecting Vietnamese LLM-Generated Text(https://arxiv.org/abs/2509.26189)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The rapid development research of Large Language Models (LLMs) based on transformer architectures raises key challenges, one of them being the task of distinguishing between human-written text and LLM-generated text. As LLM-generated textual content, becomes increasingly complex over time, and resembles human writing, traditional detection methods are proving less effective, especially as the number and diversity of LLMs continue to grow with new models and versions being released at a rapid pace. This study proposes VietBinoculars, an adaptation of the Binoculars method with optimized global thresholds, to enhance the detection of Vietnamese LLM-generated text. We have constructed new Vietnamese AI-generated datasets to determine the optimal thresholds for VietBinoculars and to enable benchmarking. The results from our experiments show results show that VietBinoculars achieves over 99\% in all two domains of accuracy, F1-score, and AUC on multiple out-of-domain datasets. It outperforms the original Binoculars model, traditional detection methods, and other state-of-the-art approaches, including commercial tools such as ZeroGPT and DetectGPT, especially under specially modified prompting strategies.</li>
</ul>

<h3>Title: IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Guo, Chuanhao Yan, Xingqian Xu, Yulin Wang, Kai Wang, Gao Huang, Humphrey Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26231">https://arxiv.org/abs/2509.26231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26231">https://arxiv.org/pdf/2509.26231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26231]] IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance(https://arxiv.org/abs/2509.26231)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Ensuring precise multimodal alignment between diffusion-generated images and input prompts has been a long-standing challenge. Earlier works finetune diffusion weight using high-quality preference data, which tends to be limited and difficult to scale up. Recent editing-based methods further refine local regions of generated images but may compromise overall image quality. In this work, we propose Implicit Multimodal Guidance (IMG), a novel re-generation-based multimodal alignment framework that requires no extra data or editing operations. Specifically, given a generated image and its prompt, IMG a) utilizes a multimodal large language model (MLLM) to identify misalignments; b) introduces an Implicit Aligner that manipulates diffusion conditioning features to reduce misalignments and enable re-generation; and c) formulates the re-alignment goal into a trainable objective, namely Iteratively Updated Preference Objective. Extensive qualitative and quantitative evaluations on SDXL, SDXL-DPO, and FLUX show that IMG outperforms existing alignment methods. Furthermore, IMG acts as a flexible plug-and-play adapter, seamlessly enhancing prior finetuning-based alignment methods. Our code will be available at this https URL.</li>
</ul>

<h3>Title: Machine Learning Detection of Lithium Plating in Lithium-ion Cells: A Gaussian Process Approach</h3>
<ul>
<li><strong>Authors: </strong>Ayush Patnaik, Adam B Zufall, Stephen K Robinson, Xinfan Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26234">https://arxiv.org/abs/2509.26234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26234">https://arxiv.org/pdf/2509.26234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26234]] Machine Learning Detection of Lithium Plating in Lithium-ion Cells: A Gaussian Process Approach(https://arxiv.org/abs/2509.26234)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Lithium plating during fast charging is a critical degradation mechanism that accelerates capacity fade and can trigger catastrophic safety failures. Recent work has identified a distinctive dQ/dV peak above 4.0 V as a reliable signature of plating onset; however, conventional methods for computing dQ/dV rely on finite differencing with filtering, which amplifies sensor noise and introduces bias in peak location. In this paper, we propose a Gaussian Process (GP) framework for lithium plating detection by directly modeling the charge-voltage relationship Q(V) as a stochastic process with calibrated uncertainty. Leveraging the property that derivatives of GPs remain GPs, we infer dQ/dV analytically and probabilistically from the posterior, enabling robust detection without ad hoc smoothing. The framework provides three key benefits: (i) noise-aware inference with hyperparameters learned from data, (ii) closed-form derivatives with credible intervals for uncertainty quantification, and (iii) scalability to online variants suitable for embedded BMS. Experimental validation on Li-ion coin cells across a range of C-rates (0.2C-1C) and temperatures (0-40°C) demonstrates that the GP-based method reliably detects plating peaks under low-temperature, high-rate charging, while correctly reporting no peaks in baseline cases. The concurrence of GP-identified differential peaks, reduced charge throughput, and capacity fade measured via reference performance tests confirms the method's accuracy and robustness, establishing a practical pathway for real-time lithium plating detection.</li>
</ul>

<h3>Title: Interpret, prune and distill Donut : towards lightweight VLMs for VQA on document</h3>
<ul>
<li><strong>Authors: </strong>Adnan Ben Mansour, Ayoub Karine, David Naccache</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26235">https://arxiv.org/abs/2509.26235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26235">https://arxiv.org/pdf/2509.26235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26235]] Interpret, prune and distill Donut : towards lightweight VLMs for VQA on document(https://arxiv.org/abs/2509.26235)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Recent advances in Visually-rich Document Understanding rely on large Vision-Language Models like Donut, which perform document-level Visual Question Answering without Optical Character Recognition. Despite their effectiveness, these models are too costly for real-time or resource-constrained applications. We investigate model compression through knowledge distillation, training compact student models from a larger teacher. We leverage mechanistic interpretability to drive student architecture design within this framework. By analyzing internal computations, we identify essential subcomponents to retain, while having a clear view of which subcomponents should be approximated, skipped, or reparametrized based on their function. This approach yields Donut-MINT (Mechanistic Interpretability-based Network Trimming), a pruned Donut variant that reduces inference time and memory usage while maintaining strong performance on DocVQA, a standard benchmark for document Visual Question Answering. Our method reframes compression as circuit discovery, bridging interpretability research and practical Vision-Language Model deployment.</li>
</ul>

<h3>Title: Beyond Linear Probes: Dynamic Safety Monitoring for Language Models</h3>
<ul>
<li><strong>Authors: </strong>James Oldfield, Philip Torr, Ioannis Patras, Adel Bibi, Fazl Barez</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26238">https://arxiv.org/abs/2509.26238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26238">https://arxiv.org/pdf/2509.26238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26238]] Beyond Linear Probes: Dynamic Safety Monitoring for Language Models(https://arxiv.org/abs/2509.26238)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Monitoring large language models' (LLMs) activations is an effective way to detect harmful requests before they lead to unsafe outputs. However, traditional safety monitors often require the same amount of compute for every query. This creates a trade-off: expensive monitors waste resources on easy inputs, while cheap ones risk missing subtle cases. We argue that safety monitors should be flexible--costs should rise only when inputs are difficult to assess, or when more compute is available. To achieve this, we introduce Truncated Polynomial Classifiers (TPCs), a natural extension of linear probes for dynamic activation monitoring. Our key insight is that polynomials can be trained and evaluated progressively, term-by-term. At test-time, one can early-stop for lightweight monitoring, or use more terms for stronger guardrails when needed. TPCs provide two modes of use. First, as a safety dial: by evaluating more terms, developers and regulators can "buy" stronger guardrails from the same model. Second, as an adaptive cascade: clear cases exit early after low-order checks, and higher-order guardrails are evaluated only for ambiguous inputs, reducing overall monitoring costs. On two large-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with up to 30B parameters, we show that TPCs compete with or outperform MLP-based probe baselines of the same size, all the while being more interpretable than their black-box counterparts. Our code is available at this http URL.</li>
</ul>

<h3>Title: Sandbagging in a Simple Survival Bandit Problem</h3>
<ul>
<li><strong>Authors: </strong>Joel Dyer, Daniel Jarne Ornia, Nicholas Bishop, Anisoara Calinescu, Michael Wooldridge</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26239">https://arxiv.org/abs/2509.26239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26239">https://arxiv.org/pdf/2509.26239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26239]] Sandbagging in a Simple Survival Bandit Problem(https://arxiv.org/abs/2509.26239)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Evaluating the safety of frontier AI systems is an increasingly important concern, helping to measure the capabilities of such models and identify risks before deployment. However, it has been recognised that if AI agents are aware that they are being evaluated, such agents may deliberately hide dangerous capabilities or intentionally demonstrate suboptimal performance in safety-related tasks in order to be released and to avoid being deactivated or retrained. Such strategic deception - often known as "sandbagging" - threatens to undermine the integrity of safety evaluations. For this reason, it is of value to identify methods that enable us to distinguish behavioural patterns that demonstrate a true lack of capability from behavioural patterns that are consistent with sandbagging. In this paper, we develop a simple model of strategic deception in sequential decision-making tasks, inspired by the recently developed survival bandit framework. We demonstrate theoretically that this problem induces sandbagging behaviour in optimal rational agents, and construct a statistical test to distinguish between sandbagging and incompetence from sequences of test scores. In simulation experiments, we investigate the reliability of this test in allowing us to distinguish between such behaviours in bandit models. This work aims to establish a potential avenue for developing robust statistical procedures for use in the science of frontier model evaluations.</li>
</ul>

<h3>Title: From Fragile to Certified: Wasserstein Audits of Group Fairness Under Distribution Shift</h3>
<ul>
<li><strong>Authors: </strong>Ahmad-Reza Ehyaei, Golnoosh Farnadi, Samira Samadi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26241">https://arxiv.org/abs/2509.26241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26241">https://arxiv.org/pdf/2509.26241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26241]] From Fragile to Certified: Wasserstein Audits of Group Fairness Under Distribution Shift(https://arxiv.org/abs/2509.26241)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Group-fairness metrics (e.g., equalized odds) can vary sharply across resamples and are especially brittle under distribution shift, undermining reliable audits. We propose a Wasserstein distributionally robust framework that certifies worst-case group fairness over a ball of plausible test distributions centered at the empirical law. Our formulation unifies common group fairness notions via a generic conditional-probability functional and defines $\varepsilon$-Wasserstein Distributional Fairness ($\varepsilon$-WDF) as the audit target. Leveraging strong duality, we derive tractable reformulations and an efficient estimator (DRUNE) for $\varepsilon$-WDF. We prove feasibility and consistency and establish finite-sample certification guarantees for auditing fairness, along with quantitative bounds under smoothness and margin conditions. Across standard benchmarks and classifiers, $\varepsilon$-WDF delivers stable fairness assessments under distribution shift, providing a principled basis for auditing and certifying group fairness beyond observational data.</li>
</ul>

<h3>Title: Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted Annealing</h3>
<ul>
<li><strong>Authors: </strong>Yang Tang, Ruijie Liu, Yifan Wang, Shiyu Li, Xi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26242">https://arxiv.org/abs/2509.26242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26242">https://arxiv.org/pdf/2509.26242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26242]] Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted Annealing(https://arxiv.org/abs/2509.26242)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) fine-tuning shows excellent implications. However, vanilla fine-tuning methods often require intricate data mixture and repeated experiments for optimal generalization. To address these challenges and streamline the training process, we propose an efficient and universal solution, Dynamic Boosted Annealing (DBA). We obtain a global gradient through zero-learning-rate training on general data, which is subsequently employed for gradient boosting and dynamic training step correction during domain training. In conjunction with annealing learning, we end up establishing a fine-tuning pipeline that relies solely on domain data without collapse. By evaluating both general and domain-specific performance across multiple tasks on several popular base models, DBA achieves an average improvement of 5.8% in joint performance over vanilla fine-tuning. Furthermore, since general data is no longer involved in annealing, repeated experiments led by data mixture are also eliminated. According to our tests, the DBA method can reduce GPU hours by 91.0% compared to the vanilla method.</li>
</ul>

<h3>Title: Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA</h3>
<ul>
<li><strong>Authors: </strong>Zhejia Cai, Yandan Yang, Xinyuan Chang, Shiyi Liang, Ronghan Chen, Feng Xiong, Mu Xu, Ruqi Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26251">https://arxiv.org/abs/2509.26251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26251">https://arxiv.org/pdf/2509.26251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26251]] Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA(https://arxiv.org/abs/2509.26251)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Latent Action Models (LAMs) enable Vision- Language-Action (VLA) systems to learn semantic action rep- resentations from large-scale unannotated data. Yet, we identify two bottlenecks of LAMs: 1) the commonly adopted end-to-end trained image encoder suffers from poor spatial understanding; 2) LAMs can be fragile when input frames are distant, leading to limited temporal perception. Such factors inevitably hinder stable and clear action modeling. To this end, we propose Farsighted-LAM, a latent action framework with geometry- aware spatial encoding and multi-scale temporal modeling, capturing structural priors and dynamic motion patterns from consecutive frames. We further propose SSM-VLA, an end- to-end VLA framework built upon Farsighted-LAM, which integrates structured perception with a visual Chain-of-Thought module to explicitly reason about environmental dynamics, enhancing decision consistency and interpretability. We validate SSM-VLA on multiple VLA tasks in both simulation and real- world settings, and achieve state-of-the-art performance. Our results demonstrate that our strategy of combining geometry- aware modeling, temporal coherence, and explicit reasoning is effective in enhancing the robustness and generalizability of embodied intelligence.</li>
</ul>

<h3>Title: PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Tuan Nguyen, Naseem Khan, Khang Tran, NhatHai Phan, Issa Khalil</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26272">https://arxiv.org/abs/2509.26272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26272">https://arxiv.org/pdf/2509.26272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26272]] PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake Detection(https://arxiv.org/abs/2509.26272)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid rise of synthetic media has made deepfake detection a critical challenge for online safety and trust. Progress remains constrained by the scarcity of large, high-quality datasets. Although multimodal large language models (LLMs) exhibit strong reasoning capabilities, their performance on deepfake detection is poor, often producing explanations that are misaligned with visual evidence or hallucinatory. To address this limitation, we introduce a reasoning-annotated dataset for deepfake detection and propose Paragraph-level Relative Policy Optimization (PRPO), a reinforcement learning algorithm that aligns LLM reasoning with image content at the paragraph level. Experiments show that PRPO improves detection accuracy by a wide margin and achieves the highest reasoning score of 4.55/5.0. Ablation studies further demonstrate that PRPO significantly outperforms GRPO under test-time conditions. These results underscore the importance of grounding multimodal reasoning in visual evidence to enable more reliable and interpretable deepfake detection.</li>
</ul>

<h3>Title: Wasserstein Distributionally Robust Optimization Through the Lens of Structural Causal Models and Individual Fairness</h3>
<ul>
<li><strong>Authors: </strong>Ahmad-Reza Ehyaei, Golnoosh Farnadi, Samira Samadi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26275">https://arxiv.org/abs/2509.26275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26275">https://arxiv.org/pdf/2509.26275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26275]] Wasserstein Distributionally Robust Optimization Through the Lens of Structural Causal Models and Individual Fairness(https://arxiv.org/abs/2509.26275)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>In recent years, Wasserstein Distributionally Robust Optimization (DRO) has garnered substantial interest for its efficacy in data-driven decision-making under distributional uncertainty. However, limited research has explored the application of DRO to address individual fairness concerns, particularly when considering causal structures and sensitive attributes in learning problems. To address this gap, we first formulate the DRO problem from causality and individual fairness perspectives. We then present the DRO dual formulation as an efficient tool to convert the DRO problem into a more tractable and computationally efficient form. Next, we characterize the closed form of the approximate worst-case loss quantity as a regularizer, eliminating the max-step in the min-max DRO problem. We further estimate the regularizer in more general cases and explore the relationship between DRO and classical robust optimization. Finally, by removing the assumption of a known structural causal model, we provide finite sample error bounds when designing DRO with empirical distributions and estimated causal structures to ensure efficiency and robust learning.</li>
</ul>

<h3>Title: Optimizing Speech Language Models for Acoustic Consistency</h3>
<ul>
<li><strong>Authors: </strong>Morteza Rohanian, Michael Krauthammer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26276">https://arxiv.org/abs/2509.26276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26276">https://arxiv.org/pdf/2509.26276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26276]] Optimizing Speech Language Models for Acoustic Consistency(https://arxiv.org/abs/2509.26276)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We study speech language models that incorporate semantic initialization and planning losses to achieve robust and consistent generation. Our approach initializes speech tokens with self-supervised features, applies a light alignment loss, and trains with thinning and auxiliary objectives that target robustness and content planning. We train three models: a 0.7B speech-only model, a 1.0B speech-only model, and a 1.0B interleaved model with both text and speech. Acoustic studies show that the speech-only models achieve the highest consistency across speaker, gender, sentiment, room, and background factors, surpassing larger systems. Interleaving improves lexical and syntactic probes and semantic--acoustic alignment but reduces consistency. Linear probes show that our initialization biases the model toward content structure while trading off prosody detail. These results show that LM-side design and training mix control the balance between acoustic stability and semantic grounding without changes to the tokenizer or runtime architecture. A demo and model weights are available for exploration.</li>
</ul>

<h3>Title: ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation</h3>
<ul>
<li><strong>Authors: </strong>Edoardo Bianchi, Jacopo Staiano, Antonio Liotta</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26278">https://arxiv.org/abs/2509.26278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26278">https://arxiv.org/pdf/2509.26278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26278]] ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation(https://arxiv.org/abs/2509.26278)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, generative</a></li>
<li><strong>Abstract: </strong>Existing approaches to skill proficiency estimation often rely on black-box video classifiers, ignoring multi-view context and lacking explainability. We present ProfVLM, a compact vision-language model that reformulates this task as generative reasoning: it jointly predicts skill level and generates expert-like feedback from egocentric and exocentric videos. Central to our method is an AttentiveGatedProjector that dynamically fuses multi-view features, projected from a frozen TimeSformer backbone into a language model tuned for feedback generation. Trained on EgoExo4D with expert commentaries, ProfVLM surpasses state-of-the-art methods while using up to 20x fewer parameters and reducing training time by up to 60%. Our approach not only achieves superior accuracy across diverse activities, but also outputs natural language critiques aligned with performance, offering transparent reasoning. These results highlight generative vision-language modeling as a powerful new direction for skill assessment.</li>
</ul>

<h3>Title: Reframing Generative Models for Physical Systems using Stochastic Interpolants</h3>
<ul>
<li><strong>Authors: </strong>Anthony Zhou, Alexander Wikner, Amaury Lancelin, Pedram Hassanzadeh, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26282">https://arxiv.org/abs/2509.26282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26282">https://arxiv.org/pdf/2509.26282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26282]] Reframing Generative Models for Physical Systems using Stochastic Interpolants(https://arxiv.org/abs/2509.26282)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have recently emerged as powerful surrogates for physical systems, demonstrating increased accuracy, stability, and/or statistical fidelity. Most approaches rely on iteratively denoising a Gaussian, a choice that may not be the most effective for autoregressive prediction tasks in PDEs and dynamical systems such as climate. In this work, we benchmark generative models across diverse physical domains and tasks, and highlight the role of stochastic interpolants. By directly learning a stochastic process between current and future states, stochastic interpolants can leverage the proximity of successive physical distributions. This allows for generative models that can use fewer sampling steps and produce more accurate predictions than models relying on transporting Gaussian noise. Our experiments suggest that generative models need to balance deterministic accuracy, spectral consistency, and probabilistic calibration, and that stochastic interpolants can potentially fulfill these requirements by adjusting their sampling. This study establishes stochastic interpolants as a competitive baseline for physical emulation and gives insight into the abilities of different generative modeling frameworks.</li>
</ul>

<h3>Title: FLOWER: A Flow-Matching Solver for Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Mehrsa Pourya, Bassam El Rawas, Michael Unser</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26287">https://arxiv.org/abs/2509.26287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26287">https://arxiv.org/pdf/2509.26287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26287]] FLOWER: A Flow-Matching Solver for Inverse Problems(https://arxiv.org/abs/2509.26287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce Flower, a solver for inverse problems. It leverages a pre-trained flow model to produce reconstructions that are consistent with the observed measurements. Flower operates through an iterative procedure over three steps: (i) a flow-consistent destination estimation, where the velocity network predicts a denoised target; (ii) a refinement step that projects the estimated destination onto a feasible set defined by the forward operator; and (iii) a time-progression step that re-projects the refined destination along the flow trajectory. We provide a theoretical analysis that demonstrates how Flower approximates Bayesian posterior sampling, thereby unifying perspectives from plug-and-play methods and generative inverse solvers. On the practical side, Flower achieves state-of-the-art reconstruction quality while using nearly identical hyperparameters across various inverse problems.</li>
</ul>

<h3>Title: Tuning the Tuner: Introducing Hyperparameter Optimization for Auto-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Floris-Jan Willemsen, Rob V. van Nieuwpoort, Ben van Werkhoven</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26300">https://arxiv.org/abs/2509.26300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26300">https://arxiv.org/pdf/2509.26300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26300]] Tuning the Tuner: Introducing Hyperparameter Optimization for Auto-Tuning(https://arxiv.org/abs/2509.26300)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Automatic performance tuning (auto-tuning) is widely used to optimize performance-critical applications across many scientific domains by finding the best program variant among many choices. Efficient optimization algorithms are crucial for navigating the vast and complex search spaces in auto-tuning. As is well known in the context of machine learning and similar fields, hyperparameters critically shape optimization algorithm efficiency. Yet for auto-tuning frameworks, these hyperparameters are almost never tuned, and their potential performance impact has not been studied. We present a novel method for general hyperparameter tuning of optimization algorithms for auto-tuning, thus "tuning the tuner". In particular, we propose a robust statistical method for evaluating hyperparameter performance across search spaces, publish a FAIR data set and software for reproducibility, and present a simulation mode that replays previously recorded tuning data, lowering the costs of hyperparameter tuning by two orders of magnitude. We show that even limited hyperparameter tuning can improve auto-tuner performance by 94.8% on average, and establish that the hyperparameters themselves can be optimized efficiently with meta-strategies (with an average improvement of 204.7%), demonstrating the often overlooked hyperparameter tuning as a powerful technique for advancing auto-tuning research and practice.</li>
</ul>

<h3>Title: NeuroTTT: Bridging Pretraining-Downstream Task Misalignment in EEG Foundation Models via Test-Time Training</h3>
<ul>
<li><strong>Authors: </strong>Suli Wang, Yangshen Deng, Zhenghua Bao, Xinyu Zhan, Yiqun Duan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26301">https://arxiv.org/abs/2509.26301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26301">https://arxiv.org/pdf/2509.26301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26301]] NeuroTTT: Bridging Pretraining-Downstream Task Misalignment in EEG Foundation Models via Test-Time Training(https://arxiv.org/abs/2509.26301)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large-scale foundation models for EEG signals offer a promising path to generalizable brain-computer interface (BCI) applications, but they often suffer from misalignment between pretraining objectives and downstream tasks, as well as significant cross-subject distribution shifts. This paper addresses these challenges by introducing a two-stage alignment strategy that bridges the gap between generic pretraining and specific EEG decoding tasks. First, we propose NeuroTTT: a domain-specific self-supervised fine-tuning paradigm that augments the foundation model with task-relevant self-supervised objectives, aligning latent representations to important spectral, spatial, and temporal EEG features without requiring additional labeled data. Second, we incorporate test-time training (TTT) at inference, we perform (i) self-supervised test-time training on individual unlabeled test samples and (ii) prediction entropy minimization (Tent), which updates only normalization statistics to continually calibrate the model to each new input on the fly. Our approach, which, to our knowledge, is the first to unify domain-tuned self-supervision with test-time training in large-scale EEG foundation models, yields substantially improved robustness and accuracy across diverse BCI tasks (imagined speech, stress detection, motor imagery). Using CBraMod and LaBraM as backbones, our method pushes their performance to a markedly higher level. Results on three diverse tasks demonstrate that the proposed alignment strategy achieves state-of-the-art performance, outperforming conventional fine-tuning and adaptation methods. Our code is available at this https URL.</li>
</ul>

<h3>Title: QUARTZ : QA-based Unsupervised Abstractive Refinement for Task-oriented Dialogue Summarization</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Imed Eddine Ghebriout (1), Gaël Guibon (1, 2), Ivan Lerner (3, 4, 5), Emmanuel Vincent (1) ((1) Universite de Lorraine, CNRS, Inria, LORIA, Nancy, France, (2) Universite Sorbonne Paris Nord, CNRS, LIPN, Villetaneuse, France, (3) Inserm, Centre de Recherche des Cordeliers, Universite Paris Cite, Sorbonne Universite, Paris, France, (4) HeKA, Inria Paris, Paris, France, (5) Assistance Publique Hopitaux de Paris, Georges Pompidou European Hospital, Paris, France)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26302">https://arxiv.org/abs/2509.26302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26302">https://arxiv.org/pdf/2509.26302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26302]] QUARTZ : QA-based Unsupervised Abstractive Refinement for Task-oriented Dialogue Summarization(https://arxiv.org/abs/2509.26302)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Dialogue summarization aims to distill the core meaning of a conversation into a concise text. This is crucial for reducing the complexity and noise inherent in dialogue-heavy applications. While recent approaches typically train language models to mimic human-written summaries, such supervision is costly and often results in outputs that lack task-specific focus limiting their effectiveness in downstream applications, such as medical tasks. In this paper, we propose \app, a framework for task-oriented utility-based dialogue summarization. \app starts by generating multiple summaries and task-oriented question-answer pairs from a dialogue in a zero-shot manner using a pool of large language models (LLMs). The quality of the generated summaries is evaluated by having LLMs answer task-related questions before \textit{(i)} selecting the best candidate answers and \textit{(ii)} identifying the most informative summary based on these answers. Finally, we fine-tune the best LLM on the selected summaries. When validated on multiple datasets, \app demonstrates its effectiveness by achieving competitive results in various zero-shot settings, rivaling fully-supervised State-of-the-Art (SotA) methods.</li>
</ul>

<h3>Title: Attribution-Guided Decoding</h3>
<ul>
<li><strong>Authors: </strong>Piotr Komorowski, Elena Golimblevskaia, Reduan Achtibat, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26307">https://arxiv.org/abs/2509.26307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26307">https://arxiv.org/pdf/2509.26307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26307]] Attribution-Guided Decoding(https://arxiv.org/abs/2509.26307)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The capacity of Large Language Models (LLMs) to follow complex instructions and generate factually accurate text is critical for their real-world application. However, standard decoding methods often fail to robustly satisfy these requirements, while existing control techniques frequently degrade general output quality. In this work, we introduce Attribution-Guided Decoding (AGD), an interpretability-based decoding strategy. Instead of directly manipulating model activations, AGD considers a set of high-probability output token candidates and selects the one that exhibits the highest attribution to a user-defined Region of Interest (ROI). This ROI can be flexibly defined over different parts of the model's input or internal components, allowing AGD to steer generation towards various desirable behaviors. We demonstrate AGD's efficacy across three challenging domains. For instruction following, we show that AGD significantly boosts adherence (e.g., improving the overall success rate on Llama 3.1 from 66.0% to 79.1%). For knowledge-intensive tasks, we show that guiding generation towards usage of internal knowledge components or contextual sources can reduce hallucinations and improve factual accuracy in both closed-book and open-book settings. Furthermore, we propose an adaptive, entropy-based variant of AGD that mitigates quality degradation and reduces computational overhead by applying guidance only when the model is uncertain. Our work presents a versatile, more interpretable, and effective method for enhancing the reliability of modern LLMs.</li>
</ul>

<h3>Title: One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient</h3>
<ul>
<li><strong>Authors: </strong>Rui Ming, Haoyuan Wu, Shoubo Hu, Zhuolun He, Bei Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26313">https://arxiv.org/abs/2509.26313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26313">https://arxiv.org/pdf/2509.26313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26313]] One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient(https://arxiv.org/abs/2509.26313)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) is the predominant method for adapting large language models (LLMs), yet it often struggles with generalization compared to reinforcement learning (RL). In this work, we posit that this performance disparity stems not just from the loss function, but from a more fundamental difference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes on-policy data sampled from the current policy. Building on this hypothesis, we introduce one-token rollout (OTR), a novel fine-tuning algorithm that guides SFT with the policy gradient method. OTR reframes the autoregressive learning process by treating each token generation as a single-step reinforcement learning trajectory. At each step, it performs a Monte Carlo ``rollout'' by sampling multiple candidate tokens from the current policy's distribution. The ground-truth token from the supervised data is then used to provide a reward signal to these samples. Guided by policy gradient, our algorithm repurposes static, off-policy supervised data into a dynamic, on-policy signal at the token level, capturing the generalization benefits of on-policy learning while bypassing the costly overhead of full sentence generation. Through extensive experiments on a diverse suite of challenging benchmarks spanning mathematical reasoning, code generation, and general domain reasoning, we demonstrate that OTR consistently outperforms standard SFT. Our findings establish OTR as a powerful and practical alternative for fine-tuning LLMs and provide compelling evidence that the on-policy nature of data is a critical driver of generalization, offering a promising new direction for fine-tuning LLMs.</li>
</ul>

<h3>Title: Latent Thinking Optimization: Your Latent Reasoning Language Model Secretly Encodes Reward Signals in its Latent Thoughts</h3>
<ul>
<li><strong>Authors: </strong>Hanwen Du, Yuxin Dong, Xia Ning</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26314">https://arxiv.org/abs/2509.26314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26314">https://arxiv.org/pdf/2509.26314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26314]] Latent Thinking Optimization: Your Latent Reasoning Language Model Secretly Encodes Reward Signals in its Latent Thoughts(https://arxiv.org/abs/2509.26314)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel at problem solving by generating chain of thoughts in natural language, but such verbal thinking is computationally costly and prone to overthinking. Recent work instead proposes a latent thinking architecture Huggin-3.5B, which represents intermediate reasoning steps as sequence of latent representations. However, latent thoughts lack interpretability and are difficult to supervise, raising concerns about the correctness and reliability of its latent thinking processes. In this paper, we provide a systematic study of how Huggin-3.5B thinks in the latent space and how external supervision signals can improve its latent thinking processes. We show that latent thoughts leading to correct versus incorrect answers exhibit highly distinguishable patterns, and that a latent classifier can reliably predict answer correctness directly from latent thoughts. Leveraging these insights, we propose Latent Thinking Optimization (LTO), a probabilistic algorithm that employs the latent classifier as a Latent Reward Model (LRM) to optimize the latent thinking processes. Extensive experiments across diverse reasoning tasks demonstrate that LRM is highly effective in detecting incorrect latent thinking patterns, and LTO can significantly improve the latent thinking processes. Furthermore, we show that LRM can generalize across diverse domains, and LTO can be seamlessly applied to general LLMs to improve their thinking processes. In contrast to verbal thinking, our method demonstrates that reward modeling and scaling test-time thinking with supervision can be performed directly in the latent space, highlighting its potential as a general, efficient, and domain-agnostic approach to improving the thinking processes of LLMs.</li>
</ul>

<h3>Title: A Generalized Information Bottleneck Theory of Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Charles Westphal, Stephen Hailes, Mirco Musolesi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26327">https://arxiv.org/abs/2509.26327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26327">https://arxiv.org/pdf/2509.26327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26327]] A Generalized Information Bottleneck Theory of Deep Learning(https://arxiv.org/abs/2509.26327)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>The Information Bottleneck (IB) principle offers a compelling theoretical framework to understand how neural networks (NNs) learn. However, its practical utility has been constrained by unresolved theoretical ambiguities and significant challenges in accurate estimation. In this paper, we present a \textit{Generalized Information Bottleneck (GIB)} framework that reformulates the original IB principle through the lens of synergy, i.e., the information obtainable only through joint processing of features. We provide theoretical and empirical evidence demonstrating that synergistic functions achieve superior generalization compared to their non-synergistic counterparts. Building on these foundations we re-formulate the IB using a computable definition of synergy based on the average interaction information (II) of each feature with those remaining. We demonstrate that the original IB objective is upper bounded by our GIB in the case of perfect estimation, ensuring compatibility with existing IB theory while addressing its limitations. Our experimental results demonstrate that GIB consistently exhibits compression phases across a wide range of architectures (including those with \textit{ReLU} activations where the standard IB fails), while yielding interpretable dynamics in both CNNs and Transformers and aligning more closely with our understanding of adversarial robustness.</li>
</ul>

<h3>Title: Fast-dLLM v2: Efficient Block-Diffusion LLM</h3>
<ul>
<li><strong>Authors: </strong>Chengyue Wu, Hao Zhang, Shuchen Xue, Shizhe Diao, Yonggan Fu, Zhijian Liu, Pavlo Molchanov, Ping Luo, Song Han, Enze Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26328">https://arxiv.org/abs/2509.26328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26328">https://arxiv.org/pdf/2509.26328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26328]] Fast-dLLM v2: Efficient Block-Diffusion LLM(https://arxiv.org/abs/2509.26328)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks, yet their inherent sequential decoding limits inference efficiency. In this work, we propose Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that efficiently adapts pretrained AR models into dLLMs for parallel text generation, requiring only approximately 1B tokens of fine-tuning. This represents a 500x reduction in training data compared to full-attention diffusion LLMs such as Dream (580B tokens), while preserving the original model's performance. Our approach introduces a novel training recipe that combines a block diffusion mechanism with a complementary attention mask, enabling blockwise bidirectional context modeling without sacrificing AR training objectives. To further accelerate decoding, we design a hierarchical caching mechanism: a block-level cache that stores historical context representations across blocks, and a sub-block cache that enables efficient parallel generation within partially decoded blocks. Coupled with our parallel decoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR decoding without compromising generation quality. Extensive experiments across diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs - marking a significant step toward the practical deployment of fast and accurate LLMs. Code and model will be publicly released.</li>
</ul>

<h3>Title: SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Ren-Di Wu, Yu-Yen Lin, Huei-Fang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26330">https://arxiv.org/abs/2509.26330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26330">https://arxiv.org/pdf/2509.26330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26330]] SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval(https://arxiv.org/abs/2509.26330)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Composed Image Retrieval (CIR) aims to retrieve target images that preserve the visual content of a reference image while incorporating user-specified textual modifications. Training-free zero-shot CIR (ZS-CIR) approaches, which require no task-specific training or labeled data, are highly desirable, yet accurately capturing user intent remains challenging. In this paper, we present SQUARE, a novel two-stage training-free framework that leverages Multimodal Large Language Models (MLLMs) to enhance ZS-CIR. In the Semantic Query-Augmented Fusion (SQAF) stage, we enrich the query embedding derived from a vision-language model (VLM) such as CLIP with MLLM-generated captions of the target image. These captions provide high-level semantic guidance, enabling the query to better capture the user's intent and improve global retrieval quality. In the Efficient Batch Reranking (EBR) stage, top-ranked candidates are presented as an image grid with visual marks to the MLLM, which performs joint visual-semantic reasoning across all candidates. Our reranking strategy operates in a single pass and yields more accurate rankings. Experiments show that SQUARE, with its simplicity and effectiveness, delivers strong performance on four standard CIR benchmarks. Notably, it maintains high performance even with lightweight pre-trained, demonstrating its potential applicability.</li>
</ul>

<h3>Title: FedMuon: Federated Learning with Bias-corrected LMO-based Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yuki Takezawa, Anastasia Koloskova, Xiaowen Jiang, Sebastian U. Stich</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26337">https://arxiv.org/abs/2509.26337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26337">https://arxiv.org/pdf/2509.26337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26337]] FedMuon: Federated Learning with Bias-corrected LMO-based Optimization(https://arxiv.org/abs/2509.26337)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Recently, a new optimization method based on the linear minimization oracle (LMO), called Muon, has been attracting increasing attention since it can train neural networks faster than existing adaptive optimization methods, such as Adam. In this paper, we study how Muon can be utilized in federated learning. We first show that straightforwardly using Muon as the local optimizer of FedAvg does not converge to the stationary point since the LMO is a biased operator. We then propose FedMuon which can mitigate this issue. We also analyze how solving the LMO approximately affects the convergence rate and find that, surprisingly, FedMuon can converge for any number of Newton-Schulz iterations, while it can converge faster as we solve the LMO more accurately. Through experiments, we demonstrated that FedMuon can outperform the state-of-the-art federated learning methods.</li>
</ul>

<h3>Title: Memory-Driven Self-Improvement for Decision Making with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xue Yan, Zijing Ou, Mengyue Yang, Yan Song, Haifeng Zhang, Yingzhen Li, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26340">https://arxiv.org/abs/2509.26340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26340">https://arxiv.org/pdf/2509.26340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26340]] Memory-Driven Self-Improvement for Decision Making with Large Language Models(https://arxiv.org/abs/2509.26340)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have emerged as effective action policies for sequential decision-making (SDM) tasks due to their extensive prior knowledge. However, this broad yet general knowledge is often insufficient for specific decision-making tasks with limited task-related data, making it challenging to efficiently adapt LLMs to specific SDM tasks. To address this challenge, we propose a memory-driven self-improvement framework that combines LLM general prior knowledge with a compact memory of domain-specific experiences. Memory retains past interactions and associated Q-values, thereby capturing decision-relevant knowledge that facilitates accurate value estimation and informs the LLM prior refinement. The refined LLM prior, in turn, generates higher-reward trajectories that further enrich memory, forming a natural self-improvement framework where memory and LLM prior mutually reinforce each other. Experiments show that our memory-driven approach significantly outperforms both traditional RL and LLM-based baselines, e.g., improving performance by over 40\% on in-distribution tasks and over 75\% when generalized to unseen tasks in ALFWorld.</li>
</ul>

<h3>Title: SoK: Systematic analysis of adversarial threats against deep learning approaches for autonomous anomaly detection systems in SDN-IoT networks</h3>
<ul>
<li><strong>Authors: </strong>Tharindu Lakshan Yasarathna, Nhien-An Le-Khac</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26350">https://arxiv.org/abs/2509.26350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26350">https://arxiv.org/pdf/2509.26350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26350]] SoK: Systematic analysis of adversarial threats against deep learning approaches for autonomous anomaly detection systems in SDN-IoT networks(https://arxiv.org/abs/2509.26350)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, membership infer, interpretability</a></li>
<li><strong>Abstract: </strong>Integrating SDN and the IoT enhances network control and flexibility. DL-based AAD systems improve security by enabling real-time threat detection in SDN-IoT networks. However, these systems remain vulnerable to adversarial attacks that manipulate input data or exploit model weaknesses, significantly degrading detection accuracy. Existing research lacks a systematic analysis of adversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT environments. This SoK study introduces a structured adversarial threat model and a comprehensive taxonomy of attacks, categorising them into data, model, and hybrid-level threats. Unlike previous studies, we systematically evaluate white, black, and grey-box attack strategies across popular benchmark datasets. Our findings reveal that adversarial attacks can reduce detection accuracy by up to 48.4%, with Membership Inference causing the most significant drop. C&W and DeepFool achieve high evasion success rates. However, adversarial training enhances robustness, and its high computational overhead limits the real-time deployment of SDN-IoT applications. We propose adaptive countermeasures, including real-time adversarial mitigation, enhanced retraining mechanisms, and explainable AI-driven security frameworks. By integrating structured threat models, this study offers a more comprehensive approach to attack categorisation, impact assessment, and defence evaluation than previous research. Our work highlights critical vulnerabilities in existing DL-based AAD models and provides practical recommendations for improving resilience, interpretability, and computational efficiency. This study serves as a foundational reference for researchers and practitioners seeking to enhance DL-based AAD security in SDN-IoT networks, offering a systematic adversarial threat model and conceptual defence evaluation based on prior empirical studies.</li>
</ul>

<h3>Title: LLM-Assisted Emergency Triage Benchmark: Bridging Hospital-Rich and MCI-Like Field Simulation</h3>
<ul>
<li><strong>Authors: </strong>Joshua Sebastian, Karma Tobden, KMA Solaiman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26351">https://arxiv.org/abs/2509.26351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26351">https://arxiv.org/pdf/2509.26351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26351]] LLM-Assisted Emergency Triage Benchmark: Bridging Hospital-Rich and MCI-Like Field Simulation(https://arxiv.org/abs/2509.26351)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Research on emergency and mass casualty incident (MCI) triage has been limited by the absence of openly usable, reproducible benchmarks. Yet these scenarios demand rapid identification of the patients most in need, where accurate deterioration prediction can guide timely interventions. While the MIMIC-IV-ED database is openly available to credentialed researchers, transforming it into a triage-focused benchmark requires extensive preprocessing, feature harmonization, and schema alignment -- barriers that restrict accessibility to only highly technical users. We address these gaps by first introducing an open, LLM-assisted emergency triage benchmark for deterioration prediction (ICU transfer, in-hospital mortality). The benchmark then defines two regimes: (i) a hospital-rich setting with vitals, labs, notes, chief complaints, and structured observations, and (ii) an MCI-like field simulation limited to vitals, observations, and notes. Large language models (LLMs) contributed directly to dataset construction by (i) harmonizing noisy fields such as AVPU and breathing devices, (ii) prioritizing clinically relevant vitals and labs, and (iii) guiding schema alignment and efficient merging of disparate tables. We further provide baseline models and SHAP-based interpretability analyses, illustrating predictive gaps between regimes and the features most critical for triage. Together, these contributions make triage prediction research more reproducible and accessible -- a step toward dataset democratization in clinical AI.</li>
</ul>

<h3>Title: Data-to-Energy Stochastic Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Kirill Tamogashev, Nikolay Malkin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26364">https://arxiv.org/abs/2509.26364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26364">https://arxiv.org/pdf/2509.26364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26364]] Data-to-Energy Stochastic Dynamics(https://arxiv.org/abs/2509.26364)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, data-free, generative</a></li>
<li><strong>Abstract: </strong>The Schrödinger bridge problem is concerned with finding a stochastic dynamical system bridging two marginal distributions that minimises a certain transportation cost. This problem, which represents a generalisation of optimal transport to the stochastic case, has received attention due to its connections to diffusion models and flow matching, as well as its applications in the natural sciences. However, all existing algorithms allow to infer such dynamics only for cases where samples from both distributions are available. In this paper, we propose the first general method for modelling Schrödinger bridges when one (or both) distributions are given by their unnormalised densities, with no access to data samples. Our algorithm relies on a generalisation of the iterative proportional fitting (IPF) procedure to the data-free case, inspired by recent developments in off-policy reinforcement learning for training of diffusion samplers. We demonstrate the efficacy of the proposed data-to-energy IPF on synthetic problems, finding that it can successfully learn transports between multimodal distributions. As a secondary consequence of our reinforcement learning formulation, which assumes a fixed time discretisation scheme for the dynamics, we find that existing data-to-data Schrödinger bridge algorithms can be substantially improved by learning the diffusion coefficient of the dynamics. Finally, we apply the newly developed algorithm to the problem of sampling posterior distributions in latent spaces of generative models, thus creating a data-free image-to-image translation method. Code: this https URL</li>
</ul>

<h3>Title: Go with Your Gut: Scaling Confidence for Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Harold Haodong Chen, Xianfeng Wu, Wen-Jie Shu, Rongjin Guo, Disen Lan, Harry Yang, Ying-Cong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26376">https://arxiv.org/abs/2509.26376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26376">https://arxiv.org/pdf/2509.26376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26376]] Go with Your Gut: Scaling Confidence for Autoregressive Image Generation(https://arxiv.org/abs/2509.26376)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Test-time scaling (TTS) has demonstrated remarkable success in enhancing large language models, yet its application to next-token prediction (NTP) autoregressive (AR) image generation remains largely uncharted. Existing TTS approaches for visual AR (VAR), which rely on frequent partial decoding and external reward models, are ill-suited for NTP-based image generation due to the inherent incompleteness of intermediate decoding results. To bridge this gap, we introduce ScalingAR, the first TTS framework specifically designed for NTP-based AR image generation that eliminates the need for early decoding or auxiliary rewards. ScalingAR leverages token entropy as a novel signal in visual token generation and operates at two complementary scaling levels: (i) Profile Level, which streams a calibrated confidence state by fusing intrinsic and conditional signals; and (ii) Policy Level, which utilizes this state to adaptively terminate low-confidence trajectories and dynamically schedule guidance for phase-appropriate conditioning strength. Experiments on both general and compositional benchmarks show that ScalingAR (1) improves base models by 12.5% on GenEval and 15.2% on TIIF-Bench, (2) efficiently reduces visual token consumption by 62.0% while outperforming baselines, and (3) successfully enhances robustness, mitigating performance drops by 26.0% in challenging scenarios.</li>
</ul>

<h3>Title: Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jinyeop Song, Song Wang, Julian Shun, Yada Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26383">https://arxiv.org/abs/2509.26383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26383">https://arxiv.org/pdf/2509.26383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26383]] Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning(https://arxiv.org/abs/2509.26383)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: PANDA: Towards Generalist Video Anomaly Detection via Agentic AI Engineer</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Yang, Chen Gao, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26386">https://arxiv.org/abs/2509.26386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26386">https://arxiv.org/pdf/2509.26386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26386]] PANDA: Towards Generalist Video Anomaly Detection via Agentic AI Engineer(https://arxiv.org/abs/2509.26386)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Video anomaly detection (VAD) is a critical yet challenging task due to the complex and diverse nature of real-world scenarios. Previous methods typically rely on domain-specific training data and manual adjustments when applying to new scenarios and unseen anomaly types, suffering from high labor costs and limited generalization. Therefore, we aim to achieve generalist VAD, i.e., automatically handle any scene and any anomaly types without training data or human involvement. In this work, we propose PANDA, an agentic AI engineer based on MLLMs. Specifically, we achieve PANDA by comprehensively devising four key capabilities: (1) self-adaptive scene-aware strategy planning, (2) goal-driven heuristic reasoning, (3) tool-augmented self-reflection, and (4) self-improving chain-of-memory. Concretely, we develop a self-adaptive scene-aware RAG mechanism, enabling PANDA to retrieve anomaly-specific knowledge for anomaly detection strategy planning. Next, we introduce a latent anomaly-guided heuristic prompt strategy to enhance reasoning precision. Furthermore, PANDA employs a progressive reflection mechanism alongside a suite of context-aware tools to iteratively refine decision-making in complex scenarios. Finally, a chain-of-memory mechanism enables PANDA to leverage historical experiences for continual performance improvement. Extensive experiments demonstrate that PANDA achieves state-of-the-art performance in multi-scenario, open-set, and complex scenario settings without training and manual involvement, validating its generalizable and robust anomaly detection capability. Code is released at this https URL.</li>
</ul>

<h3>Title: MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Chenhui Zhu, Yilu Wu, Shuai Wang, Gangshan Wu, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26391">https://arxiv.org/abs/2509.26391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26391">https://arxiv.org/pdf/2509.26391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26391]] MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation(https://arxiv.org/abs/2509.26391)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Image-to-video generation has made remarkable progress with the advancements in diffusion models, yet generating videos with realistic motion remains highly challenging. This difficulty arises from the complexity of accurately modeling motion, which involves capturing physical constraints, object interactions, and domain-specific dynamics that are not easily generalized across diverse scenarios. To address this, we propose MotionRAG, a retrieval-augmented framework that enhances motion realism by adapting motion priors from relevant reference videos through Context-Aware Motion Adaptation (CAMA). The key technical innovations include: (i) a retrieval-based pipeline extracting high-level motion features using video encoder and specialized resamplers to distill semantic motion representations; (ii) an in-context learning approach for motion adaptation implemented through a causal transformer architecture; (iii) an attention-based motion injection adapter that seamlessly integrates transferred motion features into pretrained video diffusion models. Extensive experiments demonstrate that our method achieves significant improvements across multiple domains and various base models, all with negligible computational overhead during inference. Furthermore, our modular design enables zero-shot generalization to new domains by simply updating the retrieval database without retraining any components. This research enhances the core capability of video generation systems by enabling the effective retrieval and transfer of motion priors, facilitating the synthesis of realistic motion dynamics.</li>
</ul>

<h3>Title: Exact Bias of Linear TRNG Correctors - Spectral Approach</h3>
<ul>
<li><strong>Authors: </strong>Maciej Skorski, Francisco-Javier Soto, Onur Günlü</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26393">https://arxiv.org/abs/2509.26393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26393">https://arxiv.org/pdf/2509.26393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26393]] Exact Bias of Linear TRNG Correctors - Spectral Approach(https://arxiv.org/abs/2509.26393)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction</a></li>
<li><strong>Abstract: </strong>Using Fourier analysis, this paper establishes exact security bounds for linear extractors in True Random Number Generators (TRNGs). We provide the first near-optimal total variation security characterization by interpolating between optimal $\ell_{\infty}$ and $\ell_2$ norm results, expressed through code weight enumerators and input bias parameters. Our bounds improve security assessments by an order of magnitude over previous approximations. By scanning ~20,000 codes, we reveal fundamental trade-offs between compression efficiency and cryptographic security. For instance, we show that achieving 80 bits of security can require sacrificing more than 50\% of the code rate when correcting 10\% input bias. Our bounds enhance security evaluation of TRNG post-processing schemes and quantify the inherent cost of randomness extraction in hardware implementations.</li>
</ul>

<h3>Title: SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language Model Was Trained From</h3>
<ul>
<li><strong>Authors: </strong>Yao Tong, Haonan Wang, Siquan Li, Kenji Kawaguchi, Tianyang Hu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26404">https://arxiv.org/abs/2509.26404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26404">https://arxiv.org/pdf/2509.26404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26404]] SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language Model Was Trained From(https://arxiv.org/abs/2509.26404)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, biometric, large language model</a></li>
<li><strong>Abstract: </strong>Fingerprinting Large Language Models (LLMs) is essential for provenance verification and model attribution. Existing methods typically extract post-hoc signatures based on training dynamics, data exposure, or hyperparameters -- properties that only emerge after training begins. In contrast, we propose a stronger and more intrinsic notion of LLM fingerprinting: SeedPrints, a method that leverages random initialization biases as persistent, seed-dependent identifiers present even before training. We show that untrained models exhibit reproducible token selection biases conditioned solely on their parameters at initialization. These biases are stable and measurable throughout training, enabling our statistical detection method to recover a model's lineage with high confidence. Unlike prior techniques, unreliable before convergence and vulnerable to distribution shifts, SeedPrints remains effective across all training stages and robust under domain shifts or parameter modifications. Experiments on LLaMA-style and Qwen-style models show that SeedPrints achieves seed-level distinguishability and can provide birth-to-lifecycle identity verification akin to a biometric fingerprint. Evaluations on large-scale pretrained models and fingerprinting benchmarks further confirm its effectiveness under practical deployment scenarios. These results suggest that initialization itself imprints a unique and persistent identity on neural language models, forming a true ''Galtonian'' fingerprint.</li>
</ul>

<h3>Title: Refine Drugs, Don't Complete Them: Uniform-Source Discrete Flows for Fragment-Based Drug Discovery</h3>
<ul>
<li><strong>Authors: </strong>Benno Kaech, Luis Wyss, Karsten Borgwardt, Gianvito Grasso</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26405">https://arxiv.org/abs/2509.26405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26405">https://arxiv.org/pdf/2509.26405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26405]] Refine Drugs, Don't Complete Them: Uniform-Source Discrete Flows for Fragment-Based Drug Discovery(https://arxiv.org/abs/2509.26405)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce InVirtuoGen, a discrete flow generative model for fragmented SMILES for de novo and fragment-constrained generation, and target-property/lead optimization of small molecules. The model learns to transform a uniform source over all possible tokens into the data distribution. Unlike masked models, its training loss accounts for predictions on all sequence positions at every denoising step, shifting the generation paradigm from completion to refinement, and decoupling the number of sampling steps from the sequence length. For \textit{de novo} generation, InVirtuoGen achieves a stronger quality-diversity pareto frontier than prior fragment-based models and competitive performance on fragment-constrained tasks. For property and lead optimization, we propose a hybrid scheme that combines a genetic algorithm with a Proximal Property Optimization fine-tuning strategy adapted to discrete flows. Our approach sets a new state-of-the-art on the Practical Molecular Optimization benchmark, measured by top-10 AUC across tasks, and yields higher docking scores in lead optimization than previous baselines. InVirtuoGen thus establishes a versatile generative foundation for drug discovery, from early hit finding to multi-objective lead optimization. We further contribute to open science by releasing pretrained checkpoints and code, making our results fully reproducible\footnote{this https URL}.</li>
</ul>

<h3>Title: PRISM: Progressive Rain removal with Integrated State-space Modeling</h3>
<ul>
<li><strong>Authors: </strong>Pengze Xue, Shanwen Wang, Fei Zhou, Yan Cui, Xin Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26413">https://arxiv.org/abs/2509.26413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26413">https://arxiv.org/pdf/2509.26413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26413]] PRISM: Progressive Rain removal with Integrated State-space Modeling(https://arxiv.org/abs/2509.26413)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Image deraining is an essential vision technique that removes rain streaks and water droplets, enhancing clarity for critical vision tasks like autonomous driving. However, current single-scale models struggle with fine-grained recovery and global consistency. To address this challenge, we propose Progressive Rain removal with Integrated State-space Modeling (PRISM), a progressive three-stage framework: Coarse Extraction Network (CENet), Frequency Fusion Network (SFNet), and Refine Network (RNet). Specifically, CENet and SFNet utilize a novel Hybrid Attention UNet (HA-UNet) for multi-scale feature aggregation by combining channel attention with windowed spatial transformers. Moreover, we propose Hybrid Domain Mamba (HDMamba) for SFNet to jointly model spatial semantics and wavelet domain characteristics. Finally, RNet recovers the fine-grained structures via an original-resolution subnetwork. Our model learns high-frequency rain characteristics while preserving structural details and maintaining global context, leading to improved image quality. Our method achieves competitive results on multiple datasets against recent deraining methods.</li>
</ul>

<h3>Title: Automatic Fact-checking in English and Telugu</h3>
<ul>
<li><strong>Authors: </strong>Ravi Kiran Chikkala, Tatiana Anikina, Natalia Skachkova, Ivan Vykopal, Rodrigo Agerri, Josef van Genabith</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26415">https://arxiv.org/abs/2509.26415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26415">https://arxiv.org/pdf/2509.26415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26415]] Automatic Fact-checking in English and Telugu(https://arxiv.org/abs/2509.26415)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>False information poses a significant global challenge, and manually verifying claims is a time-consuming and resource-intensive process. In this research paper, we experiment with different approaches to investigate the effectiveness of large language models (LLMs) in classifying factual claims by their veracity and generating justifications in English and Telugu. The key contributions of this work include the creation of a bilingual English-Telugu dataset and the benchmarking of different veracity classification approaches based on LLMs.</li>
</ul>

<h3>Title: AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size</h3>
<ul>
<li><strong>Authors: </strong>Guanxi Lu, Hao (Mark)Chen, Yuto Karashima, Zhican Wang, Daichi Fujiki, Hongxiang Fan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26432">https://arxiv.org/abs/2509.26432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26432">https://arxiv.org/pdf/2509.26432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26432]] AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size(https://arxiv.org/abs/2509.26432)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion-based large language models (dLLMs) are gaining attention for their inherent capacity for parallel decoding, offering a compelling alternative to autoregressive LLMs. Among various decoding strategies, blockwise semi-autoregressive (semi-AR) approaches are widely adopted due to their natural support for KV caching and their favorable accuracy-speed trade-off. However, this paper identifies two fundamental limitations in the conventional semi-AR decoding approach that applies a fixed block size: i) late decoding overhead, where the unmasking of high-confidence tokens outside the current block is unnecessarily delayed, and ii) premature decoding error, where low-confidence tokens inside the current block are committed too early, leading to incorrect tokens. This paper presents the first systematic investigation challenging the fixed block size assumption in semi-AR decoding. Through a statistical analysis of confidence dynamics during the denoising process, we identify a volatility band (VB) region during dLLM decoding, which encodes local semantic structure and can be used to guide adaptive block sizing. Leveraging these insights, we introduce AdaBlock-dLLM, a training-free, plug-and-play scheduler that adaptively aligns block boundaries with semantic steps by adjusting block size during runtime. Extensive experiments across diverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy improvement under the same throughput budget. Beyond inference-time optimization, we hope our semantics-aware adaptive scheduling approach and confidence-based analysis will inspire future training strategies for dLLMs.</li>
</ul>

<h3>Title: ACT: Agentic Classification Tree</h3>
<ul>
<li><strong>Authors: </strong>Vincent Grari, Tim Arni, Thibault Laugel, Sylvain Lamprier, James Zou, Marcin Detyniecki</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26433">https://arxiv.org/abs/2509.26433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26433">https://arxiv.org/pdf/2509.26433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26433]] ACT: Agentic Classification Tree(https://arxiv.org/abs/2509.26433)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>When used in high-stakes settings, AI systems are expected to produce decisions that are transparent, interpretable, and auditable, a requirement increasingly expected by regulations. Decision trees such as CART provide clear and verifiable rules, but they are restricted to structured tabular data and cannot operate directly on unstructured inputs such as text. In practice, large language models (LLMs) are widely used for such data, yet prompting strategies such as chain-of-thought or prompt optimization still rely on free-form reasoning, limiting their ability to ensure trustworthy behaviors. We present the Agentic Classification Tree (ACT), which extends decision-tree methodology to unstructured inputs by formulating each split as a natural-language question, refined through impurity-based evaluation and LLM feedback via TextGrad. Experiments on text benchmarks show that ACT matches or surpasses prompting-based baselines while producing transparent and interpretable decision paths.</li>
</ul>

<h3>Title: Adaptive Planning for Multi-Attribute Controllable Summarization with Monte Carlo Tree Search</h3>
<ul>
<li><strong>Authors: </strong>Sangwon Ryu, Heejin Do, Yunsu Kim, Gary Geunbae Lee, Jungseul Ok</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26435">https://arxiv.org/abs/2509.26435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26435">https://arxiv.org/pdf/2509.26435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26435]] Adaptive Planning for Multi-Attribute Controllable Summarization with Monte Carlo Tree Search(https://arxiv.org/abs/2509.26435)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Controllable summarization moves beyond generic outputs toward human-aligned summaries guided by specified attributes. In practice, the interdependence among attributes makes it challenging for language models to satisfy correlated constraints consistently. Moreover, previous approaches often require per-attribute fine-tuning, limiting flexibility across diverse summary attributes. In this paper, we propose adaptive planning for multi-attribute controllable summarization (PACO), a training-free framework that reframes the task as planning the order of sequential attribute control with a customized Monte Carlo Tree Search (MCTS). In PACO, nodes represent summaries, and actions correspond to single-attribute adjustments, enabling progressive refinement of only the attributes requiring further control. This strategy adaptively discovers optimal control orders, ultimately producing summaries that effectively meet all constraints. Extensive experiments across diverse domains and models demonstrate that PACO achieves robust multi-attribute controllability, surpassing both LLM-based self-planning models and fine-tuned baselines. Remarkably, PACO with Llama-3.2-1B rivals the controllability of the much larger Llama-3.3-70B baselines. With larger models, PACO achieves superior control performance, outperforming all competitors.</li>
</ul>

<h3>Title: Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Donghoon Kim, Dongyoung Lee, Ik Joon Chang, Sung-Ho Bae</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26436">https://arxiv.org/abs/2509.26436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26436">https://arxiv.org/pdf/2509.26436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26436]] Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models(https://arxiv.org/abs/2509.26436)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models achieve high-quality image generation but face deployment challenges due to their high computational requirements. Although 8-bit outlier-aware post-training quantization (PTQ) matches full-precision performance, extending PTQ to 4 bits remains challenging. Larger step sizes in 4-bit quantization amplify rounding errors in dense, low-magnitude activations, leading to the loss of fine-grained textures. We hypothesize that not only outliers but also small activations are critical for texture fidelity. To this end, we propose Quantization via Residual Truncation and Zero Suppression (QuaRTZ), a 4-bit PTQ scheme for diffusion models. QuaRTZ applies 8-bit min-max quantization for outlier handling and compresses to 4 bits via leading-zero suppression to retain LSBs, thereby preserving texture details. Our approach reduces rounding errors and improves quantization efficiency by balancing outlier preservation and LSB precision. Both theoretical derivations and empirical evaluations demonstrate the generalizability of QuaRTZ across diverse activation distributions. Notably, 4-bit QuaRTZ achieves an FID of 6.98 on FLUX.1-schnell, outperforming SVDQuant that requires auxiliary FP16 branches.</li>
</ul>

<h3>Title: Multi-View Camera System for Variant-Aware Autonomous Vehicle Inspection and Defect Detection</h3>
<ul>
<li><strong>Authors: </strong>Yash Kulkarni, Raman Jha, Renu Kachhoria</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26454">https://arxiv.org/abs/2509.26454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26454">https://arxiv.org/pdf/2509.26454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26454]] Multi-View Camera System for Variant-Aware Autonomous Vehicle Inspection and Defect Detection(https://arxiv.org/abs/2509.26454)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Ensuring that every vehicle leaving a modern production line is built to the correct \emph{variant} specification and is free from visible defects is an increasingly complex challenge. We present the \textbf{Automated Vehicle Inspection (AVI)} platform, an end-to-end, \emph{multi-view} perception system that couples deep-learning detectors with a semantic rule engine to deliver \emph{variant-aware} quality control in real time. Eleven synchronized cameras capture a full 360° sweep of each vehicle; task-specific views are then routed to specialised modules: YOLOv8 for part detection, EfficientNet for ICE/EV classification, Gemini-1.5 Flash for mascot OCR, and YOLOv8-Seg for scratch-and-dent segmentation. A view-aware fusion layer standardises evidence, while a VIN-conditioned rule engine compares detected features against the expected manifest, producing an interpretable pass/fail report in \(\approx\! 300\,\text{ms}\). On a mixed data set of Original Equipment Manufacturer(OEM) vehicle data sets of four distinct models plus public scratch/dent images, AVI achieves \textbf{ 93 \%} verification accuracy, \textbf{86 \%} defect-detection recall, and sustains \(\mathbf{3.3}\) vehicles/min, surpassing single-view or no segmentation baselines by large margins. To our knowledge, this is the first publicly reported system that unifies multi-camera feature validation with defect detection in a deployable automotive setting in industry.</li>
</ul>

<h3>Title: Stylos: Multi-View 3D Stylization with Single-Forward Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Hanzhou Liu, Jia Huang, Mi Lu, Srikanth Saripalli, Peng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26455">https://arxiv.org/abs/2509.26455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26455">https://arxiv.org/pdf/2509.26455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26455]] Stylos: Multi-View 3D Stylization with Single-Forward Gaussian Splatting(https://arxiv.org/abs/2509.26455)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present Stylos, a single-forward 3D Gaussian framework for 3D style transfer that operates on unposed content, from a single image to a multi-view collection, conditioned on a separate reference style image. Stylos synthesizes a stylized 3D Gaussian scene without per-scene optimization or precomputed poses, achieving geometry-aware, view-consistent stylization that generalizes to unseen categories, scenes, and styles. At its core, Stylos adopts a Transformer backbone with two pathways: geometry predictions retain self-attention to preserve geometric fidelity, while style is injected via global cross-attention to enforce visual consistency across views. With the addition of a voxel-based 3D style loss that aligns aggregated scene features to style statistics, Stylos enforces view-consistent stylization while preserving geometry. Experiments across multiple datasets demonstrate that Stylos delivers high-quality zero-shot stylization, highlighting the effectiveness of global style-content coupling, the proposed 3D style loss, and the scalability of our framework from single view to large-scale multi-view settings.</li>
</ul>

<h3>Title: Attention over Scene Graphs: Indoor Scene Representations Toward CSAI Classification</h3>
<ul>
<li><strong>Authors: </strong>Artur Barros, Carlos Caetano, João Macedo, Jefersson A. dos Santos, Sandra Avila</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26457">https://arxiv.org/abs/2509.26457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26457">https://arxiv.org/pdf/2509.26457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26457]] Attention over Scene Graphs: Indoor Scene Representations Toward CSAI Classification(https://arxiv.org/abs/2509.26457)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, explainability</a></li>
<li><strong>Abstract: </strong>Indoor scene classification is a critical task in computer vision, with wide-ranging applications that go from robotics to sensitive content analysis, such as child sexual abuse imagery (CSAI) classification. The problem is particularly challenging due to the intricate relationships between objects and complex spatial layouts. In this work, we propose the Attention over Scene Graphs for Sensitive Content Analysis (ASGRA), a novel framework that operates on structured graph representations instead of raw pixels. By first converting images into Scene Graphs and then employing a Graph Attention Network for inference, ASGRA directly models the interactions between a scene's components. This approach offers two key benefits: (i) inherent explainability via object and relationship identification, and (ii) privacy preservation, enabling model training without direct access to sensitive images. On Places8, we achieve 81.27% balanced accuracy, surpassing image-based methods. Real-world CSAI evaluation with law enforcement yields 74.27% balanced accuracy. Our results establish structured scene representations as a robust paradigm for indoor scene classification and CSAI classification. Code is publicly available at this https URL.</li>
</ul>

<h3>Title: CreAgentive: An Agent Workflow Driven Multi-Category Creative Generation Engine</h3>
<ul>
<li><strong>Authors: </strong>Yuyang Cheng, Linyue Cai, Changwei Peng, Yumiao Xu, Rongfang Bie, Yong Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26461">https://arxiv.org/abs/2509.26461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26461">https://arxiv.org/pdf/2509.26461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26461]] CreAgentive: An Agent Workflow Driven Multi-Category Creative Generation Engine(https://arxiv.org/abs/2509.26461)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We present CreAgentive, an agent workflow driven multi-category creative generation engine that addresses four key limitations of contemporary large language models in writing stories, drama and other categories of creatives: restricted genre diversity, insufficient output length, weak narrative coherence, and inability to enforce complex structural constructs. At its core, CreAgentive employs a Story Prototype, which is a genre-agnostic, knowledge graph-based narrative representation that decouples story logic from stylistic realization by encoding characters, events, and environments as semantic triples. CreAgentive engages a three-stage agent workflow that comprises: an Initialization Stage that constructs a user-specified narrative skeleton; a Generation Stage in which long- and short-term objectives guide multi-agent dialogues to instantiate the Story Prototype; a Writing Stage that leverages this prototype to produce multi-genre text with advanced structures such as retrospection and foreshadowing. This architecture reduces storage redundancy and overcomes the typical bottlenecks of long-form generation. In extensive experiments, CreAgentive generates thousands of chapters with stable quality and low cost (less than $1 per 100 chapters) using a general-purpose backbone model. To evaluate performance, we define a two-dimensional framework with 10 narrative indicators measuring both quality and length. Results show that CreAgentive consistently outperforms strong baselines and achieves robust performance across diverse genres, approaching the quality of human-authored novels.</li>
</ul>

<h3>Title: CBAM Integrated Attention Driven Model For Betel Leaf Diseases Classification With Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Sumaiya Tabassum, Md. Faysal Ahamed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26484">https://arxiv.org/abs/2509.26484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26484">https://arxiv.org/pdf/2509.26484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26484]] CBAM Integrated Attention Driven Model For Betel Leaf Diseases Classification With Explainable AI(https://arxiv.org/abs/2509.26484)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Betel leaf is an important crop because of its economic advantages and widespread use. Its betel vines are susceptible to a number of illnesses that are commonly referred to as betel leaf disease. Plant diseases are the largest threat to the food supply's security, and they are challenging to identify in time to stop possible financial damage. Interestingly, artificial intelligence can leave a big mark on the betel leaf industry since it helps with output growth by forecasting sickness. This paper presents a lightweight CBAM-CNN model with just 2.13 million parameters (8.13 MB), incorporating CBAM (Convolutional Block Attention Module) to improve feature emphasis without depending on heavy pre-trained networks. The model's capacity to discern minute variations among leaf disease classes is improved by the integrated attention mechanism, which allows it to adaptively focus on significant spatial and channel-wise information. In order to ensure class balance and diversity for efficient model training and validation, this work makes use of an enriched dataset of 10,185 images divided into three categories: Healthy Leaf, Leaf Rot, and Leaf Spot. The proposed model achieved a precision of 97%, recall of 94%, and F1 score of 95%, and 95.58% accuracy on the test set demonstrating strong and balanced classification performance outperforming traditional pre trained CNN models. The model's focus regions were visualized and interpreted using Grad-CAM (Gradient-weighted Class Activation Mapping), an explainable AI technique.</li>
</ul>

<h3>Title: dParallel: Learnable Parallel Decoding for dLLMs</h3>
<ul>
<li><strong>Authors: </strong>Zigeng Chen, Gongfan Fang, Xinyin Ma, Ruonan Yu, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26488">https://arxiv.org/abs/2509.26488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26488">https://arxiv.org/pdf/2509.26488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26488]] dParallel: Learnable Parallel Decoding for dLLMs(https://arxiv.org/abs/2509.26488)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as a promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parallel decoding potential remains largely underexplored, as existing open-source models still require nearly token-length decoding steps to ensure performance. To address this, we introduce dParallel, a simple and effective method that unlocks the inherent parallelism of dLLMs for fast sampling. We identify that the key bottleneck to parallel decoding arises from the sequential certainty convergence for masked tokens. Building on this insight, we introduce the core of our approach: certainty-forcing distillation, a novel training strategy that distills the model to follow its original sampling trajectories while enforcing it to achieve high certainty on masked tokens more rapidly and in parallel. Extensive experiments across various benchmarks demonstrate that our method can dramatically reduce the number of decoding steps while maintaining performance. When applied to the LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup while maintaining accuracy. Our code is available at this https URL</li>
</ul>

<h3>Title: Contrastive Diffusion Guidance for Spatial Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Sattwik Basu, Chaitanya Amballa, Zhongweiyang Xu, Jorge Vančo Sampedro, Srihari Nelakuditi, Romit Roy Choudhury</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26489">https://arxiv.org/abs/2509.26489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26489">https://arxiv.org/pdf/2509.26489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26489]] Contrastive Diffusion Guidance for Spatial Inverse Problems(https://arxiv.org/abs/2509.26489)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>We consider the inverse problem of reconstructing the spatial layout of a place, a home floorplan for example, from a user`s movements inside that layout. Direct inversion is ill-posed since many floorplans can explain the same movement trajectories. We adopt a diffusion-based posterior sampler to generate layouts consistent with the measurements. While active research is in progress on generative inverse solvers, we find that the forward operator in our problem poses new challenges. The path-planning process inside a floorplan is a non-invertible, non-differentiable function, and causes instability while optimizing using the likelihood score. We break-away from existing approaches and reformulate the likelihood score in a smoother embedding space. The embedding space is trained with a contrastive loss which brings compatible floorplans and trajectories close to each other, while pushing mismatched pairs far apart. We show that a surrogate form of the likelihood score in this embedding space is a valid approximation of the true likelihood score, making it possible to steer the denoising process towards the posterior. Across extensive experiments, our model CoGuide produces more consistent floorplans from trajectories, and is more robust than differentiable-planner baselines and guided-diffusion methods.</li>
</ul>

<h3>Title: VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications</h3>
<ul>
<li><strong>Authors: </strong>Wei He, Yueqing Sun, Hongyan Hao, Xueyuan Hao, Zhikang Xia, Qi Gu, Chengcheng Han, Dengchang Zhao, Hui Su, Kefeng Zhang, Man Gao, Xi Su, Xiaodong Cai, Xunliang Cai, Yu Yang, Yunke Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26490">https://arxiv.org/abs/2509.26490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26490">https://arxiv.org/pdf/2509.26490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26490]] VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications(https://arxiv.org/abs/2509.26490)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single-scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, utilize complex tool sets, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross-scenario tasks, and less than 50% success rate on others. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications. The code, dataset, and leaderboard are available at this https URL</li>
</ul>

<h3>Title: Revealing the Power of Post-Training for Small Language Models via Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Miao Rang, Zhenni Bi, Hang Zhou, Hanting Chen, An Xiao, Tianyu Guo, Kai Han, Xinghao Chen, Yunhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26497">https://arxiv.org/abs/2509.26497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26497">https://arxiv.org/pdf/2509.26497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26497]] Revealing the Power of Post-Training for Small Language Models via Knowledge Distillation(https://arxiv.org/abs/2509.26497)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has significantly advanced the capabilities of artificial intelligence across various domains. However, their massive scale and high computational costs render them unsuitable for direct deployment in resource-constrained edge environments. This creates a critical need for high-performance small models that can operate efficiently at the edge. Yet, after pre-training alone, these smaller models often fail to meet the performance requirements of complex tasks. To bridge this gap, we introduce a systematic post-training pipeline that efficiently enhances small model accuracy. Our post training pipeline consists of curriculum-based supervised fine-tuning (SFT) and offline on-policy knowledge distillation. The resulting instruction-tuned model achieves state-of-the-art performance among billion-parameter models, demonstrating strong generalization under strict hardware constraints while maintaining competitive accuracy across a variety of tasks. This work provides a practical and efficient solution for developing high-performance language models on Ascend edge devices.</li>
</ul>

<h3>Title: DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance</h3>
<ul>
<li><strong>Authors: </strong>Jijun Xiang, Longliang Liu, Xuan Zhu, Xianqi Wang, Min Lin, Xin Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26498">https://arxiv.org/abs/2509.26498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26498">https://arxiv.org/pdf/2509.26498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26498]] DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance(https://arxiv.org/abs/2509.26498)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Depth enhancement, which converts raw dToF signals into dense depth maps using RGB guidance, is crucial for improving depth perception in high-precision tasks such as 3D reconstruction and SLAM. However, existing methods often assume ideal dToF inputs and perfect dToF-RGB alignment, overlooking calibration errors and anomalies, thus limiting real-world applicability. This work systematically analyzes the noise characteristics of real-world lightweight dToF sensors and proposes a practical and novel depth completion framework, DEPTHOR++, which enhances robustness to noisy dToF inputs from three key aspects. First, we introduce a simulation method based on synthetic datasets to generate realistic training samples for robust model training. Second, we propose a learnable-parameter-free anomaly detection mechanism to identify and remove erroneous dToF measurements, preventing misleading propagation during completion. Third, we design a depth completion network tailored to noisy dToF inputs, which integrates RGB images and pre-trained monocular depth estimation priors to improve depth recovery in challenging regions. On the ZJU-L5 dataset and real-world samples, our training strategy significantly boosts existing depth completion models, with our model achieving state-of-the-art performance, improving RMSE and Rel by 22% and 11% on average. On the Mirror3D-NYU dataset, by incorporating the anomaly detection method, our model improves upon the previous SOTA by 37% in mirror regions. On the Hammer dataset, using simulated low-cost dToF data from RealSense L515, our method surpasses the L515 measurements with an average gain of 22%, demonstrating its potential to enable low-cost sensors to outperform higher-end devices. Qualitative results across diverse real-world datasets further validate the effectiveness and generalizability of our approach.</li>
</ul>

<h3>Title: BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yue Wang, Ruotian Ma, Xingyu Chen, Zhengliang Shi, Wanshun Chen, Huang Liu, Jiadi Yao, Qu Yang, Qingxuan Jiang, Fanghua Ye, Juntao Li, Min Zhang, Zhaopeng Tu, Xiaolong Li, Linus</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26514">https://arxiv.org/abs/2509.26514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26514">https://arxiv.org/pdf/2509.26514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26514]] BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs(https://arxiv.org/abs/2509.26514)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rise of Large Language Models (LLMs) is reshaping multimodel models, with speech synthesis being a prominent application. However, existing approaches often underutilize the linguistic intelligence of these models, typically failing to leverage their powerful instruction-following capabilities. This limitation hinders the model's ability to follow text instructions for controllable Text-to-Speech~(TTS). To address this, we propose a new paradigm inspired by ``operationalism'' that decouples instruction understanding from speech generation. We introduce BatonVoice, a framework where an LLM acts as a ``conductor'', understanding user instructions and generating a textual ``plan'' -- explicit vocal features (e.g., pitch, energy). A separate TTS model, the ``orchestra'', then generates the speech from these features. To realize this component, we develop BatonTTS, a TTS model trained specifically for this task. Our experiments demonstrate that BatonVoice achieves strong performance in controllable and emotional speech synthesis, outperforming strong open- and closed-source baselines. Notably, our approach enables remarkable zero-shot cross-lingual generalization, accurately applying feature control abilities to languages unseen during post-training. This demonstrates that objectifying speech into textual vocal features can more effectively unlock the linguistic intelligence of LLMs.</li>
</ul>

<h3>Title: Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization</h3>
<ul>
<li><strong>Authors: </strong>Yaoxiang Wang, Qingguo Hu, Yucheng Ding, Ruizhe Wang, Yeyun Gong, Jian Jiao, Yelong Shen, Peng Cheng, Jinsong Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26520">https://arxiv.org/abs/2509.26520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26520">https://arxiv.org/pdf/2509.26520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26520]] Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization(https://arxiv.org/abs/2509.26520)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently scaling large language models without a proportional increase in computational cost. However, the standard training strategy of Top-K router prevents MoE models from realizing their full potential for elastic inference. When the number of activated experts is altered at inference time, these models exhibit precipitous performance degradation. In this work, we introduce Matryoshka MoE (M-MoE), a training framework that instills a coarse-to-fine structure directly into the expert ensemble. By systematically varying the number of activated experts during training, M-MoE compels the model to learn a meaningful ranking: top-ranked experts collaborate to provide essential, coarse-grained capabilities, while subsequent experts add progressively finer-grained detail. We explore this principle at multiple granularities, identifying a layer-wise randomization strategy as the most effective. Our experiments demonstrate that a single M-MoE model achieves remarkable elasticity, with its performance at various expert counts closely matching that of an entire suite of specialist models, but at only a fraction of the total training cost. This flexibility not only unlocks elastic inference but also enables optimizing performance by allocating different computational budgets to different model layers. Our work paves the way for more practical and adaptable deployments of large-scale MoE models.</li>
</ul>

<h3>Title: TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Seohyun Lee, Wenzhi Fang, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher G. Brinton</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26524">https://arxiv.org/abs/2509.26524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26524">https://arxiv.org/pdf/2509.26524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26524]] TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning(https://arxiv.org/abs/2509.26524)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL), despite demonstrating impressive capabilities in the training of multiple models in a decentralized manner, has been shown to produce a final model not necessarily well-suited to the needs of each client. While extensive work has been conducted on how to create tailored personalized models, called Personalized Federated Learning (PFL), less attention has been given to personalization via fine-tuning of foundation models with multi-task and multi-modal properties. Moreover, there exists a lack of understanding in the literature on how to fine-tune and personalize such models in a setting that is heterogeneous across clients not only in data, but also in tasks and modalities. To address this gap in the literature, we propose TAP (Two-Stage Adaptive Personalization), which (i) leverages mismatched model architectures between the clients and server to selectively conduct replacement operations when it benefits a client's local tasks and (ii) engages in post-FL knowledge distillation for capturing beneficial general knowledge without compromising personalization. We also introduce the first convergence analysis of the server model under its modality-task pair architecture, and demonstrate that as the number of modality-task pairs increases, its ability to cater to all tasks suffers. Through extensive experiments, we demonstrate the effectiveness of our proposed algorithm across a variety of datasets and tasks in comparison to a multitude of baselines. Implementation code is publicly available at this https URL.</li>
</ul>

<h3>Title: Explainable and Resilient ML-Based Physical-Layer Attack Detectors</h3>
<ul>
<li><strong>Authors: </strong>Aleksandra Knapińska, Marija Furdek</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26530">https://arxiv.org/abs/2509.26530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26530">https://arxiv.org/pdf/2509.26530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26530]] Explainable and Resilient ML-Based Physical-Layer Attack Detectors(https://arxiv.org/abs/2509.26530)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, interpretability</a></li>
<li><strong>Abstract: </strong>Detection of emerging attacks on network infrastructure is a critical aspect of security management. To meet the growing scale and complexity of modern threats, machine learning (ML) techniques offer valuable tools for automating the detection of malicious activities. However, as these techniques become more complex, their internal operations grow increasingly opaque. In this context, we address the need for explainable physical-layer attack detection methods. First, we analyze the inner workings of various classifiers trained to alert about physical layer intrusions, examining how the influence of different monitored parameters varies depending on the type of attack being detected. This analysis not only improves the interpretability of the models but also suggests ways to enhance their design for increased speed. In the second part, we evaluate the detectors' resilience to malicious parameter noising. The results highlight a key trade-off between model speed and resilience. This work serves as a design guideline for developing fast and robust detectors trained on available network monitoring data.</li>
</ul>

<h3>Title: Machine-Learning Driven Load Shedding to Mitigate Instability Attacks in Power Grids</h3>
<ul>
<li><strong>Authors: </strong>Justin Tackett, Benjamin Francis, Luis Garcia, David Grimsman, Sean Warnick</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26532">https://arxiv.org/abs/2509.26532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26532">https://arxiv.org/pdf/2509.26532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26532]] Machine-Learning Driven Load Shedding to Mitigate Instability Attacks in Power Grids(https://arxiv.org/abs/2509.26532)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack</a></li>
<li><strong>Abstract: </strong>Every year critical infrastructure becomes more complex and we grow to rely on it more and more. With this reliance, it becomes an attractive target for cyberattacks from sophisticated actors, with one of the most attractive targets being the power grid. One class of attacks, instability attacks, is a newer type of attack that has relatively few protections developed. We present a cost effective, data-driven approach to training a supervised machine learning model to retrofit load shedding decision systems in power grids with the capacity to defend against instability attacks. We show a proof of concept on the IEEE 14 Bus System using the Achilles Heel Technologies Power Grid Analyzer, and show through an implementation of modified Prony analysis (MPA) that MPA is a viable method for detecting instability attacks and triggering defense mechanisms.</li>
</ul>

<h3>Title: OceanGym: A Benchmark Environment for Underwater Embodied Agents</h3>
<ul>
<li><strong>Authors: </strong>Yida Xue, Mingjun Mao, Xiangyuan Ru, Yuqi Zhu, Baochang Ren, Shuofei Qiao, Mengru Wang, Shumin Deng, Xinyu An, Ningyu Zhang, Ying Chen, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26536">https://arxiv.org/abs/2509.26536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26536">https://arxiv.org/pdf/2509.26536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26536]] OceanGym: A Benchmark Environment for Underwater Embodied Agents(https://arxiv.org/abs/2509.26536)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at this https URL.</li>
</ul>

<h3>Title: The Loss Kernel: A Geometric Probe for Deep Learning Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Maxwell Adam, Zach Furman, Jesse Hoogland</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26537">https://arxiv.org/abs/2509.26537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26537">https://arxiv.org/pdf/2509.26537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26537]] The Loss Kernel: A Geometric Probe for Deep Learning Interpretability(https://arxiv.org/abs/2509.26537)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We introduce the loss kernel, an interpretability method for measuring similarity between data points according to a trained neural network. The kernel is the covariance matrix of per-sample losses computed under a distribution of low-loss-preserving parameter perturbations. We first validate our method on a synthetic multitask problem, showing it separates inputs by task as predicted by theory. We then apply this kernel to Inception-v1 to visualize the structure of ImageNet, and we show that the kernel's structure aligns with the WordNet semantic hierarchy. This establishes the loss kernel as a practical tool for interpretability and data attribution.</li>
</ul>

<h3>Title: TASP: Topology-aware Sequence Parallelism</h3>
<ul>
<li><strong>Authors: </strong>Yida Wang (1 and 3), Ke Hong (2 and 3), Xiuhong Li (3), Yuanchao Xu (1), Wenxun Wang (2), Guohao Dai (3 and 4), Yu Wang (2) ((1) Capital Normal University, (2) Tsinghua University, (3) Infinigence-AI, (4) Shanghai Jiao Tong University)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26541">https://arxiv.org/abs/2509.26541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26541">https://arxiv.org/pdf/2509.26541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26541]] TASP: Topology-aware Sequence Parallelism(https://arxiv.org/abs/2509.26541)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long-context large language models (LLMs) face constraints due to the quadratic complexity of the self-attention mechanism. The mainstream sequence parallelism (SP) method, Ring Attention, attempts to solve this by distributing the query into multiple query chunks across accelerators and enable each Q tensor to access all KV tensors from other accelerators via the Ring AllGather communication primitive. However, it exhibits low communication efficiency, restricting its practical applicability. This inefficiency stems from the mismatch between the Ring AllGather communication primitive it adopts and the AlltoAll topology of modern accelerators. A Ring AllGather primitive is composed of iterations of ring-styled data transfer, which can only utilize a very limited fraction of an AlltoAll topology. Inspired by the Hamiltonian decomposition of complete directed graphs, we identify that modern accelerator topology can be decomposed into multiple orthogonal ring datapaths which can concurrently transfer data without interference. Based on this, we further observe that the Ring AllGather primitive can also be decomposed into the same number of concurrent ring-styled data transfer at every iteration. Based on these insights, we propose TASP, a topology-aware SP method for long-context LLMs that fully utilizes the communication capacity of modern accelerators via topology decomposition and primitive decomposition. Experimental results on both single-node and multi-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate that TASP achieves higher communication efficiency than Ring Attention on these modern accelerator topologies and achieves up to 3.58 speedup than Ring Attention and its variant Zigzag-Ring Attention. The code is available at this https URL.</li>
</ul>

<h3>Title: The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models</h3>
<ul>
<li><strong>Authors: </strong>Lina Conti, Dennis Fucci, Marco Gaido, Matteo Negri, Guillaume Wisniewski, Luisa Bentivogli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26543">https://arxiv.org/abs/2509.26543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26543">https://arxiv.org/pdf/2509.26543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26543]] The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models(https://arxiv.org/abs/2509.26543)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Contrastive explanations, which indicate why an AI system produced one output (the target) instead of another (the foil), are widely regarded in explainable AI as more informative and interpretable than standard explanations. However, obtaining such explanations for speech-to-text (S2T) generative models remains an open challenge. Drawing from feature attribution techniques, we propose the first method to obtain contrastive explanations in S2T by analyzing how parts of the input spectrogram influence the choice between alternative outputs. Through a case study on gender assignment in speech translation, we show that our method accurately identifies the audio features that drive the selection of one gender over another. By extending the scope of contrastive explanations to S2T, our work provides a foundation for better understanding S2T models.</li>
</ul>

<h3>Title: Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Agneet Chatterjee, Rahim Entezari, Maksym Zhuravinskyi, Maksim Lapin, Reshinth Adithyan, Amit Raj, Chitta Baral, Yezhou Yang, Varun Jampani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26555">https://arxiv.org/abs/2509.26555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26555">https://arxiv.org/pdf/2509.26555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26555]] Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation(https://arxiv.org/abs/2509.26555)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in video generation have enabled high-fidelity video synthesis from user provided prompts. However, existing models and benchmarks fail to capture the complexity and requirements of professional video generation. Towards that goal, we introduce Stable Cinemetrics, a structured evaluation framework that formalizes filmmaking controls into four disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera. Together, these taxonomies define 76 fine-grained control nodes grounded in industry practices. Using these taxonomies, we construct a benchmark of prompts aligned with professional use cases and develop an automated pipeline for prompt categorization and question generation, enabling independent evaluation of each control dimension. We conduct a large-scale human study spanning 10+ models and 20K videos, annotated by a pool of 80+ film professionals. Our analysis, both coarse and fine-grained reveal that even the strongest current models exhibit significant gaps, particularly in Events and Camera-related controls. To enable scalable evaluation, we train an automatic evaluator, a vision-language model aligned with expert annotations that outperforms existing zero-shot baselines. SCINE is the first approach to situate professional video generation within the landscape of video generative models, introducing taxonomies centered around cinematic controls and supporting them with structured evaluation pipelines and detailed analyses to guide future research.</li>
</ul>

<h3>Title: DeepProv: Behavioral Characterization and Repair of Neural Networks via Inference Provenance Graph Analysis</h3>
<ul>
<li><strong>Authors: </strong>Firas Ben Hmida, Abderrahmen Amich, Ata Kaboudi, Birhanu Eshete</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26562">https://arxiv.org/abs/2509.26562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26562">https://arxiv.org/pdf/2509.26562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26562]] DeepProv: Behavioral Characterization and Repair of Neural Networks via Inference Provenance Graph Analysis(https://arxiv.org/abs/2509.26562)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust, biometric, fair</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) are increasingly being deployed in high-stakes applications, from self-driving cars to biometric authentication. However, their unpredictable and unreliable behaviors in real-world settings require new approaches to characterize and ensure their reliability. This paper introduces DeepProv, a novel and customizable system designed to capture and characterize the runtime behavior of DNNs during inference by using their underlying graph structure. Inspired by system audit provenance graphs, DeepProv models the computational information flow of a DNN's inference process through Inference Provenance Graphs (IPGs). These graphs provide a detailed structural representation of the behavior of DNN, allowing both empirical and structural analysis. DeepProv uses these insights to systematically repair DNNs for specific objectives, such as improving robustness, privacy, or fairness. We instantiate DeepProv with adversarial robustness as the goal of model repair and conduct extensive case studies to evaluate its effectiveness. Our results demonstrate its effectiveness and scalability across diverse classification tasks, attack scenarios, and model complexities. DeepProv automatically identifies repair actions at the node and edge-level within IPGs, significantly enhancing the robustness of the model. In particular, applying DeepProv repair strategies to just a single layer of a DNN yields an average 55% improvement in adversarial accuracy. Moreover, DeepProv complements existing defenses, achieving substantial gains in adversarial robustness. Beyond robustness, we demonstrate the broader potential of DeepProv as an adaptable system to characterize DNN behavior in other critical areas, such as privacy auditing and fairness analysis.</li>
</ul>

<h3>Title: Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zheng Zhang, Ziwei Shan, Kaitao Song, Yexin Li, Kan Ren</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26578">https://arxiv.org/abs/2509.26578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26578">https://arxiv.org/pdf/2509.26578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26578]] Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning(https://arxiv.org/abs/2509.26578)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Process Reward Models (PRMs) have emerged as a promising approach to enhance the reasoning capabilities of large language models (LLMs) by guiding their step-by-step reasoning toward a final answer. However, existing PRMs either treat each reasoning step in isolation, failing to capture inter-step dependencies, or struggle to align process rewards with the final outcome. Consequently, the reward signal fails to respect temporal causality in sequential reasoning and faces ambiguous credit assignment. These limitations make downstream models vulnerable to reward hacking and lead to suboptimal performance. In this work, we propose Conditional Reward Modeling (CRM) that frames LLM reasoning as a temporal process leading to a correct answer. The reward of each reasoning step is not only conditioned on the preceding steps but also explicitly linked to the final outcome of the reasoning trajectory. By enforcing conditional probability rules, our design captures the causal relationships among reasoning steps, with the link to the outcome allowing precise attribution of each intermediate step, thereby resolving credit assignment ambiguity. Further, through this consistent probabilistic modeling, the rewards produced by CRM enable more reliable cross-sample comparison. Experiments across Best-of-N sampling, beam search and reinforcement learning demonstrate that CRM consistently outperforms existing reward models, offering a principled framework for enhancing LLM reasoning. In particular, CRM is more robust to reward hacking and delivers stable downstream improvements without relying on verifiable rewards derived from ground truth.</li>
</ul>

<h3>Title: Autoproof: Automated Segmentation Proofreading for Connectomics</h3>
<ul>
<li><strong>Authors: </strong>Gary B Huang, William M Katz, Stuart Berg, Louis Scheffer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26585">https://arxiv.org/abs/2509.26585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26585">https://arxiv.org/pdf/2509.26585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26585]] Autoproof: Automated Segmentation Proofreading for Connectomics(https://arxiv.org/abs/2509.26585)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Producing connectomes from electron microscopy (EM) images has historically required a great deal of human proofreading effort. This manual annotation cost is the current bottleneck in scaling EM connectomics, for example, in making larger connectome reconstructions feasible, or in enabling comparative connectomics where multiple related reconstructions are produced. In this work, we propose using the available ground-truth data generated by this manual annotation effort to learn a machine learning model to automate or optimize parts of the required proofreading workflows. We validate our approach on a recent complete reconstruction of the \emph{Drosophila} male central nervous system. We first show our method would allow for obtaining 90\% of the value of a guided proofreading workflow while reducing required cost by 80\%. We then demonstrate a second application for automatically merging many segmentation fragments to proofread neurons. Our system is able to automatically attach 200 thousand fragments, equivalent to four proofreader years of manual work, and increasing the connectivity completion rate of the connectome by 1.3\% points.</li>
</ul>

<h3>Title: Generating Difficult-to-Translate Texts</h3>
<ul>
<li><strong>Authors: </strong>Vilém Zouhar, Wenda Xu, Parker Riley, Juraj Juraska, Mara Finkelstein, Markus Freitag, Dan Deutsch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26592">https://arxiv.org/abs/2509.26592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26592">https://arxiv.org/pdf/2509.26592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26592]] Generating Difficult-to-Translate Texts(https://arxiv.org/abs/2509.26592)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Machine translation benchmarks sourced from the real world are quickly obsoleted, due to most examples being easy for state-of-the-art translation models. This limits the benchmark's ability to distinguish which model is better or to reveal models' weaknesses. Current methods for creating difficult test cases, such as subsampling or from-scratch synthesis, either fall short of identifying difficult examples or suffer from a lack of diversity and naturalness. Inspired by the iterative process of human experts probing for model failures, we propose MT-breaker, a method where a large language model iteratively refines a source text to increase its translation difficulty. The LLM iteratively queries a target machine translation model to guide its generation of difficult examples. Our approach generates examples that are more challenging for the target MT model while preserving the diversity of natural texts. While the examples are tailored to a particular machine translation model during the generation, the difficulty also transfers to other models and languages.</li>
</ul>

<h3>Title: Are Robust LLM Fingerprints Adversarially Robust?</h3>
<ul>
<li><strong>Authors: </strong>Anshul Nasery, Edoardo Contente, Alkin Kaz, Pramod Viswanath, Sewoong Oh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26598">https://arxiv.org/abs/2509.26598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26598">https://arxiv.org/pdf/2509.26598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26598]] Are Robust LLM Fingerprints Adversarially Robust?(https://arxiv.org/abs/2509.26598)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Model fingerprinting has emerged as a promising paradigm for claiming model ownership. However, robustness evaluations of these schemes have mostly focused on benign perturbations such as incremental fine-tuning, model merging, and prompting. Lack of systematic investigations into {\em adversarial robustness} against a malicious model host leaves current systems vulnerable. To bridge this gap, we first define a concrete, practical threat model against model fingerprinting. We then take a critical look at existing model fingerprinting schemes to identify their fundamental vulnerabilities. Based on these, we develop adaptive adversarial attacks tailored for each vulnerability, and demonstrate that these can bypass model authentication completely for ten recently proposed fingerprinting schemes while maintaining high utility of the model for the end users. Our work encourages fingerprint designers to adopt adversarial robustness by design. We end with recommendations for future fingerprinting methods.</li>
</ul>

<h3>Title: DiffCamera: Arbitrary Refocusing on Images</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Wang, Xi Chen, Xiaogang Xu, Yu Liu, Hengshuang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26599">https://arxiv.org/abs/2509.26599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26599">https://arxiv.org/pdf/2509.26599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26599]] DiffCamera: Arbitrary Refocusing on Images(https://arxiv.org/abs/2509.26599)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>The depth-of-field (DoF) effect, which introduces aesthetically pleasing blur, enhances photographic quality but is fixed and difficult to modify once the image has been created. This becomes problematic when the applied blur is undesirable~(e.g., the subject is out of focus). To address this, we propose DiffCamera, a model that enables flexible refocusing of a created image conditioned on an arbitrary new focus point and a blur level. Specifically, we design a diffusion transformer framework for refocusing learning. However, the training requires pairs of data with different focus planes and bokeh levels in the same scene, which are hard to acquire. To overcome this limitation, we develop a simulation-based pipeline to generate large-scale image pairs with varying focus planes and bokeh levels. With the simulated data, we find that training with only a vanilla diffusion objective often leads to incorrect DoF behaviors due to the complexity of the task. This requires a stronger constraint during training. Inspired by the photographic principle that photos of different focus planes can be linearly blended into a multi-focus image, we propose a stacking constraint during training to enforce precise DoF manipulation. This constraint enhances model training by imposing physically grounded refocusing behavior that the focusing results should be faithfully aligned with the scene structure and the camera conditions so that they can be combined into the correct multi-focus image. We also construct a benchmark to evaluate the effectiveness of our refocusing model. Extensive experiments demonstrate that DiffCamera supports stable refocusing across a wide range of scenes, providing unprecedented control over DoF adjustments for photography and generative AI applications.</li>
</ul>

<h3>Title: Deconstructing Self-Bias in LLM-generated Translation Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Wenda Xu, Sweta Agrawal, Vilém Zouhar, Markus Freitag, Daniel Deutsch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26600">https://arxiv.org/abs/2509.26600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26600">https://arxiv.org/pdf/2509.26600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26600]] Deconstructing Self-Bias in LLM-generated Translation Benchmarks(https://arxiv.org/abs/2509.26600)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) begin to saturate existing benchmarks, automated benchmark creation using LLMs (LLM as a benchmark) has emerged as a scalable alternative to slow and costly human curation. While these generated test sets have to potential to cheaply rank models, we demonstrate a critical flaw. LLM generated benchmarks systematically favor the model that created the benchmark, they exhibit self bias on low resource languages to English translation tasks. We show three key findings on automatic benchmarking of LLMs for translation: First, this bias originates from two sources: the generated test data (LLM as a testset) and the evaluation method (LLM as an evaluator), with their combination amplifying the effect. Second, self bias in LLM as a benchmark is heavily influenced by the model's generation capabilities in the source language. For instance, we observe more pronounced bias in into English translation, where the model's generation system is developed, than in out of English translation tasks. Third, we observe that low diversity in source text is one attribution to self bias. Our results suggest that improving the diversity of these generated source texts can mitigate some of the observed self bias.</li>
</ul>

<h3>Title: MENLO: From Preferences to Proficiency - Evaluating and Modeling Native-like Quality Across 47 Languages</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Whitehouse, Sebastian Ruder, Tony Lin, Oksana Kurylo, Haruka Takagi, Janice Lam, Nicolò Busetto, Denise Diaz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26601">https://arxiv.org/abs/2509.26601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26601">https://arxiv.org/pdf/2509.26601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26601]] MENLO: From Preferences to Proficiency - Evaluating and Modeling Native-like Quality Across 47 Languages(https://arxiv.org/abs/2509.26601)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Ensuring native-like quality of large language model (LLM) responses across many languages is challenging. To address this, we introduce MENLO, a framework that operationalizes the evaluation of native-like response quality based on audience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423 human-annotated prompt-response preference pairs covering four quality dimensions with high inter-annotator agreement in 47 language varieties. Our evaluation reveals that zero-shot LLM judges benefit significantly from pairwise evaluation and our structured annotation rubrics, yet they still underperform human annotators on our dataset. We demonstrate substantial improvements through fine-tuning with reinforcement learning, reward shaping, and multi-task learning approaches. Additionally, we show that RL-trained judges can serve as generative reward models to enhance LLMs' multilingual proficiency, though discrepancies with human judgment remain. Our findings suggest promising directions for scalable multilingual evaluation and preference alignment. We release our dataset and evaluation framework to support further research in multilingual LLM evaluation.</li>
</ul>

<h3>Title: Video Object Segmentation-Aware Audio Generation</h3>
<ul>
<li><strong>Authors: </strong>Ilpo Viertola, Vladimir Iashin, Esa Rahtu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26604">https://arxiv.org/abs/2509.26604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26604">https://arxiv.org/pdf/2509.26604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26604]] Video Object Segmentation-Aware Audio Generation(https://arxiv.org/abs/2509.26604)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Existing multimodal audio generation models often lack precise user control, which limits their applicability in professional Foley workflows. In particular, these models focus on the entire video and do not provide precise methods for prioritizing a specific object within a scene, generating unnecessary background sounds, or focusing on the wrong objects. To address this gap, we introduce the novel task of video object segmentation-aware audio generation, which explicitly conditions sound synthesis on object-level segmentation maps. We present SAGANet, a new multimodal generative model that enables controllable audio generation by leveraging visual segmentation masks along with video and textual cues. Our model provides users with fine-grained and visually localized control over audio generation. To support this task and further research on segmentation-aware Foley, we propose Segmented Music Solos, a benchmark dataset of musical instrument performance videos with segmentation information. Our method demonstrates substantial improvements over current state-of-the-art methods and sets a new standard for controllable, high-fidelity Foley synthesis. Code, samples, and Segmented Music Solos are available at this https URL</li>
</ul>

<h3>Title: Hy-Facial: Hybrid Feature Extraction by Dimensionality Reduction Methods for Enhanced Facial Expression Classification</h3>
<ul>
<li><strong>Authors: </strong>Xinjin Li, Yu Ma, Kaisen Ye, Jinghan Cao, Minghao Zhou, Yeyang Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26614">https://arxiv.org/abs/2509.26614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26614">https://arxiv.org/pdf/2509.26614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26614]] Hy-Facial: Hybrid Feature Extraction by Dimensionality Reduction Methods for Enhanced Facial Expression Classification(https://arxiv.org/abs/2509.26614)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Facial expression classification remains a challenging task due to the high dimensionality and inherent complexity of facial image data. This paper presents Hy-Facial, a hybrid feature extraction framework that integrates both deep learning and traditional image processing techniques, complemented by a systematic investigation of dimensionality reduction strategies. The proposed method fuses deep features extracted from the Visual Geometry Group 19-layer network (VGG19) with handcrafted local descriptors and the scale-invariant feature transform (SIFT) and Oriented FAST and Rotated BRIEF (ORB) algorithms, to obtain rich and diverse image representations. To mitigate feature redundancy and reduce computational complexity, we conduct a comprehensive evaluation of dimensionality reduction techniques and feature extraction. Among these, UMAP is identified as the most effective, preserving both local and global structures of the high-dimensional feature space. The Hy-Facial pipeline integrated VGG19, SIFT, and ORB for feature extraction, followed by K-means clustering and UMAP for dimensionality reduction, resulting in a classification accuracy of 83. 3\% in the facial expression recognition (FER) dataset. These findings underscore the pivotal role of dimensionality reduction not only as a pre-processing step but as an essential component in improving feature quality and overall classification performance.</li>
</ul>

<h3>Title: HART: Human Aligned Reconstruction Transformer</h3>
<ul>
<li><strong>Authors: </strong>Xiyi Chen, Shaofei Wang, Marko Mihajlovic, Taewon Kang, Sergey Prokudin, Ming Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26621">https://arxiv.org/abs/2509.26621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26621">https://arxiv.org/pdf/2509.26621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26621]] HART: Human Aligned Reconstruction Transformer(https://arxiv.org/abs/2509.26621)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>We introduce HART, a unified framework for sparse-view human reconstruction. Given a small set of uncalibrated RGB images of a person as input, it outputs a watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat representation for photorealistic novel-view rendering. Prior methods for clothed human reconstruction either optimize parametric templates, which overlook loose garments and human-object interactions, or train implicit functions under simplified camera assumptions, limiting applicability in real scenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body correspondences, and employs an occlusion-aware Poisson reconstruction to recover complete geometry, even in self-occluded regions. These predictions also align with a parametric SMPL-X body model, ensuring that reconstructed geometry remains consistent with human structure while capturing loose clothing and interactions. These human-aligned meshes initialize Gaussian splats to further enable sparse-view rendering. While trained on only 2.3K synthetic scans, HART achieves state-of-the-art results: Chamfer Distance improves by 18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on a wide range of datasets. These results suggest that feed-forward transformers can serve as a scalable model for robust human reconstruction in real-world settings. Code and models will be released.</li>
</ul>

<h3>Title: Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26625">https://arxiv.org/abs/2509.26625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26625">https://arxiv.org/pdf/2509.26625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26625]] Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training(https://arxiv.org/abs/2509.26625)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs.</li>
</ul>

<h3>Title: Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R. Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, Nikolay Malkin, Moksh Jain</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26626">https://arxiv.org/abs/2509.26626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26626">https://arxiv.org/pdf/2509.26626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26626]] Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models(https://arxiv.org/abs/2509.26626)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. RSA exploits the rich information embedded in the reasoning chains -- not just the final answers -- and enables bootstrapping from partially correct intermediate steps within different chains of thought. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), while outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the model to combine solutions via a novel aggregation-aware reinforcement learning approach yields significant performance gains. Code available at this https URL.</li>
</ul>

<h3>Title: Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Runze Liu, Jiakang Wang, Yuling Shi, Zhihui Xie, Chenxin An, Kaiyan Zhang, Jian Zhao, Xiaodong Gu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26628">https://arxiv.org/abs/2509.26628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26628">https://arxiv.org/pdf/2509.26628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26628]] Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models(https://arxiv.org/abs/2509.26628)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has shown remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL (PSRL) has emerged as a more effective paradigm compared to outcome-based RL. However, existing PSRL approaches suffer from limited exploration efficiency, both in terms of branching positions and sampling. In this paper, we introduce a novel PSRL framework (AttnRL), which enables efficient exploration for reasoning models. Motivated by preliminary observations that steps exhibiting high attention scores correlate with reasoning behaviors, we propose to branch from positions with high values. Furthermore, we develop an adaptive sampling strategy that accounts for problem difficulty and historical batch size, ensuring that the whole training batch maintains non-zero advantage values. To further improve sampling efficiency, we design a one-step off-policy training pipeline for PSRL. Extensive experiments on multiple challenging mathematical reasoning benchmarks demonstrate that our method consistently outperforms prior approaches in terms of performance and sampling and training efficiency.</li>
</ul>

<h3>Title: Learning Generalizable Shape Completion with SIM(3) Equivariance</h3>
<ul>
<li><strong>Authors: </strong>Yuqing Wang, Zhaiyu Chen, Xiao Xiang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26631">https://arxiv.org/abs/2509.26631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26631">https://arxiv.org/pdf/2509.26631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26631]] Learning Generalizable Shape Completion with SIM(3) Equivariance(https://arxiv.org/abs/2509.26631)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>3D shape completion methods typically assume scans are pre-aligned to a canonical frame. This leaks pose and scale cues that networks may exploit to memorize absolute positions rather than inferring intrinsic geometry. When such alignment is absent in real data, performance collapses. We argue that robust generalization demands architectural equivariance to the similarity group, SIM(3), so the model remains agnostic to pose and scale. Following this principle, we introduce the first SIM(3)-equivariant shape completion network, whose modular layers successively canonicalize features, reason over similarity-invariant geometry, and restore the original frame. Under a de-biased evaluation protocol that removes the hidden cues, our model outperforms both equivariant and augmentation baselines on the PCN benchmark. It also sets new cross-domain records on real driving and indoor scans, lowering minimal matching distance on KITTI by 17% and Chamfer distance $\ell1$ on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol still outperforms competitors under their biased settings. These results establish full SIM(3) equivariance as an effective route to truly generalizable shape completion. Project page: this https URL.</li>
</ul>

<h3>Title: Scaling Spoken Language Models with Syllabic Speech Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Nicholas Lee, Cheol Jun Cho, Alan W Black, Gopala K. Anumanchipalli</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26634">https://arxiv.org/abs/2509.26634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26634">https://arxiv.org/pdf/2509.26634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26634]] Scaling Spoken Language Models with Syllabic Speech Tokenization(https://arxiv.org/abs/2509.26634)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Spoken language models (SLMs) typically discretize speech into high-frame-rate tokens extracted from SSL speech models. As the most successful LMs are based on the Transformer architecture, processing these long token streams with self-attention is expensive, as attention scales quadratically with sequence length. A recent SSL work introduces acoustic tokenization of speech at the syllable level, which is more interpretable and potentially more scalable with significant compression in token lengths (4-5 Hz). Yet, their value for spoken language modeling is not yet fully explored. We present the first systematic study of syllabic tokenization for spoken language modeling, evaluating models on a suite of SLU benchmarks while varying training data scale. Syllabic tokens can match or surpass the previous high-frame rate tokens while significantly cutting training and inference costs, achieving more than a 2x reduction in training time and a 5x reduction in FLOPs. Our findings highlight syllable-level language modeling as a promising path to efficient long-context spoken language models.</li>
</ul>

<h3>Title: AccidentBench: Benchmarking Multimodal Understanding and Reasoning in Vehicle Accidents and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Shangding Gu, Xiaohan Wang, Donghao Ying, Haoyu Zhao, Runing Yang, Ming Jin, Boyi Li, Marco Pavone, Serena Yeung-Levy, Jun Wang, Dawn Song, Costas Spanos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26636">https://arxiv.org/abs/2509.26636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26636">https://arxiv.org/pdf/2509.26636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26636]] AccidentBench: Benchmarking Multimodal Understanding and Reasoning in Vehicle Accidents and Beyond(https://arxiv.org/abs/2509.26636)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Rapid advances in multimodal models demand benchmarks that rigorously evaluate understanding and reasoning in safety-critical, dynamic real-world settings. We present AccidentBench, a large-scale benchmark that combines vehicle accident scenarios with Beyond domains, safety-critical settings in air and water that emphasize spatial and temporal reasoning (e.g., navigation, orientation, multi-vehicle motion). The benchmark contains approximately 2000 videos and over 19000 human-annotated question--answer pairs spanning multiple video lengths (short/medium/long) and difficulty levels (easy/medium/hard). Tasks systematically probe core capabilities: temporal, spatial, and intent understanding and reasoning. By unifying accident-centric traffic scenes with broader safety-critical scenarios in air and water, AccidentBench offers a comprehensive, physically grounded testbed for evaluating models under real-world variability. Evaluations of state-of-the-art models (e.g., Gemini-2.5 Pro and GPT-5) show that even the strongest models achieve only about 18% accuracy on the hardest tasks and longest videos, revealing substantial gaps in real-world temporal, spatial, and intent reasoning. AccidentBench is designed to expose these critical gaps and drive the development of multimodal models that are safer, more robust, and better aligned with real-world safety-critical challenges. The code and dataset are available at: this https URL</li>
</ul>

<h3>Title: Benchmarking Egocentric Visual-Inertial SLAM at City Scale</h3>
<ul>
<li><strong>Authors: </strong>Anusha Krishnan, Shaohui Liu, Paul-Edouard Sarlin, Oscar Gentilhomme, David Caruso, Maurizio Monge, Richard Newcombe, Jakob Engel, Marc Pollefeys</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26639">https://arxiv.org/abs/2509.26639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26639">https://arxiv.org/pdf/2509.26639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26639]] Benchmarking Egocentric Visual-Inertial SLAM at City Scale(https://arxiv.org/abs/2509.26639)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard sensors is critical for wearable devices capturing egocentric data, which exhibits specific challenges, such as a wider diversity of motions and viewpoints, prevalent dynamic visual content, or long sessions affected by time-varying sensor calibration. While recent progress on SLAM has been swift, academic research is still driven by benchmarks that do not reflect these challenges or do not offer sufficiently accurate ground truth poses. In this paper, we introduce a new dataset and benchmark for visual-inertial SLAM with egocentric, multi-modal data. We record hours and kilometers of trajectories through a city center with glasses-like devices equipped with various sensors. We leverage surveying tools to obtain control points as indirect pose annotations that are metric, centimeter-accurate, and available at city scale. This makes it possible to evaluate extreme trajectories that involve walking at night or traveling in a vehicle. We show that state-of-the-art systems developed by academia are not robust to these challenges and we identify components that are responsible for this. In addition, we design tracks with different levels of difficulty to ease in-depth analysis and evaluation of less mature approaches. The dataset and benchmark are available at this https URL.</li>
</ul>

<h3>Title: SPATA: Systematic Pattern Analysis for Detailed and Transparent Data Cards</h3>
<ul>
<li><strong>Authors: </strong>João Vitorino, Eva Maia, Isabel Praça, Carlos Soares</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26640">https://arxiv.org/abs/2509.26640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26640">https://arxiv.org/pdf/2509.26640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26640]] SPATA: Systematic Pattern Analysis for Detailed and Transparent Data Cards(https://arxiv.org/abs/2509.26640)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Due to the susceptibility of Artificial Intelligence (AI) to data perturbations and adversarial examples, it is crucial to perform a thorough robustness evaluation before any Machine Learning (ML) model is deployed. However, examining a model's decision boundaries and identifying potential vulnerabilities typically requires access to the training and testing datasets, which may pose risks to data privacy and confidentiality. To improve transparency in organizations that handle confidential data or manage critical infrastructure, it is essential to allow external verification and validation of AI without the disclosure of private datasets. This paper presents Systematic Pattern Analysis (SPATA), a deterministic method that converts any tabular dataset to a domain-independent representation of its statistical patterns, to provide more detailed and transparent data cards. SPATA computes the projection of each data instance into a discrete space where they can be analyzed and compared, without risking data leakage. These projected datasets can be reliably used for the evaluation of how different features affect ML model robustness and for the generation of interpretable explanations of their behavior, contributing to more trustworthy AI.</li>
</ul>

<h3>Title: Query-Kontext: An Unified Multimodal Model for Image Generation and Editing</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Song, Wenkai Dong, Shizun Wang, Qi Zhang, Song Xue, Tao Yuan, Hu Yang, Haocheng Feng, Hang Zhou, Xinyan Xiao, Jingdong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26641">https://arxiv.org/abs/2509.26641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26641">https://arxiv.org/pdf/2509.26641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26641]] Query-Kontext: An Unified Multimodal Model for Image Generation and Editing(https://arxiv.org/abs/2509.26641)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Unified Multimodal Models (UMMs) have demonstrated remarkable performance in text-to-image generation (T2I) and editing (TI2I), whether instantiated as assembled unified frameworks which couple powerful vision-language model (VLM) with diffusion-based generator, or as naive Unified Multimodal Models with an early fusion of understanding and generation modalities. We contend that in current unified frameworks, the crucial capability of multimodal generative reasoning which encompasses instruction understanding, grounding, and image referring for identity preservation and faithful reconstruction, is intrinsically entangled with high-fidelity synthesis. In this work, we introduce Query-Kontext, a novel approach that bridges the VLM and diffusion model via a multimodal ``kontext'' composed of semantic cues and coarse-grained image conditions encoded from multimodal inputs. This design delegates the complex ability of multimodal generative reasoning to powerful VLM while reserving diffusion model's role for high-quality visual synthesis. To achieve this, we propose a three-stage progressive training strategy. First, we connect the VLM to a lightweight diffusion head via multimodal kontext tokens to unleash the VLM's generative reasoning ability. Second, we scale this head to a large, pre-trained diffusion model to enhance visual detail and realism. Finally, we introduce a low-level image encoder to improve image fidelity and perform instruction tuning on downstream tasks. Furthermore, we build a comprehensive data pipeline integrating real, synthetic, and open-source datasets, covering diverse multimodal reference-to-image scenarios, including image generation, instruction-driven editing, customized generation, and multi-subject composition. Experiments show that our approach matches strong unified baselines and even outperforms task-specific state-of-the-art methods in several cases.</li>
</ul>

<h3>Title: Stitch: Training-Free Position Control in Multimodal Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jessica Bader, Mateusz Pach, Maria A. Bravo, Serge Belongie, Zeynep Akata</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26644">https://arxiv.org/abs/2509.26644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26644">https://arxiv.org/pdf/2509.26644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26644]] Stitch: Training-Free Position Control in Multimodal Diffusion Transformers(https://arxiv.org/abs/2509.26644)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Text-to-Image (T2I) generation models have advanced rapidly in recent years, but accurately capturing spatial relationships like "above" or "to the right of" poses a persistent challenge. Earlier methods improved spatial relationship following with external position control. However, as architectures evolved to enhance image quality, these techniques became incompatible with modern models. We propose Stitch, a training-free method for incorporating external position control into Multi-Modal Diffusion Transformers (MMDiT) via automatically-generated bounding boxes. Stitch produces images that are both spatially accurate and visually appealing by generating individual objects within designated bounding boxes and seamlessly stitching them together. We find that targeted attention heads capture the information necessary to isolate and cut out individual objects mid-generation, without needing to fully complete the image. We evaluate Stitch on PosEval, our benchmark for position-based T2I generation. Featuring five new tasks that extend the concept of Position beyond the basic GenEval task, PosEval demonstrates that even top models still have significant room for improvement in position-based generation. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances base models, even improving FLUX by 218% on GenEval's Position task and by 206% on PosEval. Stitch achieves state-of-the-art results with Qwen-Image on PosEval, improving over previous models by 54%, all accomplished while integrating position control into leading models training-free. Code is available at this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
