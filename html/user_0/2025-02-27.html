<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-02-27</h1>
<h3>Title: MixLLM: Dynamic Routing in Mixed Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinyuan Wang, Yanchi Liu, Wei Cheng, Xujiang Zhao, Zhengzhang Chen, Wenchao Yu, Yanjie Fu, Haifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18482">https://arxiv.org/abs/2502.18482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18482">https://arxiv.org/pdf/2502.18482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18482]] MixLLM: Dynamic Routing in Mixed Large Language Models(https://arxiv.org/abs/2502.18482)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit potential artificial generic intelligence recently, however, their usage is costly with high response latency. Given mixed LLMs with their own strengths and weaknesses, LLM routing aims to identify the most suitable model for each query in the stream to maximize response quality and minimize cost and latency. However, the challenges involve: (1) dynamic trade-offs among quality, cost, and latency; (2) enabling continual learning in deployed systems; and (3) navigating a varying (e.g., new LLM addition or old LLM removal) set of LLM candidates over time. To bridge these gaps, we develop MixLLM, a dynamic contextual-bandit-based routing system for query-LLM assignment. Specifically, we first leverage query tags to enhance query embeddings for the routing task. Next, we design lightweight prediction models to estimate the response qualities and costs of queries over LLMs. We then devise a meta-decision maker to choose the query-LLM assignments to best tradeoff response quality, cost, and latency. Finally, the system benefits from continual training, allowing it to adapt to evolving queries and user feedback over time. Our extensive experiments show that MixLLM achieves the best trade-offs in response quality, cost, and latency (97.25% of GPT-4's quality at 24.18% of the cost under the time constraint).</li>
</ul>

<h3>Title: Deep Learning-based Dual Watermarking for Image Copyright Protection and Authentication</h3>
<ul>
<li><strong>Authors: </strong>Sudev Kumar Padhi, Archana Tiwari, Sk. Subidh Ali</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18501">https://arxiv.org/abs/2502.18501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18501">https://arxiv.org/pdf/2502.18501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18501]] Deep Learning-based Dual Watermarking for Image Copyright Protection and Authentication(https://arxiv.org/abs/2502.18501)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust, extraction, watermark</a></li>
<li><strong>Abstract: </strong>Advancements in digital technologies make it easy to modify the content of digital images. Hence, ensuring digital images integrity and authenticity is necessary to protect them against various attacks that manipulate them. We present a Deep Learning (DL) based dual invisible watermarking technique for performing source authentication, content authentication, and protecting digital content copyright of images sent over the internet. Beyond securing images, the proposed technique demonstrates robustness to content-preserving image manipulations. It is also impossible to imitate or overwrite watermarks because the cryptographic hash of the image and the dominant features of the image in the form of perceptual hash are used as watermarks. We highlighted the need for source authentication to safeguard image integrity and authenticity, along with identifying similar content for copyright protection. After exhaustive testing, we obtained a high peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM), which implies there is a minute change in the original image after embedding our watermarks. Our trained model achieves high watermark extraction accuracy and to the best of our knowledge, this is the first deep learning-based dual watermarking technique proposed in the literature.</li>
</ul>

<h3>Title: TurboFuzzLLM: Turbocharging Mutation-based Fuzzing for Effectively Jailbreaking Large Language Models in Practice</h3>
<ul>
<li><strong>Authors: </strong>Aman Goel, Xian Carrie Wu, Zhe Wang, Dmitriy Bespalov, Yanjun Qi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18504">https://arxiv.org/abs/2502.18504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18504">https://arxiv.org/pdf/2502.18504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18504]] TurboFuzzLLM: Turbocharging Mutation-based Fuzzing for Effectively Jailbreaking Large Language Models in Practice(https://arxiv.org/abs/2502.18504)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Jailbreaking large-language models (LLMs) involves testing their robustness against adversarial prompts and evaluating their ability to withstand prompt attacks that could elicit unauthorized or malicious responses. In this paper, we present TurboFuzzLLM, a mutation-based fuzzing technique for efficiently finding a collection of effective jailbreaking templates that, when combined with harmful questions, can lead a target LLM to produce harmful responses through black-box access via user prompts. We describe the limitations of directly applying existing template-based attacking techniques in practice, and present functional and efficiency-focused upgrades we added to mutation-based fuzzing to generate effective jailbreaking templates automatically. TurboFuzzLLM achieves $\geq$ 95\% attack success rates (ASR) on public datasets for leading LLMs (including GPT-4o \& GPT-4 Turbo), shows impressive generalizability to unseen harmful questions, and helps in improving model defenses to prompt attacks.</li>
</ul>

<h3>Title: REFINE: Inversion-Free Backdoor Defense via Model Reprogramming</h3>
<ul>
<li><strong>Authors: </strong>Yukun Chen, Shuo Shao, Enhao Huang, Yiming Li, Pin-Yu Chen, Zhan Qin, Kui Ren</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18508">https://arxiv.org/abs/2502.18508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18508">https://arxiv.org/pdf/2502.18508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18508]] REFINE: Inversion-Free Backdoor Defense via Model Reprogramming(https://arxiv.org/abs/2502.18508)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Backdoor attacks on deep neural networks (DNNs) have emerged as a significant security threat, allowing adversaries to implant hidden malicious behaviors during the model training phase. Pre-processing-based defense, which is one of the most important defense paradigms, typically focuses on input transformations or backdoor trigger inversion (BTI) to deactivate or eliminate embedded backdoor triggers during the inference process. However, these methods suffer from inherent limitations: transformation-based defenses often fail to balance model utility and defense performance, while BTI-based defenses struggle to accurately reconstruct trigger patterns without prior knowledge. In this paper, we propose REFINE, an inversion-free backdoor defense method based on model reprogramming. REFINE consists of two key components: \textbf{(1)} an input transformation module that disrupts both benign and backdoor patterns, generating new benign features; and \textbf{(2)} an output remapping module that redefines the model's output domain to guide the input transformations effectively. By further integrating supervised contrastive loss, REFINE enhances the defense capabilities while maintaining model utility. Extensive experiments on various benchmark datasets demonstrate the effectiveness of our REFINE and its resistance to potential adaptive attacks.</li>
</ul>

<h3>Title: Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents</h3>
<ul>
<li><strong>Authors: </strong>Ivoline Ngong, Swanand Kadhe, Hao Wang, Keerthiram Murugesan, Justin D. Weisz, Amit Dhurandhar, Karthikeyan Natesan Ramamurthy</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18509">https://arxiv.org/abs/2502.18509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18509">https://arxiv.org/pdf/2502.18509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18509]] Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents(https://arxiv.org/abs/2502.18509)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Conversational agents are increasingly woven into individuals' personal lives, yet users often underestimate the privacy risks involved. The moment users share information with these agents (e.g., LLMs), their private information becomes vulnerable to exposure. In this paper, we characterize the notion of contextual privacy for user interactions with LLMs. It aims to minimize privacy risks by ensuring that users (sender) disclose only information that is both relevant and necessary for achieving their intended goals when interacting with LLMs (untrusted receivers). Through a formative design user study, we observe how even "privacy-conscious" users inadvertently reveal sensitive information through indirect disclosures. Based on insights from this study, we propose a locally-deployable framework that operates between users and LLMs, and identifies and reformulates out-of-context information in user prompts. Our evaluation using examples from ShareGPT shows that lightweight models can effectively implement this framework, achieving strong gains in contextual privacy while preserving the user's intended interaction goals through different approaches to classify information relevant to the intended goals.</li>
</ul>

<h3>Title: Multi-Teacher Knowledge Distillation with Reinforcement Learning for Visual Recognition</h3>
<ul>
<li><strong>Authors: </strong>Chuanguang Yang, Xinqiang Yu, Han Yang, Zhulin An, Chengqing Yu, Libo Huang, Yongjun Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18510">https://arxiv.org/abs/2502.18510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18510">https://arxiv.org/pdf/2502.18510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18510]] Multi-Teacher Knowledge Distillation with Reinforcement Learning for Visual Recognition(https://arxiv.org/abs/2502.18510)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Multi-teacher Knowledge Distillation (KD) transfers diverse knowledge from a teacher pool to a student network. The core problem of multi-teacher KD is how to balance distillation strengths among various teachers. Most existing methods often develop weighting strategies from an individual perspective of teacher performance or teacher-student gaps, lacking comprehensive information for guidance. This paper proposes Multi-Teacher Knowledge Distillation with Reinforcement Learning (MTKD-RL) to optimize multi-teacher weights. In this framework, we construct both teacher performance and teacher-student gaps as state information to an agent. The agent outputs the teacher weight and can be updated by the return reward from the student. MTKD-RL reinforces the interaction between the student and teacher using an agent in an RL-based decision mechanism, achieving better matching capability with more meaningful weights. Experimental results on visual recognition tasks, including image classification, object detection, and semantic segmentation tasks, demonstrate that MTKD-RL achieves state-of-the-art performance compared to the existing multi-teacher KD works.</li>
</ul>

<h3>Title: ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuxu Liu, Siyuan Liang, Mengya Han, Yong Luo, Aishan Liu, Xiantao Cai, Zheng He, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18511">https://arxiv.org/abs/2502.18511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18511">https://arxiv.org/pdf/2502.18511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18511]] ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models(https://arxiv.org/abs/2502.18511)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative large language models are crucial in natural language processing, but they are vulnerable to backdoor attacks, where subtle triggers compromise their behavior. Although backdoor attacks against LLMs are constantly emerging, existing benchmarks remain limited in terms of sufficient coverage of attack, metric system integrity, backdoor attack alignment. And existing pre-trained backdoor attacks are idealized in practice due to resource access constraints. Therefore we establish $\textit{ELBA-Bench}$, a comprehensive and unified framework that allows attackers to inject backdoor through parameter efficient fine-tuning ($\textit{e.g.,}$ LoRA) or without fine-tuning techniques ($\textit{e.g.,}$ In-context-learning). $\textit{ELBA-Bench}$ provides over 1300 experiments encompassing the implementations of 12 attack methods, 18 datasets, and 12 LLMs. Extensive experiments provide new invaluable findings into the strengths and limitations of various attack strategies. For instance, PEFT attack consistently outperform without fine-tuning approaches in classification tasks while showing strong cross-dataset generalization with optimized triggers boosting robustness; Task-relevant backdoor optimization techniques or attack prompts along with clean and adversarial demonstrations can enhance backdoor attack success while preserving model performance on clean samples. Additionally, we introduce a universal toolbox designed for standardized backdoor attack research, with the goal of propelling further progress in this vital area.</li>
</ul>

<h3>Title: FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression</h3>
<ul>
<li><strong>Authors: </strong>Jianjian Li, Junquan Fan, Feng Tang, Gang Huang, Shitao Zhu, Songlin Liu, Nian Xie, Wulong Liu, Yong Liao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18512">https://arxiv.org/abs/2502.18512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18512">https://arxiv.org/pdf/2502.18512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18512]] FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression(https://arxiv.org/abs/2502.18512)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid success of Vision Large Language Models (VLLMs) often depends on the high-resolution images with abundant visual tokens, which hinders training and deployment efficiency. Current training-free visual token compression methods exhibit serious performance degradation in tasks involving high-resolution, text-oriented image understanding and reasoning. In this paper, we propose an efficient visual token compression framework for text-oriented VLLMs in high-resolution scenarios. In particular, we employ a light-weight self-distillation pre-training stage to compress the visual tokens, requiring a limited numbers of image-text pairs and minimal learnable parameters. Afterwards, to mitigate potential performance degradation of token-compressed models, we construct a high-quality post-train stage. To validate the effectiveness of our method, we apply it to an advanced VLLMs, InternVL2. Experimental results show that our approach significantly reduces computational overhead while outperforming the baselines across a range of text-oriented benchmarks. We will release the models and code soon.</li>
</ul>

<h3>Title: CipherFace: A Fully Homomorphic Encryption-Driven Framework for Secure Cloud-Based Facial Recognition</h3>
<ul>
<li><strong>Authors: </strong>Sefik Serengil, Alper Ozpinar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18514">https://arxiv.org/abs/2502.18514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18514">https://arxiv.org/pdf/2502.18514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18514]] CipherFace: A Fully Homomorphic Encryption-Driven Framework for Secure Cloud-Based Facial Recognition(https://arxiv.org/abs/2502.18514)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, robust</a></li>
<li><strong>Abstract: </strong>Facial recognition systems rely on embeddings to represent facial images and determine identity by verifying if the distance between embeddings is below a pre-tuned threshold. While embeddings are not reversible to original images, they still contain sensitive information, making their security critical. Traditional encryption methods like AES are limited in securely utilizing cloud computational power for distance calculations. Homomorphic Encryption, allowing calculations on encrypted data, offers a robust alternative. This paper introduces CipherFace, a homomorphic encryption-driven framework for secure cloud-based facial recognition, which we have open-sourced at this http URL. By leveraging FHE, CipherFace ensures the privacy of embeddings while utilizing the cloud for efficient distance computation. Furthermore, we propose a novel encrypted distance computation method for both Euclidean and Cosine distances, addressing key challenges in performing secure similarity calculations on encrypted data. We also conducted experiments with different facial recognition models, various embedding sizes, and cryptosystem configurations, demonstrating the scalability and effectiveness of CipherFace in real-world applications.</li>
</ul>

<h3>Title: A Multi-Agent Framework for Automated Vulnerability Detection and Repair in Solidity and Move Smart Contracts</h3>
<ul>
<li><strong>Authors: </strong>Rabimba Karanjai, Sam Blackshear, Lei Xu, Weidong Shi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.MA, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18515">https://arxiv.org/abs/2502.18515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18515">https://arxiv.org/pdf/2502.18515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18515]] A Multi-Agent Framework for Automated Vulnerability Detection and Repair in Solidity and Move Smart Contracts(https://arxiv.org/abs/2502.18515)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid growth of the blockchain ecosystem and the increasing value locked in smart contracts necessitate robust security measures. While languages like Solidity and Move aim to improve smart contract security, vulnerabilities persist. This paper presents Smartify, a novel multi-agent framework leveraging Large Language Models (LLMs) to automatically detect and repair vulnerabilities in Solidity and Move smart contracts. Unlike traditional methods that rely solely on vast pre-training datasets, Smartify employs a team of specialized agents working on different specially fine-tuned LLMs to analyze code based on underlying programming concepts and language-specific security principles. We evaluated Smartify on a dataset for Solidity and a curated dataset for Move, demonstrating its effectiveness in fixing a wide range of vulnerabilities. Our results show that Smartify (Gemma2+codegemma) achieves state-of-the-art performance, surpassing existing LLMs and enhancing general-purpose models' capabilities, such as Llama 3.1. Notably, Smartify can incorporate language-specific knowledge, such as the nuances of Move, without requiring massive language-specific pre-training datasets. This work offers a detailed analysis of various LLMs' performance on smart contract repair, highlighting the strengths of our multi-agent approach and providing a blueprint for developing more secure and reliable decentralized applications in the growing blockchain landscape. We also provide a detailed recipe for extending this to other similar use cases.</li>
</ul>

<h3>Title: RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via Reward Driven Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Jianwei Wang, Junyao Yang, Haoran Li, Huiping Zhuang, Cen Chen, Ziqian Zeng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18517">https://arxiv.org/abs/2502.18517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18517">https://arxiv.org/pdf/2502.18517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18517]] RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via Reward Driven Data Synthesis(https://arxiv.org/abs/2502.18517)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>The success of large language models (LLMs) has attracted many individuals to fine-tune them for domain-specific tasks by uploading their data. However, in sensitive areas like healthcare and finance, privacy concerns often arise. One promising solution is to sample synthetic data with Differential Privacy (DP) guarantees to replace private data. However, these synthetic data contain significant flawed data, which are considered as noise. Existing solutions typically rely on naive filtering by comparing ROUGE-L scores or embedding similarities, which are ineffective in addressing the noise. To address this issue, we propose RewardDS, a novel privacy-preserving framework that fine-tunes a reward proxy model and uses reward signals to guide the synthetic data generation. Our RewardDS introduces two key modules, Reward Guided Filtering and Self-Optimizing Refinement, to both filter and refine the synthetic data, effectively mitigating the noise. Extensive experiments across medical, financial, and code generation domains demonstrate the effectiveness of our method.</li>
</ul>

<h3>Title: Swallowing the Poison Pills: Insights from Vulnerability Disparity Among LLMs</h3>
<ul>
<li><strong>Authors: </strong>Peng Yifeng, Wu Zhizheng, Chen Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18518">https://arxiv.org/abs/2502.18518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18518">https://arxiv.org/pdf/2502.18518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18518]] Swallowing the Poison Pills: Insights from Vulnerability Disparity Among LLMs(https://arxiv.org/abs/2502.18518)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Modern large language models (LLMs) exhibit critical vulnerabilities to poison pill attacks: localized data poisoning that alters specific factual knowledge while preserving overall model utility. We systematically demonstrate these attacks exploit inherent architectural properties of LLMs, achieving 54.6% increased retrieval inaccuracy on long-tail knowledge versus dominant topics and up to 25.5% increase retrieval inaccuracy on compressed models versus original architectures. Through controlled mutations (e.g., temporal/spatial/entity alterations) and, our method induces localized memorization deterioration with negligible impact on models' performance on regular standard benchmarks (e.g., <2% performance drop on MMLU/GPQA), leading to potential detection evasion. Our findings suggest: (1) Disproportionate vulnerability in long-tail knowledge may result from reduced parameter redundancy; (2) Model compression may increase attack surfaces, with pruned/distilled models requiring 30% fewer poison samples for equivalent damage; (3) Associative memory enables both spread of collateral damage to related concepts and amplification of damage from simultaneous attack, particularly for dominant topics. These findings raise concerns over current scaling paradigms since attack costs are lowering while defense complexity is rising. Our work establishes poison pills as both a security threat and diagnostic tool, revealing critical security-efficiency trade-offs in language model compression that challenges prevailing safety assumptions.</li>
</ul>

<h3>Title: Class-Conditional Neural Polarizer: A Lightweight and Effective Backdoor Defense by Purifying Poisoned Features</h3>
<ul>
<li><strong>Authors: </strong>Mingli Zhu, Shaokui Wei, Hongyuan Zha, Baoyuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18520">https://arxiv.org/abs/2502.18520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18520">https://arxiv.org/pdf/2502.18520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18520]] Class-Conditional Neural Polarizer: A Lightweight and Effective Backdoor Defense by Purifying Poisoned Features(https://arxiv.org/abs/2502.18520)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Recent studies have highlighted the vulnerability of deep neural networks to backdoor attacks, where models are manipulated to rely on embedded triggers within poisoned samples, despite the presence of both benign and trigger information. While several defense methods have been proposed, they often struggle to balance backdoor mitigation with maintaining benign this http URL this work, inspired by the concept of optical polarizer-which allows light waves of specific polarizations to pass while filtering others-we propose a lightweight backdoor defense approach, NPD. This method integrates a neural polarizer (NP) as an intermediate layer within the compromised model, implemented as a lightweight linear transformation optimized via bi-level optimization. The learnable NP filters trigger information from poisoned samples while preserving benign content. Despite its effectiveness, we identify through empirical studies that NPD's performance degrades when the target labels (required for purification) are inaccurately estimated. To address this limitation while harnessing the potential of targeted adversarial mitigation, we propose class-conditional neural polarizer-based defense (CNPD). The key innovation is a fusion module that integrates the backdoored model's predicted label with the features to be purified. This architecture inherently mimics targeted adversarial defense mechanisms without requiring label estimation used in NPD. We propose three implementations of CNPD: the first is r-CNPD, which trains a replicated NP layer for each class and, during inference, selects the appropriate NP layer for defense based on the predicted class from the backdoored model. To efficiently handle a large number of classes, two variants are designed: e-CNPD, which embeds class information as additional features, and a-CNPD, which directs network attention using class information.</li>
</ul>

<h3>Title: GOD model: Privacy Preserved AI School for Personal Assistant</h3>
<ul>
<li><strong>Authors: </strong>PIN AI Team: Bill Qingyun Sun, Laura Florescu, Boliang Zhang, Regan Peng, Smile Hu, Shouqiao Wang, Ben Wu, Xi Wang, Davide Crapis, Gavin Zhen Guo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18527">https://arxiv.org/abs/2502.18527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18527">https://arxiv.org/pdf/2502.18527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18527]] GOD model: Privacy Preserved AI School for Personal Assistant(https://arxiv.org/abs/2502.18527)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect</a></li>
<li><strong>Abstract: </strong>Personal AI assistants (e.g., Apple Intelligence, Meta AI) offer proactive recommendations that simplify everyday tasks, but their reliance on sensitive user data raises concerns about privacy and trust. To address these challenges, we introduce the Guardian of Data (GOD), a secure, privacy-preserving framework for training and evaluating AI assistants directly on-device. Unlike traditional benchmarks, the GOD model measures how well assistants can anticipate user needs-such as suggesting gifts-while protecting user data and autonomy. Functioning like an AI school, it addresses the cold start problem by simulating user queries and employing a curriculum-based approach to refine the performance of each assistant. Running within a Trusted Execution Environment (TEE), it safeguards user data while applying reinforcement and imitation learning to refine AI recommendations. A token-based incentive system encourages users to share data securely, creating a data flywheel that drives continuous improvement. By integrating privacy, personalization, and trust, the GOD model provides a scalable, responsible path for advancing personal AI assistants. For community collaboration, part of the framework is open-sourced at this https URL.</li>
</ul>

<h3>Title: IMPROVE: Iterative Model Pipeline Refinement and Optimization Leveraging LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Eric Xue, Zeyi Huang, Yuyang Ji, Haohan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18530">https://arxiv.org/abs/2502.18530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18530">https://arxiv.org/pdf/2502.18530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18530]] IMPROVE: Iterative Model Pipeline Refinement and Optimization Leveraging LLM Agents(https://arxiv.org/abs/2502.18530)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Computer vision is a critical component in a wide range of real-world applications, including plant monitoring in agriculture and handwriting classification in digital systems. However, developing high-performance computer vision models traditionally demands both machine learning (ML) expertise and domain-specific knowledge, making the process costly, labor-intensive, and inaccessible to many. Large language model (LLM) agents have emerged as a promising solution to automate this workflow, but most existing methods share a common limitation: they attempt to optimize entire pipelines in a single step before evaluation, making it difficult to attribute improvements to specific changes. This lack of granularity leads to unstable optimization and slower convergence, limiting their effectiveness. To address this, we introduce Iterative Refinement, a novel strategy for LLM-driven ML pipeline design inspired by how human ML experts iteratively refine models, focusing on one component at a time rather than making sweeping changes all at once. By systematically updating individual components based on real training feedback, Iterative Refinement improves stability, interpretability, and overall model performance. We implement this strategy in IMPROVE, an end-to-end LLM agent framework for automating and optimizing object classification pipelines. Through extensive evaluations across datasets of varying sizes and domains, including standard benchmarks and Kaggle competition datasets, we demonstrate that Iterative Refinement enables IMPROVE to consistently achieve better performance over existing zero-shot LLM-based approaches. These findings establish Iterative Refinement as an effective new strategy for LLM-driven ML automation and position IMPROVE as an accessible solution for building high-quality computer vision models without requiring ML expertise.</li>
</ul>

<h3>Title: A Survey of Zero-Knowledge Proof Based Verifiable Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhizhi Peng, Taotao Wang, Chonghe Zhao, Guofu Liao, Zibin Lin, Yifeng Liu, Bin Cao, Long Shi, Qing Yang, Shengli Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18535">https://arxiv.org/abs/2502.18535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18535">https://arxiv.org/pdf/2502.18535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18535]] A Survey of Zero-Knowledge Proof Based Verifiable Machine Learning(https://arxiv.org/abs/2502.18535)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>As machine learning technologies advance rapidly across various domains, concerns over data privacy and model security have grown significantly. These challenges are particularly pronounced when models are trained and deployed on cloud platforms or third-party servers due to the computational resource limitations of users' end devices. In response, zero-knowledge proof (ZKP) technology has emerged as a promising solution, enabling effective validation of model performance and authenticity in both training and inference processes without disclosing sensitive data. Thus, ZKP ensures the verifiability and security of machine learning models, making it a valuable tool for privacy-preserving AI. Although some research has explored the verifiable machine learning solutions that exploit ZKP, a comprehensive survey and summary of these efforts remain absent. This survey paper aims to bridge this gap by reviewing and analyzing all the existing Zero-Knowledge Machine Learning (ZKML) research from June 2017 to December 2024. We begin by introducing the concept of ZKML and outlining its ZKP algorithmic setups under three key categories: verifiable training, verifiable inference, and verifiable testing. Next, we provide a comprehensive categorization of existing ZKML research within these categories and analyze the works in detail. Furthermore, we explore the implementation challenges faced in this field and discuss the improvement works to address these obstacles. Additionally, we highlight several commercial applications of ZKML technology. Finally, we propose promising directions for future advancements in this domain.</li>
</ul>

<h3>Title: FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA</h3>
<ul>
<li><strong>Authors: </strong>S M Sarwar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18536">https://arxiv.org/abs/2502.18536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18536">https://arxiv.org/pdf/2502.18536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18536]] FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA(https://arxiv.org/abs/2502.18536)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual Question Answering requires models to generate accurate answers by integrating visual and textual understanding. However, VQA models still struggle with hallucinations, producing convincing but incorrect answers, particularly in knowledge-driven and Out-of-Distribution scenarios. We introduce FilterRAG, a retrieval-augmented framework that combines BLIP-VQA with Retrieval-Augmented Generation to ground answers in external knowledge sources like Wikipedia and DBpedia. FilterRAG achieves 36.5% accuracy on the OK-VQA dataset, demonstrating its effectiveness in reducing hallucinations and improving robustness in both in-domain and Out-of-Distribution settings. These findings highlight the potential of FilterRAG to improve Visual Question Answering systems for real-world deployment.</li>
</ul>

<h3>Title: Revisiting Convolution Architecture in the Realm of DNA Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Yu Bo, Weian Mao, Yanjun Shao, Weiqiang Bai, Peng Ye, Xinzhu Ma, Junbo Zhao, Hao Chen, Chunhua Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18538">https://arxiv.org/abs/2502.18538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18538">https://arxiv.org/pdf/2502.18538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18538]] Revisiting Convolution Architecture in the Realm of DNA Foundation Models(https://arxiv.org/abs/2502.18538)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In recent years, a variety of methods based on Transformer and state space model (SSM) architectures have been proposed, advancing foundational DNA language models. However, there is a lack of comparison between these recent approaches and the classical architecture convolutional networks (CNNs) on foundation model benchmarks. This raises the question: are CNNs truly being surpassed by these recent approaches based on transformer and SSM architectures? In this paper, we develop a simple but well-designed CNN-based method termed ConvNova. ConvNova identifies and proposes three effective designs: 1) dilated convolutions, 2) gated convolutions, and 3) a dual-branch framework for gating mechanisms. Through extensive empirical experiments, we demonstrate that ConvNova significantly outperforms recent methods on more than half of the tasks across several foundation model benchmarks. For example, in histone-related tasks, ConvNova exceeds the second-best method by an average of 5.8%, while generally utilizing fewer parameters and enabling faster computation. In addition, the experiments observed findings that may be related to biological characteristics. This indicates that CNNs are still a strong competitor compared to Transformers and SSMs. We anticipate that this work will spark renewed interest in CNN-based methods for DNA foundation models.</li>
</ul>

<h3>Title: PII-Bench: Evaluating Query-Aware Privacy Protection Systems</h3>
<ul>
<li><strong>Authors: </strong>Hao Shen, Zhouhong Gu, Haokai Hong, Weili Han</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18545">https://arxiv.org/abs/2502.18545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18545">https://arxiv.org/pdf/2502.18545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18545]] PII-Bench: Evaluating Query-Aware Privacy Protection Systems(https://arxiv.org/abs/2502.18545)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>The widespread adoption of Large Language Models (LLMs) has raised significant privacy concerns regarding the exposure of personally identifiable information (PII) in user prompts. To address this challenge, we propose a query-unrelated PII masking strategy and introduce PII-Bench, the first comprehensive evaluation framework for assessing privacy protection systems. PII-Bench comprises 2,842 test samples across 55 fine-grained PII categories, featuring diverse scenarios from single-subject descriptions to complex multi-party interactions. Each sample is carefully crafted with a user query, context description, and standard answer indicating query-relevant PII. Our empirical evaluation reveals that while current models perform adequately in basic PII detection, they show significant limitations in determining PII query relevance. Even state-of-the-art LLMs struggle with this task, particularly in handling complex multi-subject scenarios, indicating substantial room for improvement in achieving intelligent PII masking.</li>
</ul>

<h3>Title: Steganography Beyond Space-Time With Chain of Multimodal AI Agents</h3>
<ul>
<li><strong>Authors: </strong>Ching-Chun Chang, Isao Echizen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.MA, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18547">https://arxiv.org/abs/2502.18547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18547">https://arxiv.org/pdf/2502.18547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18547]] Steganography Beyond Space-Time With Chain of Multimodal AI Agents(https://arxiv.org/abs/2502.18547)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, biometric</a></li>
<li><strong>Abstract: </strong>Steganography is the art and science of covert writing, with a broad range of applications interwoven within the realm of cybersecurity. As artificial intelligence continues to evolve, its ability to synthesise realistic content emerges as a threat in the hands of cybercriminals who seek to manipulate and misrepresent the truth. Such synthetic content introduces a non-trivial risk of overwriting the subtle changes made for the purpose of steganography. When the signals in both the spatial and temporal domains are vulnerable to unforeseen overwriting, it calls for reflection on what can remain invariant after all. This study proposes a paradigm in steganography for audiovisual media, where messages are concealed beyond both spatial and temporal domains. A chain of multimodal agents is developed to deconstruct audiovisual content into a cover text, embed a message within the linguistic domain, and then reconstruct the audiovisual content through synchronising both aural and visual modalities with the resultant stego text. The message is encoded by biasing the word sampling process of a language generation model and decoded by analysing the probability distribution of word choices. The accuracy of message transmission is evaluated under both zero-bit and multi-bit capacity settings. Fidelity is assessed through both biometric and semantic similarities, capturing the identities of the recorded face and voice, as well as the core ideas conveyed through the media. Secrecy is examined through statistical comparisons between cover and stego texts. Robustness is tested across various scenarios, including audiovisual compression, face-swapping, voice-cloning and their combinations.</li>
</ul>

<h3>Title: Target Defense with Multiple Defenders and an Agile Attacker via Residual Policy Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiyue Tao, Tongsheng Shen, Dexin Zhao, Feitian Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18549">https://arxiv.org/abs/2502.18549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18549">https://arxiv.org/pdf/2502.18549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18549]] Target Defense with Multiple Defenders and an Agile Attacker via Residual Policy Learning(https://arxiv.org/abs/2502.18549)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>The target defense problem involves intercepting an attacker before it reaches a designated target region using one or more defenders. This letter focuses on a particularly challenging scenario in which the attacker is more agile than the defenders, significantly increasing the difficulty of effective interception. To address this challenge, we propose a novel residual policy framework that integrates deep reinforcement learning (DRL) with the force-based Boids model. In this framework, the Boids model serves as a baseline policy, while DRL learns a residual policy to refine and optimize the defenders' actions. Simulation experiments demonstrate that the proposed method consistently outperforms traditional interception policies, whether learned via vanilla DRL or fine-tuned from force-based methods. Moreover, the learned policy exhibits strong scalability and adaptability, effectively handling scenarios with varying numbers of defenders and attackers with different agility levels.</li>
</ul>

<h3>Title: Application of Attention Mechanism with Bidirectional Long Short-Term Memory (BiLSTM) and CNN for Human Conflict Detection using Computer Vision</h3>
<ul>
<li><strong>Authors: </strong>Erick da Silva Farias, Eduardo Palhares Junior</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18555">https://arxiv.org/abs/2502.18555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18555">https://arxiv.org/pdf/2502.18555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18555]] Application of Attention Mechanism with Bidirectional Long Short-Term Memory (BiLSTM) and CNN for Human Conflict Detection using Computer Vision(https://arxiv.org/abs/2502.18555)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The automatic detection of human conflicts through videos is a crucial area in computer vision, with significant applications in monitoring and public safety policies. However, the scarcity of public datasets and the complexity of human interactions make this task challenging. This study investigates the integration of advanced deep learning techniques, including Attention Mechanism, Convolutional Neural Networks (CNNs), and Bidirectional Long ShortTerm Memory (BiLSTM), to improve the detection of violent behaviors in videos. The research explores how the use of the attention mechanism can help focus on the most relevant parts of the video, enhancing the accuracy and robustness of the model. The experiments indicate that the combination of CNNs with BiLSTM and the attention mechanism provides a promising solution for conflict monitoring, offering insights into the effectiveness of different strategies. This work opens new possibilities for the development of automated surveillance systems that can operate more efficiently in real-time detection of violent events.</li>
</ul>

<h3>Title: FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Radu Marinescu, Debarun Bhattacharjya, Junkyu Lee, Tigran Tchrakian, Javier Carnerero Cano, Yufang Hou, Elizabeth Daly, Alessandra Pascale</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18573">https://arxiv.org/abs/2502.18573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18573">https://arxiv.org/pdf/2502.18573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18573]] FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models(https://arxiv.org/abs/2502.18573)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated vast capabilities on generative tasks in recent years, yet they struggle with guaranteeing the factual correctness of the generated content. This makes these models unreliable in realistic situations where factually accurate responses are expected. In this paper, we propose FactReasoner, a new factuality assessor that relies on probabilistic reasoning to assess the factuality of a long-form generated response. Specifically, FactReasoner decomposes the response into atomic units, retrieves relevant contexts for them from an external knowledge source, and constructs a joint probability distribution over the atoms and contexts using probabilistic encodings of the logical relationships (entailment, contradiction) between the textual utterances corresponding to the atoms and contexts. FactReasoner then computes the posterior probability of whether atomic units in the response are supported by the retrieved contexts. Our experiments on labeled and unlabeled benchmark datasets demonstrate clearly that FactReasoner improves considerably over state-of-the-art prompt-based approaches in terms of both factual precision and recall.</li>
</ul>

<h3>Title: Differentially Private Iterative Screening Rules for Linear Regression</h3>
<ul>
<li><strong>Authors: </strong>Amol Khanna, Fred Lu, Edward Raff</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18578">https://arxiv.org/abs/2502.18578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18578">https://arxiv.org/pdf/2502.18578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18578]] Differentially Private Iterative Screening Rules for Linear Regression(https://arxiv.org/abs/2502.18578)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Linear $L_1$-regularized models have remained one of the simplest and most effective tools in data science. Over the past decade, screening rules have risen in popularity as a way to eliminate features when producing the sparse regression weights of $L_1$ models. However, despite the increasing need of privacy-preserving models for data analysis, to the best of our knowledge, no differentially private screening rule exists. In this paper, we develop the first private screening rule for linear regression. We initially find that this screening rule is too strong: it screens too many coefficients as a result of the private screening step. However, a weakened implementation of private screening reduces overscreening and improves performance.</li>
</ul>

<h3>Title: Scalable Best-of-N Selection for Large Language Models via Self-Certainty</h3>
<ul>
<li><strong>Authors: </strong>Zhewei Kang, Xuandong Zhao, Dawn Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18581">https://arxiv.org/abs/2502.18581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18581">https://arxiv.org/pdf/2502.18581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18581]] Scalable Best-of-N Selection for Large Language Models via Self-Certainty(https://arxiv.org/abs/2502.18581)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size $N$, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at this https URL</li>
</ul>

<h3>Title: Neurobiber: Fast and Interpretable Stylistic Feature Extraction</h3>
<ul>
<li><strong>Authors: </strong>Kenan Alkiek, Anna Wegmann, Jian Zhu, David Jurgens</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18590">https://arxiv.org/abs/2502.18590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18590">https://arxiv.org/pdf/2502.18590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18590]] Neurobiber: Fast and Interpretable Stylistic Feature Extraction(https://arxiv.org/abs/2502.18590)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Linguistic style is pivotal for understanding how texts convey meaning and fulfill communicative purposes, yet extracting detailed stylistic features at scale remains challenging. We present Neurobiber, a transformer-based system for fast, interpretable style profiling built on Biber's Multidimensional Analysis (MDA). Neurobiber predicts 96 Biber-style features from our open-source BiberPlus library (a Python toolkit that computes stylistic features and provides integrated analytics, e.g., PCA and factor analysis). Despite being up to 56 times faster than existing open source systems, Neurobiber replicates classic MDA insights on the CORE corpus and achieves competitive performance on the PAN 2020 authorship verification task without extensive retraining. Its efficient and interpretable representations readily integrate into downstream NLP pipelines, facilitating large-scale stylometric research, forensic analysis, and real-time text monitoring. All components are made publicly available.</li>
</ul>

<h3>Title: DeBUGCN -- Detecting Backdoors in CNNs Using Graph Convolutional Networks</h3>
<ul>
<li><strong>Authors: </strong>Akash Vartak, Khondoker Murad Hossain, Tim Oates</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18592">https://arxiv.org/abs/2502.18592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18592">https://arxiv.org/pdf/2502.18592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18592]] DeBUGCN -- Detecting Backdoors in CNNs Using Graph Convolutional Networks(https://arxiv.org/abs/2502.18592)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) are becoming commonplace in critical applications, making their susceptibility to backdoor (trojan) attacks a significant problem. In this paper, we introduce a novel backdoor attack detection pipeline, detecting attacked models using graph convolution networks (DeBUGCN). To the best of our knowledge, ours is the first use of GCNs for trojan detection. We use the static weights of a DNN to create a graph structure of its layers. A GCN is then used as a binary classifier on these graphs, yielding a trojan or clean determination for the DNN. To demonstrate the efficacy of our pipeline, we train hundreds of clean and trojaned CNN models on the MNIST handwritten digits and CIFAR-10 image datasets, and show the DNN classification results using DeBUGCN. For a true In-the-Wild use case, our pipeline is evaluated on the TrojAI dataset which consists of various CNN architectures, thus showing the robustness and model-agnostic behaviour of DeBUGCN. Furthermore, on comparing our results on several datasets with state-of-the-art trojan detection algorithms, DeBUGCN is faster and more accurate.</li>
</ul>

<h3>Title: Error-related Potential driven Reinforcement Learning for adaptive Brain-Computer Interfaces</h3>
<ul>
<li><strong>Authors: </strong>Aline Xavier Fidncio, Felix Grn, Christian Klaes, Ioannis Iossifidis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18594">https://arxiv.org/abs/2502.18594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18594">https://arxiv.org/pdf/2502.18594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18594]] Error-related Potential driven Reinforcement Learning for adaptive Brain-Computer Interfaces(https://arxiv.org/abs/2502.18594)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Brain-computer interfaces (BCIs) provide alternative communication methods for individuals with motor disabilities by allowing control and interaction with external devices. Non-invasive BCIs, especially those using electroencephalography (EEG), are practical and safe for various applications. However, their performance is often hindered by EEG non-stationarities, caused by changing mental states or device characteristics like electrode impedance. This challenge has spurred research into adaptive BCIs that can handle such variations. In recent years, interest has grown in using error-related potentials (ErrPs) to enhance BCI performance. ErrPs, neural responses to errors, can be detected non-invasively and have been integrated into different BCI paradigms to improve performance through error correction or adaptation. This research introduces a novel adaptive ErrP-based BCI approach using reinforcement learning (RL). We demonstrate the feasibility of an RL-driven adaptive framework incorporating ErrPs and motor imagery. Utilizing two RL agents, the framework adapts dynamically to EEG non-stationarities. Validation was conducted using a publicly available motor imagery dataset and a fast-paced game designed to boost user engagement. Results show the framework's promise, with RL agents learning control policies from user interactions and achieving robust performance across datasets. However, a critical insight from the game-based protocol revealed that motor imagery in a high-speed interaction paradigm was largely ineffective for participants, highlighting task design limitations in real-time BCI applications. These findings underscore the potential of RL for adaptive BCIs while pointing out practical constraints related to task complexity and user responsiveness.</li>
</ul>

<h3>Title: Chain of Draft: Thinking Faster by Writing Less</h3>
<ul>
<li><strong>Authors: </strong>Silei Xu, Wenhao Xie, Lingxiao Zhao, Pengcheng He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18600">https://arxiv.org/abs/2502.18600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18600">https://arxiv.org/pdf/2502.18600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18600]] Chain of Draft: Thinking Faster by Writing Less(https://arxiv.org/abs/2502.18600)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable performance in solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT) prompting, which emphasizes verbose, step-by-step reasoning. However, humans typically employ a more efficient strategy: drafting concise intermediate thoughts that capture only essential information. In this work, we propose Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes, where LLMs generate minimalistic yet informative intermediate reasoning outputs while solving tasks. By reducing verbosity and focusing on critical insights, CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens, significantly reducing cost and latency across various reasoning tasks.</li>
</ul>

<h3>Title: Tighten The Lasso: A Convex Hull Volume-based Anomaly Detection Method</h3>
<ul>
<li><strong>Authors: </strong>Uri Itai, Asael Bar Ilan, Teddy Lazebnik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18601">https://arxiv.org/abs/2502.18601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18601">https://arxiv.org/pdf/2502.18601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18601]] Tighten The Lasso: A Convex Hull Volume-based Anomaly Detection Method(https://arxiv.org/abs/2502.18601)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rapid advancements in data-driven methodologies have underscored the critical importance of ensuring data quality. Consequently, detecting out-of-distribution (OOD) data has emerged as an essential task to maintain the reliability and robustness of data-driven models, in general, and machine and deep learning models, in particular. In this study, we leveraged the convex hull property of a dataset and the fact that anomalies highly contribute to the increase of the CH's volume to propose a novel anomaly detection algorithm. Our algorithm computes the CH's volume as an increasing number of data points are removed from the dataset to define a decision line between OOD and in-distribution data points. We compared the proposed algorithm to seven widely used anomaly detection algorithms over ten datasets, showing comparable results for state-of-the-art (SOTA) algorithms. Moreover, we show that with a computationally cheap and simple check, one can detect datasets that are well-suited for the proposed algorithm which outperforms the SOTA anomaly detection algorithms.</li>
</ul>

<h3>Title: Toward Breaking Watermarks in Distortion-free Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shayleen Reynolds, Saheed Obitayo, Niccol Dalmasso, Dung Daniel T. Ngo, Vamsi K. Potluru, Manuela Veloso</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18608">https://arxiv.org/abs/2502.18608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18608">https://arxiv.org/pdf/2502.18608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18608]] Toward Breaking Watermarks in Distortion-free Large Language Models(https://arxiv.org/abs/2502.18608)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal, watermark, large language model</a></li>
<li><strong>Abstract: </strong>In recent years, LLM watermarking has emerged as an attractive safeguard against AI-generated content, with promising applications in many real-world domains. However, there are growing concerns that the current LLM watermarking schemes are vulnerable to expert adversaries wishing to reverse-engineer the watermarking mechanisms. Prior work in "breaking" or "stealing" LLM watermarks mainly focuses on the distribution-modifying algorithm of Kirchenbauer et al. (2023), which perturbs the logit vector before sampling. In this work, we focus on reverse-engineering the other prominent LLM watermarking scheme, distortion-free watermarking (Kuditipudi et al. 2024), which preserves the underlying token distribution by using a hidden watermarking key sequence. We demonstrate that, even under a more sophisticated watermarking scheme, it is possible to "compromise" the LLM and carry out a "spoofing" attack. Specifically, we propose a mixed integer linear programming framework that accurately estimates the secret key used for watermarking using only a few samples of the watermarked dataset. Our initial findings challenge the current theoretical claims on the robustness and usability of existing LLM watermarking techniques.</li>
</ul>

<h3>Title: IID-Based QPP-RNG: A Random Number Generator Utilizing Random Permutation Sorting Driven by System Jitter</h3>
<ul>
<li><strong>Authors: </strong>Randy Kuang, Dafu Lou</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18609">https://arxiv.org/abs/2502.18609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18609">https://arxiv.org/pdf/2502.18609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18609]] IID-Based QPP-RNG: A Random Number Generator Utilizing Random Permutation Sorting Driven by System Jitter(https://arxiv.org/abs/2502.18609)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>We present IID-Based QPP-RNG, a groundbreaking random number generator that achieves truly uniform, independent, and identically distributed (IID) randomness by integrating Quantum Permutation Pads (QPP) with system jitter-derived entropy. Unlike conventional RNGs that rely directly on raw timing variations, IID-Based QPP-RNG uses system jitter solely to generate ephemeral QPP pads and derives its final 8-bit outputs from permutation counts -- without the post-processing module. This design leverages the factorial complexity of random permutation-based sorting to systematically accumulate entropy from dynamic hardware interactions, ensuring non-deterministic outputs even when starting from fixed seeds. Our implementation employs a dynamic seed evolution protocol that continuously refreshes the internal state using unpredictable system jitter, decoupling the deterministic QPP sequence from the initial seed. Rigorous cross-platform validation on macOS (x86 and ARM) and Windows (x86) confirms that the generator produces uniformly distributed 8-bit outputs. Evaluations compliant with NIST SP 800-90B demonstrate exceptional statistical quality, with a Shannon entropy of 7.9999 bits per byte and a min-entropy of 7.18 bits per byte. IID-Based QPP-RNG represents a significant advancement in random number generation by bridging algorithmic complexity with system-level entropy, offering a scalable, software-only, post-quantum-secure solution for cryptographic applications in environments lacking dedicated hardware entropy sources.</li>
</ul>

<h3>Title: Diffusion Models for conditional MRI generation</h3>
<ul>
<li><strong>Authors: </strong>Miguel Herencia Garca del Castillo, Ricardo Moya Garcia, Manuel Jess Cerezo Mazn, Ekaitz Arriola Garcia, Pablo Menndez Fernndez-Miranda</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18620">https://arxiv.org/abs/2502.18620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18620">https://arxiv.org/pdf/2502.18620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18620]] Diffusion Models for conditional MRI generation(https://arxiv.org/abs/2502.18620)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>In this article, we present a Latent Diffusion Model (LDM) for the generation of brain Magnetic Resonance Imaging (MRI), conditioning its generation based on pathology (Healthy, Glioblastoma, Sclerosis, Dementia) and acquisition modality (T1w, T1ce, T2w, Flair, PD). To evaluate the quality of the generated images, the Frchet Inception Distance (FID) and Multi-Scale Structural Similarity Index (MS-SSIM) metrics were employed. The results indicate that the model generates images with a distribution similar to real ones, maintaining a balance between visual fidelity and diversity. Additionally, the model demonstrates extrapolation capability, enabling the generation of configurations that were not present in the training data. The results validate the potential of the model to increase in the number of samples in clinical datasets, balancing underrepresented classes, and evaluating AI models in medicine, contributing to the development of diagnostic tools in radiology without compromising patient privacy.</li>
</ul>

<h3>Title: On the Privacy-Preserving Properties of Spiking Neural Networks with Unique Surrogate Gradients and Quantization Levels</h3>
<ul>
<li><strong>Authors: </strong>Ayana Moshruba, Shay Snyder, Hamed Poursiami, Maryam Parsa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18623">https://arxiv.org/abs/2502.18623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18623">https://arxiv.org/pdf/2502.18623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18623]] On the Privacy-Preserving Properties of Spiking Neural Networks with Unique Surrogate Gradients and Quantization Levels(https://arxiv.org/abs/2502.18623)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>As machine learning models increasingly process sensitive data, understanding their vulnerability to privacy attacks is vital. Membership inference attacks (MIAs) exploit model responses to infer whether specific data points were used during training, posing a significant privacy risk. Prior research suggests that spiking neural networks (SNNs), which rely on event-driven computation and discrete spike-based encoding, exhibit greater resilience to MIAs than artificial neural networks (ANNs). This resilience stems from their non-differentiable activations and inherent stochasticity, which obscure the correlation between model responses and individual training samples. To enhance privacy in SNNs, we explore two techniques: quantization and surrogate gradients. Quantization, which reduces precision to limit information leakage, has improved privacy in ANNs. Given SNNs' sparse and irregular activations, quantization may further disrupt the activation patterns exploited by MIAs. We assess the vulnerability of SNNs and ANNs under weight and activation quantization across multiple datasets, using the attack model's receiver operating characteristic (ROC) curve area under the curve (AUC) metric, where lower values indicate stronger privacy, and evaluate the privacy-accuracy trade-off. Our findings show that quantization enhances privacy in both architectures with minimal performance loss, though full-precision SNNs remain more resilient than quantized ANNs. Additionally, we examine the impact of surrogate gradients on privacy in SNNs. Among five evaluated gradients, spike rate escape provides the best privacy-accuracy trade-off, while arctangent increases vulnerability to MIAs. These results reinforce SNNs' inherent privacy advantages and demonstrate that quantization and surrogate gradient selection significantly influence privacy-accuracy trade-offs in SNNs.</li>
</ul>

<h3>Title: Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems</h3>
<ul>
<li><strong>Authors: </strong>Matthew Barker, Andrew Bell, Evan Thomas, James Carr, Thomas Andrews, Umang Bhatt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18635">https://arxiv.org/abs/2502.18635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18635">https://arxiv.org/pdf/2502.18635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18635]] Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems(https://arxiv.org/abs/2502.18635)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Retrieval Augmented Generation (RAG) has emerged as a popular technique for improving Large Language Model (LLM) systems, it introduces a large number of choices, parameters and hyperparameters that must be made or tuned. This includes the LLM, embedding, and ranker models themselves, as well as hyperparameters governing individual RAG components. Yet, collectively optimizing the entire configuration in a RAG or LLM system remains under-explored - especially in multi-objective settings - due to intractably large solution spaces, noisy objective evaluations, and the high cost of evaluations. In this work, we introduce the first approach for multi-objective parameter optimization of cost, latency, safety and alignment over entire LLM and RAG systems. We find that Bayesian optimization methods significantly outperform baseline approaches, obtaining a superior Pareto front on two new RAG benchmark tasks. We conclude our work with important considerations for practitioners who are designing multi-objective RAG systems, highlighting nuances such as how optimal configurations may not generalize across tasks and objectives.</li>
</ul>

<h3>Title: Steered Generation via Gradient Descent on Sparse Features</h3>
<ul>
<li><strong>Authors: </strong>Sumanta Bhattacharyya, Pedram Rooshenas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18644">https://arxiv.org/abs/2502.18644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18644">https://arxiv.org/pdf/2502.18644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18644]] Steered Generation via Gradient Descent on Sparse Features(https://arxiv.org/abs/2502.18644)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) encode a diverse range of linguistic features within their latent representations, which can be harnessed to steer their output toward specific target characteristics. In this paper, we modify the internal structure of LLMs by training sparse autoencoders to learn a sparse representation of the query embedding, allowing precise control over the model's attention distribution. We demonstrate that manipulating this sparse representation effectively transforms the output toward different stylistic and cognitive targets. Specifically, in an educational setting, we show that the cognitive complexity of LLM-generated feedback can be systematically adjusted by modifying the encoded query representation at a specific layer. To achieve this, we guide the learned sparse embedding toward the representation of samples from the desired cognitive complexity level, using gradient-based optimization in the latent space.</li>
</ul>

<h3>Title: Single- vs. Dual-Prompt Dialogue Generation with LLMs for Job Interviews in Human Resources</h3>
<ul>
<li><strong>Authors: </strong>Joachim De Baer, A. Seza Doruz, Thomas Demeester, Chris Develder</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18650">https://arxiv.org/abs/2502.18650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18650">https://arxiv.org/pdf/2502.18650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18650]] Single- vs. Dual-Prompt Dialogue Generation with LLMs for Job Interviews in Human Resources(https://arxiv.org/abs/2502.18650)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Optimizing language models for use in conversational agents requires large quantities of example dialogues. Increasingly, these dialogues are synthetically generated by using powerful large language models (LLMs), especially in domains with challenges to obtain authentic human data. One such domain is human resources (HR). In this context, we compare two LLM-based dialogue generation methods for the use case of generating HR job interviews, and assess whether one method generates higher-quality dialogues that are more challenging to distinguish from genuine human discourse. The first method uses a single prompt to generate the complete interview dialog. The second method uses two agents that converse with each other. To evaluate dialogue quality under each method, we ask a judge LLM to determine whether AI was used for interview generation, using pairwise interview comparisons. We demonstrate that despite a sixfold increase in token cost, interviews generated with the dual-prompt method achieve a win rate up to ten times higher than those generated with the single-prompt method. This difference remains consistent regardless of whether GPT-4o or Llama 3.3 70B is used for either interview generation or judging quality.</li>
</ul>

<h3>Title: Enhancing Text Classification with a Novel Multi-Agent Collaboration Framework Leveraging BERT</h3>
<ul>
<li><strong>Authors: </strong>Hediyeh Baban, Sai A Pidapar, Aashutosh Nema, Sichen Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18653">https://arxiv.org/abs/2502.18653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18653">https://arxiv.org/pdf/2502.18653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18653]] Enhancing Text Classification with a Novel Multi-Agent Collaboration Framework Leveraging BERT(https://arxiv.org/abs/2502.18653)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>We introduce a novel multi-agent collaboration framework designed to enhance the accuracy and robustness of text classification models. Leveraging BERT as the primary classifier, our framework dynamically escalates low-confidence predictions to a specialized multi-agent system comprising Lexical, Contextual, Logic, Consensus, and Explainability agents. This collaborative approach allows for comprehensive analysis and consensus-driven decision-making, significantly improving classification performance across diverse text classification tasks. Empirical evaluations on benchmark datasets demonstrate that our framework achieves a 5.5% increase in accuracy compared to standard BERT-based classifiers, underscoring its effectiveness and academic novelty in advancing multi-agent systems within natural language processing.</li>
</ul>

<h3>Title: CayleyPy RL: Pathfinding and Reinforcement Learning on Cayley Graphs</h3>
<ul>
<li><strong>Authors: </strong>A.Chervov, A.Soibelman, S.Lytkin, I.Kiselev, S.Fironov, A.Lukyanenko, A.Dolgorukova, A.Ogurtsov, F.Petrov, S.Krymskii, M.Evseev, L.Grunvald, D.Gorodkov, G.Antiufeev, G.Verbii, V.Zamkovoy, L.Cheldieva, I.Koltsov, A. Sychev, M.Obozov, A.Eliseev, S.Nikolenko, N.Narynbaev, R.Turtayev, N. Rokotyan, S.Kovalev, A.Rozanov, V.Nelin, S.Ermilov, L.Shishina, D.Mamayeva, A.Korolkova, K.Khoruzhii, A.Romanov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DM, math.CO, math.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18663">https://arxiv.org/abs/2502.18663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18663">https://arxiv.org/pdf/2502.18663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18663]] CayleyPy RL: Pathfinding and Reinforcement Learning on Cayley Graphs(https://arxiv.org/abs/2502.18663)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper is the second in a series of studies on developing efficient artificial intelligence-based approaches to pathfinding on extremely large graphs (e.g. $10^{70}$ nodes) with a focus on Cayley graphs and mathematical applications. The open-source CayleyPy project is a central component of our research. The present paper proposes a novel combination of a reinforcement learning approach with a more direct diffusion distance approach from the first paper. Our analysis includes benchmarking various choices for the key building blocks of the approach: architectures of the neural network, generators for the random walks and beam search pathfinding. We compared these methods against the classical computer algebra system GAP, demonstrating that they "overcome the GAP" for the considered examples. As a particular mathematical application we examine the Cayley graph of the symmetric group with cyclic shift and transposition generators. We provide strong support for the OEIS-A186783 conjecture that the diameter is equal to n(n-1)/2 by machine learning and mathematical methods. We identify the conjectured longest element and generate its decomposition of the desired length. We prove a diameter lower bound of n(n-1)/2-n/2 and an upper bound of n(n-1)/2+ 3n by presenting the algorithm with given complexity. We also present several conjectures motivated by numerical experiments, including observations on the central limit phenomenon (with growth approximated by a Gumbel distribution), the uniform distribution for the spectrum of the graph, and a numerical study of sorting networks. To stimulate crowdsourcing activity, we create challenges on the Kaggle platform and invite contributions to improve and benchmark approaches on Cayley graph pathfinding and other tasks.</li>
</ul>

<h3>Title: Discriminative Finetuning of Generative Large Language Models without Reward Models and Preference Data</h3>
<ul>
<li><strong>Authors: </strong>Siqi Guo, Ilgee Hong, Vicente Balmaseda, Tuo Zhao, Tianbao Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18679">https://arxiv.org/abs/2502.18679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18679">https://arxiv.org/pdf/2502.18679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18679]] Discriminative Finetuning of Generative Large Language Models without Reward Models and Preference Data(https://arxiv.org/abs/2502.18679)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) followed by preference optimization (PO) denoted by SFT$\rightarrow$PO has become the standard for improving pretrained large language models (LLMs), with PO demonstrating significant performance gains. However, PO methods rely on either human-labeled preference data or a strong reward model to generate preference data. Can we fine-tune LLMs without preference data or reward models while achieving competitive performance to SFT$\rightarrow$PO? We address this question by introducing Discriminative Fine-Tuning (DFT), a novel approach that eliminates the need for preference data. Unlike SFT, which employs a generative approach and overlooks negative data, DFT adopts a discriminative paradigm that that increases the probability of positive answers while suppressing potentially negative ones, shifting from token prediction to data prediction. Our contributions include: (i) a discriminative probabilistic framework for fine-tuning LLMs by explicitly modeling the discriminative likelihood of an answer among all possible outputs given an input; (ii) efficient algorithms to optimize this discriminative likelihood; and (iii) extensive experiments demonstrating DFT's effectiveness, achieving performance better than SFT and comparable to if not better than SFT$\rightarrow$PO. The code can be found at this https URL.</li>
</ul>

<h3>Title: H-FLTN: A Privacy-Preserving Hierarchical Framework for Electric Vehicle Spatio-Temporal Charge Prediction</h3>
<ul>
<li><strong>Authors: </strong>Robert Marlin, Raja Jurdak, Alsharif Abuadbba</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18697">https://arxiv.org/abs/2502.18697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18697">https://arxiv.org/pdf/2502.18697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18697]] H-FLTN: A Privacy-Preserving Hierarchical Framework for Electric Vehicle Spatio-Temporal Charge Prediction(https://arxiv.org/abs/2502.18697)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate, transformer</a></li>
<li><strong>Abstract: </strong>The widespread adoption of Electric Vehicles (EVs) poses critical challenges for energy providers, particularly in predicting charging time (temporal prediction), ensuring user privacy, and managing resources efficiently in mobility-driven networks. This paper introduces the Hierarchical Federated Learning Transformer Network (H-FLTN) framework to address these challenges. H-FLTN employs a three-tier hierarchical architecture comprising EVs, community Distributed Energy Resource Management Systems (DERMS), and the Energy Provider Data Centre (EPDC) to enable accurate spatio-temporal predictions of EV charging needs while preserving privacy. Temporal prediction is enhanced using Transformer-based learning, capturing complex dependencies in charging behavior. Privacy is ensured through Secure Aggregation, Additive Secret Sharing, and Peer-to-Peer (P2P) Sharing with Augmentation, which allow only secret shares of model weights to be exchanged while securing all transmissions. To improve training efficiency and resource management, H-FLTN integrates Dynamic Client Capping Mechanism (DCCM) and Client Rotation Management (CRM), ensuring that training remains both computationally and temporally efficient as the number of participating EVs increases. DCCM optimises client participation by limiting excessive computational loads, while CRM balances training contributions across epochs, preventing imbalanced participation. Our simulation results based on large-scale empirical vehicle mobility data reveal that DCCM and CRM reduce the training time complexity with increasing EVs from linear to constant. Its integration into real-world smart city infrastructure enhances energy demand forecasting, resource allocation, and grid stability, ensuring reliability and sustainability in future mobility ecosystems.</li>
</ul>

<h3>Title: Tukey Depth Mechanisms for Practical Private Mean Estimation</h3>
<ul>
<li><strong>Authors: </strong>Gavin Brown, Lydia Zakynthinou</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18698">https://arxiv.org/abs/2502.18698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18698">https://arxiv.org/pdf/2502.18698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18698]] Tukey Depth Mechanisms for Practical Private Mean Estimation(https://arxiv.org/abs/2502.18698)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Mean estimation is a fundamental task in statistics and a focus within differentially private statistical estimation. While univariate methods based on the Gaussian mechanism are widely used in practice, more advanced techniques such as the exponential mechanism over quantiles offer robustness and improved performance, especially for small sample sizes. Tukey depth mechanisms carry these advantages to multivariate data, providing similar strong theoretical guarantees. However, practical implementations fall behind these theoretical developments. In this work, we take the first step to bridge this gap by implementing the (Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for multivariate Gaussian distributions, yielding improved practical methods for private mean estimation. Our implementations enable the use of these mechanisms for small sample sizes or low-dimensional data. Additionally, we implement variants of these mechanisms that use approximate versions of Tukey depth, trading off accuracy for faster computation. We demonstrate their efficiency in practice, showing that they are viable options for modest dimensions. Given their strong accuracy and robustness guarantees, we contend that they are competitive approaches for mean estimation in this regime. We explore future directions for improving the computational efficiency of these algorithms by leveraging fast polytope volume approximation techniques, paving the way for more accurate private mean estimation in higher dimensions.</li>
</ul>

<h3>Title: MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment</h3>
<ul>
<li><strong>Authors: </strong>Tianze Wang, Dongnan Gui, Yifan Hu, Shuhang Lin, Linjun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18699">https://arxiv.org/abs/2502.18699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18699">https://arxiv.org/pdf/2502.18699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18699]] MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment(https://arxiv.org/abs/2502.18699)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) has shown promise in aligning large language models (LLMs). Yet its reliance on a singular reward model often overlooks the diversity of human preferences. Recent approaches address this limitation by leveraging multi-dimensional feedback to fine-tune corresponding reward models and train LLMs using reinforcement learning. However, the process is costly and unstable, especially given the competing and heterogeneous nature of human preferences. In this paper, we propose Mixing Preference Optimization (MPO), a post-processing framework for aggregating single-objective policies as an alternative to both multi-objective RLHF (MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it log-linearly combines existing policies into a unified one with the weight of each policy computed via a batch stochastic mirror descent. Empirical results demonstrate that MPO achieves balanced performance across diverse preferences, outperforming or matching existing models with significantly reduced computational costs.</li>
</ul>

<h3>Title: Differentially Private Federated Learning With Time-Adaptive Privacy Spending</h3>
<ul>
<li><strong>Authors: </strong>Shahrzad Kiani, Nupur Kulkarni, Adam Dziedzic, Stark Draper, Franziska Boenisch</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18706">https://arxiv.org/abs/2502.18706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18706">https://arxiv.org/pdf/2502.18706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18706]] Differentially Private Federated Learning With Time-Adaptive Privacy Spending(https://arxiv.org/abs/2502.18706)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) with differential privacy (DP) provides a framework for collaborative machine learning, enabling clients to train a shared model while adhering to strict privacy constraints. The framework allows each client to have an individual privacy guarantee, e.g., by adding different amounts of noise to each client's model updates. One underlying assumption is that all clients spend their privacy budgets uniformly over time (learning rounds). However, it has been shown in the literature that learning in early rounds typically focuses on more coarse-grained features that can be learned at lower signal-to-noise ratios while later rounds learn fine-grained features that benefit from higher signal-to-noise ratios. Building on this intuition, we propose a time-adaptive DP-FL framework that expends the privacy budget non-uniformly across both time and clients. Our framework enables each client to save privacy budget in early rounds so as to be able to spend more in later rounds when additional accuracy is beneficial in learning more fine-grained features. We theoretically prove utility improvements in the case that clients with stricter privacy budgets spend budgets unevenly across rounds, compared to clients with more relaxed budgets, who have sufficient budgets to distribute their spend more evenly. Our practical experiments on standard benchmark datasets support our theoretical results and show that, in practice, our algorithms improve the privacy-utility trade-offs compared to baseline schemes.</li>
</ul>

<h3>Title: Adversarial Universal Stickers: Universal Perturbation Attacks on Traffic Sign using Stickers</h3>
<ul>
<li><strong>Authors: </strong>Anthony Etim, Jakub Szefer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18724">https://arxiv.org/abs/2502.18724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18724">https://arxiv.org/pdf/2502.18724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18724]] Adversarial Universal Stickers: Universal Perturbation Attacks on Traffic Sign using Stickers(https://arxiv.org/abs/2502.18724)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Adversarial attacks on deep learning models have proliferated in recent years. In many cases, a different adversarial perturbation is required to be added to each image to cause the deep learning model to misclassify it. This is ineffective as each image has to be modified in a different way. Meanwhile, research on universal perturbations focuses on designing a single perturbation that can be applied to all images in a data set, and cause a deep learning model to misclassify the images. This work advances the field of universal perturbations by exploring universal perturbations in the context of traffic signs and autonomous vehicle systems. This work introduces a novel method for generating universal perturbations that visually look like simple black and white stickers, and using them to cause incorrect street sign predictions. Unlike traditional adversarial perturbations, the adversarial universal stickers are designed to be applicable to any street sign: same sticker, or stickers, can be applied in same location to any street sign and cause it to be misclassified. Further, to enable safe experimentation with adversarial images and street signs, this work presents a virtual setting that leverages Street View images of street signs, rather than the need to physically modify street signs, to test the attacks. The experiments in the virtual setting demonstrate that these stickers can consistently mislead deep learning models used commonly in street sign recognition, and achieve high attack success rates on dataset of US traffic signs. The findings highlight the practical security risks posed by simple stickers applied to traffic signs, and the ease with which adversaries can generate adversarial universal stickers that can be applied to many street signs.</li>
</ul>

<h3>Title: Random Forest-of-Thoughts: Uncertainty-aware Reasoning for Computational Social Science</h3>
<ul>
<li><strong>Authors: </strong>Xiaohua Wu, Xiaohui Tao, Wenjie Wu, Yuefeng Li, Lin Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18729">https://arxiv.org/abs/2502.18729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18729">https://arxiv.org/pdf/2502.18729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18729]] Random Forest-of-Thoughts: Uncertainty-aware Reasoning for Computational Social Science(https://arxiv.org/abs/2502.18729)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Social surveys in computational social science are well-designed by elaborate domain theories that can effectively reflect the interviewee's deep thoughts without concealing their true feelings. The candidate questionnaire options highly depend on the interviewee's previous answer, which results in the complexity of social survey analysis, the time, and the expertise required. The ability of large language models (LLMs) to perform complex reasoning is well-enhanced by prompting learning such as Chain-of-thought (CoT) but still confined to left-to-right decision-making processes or limited paths during inference. This means they can fall short in problems that require exploration and uncertainty searching. In response, a novel large language model prompting method, called Random Forest of Thoughts (RFoT), is proposed for generating uncertainty reasoning to fit the area of computational social science. The RFoT allows LLMs to perform deliberate decision-making by generating diverse thought space and randomly selecting the sub-thoughts to build the forest of thoughts. It can extend the exploration and prediction of overall performance, benefiting from the extensive research space of response. The method is applied to optimize computational social science analysis on two datasets covering a spectrum of social survey analysis problems. Our experiments show that RFoT significantly enhances language models' abilities on two novel social survey analysis problems requiring non-trivial reasoning.</li>
</ul>

<h3>Title: Cross-Modality Investigation on WESAD Stress Classification</h3>
<ul>
<li><strong>Authors: </strong>Eric Oliver, Sagnik Dakshit</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18733">https://arxiv.org/abs/2502.18733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18733">https://arxiv.org/pdf/2502.18733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18733]] Cross-Modality Investigation on WESAD Stress Classification(https://arxiv.org/abs/2502.18733)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Deep learning's growing prevalence has driven its widespread use in healthcare, where AI and sensor advancements enhance diagnosis, treatment, and monitoring. In mobile health, AI-powered tools enable early diagnosis and continuous monitoring of conditions like stress. Wearable technologies and multimodal physiological data have made stress detection increasingly viable, but model efficacy depends on data quality, quantity, and modality. This study develops transformer models for stress detection using the WESAD dataset, training on electrocardiograms (ECG), electrodermal activity (EDA), electromyography (EMG), respiration rate (RESP), temperature (TEMP), and 3-axis accelerometer (ACC) signals. The results demonstrate the effectiveness of single-modality transformers in analyzing physiological signals, achieving state-of-the-art performance with accuracy, precision and recall values in the range of $99.73\%$ to $99.95\%$ for stress detection. Furthermore, this study explores cross-modal performance and also explains the same using 2D visualization of the learned embedding space and quantitative analysis based on data variance. Despite the large body of work on stress detection and monitoring, the robustness and generalization of these models across different modalities has not been explored. This research represents one of the initial efforts to interpret embedding spaces for stress detection, providing valuable information on cross-modal performance.</li>
</ul>

<h3>Title: Automatic Prompt Optimization via Heuristic Search: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun, Damien Lopez, Kamalika Das, Bradley A. Malin, Sricharan Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18746">https://arxiv.org/abs/2502.18746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18746">https://arxiv.org/pdf/2502.18746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18746]] Automatic Prompt Optimization via Heuristic Search: A Survey(https://arxiv.org/abs/2502.18746)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models have led to remarkable achievements across a variety of Natural Language Processing tasks, making prompt engineering increasingly central to guiding model outputs. While manual methods can be effective, they typically rely on intuition and do not automatically refine prompts over time. In contrast, automatic prompt optimization employing heuristic-based search algorithms can systematically explore and improve prompts with minimal human oversight. This survey proposes a comprehensive taxonomy of these methods, categorizing them by where optimization occurs, what is optimized, what criteria drive the optimization, which operators generate new prompts, and which iterative search algorithms are applied. We further highlight specialized datasets and tools that support and accelerate automated prompt refinement. We conclude by discussing key open challenges pointing toward future opportunities for more robust and versatile LLM applications.</li>
</ul>

<h3>Title: Spectral-Enhanced Transformers: Leveraging Large-Scale Pretrained Models for Hyperspectral Object Tracking</h3>
<ul>
<li><strong>Authors: </strong>Shaheer Mohamed, Tharindu Fernando, Sridha Sridharan, Peyman Moghadam, Clinton Fookes</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18748">https://arxiv.org/abs/2502.18748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18748">https://arxiv.org/pdf/2502.18748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18748]] Spectral-Enhanced Transformers: Leveraging Large-Scale Pretrained Models for Hyperspectral Object Tracking(https://arxiv.org/abs/2502.18748)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Hyperspectral object tracking using snapshot mosaic cameras is emerging as it provides enhanced spectral information alongside spatial data, contributing to a more comprehensive understanding of material properties. Using transformers, which have consistently outperformed convolutional neural networks (CNNs) in learning better feature representations, would be expected to be effective for Hyperspectral object tracking. However, training large transformers necessitates extensive datasets and prolonged training periods. This is particularly critical for complex tasks like object tracking, and the scarcity of large datasets in the hyperspectral domain acts as a bottleneck in achieving the full potential of powerful transformer models. This paper proposes an effective methodology that adapts large pretrained transformer-based foundation models for hyperspectral object tracking. We propose an adaptive, learnable spatial-spectral token fusion module that can be extended to any transformer-based backbone for learning inherent spatial-spectral features in hyperspectral data. Furthermore, our model incorporates a cross-modality training pipeline that facilitates effective learning across hyperspectral datasets collected with different sensor modalities. This enables the extraction of complementary knowledge from additional modalities, whether or not they are present during testing. Our proposed model also achieves superior performance with minimal training iterations.</li>
</ul>

<h3>Title: Reward Shaping to Mitigate Reward Hacking in RLHF</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18770">https://arxiv.org/abs/2502.18770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18770">https://arxiv.org/pdf/2502.18770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18770]] Reward Shaping to Mitigate Reward Hacking in RLHF(https://arxiv.org/abs/2502.18770)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to reward hacking, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. While reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests three key design principles: (1) RL reward is ideally bounded, (2) RL benefits from rapid initial growth followed by gradual convergence, and (3) RL reward is best formulated as a function of centered reward. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model itself as the signal for reinforcement learning. We evaluated PAR on two base models, Gemma2-2B and Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. Code is available at this https URL.</li>
</ul>

<h3>Title: Exploring Graph Tasks with Pure LLMs: A Comprehensive Benchmark and Investigation</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Wang, Xinnan Dai, Wenqi Fan, Yao Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18771">https://arxiv.org/abs/2502.18771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18771">https://arxiv.org/pdf/2502.18771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18771]] Exploring Graph Tasks with Pure LLMs: A Comprehensive Benchmark and Investigation(https://arxiv.org/abs/2502.18771)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Graph-structured data has become increasingly prevalent across various domains, raising the demand for effective models to handle graph tasks like node classification and link prediction. Traditional graph learning models like Graph Neural Networks (GNNs) have made significant strides, but their capabilities in handling graph data remain limited in certain contexts. In recent years, large language models (LLMs) have emerged as promising candidates for graph tasks, yet most studies focus primarily on performance benchmarks and fail to address their broader potential, including their ability to handle limited data, their transferability across tasks, and their robustness. In this work, we provide a comprehensive exploration of LLMs applied to graph tasks. We evaluate the performance of pure LLMs, including those without parameter optimization and those fine-tuned with instructions, across various scenarios. Our analysis goes beyond accuracy, assessing LLM ability to perform in few-shot/zero-shot settings, transfer across domains, understand graph structures, and demonstrate robustness in challenging scenarios. We conduct extensive experiments with 16 graph learning models alongside 6 LLMs (e.g., Llama3B, GPT-4o, Qwen-plus), comparing their performance on datasets like Cora, PubMed, ArXiv, and Products. Our findings show that LLMs, particularly those with instruction tuning, outperform traditional models in few-shot settings, exhibit strong domain transferability, and demonstrate excellent generalization and robustness. This work offers valuable insights into the capabilities of LLMs for graph learning, highlighting their advantages and potential for real-world applications, and paving the way for future research in this area. Codes and datasets are released in this https URL.</li>
</ul>

<h3>Title: Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance</h3>
<ul>
<li><strong>Authors: </strong>Xueqing Peng, Triantafillos Papadopoulos, Efstathia Soufleri, Polydoros Giannouris, Ruoyu Xiang, Yan Wang, Lingfei Qian, Jimin Huang, Qianqian Xie, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18772">https://arxiv.org/abs/2502.18772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18772">https://arxiv.org/pdf/2502.18772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18772]] Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance(https://arxiv.org/abs/2502.18772)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite Greece's pivotal role in the global economy, large language models (LLMs) remain underexplored for Greek financial context due to the linguistic complexity of Greek and the scarcity of domain-specific datasets. Previous efforts in multilingual financial natural language processing (NLP) have exposed considerable performance disparities, yet no dedicated Greek financial benchmarks or Greek-specific financial LLMs have been developed until now. To bridge this gap, we introduce Plutus-ben, the first Greek Financial Evaluation Benchmark, and Plutus-8B, the pioneering Greek Financial LLM, fine-tuned with Greek domain-specific data. Plutus-ben addresses five core financial NLP tasks in Greek: numeric and textual named entity recognition, question answering, abstractive summarization, and topic classification, thereby facilitating systematic and reproducible LLM assessments. To underpin these tasks, we present three novel, high-quality Greek financial datasets, thoroughly annotated by expert native Greek speakers, augmented by two existing resources. Our comprehensive evaluation of 22 LLMs on Plutus-ben reveals that Greek financial NLP remains challenging due to linguistic complexity, domain-specific terminology, and financial reasoning gaps. These findings underscore the limitations of cross-lingual transfer, the necessity for financial expertise in Greek-trained models, and the challenges of adapting financial LLMs to Greek text. We release Plutus-ben, Plutus-8B, and all associated datasets publicly to promote reproducible research and advance Greek financial NLP, fostering broader multilingual inclusivity in finance.</li>
</ul>

<h3>Title: M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with Competitive Performance</h3>
<ul>
<li><strong>Authors: </strong>Qingpei Guo, Kaiyou Song, Zipeng Feng, Ziping Ma, Qinglong Zhang, Sirui Gao, Xuzheng Yu, Yunxiao Sun, Tai-WeiChang, Jingdong Chen, Ming Yang, Jun Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18778">https://arxiv.org/abs/2502.18778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18778">https://arxiv.org/pdf/2502.18778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18778]] M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with Competitive Performance(https://arxiv.org/abs/2502.18778)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We present M2-omni, a cutting-edge, open-source omni-MLLM that achieves competitive performance to GPT-4o. M2-omni employs a unified multimodal sequence modeling framework, which empowers Large Language Models(LLMs) to acquire comprehensive cross-modal understanding and generation capabilities. Specifically, M2-omni can process arbitrary combinations of audio, video, image, and text modalities as input, generating multimodal sequences interleaving with audio, image, or text outputs, thereby enabling an advanced and interactive real-time experience. The training of such an omni-MLLM is challenged by significant disparities in data quantity and convergence rates across modalities. To address these challenges, we propose a step balance strategy during pre-training to handle the quantity disparities in modality-specific data. Additionally, a dynamically adaptive balance strategy is introduced during the instruction tuning stage to synchronize the modality-wise training progress, ensuring optimal convergence. Notably, we prioritize preserving strong performance on pure text tasks to maintain the robustness of M2-omni's language understanding capability throughout the training process. To our best knowledge, M2-omni is currently a very competitive open-source model to GPT-4o, characterized by its comprehensive modality and task support, as well as its exceptional performance. We expect M2-omni will advance the development of omni-MLLMs, thus facilitating future research in this domain.</li>
</ul>

<h3>Title: Active Few-Shot Learning for Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Saeed Ahmadnia, Arash Yousefi Jordehi, Mahsa Hosseini Khasheh Heyran, Seyed Abolghasem Mirroshandel, Owen Rambow, Cornelia Caragea</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18782">https://arxiv.org/abs/2502.18782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18782">https://arxiv.org/pdf/2502.18782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18782]] Active Few-Shot Learning for Text Classification(https://arxiv.org/abs/2502.18782)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rise of Large Language Models (LLMs) has boosted the use of Few-Shot Learning (FSL) methods in natural language processing, achieving acceptable performance even when working with limited training data. The goal of FSL is to effectively utilize a small number of annotated samples in the learning process. However, the performance of FSL suffers when unsuitable support samples are chosen. This problem arises due to the heavy reliance on a limited number of support samples, which hampers consistent performance improvement even when more support samples are added. To address this challenge, we propose an active learning-based instance selection mechanism that identifies effective support instances from the unlabeled pool and can work with different LLMs. Our experiments on five tasks show that our method frequently improves the performance of FSL. We make our implementation available on GitHub.</li>
</ul>

<h3>Title: Seeing the Forest for the Trees: A Large Scale, Continuously Updating Meta-Analysis of Frontier LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jungsoo Park, Junmo Kang, Gabriel Stanovsky, Alan Ritter</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18791">https://arxiv.org/abs/2502.18791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18791">https://arxiv.org/pdf/2502.18791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18791]] Seeing the Forest for the Trees: A Large Scale, Continuously Updating Meta-Analysis of Frontier LLMs(https://arxiv.org/abs/2502.18791)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The surge of LLM studies makes synthesizing their findings challenging. Meta-analysis can uncover important trends across studies, but its use is limited by the time-consuming nature of manual data extraction. Our study presents a semi-automated approach for meta-analysis that accelerates data extraction using LLMs. It automatically identifies relevant arXiv papers, extracts experimental results and related attributes, and organizes them into a structured dataset. We conduct a comprehensive meta-analysis of frontier LLMs using an automatically extracted dataset, reducing the effort of paper surveying and data extraction by more than 93\% compared to manual approaches. We validate our dataset by showing that it reproduces key findings from a recent manual meta-analysis about Chain-of-Thought (CoT), and also uncovers new insights that go beyond it, showing for example that in-context examples benefit multimodal tasks but offer limited gains in mathematical tasks compared to CoT. Our automatically updatable dataset enables continuous tracking of target models by extracting evaluation studies as new data becomes available. Through our scientific artifacts and empirical analysis, we provide novel insights into LLMs while facilitating ongoing meta-analyses of their behavior.</li>
</ul>

<h3>Title: Optimal Stochastic Trace Estimation in Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Xinyang Liu, Hengrong Du, Wei Deng, Ruqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18808">https://arxiv.org/abs/2502.18808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18808">https://arxiv.org/pdf/2502.18808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18808]] Optimal Stochastic Trace Estimation in Generative Modeling(https://arxiv.org/abs/2502.18808)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Hutchinson estimators are widely employed in training divergence-based likelihoods for diffusion models to ensure optimal transport (OT) properties. However, this estimator often suffers from high variance and scalability concerns. To address these challenges, we investigate Hutch++, an optimal stochastic trace estimator for generative models, designed to minimize training variance while maintaining transport optimality. Hutch++ is particularly effective for handling ill-conditioned matrices with large condition numbers, which commonly arise when high-dimensional data exhibits a low-dimensional structure. To mitigate the need for frequent and costly QR decompositions, we propose practical schemes that balance frequency and accuracy, backed by theoretical guarantees. Our analysis demonstrates that Hutch++ leads to generations of higher quality. Furthermore, this method exhibits effective variance reduction in various applications, including simulations, conditional time series forecasts, and image generation.</li>
</ul>

<h3>Title: Grad-ECLIP: Gradient-based Visual and Textual Explanations for CLIP</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Zhao, Kun Wang, Janet H. Hsiao, Antoni B. Chan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18816">https://arxiv.org/abs/2502.18816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18816">https://arxiv.org/pdf/2502.18816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18816]] Grad-ECLIP: Gradient-based Visual and Textual Explanations for CLIP(https://arxiv.org/abs/2502.18816)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Significant progress has been achieved on the improvement and downstream usages of the Contrastive Language-Image Pre-training (CLIP) vision-language model, while less attention is paid to the interpretation of CLIP. We propose a Gradient-based visual and textual Explanation method for CLIP (Grad-ECLIP), which interprets the matching result of CLIP for specific input image-text pair. By decomposing the architecture of the encoder and discovering the relationship between the matching similarity and intermediate spatial features, Grad-ECLIP produces effective heat maps that show the influence of image regions or words on the CLIP results. Different from the previous Transformer interpretation methods that focus on the utilization of self-attention maps, which are typically extremely sparse in CLIP, we produce high-quality visual explanations by applying channel and spatial weights on token features. Qualitative and quantitative evaluations verify the effectiveness and superiority of Grad-ECLIP compared with the state-of-the-art methods. Furthermore, a series of analysis are conducted based on our visual and textual explanation results, from which we explore the working mechanism of image-text matching, the strengths and limitations in attribution identification of CLIP, and the relationship between the concreteness/abstractness of a word and its usage in CLIP. Finally, based on the ability of explanation map that indicates text-specific saliency region of input image, we also propose an application with Grad-ECLIP, which is adopted to boost the fine-grained alignment in the CLIP fine-tuning. The code of Grad-ECLIP is available here: this https URL.</li>
</ul>

<h3>Title: Judge as A Judge: Improving the Evaluation of Retrieval-Augmented Generation through the Judge-Consistency of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shuliang Liu, Xinze Li, Zhenghao Liu, Yukun Yan, Cheng Yang, Zheni Zeng, Zhiyuan Liu, Maosong Sun, Ge Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18817">https://arxiv.org/abs/2502.18817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18817">https://arxiv.org/pdf/2502.18817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18817]] Judge as A Judge: Improving the Evaluation of Retrieval-Augmented Generation through the Judge-Consistency of Large Language Models(https://arxiv.org/abs/2502.18817)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has proven its effectiveness in alleviating hallucinations for Large Language Models (LLMs). However, existing automated evaluation metrics cannot fairly evaluate the outputs generated by RAG models during training and evaluation. LLM-based judgment models provide the potential to produce high-quality judgments, but they are highly sensitive to evaluation prompts, leading to inconsistencies when judging the output of RAG models. This paper introduces the Judge-Consistency (ConsJudge) method, which aims to enhance LLMs to generate more accurate evaluations for RAG models. Specifically, ConsJudge prompts LLMs to generate different judgments based on various combinations of judgment dimensions, utilize the judge-consistency to evaluate these judgments and select the accepted and rejected judgments for DPO training. Our experiments show that ConsJudge can effectively provide more accurate judgments for optimizing RAG models across various RAG models and datasets. Further analysis reveals that judgments generated by ConsJudge have a high agreement with the superior LLM. All codes are available at this https URL.</li>
</ul>

<h3>Title: CAMEx: Curvature-aware Merging of Experts</h3>
<ul>
<li><strong>Authors: </strong>Dung V. Nguyen, Minh H. Nguyen, Luc Q. Nguyen, Rachel S.Y. Teo, Tan M. Nguyen, Linh Duy Tran</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18821">https://arxiv.org/abs/2502.18821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18821">https://arxiv.org/pdf/2502.18821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18821]] CAMEx: Curvature-aware Merging of Experts(https://arxiv.org/abs/2502.18821)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing methods for merging experts during model training and fine-tuning predominantly rely on Euclidean geometry, which assumes a flat parameter space. This assumption can limit the model's generalization ability, especially during the pre-training phase, where the parameter manifold might exhibit more complex curvature. Curvature-aware merging methods typically require additional information and computational resources to approximate the Fisher Information Matrix, adding memory overhead. In this paper, we introduce CAMEx (\textbf{C}urvature-\textbf{A}ware \textbf{M}erging of \textbf{Ex}perts), a novel expert merging protocol that incorporates natural gradients to account for the non-Euclidean curvature of the parameter manifold. By leveraging natural gradients, CAMEx adapts more effectively to the structure of the parameter space, improving alignment between model updates and the manifold's geometry. This approach enhances both pre-training and fine-tuning, resulting in better optimization trajectories and improved generalization without the substantial memory overhead typically associated with curvature-aware methods. Our contributions are threefold: (1) CAMEx significantly outperforms traditional Euclidean-based expert merging techniques across various natural language processing tasks, leading to enhanced performance during pre-training and fine-tuning; (2) we introduce a dynamic merging architecture that optimizes resource utilization, achieving high performance while reducing computational costs, facilitating efficient scaling of large language models; and (3) we provide both theoretical and empirical evidence to demonstrate the efficiency of our proposed method.</li>
</ul>

<h3>Title: Evidence-Driven Marker Extraction for Social Media Suicide Risk Detection</h3>
<ul>
<li><strong>Authors: </strong>Carter Adams, Caleb Carter, Jackson Simmons</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18823">https://arxiv.org/abs/2502.18823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18823">https://arxiv.org/pdf/2502.18823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18823]] Evidence-Driven Marker Extraction for Social Media Suicide Risk Detection(https://arxiv.org/abs/2502.18823)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Early detection of suicide risk from social media text is crucial for timely intervention. While Large Language Models (LLMs) offer promising capabilities in this domain, challenges remain in terms of interpretability and computational efficiency. This paper introduces Evidence-Driven LLM (ED-LLM), a novel approach for clinical marker extraction and suicide risk classification. ED-LLM employs a multi-task learning framework, jointly training a Mistral-7B based model to identify clinical marker spans and classify suicide risk levels. This evidence-driven strategy enhances interpretability by explicitly highlighting textual evidence supporting risk assessments. Evaluated on the CLPsych datasets, ED-LLM demonstrates competitive performance in risk classification and superior capability in clinical marker span identification compared to baselines including fine-tuned LLMs, traditional machine learning, and prompt-based methods. The results highlight the effectiveness of multi-task learning for interpretable and efficient LLM-based suicide risk assessment, paving the way for clinically relevant applications.</li>
</ul>

<h3>Title: Sentiment Analysis of Movie Reviews Using BERT</h3>
<ul>
<li><strong>Authors: </strong>Gibson Nkhata, Usman Anjum, Justin Zhan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18841">https://arxiv.org/abs/2502.18841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18841">https://arxiv.org/pdf/2502.18841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18841]] Sentiment Analysis of Movie Reviews Using BERT(https://arxiv.org/abs/2502.18841)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Sentiment Analysis (SA) or opinion mining is analysis of emotions and opinions from any kind of text. SA helps in tracking peoples viewpoints and it is an important factor when it comes to social media monitoring product and brand recognition customer satisfaction customer loyalty advertising and promotions success and product acceptance. That is why SA is one of the active research areas in Natural Language Processing (NLP). SA is applied on data sourced from various media platforms to mine sentiment knowledge from them. Various approaches have been deployed in the literature to solve the problem. Most techniques devise complex and sophisticated frameworks in order to attain optimal accuracy. This work aims to finetune Bidirectional Encoder Representations from Transformers (BERT) with Bidirectional Long Short-Term Memory (BiLSTM) for movie reviews sentiment analysis and still provide better accuracy than the State-of-The-Art (SOTA) methods. The paper also shows how sentiment analysis can be applied if someone wants to recommend a certain movie for example by computing overall polarity of its sentiments predicted by the model. That is our proposed method serves as an upper-bound baseline in prediction of a predominant reaction to a movie. To compute overall polarity a heuristic algorithm is applied to BERTBiLSTM output vector. Our model can be extended to three-class four-class or any fine-grained classification and apply overall polarity computation again. This is intended to be exploited in future work.</li>
</ul>

<h3>Title: BarkXAI: A Lightweight Post-Hoc Explainable Method for Tree Species Classification with Quantifiable Concepts</h3>
<ul>
<li><strong>Authors: </strong>Yunmei Huang, Songlin Hou, Zachary Nelson Horve, Songlin Fei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18844">https://arxiv.org/abs/2502.18844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18844">https://arxiv.org/pdf/2502.18844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18844]] BarkXAI: A Lightweight Post-Hoc Explainable Method for Tree Species Classification with Quantifiable Concepts(https://arxiv.org/abs/2502.18844)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The precise identification of tree species is fundamental to forestry, conservation, and environmental monitoring. Though many studies have demonstrated that high accuracy can be achieved using bark-based species classification, these models often function as "black boxes", limiting interpretability, trust, and adoption in critical forestry applications. Attribution-based Explainable AI (XAI) methods have been used to address this issue in related works. However, XAI applications are often dependent on local features (such as a head shape or paw in animal applications) and cannot describe global visual features (such as ruggedness or smoothness) that are present in texture-dominant images such as tree bark. Concept-based XAI methods, on the other hand, offer explanations based on global visual features with concepts, but they tend to require large overhead in building external concept image datasets and the concepts can be vague and subjective without good means of precise quantification. To address these challenges, we propose a lightweight post-hoc method to interpret visual models for tree species classification using operators and quantifiable concepts. Our approach eliminates computational overhead, enables the quantification of complex concepts, and evaluates both concept importance and the model's reasoning process. To the best of our knowledge, our work is the first study to explain bark vision models in terms of global visual features with concepts. Using a human-annotated dataset as ground truth, our experiments demonstrate that our method significantly outperforms TCAV and Llama3.2 in concept importance ranking based on Kendall's Tau, highlighting its superior alignment with human perceptions.</li>
</ul>

<h3>Title: Sliding Window Attention Training for Efficient Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu, Xuetao Wei, Tong Xu, Xiangyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18845">https://arxiv.org/abs/2502.18845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18845">https://arxiv.org/pdf/2502.18845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18845]] Sliding Window Attention Training for Efficient Large Language Models(https://arxiv.org/abs/2502.18845)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in transformer-based Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their quadratic computational complexity concerning sequence length remains a significant bottleneck for processing long documents. As a result, many efforts like sparse attention and state space models have been proposed to improve the efficiency of LLMs over long sequences. Though effective, these approaches compromise the performance or introduce structural complexity. This calls for a simple yet efficient model that preserves the fundamental Transformer architecture. To this end, we introduce SWAT, which enables efficient long-context handling via Sliding Window Attention Training. This paper first attributes the inefficiency of Transformers to the attention sink phenomenon resulting from the high variance of softmax operation. Then, we replace softmax with the sigmoid function and utilize a balanced ALiBi and Rotary Position Embedding for efficient information compression and retention. Experiments demonstrate that SWAT achieves SOTA performance compared with state-of-the-art linear recurrent architectures on eight benchmarks. Code is available at this https URL.</li>
</ul>

<h3>Title: A Causal Lens for Evaluating Faithfulness Metrics</h3>
<ul>
<li><strong>Authors: </strong>Kerem Zaman, Shashank Srivastava</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18848">https://arxiv.org/abs/2502.18848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18848">https://arxiv.org/pdf/2502.18848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18848]] A Causal Lens for Evaluating Faithfulness Metrics(https://arxiv.org/abs/2502.18848)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's internal reasoning faithfully, which is crucial for understanding the model's true decision-making processes. Although several faithfulness metrics have been proposed, a unified evaluation framework remains absent. To address this gap, we present Causal Diagnosticity, a framework to evaluate faithfulness metrics for natural language explanations. Our framework employs the concept of causal diagnosticity, and uses model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate a variety of faithfulness metrics, including post-hoc explanation and chain-of-thought-based methods. We find that all tested faithfulness metrics often fail to surpass a random baseline. Our work underscores the need for improved metrics and more reliable interpretability methods in LLMs.</li>
</ul>

<h3>Title: Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code</h3>
<ul>
<li><strong>Authors: </strong>Jungin Kim, Shinwoo Park, Yo-Sub Han</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18851">https://arxiv.org/abs/2502.18851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18851">https://arxiv.org/pdf/2502.18851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18851]] Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code(https://arxiv.org/abs/2502.18851)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark</a></li>
<li><strong>Abstract: </strong>Code watermarking identifies AI-generated code by embedding patterns into the code during generation. Effective watermarking requires meeting two key conditions: the watermark should be reliably detectable, and the code should retain its original functionality. However, existing methods often modify tokens that are critical for program logic, such as keywords in conditional expressions or operators in arithmetic computations. These modifications can cause syntax errors or functional failures, limiting the practical use of watermarking. We present STONE, a method that preserves functional integrity by selectively inserting watermarks only into non-syntax tokens. By excluding tokens essential for code execution, STONE minimizes the risk of functional degradation. In addition, we introduce CWEM, a comprehensive evaluation metric that evaluates watermarking techniques based on correctness, detectability, and naturalness. While correctness and detectability have been widely used, naturalness remains underexplored despite its importance. Unnatural patterns can reveal the presence of a watermark, making it easier for adversaries to remove. We evaluate STONE using CWEM and compare its performance with the state-of-the-art approach. The results show that STONE achieves an average improvement of 7.69% in CWEM across Python, C++, and Java. Our code is available in this https URL.</li>
</ul>

<h3>Title: Exploring Rewriting Approaches for Different Conversational Tasks</h3>
<ul>
<li><strong>Authors: </strong>Md Mehrab Tanjim, Ryan A. Rossi, Mike Rimer, Xiang Chen, Sungchul Kim, Vaishnavi Muppala, Tong Yu, Zhengmian Hu, Ritwik Sinha, Wei Zhang, Iftikhar Ahamath Burhanuddin, Franck Dernoncourt</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18860">https://arxiv.org/abs/2502.18860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18860">https://arxiv.org/pdf/2502.18860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18860]] Exploring Rewriting Approaches for Different Conversational Tasks(https://arxiv.org/abs/2502.18860)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Conversational assistants often require a question rewriting algorithm that leverages a subset of past interactions to provide a more meaningful (accurate) answer to the user's question or request. However, the exact rewriting approach may often depend on the use case and application-specific tasks supported by the conversational assistant, among other constraints. In this paper, we systematically investigate two different approaches, denoted as rewriting and fusion, on two fundamentally different generation tasks, including a text-to-text generation task and a multimodal generative task that takes as input text and generates a visualization or data table that answers the user's question. Our results indicate that the specific rewriting or fusion approach highly depends on the underlying use case and generative task. In particular, we find that for a conversational question-answering assistant, the query rewriting approach performs best, whereas for a data analysis assistant that generates visualizations and data tables based on the user's conversation with the assistant, the fusion approach works best. Notably, we explore two datasets for the data analysis assistant use case, for short and long conversations, and we find that query fusion always performs better, whereas for the conversational text-based question-answering, the query rewrite approach performs best.</li>
</ul>

<h3>Title: Investigating Generalization of One-shot LLM Steering Vectors</h3>
<ul>
<li><strong>Authors: </strong>Jacob Dunefsky, Arman Cohan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18862">https://arxiv.org/abs/2502.18862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18862">https://arxiv.org/pdf/2502.18862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18862]] Investigating Generalization of One-shot LLM Steering Vectors(https://arxiv.org/abs/2502.18862)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Steering vectors have emerged as a promising approach for interpreting and controlling LLMs, but current methods typically require large contrastive datasets that are often impractical to construct and may capture spurious correlations. We propose directly optimizing steering vectors through gradient descent on a single training example, and systematically investigate how these vectors generalize. We consider several steering optimization techniques, including multiple novel ones, and find that the resulting vectors effectively mediate safety-relevant behaviors in multiple models. Indeed, in experiments on an alignment-faking model, we are able to optimize one-shot steering vectors that induce harmful behavior on benign examples and whose negations suppress harmful behavior on malign examples. And in experiments on refusal suppression, we demonstrate that one-shot optimized steering vectors can transfer across inputs, yielding a Harmbench attack success rate of 96.9%. Furthermore, to quantitatively assess steering effectiveness in instruction-tuned models, we develop a novel evaluation framework using sequence probabilities from the corresponding base model. With this framework, we analyze how steering vectors modulate an instruction-tuned LLM's ability to recover from outputting false information, and find that this ability derives from the base model. Overall, our findings suggest that optimizing steering vectors on a single example can mediate misaligned behavior in LLMs, and provide a path toward better understanding the relationship between LLM behavior and activation space structure.</li>
</ul>

<h3>Title: Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM</h3>
<ul>
<li><strong>Authors: </strong>Junxiao Ma, Jingjing Wang, Jiamin Luo, Peiying Yu, Guodong Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18863">https://arxiv.org/abs/2502.18863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18863">https://arxiv.org/pdf/2502.18863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18863]] Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM(https://arxiv.org/abs/2502.18863)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Prior studies on Video Anomaly Detection (VAD) mainly focus on detecting whether each video frame is abnormal or not in the video, which largely ignore the structured video semantic information (i.e., what, when, and where does the abnormal event happen). With this in mind, we propose a new chat-paradigm \textbf{M}ulti-scene Video Abnormal Event Extraction and Localization (M-VAE) task, aiming to extract the abnormal event quadruples (i.e., subject, event type, object, scene) and localize such event. Further, this paper believes that this new task faces two key challenges, i.e., global-local spatial modeling and global-local spatial balancing. To this end, this paper proposes a Global-local Spatial-sensitive Large Language Model (LLM) named Sherlock, i.e., acting like Sherlock Holmes to track down the criminal events, for this M-VAE task. Specifically, this model designs a Global-local Spatial-enhanced MoE (GSM) module and a Spatial Imbalance Regulator (SIR) to address the two challenges respectively. Extensive experiments on our M-VAE instruction dataset show the significant advantages of Sherlock over several advanced Video-LLMs. This justifies the importance of global-local spatial information for the M-VAE task and the effectiveness of Sherlock in capturing such information.</li>
</ul>

<h3>Title: A Theoretical Perspective: How to Prevent Model Collapse in Self-consuming Training Loops</h3>
<ul>
<li><strong>Authors: </strong>Shi Fu, Yingjie Wang, Yuzhu Chen, Xinmei Tian, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18865">https://arxiv.org/abs/2502.18865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18865">https://arxiv.org/pdf/2502.18865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18865]] A Theoretical Perspective: How to Prevent Model Collapse in Self-consuming Training Loops(https://arxiv.org/abs/2502.18865)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>High-quality data is essential for training large generative models, yet the vast reservoir of real data available online has become nearly depleted. Consequently, models increasingly generate their own data for further training, forming Self-consuming Training Loops (STLs). However, the empirical results have been strikingly inconsistent: some models degrade or even collapse, while others successfully avoid these failures, leaving a significant gap in theoretical understanding to explain this discrepancy. This paper introduces the intriguing notion of recursive stability and presents the first theoretical generalization analysis, revealing how both model architecture and the proportion between real and synthetic data influence the success of STLs. We further extend this analysis to transformers in in-context learning, showing that even a constant-sized proportion of real data ensures convergence, while also providing insights into optimal synthetic data sizing.</li>
</ul>

<h3>Title: Enhanced Transformer-Based Tracking for Skiing Events: Overcoming Multi-Camera Challenges, Scale Variations and Rapid Motion -- SkiTB Visual Tracking Challenge 2025</h3>
<ul>
<li><strong>Authors: </strong>Akhil Penta, Vaibhav Adwani, Ankush Chopra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18867">https://arxiv.org/abs/2502.18867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18867">https://arxiv.org/pdf/2502.18867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18867]] Enhanced Transformer-Based Tracking for Skiing Events: Overcoming Multi-Camera Challenges, Scale Variations and Rapid Motion -- SkiTB Visual Tracking Challenge 2025(https://arxiv.org/abs/2502.18867)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate skier tracking is essential for performance analysis, injury prevention, and optimizing training strategies in alpine sports. Traditional tracking methods often struggle with occlusions, dynamic movements, and varying environmental conditions, limiting their effectiveness. In this work, we used STARK (Spatio-Temporal Transformer Network for Visual Tracking), a transformer-based model, to track skiers. We adapted STARK to address domain-specific challenges such as camera movements, camera changes, occlusions, etc. by optimizing the model's architecture and hyperparameters to better suit the dataset.</li>
</ul>

<h3>Title: Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework</h3>
<ul>
<li><strong>Authors: </strong>Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18874">https://arxiv.org/abs/2502.18874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18874">https://arxiv.org/pdf/2502.18874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18874]] Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework(https://arxiv.org/abs/2502.18874)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are being used more and more extensively for automated evaluation in various scenarios. Previous studies have attempted to fine-tune open-source LLMs to replicate the evaluation explanations and judgments of powerful proprietary models, such as GPT-4. However, these methods are largely limited to text-based analyses under predefined general criteria, resulting in reduced adaptability for unseen instructions and demonstrating instability in evaluating adherence to quantitative and structural constraints. To address these limitations, we propose a novel evaluation framework, ARJudge, that adaptively formulates evaluation criteria and synthesizes both text-based and code-driven analyses to evaluate LLM responses. ARJudge consists of two components: a fine-tuned Analyzer that generates multi-faceted evaluation analyses and a tuning-free Refiner that combines and refines all analyses to make the final judgment. We construct a Composite Analysis Corpus that integrates tasks for evaluation criteria generation alongside text-based and code-driven analysis generation to train the Analyzer. Our results demonstrate that ARJudge outperforms existing fine-tuned evaluators in effectiveness and robustness. Furthermore, it demonstrates the importance of multi-faceted evaluation and code-driven analyses in enhancing evaluation capabilities.</li>
</ul>

<h3>Title: Learning to Generate Structured Output with Schema Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yaxi Lu, Haolun Li, Xin Cong, Zhong Zhang, Yesai Wu, Yankai Lin, Zhiyuan Liu, Fangming Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18878">https://arxiv.org/abs/2502.18878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18878">https://arxiv.org/pdf/2502.18878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18878]] Learning to Generate Structured Output with Schema Reinforcement Learning(https://arxiv.org/abs/2502.18878)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study investigates the structured generation capabilities of large language models (LLMs), focusing on producing valid JSON outputs against a given schema. Despite the widespread use of JSON in integrating language models with programs, there is a lack of comprehensive analysis and benchmarking of these capabilities. We explore various aspects of JSON generation, such as structure understanding, escaping, and natural language description, to determine how to assess and enable LLMs to generate valid responses. Building upon this, we propose SchemaBench features around 40K different JSON schemas to obtain and assess models' abilities in generating valid JSON. We find that the latest LLMs are still struggling to generate a valid JSON string. Moreover, we demonstrate that incorporating reinforcement learning with a Fine-grained Schema Validator can further enhance models' understanding of JSON schema, leading to improved performance. Our models demonstrate significant improvement in both generating JSON outputs and downstream tasks.</li>
</ul>

<h3>Title: On Pruning State-Space LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tamer Ghattas, Michael Hassid, Roy Schwartz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18886">https://arxiv.org/abs/2502.18886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18886">https://arxiv.org/pdf/2502.18886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18886]] On Pruning State-Space LLMs(https://arxiv.org/abs/2502.18886)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Recent work proposed state-space models (SSMs) as an efficient alternative to transformer-based LLMs. Can these models be pruned to further reduce their computation costs? We adapt several pruning methods to the SSM structure, and apply them to four SSM-based LLMs across multiple tasks. We find that such models are quite robust to some pruning methods (e.g. WANDA), while using other methods lead to fast performance degradation.</li>
</ul>

<h3>Title: From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens</h3>
<ul>
<li><strong>Authors: </strong>Tong Wu, Junzhe Shen, Zixia Jia, Yuxuan Wang, Zilong Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18890">https://arxiv.org/abs/2502.18890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18890">https://arxiv.org/pdf/2502.18890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18890]] From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens(https://arxiv.org/abs/2502.18890)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Generating ultra-long sequences with large language models (LLMs) has become increasingly crucial but remains a highly time-intensive task, particularly for sequences up to 100K tokens. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate the process and can be detrimental. Through an in-depth analysis, we identify three major challenges hindering efficient generation: frequent model reloading, dynamic key-value (KV) management and repetitive generation. To address these issues, we introduce TOKENSWIFT, a novel framework designed to substantially accelerate the generation process of ultra-long sequences while maintaining the target model's inherent quality. Experimental results demonstrate that TOKENSWIFT achieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA). This acceleration translates to hours of time savings for ultra-long sequence generation, establishing TOKENSWIFT as a scalable and effective solution at unprecedented lengths. Code can be found at this https URL.</li>
</ul>

<h3>Title: Dynamic Classification: Leveraging Self-Supervised Classification to Enhance Prediction Performance</h3>
<ul>
<li><strong>Authors: </strong>Ziyuan Zhong, Junyang Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18891">https://arxiv.org/abs/2502.18891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18891">https://arxiv.org/pdf/2502.18891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18891]] Dynamic Classification: Leveraging Self-Supervised Classification to Enhance Prediction Performance(https://arxiv.org/abs/2502.18891)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we propose an innovative dynamic classification algorithm designed to achieve the objective of zero missed detections and minimal false positives. The algorithm partitions the data into N equivalent training subsets and N prediction subsets using a supervised model, followed by independent predictions from N separate predictive models. This enables each predictive model to operate within a smaller data range, thereby improving overall accuracy. Additionally, the algorithm leverages data generated through supervised learning to further refine prediction results, filtering out predictions that do not meet accuracy requirements without the need to introduce additional models. Experimental results demonstrate that, when data partitioning errors are minimal, the dynamic classification algorithm achieves exceptional performance with zero missed detections and minimal false positives, significantly outperforming existing model ensembles. Even in cases where classification errors are larger, the algorithm remains comparable to state of the art models. The key innovations of this study include self-supervised classification learning, the use of small-range subset predictions, and the direct rejection of substandard predictions. While the current algorithm still has room for improvement in terms of automatic parameter tuning and classification model efficiency, it has demonstrated outstanding performance across multiple datasets. Future research will focus on optimizing the classification component to further enhance the algorithm's robustness and adaptability.</li>
</ul>

<h3>Title: A Pipeline of Augmentation and Sequence Embedding for Classification of Imbalanced Network Traffic</h3>
<ul>
<li><strong>Authors: </strong>Matin Shokri, Ramin Hasibi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18909">https://arxiv.org/abs/2502.18909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18909">https://arxiv.org/pdf/2502.18909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18909]] A Pipeline of Augmentation and Sequence Embedding for Classification of Imbalanced Network Traffic(https://arxiv.org/abs/2502.18909)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Network Traffic Classification (NTC) is one of the most important tasks in network management. The imbalanced nature of classes on the internet presents a critical challenge in classification tasks. For example, some classes of applications are much more prevalent than others, such as HTTP. As a result, machine learning classification models do not perform well on those classes with fewer data. To address this problem, we propose a pipeline to balance the dataset and classify it using a robust and accurate embedding technique. First, we generate artificial data using Long Short-Term Memory (LSTM) networks and Kernel Density Estimation (KDE). Next, we propose replacing one-hot encoding for categorical features with a novel embedding framework based on the "Flow as a Sentence" perspective, which we name FS-Embedding. This framework treats the source and destination ports, along with the packet's direction, as one word in a flow, then trains an embedding vector space based on these new features through the learning classification task. Finally, we compare our pipeline with the training of a Convolutional Recurrent Neural Network (CRNN) and Transformers, both with imbalanced and sampled datasets, as well as with the one-hot encoding approach. We demonstrate that the proposed augmentation pipeline, combined with FS-Embedding, increases convergence speed and leads to a significant reduction in the number of model parameters, all while maintaining the same performance in terms of accuracy.</li>
</ul>

<h3>Title: CLLoRA: An Approach to Measure the Effects of the Context Length for LLM Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Ping Zhang, Zhaorui Zhang, Sheng Di, Yao Xin, Benben Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18910">https://arxiv.org/abs/2502.18910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18910">https://arxiv.org/pdf/2502.18910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18910]] CLLoRA: An Approach to Measure the Effects of the Context Length for LLM Fine-Tuning(https://arxiv.org/abs/2502.18910)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>Large language model fine-tuning has been identified as an efficient approach to applying the pre-trained Large language models to other domains. To guarantee data privacy for different data owners, models are often fine-tuned in federated learning environments across different data owners, which often involve data heterogeneity issues and affect the fine-tuning performance. In addition, the length of the context for the training data has been identified as a major factor that affects the LLM's model performance. To efficiently measure how the context length affects the LLM's model performance in heterogeneous federated learning environments, we propose CLLoRA. CLLoRA utilizes the parameter-efficient fine-tuning approach LoRA based on different kinds of LLMs with varying sizes as the fine-tuning approach to investigate whether the quality and length of contexts can serve as standards for measuring non-IID context. The findings indicate that an imbalance in context quality not only affects local training on clients but also impacts the global model's performance. However, context length has a minimal effect on local training but a more significant influence on the global model. These results provide insights into how context quality and length affect the model performance for LLM fine-tuning in federated learning environments.</li>
</ul>

<h3>Title: CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English Code-Switching Dialogues for Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Zhou, Yujie Guo, Shiwan Zhao, Haoqin Sun, Hui Wang, Jiabei He, Aobo Kong, Shiyao Wang, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18913">https://arxiv.org/abs/2502.18913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18913">https://arxiv.org/pdf/2502.18913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18913]] CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English Code-Switching Dialogues for Speech Recognition(https://arxiv.org/abs/2502.18913)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Code-switching (CS), the alternation between two or more languages within a single conversation, presents significant challenges for automatic speech recognition (ASR) systems. Existing Mandarin-English code-switching datasets often suffer from limitations in size, spontaneity, and the lack of full-length dialogue recordings with transcriptions, hindering the development of robust ASR models for real-world conversational scenarios. This paper introduces CS-Dialogue, a novel large-scale Mandarin-English code-switching speech dataset comprising 104 hours of spontaneous conversations from 200 speakers. Unlike previous datasets, CS-Dialogue provides full-length dialogue recordings with complete transcriptions, capturing naturalistic code-switching patterns in continuous speech. We describe the data collection and annotation processes, present detailed statistics of the dataset, and establish benchmark ASR performance using state-of-the-art models. Our experiments, using Transformer, Conformer, and Branchformer, demonstrate the challenges of code-switching ASR, and show that existing pre-trained models such as Whisper still have the space to improve. The CS-Dialogue dataset will be made freely available for all academic purposes.</li>
</ul>

<h3>Title: END: Early Noise Dropping for Efficient and Effective Context Denoising</h3>
<ul>
<li><strong>Authors: </strong>Hongye Jin, Pei Chen, Jingfeng Yang, Zhengyang Wang, Meng Jiang, Yifan Gao, Binxuan Huang, Xinyang Zhang, Zheng Li, Tianyi Liu, Huasheng Li, Bing Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18915">https://arxiv.org/abs/2502.18915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18915">https://arxiv.org/pdf/2502.18915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18915]] END: Early Noise Dropping for Efficient and Effective Context Denoising(https://arxiv.org/abs/2502.18915)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, they are often distracted by irrelevant or noisy context in input sequences that degrades output quality. This problem affects both long- and short-context scenarios, such as retrieval-augmented generation, table question-answering, and in-context learning. We reveal that LLMs can implicitly identify whether input sequences contain useful information at early layers, prior to token generation. Leveraging this insight, we introduce Early Noise Dropping (\textsc{END}), a novel approach to mitigate this issue without requiring fine-tuning the LLMs. \textsc{END} segments input sequences into chunks and employs a linear prober on the early layers of LLMs to differentiate between informative and noisy chunks. By discarding noisy chunks early in the process, \textsc{END} preserves critical information, reduces distraction, and lowers computational overhead. Extensive experiments demonstrate that \textsc{END} significantly improves both performance and efficiency across different LLMs on multiple evaluation datasets. Furthermore, by investigating LLMs' implicit understanding to the input with the prober, this work also deepens understanding of how LLMs do reasoning with contexts internally.</li>
</ul>

<h3>Title: Brain-inspired analogical mixture prototypes for few-shot class-incremental learning</h3>
<ul>
<li><strong>Authors: </strong>Wanyi Li, Wei Wei, Yongkang Luo, Peng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18923">https://arxiv.org/abs/2502.18923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18923">https://arxiv.org/pdf/2502.18923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18923]] Brain-inspired analogical mixture prototypes for few-shot class-incremental learning(https://arxiv.org/abs/2502.18923)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Few-shot class-incremental learning (FSCIL) poses significant challenges for artificial neural networks due to the need to efficiently learn from limited data while retaining knowledge of previously learned tasks. Inspired by the brain's mechanisms for categorization and analogical learning, we propose a novel approach called Brain-inspired Analogical Mixture Prototypes (BAMP). BAMP has three components: mixed prototypical feature learning, statistical analogy, and soft voting. Starting from a pre-trained Vision Transformer (ViT), mixed prototypical feature learning represents each class using a mixture of prototypes and fine-tunes these representations during the base session. The statistical analogy calibrates the mean and covariance matrix of prototypes for new classes according to similarity to the base classes, and computes classification score with Mahalanobis distance. Soft voting combines both merits of statistical analogy and an off-shelf FSCIL method. Our experiments on benchmark datasets demonstrate that BAMP outperforms state-of-the-art on both traditional big start FSCIL setting and challenging small start FSCIL setting. The study suggests that brain-inspired analogical mixture prototypes can alleviate catastrophic forgetting and over-fitting problems in FSCIL.</li>
</ul>

<h3>Title: BeamVQ: Beam Search with Vector Quantization to Mitigate Data Scarcity in Physical Spatiotemporal Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Weiyan Wang, Xingjian Shi, Ruiqi Shu, Yuan Gao, Rui Ray Chen, Kun Wang, Fan Xu, Jinbao Xue, Shuaipeng Li, Yangyu Tao, Di Wang, Hao Wu, Xiaomeng Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18925">https://arxiv.org/abs/2502.18925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18925">https://arxiv.org/pdf/2502.18925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18925]] BeamVQ: Beam Search with Vector Quantization to Mitigate Data Scarcity in Physical Spatiotemporal Forecasting(https://arxiv.org/abs/2502.18925)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In practice, physical spatiotemporal forecasting can suffer from data scarcity, because collecting large-scale data is non-trivial, especially for extreme events. Hence, we propose \method{}, a novel probabilistic framework to realize iterative self-training with new self-ensemble strategies, achieving better physical consistency and generalization on extreme events. Following any base forecasting model, we can encode its deterministic outputs into a latent space and retrieve multiple codebook entries to generate probabilistic outputs. Then BeamVQ extends the beam search from discrete spaces to the continuous state spaces in this field. We can further employ domain-specific metrics (e.g., Critical Success Index for extreme events) to filter out the top-k candidates and develop the new self-ensemble strategy by combining the high-quality candidates. The self-ensemble can not only improve the inference quality and robustness but also iteratively augment the training datasets during continuous self-training. Consequently, BeamVQ realizes the exploration of rare but critical phenomena beyond the original dataset. Comprehensive experiments on different benchmarks and backbones show that BeamVQ consistently reduces forecasting MSE (up to 39%), enhancing extreme events detection and proving its effectiveness in handling data scarcity.</li>
</ul>

<h3>Title: JailBench: A Comprehensive Chinese Security Assessment Benchmark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shuyi Liu, Simiao Cui, Haoran Bu, Yuming Shang, Xi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18935">https://arxiv.org/abs/2502.18935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18935">https://arxiv.org/pdf/2502.18935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18935]] JailBench: A Comprehensive Chinese Security Assessment Benchmark for Large Language Models(https://arxiv.org/abs/2502.18935)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities across various applications, highlighting the urgent need for comprehensive safety evaluations. In particular, the enhanced Chinese language proficiency of LLMs, combined with the unique characteristics and complexity of Chinese expressions, has driven the emergence of Chinese-specific benchmarks for safety assessment. However, these benchmarks generally fall short in effectively exposing LLM safety vulnerabilities. To address the gap, we introduce JailBench, the first comprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in LLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese context. To improve generation efficiency, we employ a novel Automatic Jailbreak Prompt Engineer (AJPE) framework for JailBench construction, which incorporates jailbreak techniques to enhance assessing effectiveness and leverages LLMs to automatically scale up the dataset through context-learning. The proposed JailBench is extensively evaluated over 13 mainstream LLMs and achieves the highest attack success rate against ChatGPT compared to existing Chinese benchmarks, underscoring its efficacy in identifying latent vulnerabilities in LLMs, as well as illustrating the substantial room for improvement in the security and trustworthiness of LLMs within the Chinese context. Our benchmark is publicly available at this https URL.</li>
</ul>

<h3>Title: Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yu He, Boheng Li, Liu Liu, Zhongjie Ba, Wei Dong, Yiming Li, Zhan Qin, Kui Ren, Chun Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18943">https://arxiv.org/abs/2502.18943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18943">https://arxiv.org/pdf/2502.18943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18943]] Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models(https://arxiv.org/abs/2502.18943)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Membership Inference Attacks (MIAs) aim to predict whether a data sample belongs to the model's training set or not. Although prior research has extensively explored MIAs in Large Language Models (LLMs), they typically require accessing to complete output logits (\ie, \textit{logits-based attacks}), which are usually not available in practice. In this paper, we study the vulnerability of pre-trained LLMs to MIAs in the \textit{label-only setting}, where the adversary can only access generated tokens (text). We first reveal that existing label-only MIAs have minor effects in attacking pre-trained LLMs, although they are highly effective in inferring fine-tuning datasets used for personalized LLMs. We find that their failure stems from two main reasons, including better generalization and overly coarse perturbation. Specifically, due to the extensive pre-training corpora and exposing each sample only a few times, LLMs exhibit minimal robustness differences between members and non-members. This makes token-level perturbations too coarse to capture such differences. To alleviate these problems, we propose \textbf{PETAL}: a label-only membership inference attack based on \textbf{PE}r-\textbf{T}oken sem\textbf{A}ntic simi\textbf{L}arity. Specifically, PETAL leverages token-level semantic similarity to approximate output probabilities and subsequently calculate the perplexity. It finally exposes membership based on the common assumption that members are `better' memorized and have smaller perplexity. We conduct extensive experiments on the WikiMIA benchmark and the more challenging MIMIR benchmark. Empirically, our PETAL performs better than the extensions of existing label-only attacks against personalized LLMs and even on par with other advanced logit-based attacks across all metrics on five prevalent open-source LLMs.</li>
</ul>

<h3>Title: Switching multiplicative watermark design against covert attacks</h3>
<ul>
<li><strong>Authors: </strong>Alexander J. Gallo, Sribalaji C. Anand, Andr M. H. Teixeira, Riccardo M. G. Ferrari</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18948">https://arxiv.org/abs/2502.18948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18948">https://arxiv.org/pdf/2502.18948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18948]] Switching multiplicative watermark design against covert attacks(https://arxiv.org/abs/2502.18948)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, watermark</a></li>
<li><strong>Abstract: </strong>Active techniques have been introduced to give better detectability performance for cyber-attack diagnosis in cyber-physical systems (CPS). In this paper, switching multiplicative watermarking is considered, whereby we propose an optimal design strategy to define switching filter parameters. Optimality is evaluated exploiting the so-called output-to-output gain of the closed loop system, including some supposed attack dynamics. A worst-case scenario of a matched covert attack is assumed, presuming that an attacker with full knowledge of the closed-loop system injects a stealthy attack of bounded energy. Our algorithm, given watermark filter parameters at some time instant, provides optimal next-step parameters. Analysis of the algorithm is given, demonstrating its features, and demonstrating that through initialization of certain parameters outside of the algorithm, the parameters of the multiplicative watermarking can be randomized. Simulation shows how, by adopting our method for parameter design, the attacker's impact on performance diminishes.</li>
</ul>

<h3>Title: Nonparametric Heterogeneous Long-term Causal Effect Estimation via Data Combination</h3>
<ul>
<li><strong>Authors: </strong>Weilin Chen, Ruichu Cai, Junjie Wan, Zeqin Yang, Jos Miguel Hernndez-Lobato</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18960">https://arxiv.org/abs/2502.18960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18960">https://arxiv.org/pdf/2502.18960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18960]] Nonparametric Heterogeneous Long-term Causal Effect Estimation via Data Combination(https://arxiv.org/abs/2502.18960)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Long-term causal inference has drawn increasing attention in many scientific domains. Existing methods mainly focus on estimating average long-term causal effects by combining long-term observational data and short-term experimental data. However, it is still understudied how to robustly and effectively estimate heterogeneous long-term causal effects, significantly limiting practical applications. In this paper, we propose several two-stage style nonparametric estimators for heterogeneous long-term causal effect estimation, including propensity-based, regression-based, and multiple robust estimators. We conduct a comprehensive theoretical analysis of their asymptotic properties under mild assumptions, with the ultimate goal of building a better understanding of the conditions under which some estimators can be expected to perform better. Extensive experiments across several semi-synthetic and real-world datasets validate the theoretical results and demonstrate the effectiveness of the proposed estimators.</li>
</ul>

<h3>Title: Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles</h3>
<ul>
<li><strong>Authors: </strong>Kuang Wang, Xianfei Li, Shenghao Yang, Li Zhou, Feng Jiang, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18968">https://arxiv.org/abs/2502.18968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18968">https://arxiv.org/pdf/2502.18968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18968]] Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles(https://arxiv.org/abs/2502.18968)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>User simulators are crucial for replicating human interactions with dialogue systems, supporting both collaborative training and automatic evaluation, especially for large language models (LLMs). However, existing simulators often rely solely on text utterances, missing implicit user traits such as personality, speaking style, and goals. In contrast, persona-based methods lack generalizability, as they depend on predefined profiles of famous individuals or archetypes. To address these challenges, we propose User Simulator with implicit Profiles (USP), a framework that infers implicit user profiles from human-machine conversations and uses them to generate more personalized and realistic dialogues. We first develop an LLM-driven extractor with a comprehensive profile schema. Then, we refine the simulation through conditional supervised fine-tuning and reinforcement learning with cycle consistency, optimizing it at both the utterance and conversation levels. Finally, we adopt a diverse profile sampler to capture the distribution of real-world user profiles. Experimental results demonstrate that USP outperforms strong baselines in terms of authenticity and diversity while achieving comparable performance in consistency. Furthermore, dynamic multi-turn evaluations based on USP strongly align with mainstream benchmarks, demonstrating its effectiveness in real-world applications.</li>
</ul>

<h3>Title: Invariance Pair-Guided Learning: Enhancing Robustness in Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Martin Surner, Abdelmajid Khelil, Ludwig Bothmann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18975">https://arxiv.org/abs/2502.18975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18975">https://arxiv.org/pdf/2502.18975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18975]] Invariance Pair-Guided Learning: Enhancing Robustness in Neural Networks(https://arxiv.org/abs/2502.18975)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Out-of-distribution generalization of machine learning models remains challenging since the models are inherently bound to the training data distribution. This especially manifests, when the learned models rely on spurious correlations. Most of the existing approaches apply data manipulation, representation learning, or learning strategies to achieve generalizable models. Unfortunately, these approaches usually require multiple training domains, group labels, specialized augmentation, or pre-processing to reach generalizable models. We propose a novel approach that addresses these limitations by providing a technique to guide the neural network through the training phase. We first establish input pairs, representing the spurious attribute and describing the invariance, a characteristic that should not affect the outcome of the model. Based on these pairs, we form a corrective gradient complementing the traditional gradient descent approach. We further make this correction mechanism adaptive based on a predefined invariance condition. Experiments on ColoredMNIST, Waterbird-100, and CelebA datasets demonstrate the effectiveness of our approach and the robustness to group shifts.</li>
</ul>

<h3>Title: Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Hongyi Cal, ie Li, Wenzhen Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18978">https://arxiv.org/abs/2502.18978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18978">https://arxiv.org/pdf/2502.18978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18978]] Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning(https://arxiv.org/abs/2502.18978)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The effectiveness of instruction fine-tuning for Large Language Models is fundamentally constrained by the quality and efficiency of training datasets. This work introduces Low-Confidence Gold (LCG), a novel filtering framework that employs centroid-based clustering and confidence-guided selection for identifying valuable instruction pairs. Through a semi-supervised approach using a lightweight classifier trained on representative samples, LCG curates high-quality subsets while preserving data diversity. Experimental evaluation demonstrates that models fine-tuned on LCG-filtered subsets of 6K samples achieve superior performance compared to existing methods, with substantial improvements on MT-bench and consistent gains across comprehensive evaluation metrics. The framework's efficacy while maintaining model performance establishes a promising direction for efficient instruction tuning.</li>
</ul>

<h3>Title: PEToolLLM: Towards Personalized Tool Learning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qiancheng Xu, Yongqi Li, Heming Xia, Fan Liu, Min Yang, Wenjie Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18980">https://arxiv.org/abs/2502.18980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18980">https://arxiv.org/pdf/2502.18980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18980]] PEToolLLM: Towards Personalized Tool Learning in Large Language Models(https://arxiv.org/abs/2502.18980)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tool learning has emerged as a promising direction by extending Large Language Models' (LLMs) capabilities with external tools. Existing tool learning studies primarily focus on the general-purpose tool-use capability, which addresses explicit user requirements in instructions. However, they overlook the importance of personalized tool-use capability, leading to an inability to handle implicit user preferences. To address the limitation, we first formulate the task of personalized tool learning, which integrates user's interaction history towards personalized tool usage. To fill the gap of missing benchmarks, we construct PEToolBench, featuring diverse user preferences reflected in interaction history under three distinct personalized settings, and encompassing a wide range of tool-use scenarios. Moreover, we propose a framework PEToolLLaMA to adapt LLMs to the personalized tool learning task, which is trained through supervised fine-tuning and direct preference optimization. Extensive experiments on PEToolBench demonstrate the superiority of PEToolLLaMA over existing LLMs.</li>
</ul>

<h3>Title: Enhanced Neuromorphic Semantic Segmentation Latency through Stream Event</h3>
<ul>
<li><strong>Authors: </strong>D. Hareb, J. Martinet, B. Miramond</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18982">https://arxiv.org/abs/2502.18982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18982">https://arxiv.org/pdf/2502.18982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18982]] Enhanced Neuromorphic Semantic Segmentation Latency through Stream Event(https://arxiv.org/abs/2502.18982)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Achieving optimal semantic segmentation with frame-based vision sensors poses significant challenges for real-time systems like UAVs and self-driving cars, which require rapid and precise processing. Traditional frame-based methods often struggle to balance latency, accuracy, and energy efficiency. To address these challenges, we leverage event streams from event-based cameras-bio-inspired sensors that trigger events in response to changes in the scene. Specifically, we analyze the number of events triggered between successive frames, with a high number indicating significant changes and a low number indicating minimal changes. We exploit this event information to solve the semantic segmentation task by employing a Spiking Neural Network (SNN), a bio-inspired computing paradigm known for its low energy consumption. Our experiments on the DSEC dataset show that our approach significantly reduces latency with only a limited drop in accuracy. Additionally, by using SNNs, we achieve low power consumption, making our method suitable for energy-constrained real-time applications. To the best of our knowledge, our approach is the first to effectively balance reduced latency, minimal accuracy loss, and energy efficiency using events stream to enhance semantic segmentation in dynamic and resource-limited environments.</li>
</ul>

<h3>Title: Evaluating Membership Inference Attacks in heterogeneous-data setups</h3>
<ul>
<li><strong>Authors: </strong>Bram van Dartel, Marc Damie, Florian Hahn</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18986">https://arxiv.org/abs/2502.18986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18986">https://arxiv.org/pdf/2502.18986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18986]] Evaluating Membership Inference Attacks in heterogeneous-data setups(https://arxiv.org/abs/2502.18986)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Among all privacy attacks against Machine Learning (ML), membership inference attacks (MIA) attracted the most attention. In these attacks, the attacker is given an ML model and a data point, and they must infer whether the data point was used for training. The attacker also has an auxiliary dataset to tune their inference algorithm. Attack papers commonly simulate setups in which the attacker's and the target's datasets are sampled from the same distribution. This setting is convenient to perform experiments, but it rarely holds in practice. ML literature commonly starts with similar simplifying assumptions (i.e., "i.i.d." datasets), and later generalizes the results to support heterogeneous data distributions. Similarly, our work makes a first step in the generalization of the MIA evaluation to heterogeneous data. First, we design a metric to measure the heterogeneity between any pair of tabular data distributions. This metric provides a continuous scale to analyze the phenomenon. Second, we compare two methodologies to simulate a data heterogeneity between the target and the attacker. These setups provide opposite performances: 90% attack accuracy vs. 50% (i.e., random guessing). Our results show that the MIA accuracy depends on the experimental setup; and even if research on MIA considers heterogeneous data setups, we have no standardized baseline of how to simulate it. The lack of such a baseline for MIA experiments poses a significant challenge to risk assessments in real-world machine learning scenarios.</li>
</ul>

<h3>Title: GenTool: Enhancing Tool Generalization in Language Models through Zero-to-One and Weak-to-Strong Simulation</h3>
<ul>
<li><strong>Authors: </strong>Jie He, Jennifer Neville, Mengting Wan, Longqi Yang, Hui Liu, Xiaofeng Xu, Xia Song, Jeff Z. Pan, Pei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18990">https://arxiv.org/abs/2502.18990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18990">https://arxiv.org/pdf/2502.18990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18990]] GenTool: Enhancing Tool Generalization in Language Models through Zero-to-One and Weak-to-Strong Simulation(https://arxiv.org/abs/2502.18990)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can enhance their capabilities as AI assistants by integrating external tools, allowing them to access a wider range of information. While recent LLMs are typically fine-tuned with tool usage examples during supervised fine-tuning (SFT), questions remain about their ability to develop robust tool-usage skills and can effectively generalize to unseen queries and tools. In this work, we present GenTool, a novel training framework that prepares LLMs for diverse generalization challenges in tool utilization. Our approach addresses two fundamental dimensions critical for real-world applications: Zero-to-One Generalization, enabling the model to address queries initially lacking a suitable tool by adopting and utilizing one when it becomes available, and Weak-to-Strong Generalization, allowing models to leverage enhanced versions of existing tools to solve queries. To achieve this, we develop synthetic training data simulating these two dimensions of tool usage and introduce a two-stage fine-tuning approach: optimizing tool ranking, then refining tool selection. Through extensive experiments across four generalization scenarios, we demonstrate that our method significantly enhances the tool-usage capabilities of LLMs ranging from 1B to 8B parameters, achieving performance that surpasses GPT-4o. Furthermore, our analysis also provides valuable insights into the challenges LLMs encounter in tool generalization.</li>
</ul>

<h3>Title: MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Teng Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.18993">https://arxiv.org/abs/2502.18993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.18993">https://arxiv.org/pdf/2502.18993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.18993]] MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering(https://arxiv.org/abs/2502.18993)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Multi-entity question answering (MEQA) represents significant challenges for large language models (LLM) and retrieval-augmented generation (RAG) systems, which frequently struggle to consolidate scattered information across diverse documents. While existing methods excel at single-document comprehension, they often struggle with cross-document aggregation, particularly when resolving entity-dense questions like "What is the distribution of ACM Fellows among various fields of study?", which require integrating entity-centric insights from heterogeneous sources (e.g., Wikipedia pages). To address this gap, we introduce MEBench, a novel multi-document, multi-entity benchmark designed to systematically evaluate LLMs' capacity to retrieve, consolidate, and reason over fragmented information. Our benchmark comprises 4,780 questions which are systematically categorized into three primary categories, further divided into eight distinct types, ensuring broad coverage of real-world multi-entity reasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4, Llama-3) and RAG pipelines reveal critical limitations: even advanced models achieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance of completeness and factual precision of information extraction in MEQA tasks, using Entity-Attributed F1 (EA-F1) metric for granular evaluation of entity-level correctness and attribution validity. MEBench not only highlights systemic weaknesses in current LLM frameworks but also provides a foundation for advancing robust, entity-aware QA architectures.</li>
</ul>

<h3>Title: The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Jinbo Wang, Mingze Wang, Zhanpeng Zhou, Junchi Yan, Weinan E, Lei Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19002">https://arxiv.org/abs/2502.19002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19002">https://arxiv.org/pdf/2502.19002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19002]] The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training(https://arxiv.org/abs/2502.19002)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformers consist of diverse building blocks, such as embedding layers, normalization layers, self-attention mechanisms, and point-wise feedforward networks. Thus, understanding the differences and interactions among these blocks is important. In this paper, we uncover a clear Sharpness Disparity across these blocks, which emerges early in training and intriguingly persists throughout the training process. Motivated by this finding, we propose Blockwise Learning Rate (LR), a strategy that tailors the LR to each block's sharpness, accelerating large language model (LLM) pre-training. By integrating Blockwise LR into AdamW, we consistently achieve lower terminal loss and nearly $2\times$ speedup compared to vanilla AdamW. We demonstrate this acceleration across GPT-2 and LLaMA, with model sizes ranging from 0.12B to 1.1B and datasets of OpenWebText and MiniPile. Finally, we incorporate Blockwise LR into Adam-mini (Zhang et al., 2024), a recently proposed memory-efficient variant of Adam, achieving a combined $2\times$ speedup and $2\times$ memory saving. These results underscore the potential of exploiting the sharpness disparity to improve LLM training.</li>
</ul>

<h3>Title: Binary Neural Networks for Large Language Model: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Liangdong Liu, Zhitong Zheng, Cong Wang, Tianhuang Su, Zhenyu Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19008">https://arxiv.org/abs/2502.19008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19008">https://arxiv.org/pdf/2502.19008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19008]] Binary Neural Networks for Large Language Model: A Survey(https://arxiv.org/abs/2502.19008)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have wide applications in the field of natural language processing(NLP), such as GPT-4 and Llama. However, with the exponential growth of model parameter sizes, LLMs bring significant resource overheads. Low-bit quantization, as a key technique, reduces memory usage and computational demands by decreasing the bit-width of model parameters, activations, and gradients. Previous quantization methods for LLMs have largely employed Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ does not require any retraining of the original model, while QAT involves optimizing precision during training to achieve the best quantization parameters. The BitNet team proposed a radically different approach, where quantization is performed from the start of model training, utilizing low-precision binary weights during the training process. This approach has led to the emergence of many binary quantization techniques for large language models. This paper provides a comprehensive review of these binary quantization techniques. Specifically, we will introduce binary quantization techniques in deep neural networks and further explore their application to LLMs, reviewing their various contributions, implementations, and applications.</li>
</ul>

<h3>Title: Distilling Reinforcement Learning Algorithms for In-Context Model-Based Planning</h3>
<ul>
<li><strong>Authors: </strong>Jaehyeon Son, Soochan Lee, Gunhee Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19009">https://arxiv.org/abs/2502.19009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19009">https://arxiv.org/pdf/2502.19009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19009]] Distilling Reinforcement Learning Algorithms for In-Context Model-Based Planning(https://arxiv.org/abs/2502.19009)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent studies have shown that Transformers can perform in-context reinforcement learning (RL) by imitating existing RL algorithms, enabling sample-efficient adaptation to unseen tasks without parameter updates. However, these models also inherit the suboptimal behaviors of the RL algorithms they imitate. This issue primarily arises due to the gradual update rule employed by those algorithms. Model-based planning offers a promising solution to this limitation by allowing the models to simulate potential outcomes before taking action, providing an additional mechanism to deviate from the suboptimal behavior. Rather than learning a separate dynamics model, we propose Distillation for In-Context Planning (DICP), an in-context model-based RL framework where Transformers simultaneously learn environment dynamics and improve policy in-context. We evaluate DICP across a range of discrete and continuous environments, including Darkroom variants and Meta-World. Our results show that DICP achieves state-of-the-art performance while requiring significantly fewer environment interactions than baselines, which include both model-free counterparts and existing meta-RL methods.</li>
</ul>

<h3>Title: FungalZSL: Zero-Shot Fungal Classification with Image Captioning Using a Synthetic Data Approach</h3>
<ul>
<li><strong>Authors: </strong>Anju Rani, Daniel O. Arroyo, Petar Durdevic</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19038">https://arxiv.org/abs/2502.19038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19038">https://arxiv.org/pdf/2502.19038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19038]] FungalZSL: Zero-Shot Fungal Classification with Image Captioning Using a Synthetic Data Approach(https://arxiv.org/abs/2502.19038)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The effectiveness of zero-shot classification in large vision-language models (VLMs), such as Contrastive Language-Image Pre-training (CLIP), depends on access to extensive, well-aligned text-image datasets. In this work, we introduce two complementary data sources, one generated by large language models (LLMs) to describe the stages of fungal growth and another comprising a diverse set of synthetic fungi images. These datasets are designed to enhance CLIPs zero-shot classification capabilities for fungi-related tasks. To ensure effective alignment between text and image data, we project them into CLIPs shared representation space, focusing on different fungal growth stages. We generate text using LLaMA3.2 to bridge modality gaps and synthetically create fungi images. Furthermore, we investigate knowledge transfer by comparing text outputs from different LLM techniques to refine classification across growth stages.</li>
</ul>

<h3>Title: Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shiyu Xiang, Ansen Zhang, Yanfei Cao, Yang Fan, Ronghao Chen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19041">https://arxiv.org/abs/2502.19041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19041">https://arxiv.org/pdf/2502.19041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19041]] Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs(https://arxiv.org/abs/2502.19041)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Although Aligned Large Language Models (LLMs) are trained to refuse harmful requests, they remain vulnerable to jailbreak attacks. Unfortunately, existing methods often focus on surface-level patterns, overlooking the deeper attack essences. As a result, defenses fail when attack prompts change, even though the underlying "attack essence" remains the same. To address this issue, we introduce EDDF, an \textbf{E}ssence-\textbf{D}riven \textbf{D}efense \textbf{F}ramework Against Jailbreak Attacks in LLMs. EDDF is a plug-and-play input-filtering method and operates in two stages: 1) offline essence database construction, and 2) online adversarial query detection. The key idea behind EDDF is to extract the "attack essence" from a diverse set of known attack instances and store it in an offline vector database. Experimental results demonstrate that EDDF significantly outperforms existing methods by reducing the Attack Success Rate by at least 20\%, underscoring its superior robustness against jailbreak attacks.</li>
</ul>

<h3>Title: A HEART for the environment: Transformer-Based Spatiotemporal Modeling for Air Quality Prediction</h3>
<ul>
<li><strong>Authors: </strong>Norbert Bodendorfer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19042">https://arxiv.org/abs/2502.19042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19042">https://arxiv.org/pdf/2502.19042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19042]] A HEART for the environment: Transformer-Based Spatiotemporal Modeling for Air Quality Prediction(https://arxiv.org/abs/2502.19042)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate and reliable air pollution forecasting is crucial for effective environmental management and policy-making. llull-environment is a sophisticated and scalable forecasting system for air pollution, inspired by previous models currently operational in Madrid and Valladolid (Spain). It contains (among other key components) an encoder-decoder convolutional neural network to forecast mean pollution levels for four key pollutants (NO$_2$, O$_3$, PM$_{10}$, PM$_{2.5}$) using historical data, external forecasts, and other contextual features. This paper investigates the augmentation of this neural network with an attention mechanism to improve predictive accuracy. The proposed attention mechanism pre-processes tensors containing the input features before passing them to the existing mean forecasting model. The resulting model is a combination of several architectures and ideas and can be described as a "Hybrid Enhanced Autoregressive Transformer", or HEART. The effectiveness of the approach is evaluated by comparing the mean square error (MSE) across different attention layouts against the system without such a mechanism. We observe a significant reduction in MSE of up to 22%, with an average of 7.5% across tested cities and pollutants. The performance of a given attention mechanism turns out to depend on the pollutant, highlighting the differences in their creation and dissipation processes. Our findings are not restricted to optimizing air quality prediction models, but are applicable generally to (fixed length) time series forecasting.</li>
</ul>

<h3>Title: A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Vu Tuan Truong Long, Bao Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19047">https://arxiv.org/abs/2502.19047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19047">https://arxiv.org/pdf/2502.19047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19047]] A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification in Diffusion Models(https://arxiv.org/abs/2502.19047)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as state-of-the-art generative frameworks, excelling in producing high-quality multi-modal samples. However, recent studies have revealed their vulnerability to backdoor attacks, where backdoored models generate specific, undesirable outputs called backdoor target (e.g., harmful images) when a pre-defined trigger is embedded to their inputs. In this paper, we propose PureDiffusion, a dual-purpose framework that simultaneously serves two contrasting roles: backdoor defense and backdoor attack amplification. For defense, we introduce two novel loss functions to invert backdoor triggers embedded in diffusion models. The first leverages trigger-induced distribution shifts across multiple timesteps of the diffusion process, while the second exploits the denoising consistency effect when a backdoor is activated. Once an accurate trigger inversion is achieved, we develop a backdoor detection method that analyzes both the inverted trigger and the generated backdoor targets to identify backdoor attacks. In terms of attack amplification with the role of an attacker, we describe how our trigger inversion algorithm can be used to reinforce the original trigger embedded in the backdoored diffusion model. This significantly boosts attack performance while reducing the required backdoor training time. Experimental results demonstrate that PureDiffusion achieves near-perfect detection accuracy, outperforming existing defenses by a large margin, particularly against complex trigger patterns. Additionally, in an attack scenario, our attack amplification approach elevates the attack success rate (ASR) of existing backdoor attacks to nearly 100\% while reducing training time by up to 20x.</li>
</ul>

<h3>Title: Foundation Inference Models for Stochastic Differential Equations: A Transformer-based Approach for Zero-shot Function Estimation</h3>
<ul>
<li><strong>Authors: </strong>Patrick Seifner, Kostadin Cvejoski, David Berghaus, Cesar Ojeda, Ramses J. Sanchez</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19049">https://arxiv.org/abs/2502.19049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19049">https://arxiv.org/pdf/2502.19049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19049]] Foundation Inference Models for Stochastic Differential Equations: A Transformer-based Approach for Zero-shot Function Estimation(https://arxiv.org/abs/2502.19049)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Stochastic differential equations (SDEs) describe dynamical systems where deterministic flows, governed by a drift function, are superimposed with random fluctuations dictated by a diffusion function. The accurate estimation (or discovery) of these functions from data is a central problem in machine learning, with wide application across natural and social sciences alike. Yet current solutions are brittle, and typically rely on symbolic regression or Bayesian non-parametrics. In this work, we introduce FIM-SDE (Foundation Inference Model for SDEs), a transformer-based recognition model capable of performing accurate zero-shot estimation of the drift and diffusion functions of SDEs, from noisy and sparse observations on empirical processes of different dimensionalities. Leveraging concepts from amortized inference and neural operators, we train FIM-SDE in a supervised fashion, to map a large set of noisy and discretely observed SDE paths to their corresponding drift and diffusion functions. We demonstrate that one and the same (pretrained) FIM-SDE achieves robust zero-shot function estimation (i.e. without any parameter fine-tuning) across a wide range of synthetic and real-world processes, from canonical SDE systems (e.g. double-well dynamics or weakly perturbed Hopf bifurcations) to human motion recordings and oil price and wind speed fluctuations.</li>
</ul>

<h3>Title: MathClean: A Benchmark for Synthetic Mathematical Data Cleaning</h3>
<ul>
<li><strong>Authors: </strong>Hao Liang, Meiyi Qiang, Yuying Li, Zefeng He, Yongzhen Guo, Zhengzhou Zhu, Wentao Zhang, Bin Cui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19058">https://arxiv.org/abs/2502.19058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19058">https://arxiv.org/pdf/2502.19058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19058]] MathClean: A Benchmark for Synthetic Mathematical Data Cleaning(https://arxiv.org/abs/2502.19058)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid development of large language models (LLMs), the quality of training data has become crucial. Among the various types of training data, mathematical data plays a key role in enabling LLMs to acquire strong reasoning abilities. While high-quality open-source data is important, it is often insufficient for pre-training, necessitating the addition of synthetic math problems. However, synthetic math questions and answers can introduce inaccuracies, which may degrade both the training data and web data. Therefore, an effective method for cleaning synthetic math data is essential. In this paper, we propose the MathClean benchmark to evaluate the effectiveness of math data cleaning models. The MathClean benchmark consists of 2,000 correct questions and 2,000 erroneous questions with additional 2,000 correct and erroneous answers sourced from augmented data based on GSM8K and MATH. Moreover, we also annotate error types for each question or answer, since it can assess whether models can correctly identify the error categories for future improvements. Finally, we present comprehensive evaluations using state-of-the-art (SOTA) models. Our results demonstrate that even strong models like GPT-o1 and DeepSeek-R1 perform poorly on this benchmark, highlighting the utility of MathClean. Our code and data is available at this https URL.</li>
</ul>

<h3>Title: Can Large Language Models Outperform Non-Experts in Poetry Evaluation? A Comparative Study Using the Consensual Assessment Technique</h3>
<ul>
<li><strong>Authors: </strong>Piotr Sawicki, Marek Grze, Dan Brown, Fabrcio Ges</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19064">https://arxiv.org/abs/2502.19064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19064">https://arxiv.org/pdf/2502.19064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19064]] Can Large Language Models Outperform Non-Experts in Poetry Evaluation? A Comparative Study Using the Consensual Assessment Technique(https://arxiv.org/abs/2502.19064)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Consensual Assessment Technique (CAT) evaluates creativity through holistic expert judgments. We investigate the use of two advanced Large Language Models (LLMs), Claude-3-Opus and GPT-4o, to evaluate poetry by a methodology inspired by the CAT. Using a dataset of 90 poems, we found that these LLMs can surpass the results achieved by non-expert human judges at matching a ground truth based on publication venue, particularly when assessing smaller subsets of poems. Claude-3-Opus exhibited slightly superior performance than GPT-4o. We show that LLMs are viable tools for accurately assessing poetry, paving the way for their broader application into other creative domains.</li>
</ul>

<h3>Title: A Sample-Level Evaluation and Generative Framework for Model Inversion Attacks</h3>
<ul>
<li><strong>Authors: </strong>Haoyang Li, Li Bai, Qingqing Ye, Haibo Hu, Yaxin Xiao, Huadi Zheng, Jianliang Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19070">https://arxiv.org/abs/2502.19070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19070">https://arxiv.org/pdf/2502.19070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19070]] A Sample-Level Evaluation and Generative Framework for Model Inversion Attacks(https://arxiv.org/abs/2502.19070)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, generative</a></li>
<li><strong>Abstract: </strong>Model Inversion (MI) attacks, which reconstruct the training dataset of neural networks, pose significant privacy concerns in machine learning. Recent MI attacks have managed to reconstruct realistic label-level private data, such as the general appearance of a target person from all training images labeled on him. Beyond label-level privacy, in this paper we show sample-level privacy, the private information of a single target sample, is also important but under-explored in the MI literature due to the limitations of existing evaluation metrics. To address this gap, this study introduces a novel metric tailored for training-sample analysis, namely, the Diversity and Distance Composite Score (DDCS), which evaluates the reconstruction fidelity of each training sample by encompassing various MI attack attributes. This, in turn, enhances the precision of sample-level privacy assessments. Leveraging DDCS as a new evaluative lens, we observe that many training samples remain resilient against even the most advanced MI attack. As such, we further propose a transfer learning framework that augments the generative capabilities of MI attackers through the integration of entropy loss and natural gradient descent. Extensive experiments verify the effectiveness of our framework on improving state-of-the-art MI attacks over various metrics including DDCS, coverage and FID. Finally, we demonstrate that DDCS can also be useful for MI defense, by identifying samples susceptible to MI attacks in an unsupervised manner.</li>
</ul>

<h3>Title: MCLRL: A Multi-Domain Contrastive Learning with Reinforcement Learning Framework for Few-Shot Modulation Recognition</h3>
<ul>
<li><strong>Authors: </strong>Dongwei Xu, Yutao Zhu, Yao Lu, Youpeng Feng, Yun Lin, Qi Xuan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19071">https://arxiv.org/abs/2502.19071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19071">https://arxiv.org/pdf/2502.19071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19071]] MCLRL: A Multi-Domain Contrastive Learning with Reinforcement Learning Framework for Few-Shot Modulation Recognition(https://arxiv.org/abs/2502.19071)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction</a></li>
<li><strong>Abstract: </strong>With the rapid advancements in wireless communication technology, automatic modulation recognition (AMR) plays a critical role in ensuring communication security and reliability. However, numerous challenges, including higher performance demands, difficulty in data acquisition under specific scenarios, limited sample size, and low-quality labeled data, hinder its development. Few-shot learning (FSL) offers an effective solution by enabling models to achieve satisfactory performance with only a limited number of labeled samples. While most FSL techniques are applied in the field of computer vision, they are not directly applicable to wireless signal processing. This study does not propose a new FSL-specific signal model but introduces a framework called MCLRL. This framework combines multi-domain contrastive learning with reinforcement learning. Multi-domain representations of signals enhance feature richness, while integrating contrastive learning and reinforcement learning architectures enables the extraction of deep features for classification. In downstream tasks, the model achieves excellent performance using only a few samples and minimal training cycles. Experimental results show that the MCLRL framework effectively extracts key features from signals, performs well in FSL tasks, and maintains flexibility in signal model selection.</li>
</ul>

<h3>Title: Sparse Brains are Also Adaptive Brains: Cognitive-Load-Aware Dynamic Activation for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yiheng Yang, Yujie Wang, Chi Ma, Lei Yu, Emmanuele Chersoni, Chu-Ren Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19078">https://arxiv.org/abs/2502.19078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19078">https://arxiv.org/pdf/2502.19078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19078]] Sparse Brains are Also Adaptive Brains: Cognitive-Load-Aware Dynamic Activation for LLMs(https://arxiv.org/abs/2502.19078)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Dense large language models(LLMs) face critical efficiency bottlenecks as they rigidly activate all parameters regardless of input complexity. While existing sparsity methods(static pruning or dynamic activation) address this partially, they either lack adaptivity to contextual or model structural demands or incur prohibitive computational overhead. Inspired by human brain's dual-process mechanisms - predictive coding (N400) for backbone sparsity and structural reanalysis (P600) for complex context - we propose CLADA, a \textit{\textbf{C}ognitive-\textbf{L}oad-\textbf{A}ware \textbf{D}ynamic \textbf{A}ctivation} framework that synergizes statistical sparsity with semantic adaptability. Our key insight is that LLM activations exhibit two complementary patterns: 1) \textit{Global statistical sparsity} driven by sequence-level prefix information, and 2) \textit{Local semantic adaptability} modulated by cognitive load metrics(e.g., surprisal and entropy). CLADA employs a hierarchical thresholding strategy: a baseline from offline error-controlled optimization ensures 40\%+ sparsity, dynamically adjusted by real-time cognitive signals. Evaluations across six mainstream LLMs and nine benchmarks demonstrate that CLADA achieves \textbf{~20\% average speedup with <2\% accuracy drop}, outperforming Griffin (5\%+ degradation) and TT (negligible speedup). Crucially, we establish the first formal connection between neurolinguistic event-related potential (ERP) components and LLM efficiency mechanisms through multi-level regression analysis ($R^2=0.17$ for sparsity-adaptation synergy). Requiring no retraining or architectural changes, CLADA offers a deployable solution for resource-aware LLM inference while advancing biologically-inspired AI design. Our code is available at \href{this https URL}{CLADA}.</li>
</ul>

<h3>Title: EndoMamba: An Efficient Foundation Model for Endoscopic Videos</h3>
<ul>
<li><strong>Authors: </strong>Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Dongdong Lei, Sebastien Ourselin, Hongbin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19090">https://arxiv.org/abs/2502.19090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19090">https://arxiv.org/pdf/2502.19090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19090]] EndoMamba: An Efficient Foundation Model for Endoscopic Videos(https://arxiv.org/abs/2502.19090)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Endoscopic video-based tasks, such as visual navigation and surgical phase recognition, play a crucial role in minimally invasive surgeries by providing real-time assistance. While recent video foundation models have shown promise, their applications are hindered by (1) computational inefficiencies and (2) suboptimal performance caused by limited data for pre-training in endoscopy. To address these issues, we present EndoMamba, a foundation model designed for real-time inference while learning generalized spatiotemporal representations. First, to mitigate computational inefficiencies, we propose the EndoMamba backbone, optimized for real-time inference. Inspired by recent advancements in state space models, EndoMamba integrates Bidirectional Mamba blocks for spatial modeling within individual frames and vanilla Mamba blocks for past-to-present reasoning across the temporal domain. This design enables both strong spatiotemporal modeling and efficient inference in online video streams. Second, we propose a self-supervised hierarchical pre-training diagram to enhance EndoMamba's representation learning using endoscopic videos and incorporating general video domain knowledge. Specifically, our approach combines masked reconstruction with auxiliary supervision, leveraging low-level reconstruction to capture spatial-temporal structures and high-level alignment to transfer broader knowledge from a pretrained general-video domain foundation model. Extensive experiments on four downstream tasks--classification, segmentation, surgical phase recognition, and localization--demonstrate that EndoMamba outperforms existing foundation models and task-specific methods while maintaining real-time inference speed. The source code will be released upon acceptance.</li>
</ul>

<h3>Title: LongEval: A Comprehensive Analysis of Long-Text Generation Through a Plan-based Paradigm</h3>
<ul>
<li><strong>Authors: </strong>Siwei Wu, Yizhi Li, Xingwei Qu, Rishi Ravikumar, Yucheng Li, Tyler Loakman Shanghaoran Quan Xiaoyong Wei, Riza Batista-Navarro, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19103">https://arxiv.org/abs/2502.19103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19103">https://arxiv.org/pdf/2502.19103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19103]] LongEval: A Comprehensive Analysis of Long-Text Generation Through a Plan-based Paradigm(https://arxiv.org/abs/2502.19103)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success in various natural language processing tasks, yet their ability to generate long-form content remains poorly understood and evaluated. Our analysis reveals that current LLMs struggle with length requirements and information density in long-text generation, with performance deteriorating as text length increases. To quantitively locate such a performance degradation and provide further insights on model development, we present LongEval, a benchmark that evaluates long-text generation through both direct and plan-based generation paradigms, inspired by cognitive and linguistic writing models. The comprehensive experiments in this work reveal interesting findings such as that while model size correlates with generation ability, the small-scale model (e.g., LongWriter), well-trained on long texts, has comparable performance. All code and datasets are released in this https URL.</li>
</ul>

<h3>Title: Evaluating Gender Bias in German Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Michelle Kappl</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19104">https://arxiv.org/abs/2502.19104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19104">https://arxiv.org/pdf/2502.19104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19104]] Evaluating Gender Bias in German Machine Translation(https://arxiv.org/abs/2502.19104)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present WinoMTDE, a new gender bias evaluation test set designed to assess occupational stereotyping and underrepresentation in German machine translation (MT) systems. Building on the automatic evaluation method introduced by arXiv:1906.00591v1 [cs.CL], we extend the approach to German, a language with grammatical gender. The WinoMTDE dataset comprises 288 German sentences that are balanced in regard to gender, as well as stereotype, which was annotated using German labor statistics. We conduct a large-scale evaluation of five widely used MT systems and a large language model. Our results reveal persistent bias in most models, with the LLM outperforming traditional systems. The dataset and evaluation code are publicly available under this https URL.</li>
</ul>

<h3>Title: Chemical knowledge-informed framework for privacy-aware retrosynthesis learning</h3>
<ul>
<li><strong>Authors: </strong>Guikun Chen, Xu Zhang, Yi Yang, Wenguan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19119">https://arxiv.org/abs/2502.19119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19119">https://arxiv.org/pdf/2502.19119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19119]] Chemical knowledge-informed framework for privacy-aware retrosynthesis learning(https://arxiv.org/abs/2502.19119)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Chemical reaction data is a pivotal asset, driving advances in competitive fields such as pharmaceuticals, materials science, and industrial chemistry. Its proprietary nature renders it sensitive, as it often includes confidential insights and competitive advantages organizations strive to protect. However, in contrast to this need for confidentiality, the current standard training paradigm for machine learning-based retrosynthesis gathers reaction data from multiple sources into one single edge to train prediction models. This paradigm poses considerable privacy risks as it necessitates broad data availability across organizational boundaries and frequent data transmission between entities, potentially exposing proprietary information to unauthorized access or interception during storage and transfer. In the present study, we introduce the chemical knowledge-informed framework (CKIF), a privacy-preserving approach for learning retrosynthesis models. CKIF enables distributed training across multiple chemical organizations without compromising the confidentiality of proprietary reaction data. Instead of gathering raw reaction data, CKIF learns retrosynthesis models through iterative, chemical knowledge-informed aggregation of model parameters. In particular, the chemical properties of predicted reactants are leveraged to quantitatively assess the observable behaviors of individual models, which in turn determines the adaptive weights used for model aggregation. On a variety of reaction datasets, CKIF outperforms several strong baselines by a clear margin (e.g., ~20% performance improvement over FedAvg on USPTO-50K), showing its feasibility and superiority to stimulate further research on privacy-preserving retrosynthesis.</li>
</ul>

<h3>Title: The NeRF Signature: Codebook-Aided Watermarking for Neural Radiance Fields</h3>
<ul>
<li><strong>Authors: </strong>Ziyuan Luo, Anderson Rocha, Boxin Shi, Qing Guo, Haoliang Li, Renjie Wan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19125">https://arxiv.org/abs/2502.19125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19125">https://arxiv.org/pdf/2502.19125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19125]] The NeRF Signature: Codebook-Aided Watermarking for Neural Radiance Fields(https://arxiv.org/abs/2502.19125)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, watermark</a></li>
<li><strong>Abstract: </strong>Neural Radiance Fields (NeRF) have been gaining attention as a significant form of 3D content representation. With the proliferation of NeRF-based creations, the need for copyright protection has emerged as a critical issue. Although some approaches have been proposed to embed digital watermarks into NeRF, they often neglect essential model-level considerations and incur substantial time overheads, resulting in reduced imperceptibility and robustness, along with user inconvenience. In this paper, we extend the previous criteria for image watermarking to the model level and propose NeRF Signature, a novel watermarking method for NeRF. We employ a Codebook-aided Signature Embedding (CSE) that does not alter the model structure, thereby maintaining imperceptibility and enhancing robustness at the model level. Furthermore, after optimization, any desired signatures can be embedded through the CSE, and no fine-tuning is required when NeRF owners want to use new binary signatures. Then, we introduce a joint pose-patch encryption watermarking strategy to hide signatures into patches rendered from a specific viewpoint for higher robustness. In addition, we explore a Complexity-Aware Key Selection (CAKS) scheme to embed signatures in high visual complexity patches to enhance imperceptibility. The experimental results demonstrate that our method outperforms other baseline methods in terms of imperceptibility and robustness. The source code is available at: this https URL.</li>
</ul>

<h3>Title: Self-Memory Alignment: Mitigating Factual Hallucinations with Generalized Improvement</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Zhang, Yichi Zhang, Yinpeng Dong, Hang Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19127">https://arxiv.org/abs/2502.19127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19127">https://arxiv.org/pdf/2502.19127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19127]] Self-Memory Alignment: Mitigating Factual Hallucinations with Generalized Improvement(https://arxiv.org/abs/2502.19127)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often struggle to align their responses with objective facts, resulting in the issue of factual hallucinations, which can be difficult to detect and mislead users without relevant knowledge. While post-training techniques have been employed to mitigate the issue, existing methods usually suffer from poor generalization and trade-offs in different capabilities. In this paper, we propose to address it by directly augmenting LLM's fundamental ability to precisely leverage its existing memory--the knowledge acquired from pre-training data. We introduce self-memory alignment (SMA), which fine-tunes the model on self-generated responses to precise and simple factual questions through preference optimization. Furthermore, we construct FactualBench, a comprehensive and precise factual QA dataset containing 181k Chinese data spanning 21 domains, to facilitate both evaluation and training. Extensive experiments show that SMA significantly improves LLMs' overall performance, with consistent enhancement across various benchmarks concerning factuality, as well as helpfulness and comprehensive skills.</li>
</ul>

<h3>Title: Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhaowei Zhang, Fengshuo Bai, Qizhi Chen, Chengdong Ma, Mingzhi Wang, Haoran Sun, Zilong Zheng, Yaodong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19148">https://arxiv.org/abs/2502.19148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19148">https://arxiv.org/pdf/2502.19148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19148]] Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs(https://arxiv.org/abs/2502.19148)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>How to align large language models (LLMs) with user preferences from a static general dataset has been frequently studied. However, user preferences are usually personalized, changing, and diverse regarding culture, values, or time. This leads to the problem that the actual user preferences often do not coincide with those trained by the model developers in the practical use of LLMs. Since we cannot collect enough data and retrain for every demand, researching efficient real-time preference adaptation methods based on the backbone LLMs during test time is important. To this end, we introduce Amulet, a novel, training-free framework that formulates the decoding process of every token as a separate online learning problem with the guidance of simple user-provided prompts, thus enabling real-time optimization to satisfy users' personalized preferences. To reduce the computational cost brought by this optimization process for each token, we additionally provide a closed-form solution for each iteration step of the optimization process, thereby reducing the computational time cost to a negligible level. The detailed experimental results demonstrate that Amulet can achieve significant performance improvements in rich settings with combinations of different LLMs, datasets, and user preferences, while maintaining acceptable computational efficiency.</li>
</ul>

<h3>Title: Towards Privacy-Preserving Anomaly-Based Intrusion Detection in Energy Communities</h3>
<ul>
<li><strong>Authors: </strong>Zeeshan Afzal, Giovanni Gaggero, Mikael Asplund</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19154">https://arxiv.org/abs/2502.19154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19154">https://arxiv.org/pdf/2502.19154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19154]] Towards Privacy-Preserving Anomaly-Based Intrusion Detection in Energy Communities(https://arxiv.org/abs/2502.19154)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>Energy communities consist of decentralized energy production, storage, consumption, and distribution and are gaining traction in modern power systems. However, these communities may increase the vulnerability of the grid to cyber threats. We propose an anomaly-based intrusion detection system to enhance the security of energy communities. The system leverages deep autoencoders to detect deviations from normal operational patterns in order to identify anomalies induced by malicious activities and attacks. Operational data for training and evaluation are derived from a Simulink model of an energy community. The results show that the autoencoder-based intrusion detection system achieves good detection performance across multiple attack scenarios. We also demonstrate potential for real-world application of the system by training a federated model that enables distributed intrusion detection while preserving data privacy.</li>
</ul>

<h3>Title: When Personalization Meets Reality: A Multi-Faceted Analysis of Personalized Preference Learning</h3>
<ul>
<li><strong>Authors: </strong>Yijiang River Dong, Tiancheng Hu, Yinhong Liu, Ahmet stn, Nigel Collier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19158">https://arxiv.org/abs/2502.19158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19158">https://arxiv.org/pdf/2502.19158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19158]] When Personalization Meets Reality: A Multi-Faceted Analysis of Personalized Preference Learning(https://arxiv.org/abs/2502.19158)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>While Reinforcement Learning from Human Feedback (RLHF) is widely used to align Large Language Models (LLMs) with human preferences, it typically assumes homogeneous preferences across users, overlooking diverse human values and minority viewpoints. Although personalized preference learning addresses this by tailoring separate preferences for individual users, the field lacks standardized methods to assess its effectiveness. We present a multi-faceted evaluation framework that measures not only performance but also fairness, unintended effects, and adaptability across varying levels of preference divergence. Through extensive experiments comparing eight personalization methods across three preference datasets, we demonstrate that performance differences between methods could reach 36% when users strongly disagree, and personalization can introduce up to 20% safety misalignment. These findings highlight the critical need for holistic evaluation approaches to advance the development of more effective and inclusive preference learning systems.</li>
</ul>

<h3>Title: A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xuan Ding, Yao Zhu, Yunjian Zhang, Chuanlong Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19159">https://arxiv.org/abs/2502.19159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19159">https://arxiv.org/pdf/2502.19159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19159]] A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs(https://arxiv.org/abs/2502.19159)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Compared to width-wise pruning, depth-wise pruning can significantly accelerate inference in resource-constrained scenarios. Howerver, treating the entire Transformer layer as the minimum pruning unit may degrade model performance by indiscriminately discarding the entire information of the layer. This paper reveals the "Patch-like" feature relationship between layers in large language models by analyzing the correlation of the outputs of different layers in the reproducing kernel Hilbert space. Building on this observation, we proposes a sliding layer merging method that dynamically selects and fuses consecutive layers from top to bottom according to a pre-defined similarity threshold, thereby simplifying the model structure while maintaining its performance. Extensive experiments on LLMs with various architectures and different parameter scales show that our method outperforms existing pruning techniques in both zero-shot inference performance and retraining recovery quality after pruning. In particular, in the experiment with 35\% pruning on the Vicuna-7B model, our method achieved a 1.654\% improvement in average performance on zero-shot tasks compared to the existing method. Moreover, we further reveal the potential of combining depth pruning with width pruning to enhance the pruning effect. Our codes are available at this https URL.</li>
</ul>

<h3>Title: Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rebekka Grge, Michael Mock, Hctor Allende-Cid</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19160">https://arxiv.org/abs/2502.19160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19160">https://arxiv.org/pdf/2502.19160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19160]] Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models(https://arxiv.org/abs/2502.19160)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Social categories and stereotypes are embedded in language and can introduce data bias into Large Language Models (LLMs). Despite safeguards, these biases often persist in model behavior, potentially leading to representational harm in outputs. While sociolinguistic research provides valuable insights into the formation of stereotypes, NLP approaches for stereotype detection rarely draw on this foundation and often lack objectivity, precision, and interpretability. To fill this gap, in this work we propose a new approach that detects and quantifies the linguistic indicators of stereotypes in a sentence. We derive linguistic indicators from the Social Category and Stereotype Communication (SCSC) framework which indicate strong social category formulation and stereotyping in language, and use them to build a categorization scheme. To automate this approach, we instruct different LLMs using in-context learning to apply the approach to a sentence, where the LLM examines the linguistic properties and provides a basis for a fine-grained assessment. Based on an empirical evaluation of the importance of different linguistic indicators, we learn a scoring function that measures the linguistic indicators of a stereotype. Our annotations of stereotyped sentences show that these indicators are present in these sentences and explain the strength of a stereotype. In terms of model performance, our results show that the models generally perform well in detecting and classifying linguistic indicators of category labels used to denote a category, but sometimes struggle to correctly evaluate the associated behaviors and characteristics. Using more few-shot examples within the prompts, significantly improves performance. Model performance increases with size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that surpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct.</li>
</ul>

<h3>Title: TestNUC: Enhancing Test-Time Computing Approaches through Neighboring Unlabeled Data Consistency</h3>
<ul>
<li><strong>Authors: </strong>Henry Peng Zou, Zhengyao Gu, Yue Zhou, Yankai Chen, Weizhi Zhang, Liancheng Fang, Yibo Wang, Yangning Li, Kay Liu, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19163">https://arxiv.org/abs/2502.19163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19163">https://arxiv.org/pdf/2502.19163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19163]] TestNUC: Enhancing Test-Time Computing Approaches through Neighboring Unlabeled Data Consistency(https://arxiv.org/abs/2502.19163)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Test-time computing approaches, which leverage additional computational resources during inference, have been proven effective in enhancing large language model performance. This work introduces a novel, linearly scaling approach, TestNUC, that improves test-time predictions by leveraging the local consistency of neighboring unlabeled data-it classifies an input instance by considering not only the model's prediction on that instance but also on neighboring unlabeled instances. We evaluate TestNUC across eight diverse datasets, spanning intent classification, topic mining, domain discovery, and emotion detection, demonstrating its consistent superiority over baseline methods such as standard prompting and self-consistency. Furthermore, TestNUC can be seamlessly integrated with existing test-time computing approaches, substantially boosting their performance. Our analysis reveals that TestNUC scales effectively with increasing amounts of unlabeled data and performs robustly across different embedding models, making it practical for real-world applications. Our code is available at this https URL.</li>
</ul>

<h3>Title: Design of Cavity Backed Slotted Antenna using Machine Learning Regression Model</h3>
<ul>
<li><strong>Authors: </strong>Vijay Kumar Sutrakar, Anjana PK, Rohit Bisariya, Soumya KK, Gopal Chawan M</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, physics.app-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19164">https://arxiv.org/abs/2502.19164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19164">https://arxiv.org/pdf/2502.19164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19164]] Design of Cavity Backed Slotted Antenna using Machine Learning Regression Model(https://arxiv.org/abs/2502.19164)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>In this paper, a regression-based machine learning model is used for the design of cavity backed slotted antenna. This type of antenna is commonly used in military and aviation communication systems. Initial reflection coefficient data of cavity backed slotted antenna is generated using electromagnetic solver. These reflection coefficient data is then used as input for training regression-based machine learning model. The model is trained to predict the dimensions of cavity backed slotted antenna based on the input reflection coefficient for a wide frequency band varying from 1 GHz to 8 GHz. This approach allows for rapid prediction of optimal antenna configurations, reducing the need for repeated physical testing and manual adjustments, may lead to significant amount of design and development cost saving. The proposed model also demonstrates its versatility in predicting multi frequency resonance across 1 GHz to 8 GHz. Also, the proposed approach demonstrates the potential for leveraging machine learning in advanced antenna design, enhancing efficiency and accuracy in practical applications such as radar, military identification systems and secure communication networks.</li>
</ul>

<h3>Title: On the Byzantine Fault Tolerance of signSGD with Majority Vote</h3>
<ul>
<li><strong>Authors: </strong>Emanuele Mengoli, Luzius Moll, Virgilio Strozzi, El-Mahdi El-Mhamdi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19170">https://arxiv.org/abs/2502.19170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19170">https://arxiv.org/pdf/2502.19170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19170]] On the Byzantine Fault Tolerance of signSGD with Majority Vote(https://arxiv.org/abs/2502.19170)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>In distributed learning, sign-based compression algorithms such as signSGD with majority vote provide a lightweight alternative to SGD with an additional advantage: fault tolerance (almost) for free. However, for signSGD with majority vote, this fault tolerance has been shown to cover only the case of weaker adversaries, i.e., ones that are not omniscient or cannot collude to base their attack on common knowledge and strategy. In this work, we close this gap and provide new insights into how signSGD with majority vote can be resilient against omniscient and colluding adversaries, which craft an attack after communicating with other adversaries, thus having better information to perform the most damaging attack based on a common optimal strategy. Our core contribution is in providing a proof that begins by defining the omniscience framework and the strongest possible damage against signSGD with majority vote without imposing any restrictions on the attacker. Thanks to the filtering effect of the sign-based method, we upper-bound the space of attacks to the optimal strategy for maximizing damage by an attacker. Hence, we derive an explicit probabilistic bound in terms of incorrect aggregation without resorting to unknown constants, providing a convergence bound on signSGD with majority vote in the presence of Byzantine attackers, along with a precise convergence rate. Our findings are supported by experiments on the MNIST dataset in a distributed learning environment with adversaries of varying strength.</li>
</ul>

<h3>Title: A Model-Centric Review of Deep Learning for Protein Design</h3>
<ul>
<li><strong>Authors: </strong>Gregory W. Kyro, Tianyin Qiu, Victor S. Batista</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19173">https://arxiv.org/abs/2502.19173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19173">https://arxiv.org/pdf/2502.19173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19173]] A Model-Centric Review of Deep Learning for Protein Design(https://arxiv.org/abs/2502.19173)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Deep learning has transformed protein design, enabling accurate structure prediction, sequence optimization, and de novo protein generation. Advances in single-chain protein structure prediction via AlphaFold2, RoseTTAFold, ESMFold, and others have achieved near-experimental accuracy, inspiring successive work extended to biomolecular complexes via AlphaFold Multimer, RoseTTAFold All-Atom, AlphaFold 3, Chai-1, Boltz-1 and others. Generative models such as ProtGPT2, ProteinMPNN, and RFdiffusion have enabled sequence and backbone design beyond natural evolution-based limitations. More recently, joint sequence-structure co-design models, including ESM3, have integrated both modalities into a unified framework, resulting in improved designability. Despite these advances, challenges still exist pertaining to modeling sequence-structure-function relationships and ensuring robust generalization beyond the regions of protein space spanned by the training data. Future advances will likely focus on joint sequence-structure-function co-design frameworks that are able to model the fitness landscape more effectively than models that treat these modalities independently. Current capabilities, coupled with the dizzying rate of progress, suggest that the field will soon enable rapid, rational design of proteins with tailored structures and functions that transcend the limitations imposed by natural evolution. In this review, we discuss the current capabilities of deep learning methods for protein design, focusing on some of the most revolutionary and capable models with respect to their functionality and the applications that they enable, leading up to the current challenges of the field and the optimal path forward.</li>
</ul>

<h3>Title: MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Daniel Rose, Chia-Chien Hung, Marco Lepri, Israa Alqassem, Kiril Gashteovski, Carolin Lawrence</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19175">https://arxiv.org/abs/2502.19175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19175">https://arxiv.org/pdf/2502.19175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19175]] MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis(https://arxiv.org/abs/2502.19175)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability, large language model</a></li>
<li><strong>Abstract: </strong>Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical decision-making, in which physicians iteratively refine a ranked list of possible diseases based on symptoms, antecedents, and medical knowledge. While recent advances in large language models have shown promise in supporting DDx, existing approaches face key limitations, including single-dataset evaluations, isolated optimization of components, unrealistic assumptions about complete patient profiles, and single-attempt diagnosis. We introduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for interactive DDx, where diagnostic reasoning evolves through iterative learning, rather than assuming a complete patient profile is accessible. MEDDxAgent integrates three modular components: (1) an orchestrator (DDxDriver), (2) a history taking simulator, and (3) two specialized agents for knowledge retrieval and diagnosis strategy. To ensure robust evaluation, we introduce a comprehensive DDx benchmark covering respiratory, skin, and rare diseases. We analyze single-turn diagnostic approaches and demonstrate the importance of iterative refinement when patient profiles are not available at the outset. Our broad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy improvements in interactive DDx across both large and small LLMs, while offering critical explainability into its diagnostic reasoning process.</li>
</ul>

<h3>Title: Knowledge Distillation for Semantic Segmentation: A Label Space Unification Approach</h3>
<ul>
<li><strong>Authors: </strong>Anton Backhaus, Thorsten Luettel, Mirko Maehlisch</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19177">https://arxiv.org/abs/2502.19177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19177">https://arxiv.org/pdf/2502.19177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19177]] Knowledge Distillation for Semantic Segmentation: A Label Space Unification Approach(https://arxiv.org/abs/2502.19177)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>An increasing number of datasets sharing similar domains for semantic segmentation have been published over the past few years. But despite the growing amount of overall data, it is still difficult to train bigger and better models due to inconsistency in taxonomy and/or labeling policies of different datasets. To this end, we propose a knowledge distillation approach that also serves as a label space unification method for semantic segmentation. In short, a teacher model is trained on a source dataset with a given taxonomy, then used to pseudo-label additional data for which ground truth labels of a related label space exist. By mapping the related taxonomies to the source taxonomy, we create constraints within which the model can predict pseudo-labels. Using the improved pseudo-labels we train student models that consistently outperform their teachers in two challenging domains, namely urban and off-road driving. Our ground truth-corrected pseudo-labels span over 12 and 7 public datasets with 388.230 and 18.558 images for the urban and off-road domains, respectively, creating the largest compound datasets for autonomous driving to date.</li>
</ul>

<h3>Title: INFO-SEDD: Continuous Time Markov Chains as Scalable Information Metrics Estimators</h3>
<ul>
<li><strong>Authors: </strong>Alberto Foresti, Giulio Franzese, Pietro Michiardi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19183">https://arxiv.org/abs/2502.19183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19183">https://arxiv.org/pdf/2502.19183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19183]] INFO-SEDD: Continuous Time Markov Chains as Scalable Information Metrics Estimators(https://arxiv.org/abs/2502.19183)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Information-theoretic quantities play a crucial role in understanding non-linear relationships between random variables and are widely used across scientific disciplines. However, estimating these quantities remains an open problem, particularly in the case of high-dimensional discrete distributions. Current approaches typically rely on embedding discrete data into a continuous space and applying neural estimators originally designed for continuous distributions, a process that may not fully capture the discrete nature of the underlying data. We consider Continuous-Time Markov Chains (CTMCs), stochastic processes on discrete state-spaces which have gained popularity due to their generative modeling applications. In this work, we introduce INFO-SEDD, a novel method for estimating information-theoretic quantities of discrete data, including mutual information and entropy. Our approach requires the training of a single parametric model, offering significant computational and memory advantages. Additionally, it seamlessly integrates with pretrained networks, allowing for efficient reuse of pretrained generative models. To evaluate our approach, we construct a challenging synthetic benchmark. Our experiments demonstrate that INFO-SEDD is robust and outperforms neural competitors that rely on embedding techniques. Moreover, we validate our method on a real-world task: estimating the entropy of an Ising model. Overall, INFO-SEDD outperforms competing methods and shows scalability to high-dimensional scenarios, paving the way for new applications where estimating MI between discrete distribution is the focus. The promising results in this complex, high-dimensional scenario highlight INFO-SEDD as a powerful new estimator in the toolkit for information-theoretical analysis.</li>
</ul>

<h3>Title: BIG-Bench Extra Hard</h3>
<ul>
<li><strong>Authors: </strong>Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, Orhan Firat</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19187">https://arxiv.org/abs/2502.19187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19187">https://arxiv.org/pdf/2502.19187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19187]] BIG-Bench Extra Hard(https://arxiv.org/abs/2502.19187)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed in everyday applications, demanding robust general reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning benchmarks predominantly focus on mathematical and coding abilities, leaving a gap in evaluating broader reasoning proficiencies. One particular exception is the BIG-Bench dataset, which has served as a crucial benchmark for evaluating the general reasoning capabilities of LLMs, thanks to its diverse set of challenging tasks that allowed for a comprehensive assessment of general reasoning across various skills within a unified framework. However, recent advances in LLMs have led to saturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH). State-of-the-art models achieve near-perfect scores on many tasks in BBH, thus diminishing its utility. To address this limitation, we introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly increased difficulty. We evaluate various models on BBEH and observe a (harmonic) average accuracy of 9.8\% for the best general-purpose model and 44.8\% for the best reasoning-specialized model, indicating substantial room for improvement and highlighting the ongoing challenge of achieving robust general reasoning in LLMs. We release BBEH publicly at: this https URL.</li>
</ul>

<h3>Title: EGR-Net: A Novel Embedding Gramian Representation CNN for Intelligent Fault Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Linshan Jia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19199">https://arxiv.org/abs/2502.19199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19199">https://arxiv.org/pdf/2502.19199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19199]] EGR-Net: A Novel Embedding Gramian Representation CNN for Intelligent Fault Diagnosis(https://arxiv.org/abs/2502.19199)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Feature extraction is crucial in intelligent fault diagnosis of rotating machinery. It is easier for convolutional neural networks(CNNs) to visually recognize and learn fault features by converting the complicated one-dimensional (1D) vibrational signals into two-dimensional (2D) images with simple textures. However, the existing representation methods for encoding 1D signals as images have two main problems, including complicated computation and low separability. Meanwhile, the existing 2D-CNN fault diagnosis methods taking 2D images as the only inputs still suffer from the inevitable information loss because of the conversion process. Considering the above issues, this paper proposes a new 1D-to-2D conversion method called Embedding Gramian Representation (EGR), which is easy to calculate and shows good separability. In EGR, 1D signals are projected in the embedding space and the intrinsic periodicity of vibrational signals is captured enabling the faulty characteristics contained in raw signals to be uncovered. Second, aiming at the information loss problem of existing CNN models with the single input of converted images, a double-branch EGR-based CNN, called EGR-Net, is proposed to learn faulty features from both raw signal feature maps and their corresponding EGRs. The bridge connection is designed to improve the feature learning interaction between the two branches. Widely used open domain gearbox dataset and bearing dataset are used to verify the effectiveness and efficiency of the proposed methods. EGR-Net is compared with traditional and state-of-the-art approaches, and the results show that the proposed method can deliver enhanced performance.</li>
</ul>

<h3>Title: HDM: Hybrid Diffusion Model for Unified Image Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Zekang Weng, Jinjin Shi, Jinwei Wang, Zeming Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19200">https://arxiv.org/abs/2502.19200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19200">https://arxiv.org/pdf/2502.19200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19200]] HDM: Hybrid Diffusion Model for Unified Image Anomaly Detection(https://arxiv.org/abs/2502.19200)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image anomaly detection plays a vital role in applications such as industrial quality inspection and medical imaging, where it directly contributes to improving product quality and system reliability. However, existing methods often struggle with complex and diverse anomaly patterns. In particular, the separation between generation and discrimination tasks limits the effective coordination between anomaly sample generation and anomaly region detection. To address these challenges, we propose a novel hybrid diffusion model (HDM) that integrates generation and discrimination into a unified framework. The model consists of three key modules: the Diffusion Anomaly Generation Module (DAGM), the Diffusion Discriminative Module (DDM), and the Probability Optimization Module (POM). DAGM generates realistic and diverse anomaly samples, improving their representativeness. DDM then applies a reverse diffusion process to capture the differences between generated and normal samples, enabling precise anomaly region detection and localization based on probability distributions. POM refines the probability distributions during both the generation and discrimination phases, ensuring high-quality samples are used for training. Extensive experiments on multiple industrial image datasets demonstrate that our method outperforms state-of-the-art approaches, significantly improving both image-level and pixel-level anomaly detection performance, as measured by AUROC.</li>
</ul>

<h3>Title: LiGT: Layout-infused Generative Transformer for Visual Question Answering on Vietnamese Receipts</h3>
<ul>
<li><strong>Authors: </strong>Thanh-Phong Le, Trung Le Chi Phan, Nghia Hieu Nguyen, Kiet Van Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19202">https://arxiv.org/abs/2502.19202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19202">https://arxiv.org/pdf/2502.19202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19202]] LiGT: Layout-infused Generative Transformer for Visual Question Answering on Vietnamese Receipts(https://arxiv.org/abs/2502.19202)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>\textbf{Purpose:} Document Visual Question Answering (document VQA) challenges multimodal systems to holistically handle textual, layout, and visual modalities to provide appropriate answers. Document VQA has gained popularity in recent years due to the increasing amount of documents and the high demand for digitization. Nonetheless, most of document VQA datasets are developed in high-resource languages such as English. \textbf{Methods:} In this paper, we present ReceiptVQA (\textbf{Receipt} \textbf{V}isual \textbf{Q}uestion \textbf{A}nswering), the initial large-scale document VQA dataset in Vietnamese dedicated to receipts, a document kind with high commercial potentials. The dataset encompasses \textbf{9,000+} receipt images and \textbf{60,000+} manually annotated question-answer pairs. In addition to our study, we introduce LiGT (\textbf{L}ayout-\textbf{i}nfused \textbf{G}enerative \textbf{T}ransformer), a layout-aware encoder-decoder architecture designed to leverage embedding layers of language models to operate layout embeddings, minimizing the use of additional neural modules. \textbf{Results:} Experiments on ReceiptVQA show that our architecture yielded promising performance, achieving competitive results compared with outstanding baselines. Furthermore, throughout analyzing experimental results, we found evident patterns that employing encoder-only model architectures has considerable disadvantages in comparison to architectures that can generate answers. We also observed that it is necessary to combine multiple modalities to tackle our dataset, despite the critical role of semantic understanding from language models. \textbf{Conclusion:} We hope that our work will encourage and facilitate future development in Vietnamese document VQA, contributing to a diverse multimodal research community in the Vietnamese language.</li>
</ul>

<h3>Title: Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator</h3>
<ul>
<li><strong>Authors: </strong>Xiankang He, Dongyan Guo, Hongji Li, Ruibo Li, Ying Cui, Chi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19204">https://arxiv.org/abs/2502.19204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19204">https://arxiv.org/pdf/2502.19204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19204]] Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator(https://arxiv.org/abs/2502.19204)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation (MDE) aims to predict scene depth from a single RGB image and plays a crucial role in 3D scene understanding. Recent advances in zero-shot MDE leverage normalized depth representations and distillation-based learning to improve generalization across diverse scenes. However, current depth normalization methods for distillation, relying on global normalization, can amplify noisy pseudo-labels, reducing distillation effectiveness. In this paper, we systematically analyze the impact of different depth normalization strategies on pseudo-label distillation. Based on our findings, we propose Cross-Context Distillation, which integrates global and local depth cues to enhance pseudo-label quality. Additionally, we introduce a multi-teacher distillation framework that leverages complementary strengths of different depth estimation models, leading to more robust and accurate depth predictions. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, both quantitatively and qualitatively.</li>
</ul>

<h3>Title: FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Nakyeong Yang, Minsung Kim, Seunghyun Yoon, Joongbo Shin, Kyomin Jung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19207">https://arxiv.org/abs/2502.19207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19207">https://arxiv.org/pdf/2502.19207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19207]] FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge(https://arxiv.org/abs/2502.19207)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Various studies have attempted to remove sensitive or private knowledge from a language model to prevent its unauthorized exposure. However, prior studies have overlooked the complex and interconnected nature of knowledge, where related knowledge must be carefully examined. Specifically, they have failed to evaluate whether an unlearning method faithfully erases interconnected knowledge that should be removed, retaining knowledge that appears relevant but exists in a completely different context. To resolve this problem, we first define a new concept called superficial unlearning, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge. Based on the definition, we introduce a new benchmark, FaithUn, to analyze and evaluate the faithfulness of unlearning in real-world knowledge QA settings. Furthermore, we propose a novel unlearning method, KLUE, which updates only knowledge-related neurons to achieve faithful unlearning. KLUE identifies knowledge neurons using an explainability method and updates only those neurons using selected unforgotten samples. Experimental results demonstrate that widely-used unlearning methods fail to ensure faithful unlearning, while our method shows significant effectiveness in real-world QA unlearning.</li>
</ul>

<h3>Title: MultiConAD: A Unified Multilingual Conversational Dataset for Early Alzheimer's Detection</h3>
<ul>
<li><strong>Authors: </strong>Arezo Shakeri, Mina Farmanbar, Krisztian Balog</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19208">https://arxiv.org/abs/2502.19208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19208">https://arxiv.org/pdf/2502.19208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19208]] MultiConAD: A Unified Multilingual Conversational Dataset for Early Alzheimer's Detection(https://arxiv.org/abs/2502.19208)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Dementia is a progressive cognitive syndrome with Alzheimer's disease (AD) as the leading cause. Conversation-based AD detection offers a cost-effective alternative to clinical methods, as language dysfunction is an early biomarker of AD. However, most prior research has framed AD detection as a binary classification problem, limiting the ability to identify Mild Cognitive Impairment (MCI)-a crucial stage for early intervention. Also, studies primarily rely on single-language datasets, mainly in English, restricting cross-language generalizability. To address this gap, we make three key contributions. First, we introduce a novel, multilingual dataset for AD detection by unifying 16 publicly available dementia-related conversational datasets. This corpus spans English, Spanish, Chinese, and Greek and incorporates both audio and text data derived from a variety of cognitive assessment tasks. Second, we perform finer-grained classification, including MCI, and evaluate various classifiers using sparse and dense text representations. Third, we conduct experiments in monolingual and multilingual settings, finding that some languages benefit from multilingual training while others perform better independently. This study highlights the challenges in multilingual AD detection and enables future research on both language-specific approaches and techniques aimed at improving model generalization and robustness.</li>
</ul>

<h3>Title: Bi'an: A Bilingual Benchmark and Model for Hallucination Detection in Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhouyu Jiang, Mengshu Sun, Zhiqiang Zhang, Lei Liang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19209">https://arxiv.org/abs/2502.19209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19209">https://arxiv.org/pdf/2502.19209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19209]] Bi'an: A Bilingual Benchmark and Model for Hallucination Detection in Retrieval-Augmented Generation(https://arxiv.org/abs/2502.19209)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) effectively reduces hallucinations in Large Language Models (LLMs) but can still produce inconsistent or unsupported content. Although LLM-as-a-Judge is widely used for RAG hallucination detection due to its implementation simplicity, it faces two main challenges: the absence of comprehensive evaluation benchmarks and the lack of domain-optimized judge models. To bridge these gaps, we introduce \textbf{Bi'an}, a novel framework featuring a bilingual benchmark dataset and lightweight judge models. The dataset supports rigorous evaluation across multiple RAG scenarios, while the judge models are fine-tuned from compact open-source LLMs. Extensive experimental evaluations on Bi'anBench show our 14B model outperforms baseline models with over five times larger parameter scales and rivals state-of-the-art closed-source LLMs. We will release our data and models soon at this https URL.</li>
</ul>

<h3>Title: Negation-Induced Forgetting in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Francesca Capuano, Ellen Boschert, Barbara Kaup</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19211">https://arxiv.org/abs/2502.19211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19211">https://arxiv.org/pdf/2502.19211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19211]] Negation-Induced Forgetting in LLMs(https://arxiv.org/abs/2502.19211)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The study explores whether Large Language Models (LLMs) exhibit negation-induced forgetting (NIF), a cognitive phenomenon observed in humans where negating incorrect attributes of an object or event leads to diminished recall of this object or event compared to affirming correct attributes (Mayo et al., 2014; Zang et al., 2023). We adapted Zang et al. (2023) experimental framework to test this effect in ChatGPT-3.5, GPT-4o mini and Llama3-70b-instruct. Our results show that ChatGPT-3.5 exhibits NIF, with negated information being less likely to be recalled than affirmed information. GPT-4o-mini showed a marginally significant NIF effect, while LLaMA-3-70B did not exhibit NIF. The findings provide initial evidence of negation-induced forgetting in some LLMs, suggesting that similar cognitive biases may emerge in these models. This work is a preliminary step in understanding how memory-related phenomena manifest in LLMs.</li>
</ul>

<h3>Title: A Lightweight and Extensible Cell Segmentation and Classification Model for Whole Slide Images</h3>
<ul>
<li><strong>Authors: </strong>Nikita Shvetsov, Thomas K. Kilvaer, Masoud Tafavvoghi, Anders Sildnes, Kajsa Mllersen, Lill-Tove Rasmussen Busund, Lars Ailo Bongo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19217">https://arxiv.org/abs/2502.19217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19217">https://arxiv.org/pdf/2502.19217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19217]] A Lightweight and Extensible Cell Segmentation and Classification Model for Whole Slide Images(https://arxiv.org/abs/2502.19217)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Developing clinically useful cell-level analysis tools in digital pathology remains challenging due to limitations in dataset granularity, inconsistent annotations, high computational demands, and difficulties integrating new technologies into workflows. To address these issues, we propose a solution that enhances data quality, model performance, and usability by creating a lightweight, extensible cell segmentation and classification model. First, we update data labels through cross-relabeling to refine annotations of PanNuke and MoNuSAC, producing a unified dataset with seven distinct cell types. Second, we leverage the H-Optimus foundation model as a fixed encoder to improve feature representation for simultaneous segmentation and classification tasks. Third, to address foundation models' computational demands, we distill knowledge to reduce model size and complexity while maintaining comparable performance. Finally, we integrate the distilled model into QuPath, a widely used open-source digital pathology platform. Results demonstrate improved segmentation and classification performance using the H-Optimus-based model compared to a CNN-based model. Specifically, average $R^2$ improved from 0.575 to 0.871, and average $PQ$ score improved from 0.450 to 0.492, indicating better alignment with actual cell counts and enhanced segmentation quality. The distilled model maintains comparable performance while reducing parameter count by a factor of 48. By reducing computational complexity and integrating into workflows, this approach may significantly impact diagnostics, reduce pathologist workload, and improve outcomes. Although the method shows promise, extensive validation is necessary prior to clinical deployment.</li>
</ul>

<h3>Title: Two Heads Are Better Than One: Dual-Model Verbal Reflection at Inference-Time</h3>
<ul>
<li><strong>Authors: </strong>Jiazheng Li, Yuxiang Zhou, Junru Lu, Gladys Tyen, Lin Gui, Cesare Aloisi, Yulan He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19230">https://arxiv.org/abs/2502.19230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19230">https://arxiv.org/pdf/2502.19230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19230]] Two Heads Are Better Than One: Dual-Model Verbal Reflection at Inference-Time(https://arxiv.org/abs/2502.19230)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often struggle with complex reasoning scenarios. While preference optimization methods enhance reasoning performance through training, they often lack transparency in why one reasoning outcome is preferred over another. Verbal reflection techniques improve explainability but are limited in LLMs' critique and refinement capacity. To address these challenges, we introduce a contrastive reflection synthesis pipeline that enhances the accuracy and depth of LLM-generated reflections. We further propose a dual-model reasoning framework within a verbal reinforcement learning paradigm, decoupling inference-time self-reflection into specialized, trained models for reasoning critique and refinement. Extensive experiments show that our framework outperforms traditional preference optimization methods across all evaluation metrics. Our findings also show that "two heads are better than one", demonstrating that a collaborative Reasoner-Critic model achieves superior reasoning performance and transparency, compared to single-model approaches.</li>
</ul>

<h3>Title: ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding</h3>
<ul>
<li><strong>Authors: </strong>Qihang Peng, Henry Zheng, Gao Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19247">https://arxiv.org/abs/2502.19247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19247">https://arxiv.org/pdf/2502.19247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19247]] ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding(https://arxiv.org/abs/2502.19247)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Embodied intelligence requires agents to interact with 3D environments in real time based on language instructions. A foundational task in this domain is ego-centric 3D visual grounding. However, the point clouds rendered from RGB-D images retain a large amount of redundant background data and inherent noise, both of which can interfere with the manifold structure of the target regions. Existing point cloud enhancement methods often require a tedious process to improve the manifold, which is not suitable for real-time tasks. We propose Proxy Transformation suitable for multimodal task to efficiently improve the point cloud manifold. Our method first leverages Deformable Point Clustering to identify the point cloud sub-manifolds in target regions. Then, we propose a Proxy Attention module that utilizes multimodal proxies to guide point cloud transformation. Built upon Proxy Attention, we design a submanifold transformation generation module where textual information globally guides translation vectors for different submanifolds, optimizing relative spatial relationships of target regions. Simultaneously, image information guides linear transformations within each submanifold, refining the local point cloud manifold of target regions. Extensive experiments demonstrate that Proxy Transformation significantly outperforms all existing methods, achieving an impressive improvement of 7.49% on easy targets and 4.60% on hard targets, while reducing the computational overhead of attention blocks by 40.6%. These results establish a new SOTA in ego-centric 3D visual grounding, showcasing the effectiveness and robustness of our approach.</li>
</ul>

<h3>Title: Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases</h3>
<ul>
<li><strong>Authors: </strong>Michael Y. Hu, Jackson Petty, Chuan Shi, William Merrill, Tal Linzen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19249">https://arxiv.org/abs/2502.19249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19249">https://arxiv.org/pdf/2502.19249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19249]] Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases(https://arxiv.org/abs/2502.19249)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Pretraining language models on formal languages can improve their acquisition of natural language, but it is unclear which features of the formal language impart an inductive bias that leads to effective transfer. Drawing on insights from linguistics and complexity theory, we hypothesize that effective transfer occurs when the formal language both captures dependency structures in natural language and remains within the computational limitations of the model architecture. Focusing on transformers, we find that formal languages with both these properties enable language models to achieve lower loss on natural language and better linguistic generalization compared to other languages. In fact, pre-pretraining, or training on formal-then-natural language, reduces loss more efficiently than the same amount of natural language. For a 1B-parameter language model trained on roughly 1.6B tokens of natural language, pre-pretraining achieves the same loss and better linguistic generalization with a 33% smaller token budget. We also give mechanistic evidence of cross-task transfer from formal to natural language: attention heads acquired during formal language pretraining remain crucial for the model's performance on syntactic evaluations.</li>
</ul>

<h3>Title: Poster: Long PHP webshell files detection based on sliding window attention</h3>
<ul>
<li><strong>Authors: </strong>Zhiqiang Wang, Haoyu Wang, Lu Hao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19257">https://arxiv.org/abs/2502.19257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19257">https://arxiv.org/pdf/2502.19257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19257]] Poster: Long PHP webshell files detection based on sliding window attention(https://arxiv.org/abs/2502.19257)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Webshell is a type of backdoor, and web applications are widely exposed to webshell injection attacks. Therefore, it is important to study webshell detection techniques. In this study, we propose a webshell detection method. We first convert PHP source code to opcodes and then extract Opcode Double-Tuples (ODTs). Next, we combine CodeBert and FastText models for feature representation and classification. To address the challenge that deep learning methods have difficulty detecting long webshell files, we introduce a sliding window attention mechanism. This approach effectively captures malicious behavior within long files. Experimental results show that our method reaches high accuracy in webshell detection, solving the problem of traditional methods that struggle to address new webshell variants and anti-detection techniques.</li>
</ul>

<h3>Title: Neural Antidote: Class-Wise Prompt Tuning for Purifying Backdoors in Pre-trained Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Kong, Hao Fang, Sihang Guo, Chenxi Qing, Bin Chen, Bin Wang, Shu-Tao Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19269">https://arxiv.org/abs/2502.19269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19269">https://arxiv.org/pdf/2502.19269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19269]] Neural Antidote: Class-Wise Prompt Tuning for Purifying Backdoors in Pre-trained Vision-Language Models(https://arxiv.org/abs/2502.19269)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>While pre-trained Vision-Language Models (VLMs) such as CLIP exhibit excellent representational capabilities for multimodal data, recent studies have shown that they are vulnerable to backdoor attacks. To alleviate the threat, existing defense strategies primarily focus on fine-tuning the entire suspicious model, yet offer only marginal resistance to state-of-the-art attacks and often result in a decrease in clean accuracy, particularly in data-limited scenarios. Their failure may be attributed to the mismatch between insufficient fine-tuning data and massive parameters in VLMs. To address this challenge, we propose Class-wise Backdoor Prompt Tuning (CBPT) defense, an efficient and effective method that operates on the text prompts to indirectly purify the poisoned VLMs. Specifically, we first employ the advanced contrastive learning via our carefully crafted positive and negative samples, to effectively invert the backdoor triggers that are potentially adopted by the attacker. Once the dummy trigger is established, we utilize the efficient prompt tuning technique to optimize these class-wise text prompts for modifying the model's decision boundary to further reclassify the feature regions of backdoor triggers. Extensive experiments demonstrate that CBPT significantly mitigates backdoor threats while preserving model utility, e.g. an average Clean Accuracy (CA) of 58.86\% and an Attack Success Rate (ASR) of 0.39\% across seven mainstream backdoor attacks. These results underscore the superiority of our prompt purifying design to strengthen model robustness against backdoor attacks.</li>
</ul>

<h3>Title: Disentangled VAD Representations via a Variational Framework for Political Stance Detection</h3>
<ul>
<li><strong>Authors: </strong>Beiyu Xu, Zhiwei Liu, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19276">https://arxiv.org/abs/2502.19276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19276">https://arxiv.org/pdf/2502.19276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19276]] Disentangled VAD Representations via a Variational Framework for Political Stance Detection(https://arxiv.org/abs/2502.19276)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The stance detection task aims to categorise the stance regarding specified targets. Current methods face challenges in effectively integrating sentiment information for stance detection. Moreover, the role of highly granular sentiment labelling in stance detection has been largely overlooked. This study presents a novel stance detection framework utilizing a variational autoencoder (VAE) to disentangle latent emotional features-value, arousal, and dominance (VAD)-from political discourse on social media. This approach addresses limitations in current methods, particularly in in-target and cross-target stance detection scenarios. This research uses an advanced emotional annotation tool to annotate seven-class sentiment labels for P-STANCE. Evaluations on benchmark datasets, including P-STANCE and SemEval-2016, reveal that PoliStance-VAE achieves state-of-the-art performance, surpassing models like BERT, BERTweet, and GPT-4o. PoliStance-VAE offers a robust and interpretable solution for stance detection, demonstrating the effectiveness of integrating nuanced emotional representations. This framework paves the way for advancements in natural language processing tasks, particularly those requiring detailed emotional understanding.</li>
</ul>

<h3>Title: Efficient Federated Search for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Rachid Guerraoui, Anne-Marie Kermarrec, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19280">https://arxiv.org/abs/2502.19280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19280">https://arxiv.org/pdf/2502.19280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19280]] Efficient Federated Search for Retrieval-Augmented Generation(https://arxiv.org/abs/2502.19280)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities across various domains but remain susceptible to hallucinations and inconsistencies, limiting their reliability. Retrieval-augmented generation (RAG) mitigates these issues by grounding model responses in external knowledge sources. Existing RAG workflows often leverage a single vector database, which is impractical in the common setting where information is distributed across multiple repositories. We introduce RAGRoute, a novel mechanism for federated RAG search. RAGRoute dynamically selects relevant data sources at query time using a lightweight neural network classifier. By not querying every data source, this approach significantly reduces query overhead, improves retrieval efficiency, and minimizes the retrieval of irrelevant information. We evaluate RAGRoute using the MIRAGE and MMLU benchmarks and demonstrate its effectiveness in retrieving relevant documents while reducing the number of queries. RAGRoute reduces the total number of queries up to 77.5% and communication volume up to 76.2%.</li>
</ul>

<h3>Title: Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go Beyond</h3>
<ul>
<li><strong>Authors: </strong>Qizhou Wang, Jin Peng Zhou, Zhanke Zhou, Saebyeol Shin, Bo Han, Kilian Q. Weinberger</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19301">https://arxiv.org/abs/2502.19301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19301">https://arxiv.org/pdf/2502.19301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19301]] Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go Beyond(https://arxiv.org/abs/2502.19301)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) should undergo rigorous audits to identify potential risks, such as copyright and privacy infringements. Once these risks emerge, timely updates are crucial to remove undesirable responses, ensuring legal and safe model usage. It has spurred recent research into LLM unlearning, focusing on erasing targeted undesirable knowledge without compromising the integrity of other, non-targeted responses. Existing studies have introduced various unlearning objectives to pursue LLM unlearning without necessitating complete retraining. However, each of these objectives has unique properties, and no unified framework is currently available to comprehend them thoroughly. To fill the gap, we propose a toolkit of the gradient effect (G-effect), quantifying the impacts of unlearning objectives on model performance from a gradient perspective. A notable advantage is its broad ability to detail the unlearning impacts from various aspects across instances, updating steps, and LLM layers. Accordingly, the G-effect offers new insights into identifying drawbacks of existing unlearning objectives, further motivating us to explore a series of new solutions for their mitigation and improvements. Finally, we outline promising directions that merit further studies, aiming at contributing to the community to advance this important field.</li>
</ul>

<h3>Title: Corporate Fraud Detection in Rich-yet-Noisy Financial Graph</h3>
<ul>
<li><strong>Authors: </strong>Shiqi Wang, Zhibo Zhang, Libing Fang, Cam-Tu Nguyen, Wenzhon Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-fin.RM, q-fin.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19305">https://arxiv.org/abs/2502.19305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19305">https://arxiv.org/pdf/2502.19305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19305]] Corporate Fraud Detection in Rich-yet-Noisy Financial Graph(https://arxiv.org/abs/2502.19305)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Corporate fraud detection aims to automatically recognize companies that conduct wrongful activities such as fraudulent financial statements or illegal insider trading. Previous learning-based methods fail to effectively integrate rich interactions in the company network. To close this gap, we collect 18-year financial records in China to form three graph datasets with fraud labels. We analyze the characteristics of the financial graphs, highlighting two pronounced issues: (1) information overload: the dominance of (noisy) non-company nodes over company nodes hinders the message-passing process in Graph Convolution Networks (GCN); and (2) hidden fraud: there exists a large percentage of possible undetected violations in the collected data. The hidden fraud problem will introduce noisy labels in the training dataset and compromise fraud detection results. To handle such challenges, we propose a novel graph-based method, namely, Knowledge-enhanced GCN with Robust Two-stage Learning (${\rm KeGCN}_{R}$), which leverages Knowledge Graph Embeddings to mitigate the information overload and effectively learns rich representations. The proposed model adopts a two-stage learning method to enhance robustness against hidden frauds. Extensive experimental results not only confirm the importance of interactions but also show the superiority of ${\rm KeGCN}_{R}$ over a number of strong baselines in terms of fraud detection effectiveness and robustness.</li>
</ul>

<h3>Title: Anomaly Detection in Complex Dynamical Systems: A Systematic Framework Using Embedding Theory and Physics-Inspired Consistency</h3>
<ul>
<li><strong>Authors: </strong>Michael Somma, Thomas Gallien, Branka Stojanovic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19307">https://arxiv.org/abs/2502.19307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19307">https://arxiv.org/pdf/2502.19307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19307]] Anomaly Detection in Complex Dynamical Systems: A Systematic Framework Using Embedding Theory and Physics-Inspired Consistency(https://arxiv.org/abs/2502.19307)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, transformer</a></li>
<li><strong>Abstract: </strong>Anomaly detection in complex dynamical systems is essential for ensuring reliability, safety, and efficiency in industrial and cyber-physical infrastructures. Predictive maintenance helps prevent costly failures, while cybersecurity monitoring has become critical as digitized systems face growing threats. Many of these systems exhibit oscillatory behaviors and bounded motion, requiring anomaly detection methods that capture structured temporal dependencies while adhering to physical consistency principles. In this work, we propose a system-theoretic approach to anomaly detection, grounded in classical embedding theory and physics-inspired consistency principles. We build upon the Fractal Whitney Embedding Prevalence Theorem, extending traditional embedding techniques to complex system dynamics. Additionally, we introduce state-derivative pairs as an embedding strategy to capture system evolution. To enforce temporal coherence, we develop a Temporal Differential Consistency Autoencoder (TDC-AE), incorporating a TDC-Loss that aligns the approximated derivatives of latent variables with their dynamic representations. We evaluate our method on the C-MAPSS dataset, a benchmark for turbofan aeroengine degradation. TDC-AE outperforms LSTMs and Transformers while achieving a 200x reduction in MAC operations, making it particularly suited for lightweight edge computing. Our findings support the hypothesis that anomalies disrupt stable system dynamics, providing a robust, interpretable signal for anomaly detection.</li>
</ul>

<h3>Title: CoopDETR: A Unified Cooperative Perception Framework for 3D Detection via Object Query</h3>
<ul>
<li><strong>Authors: </strong>Zhe Wang, Shaocong Xu, Xucai Zhuang, Tongda Xu, Yan Wang, Jingjing Liu, Yilun Chen, Ya-Qin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19313">https://arxiv.org/abs/2502.19313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19313">https://arxiv.org/pdf/2502.19313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19313]] CoopDETR: A Unified Cooperative Perception Framework for 3D Detection via Object Query(https://arxiv.org/abs/2502.19313)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Cooperative perception enhances the individual perception capabilities of autonomous vehicles (AVs) by providing a comprehensive view of the environment. However, balancing perception performance and transmission costs remains a significant challenge. Current approaches that transmit region-level features across agents are limited in interpretability and demand substantial bandwidth, making them unsuitable for practical applications. In this work, we propose CoopDETR, a novel cooperative perception framework that introduces object-level feature cooperation via object query. Our framework consists of two key modules: single-agent query generation, which efficiently encodes raw sensor data into object queries, reducing transmission cost while preserving essential information for detection; and cross-agent query fusion, which includes Spatial Query Matching (SQM) and Object Query Aggregation (OQA) to enable effective interaction between queries. Our experiments on the OPV2V and V2XSet datasets demonstrate that CoopDETR achieves state-of-the-art performance and significantly reduces transmission costs to 1/782 of previous methods.</li>
</ul>

<h3>Title: Model Adaptation: Unsupervised Domain Adaptation without Source Data</h3>
<ul>
<li><strong>Authors: </strong>Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, Si Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19316">https://arxiv.org/abs/2502.19316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19316">https://arxiv.org/pdf/2502.19316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19316]] Model Adaptation: Unsupervised Domain Adaptation without Source Data(https://arxiv.org/abs/2502.19316)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate a challenging unsupervised domain adaptation setting -- unsupervised model adaptation. We aim to explore how to rely only on unlabeled target data to improve performance of an existing source prediction model on the target domain, since labeled source data may not be available in some real-world scenarios due to data privacy issues. For this purpose, we propose a new framework, which is referred to as collaborative class conditional generative adversarial net to bypass the dependence on the source data. Specifically, the prediction model is to be improved through generated target-style data, which provides more accurate guidance for the generator. As a result, the generator and the prediction model can collaborate with each other without source data. Furthermore, due to the lack of supervision from source data, we propose a weight constraint that encourages similarity to the source model. A clustering-based regularization is also introduced to produce more discriminative features in the target domain. Compared to conventional domain adaptation methods, our model achieves superior performance on multiple adaptation tasks with only unlabeled target data, which verifies its effectiveness in this challenging setting.</li>
</ul>

<h3>Title: Shh, don't say that! Domain Certification in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Cornelius Emde, Alasdair Paren, Preetham Arvind, Maxime Kayser, Tom Rainforth, Thomas Lukasiewicz, Bernard Ghanem, Philip H.S. Torr, Adel Bibi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19320">https://arxiv.org/abs/2502.19320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19320">https://arxiv.org/pdf/2502.19320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19320]] Shh, don't say that! Domain Certification in LLMs(https://arxiv.org/abs/2502.19320)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are often deployed to perform constrained tasks, with narrow domains. For example, customer support bots can be built on top of LLMs, relying on their broad language understanding and capabilities to enhance performance. However, these LLMs are adversarially susceptible, potentially generating outputs outside the intended domain. To formalize, assess, and mitigate this risk, we introduce domain certification; a guarantee that accurately characterizes the out-of-domain behavior of language models. We then propose a simple yet effective approach, which we call VALID that provides adversarial bounds as a certificate. Finally, we evaluate our method across a diverse set of datasets, demonstrating that it yields meaningful certificates, which bound the probability of out-of-domain samples tightly with minimum penalty to refusal behavior.</li>
</ul>

<h3>Title: Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems</h3>
<ul>
<li><strong>Authors: </strong>Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19328">https://arxiv.org/abs/2502.19328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19328">https://arxiv.org/pdf/2502.19328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19328]] Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems(https://arxiv.org/abs/2502.19328)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects to provide reliable rewards. We empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards. We conduct comprehensive experiments on existing reward model benchmarks and inference time best-of-n searches on real-world downstream tasks. RewardAgent significantly outperforms vanilla reward models, demonstrating its effectiveness. We further construct training preference pairs using RewardAgent and train an LLM with the DPO objective, achieving superior performance on various NLP benchmarks compared to conventional reward models. Our codes are publicly released to facilitate further research (this https URL).</li>
</ul>

<h3>Title: Consistent Amortized Clustering via Generative Flow Networks</h3>
<ul>
<li><strong>Authors: </strong>Irit Chelly, Roy Uziel, Oren Freifeld, Ari Pakman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19337">https://arxiv.org/abs/2502.19337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19337">https://arxiv.org/pdf/2502.19337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19337]] Consistent Amortized Clustering via Generative Flow Networks(https://arxiv.org/abs/2502.19337)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural models for amortized probabilistic clustering yield samples of cluster labels given a set-structured input, while avoiding lengthy Markov chain runs and the need for explicit data likelihoods. Existing methods which label each data point sequentially, like the Neural Clustering Process, often lead to cluster assignments highly dependent on the data order. Alternatively, methods that sequentially create full clusters, do not provide assignment probabilities. In this paper, we introduce GFNCP, a novel framework for amortized clustering. GFNCP is formulated as a Generative Flow Network with a shared energy-based parametrization of policy and reward. We show that the flow matching conditions are equivalent to consistency of the clustering posterior under marginalization, which in turn implies order invariance. GFNCP also outperforms existing methods in clustering performance on both synthetic and real-world data.</li>
</ul>

<h3>Title: Evaluating LLMs and Pre-trained Models for Text Summarization Across Diverse Datasets</h3>
<ul>
<li><strong>Authors: </strong>Tohida Rehman, Soumabha Ghosh, Kuntal Das, Souvik Bhattacharjee, Debarshi Kumar Sanyal, Samiran Chattopadhyay</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19339">https://arxiv.org/abs/2502.19339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19339">https://arxiv.org/pdf/2502.19339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19339]] Evaluating LLMs and Pre-trained Models for Text Summarization Across Diverse Datasets(https://arxiv.org/abs/2502.19339)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text summarization plays a crucial role in natural language processing by condensing large volumes of text into concise and coherent summaries. As digital content continues to grow rapidly and the demand for effective information retrieval increases, text summarization has become a focal point of research in recent years. This study offers a thorough evaluation of four leading pre-trained and open-source large language models: BART, FLAN-T5, LLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News Summary, XSum, and BBC News. The evaluation employs widely recognized automatic metrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess the models' capabilities in generating coherent and informative summaries. The results reveal the comparative strengths and limitations of these models in processing various text types.</li>
</ul>

<h3>Title: Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?</h3>
<ul>
<li><strong>Authors: </strong>Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Wenbo Su, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19361">https://arxiv.org/abs/2502.19361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19361">https://arxiv.org/pdf/2502.19361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19361]] Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?(https://arxiv.org/abs/2502.19361)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique abilities of existing LLMs on these long CoTs, we introduce the DeltaBench, including the generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the ability to detect errors in long CoT reasoning. Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models. Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process, which aims to investigate the boundaries and limitations of existing PRMs and critic models. Finally, we hope that DeltaBench could guide developers to better understand the long CoT reasoning abilities of their models.</li>
</ul>

<h3>Title: DataMan: Data Manager for Pre-training Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ru Peng, Kexin Yang, Yawen Zeng, Junyang Lin, Dayiheng Liu, Junbo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19363">https://arxiv.org/abs/2502.19363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19363">https://arxiv.org/pdf/2502.19363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19363]] DataMan: Data Manager for Pre-training Large Language Models(https://arxiv.org/abs/2502.19363)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The performance emergence of large language models (LLMs) driven by data scaling laws makes the selection of pre-training data increasingly important. However, existing methods rely on limited heuristics and human intuition, lacking comprehensive and clear guidelines. To address this, we are inspired by ``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit its performance. As its pre-training capabilities are related to perplexity (PPL), we derive 14 quality criteria from the causes of text perplexity anomalies and introduce 15 common application domains to support domain mixing. In this paper, we train a Data Manager (DataMan) to learn quality ratings and domain recognition from pointwise rating, and use it to annotate a 447B token pre-training corpus with 14 quality ratings and domain type. Our experiments validate our approach, using DataMan to select 30B tokens to train a 1.3B-parameter language model, demonstrating significant improvements in in-context learning (ICL), perplexity, and instruction-following ability over the state-of-the-art baseline. The best-performing model, based on the Overall Score l=5 surpasses a model trained with 50% more data using uniform sampling. We continue pre-training with high-rated, domain-specific data annotated by DataMan to enhance domain-specific ICL performance and thus verify DataMan's domain mixing ability. Our findings emphasize the importance of quality ranking, the complementary nature of quality criteria, and their low correlation with perplexity, analyzing misalignment between PPL and ICL performance. We also thoroughly analyzed our pre-training dataset, examining its composition, the distribution of quality ratings, and the original document sources.</li>
</ul>

<h3>Title: Deep Learning For Time Series Analysis With Application On Human Motion</h3>
<ul>
<li><strong>Authors: </strong>Ali Ismail-Fawaz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19364">https://arxiv.org/abs/2502.19364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19364">https://arxiv.org/pdf/2502.19364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19364]] Deep Learning For Time Series Analysis With Application On Human Motion(https://arxiv.org/abs/2502.19364)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Time series data, defined by equally spaced points over time, is essential in fields like medicine, telecommunications, and energy. Analyzing it involves tasks such as classification, clustering, prototyping, and regression. Classification identifies normal vs. abnormal movements in skeleton-based motion sequences, clustering detects stock market behavior patterns, prototyping expands physical therapy datasets, and regression predicts patient recovery. Deep learning has recently gained traction in time series analysis due to its success in other domains. This thesis leverages deep learning to enhance classification with feature engineering, introduce foundation models, and develop a compact yet state-of-the-art architecture. We also address limited labeled data with self-supervised learning. Our contributions apply to real-world tasks, including human motion analysis for action recognition and rehabilitation. We introduce a generative model for human motion data, valuable for cinematic production and gaming. For prototyping, we propose a shape-based synthetic sample generation method to support regression models when data is scarce. Lastly, we critically evaluate discriminative and generative models, identifying limitations in current methodologies and advocating for a robust, standardized evaluation framework. Our experiments on public datasets provide novel insights and methodologies, advancing time series analysis with practical applications.</li>
</ul>

<h3>Title: General Reasoning Requires Learning to Reason from the Get-go</h3>
<ul>
<li><strong>Authors: </strong>Seungwook Han, Jyothish Pari, Samuel J. Gershman, Pulkit Agrawal</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19402">https://arxiv.org/abs/2502.19402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19402">https://arxiv.org/pdf/2502.19402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19402]] General Reasoning Requires Learning to Reason from the Get-go(https://arxiv.org/abs/2502.19402)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive real-world utility, exemplifying artificial useful intelligence (AUI). However, their ability to reason adaptively and robustly -- the hallmarks of artificial general intelligence (AGI) -- remains fragile. While LLMs seemingly succeed in commonsense reasoning, programming, and mathematics, they struggle to generalize algorithmic understanding across novel contexts. Our experiments with algorithmic tasks in esoteric programming languages reveal that LLM's reasoning overfits to the training data and is limited in its transferability. We hypothesize that the core issue underlying such limited transferability is the coupling of reasoning and knowledge in LLMs. To transition from AUI to AGI, we propose disentangling knowledge and reasoning through three key directions: (1) pretaining to reason using RL from scratch as an alternative to the widely used next-token prediction pretraining, (2) using a curriculum of synthetic tasks to ease the learning of a \textit{reasoning prior} for RL that can then be transferred to natural language tasks, and (3) learning more generalizable reasoning functions using a small context window to reduce exploiting spurious correlations between tokens. Such a reasoning system coupled with a trained retrieval system and a large external memory bank as a knowledge store can overcome several limitations of existing architectures at learning to reason in novel scenarios.</li>
</ul>

<h3>Title: ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Danae Snchez Villegas, Ingo Ziegler, Desmond Elliott</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19409">https://arxiv.org/abs/2502.19409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19409">https://arxiv.org/pdf/2502.19409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19409]] ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models(https://arxiv.org/abs/2502.19409)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reasoning over sequences of images remains a challenge for multimodal large language models (MLLMs). While recent models incorporate multi-image data during pre-training, they still struggle to recognize sequential structures, often treating images independently. This work introduces ImageChain, a framework that enhances MLLMs with sequential reasoning capabilities over image data by modeling visual sequences as a multi-turn conversation. In ImageChain, images are interleaved with corresponding textual descriptions to form a controlled dialogue that explicitly captures temporal dependencies and narrative progression. Our method optimizes for the task of next-scene description, where the model generates a context-aware description of an upcoming scene based on preceding visual and textual cues. We demonstrate that our approach improves performance on the next-scene description task -- achieving an average improvement from 3.7% to 19% in SimRate, a metric that quantifies semantic similarity to human-annotated ground truths. Moreover, ImageChain achieves robust zero-shot out-of-domain performance in applications ranging from comics to robotics. Extensive experiments validate that instruction-tuning in a multimodal, multi-turn conversation design is key to bridging the gap between static image understanding and temporally-aware reasoning.</li>
</ul>

<h3>Title: Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Dayu Yang, Tianyang Liu, Daoan Zhang, Antoine Simoulin, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Xin Qian, Grey Yang, Jiebo Luo, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19411">https://arxiv.org/abs/2502.19411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19411">https://arxiv.org/pdf/2502.19411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19411]] Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs(https://arxiv.org/abs/2502.19411)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In large language models (LLMs), code and reasoning reinforce each other: code offers an abstract, modular, and logic-driven structure that supports reasoning, while reasoning translates high-level goals into smaller, executable steps that drive more advanced code intelligence. In this study, we examine how code serves as a structured medium for enhancing reasoning: it provides verifiable execution paths, enforces logical decomposition, and enables runtime validation. We also explore how improvements in reasoning have transformed code intelligence from basic completion to advanced capabilities, enabling models to address complex software engineering tasks through planning and debugging. Finally, we identify key challenges and propose future research directions to strengthen this synergy, ultimately improving LLM's performance in both areas.</li>
</ul>

<h3>Title: The Mighty ToRR: A Benchmark for Table Reasoning and Robustness</h3>
<ul>
<li><strong>Authors: </strong>Shir Ashury-Tahan, Yifan Mai, Rajmohan C, Ariel Gera, Yotam Perlitz, Asaf Yehudai, Elron Bandel, Leshem Choshen, Eyal Shnarch, Percy Liang, Michal Shmueli-Scheuer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19412">https://arxiv.org/abs/2502.19412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19412">https://arxiv.org/pdf/2502.19412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19412]] The Mighty ToRR: A Benchmark for Table Reasoning and Robustness(https://arxiv.org/abs/2502.19412)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite its real-world significance, model performance on tabular data remains underexplored, leaving uncertainty about which model to rely on and which prompt configuration to adopt. To address this gap, we create ToRR, a benchmark for Table Reasoning and Robustness, that measures model performance and robustness on table-related tasks. The benchmark includes 10 datasets that cover different types of table reasoning capabilities across varied domains. ToRR goes beyond model performance rankings, and is designed to reflect whether models can handle tabular data consistently and robustly, across a variety of common table representation formats. We present a leaderboard as well as comprehensive analyses of the results of leading models over ToRR. Our results reveal a striking pattern of brittle model behavior, where even strong models are unable to perform robustly on tabular data tasks. Although no specific table format leads to consistently better performance, we show that testing over multiple formats is crucial for reliably estimating model capabilities. Moreover, we show that the reliability boost from testing multiple prompts can be equivalent to adding more test examples. Overall, our findings show that table understanding and reasoning tasks remain a significant challenge.</li>
</ul>

<h3>Title: Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs</h3>
<ul>
<li><strong>Authors: </strong>Christoph Schuhmann, Gollam Rabby, Ameya Prabhu, Tawsif Ahmed, Andreas Hochlehnert, Huu Nguyen, Nick Akinci Heidrich, Ludwig Schmidt, Robert Kaczmarczyk, Sren Auer, Jenia Jitsev, Matthias Bethge</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19413">https://arxiv.org/abs/2502.19413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19413">https://arxiv.org/pdf/2502.19413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19413]] Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs(https://arxiv.org/abs/2502.19413)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Paywalls, licenses and copyright rules often restrict the broad dissemination and reuse of scientific knowledge. We take the position that it is both legally and technically feasible to extract the scientific knowledge in scholarly texts. Current methods, like text embeddings, fail to reliably preserve factual content, and simple paraphrasing may not be legally sound. We urge the community to adopt a new idea: convert scholarly documents into Knowledge Units using LLMs. These units use structured data capturing entities, attributes and relationships without stylistic content. We provide evidence that Knowledge Units: (1) form a legally defensible framework for sharing knowledge from copyrighted research texts, based on legal analyses of German copyright law and U.S. Fair Use doctrine, and (2) preserve most (~95%) factual knowledge from original text, measured by MCQ performance on facts from the original copyrighted text across four research domains. Freeing scientific knowledge from copyright promises transformative benefits for scientific research and education by allowing language models to reuse important facts from copyrighted text. To support this, we share open-source tools for converting research documents into Knowledge Units. Overall, our work posits the feasibility of democratizing access to scientific knowledge while respecting copyright.</li>
</ul>

<h3>Title: Norm Growth and Stability Challenges in Localized Sequential Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Akshat Gupta, Christine Fang, Atahan Ozdemir, Maochuan Lu, Ahmed Alaa, Thomas Hartvigsen, Gopala Anumanchipalli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.19416">https://arxiv.org/abs/2502.19416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.19416">https://arxiv.org/pdf/2502.19416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.19416]] Norm Growth and Stability Challenges in Localized Sequential Knowledge Editing(https://arxiv.org/abs/2502.19416)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study investigates the impact of localized updates to large language models (LLMs), specifically in the context of knowledge editing - a task aimed at incorporating or modifying specific facts without altering broader model capabilities. We first show that across different post-training interventions like continuous pre-training, full fine-tuning and LORA-based fine-tuning, the Frobenius norm of the updated matrices always increases. This increasing norm is especially detrimental for localized knowledge editing, where only a subset of matrices are updated in a model . We reveal a consistent phenomenon across various editing techniques, including fine-tuning, hypernetwork-based approaches, and locate-and-edit methods: the norm of the updated matrix invariably increases with successive updates. Such growth disrupts model balance, particularly when isolated matrices are updated while the rest of the model remains static, leading to potential instability and degradation of downstream performance. Upon deeper investigations of the intermediate activation vectors, we find that the norm of internal activations decreases and is accompanied by shifts in the subspaces occupied by these activations, which shows that these activation vectors now occupy completely different regions in the representation space compared to the unedited model. With our paper, we highlight the technical challenges with continuous and localized sequential knowledge editing and their implications for maintaining model stability and utility.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
