<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-26</h1>
<h3>Title: Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Leyi Ouyang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20367">https://arxiv.org/abs/2509.20367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20367">https://arxiv.org/pdf/2509.20367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20367]] Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models(https://arxiv.org/abs/2509.20367)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Diplomatic events consistently prompt widespread public discussion and debate. Public sentiment plays a critical role in diplomacy, as a good sentiment provides vital support for policy implementation, helps resolve international issues, and shapes a nation's international image. Traditional methods for gauging public sentiment, such as large-scale surveys or manual content analysis of media, are typically time-consuming, labor-intensive, and lack the capacity for forward-looking analysis. We propose a novel framework that identifies specific modifications for diplomatic event narratives to shift public sentiment from negative to neutral or positive. First, we train a language model to predict public reaction towards diplomatic events. To this end, we construct a dataset comprising descriptions of diplomatic events and their associated public discussions. Second, guided by communication theories and in collaboration with domain experts, we predetermined several textual features for modification, ensuring that any alterations changed the event's narrative framing while preserving its core this http URL develop a counterfactual generation algorithm that employs a large language model to systematically produce modified versions of an original text. The results show that this framework successfully shifted public sentiment to a more favorable state with a 70\% success rate. This framework can therefore serve as a practical tool for diplomats, policymakers, and communication specialists, offering data-driven insights on how to frame diplomatic initiatives or report on events to foster a more desirable public sentiment.</li>
</ul>

<h3>Title: CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Nithin Somasekharan, Ling Yue, Yadi Cao, Weichao Li, Patrick Emami, Pochinapeddi Sai Bhargav, Anurag Acharya, Xingyu Xie, Shaowu Pan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20374">https://arxiv.org/abs/2509.20374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20374">https://arxiv.org/pdf/2509.20374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20374]] CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics(https://arxiv.org/abs/2509.20374)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated strong performance across general NLP tasks, but their utility in automating numerical experiments of complex physical system -- a critical and labor-intensive component -- remains underexplored. As the major workhorse of computational science over the past decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging testbed for evaluating the scientific capabilities of LLMs. We introduce CFDLLMBench, a benchmark suite comprising three complementary components -- CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM performance across three key competencies: graduate-level CFD knowledge, numerical and physical reasoning of CFD, and context-dependent implementation of CFD workflows. Grounded in real-world CFD practices, our benchmark combines a detailed task taxonomy with a rigorous evaluation framework to deliver reproducible results and quantify LLM performance across code executability, solution accuracy, and numerical convergence behavior. CFDLLMBench establishes a solid foundation for the development and evaluation of LLM-driven automation of numerical experiments for complex physical systems. Code and data are available at this https URL.</li>
</ul>

<h3>Title: Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text</h3>
<ul>
<li><strong>Authors: </strong>Sharanya Parimanoharan, Ruwan D. Nawarathna</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20375">https://arxiv.org/abs/2509.20375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20375">https://arxiv.org/pdf/2509.20375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20375]] Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text(https://arxiv.org/abs/2509.20375)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>The rapid adoption of large language models (LLMs) such as ChatGPT has blurred the line between human and AI-generated texts, raising urgent questions about academic integrity, intellectual property, and the spread of misinformation. Thus, reliable AI-text detection is needed for fair assessment to safeguard human authenticity and cultivate trust in digital communication. In this study, we investigate how well current machine learning (ML) approaches can distinguish ChatGPT-3.5-generated texts from human-written texts employing a labeled data set of 250 pairs of abstracts from a wide range of research topics. We test and compare both classical (Logistic Regression armed with classical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT augmented with N-grams, DistilBERT, BERT with a lightweight custom classifier, and LSTM-based N-gram models) ML detection techniques. As we aim to assess each model's performance in detecting AI-generated research texts, we also aim to test whether an ensemble of these models can outperform any single detector. Results show DistilBERT achieves the overall best performance, while Logistic Regression and BERT-Custom offer solid, balanced alternatives; LSTM- and BERT-N-gram approaches lag. The max voting ensemble of the three best models fails to surpass DistilBERT itself, highlighting the primacy of a single transformer-based representation over mere model diversity. By comprehensively assessing the strengths and weaknesses of these AI-text detection approaches, this work lays a foundation for more robust transformer frameworks with larger, richer datasets to keep pace with ever-improving generative AI models.</li>
</ul>

<h3>Title: ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoxuan Li, Zhen Wen, Qiqi Jiang, Chenxiao Li, Yuwei Wu, Yuchen Yang, Yiyao Wang, Xiuqi Huang, Minfeng Zhu, Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20376">https://arxiv.org/abs/2509.20376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20376">https://arxiv.org/pdf/2509.20376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20376]] ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models(https://arxiv.org/abs/2509.20376)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks. Understanding how LLMs internally represent knowledge remains a significant challenge. Despite Sparse Autoencoders (SAEs) have emerged as a promising technique for extracting interpretable features from LLMs, SAE features do not inherently align with human-understandable concepts, making their interpretation cumbersome and labor-intensive. To bridge the gap between SAE features and human concepts, we present ConceptViz, a visual analytics system designed for exploring concepts in LLMs. ConceptViz implements a novel dentification => Interpretation => Validation pipeline, enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification. We demonstrate the effectiveness of ConceptViz through two usage scenarios and a user study. Our results show that ConceptViz enhances interpretability research by streamlining the discovery and validation of meaningful concept representations in LLMs, ultimately aiding researchers in building more accurate mental models of LLM features. Our code and user guide are publicly available at this https URL.</li>
</ul>

<h3>Title: SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Tomoaki Isoda</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20377">https://arxiv.org/abs/2509.20377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20377">https://arxiv.org/pdf/2509.20377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20377]] SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation(https://arxiv.org/abs/2509.20377)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has significantly improved the performance of large language models (LLMs) on knowledge-intensive tasks in recent years. However, since retrieval systems may return irrelevant content, incorporating such information into the model often leads to hallucinations. Thus, identifying and filtering out unhelpful retrieved content is a key challenge for improving RAG this http URL better integrate the internal knowledge of the model with external knowledge from retrieval, it is essential to understand what the model "knows" and "does not know" (which is also called "self-knowledge"). Based on this insight, we propose SKILL-RAG (Self-Knowledge Induced Learning and Filtering for RAG), a novel method that leverages the model's self-knowledge to determine which retrieved documents are beneficial for answering a given query. We design a reinforcement learning-based training framework to explicitly elicit self-knowledge from the model and employs sentence-level granularity to filter out irrelevant content while preserving useful this http URL evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several question answering benchmarks. Experimental results demonstrate that SKILL-RAG not only improves generation quality but also significantly reduces the number of input documents, validating the importance of self-knowledge in guiding the selection of high-quality retrievals.</li>
</ul>

<h3>Title: USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Jianyu Wen, Jingyun Wang, Cilin Yan, Jiayin Cai, Xiaolong Jiang, Ying Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20381">https://arxiv.org/abs/2509.20381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20381">https://arxiv.org/pdf/2509.20381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20381]] USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model(https://arxiv.org/abs/2509.20381)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, Large Language Models (LLMs) have been widely employed in Conversational Recommender Systems (CRSs). Unlike traditional language model approaches that focus on training, all existing LLMs-based approaches are mainly centered around how to leverage the summarization and analysis capabilities of LLMs while ignoring the issue of training. Therefore, in this work, we propose an integrated training-inference framework, User-Simulator-Based framework (USB-Rec), for improving the performance of LLMs in conversational recommendation at the model level. Firstly, we design a LLM-based Preference Optimization (PO) dataset construction strategy for RL training, which helps the LLMs understand the strategies and methods in conversational recommendation. Secondly, we propose a Self-Enhancement Strategy (SES) at the inference stage to further exploit the conversational recommendation potential obtained from RL training. Extensive experiments on various datasets demonstrate that our method consistently outperforms previous state-of-the-art methods.</li>
</ul>

<h3>Title: Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated and Adversarial Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Dilli Hang Rai, Sabin Kafley</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20382">https://arxiv.org/abs/2509.20382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20382">https://arxiv.org/pdf/2509.20382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20382]] Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated and Adversarial Evaluation(https://arxiv.org/abs/2509.20382)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack, biometric, federate</a></li>
<li><strong>Abstract: </strong>ECG biometrics offer a unique, secure authentication method, yet their deployment on wearable devices faces real-time processing, privacy, and spoofing vulnerability challenges. This paper proposes a lightweight deep learning model (MobileNetV1+GRU) for ECG-based authentication, injection of 20dB Gaussian noise & custom preprocessing. We simulate wearable conditions and edge deployment using the ECGID, MIT-BIH, CYBHi, and PTB datasets, achieving accuracies of 99.34%, 99.31%, 91.74%, and 98.49%, F1-scores of 0.9869, 0.9923, 0.9125, and 0.9771, Precision of 0.9866, 0.9924, 0.9180 and 0.9845, Recall of 0.9878, 0.9923, 0.9129, and 0.9756, equal error rates (EER) of 0.0009, 0.00013, 0.0091, and 0.0009, and ROC-AUC values of 0.9999, 0.9999, 0.9985, and 0.9998, while under FGSM adversarial attacks, accuracy drops from 96.82% to as low as 0.80%. This paper highlights federated learning, adversarial testing, and the need for diverse wearable physiological datasets to ensure secure and scalable biometrics.</li>
</ul>

<h3>Title: MARS: A Malignity-Aware Backdoor Defense in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Wei Wan, Yuxuan Ning, Zhicong Huang, Cheng Hong, Shengshan Hu, Ziqi Zhou, Yechao Zhang, Tianqing Zhu, Wanlei Zhou, Leo Yu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20383">https://arxiv.org/abs/2509.20383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20383">https://arxiv.org/pdf/2509.20383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20383]] MARS: A Malignity-Aware Backdoor Defense in Federated Learning(https://arxiv.org/abs/2509.20383)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a distributed paradigm aimed at protecting participant data privacy by exchanging model parameters to achieve high-quality model training. However, this distributed nature also makes FL highly vulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art (SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether the backdoor models have been accepted by the defender and adaptively optimizes backdoor models, rendering existing defenses ineffective. In this paper, we first reveal that the failure of existing defenses lies in the employment of empirical statistical measures that are loosely coupled with backdoor attacks. Motivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that leverages backdoor energy (BE) to indicate the malicious extent of each neuron. To amplify malignity, we further extract the most prominent BE values from each model to form a concentrated backdoor energy (CBE). Finally, a novel Wasserstein distance-based clustering method is introduced to effectively identify backdoor models. Extensive experiments demonstrate that MARS can defend against SOTA backdoor attacks and significantly outperforms existing defenses.</li>
</ul>

<h3>Title: Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants</h3>
<ul>
<li><strong>Authors: </strong>Amir AL-Maamari</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20388">https://arxiv.org/abs/2509.20388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20388">https://arxiv.org/pdf/2509.20388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20388]] Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants(https://arxiv.org/abs/2509.20388)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>The rapid integration of AI-powered coding assistants into developer workflows has raised significant privacy and trust concerns. As developers entrust proprietary code to services like OpenAI's GPT, Google's Gemini, and GitHub Copilot, the unclear data handling practices of these tools create security and compliance risks. This paper addresses this challenge by introducing and applying a novel, expert-validated privacy scorecard. The methodology involves a detailed analysis of four document types; from legal policies to external audits; to score five leading assistants against 14 weighted criteria. A legal expert and a data protection officer refined these criteria and their weighting. The results reveal a distinct hierarchy of privacy protections, with a 20-point gap between the highest- and lowest-ranked tools. The analysis uncovers common industry weaknesses, including the pervasive use of opt-out consent for model training and a near-universal failure to filter secrets from user prompts proactively. The resulting scorecard provides actionable guidance for developers and organizations, enabling evidence-based tool selection. This work establishes a new benchmark for transparency and advocates for a shift towards more user-centric privacy standards in the AI industry.</li>
</ul>

<h3>Title: A Comparative Analysis of Ensemble-Based Machine Learning Approaches with Explainable AI for Multi-Class Intrusion Detection in Drone Networks</h3>
<ul>
<li><strong>Authors: </strong>Md. Alamgir Hossain, Waqas Ishtiaq, Md. Samiul Islam</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20391">https://arxiv.org/abs/2509.20391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20391">https://arxiv.org/pdf/2509.20391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20391]] A Comparative Analysis of Ensemble-Based Machine Learning Approaches with Explainable AI for Multi-Class Intrusion Detection in Drone Networks(https://arxiv.org/abs/2509.20391)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>The growing integration of drones into civilian, commercial, and defense sectors introduces significant cybersecurity concerns, particularly with the increased risk of network-based intrusions targeting drone communication protocols. Detecting and classifying these intrusions is inherently challenging due to the dynamic nature of drone traffic and the presence of multiple sophisticated attack vectors such as spoofing, injection, replay, and man-in-the-middle (MITM) attacks. This research aims to develop a robust and interpretable intrusion detection framework tailored for drone networks, with a focus on handling multi-class classification and model explainability. We present a comparative analysis of ensemble-based machine learning models, namely Random Forest, Extra Trees, AdaBoost, CatBoost, and XGBoost, trained on a labeled dataset comprising benign traffic and nine distinct intrusion types. Comprehensive data preprocessing was performed, including missing value imputation, scaling, and categorical encoding, followed by model training and extensive evaluation using metrics such as macro F1-score, ROC AUC, Matthews Correlation Coefficient, and Log Loss. Random Forest achieved the highest performance with a macro F1-score of 0.9998 and ROC AUC of 1.0000. To validate the superiority of the models, statistical tests, including Friedmans test, the Wilcoxon signed-rank test with Holm correction, and bootstrapped confidence intervals, were applied. Furthermore, explainable AI methods, SHAP and LIME, were integrated to interpret both global and local feature importance, enhancing model transparency and decision trustworthiness. The proposed approach not only delivers near-perfect accuracy but also ensures interpretability, making it highly suitable for real-time and safety-critical drone operations.</li>
</ul>

<h3>Title: Centralized vs. Decentralized Security for Space AI Systems? A New Look</h3>
<ul>
<li><strong>Authors: </strong>Noam Schmitt (IP Paris, TSP, ENS Paris Saclay), Marc Antoine Lacoste</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20395">https://arxiv.org/abs/2509.20395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20395">https://arxiv.org/pdf/2509.20395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20395]] Centralized vs. Decentralized Security for Space AI Systems? A New Look(https://arxiv.org/abs/2509.20395)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, federate</a></li>
<li><strong>Abstract: </strong>This paper investigates the trade-off between centralized and decentralized security management in constellations of satellites to balance security and performance. We highlight three key AI architectures for automated security management: (a) centralized, (b) distributed and (c) federated. The centralized architecture is the best option short term, providing fast training, despite the hard challenge of the communication latency overhead across space. Decentralized architectures are better alternatives in the longer term, providing enhanced scalability and security.</li>
</ul>

<h3>Title: Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry</h3>
<ul>
<li><strong>Authors: </strong>Birk Torpmann-Hagen, Michael A. Riegler, Pål Halvorsen, Dag Johansen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20399">https://arxiv.org/abs/2509.20399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20399">https://arxiv.org/pdf/2509.20399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20399]] Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry(https://arxiv.org/abs/2509.20399)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Deep neural networks are being utilized in a growing number of applications, both in production systems and for personal use. Network checkpoints are as a consequence often shared and distributed on various platforms to ease the development process. This work considers the threat of neural network stegomalware, where malware is embedded in neural network checkpoints at a negligible cost to network accuracy. This constitutes a significant security concern, but is nevertheless largely neglected by the deep learning practitioners and security specialists alike. We propose the first effective countermeasure to these attacks. In particular, we show that state-of-the-art neural network stegomalware can be efficiently and effectively neutralized through shuffling the column order of the weight- and bias-matrices, or equivalently the channel-order of convolutional layers. We show that this effectively corrupts payloads that have been embedded by state-of-the-art methods in neural network steganography at no cost to network accuracy, outperforming competing methods by a significant margin. We then discuss possible means by which to bypass this defense, additional defense methods, and advocate for continued research into the security of machine learning systems.</li>
</ul>

<h3>Title: Why Speech Deepfake Detectors Won't Generalize: The Limits of Detection in an Open World</h3>
<ul>
<li><strong>Authors: </strong>Visar Berisha, Prad Kadambi, Isabella Lenz</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20405">https://arxiv.org/abs/2509.20405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20405">https://arxiv.org/pdf/2509.20405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20405]] Why Speech Deepfake Detectors Won't Generalize: The Limits of Detection in an Open World(https://arxiv.org/abs/2509.20405)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>Speech deepfake detectors are often evaluated on clean, benchmark-style conditions, but deployment occurs in an open world of shifting devices, sampling rates, codecs, environments, and attack families. This creates a ``coverage debt" for AI-based detectors: every new condition multiplies with existing ones, producing data blind spots that grow faster than data can be collected. Because attackers can target these uncovered regions, worst-case performance (not average benchmark scores) determines security. To demonstrate the impact of the coverage debt problem, we analyze results from a recent cross-testing framework. Grouping performance by bona fide domain and spoof release year, two patterns emerge: newer synthesizers erase the legacy artifacts detectors rely on, and conversational speech domains (teleconferencing, interviews, social media) are consistently the hardest to secure. These findings show that detection alone should not be relied upon for high-stakes decisions. Detectors should be treated as auxiliary signals within layered defenses that include provenance, personhood credentials, and policy safeguards.</li>
</ul>

<h3>Title: A Theory of Multi-Agent Generative Flow Networks</h3>
<ul>
<li><strong>Authors: </strong>Leo Maxime Brunswic, Haozhi Wang, Shuang Luo, Jianye Hao, Amir Rasouli, Yinchuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20408">https://arxiv.org/abs/2509.20408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20408">https://arxiv.org/pdf/2509.20408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20408]] A Theory of Multi-Agent Generative Flow Networks(https://arxiv.org/abs/2509.20408)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative flow networks utilize a flow-matching loss to learn a stochastic policy for generating objects from a sequence of actions, such that the probability of generating a pattern can be proportional to the corresponding given reward. However, a theoretical framework for multi-agent generative flow networks (MA-GFlowNets) has not yet been proposed. In this paper, we propose the theory framework of MA-GFlowNets, which can be applied to multiple agents to generate objects collaboratively through a series of joint actions. We further propose four algorithms: a centralized flow network for centralized training of MA-GFlowNets, an independent flow network for decentralized execution, a joint flow network for achieving centralized training with decentralized execution, and its updated conditional version. Joint Flow training is based on a local-global principle allowing to train a collection of (local) GFN as a unique (global) GFN. This principle provides a loss of reasonable complexity and allows to leverage usual results on GFN to provide theoretical guarantees that the independent policies generate samples with probability proportional to the reward function. Experimental results demonstrate the superiority of the proposed framework compared to reinforcement learning and MCMC-based methods.</li>
</ul>

<h3>Title: Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Tharcisse Ndayipfukamiye, Jianguo Ding, Doreen Sebastian Sarwatt, Adamu Gaston Philipo, Huansheng Ning</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20411">https://arxiv.org/abs/2509.20411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20411">https://arxiv.org/pdf/2509.20411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20411]] Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation(https://arxiv.org/abs/2509.20411)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, explainability, generative</a></li>
<li><strong>Abstract: </strong>Machine learning-based cybersecurity systems are highly vulnerable to adversarial attacks, while Generative Adversarial Networks (GANs) act as both powerful attack enablers and promising defenses. This survey systematically reviews GAN-based adversarial defenses in cybersecurity (2021--August 31, 2025), consolidating recent progress, identifying gaps, and outlining future directions. Using a PRISMA-compliant systematic literature review protocol, we searched five major digital libraries. From 829 initial records, 185 peer-reviewed studies were retained and synthesized through quantitative trend analysis and thematic taxonomy development. We introduce a four-dimensional taxonomy spanning defensive function, GAN architecture, cybersecurity domain, and adversarial threat model. GANs improve detection accuracy, robustness, and data utility across network intrusion detection, malware analysis, and IoT security. Notable advances include WGAN-GP for stable training, CGANs for targeted synthesis, and hybrid GAN models for improved resilience. Yet, persistent challenges remain such as instability in training, lack of standardized benchmarks, high computational cost, and limited explainability. GAN-based defenses demonstrate strong potential but require advances in stable architectures, benchmarking, transparency, and deployment. We propose a roadmap emphasizing hybrid models, unified evaluation, real-world integration, and defenses against emerging threats such as LLM-driven cyberattacks. This survey establishes the foundation for scalable, trustworthy, and adaptive GAN-powered defenses.</li>
</ul>

<h3>Title: A Taxonomy of Data Risks in AI and Quantum Computing (QAI) - A Systematic Review</h3>
<ul>
<li><strong>Authors: </strong>Grace Billiris, Asif Gill, Madhushi Bandara</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20418">https://arxiv.org/abs/2509.20418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20418">https://arxiv.org/pdf/2509.20418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20418]] A Taxonomy of Data Risks in AI and Quantum Computing (QAI) - A Systematic Review(https://arxiv.org/abs/2509.20418)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Quantum Artificial Intelligence (QAI), the integration of Artificial Intelligence (AI) and Quantum Computing (QC), promises transformative advances, including AI-enabled quantum cryptography and quantum-resistant encryption protocols. However, QAI inherits data risks from both AI and QC, creating complex privacy and security vulnerabilities that are not systematically studied. These risks affect the trustworthiness and reliability of AI and QAI systems, making their understanding critical. This study systematically reviews 67 privacy- and security-related studies to expand understanding of QAI data risks. We propose a taxonomy of 22 key data risks, organised into five categories: governance, risk assessment, control implementation, user considerations, and continuous monitoring. Our findings reveal vulnerabilities unique to QAI and identify gaps in holistic risk assessment. This work contributes to trustworthy AI and QAI research and provides a foundation for developing future risk assessment tools.</li>
</ul>

<h3>Title: Seedream 4.0: Toward Next-generation Multimodal Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, Xiaowen Jian, Huafeng Kuang, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, Wei Liu, Yanzuo Lu, Zhengxiong Luo, Tongtong Ou, Guang Shi, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Rui Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Wenxu Wu, Yonghui Wu, Xin Xia, Xuefeng Xiao, Shuang Xu, Xin Yan, Ceyuan Yang, Jianchao Yang, Zhonghua Zhai, Chenlin Zhang, Heng Zhang, Qi Zhang, Xinyu Zhang, Yuwei Zhang, Shijia Zhao, Wenliang Zhao, Wenjia Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20427">https://arxiv.org/abs/2509.20427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20427">https://arxiv.org/pdf/2509.20427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20427]] Seedream 4.0: Toward Next-generation Multimodal Image Generation(https://arxiv.org/abs/2509.20427)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. Seedream 4.0 is now accessible on this https URL.</li>
</ul>

<h3>Title: Bridging Privacy and Utility: Synthesizing anonymized EEG with constraining utility functions</h3>
<ul>
<li><strong>Authors: </strong>Kay Fuhrmeister, Arne Pelzer, Fabian Radke, Julia Lechinger, Mahzad Gharleghi, Thomas Köllmer, Insa Wolf</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20454">https://arxiv.org/abs/2509.20454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20454">https://arxiv.org/pdf/2509.20454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20454]] Bridging Privacy and Utility: Synthesizing anonymized EEG with constraining utility functions(https://arxiv.org/abs/2509.20454)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, transformer</a></li>
<li><strong>Abstract: </strong>Electroencephalography (EEG) is widely used for recording brain activity and has seen numerous applications in machine learning, such as detecting sleep stages and neurological disorders. Several studies have successfully shown the potential of EEG data for re-identification and leakage of other personal information. Therefore, the increasing availability of EEG consumer devices raises concerns about user privacy, motivating us to investigate how to safeguard this sensitive data while retaining its utility for EEG applications. To address this challenge, we propose a transformer-based autoencoder to create EEG data that does not allow for subject re-identification while still retaining its utility for specific machine learning tasks. We apply our approach to automatic sleep staging by evaluating the re-identification and utility potential of EEG data before and after anonymization. The results show that the re-identifiability of the EEG signal can be substantially reduced while preserving its utility for machine learning.</li>
</ul>

<h3>Title: Differential Privacy of Network Parameters from a System Identification Perspective</h3>
<ul>
<li><strong>Authors: </strong>Andrew Campbell, Anna Scaglione, Hang Liu, Victor Elvira, Sean Peisert, Daniel Arnold</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20460">https://arxiv.org/abs/2509.20460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20460">https://arxiv.org/pdf/2509.20460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20460]] Differential Privacy of Network Parameters from a System Identification Perspective(https://arxiv.org/abs/2509.20460)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>This paper addresses the problem of protecting network information from privacy system identification (SI) attacks when sharing cyber-physical system simulations. We model analyst observations of networked states as time-series outputs of a graph filter driven by differentially private (DP) nodal excitations, with the analyst aiming to infer the underlying graph shift operator (GSO). Unlike traditional SI, which estimates system parameters, we study the inverse problem: what assumptions prevent adversaries from identifying the GSO while preserving utility for legitimate analysis. We show that applying DP mechanisms to inputs provides formal privacy guarantees for the GSO, linking the $(\epsilon,\delta)$-DP bound to the spectral properties of the graph filter and noise covariance. More precisely, for DP Gaussian signals, the spectral characteristics of both the filter and noise covariance determine the privacy bound, with smooth filters and low-condition-number covariance yielding greater privacy.</li>
</ul>

<h3>Title: Document Summarization with Conformal Importance Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Bruce Kuwahara, Chen-Yuan Lin, Xiao Shi Huang, Kin Kwan Leung, Jullian Arta Yapeter, Ilya Stanevich, Felipe Perez, Jesse C. Cresswell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20461">https://arxiv.org/abs/2509.20461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20461">https://arxiv.org/pdf/2509.20461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20461]] Document Summarization with Conformal Importance Guarantees(https://arxiv.org/abs/2509.20461)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic summarization systems have advanced rapidly with large language models (LLMs), yet they still lack reliable guarantees on inclusion of critical content in high-stakes domains like healthcare, law, and finance. In this work, we introduce Conformal Importance Summarization, the first framework for importance-preserving summary generation which uses conformal prediction to provide rigorous, distribution-free coverage guarantees. By calibrating thresholds on sentence-level importance scores, we enable extractive document summarization with user-specified coverage and recall rates over critical content. Our method is model-agnostic, requires only a small calibration set, and seamlessly integrates with existing black-box LLMs. Experiments on established summarization benchmarks demonstrate that Conformal Importance Summarization achieves the theoretically assured information coverage rate. Our work suggests that Conformal Importance Summarization can be combined with existing techniques to achieve reliable, controllable automatic summarization, paving the way for safer deployment of AI summarization tools in critical applications. Code is available at this https URL.</li>
</ul>

<h3>Title: Efficiently Attacking Memorization Scores</h3>
<ul>
<li><strong>Authors: </strong>Tue Do, Varun Chandrasekaran, Daniel Alabi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20463">https://arxiv.org/abs/2509.20463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20463">https://arxiv.org/pdf/2509.20463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20463]] Efficiently Attacking Memorization Scores(https://arxiv.org/abs/2509.20463)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Influence estimation tools -- such as memorization scores -- are widely used to understand model behavior, attribute training data, and inform dataset curation. However, recent applications in data valuation and responsible machine learning raise the question: can these scores themselves be adversarially manipulated? In this work, we present a systematic study of the feasibility of attacking memorization-based influence estimators. We characterize attacks for producing highly memorized samples as highly sensitive queries in the regime where a trained algorithm is accurate. Our attack (calculating the pseudoinverse of the input) is practical, requiring only black-box access to model outputs and incur modest computational overhead. We empirically validate our attack across a wide suite of image classification tasks, showing that even state-of-the-art proxies are vulnerable to targeted score manipulations. In addition, we provide a theoretical analysis of the stability of memorization scores under adversarial perturbations, revealing conditions under which influence estimates are inherently fragile. Our findings highlight critical vulnerabilities in influence-based attribution and suggest the need for robust defenses. All code can be found at this https URL</li>
</ul>

<h3>Title: Advancing Practical Homomorphic Encryption for Federated Learning: Theoretical Guarantees and Efficiency Optimizations</h3>
<ul>
<li><strong>Authors: </strong>Ren-Yi Huang, Dumindu Samaraweera, Prashant Shekhar, J. Morris Chang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20476">https://arxiv.org/abs/2509.20476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20476">https://arxiv.org/pdf/2509.20476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20476]] Advancing Practical Homomorphic Encryption for Federated Learning: Theoretical Guarantees and Efficiency Optimizations(https://arxiv.org/abs/2509.20476)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative model training while preserving data privacy by keeping raw data locally stored on client devices, preventing access from other clients or the central server. However, recent studies reveal that sharing model gradients creates vulnerability to Model Inversion Attacks, particularly Deep Leakage from Gradients (DLG), which reconstructs private training data from shared gradients. While Homomorphic Encryption has been proposed as a promising defense mechanism to protect gradient privacy, fully encrypting all model gradients incurs high computational overhead. Selective encryption approaches aim to balance privacy protection with computational efficiency by encrypting only specific gradient components. However, the existing literature largely overlooks a theoretical exploration of the spectral behavior of encrypted versus unencrypted parameters, relying instead primarily on empirical evaluations. To address this gap, this paper presents a framework for theoretical analysis of the underlying principles of selective encryption as a defense against model inversion attacks. We then provide a comprehensive empirical study that identifies and quantifies the critical factors, such as model complexity, encryption ratios, and exposed gradients, that influence defense effectiveness. Our theoretical framework clarifies the relationship between gradient selection and privacy preservation, while our experimental evaluation demonstrates how these factors shape the robustness of defenses against model inversion attacks. Collectively, these contributions advance the understanding of selective encryption mechanisms and offer principled guidance for designing efficient, scalable, privacy-preserving federated learning systems.</li>
</ul>

<h3>Title: Shared Neural Space: Unified Precomputed Feature Encoding for Multi-Task and Cross Domain Vision</h3>
<ul>
<li><strong>Authors: </strong>Jing Li, Oskar Bartosz, Chengyu Wang, Michal Wnuczynski, Dilshan Godaliyadda, Michael Polley</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20481">https://arxiv.org/abs/2509.20481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20481">https://arxiv.org/pdf/2509.20481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20481]] Shared Neural Space: Unified Precomputed Feature Encoding for Multi-Task and Cross Domain Vision(https://arxiv.org/abs/2509.20481)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The majority of AI models in imaging and vision are customized to perform on specific high-precision task. However, this strategy is inefficient for applications with a series of modular tasks, since each requires a mapping into a disparate latent domain. To address this inefficiency, we proposed a universal Neural Space (NS), where an encoder-decoder framework pre-computes features across vision and imaging tasks. Our encoder learns transformation aware, generalizable representations, which enable multiple downstream AI modules to share the same feature space. This architecture reduces redundancy, improves generalization across domain shift, and establishes a foundation for effecient multi-task vision pipelines. Furthermore, as opposed to larger transformer backbones, our backbone is lightweight and CNN-based, allowing for wider across hardware. We furthur demonstrate that imaging and vision modules, such as demosaicing, denoising, depth estimation and semantic segmentation can be performed efficiently in the NS.</li>
</ul>

<h3>Title: CoSupFormer : A Contrastive Supervised learning approach for EEG signal Classification</h3>
<ul>
<li><strong>Authors: </strong>D. Darankoum, C. Habermacher, J. Volle, S. Grudinin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20489">https://arxiv.org/abs/2509.20489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20489">https://arxiv.org/pdf/2509.20489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20489]] CoSupFormer : A Contrastive Supervised learning approach for EEG signal Classification(https://arxiv.org/abs/2509.20489)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Electroencephalography signals (EEGs) contain rich multi-scale information crucial for understanding brain states, with potential applications in diagnosing and advancing the drug development landscape. However, extracting meaningful features from raw EEG signals while handling noise and channel variability remains a major challenge. This work proposes a novel end-to-end deep-learning framework that addresses these issues through several key innovations. First, we designed an encoder capable of explicitly capturing multi-scale frequency oscillations covering a wide range of features for different EEG-related tasks. Secondly, to model complex dependencies and handle the high temporal resolution of EEGs, we introduced an attention-based encoder that simultaneously learns interactions across EEG channels and within localized {\em patches} of individual channels. We integrated a dedicated gating network on top of the attention encoder to dynamically filter out noisy and non-informative channels, enhancing the reliability of EEG data. The entire encoding process is guided by a novel loss function, which leverages supervised and contrastive learning, significantly improving model generalization. We validated our approach in multiple applications, ranging from the classification of effects across multiple Central Nervous System (CNS) disorders treatments to the diagnosis of Parkinson's and Alzheimer's disease. Our results demonstrate that the proposed learning paradigm can extract biologically meaningful patterns from raw EEG signals across different species, autonomously select high-quality channels, and achieve robust generalization through innovative architectural and loss design.</li>
</ul>

<h3>Title: Beyond Visual Similarity: Rule-Guided Multimodal Clustering with explicit domain rules</h3>
<ul>
<li><strong>Authors: </strong>Kishor Datta Gupta, Mohd Ariful Haque, Marufa Kamal, Ahmed Rafi Hasan, Md. Mahfuzur Rahman, Roy George</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20501">https://arxiv.org/abs/2509.20501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20501">https://arxiv.org/pdf/2509.20501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20501]] Beyond Visual Similarity: Rule-Guided Multimodal Clustering with explicit domain rules(https://arxiv.org/abs/2509.20501)</code><input type="text"></li>
<li><strong>Keywords: </strong>steal</a></li>
<li><strong>Abstract: </strong>Traditional clustering techniques often rely solely on similarity in the input data, limiting their ability to capture structural or semantic constraints that are critical in many domains. We introduce the Domain Aware Rule Triggered Variational Autoencoder (DARTVAE), a rule guided multimodal clustering framework that incorporates domain specific constraints directly into the representation learning process. DARTVAE extends the VAE architecture by embedding explicit rules, semantic representations, and data driven features into a unified latent space, while enforcing constraint compliance through rule consistency and violation penalties in the loss function. Unlike conventional clustering methods that rely only on visual similarity or apply rules as post hoc filters, DARTVAE treats rules as first class learning signals. The rules are generated by LLMs, structured into knowledge graphs, and enforced through a loss function combining reconstruction, KL divergence, consistency, and violation penalties. Experiments on aircraft and automotive datasets demonstrate that rule guided clustering produces more operationally meaningful and interpretable clusters for example, isolating UAVs, unifying stealth aircraft, or separating SUVs from sedans while improving traditional clustering metrics. However, the framework faces challenges: LLM generated rules may hallucinate or conflict, excessive rules risk overfitting, and scaling to complex domains increases computational and consistency difficulties. By combining rule encodings with learned representations, DARTVAE achieves more meaningful and consistent clustering outcomes than purely data driven models, highlighting the utility of constraint guided multimodal clustering for complex, knowledge intensive settings.</li>
</ul>

<h3>Title: MARS: toward more efficient multi-agent collaboration for LLM reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xiao Wang, Jia Wang, Yijie Wang, Pengtao Dang, Sha Cao, Chi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20502">https://arxiv.org/abs/2509.20502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20502">https://arxiv.org/pdf/2509.20502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20502]] MARS: toward more efficient multi-agent collaboration for LLM reasoning(https://arxiv.org/abs/2509.20502)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved impressive results in natural language understanding, yet their reasoning capabilities remain limited when operating as single agents. Multi-Agent Debate (MAD) has been proposed to address this limitation by enabling collaborative reasoning among multiple models in a round-table debate manner. While effective, MAD introduces substantial computational overhead due to the number of agents involved and the frequent communication required. In this paper, we propose MARS (Multi-Agent Review System), a role-based collaboration framework inspired by the review process. In MARS, an author agent generates an initial solution, reviewer agents provide decisions and comments independently, and a meta-reviewer integrates the feedback to make the final decision and guide further revision. This design enhances reasoning quality while avoiding costly reviewer-to-reviewer interactions, thereby controlling token consumption and inference time. We compared MARS with both MAD and other state-of-the-art reasoning strategies across multiple benchmarks. Extensive experiments with different LLMs show that MARS matches the accuracy of MAD while reducing both token usage and inference time by approximately 50\%. Code is available at this https URL.</li>
</ul>

<h3>Title: Auto-Regressive U-Net for Full-Field Prediction of Shrinkage-Induced Damage in Concrete</h3>
<ul>
<li><strong>Authors: </strong>Liya Gaynutdinova, Petr Havlásek, Ondřej Rokoš, Fleur Hendriks, Martin Doškář</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20507">https://arxiv.org/abs/2509.20507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20507">https://arxiv.org/pdf/2509.20507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20507]] Auto-Regressive U-Net for Full-Field Prediction of Shrinkage-Induced Damage in Concrete(https://arxiv.org/abs/2509.20507)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper introduces a deep learning approach for predicting time-dependent full-field damage in concrete. The study uses an auto-regressive U-Net model to predict the evolution of the scalar damage field in a unit cell given microstructural geometry and evolution of an imposed shrinkage profile. By sequentially using the predicted damage output as input for subsequent predictions, the model facilitates the continuous assessment of damage progression. Complementarily, a convolutional neural network (CNN) utilises the damage estimations to forecast key mechanical properties, including observed shrinkage and residual stiffness. The proposed dual-network architecture demonstrates high computational efficiency and robust predictive performance on the synthesised datasets. The approach reduces the computational load traditionally associated with full-field damage evaluations and is used to gain insights into the relationship between aggregate properties, such as shape, size, and distribution, and the effective shrinkage and reduction in stiffness. Ultimately, this can help to optimize concrete mix designs, leading to improved durability and reduced internal damage.</li>
</ul>

<h3>Title: Complexity-Driven Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Luca Serfilippi, Giorgio Franceschelli, Antonio Corradi, Mirco Musolesi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20509">https://arxiv.org/abs/2509.20509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20509">https://arxiv.org/pdf/2509.20509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20509]] Complexity-Driven Policy Optimization(https://arxiv.org/abs/2509.20509)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Policy gradient methods often balance exploitation and exploration via entropy maximization. However, maximizing entropy pushes the policy towards a uniform random distribution, which represents an unstructured and sometimes inefficient exploration strategy. In this work, we propose replacing the entropy bonus with a more robust complexity bonus. In particular, we adopt a measure of complexity, defined as the product of Shannon entropy and disequilibrium, where the latter quantifies the distance from the uniform distribution. This regularizer encourages policies that balance stochasticity (high entropy) with structure (high disequilibrium), guiding agents toward regimes where useful, non-trivial behaviors can emerge. Such behaviors arise because the regularizer suppresses both extremes, e.g., maximal disorder and complete order, creating pressure for agents to discover structured yet adaptable strategies. Starting from Proximal Policy Optimization (PPO), we introduce Complexity-Driven Policy Optimization (CDPO), a new learning algorithm that replaces entropy with complexity. We show empirically across a range of discrete action space tasks that CDPO is more robust to the choice of the complexity coefficient than PPO is with the entropy coefficient, especially in environments requiring greater exploration.</li>
</ul>

<h3>Title: A Recovery Theory for Diffusion Priors: Deterministic Analysis of the Implicit Prior Algorithm</h3>
<ul>
<li><strong>Authors: </strong>Oscar Leong, Yann Traonmilin</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20511">https://arxiv.org/abs/2509.20511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20511">https://arxiv.org/pdf/2509.20511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20511]] A Recovery Theory for Diffusion Priors: Deterministic Analysis of the Implicit Prior Algorithm(https://arxiv.org/abs/2509.20511)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recovering high-dimensional signals from corrupted measurements is a central challenge in inverse problems. Recent advances in generative diffusion models have shown remarkable empirical success in providing strong data-driven priors, but rigorous recovery guarantees remain limited. In this work, we develop a theoretical framework for analyzing deterministic diffusion-based algorithms for inverse problems, focusing on a deterministic version of the algorithm proposed by Kadkhodaie \& Simoncelli \cite{kadkhodaie2021stochastic}. First, we show that when the underlying data distribution concentrates on a low-dimensional model set, the associated noise-convolved scores can be interpreted as time-varying projections onto such a set. This leads to interpreting previous algorithms using diffusion priors for inverse problems as generalized projected gradient descent methods with varying projections. When the sensing matrix satisfies a restricted isometry property over the model set, we can derive quantitative convergence rates that depend explicitly on the noise schedule. We apply our framework to two instructive data distributions: uniform distributions over low-dimensional compact, convex sets and low-rank Gaussian mixture models. In the latter setting, we can establish global convergence guarantees despite the nonconvexity of the underlying model set.</li>
</ul>

<h3>Title: InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On</h3>
<ul>
<li><strong>Authors: </strong>Julien Han, Shuwen Qiu, Qi Li, Xingzi Xu, Mehmet Saygin Seyfioglu, Kavosh Asadi, Karim Bouyarmane</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20524">https://arxiv.org/abs/2509.20524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20524">https://arxiv.org/pdf/2509.20524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20524]] InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On(https://arxiv.org/abs/2509.20524)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present InstructVTON, an instruction-following interactive virtual try-on system that allows fine-grained and complex styling control of the resulting generation, guided by natural language, on single or multiple garments. A computationally efficient and scalable formulation of virtual try-on formulates the problem as an image-guided or image-conditioned inpainting task. These inpainting-based virtual try-on models commonly use a binary mask to control the generation layout. Producing a mask that yields desirable result is difficult, requires background knowledge, might be model dependent, and in some cases impossible with the masking-based approach (e.g. trying on a long-sleeve shirt with "sleeves rolled up" styling on a person wearing long-sleeve shirt with sleeves down, where the mask will necessarily cover the entire sleeve). InstructVTON leverages Vision Language Models (VLMs) and image segmentation models for automated binary mask generation. These masks are generated based on user-provided images and free-text style instructions. InstructVTON simplifies the end-user experience by removing the necessity of a precisely drawn mask, and by automating execution of multiple rounds of image generation for try-on scenarios that cannot be achieved with masking-based virtual try-on models alone. We show that InstructVTON is interoperable with existing virtual try-on models to achieve state-of-the-art results with styling control.</li>
</ul>

<h3>Title: MDBench: Benchmarking Data-Driven Methods for Model Discovery</h3>
<ul>
<li><strong>Authors: </strong>Amirmohammad Ziaei Bideh, Aleksandra Georgievska, Jonathan Gryak</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20529">https://arxiv.org/abs/2509.20529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20529">https://arxiv.org/pdf/2509.20529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20529]] MDBench: Benchmarking Data-Driven Methods for Model Discovery(https://arxiv.org/abs/2509.20529)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Model discovery aims to uncover governing differential equations of dynamical systems directly from experimental data. Benchmarking such methods is essential for tracking progress and understanding trade-offs in the field. While prior efforts have focused mostly on identifying single equations, typically framed as symbolic regression, there remains a lack of comprehensive benchmarks for discovering dynamical models. To address this, we introduce MDBench, an open-source benchmarking framework for evaluating model discovery methods on dynamical systems. MDBench assesses 12 algorithms on 14 partial differential equations (PDEs) and 63 ordinary differential equations (ODEs) under varying levels of noise. Evaluation metrics include derivative prediction accuracy, model complexity, and equation fidelity. We also introduce seven challenging PDE systems from fluid dynamics and thermodynamics, revealing key limitations in current methods. Our findings illustrate that linear methods and genetic programming methods achieve the lowest prediction error for PDEs and ODEs, respectively. Moreover, linear models are in general more robust against noise. MDBench accelerates the advancement of model discovery methods by offering a rigorous, extensible benchmarking framework and a rich, diverse collection of dynamical system datasets, enabling systematic evaluation, comparison, and improvement of equation accuracy and robustness.</li>
</ul>

<h3>Title: Innovative Deep Learning Architecture for Enhanced Altered Fingerprint Recognition</h3>
<ul>
<li><strong>Authors: </strong>Dana A Abdullah, Dana Rasul Hamad, Bishar Rasheed Ibrahim, Sirwan Abdulwahid Aula, Aso Khaleel Ameen, Sabat Salih Hamadamin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20537">https://arxiv.org/abs/2509.20537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20537">https://arxiv.org/pdf/2509.20537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20537]] Innovative Deep Learning Architecture for Enhanced Altered Fingerprint Recognition(https://arxiv.org/abs/2509.20537)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, biometric</a></li>
<li><strong>Abstract: </strong>Altered fingerprint recognition (AFR) is challenging for biometric verification in applications such as border control, forensics, and fiscal admission. Adversaries can deliberately modify ridge patterns to evade detection, so robust recognition of altered prints is essential. We present DeepAFRNet, a deep learning recognition model that matches and recognizes distorted fingerprint samples. The approach uses a VGG16 backbone to extract high-dimensional features and cosine similarity to compare embeddings. We evaluate on the SOCOFing Real-Altered subset with three difficulty levels (Easy, Medium, Hard). With strict thresholds, DeepAFRNet achieves accuracies of 96.7 percent, 98.76 percent, and 99.54 percent for the three levels. A threshold-sensitivity study shows that relaxing the threshold from 0.92 to 0.72 sharply degrades accuracy to 7.86 percent, 27.05 percent, and 29.51 percent, underscoring the importance of threshold selection in biometric systems. By using real altered samples and reporting per-level metrics, DeepAFRNet addresses limitations of prior work based on synthetic alterations or limited verification protocols, and indicates readiness for real-world deployments where both security and recognition resilience are critical.</li>
</ul>

<h3>Title: Understanding and Improving Adversarial Robustness of Neural Probabilistic Circuits</h3>
<ul>
<li><strong>Authors: </strong>Weixin Chen, Han Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20549">https://arxiv.org/abs/2509.20549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20549">https://arxiv.org/pdf/2509.20549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20549]] Understanding and Improving Adversarial Robustness of Neural Probabilistic Circuits(https://arxiv.org/abs/2509.20549)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, interpretability</a></li>
<li><strong>Abstract: </strong>Neural Probabilistic Circuits (NPCs), a new class of concept bottleneck models, comprise an attribute recognition model and a probabilistic circuit for reasoning. By integrating the outputs from these two modules, NPCs produce compositional and interpretable predictions. While offering enhanced interpretability and high performance on downstream tasks, the neural-network-based attribute recognition model remains a black box. This vulnerability allows adversarial attacks to manipulate attribute predictions by introducing carefully crafted subtle perturbations to input images, potentially compromising the final predictions. In this paper, we theoretically analyze the adversarial robustness of NPC and demonstrate that it only depends on the robustness of the attribute recognition model and is independent of the robustness of the probabilistic circuit. Moreover, we propose RNPC, the first robust neural probabilistic circuit against adversarial attacks on the recognition module. RNPC introduces a novel class-wise integration for inference, ensuring a robust combination of outputs from the two modules. Our theoretical analysis demonstrates that RNPC exhibits provably improved adversarial robustness compared to NPC. Empirical results on image classification tasks show that RNPC achieves superior adversarial robustness compared to existing concept bottleneck models while maintaining high accuracy on benign inputs.</li>
</ul>

<h3>Title: Generalizable Diabetes Risk Stratification via Hybrid Machine Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Athar Parvez, Muhammad Jawad Mufti</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20565">https://arxiv.org/abs/2509.20565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20565">https://arxiv.org/pdf/2509.20565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20565]] Generalizable Diabetes Risk Stratification via Hybrid Machine Learning Models(https://arxiv.org/abs/2509.20565)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Background/Purpose: Diabetes affects over 537 million people worldwide and is projected to reach 783 million by 2045. Early risk stratification can benefit from machine learning. We compare two hybrid classifiers and assess their generalizability on an external cohort. Methods: Two hybrids were built: (i) XGBoost + Random Forest (XGB-RF) and (ii) Support Vector Machine + Logistic Regression (SVM-LR). A leakage-safe, standardized pipeline (encoding, imputation, min-max scaling; SMOTE on training folds only; probability calibration for SVM) was fit on the primary dataset and frozen. Evaluation prioritized threshold-independent discrimination (AUROC/AUPRC) and calibration (Brier, slope/intercept). External validation used the PIMA cohort (N=768) with the frozen pipeline; any thresholded metrics on PIMA were computed at the default rule tau = 0.5. Results: On the primary dataset (PR baseline = 0.50), XGB-RF achieved AUROC ~0.995 and AUPRC ~0.998, outperforming SVM-LR (AUROC ~0.978; AUPRC ~0.947). On PIMA (PR baseline ~0.349), XGB-RF retained strong performance (AUROC ~0.990; AUPRC ~0.959); SVM-LR was lower (AUROC ~0.963; AUPRC ~0.875). Thresholded metrics on PIMA at tau = 0.5 were XGB-RF (Accuracy 0.960; Precision 0.941; Recall 0.944; F1 0.942) and SVM-LR (Accuracy 0.900; Precision 0.855; Recall 0.858; F1 0.857). Conclusions: Across internal and external cohorts, XGB-RF consistently dominated SVM-LR and exhibited smaller external attenuation on ROC/PR with acceptable calibration. These results support gradient-boosting-based hybridization as a robust, transferable approach for diabetes risk stratification and motivate prospective, multi-site validation with deployment-time threshold selection based on clinical trade-offs.</li>
</ul>

<h3>Title: SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations</h3>
<ul>
<li><strong>Authors: </strong>Ayan Sar, Pranav Singh Puri, Sumit Aich, Tanupriya Choudhury, Abhijit Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20567">https://arxiv.org/abs/2509.20567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20567">https://arxiv.org/pdf/2509.20567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20567]] SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations(https://arxiv.org/abs/2509.20567)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In multilingual healthcare environments, automatic disease diagnosis from clinical text remains a challenging task due to the scarcity of annotated medical data in low-resource languages and the linguistic variability across populations. This paper proposes SwasthLLM, a unified, zero-shot, cross-lingual, and multi-task learning framework for medical diagnosis that operates effectively across English, Hindi, and Bengali without requiring language-specific fine-tuning. At its core, SwasthLLM leverages the multilingual XLM-RoBERTa encoder augmented with a language-aware attention mechanism and a disease classification head, enabling the model to extract medically relevant information regardless of the language structure. To align semantic representations across languages, a Siamese contrastive learning module is introduced, ensuring that equivalent medical texts in different languages produce similar embeddings. Further, a translation consistency module and a contrastive projection head reinforce language-invariant representation learning. SwasthLLM is trained using a multi-task learning strategy, jointly optimizing disease classification, translation alignment, and contrastive learning objectives. Additionally, we employ Model-Agnostic Meta-Learning (MAML) to equip the model with rapid adaptation capabilities for unseen languages or tasks with minimal data. Our phased training pipeline emphasizes robust representation alignment before task-specific fine-tuning. Extensive evaluation shows that SwasthLLM achieves high diagnostic performance, with a test accuracy of 97.22% and an F1-score of 97.17% in supervised settings. Crucially, in zero-shot scenarios, it attains 92.78% accuracy on Hindi and 73.33% accuracy on Bengali medical text, demonstrating strong generalization in low-resource contexts.</li>
</ul>

<h3>Title: PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mingze Yuan, Pengfei Jin, Na Li, Quanzheng Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20570">https://arxiv.org/abs/2509.20570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20570">https://arxiv.org/pdf/2509.20570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20570]] PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models(https://arxiv.org/abs/2509.20570)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated strong generative capabilities across scientific domains, but often produce outputs that violate physical laws. We propose a new perspective by framing physics-informed generation as a sparse reward optimization problem, where adherence to physical constraints is treated as a reward signal. This formulation unifies prior approaches under a reward-based paradigm and reveals a shared bottleneck: reliance on diffusion posterior sampling (DPS)-style value function approximations, which introduce non-negligible errors and lead to training instability and inference inefficiency. To overcome this, we introduce Physics-Informed Reward Fine-tuning (PIRF), a method that bypasses value approximation by computing trajectory-level rewards and backpropagating their gradients directly. However, a naive implementation suffers from low sample efficiency and compromised data fidelity. PIRF mitigates these issues through two key strategies: (1) a layer-wise truncated backpropagation method that leverages the spatiotemporally localized nature of physics-based rewards, and (2) a weight-based regularization scheme that improves efficiency over traditional distillation-based methods. Across five PDE benchmarks, PIRF consistently achieves superior physical enforcement under efficient sampling regimes, highlighting the potential of reward fine-tuning for advancing scientific generative modeling.</li>
</ul>

<h3>Title: Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures</h3>
<ul>
<li><strong>Authors: </strong>Sampurna Roy, Ayan Sar, Anurag Kaushish, Kanav Gupta, Tanupriya Choudhury, Abhijit Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20577">https://arxiv.org/abs/2509.20577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20577">https://arxiv.org/pdf/2509.20577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20577]] Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures(https://arxiv.org/abs/2509.20577)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Contemporary transformer architectures apply identical processing depth to all inputs, creating inefficiencies and limiting reasoning quality. Simple factual queries are subjected to the same multilayered computation as complex logical problems, wasting resources while constraining deep inference. To overcome this, we came up with a concept of Dynamic Reasoning Chains through Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends the Mixture of Experts paradigm from width-based to depth specialised computation. DS-MoE introduces expert modules optimised for distinct reasoning depths, shallow pattern recognition, compositional reasoning, logical inference, memory integration, and meta-cognitive supervision. A learned routing network dynamically assembles custom reasoning chains, activating only the necessary experts to match input complexity. The dataset on which we trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse domains such as scientific papers, legal texts, programming code, and web content, enabling systematic assessment across reasoning depths. Experimental results demonstrate that DS-MoE achieves up to 16 per cent computational savings and 35 per cent faster inference compared to uniform-depth transformers, while delivering 2.8 per cent higher accuracy on complex multi-step reasoning benchmarks. Furthermore, routing decisions yield interpretable reasoning chains, enhancing transparency and scalability. These findings establish DS-MoE as a significant advancement in adaptive neural architectures, demonstrating that depth-specialised modular processing can simultaneously improve efficiency, reasoning quality, and interpretability in large-scale language models.</li>
</ul>

<h3>Title: Large Pre-Trained Models for Bimanual Manipulation in 3D</h3>
<ul>
<li><strong>Authors: </strong>Hanna Yurchyk, Wei-Di Chang, Gregory Dudek, David Meger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20579">https://arxiv.org/abs/2509.20579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20579">https://arxiv.org/pdf/2509.20579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20579]] Large Pre-Trained Models for Bimanual Manipulation in 3D(https://arxiv.org/abs/2509.20579)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We investigate the integration of attention maps from a pre-trained Vision Transformer into voxel representations to enhance bimanual robotic manipulation. Specifically, we extract attention maps from DINOv2, a self-supervised ViT model, and interpret them as pixel-level saliency scores over RGB images. These maps are lifted into a 3D voxel grid, resulting in voxel-level semantic cues that are incorporated into a behavior cloning policy. When integrated into a state-of-the-art voxel-based policy, our attention-guided featurization yields an average absolute improvement of 8.2% and a relative gain of 21.9% across all tasks in the RLBench bimanual benchmark.</li>
</ul>

<h3>Title: A Comparative Benchmark of Real-time Detectors for Blueberry Detection towards Precision Orchard Management</h3>
<ul>
<li><strong>Authors: </strong>Xinyang Mu, Yuzhen Lu, Boyang Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20580">https://arxiv.org/abs/2509.20580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20580">https://arxiv.org/pdf/2509.20580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20580]] A Comparative Benchmark of Real-time Detectors for Blueberry Detection towards Precision Orchard Management(https://arxiv.org/abs/2509.20580)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Blueberry detection in natural environments remains challenging due to variable lighting, occlusions, and motion blur due to environmental factors and imaging devices. Deep learning-based object detectors promise to address these challenges, but they demand a large-scale, diverse dataset that captures the real-world complexities. Moreover, deploying these models in practical scenarios often requires the right accuracy/speed/memory trade-off in model selection. This study presents a novel comparative benchmark analysis of advanced real-time object detectors, including YOLO (You Only Look Once) (v8-v12) and RT-DETR (Real-Time Detection Transformers) (v1-v2) families, consisting of 36 model variants, evaluated on a newly curated dataset for blueberry detection. This dataset comprises 661 canopy images collected with smartphones during the 2022-2023 seasons, consisting of 85,879 labelled instances (including 36,256 ripe and 49,623 unripe blueberries) across a wide range of lighting conditions, occlusions, and fruit maturity stages. Among the YOLO models, YOLOv12m achieved the best accuracy with a mAP@50 of 93.3%, while RT-DETRv2-X obtained a mAP@50 of 93.6%, the highest among all the RT-DETR variants. The inference time varied with the model scale and complexity, and the mid-sized models appeared to offer a good accuracy-speed balance. To further enhance detection performance, all the models were fine-tuned using Unbiased Mean Teacher-based semi-supervised learning (SSL) on a separate set of 1,035 unlabeled images acquired by a ground-based machine vision platform in 2024. This resulted in accuracy gains ranging from -1.4% to 2.9%, with RT-DETR-v2-X achieving the best mAP@50 of 94.8%. More in-depth research into SSL is needed to better leverage cross-domain unlabeled data. Both the dataset and software programs of this study are made publicly available to support further research.</li>
</ul>

<h3>Title: Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture for Multi-Scale Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Ayan Sar, Sampurna Roy, Kanav Gupta, Anurag Kaushish, Tanupriya Choudhury, Abhijit Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20581">https://arxiv.org/abs/2509.20581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20581">https://arxiv.org/pdf/2509.20581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20581]] Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture for Multi-Scale Language Understanding(https://arxiv.org/abs/2509.20581)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer architectures have achieved state-of-the-art performance across natural language tasks, yet they fundamentally misrepresent the hierarchical nature of human language by processing text as flat token sequences. This results in quadratic computational cost, weak computational cost, weak compositional generalization, and inadequate discourse-level modeling. We propose Hierarchical Resolution Transformer (HRT), a novel wavelet-inspired neural architecture that processes language simultaneously across multiple resolutions, from characters to discourse-level units. HRT constructs a multi-resolution attention, enabling bottom-up composition and top-down contextualization. By employing exponential sequence reduction across scales, HRT achieves O(nlogn) complexity, offering significant efficiency improvements over standard transformers. We evaluated HRT on a diverse suite of benchmarks, including GLUE, SuperGLUE, Long Range Arena, and WikiText-103, and results demonstrated that HRT outperforms standard transformer baselines by an average of +3.8% on GLUE, +4.5% on SuperGLUE, and +6.1% on Long Range Arena, while reducing memory usage by 42% and inference latency by 37% compared to BERT and GPT style models of similar parameter count. Ablation studies confirm the effectiveness of cross-resolution attention and scale-specialized modules, showing that each contributes independently to both efficiency and accuracy. Our findings establish HRT as the first architecture to align computational structure with the hierarchical organization of human language, demonstrating that multi-scale, wavelet-inspired processing yields both theoretical efficiency gains and practical improvements in language understanding.</li>
</ul>

<h3>Title: Every Character Counts: From Vulnerability to Defense in Phishing Detection</h3>
<ul>
<li><strong>Authors: </strong>Maria Chiper, Radu Tudor Ionescu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20589">https://arxiv.org/abs/2509.20589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20589">https://arxiv.org/pdf/2509.20589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20589]] Every Character Counts: From Vulnerability to Defense in Phishing Detection(https://arxiv.org/abs/2509.20589)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Phishing attacks targeting both organizations and individuals are becoming an increasingly significant threat as technology advances. Current automatic detection methods often lack explainability and robustness in detecting new phishing attacks. In this work, we investigate the effectiveness of character-level deep learning models for phishing detection, which can provide both robustness and interpretability. We evaluate three neural architectures adapted to operate at the character level, namely CharCNN, CharGRU, and CharBiLSTM, on a custom-built email dataset, which combines data from multiple sources. Their performance is analyzed under three scenarios: (i) standard training and testing, (ii) standard training and testing under adversarial attacks, and (iii) training and testing with adversarial examples. Aiming to develop a tool that operates as a browser extension, we test all models under limited computational resources. In this constrained setup, CharGRU proves to be the best-performing model across all scenarios. All models show vulnerability to adversarial attacks, but adversarial training substantially improves their robustness. In addition, by adapting the Gradient-weighted Class Activation Mapping (Grad-CAM) technique to character-level inputs, we are able to visualize which parts of each email influence the decision of each model. Our open-source code and data is released at this https URL.</li>
</ul>

<h3>Title: Beyond SSO: Mobile Money Authentication for Inclusive e-Government in Sub-Saharan Africa</h3>
<ul>
<li><strong>Authors: </strong>Oluwole Adewusi, Wallace S. Msagusa, Jean Pierre Imanirumva, Okemawo Obadofin, Jema D. Ndibwile</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20592">https://arxiv.org/abs/2509.20592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20592">https://arxiv.org/pdf/2509.20592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20592]] Beyond SSO: Mobile Money Authentication for Inclusive e-Government in Sub-Saharan Africa(https://arxiv.org/abs/2509.20592)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack, biometric</a></li>
<li><strong>Abstract: </strong>The rapid adoption of Mobile Money Services (MMS) in Sub-Saharan Africa (SSA) offers a viable path to improve e-Government service accessibility in the face of persistent low internet penetration. However, existing Mobile Money Authentication (MMA) methods face critical limitations, including susceptibility to SIM swapping, weak session protection, and poor scalability during peak demand. This study introduces a hybrid MMA framework that combines Unstructured Supplementary Service Data (USSD)-based multi-factor authentication with secure session management via cryptographically bound JSON Web Tokens (JWT). Unlike traditional MMA systems that rely solely on SIM-PIN verification or smartphone-dependent biometrics, our design implements a three-factor authentication model; SIM verification, PIN entry, and session token binding, tailored for resource-constrained environments. Simulations and comparative analysis against OAuth-based Single Sign-On (SSO) methods reveal a 45% faster authentication time (8 seconds vs. 12 to 15 seconds), 15% higher success under poor network conditions (95% vs. 80%), and increased resistance to phishing and brute-force attacks. Penetration testing and threat modeling further demonstrate a substantial reduction in vulnerability exposure compared to conventional approaches. The primary contributions of this work are: (1) a hybrid authentication protocol that ensures offline accessibility and secure session continuity; (2) a tailored security framework addressing threats like SIM swapping and social engineering in SSA; and (3) demonstrated scalability for thousands of users with reduced infrastructure overhead. The proposed approach advances secure digital inclusion in SSA and other regions with similar constraints.</li>
</ul>

<h3>Title: TSKAN: Interpretable Machine Learning for QoE modeling over Time Series Data</h3>
<ul>
<li><strong>Authors: </strong>Kamal Singh, Priyanka Rawat, Sami Marouani, Baptiste Jeudy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20595">https://arxiv.org/abs/2509.20595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20595">https://arxiv.org/pdf/2509.20595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20595]] TSKAN: Interpretable Machine Learning for QoE modeling over Time Series Data(https://arxiv.org/abs/2509.20595)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Quality of Experience (QoE) modeling is crucial for optimizing video streaming services to capture the complex relationships between different features and user experience. We propose a novel approach to QoE modeling in video streaming applications using interpretable Machine Learning (ML) techniques over raw time series data. Unlike traditional black-box approaches, our method combines Kolmogorov-Arnold Networks (KANs) as an interpretable readout on top of compact frequency-domain features, allowing us to capture temporal information while retaining a transparent and explainable model. We evaluate our method on popular datasets and demonstrate its enhanced accuracy in QoE prediction, while offering transparency and interpretability.</li>
</ul>

<h3>Title: Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections</h3>
<ul>
<li><strong>Authors: </strong>Jing Wu, Zirui Wang, Iro Laina, Victor Adrian Prisacariu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20607">https://arxiv.org/abs/2509.20607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20607">https://arxiv.org/pdf/2509.20607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20607]] Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections(https://arxiv.org/abs/2509.20607)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Mirror reflections are common in everyday environments and can provide stereo information within a single capture, as the real and reflected virtual views are visible simultaneously. We exploit this property by treating the reflection as an auxiliary view and designing a transformation that constructs a physically valid virtual camera, allowing direct pixel-domain generation of the virtual view while adhering to the real-world imaging process. This enables a multi-view stereo setup from a single image, simplifying the imaging process, making it compatible with powerful feed-forward reconstruction models for generalizable and robust 3D reconstruction. To further exploit the geometric symmetry introduced by mirrors, we propose a symmetric-aware loss to refine pose estimation. Our framework also naturally extends to dynamic scenes, where each frame contains a mirror reflection, enabling efficient per-frame geometry recovery. For quantitative evaluation, we provide a fully customizable synthetic dataset of 16 Blender scenes, each with ground-truth point clouds and camera poses. Extensive experiments on real-world data and synthetic data are conducted to illustrate the effectiveness of our method.</li>
</ul>

<h3>Title: MMG: Mutual Information Estimation via the MMSE Gap in Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Longxuan Yu, Xing Shi, Xianghao Kong, Tong Jia, Greg Ver Steeg</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20609">https://arxiv.org/abs/2509.20609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20609">https://arxiv.org/pdf/2509.20609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20609]] MMG: Mutual Information Estimation via the MMSE Gap in Diffusion(https://arxiv.org/abs/2509.20609)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Mutual information (MI) is one of the most general ways to measure relationships between random variables, but estimating this quantity for complex systems is challenging. Denoising diffusion models have recently set a new bar for density estimation, so it is natural to consider whether these methods could also be used to improve MI estimation. Using the recently introduced information-theoretic formulation of denoising diffusion models, we show the diffusion models can be used in a straightforward way to estimate MI. In particular, the MI corresponds to half the gap in the Minimum Mean Square Error (MMSE) between conditional and unconditional diffusion, integrated over all Signal-to-Noise-Ratios (SNRs) in the noising process. Our approach not only passes self-consistency tests but also outperforms traditional and score-based diffusion MI estimators. Furthermore, our method leverages adaptive importance sampling to achieve scalable MI estimation, while maintaining strong performance even when the MI is high.</li>
</ul>

<h3>Title: Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Hanjiang Hu, Changliu Liu, Na Li, Yebin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20616">https://arxiv.org/abs/2509.20616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20616">https://arxiv.org/pdf/2509.20616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20616]] Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning(https://arxiv.org/abs/2509.20616)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in higher multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks with over 30 steps. We also theoretically and empirically validate the strong cross-task generalizability that the models trained on complex tasks can lead to the successful completion of all simpler subtasks.</li>
</ul>

<h3>Title: FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Amin Karimi Monsefi, Nikhil Bhendawade, Manuel Rafael Ciosici, Dominic Culver, Yizhe Zhang, Irina Belousova</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20624">https://arxiv.org/abs/2509.20624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20624">https://arxiv.org/pdf/2509.20624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20624]] FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models(https://arxiv.org/abs/2509.20624)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Autoregressive language models (ARMs) deliver strong likelihoods, but are inherently serial: they generate one token per forward pass, which limits throughput and inflates latency for long sequences. Diffusion Language Models (DLMs) parallelize across positions and thus appear promising for language generation, yet standard discrete diffusion typically needs hundreds to thousands of model evaluations to reach high quality, trading serial depth for iterative breadth. We introduce FS-DFM, Few-Step Discrete Flow-Matching. A discrete flow-matching model designed for speed without sacrificing quality. The core idea is simple: make the number of sampling steps an explicit parameter and train the model to be consistent across step budgets, so one big move lands where many small moves would. We pair this with a reliable update rule that moves probability in the right direction without overshooting, and with strong teacher guidance distilled from long-run trajectories. Together, these choices make few-step sampling stable, accurate, and easy to control. On language modeling benchmarks, FS-DFM with 8 sampling steps achieves perplexity parity with a 1,024-step discrete-flow baseline for generating 1,024 tokens using a similar-size model, delivering up to 128 times faster sampling and corresponding latency/throughput gains.</li>
</ul>

<h3>Title: Personalized Federated Dictionary Learning for Modeling Heterogeneity in Multi-site fMRI Data</h3>
<ul>
<li><strong>Authors: </strong>Yipu Zhang, Chengshuo Zhang, Ziyu Zhou, Gang Qu, Hao Zheng, Yuping Wang, Hui Shen, Hongwen Deng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20627">https://arxiv.org/abs/2509.20627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20627">https://arxiv.org/pdf/2509.20627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20627]] Personalized Federated Dictionary Learning for Modeling Heterogeneity in Multi-site fMRI Data(https://arxiv.org/abs/2509.20627)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Data privacy constraints pose significant challenges for large-scale neuroimaging analysis, especially in multi-site functional magnetic resonance imaging (fMRI) studies, where site-specific heterogeneity leads to non-independent and identically distributed (non-IID) data. These factors hinder the development of generalizable models. To address these challenges, we propose Personalized Federated Dictionary Learning (PFedDL), a novel federated learning framework that enables collaborative modeling across sites without sharing raw data. PFedDL performs independent dictionary learning at each site, decomposing each site-specific dictionary into a shared global component and a personalized local component. The global atoms are updated via federated aggregation to promote cross-site consistency, while the local atoms are refined independently to capture site-specific variability, thereby enhancing downstream analysis. Experiments on the ABIDE dataset demonstrate that PFedDL outperforms existing methods in accuracy and robustness across non-IID datasets.</li>
</ul>

<h3>Title: A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks</h3>
<ul>
<li><strong>Authors: </strong>Adam Swanda, Amy Chang, Alexander Chen, Fraser Burch, Paul Kassianik, Konstantin Berlin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20639">https://arxiv.org/abs/2509.20639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20639">https://arxiv.org/pdf/2509.20639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20639]] A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks(https://arxiv.org/abs/2509.20639)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The widespread adoption of Large Language Models (LLMs) has revolutionized AI deployment, enabling autonomous and semi-autonomous applications across industries through intuitive language interfaces and continuous improvements in model development. However, the attendant increase in autonomy and expansion of access permissions among AI applications also make these systems compelling targets for malicious attacks. Their inherent susceptibility to security flaws necessitates robust defenses, yet no known approaches can prevent zero-day or novel attacks against LLMs. This places AI protection systems in a category similar to established malware protection systems: rather than providing guaranteed immunity, they minimize risk through enhanced observability, multi-layered defense, and rapid threat response, supported by a threat intelligence function designed specifically for AI-related threats. Prior work on LLM protection has largely evaluated individual detection models rather than end-to-end systems designed for continuous, rapid adaptation to a changing threat landscape. We present a production-grade defense system rooted in established malware detection and threat intelligence practices. Our platform integrates three components: a threat intelligence system that turns emerging threats into protections; a data platform that aggregates and enriches information while providing observability, monitoring, and ML operations; and a release platform enabling safe, rapid detection updates without disrupting customer workflows. Together, these components deliver layered protection against evolving LLM threats while generating training data for continuous model improvement and deploying updates without interrupting production.</li>
</ul>

<h3>Title: Investigating Modality Contribution in Audio LLMs for Music</h3>
<ul>
<li><strong>Authors: </strong>Giovana Morais, Magdalena Fuentes</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20641">https://arxiv.org/abs/2509.20641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20641">https://arxiv.org/pdf/2509.20641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20641]] Investigating Modality Contribution in Audio LLMs for Music(https://arxiv.org/abs/2509.20641)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Audio Large Language Models (Audio LLMs) enable human-like conversation about music, yet it is unclear if they are truly listening to the audio or just using textual reasoning, as recent benchmarks suggest. This paper investigates this issue by quantifying the contribution of each modality to a model's output. We adapt the MM-SHAP framework, a performance-agnostic score based on Shapley values that quantifies the relative contribution of each modality to a model's prediction. We evaluate two models on the MuChoMusic benchmark and find that the model with higher accuracy relies more on text to answer questions, but further inspection shows that even if the overall audio contribution is low, models can successfully localize key sound events, suggesting that audio is not entirely ignored. Our study is the first application of MM-SHAP to Audio LLMs and we hope it will serve as a foundational step for future research in explainable AI and audio.</li>
</ul>

<h3>Title: Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Jungsoo Park, Ethan Mendes, Gabriel Stanovsky, Alan Ritter</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20645">https://arxiv.org/abs/2509.20645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20645">https://arxiv.org/pdf/2509.20645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20645]] Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions(https://arxiv.org/abs/2509.20645)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Progress in large language models is constrained by an evaluation bottleneck: build a benchmark, evaluate models and settings, then iterate. We therefore ask a simple question: can we forecast outcomes before running any experiments? We study text-only performance forecasting: estimating a model's score from a redacted task description and intended configuration, with no access to dataset instances. To support systematic study, we curate PRECOG, a corpus of redacted description-performance pairs spanning diverse tasks, domains, and metrics. Experiments show the task is challenging but feasible: models equipped with a retrieval module that excludes source papers achieve moderate prediction performance with well-calibrated uncertainty, reaching mean absolute error as low as 8.7 on the Accuracy subset at high-confidence thresholds. Our analysis indicates that stronger reasoning models engage in diverse, iterative querying, whereas current open-source models lag and often skip retrieval or gather evidence with limited diversity. We further test a zero-leakage setting, forecasting on newly released datasets or experiments before their papers are indexed, where GPT-5 with built-in web search still attains nontrivial prediction accuracy. Overall, our corpus and analyses offer an initial step toward open-ended anticipatory evaluation, supporting difficulty estimation and smarter experiment prioritization.</li>
</ul>

<h3>Title: Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration</h3>
<ul>
<li><strong>Authors: </strong>Yiyuan Pan, Zhe Liu, Hesheng Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20648">https://arxiv.org/abs/2509.20648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20648">https://arxiv.org/pdf/2509.20648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20648]] Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration(https://arxiv.org/abs/2509.20648)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Autonomous exploration in complex multi-agent reinforcement learning (MARL) with sparse rewards critically depends on providing agents with effective intrinsic motivation. While artificial curiosity offers a powerful self-supervised signal, it often confuses environmental stochasticity with meaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform novelty bias, treating all unexpected observations equally. However, peer behavior novelty, which encode latent task dynamics, are often overlooked, resulting in suboptimal exploration in decentralized, communication-free MARL settings. To this end, inspired by how human children adaptively calibrate their own exploratory behaviors via observing peers, we propose a novel approach to enhance multi-agent exploration. We introduce CERMIC, a principled framework that empowers agents to robustly filter noisy surprise signals and guide exploration by dynamically calibrating their intrinsic curiosity with inferred multi-agent context. Additionally, CERMIC generates theoretically-grounded intrinsic rewards, encouraging agents to explore state transitions with high information gain. We evaluate CERMIC on benchmark suites including VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that exploration with CERMIC significantly outperforms SoTA algorithms in sparse-reward environments.</li>
</ul>

<h3>Title: Enhancing Molecular Property Prediction with Knowledge from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Peng Zhou, Lai Hou Tim, Zhixiang Cheng, Kun Xie, Chaoyi Li, Wei Liu, Xiangxiang Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20664">https://arxiv.org/abs/2509.20664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20664">https://arxiv.org/pdf/2509.20664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20664]] Enhancing Molecular Property Prediction with Knowledge from Large Language Models(https://arxiv.org/abs/2509.20664)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Predicting molecular properties is a critical component of drug discovery. Recent advances in deep learning, particularly Graph Neural Networks (GNNs), have enabled end-to-end learning from molecular structures, reducing reliance on manual feature engineering. However, while GNNs and self-supervised learning approaches have advanced molecular property prediction (MPP), the integration of human prior knowledge remains indispensable, as evidenced by recent methods that leverage large language models (LLMs) for knowledge extraction. Despite their strengths, LLMs are constrained by knowledge gaps and hallucinations, particularly for less-studied molecular properties. In this work, we propose a novel framework that, for the first time, integrates knowledge extracted from LLMs with structural features derived from pre-trained molecular models to enhance MPP. Our approach prompts LLMs to generate both domain-relevant knowledge and executable code for molecular vectorization, producing knowledge-based features that are subsequently fused with structural representations. We employ three state-of-the-art LLMs, GPT-4o, GPT-4.1, and DeepSeek-R1, for knowledge extraction. Extensive experiments demonstrate that our integrated method outperforms existing approaches, confirming that the combination of LLM-derived knowledge and structural information provides a robust and effective solution for MPP.</li>
</ul>

<h3>Title: Theoretical Bounds for Stable In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Tongxi Wang, Zhuoyang Xia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20677">https://arxiv.org/abs/2509.20677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20677">https://arxiv.org/pdf/2509.20677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20677]] Theoretical Bounds for Stable In-Context Learning(https://arxiv.org/abs/2509.20677)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) is flexible but its reliability is highly sensitive to prompt length. This paper establishes a non-asymptotic lower bound that links the minimal number of demonstrations to ICL stability under fixed high-dimensional sub-Gaussian representations. The bound gives explicit sufficient conditions in terms of spectral properties of the covariance, providing a computable criterion for practice. Building on this analysis, we propose a two-stage observable estimator with a one-shot calibration that produces practitioner-ready prompt-length estimates without distributional priors. Experiments across diverse datasets, encoders, and generators show close alignment between the predicted thresholds and empirical knee-points, with the theory acting as a conservative but reliable upper bound; the calibrated variant further tightens this gap. These results connect spectral coverage to stable ICL, bridge theory and deployment, and improve the interpretability and reliability of large-scale prompting in realistic finite-sample regimes.</li>
</ul>

<h3>Title: Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Wenkai Guo, Xuefeng Liu, Haolin Wang, Jianwei Niu, Shaojie Tang, Jing Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20680">https://arxiv.org/abs/2509.20680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20680">https://arxiv.org/pdf/2509.20680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20680]] Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation(https://arxiv.org/abs/2509.20680)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, attack, federate, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) with local data is a widely adopted approach for organizations seeking to adapt LLMs to their specific domains. Given the shared characteristics in data across different organizations, the idea of collaboratively fine-tuning an LLM using data from multiple sources presents an appealing opportunity. However, organizations are often reluctant to share local data, making centralized fine-tuning impractical. Federated learning (FL), a privacy-preserving framework, enables clients to retain local data while sharing only model parameters for collaborative training, offering a potential solution. While fine-tuning LLMs on centralized datasets risks data leakage through next-token prediction, the iterative aggregation process in FL results in a global model that encapsulates generalized knowledge, which some believe protects client privacy. In this paper, however, we present contradictory findings through extensive experiments. We show that attackers can still extract training data from the global model, even using straightforward generation methods, with leakage increasing as the model size grows. Moreover, we introduce an enhanced attack strategy tailored to FL, which tracks global model updates during training to intensify privacy leakage. To mitigate these risks, we evaluate privacy-preserving techniques in FL, including differential privacy, regularization-constrained updates and adopting LLMs with safety alignment. Our results provide valuable insights and practical guidelines for reducing privacy risks when training LLMs with FL.</li>
</ul>

<h3>Title: Enhancing Cross-View Geo-Localization Generalization via Global-Local Consistency and Geometric Equivariance</h3>
<ul>
<li><strong>Authors: </strong>Xiaowei Wang, Di Wang, Ke Li, Yifeng Wang, Chengjian Wang, Libin Sun, Zhihong Wu, Yiming Zhang, Quan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20684">https://arxiv.org/abs/2509.20684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20684">https://arxiv.org/pdf/2509.20684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20684]] Enhancing Cross-View Geo-Localization Generalization via Global-Local Consistency and Geometric Equivariance(https://arxiv.org/abs/2509.20684)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Cross-view geo-localization (CVGL) aims to match images of the same location captured from drastically different viewpoints. Despite recent progress, existing methods still face two key challenges: (1) achieving robustness under severe appearance variations induced by diverse UAV orientations and fields of view, which hinders cross-domain generalization, and (2) establishing reliable correspondences that capture both global scene-level semantics and fine-grained local details. In this paper, we propose EGS, a novel CVGL framework designed to enhance cross-domain generalization. Specifically, we introduce an E(2)-Steerable CNN encoder to extract stable and reliable features under rotation and viewpoint shifts. Furthermore, we construct a graph with a virtual super-node that connects to all local nodes, enabling global semantics to be aggregated and redistributed to local regions, thereby enforcing global-local consistency. Extensive experiments on the University-1652 and SUES-200 benchmarks demonstrate that EGS consistently achieves substantial performance gains and establishes a new state of the art in cross-domain CVGL.</li>
</ul>

<h3>Title: Reliability Analysis of Fully Homomorphic Encryption Systems Under Memory Faults</h3>
<ul>
<li><strong>Authors: </strong>Rian Adam Rajagede, Yan Solihin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20686">https://arxiv.org/abs/2509.20686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20686">https://arxiv.org/pdf/2509.20686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20686]] Reliability Analysis of Fully Homomorphic Encryption Systems Under Memory Faults(https://arxiv.org/abs/2509.20686)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Fully Homomorphic Encryption (FHE) represents a paradigm shift in cryptography, enabling computation directly on encrypted data and unlocking privacy-critical computation. Despite being increasingly deployed in real platforms, the reliability aspects of FHE systems, especially how they respond to faults, have been mostly neglected. This paper aims to better understand of how FHE computation behaves in the presence of memory faults, both in terms of individual operations as well as at the level of applications, for different FHE schemes. Finally, we investigate how effective traditional and FHE-specific fault mitigation techniques are.</li>
</ul>

<h3>Title: RedHerring Attack: Testing the Reliability of Attack Detection</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Rusert</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20691">https://arxiv.org/abs/2509.20691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20691">https://arxiv.org/pdf/2509.20691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20691]] RedHerring Attack: Testing the Reliability of Attack Detection(https://arxiv.org/abs/2509.20691)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>In response to adversarial text attacks, attack detection models have been proposed and shown to successfully identify text modified by adversaries. Attack detection models can be leveraged to provide an additional check for NLP models and give signals for human input. However, the reliability of these models has not yet been thoroughly explored. Thus, we propose and test a novel attack setting and attack, RedHerring. RedHerring aims to make attack detection models unreliable by modifying a text to cause the detection model to predict an attack, while keeping the classifier correct. This creates a tension between the classifier and detector. If a human sees that the detector is giving an ``incorrect'' prediction, but the classifier a correct one, then the human will see the detector as unreliable. We test this novel threat model on 4 datasets against 3 detectors defending 4 classifiers. We find that RedHerring is able to drop detection accuracy between 20 - 71 points, while maintaining (or improving) classifier accuracy. As an initial defense, we propose a simple confidence check which requires no retraining of the classifier or detector and increases detection accuracy greatly. This novel threat model offers new insights into how adversaries may target detection models.</li>
</ul>

<h3>Title: Learning to Align Molecules and Proteins: A Geometry-Aware Approach to Binding Affinity</h3>
<ul>
<li><strong>Authors: </strong>Mohammadsaleh Refahi, Bahrad A. Sokhansanj, James R. Brown, Gail Rosen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.MN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20693">https://arxiv.org/abs/2509.20693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20693">https://arxiv.org/pdf/2509.20693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20693]] Learning to Align Molecules and Proteins: A Geometry-Aware Approach to Binding Affinity(https://arxiv.org/abs/2509.20693)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate prediction of drug-target binding affinity can accelerate drug discovery by prioritizing promising compounds before costly wet-lab screening. While deep learning has advanced this task, most models fuse ligand and protein representations via simple concatenation and lack explicit geometric regularization, resulting in poor generalization across chemical space and time. We introduce FIRM-DTI, a lightweight framework that conditions molecular embeddings on protein embeddings through a feature-wise linear modulation (FiLM) layer and enforces metric structure with a triplet loss. An RBF regression head operating on embedding distances yields smooth, interpretable affinity predictions. Despite its modest size, FIRM-DTI achieves state-of-the-art performance on the Therapeutics Data Commons DTI-DG benchmark, as demonstrated by an extensive ablation study and out-of-domain evaluation. Our results underscore the value of conditioning and metric learning for robust drug-target affinity prediction.</li>
</ul>

<h3>Title: Overcoming Black-box Attack Inefficiency with Hybrid and Dynamic Select Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Abhinay Shankar Belde, Rohit Ramkumar, Jonathan Rusert</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20699">https://arxiv.org/abs/2509.20699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20699">https://arxiv.org/pdf/2509.20699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20699]] Overcoming Black-box Attack Inefficiency with Hybrid and Dynamic Select Algorithms(https://arxiv.org/abs/2509.20699)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Adversarial text attack research plays a crucial role in evaluating the robustness of NLP models. However, the increasing complexity of transformer-based architectures has dramatically raised the computational cost of attack testing, especially for researchers with limited resources (e.g., GPUs). Existing popular black-box attack methods often require a large number of queries, which can make them inefficient and impractical for researchers. To address these challenges, we propose two new attack selection strategies called Hybrid and Dynamic Select, which better combine the strengths of previous selection algorithms. Hybrid Select merges generalized BinarySelect techniques with GreedySelect by introducing a size threshold to decide which selection algorithm to use. Dynamic Select provides an alternative approach of combining the generalized Binary and GreedySelect by learning which lengths of texts each selection method should be applied to. This greatly reduces the number of queries needed while maintaining attack effectiveness (a limitation of BinarySelect). Across 4 datasets and 6 target models, our best method(sentence-level Hybrid Select) is able to reduce the number of required queries per attack up 25.82\% on average against both encoder models and LLMs, without losing the effectiveness of the attack.</li>
</ul>

<h3>Title: DENet: Dual-Path Edge Network with Global-Local Attention for Infrared Small Target Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Zuo, Songwei Pei, Qian Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20701">https://arxiv.org/abs/2509.20701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20701">https://arxiv.org/pdf/2509.20701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20701]] DENet: Dual-Path Edge Network with Global-Local Attention for Infrared Small Target Detection(https://arxiv.org/abs/2509.20701)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Infrared small target detection is crucial for remote sensing applications like disaster warning and maritime surveillance. However, due to the lack of distinctive texture and morphological features, infrared small targets are highly susceptible to blending into cluttered and noisy backgrounds. A fundamental challenge in designing deep models for this task lies in the inherent conflict between capturing high-resolution spatial details for minute targets and extracting robust semantic context for larger targets, often leading to feature misalignment and suboptimal performance. Existing methods often rely on fixed gradient operators or simplistic attention mechanisms, which are inadequate for accurately extracting target edges under low contrast and high noise. In this paper, we propose a novel Dual-Path Edge Network that explicitly addresses this challenge by decoupling edge enhancement and semantic modeling into two complementary processing paths. The first path employs a Bidirectional Interaction Module, which uses both Local Self-Attention and Global Self-Attention to capture multi-scale local and global feature dependencies. The global attention mechanism, based on a Transformer architecture, integrates long-range semantic relationships and contextual information, ensuring robust scene understanding. The second path introduces the Multi-Edge Refiner, which enhances fine-grained edge details using cascaded Taylor finite difference operators at multiple scales. This mathematical approach, along with an attention-driven gating mechanism, enables precise edge localization and feature enhancement for targets of varying sizes, while effectively suppressing noise. Our method provides a promising solution for precise infrared small target detection and localization, combining structural semantics and edge refinement in a unified framework.</li>
</ul>

<h3>Title: CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhenpeng Su, Leiyu Pan, Minxuan Lv, Yuntao Li, Wenping Hu, Fuzheng Zhang, Kun Gai, Guorui Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20712">https://arxiv.org/abs/2509.20712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20712">https://arxiv.org/pdf/2509.20712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20712]] CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning(https://arxiv.org/abs/2509.20712)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. A core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play a critical yet overlooked role in regulating entropy evolution. We propose \textbf{C}ontrolling \textbf{E}ntropy via \textbf{G}radient-\textbf{P}reserving \textbf{P}olicy \textbf{O}ptimization (CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an exploration-exploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales.</li>
</ul>

<h3>Title: Cryptographic Backdoor for Neural Networks: Boon and Bane</h3>
<ul>
<li><strong>Authors: </strong>Anh Tu Ngo, Anupam Chattopadhyay, Subhamoy Maitra</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20714">https://arxiv.org/abs/2509.20714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20714">https://arxiv.org/pdf/2509.20714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20714]] Cryptographic Backdoor for Neural Networks: Boon and Bane(https://arxiv.org/abs/2509.20714)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, watermark</a></li>
<li><strong>Abstract: </strong>In this paper we show that cryptographic backdoors in a neural network (NN) can be highly effective in two directions, namely mounting the attacks as well as in presenting the defenses as well. On the attack side, a carefully planted cryptographic backdoor enables powerful and invisible attack on the NN. Considering the defense, we present applications: first, a provably robust NN watermarking scheme; second, a protocol for guaranteeing user authentication; and third, a protocol for tracking unauthorized sharing of the NN intellectual property (IP). From a broader theoretical perspective, borrowing the ideas from Goldwasser et. al. [FOCS 2022], our main contribution is to show that all these instantiated practical protocol implementations are provably robust. The protocols for watermarking, authentication and IP tracking resist an adversary with black-box access to the NN, whereas the backdoor-enabled adversarial attack is impossible to prevent under the standard assumptions. While the theoretical tools used for our attack is mostly in line with the Goldwasser et. al. ideas, the proofs related to the defense need further studies. Finally, all these protocols are implemented on state-of-the-art NN architectures with empirical results corroborating the theoretical claims. Further, one can utilize post-quantum primitives for implementing the cryptographic backdoors, laying out foundations for quantum-era applications in machine learning (ML).</li>
</ul>

<h3>Title: Scaling Laws are Redundancy Laws</h3>
<ul>
<li><strong>Authors: </strong>Yuda Bi, Vince D Calhoun</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20721">https://arxiv.org/abs/2509.20721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20721">https://arxiv.org/pdf/2509.20721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20721]] Scaling Laws are Redundancy Laws(https://arxiv.org/abs/2509.20721)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Scaling laws, a defining feature of deep learning, reveal a striking power-law improvement in model performance with increasing dataset and model size. Yet, their mathematical origins, especially the scaling exponent, have remained elusive. In this work, we show that scaling laws can be formally explained as redundancy laws. Using kernel regression, we show that a polynomial tail in the data covariance spectrum yields an excess risk power law with exponent alpha = 2s / (2s + 1/beta), where beta controls the spectral tail and 1/beta measures redundancy. This reveals that the learning curve's slope is not universal but depends on data redundancy, with steeper spectra accelerating returns to scale. We establish the law's universality across boundedly invertible transformations, multi-modal mixtures, finite-width approximations, and Transformer architectures in both linearized (NTK) and feature-learning regimes. This work delivers the first rigorous mathematical explanation of scaling laws as finite-sample redundancy laws, unifying empirical observations with theoretical foundations.</li>
</ul>

<h3>Title: The Impact of Audio Watermarking on Audio Anti-Spoofing Countermeasures</h3>
<ul>
<li><strong>Authors: </strong>Zhenshan Zhang, Xueping Zhang, Yechen Wang, Liwei Jin, Ming Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20736">https://arxiv.org/abs/2509.20736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20736">https://arxiv.org/pdf/2509.20736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20736]] The Impact of Audio Watermarking on Audio Anti-Spoofing Countermeasures(https://arxiv.org/abs/2509.20736)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, watermark</a></li>
<li><strong>Abstract: </strong>This paper presents the first study on the impact of audio watermarking on spoofing countermeasures. While anti-spoofing systems are essential for securing speech-based applications, the influence of widely used audio watermarking, originally designed for copyright protection, remains largely unexplored. We construct watermark-augmented training and evaluation datasets, named the Watermark-Spoofing dataset, by applying diverse handcrafted and neural watermarking methods to existing anti-spoofing datasets. Experiments show that watermarking consistently degrades anti-spoofing performance, with higher watermark density correlating with higher Equal Error Rates (EERs). To mitigate this, we propose the Knowledge-Preserving Watermark Learning (KPWL) framework, enabling models to adapt to watermark-induced shifts while preserving their original-domain spoofing detection capability. These findings reveal audio watermarking as a previously overlooked domain shift and establish the first benchmark for developing watermark-resilient anti-spoofing systems. All related protocols are publicly available at this https URL</li>
</ul>

<h3>Title: Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Yu Guo, Shengfeng He, Yuxu Lu, Haonan An, Yihang Tao, Huilin Zhu, Jingxian Liu, Yuguang Fang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20745">https://arxiv.org/abs/2509.20745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20745">https://arxiv.org/pdf/2509.20745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20745]] Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection(https://arxiv.org/abs/2509.20745)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Maritime object detection is essential for navigation safety, surveillance, and autonomous operations, yet constrained by two key challenges: the scarcity of annotated maritime data and poor generalization across various maritime attributes (e.g., object category, viewpoint, location, and imaging environment). % In particular, models trained on existing datasets often underperform in underrepresented scenarios such as open-sea environments. To address these challenges, we propose Neptune-X, a data-centric generative-selection framework that enhances training effectiveness by leveraging synthetic data generation with task-aware sample selection. From the generation perspective, we develop X-to-Maritime, a multi-modality-conditioned generative model that synthesizes diverse and realistic maritime scenes. A key component is the Bidirectional Object-Water Attention module, which captures boundary interactions between objects and their aquatic surroundings to improve visual fidelity. To further improve downstream tasking performance, we propose Attribute-correlated Active Sampling, which dynamically selects synthetic samples based on their task relevance. To support robust benchmarking, we construct the Maritime Generation Dataset, the first dataset tailored for generative maritime learning, encompassing a wide range of semantic conditions. Extensive experiments demonstrate that our approach sets a new benchmark in maritime scene synthesis, significantly improving detection accuracy, particularly in challenging and previously underrepresented this http URL code is available at this https URL.</li>
</ul>

<h3>Title: AI-Enabled Crater-Based Navigation for Lunar Mapping</h3>
<ul>
<li><strong>Authors: </strong>Sofia McLeod, Chee-Kheng Chng, Matthew Rodda, Tat-Jun Chin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20748">https://arxiv.org/abs/2509.20748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20748">https://arxiv.org/pdf/2509.20748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20748]] AI-Enabled Crater-Based Navigation for Lunar Mapping(https://arxiv.org/abs/2509.20748)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Crater-Based Navigation (CBN) uses the ubiquitous impact craters of the Moon observed on images as natural landmarks to determine the six degrees of freedom pose of a spacecraft. To date, CBN has primarily been studied in the context of powered descent and landing. These missions are typically short in duration, with high-frequency imagery captured from a nadir viewpoint over well-lit terrain. In contrast, lunar mapping missions involve sparse, oblique imagery acquired under varying illumination conditions over potentially year-long campaigns, posing significantly greater challenges for pose estimation. We bridge this gap with STELLA - the first end-to-end CBN pipeline for long-duration lunar mapping. STELLA combines a Mask R-CNN-based crater detector, a descriptor-less crater identification module, a robust perspective-n-crater pose solver, and a batch orbit determination back-end. To rigorously test STELLA, we introduce CRESENT-365 - the first public dataset that emulates a year-long lunar mapping mission. Each of its 15,283 images is rendered from high-resolution digital elevation models with SPICE-derived Sun angles and Moon motion, delivering realistic global coverage, illumination cycles, and viewing geometries. Experiments on CRESENT+ and CRESENT-365 show that STELLA maintains metre-level position accuracy and sub-degree attitude accuracy on average across wide ranges of viewing angles, illumination conditions, and lunar latitudes. These results constitute the first comprehensive assessment of CBN in a true lunar mapping setting and inform operational conditions that should be considered for future missions.</li>
</ul>

<h3>Title: Confidence-guided Refinement Reasoning for Zero-shot Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Youwon Jang, Woo Suk Choi, Minjoon Jung, Minsu Lee, Byoung-Tak Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20750">https://arxiv.org/abs/2509.20750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20750">https://arxiv.org/pdf/2509.20750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20750]] Confidence-guided Refinement Reasoning for Zero-shot Question Answering(https://arxiv.org/abs/2509.20750)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose Confidence-guided Refinement Reasoning (C2R), a novel training-free framework applicable to question-answering (QA) tasks across text, image, and video domains. C2R strategically constructs and refines sub-questions and their answers (sub-QAs), deriving a better confidence score for the target answer. C2R first curates a subset of sub-QAs to explore diverse reasoning paths, then compares the confidence scores of the resulting answer candidates to select the most reliable final answer. Since C2R relies solely on confidence scores derived from the model itself, it can be seamlessly integrated with various existing QA models, demonstrating consistent performance improvements across diverse models and benchmarks. Furthermore, we provide essential yet underexplored insights into how leveraging sub-QAs affects model behavior, specifically analyzing the impact of both the quantity and quality of sub-QAs on achieving robust and reliable reasoning.</li>
</ul>

<h3>Title: Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment Between Vision and Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zoe Wanying He, Sean Trott, Meenakshi Khosla</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20751">https://arxiv.org/abs/2509.20751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20751">https://arxiv.org/pdf/2509.20751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20751]] Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment Between Vision and Language Models(https://arxiv.org/abs/2509.20751)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent studies show that deep vision-only and language-only models--trained on disjoint modalities--nonetheless project their inputs into a partially aligned representational space. Yet we still lack a clear picture of where in each network this convergence emerges, what visual or linguistic cues support it, whether it captures human preferences in many-to-many image-text scenarios, and how aggregating exemplars of the same concept affects alignment. Here, we systematically investigate these questions. We find that alignment peaks in mid-to-late layers of both model types, reflecting a shift from modality-specific to conceptually shared representations. This alignment is robust to appearance-only changes but collapses when semantics are altered (e.g., object removal or word-order scrambling), highlighting that the shared code is truly semantic. Moving beyond the one-to-one image-caption paradigm, a forced-choice "Pick-a-Pic" task shows that human preferences for image-caption matches are mirrored in the embedding spaces across all vision-language model pairs. This pattern holds bidirectionally when multiple captions correspond to a single image, demonstrating that models capture fine-grained semantic distinctions akin to human judgments. Surprisingly, averaging embeddings across exemplars amplifies alignment rather than blurring detail. Together, our results demonstrate that unimodal networks converge on a shared semantic code that aligns with human judgments and strengthens with exemplar aggregation.</li>
</ul>

<h3>Title: FreeInsert: Personalized Object Insertion with Geometric and Style Control</h3>
<ul>
<li><strong>Authors: </strong>Yuhong Zhang, Han Wang, Yiwen Wang, Rong Xie, Li Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20756">https://arxiv.org/abs/2509.20756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20756">https://arxiv.org/pdf/2509.20756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20756]] FreeInsert: Personalized Object Insertion with Geometric and Style Control(https://arxiv.org/abs/2509.20756)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have made significant progress in image generation, allowing for effortless customized generation. However, existing image editing methods still face certain limitations when dealing with personalized image composition tasks. First, there is the issue of lack of geometric control over the inserted objects. Current methods are confined to 2D space and typically rely on textual instructions, making it challenging to maintain precise geometric control over the objects. Second, there is the challenge of style consistency. Existing methods often overlook the style consistency between the inserted object and the background, resulting in a lack of realism. In addition, the challenge of inserting objects into images without extensive training remains significant. To address these issues, we propose \textit{FreeInsert}, a novel training-free framework that customizes object insertion into arbitrary scenes by leveraging 3D geometric information. Benefiting from the advances in existing 3D generation models, we first convert the 2D object into 3D, perform interactive editing at the 3D level, and then re-render it into a 2D image from a specified view. This process introduces geometric controls such as shape or view. The rendered image, serving as geometric control, is combined with style and content control achieved through diffusion adapters, ultimately producing geometrically controlled, style-consistent edited images via the diffusion model.</li>
</ul>

<h3>Title: SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Lin, Zhongruo Wang, Kun Qian, Tian Wang, Arvind Srinivasan, Hansi Zeng, Ruochen Jiao, Xie Zhou, Jiri Gesi, Dakuo Wang, Yufan Guo, Kai Zhong, Weiqi Zhang, Sujay Sanghavi, Changyou Chen, Hyokun Yun, Lihong Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20758">https://arxiv.org/abs/2509.20758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20758">https://arxiv.org/pdf/2509.20758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20758]] SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs(https://arxiv.org/abs/2509.20758)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Supervised Fine-Tuning (SFT) on domain-specific datasets is a common approach to adapt Large Language Models (LLMs) to specialized tasks but is often believed to degrade their general capabilities. In this work, we revisit this trade-off and present both empirical and theoretical insights. First, we show that SFT does not always hurt: using a smaller learning rate can substantially mitigate general performance degradation while preserving comparable target-domain performance. We then provide a theoretical analysis that explains these phenomena and further motivates a new method, Token-Adaptive Loss Reweighting (TALR). Building on this, and recognizing that smaller learning rates alone do not fully eliminate general-performance degradation in all cases, we evaluate a range of strategies for reducing general capability loss, including L2 regularization, LoRA, model averaging, FLOW, and our proposed TALR. Experimental results demonstrate that while no method completely eliminates the trade-off, TALR consistently outperforms these baselines in balancing domain-specific gains and general capabilities. Finally, we distill our findings into practical guidelines for adapting LLMs to new domains: (i) using a small learning rate to achieve a favorable trade-off, and (ii) when a stronger balance is further desired, adopt TALR as an effective strategy.</li>
</ul>

<h3>Title: ExpIDS: A Drift-adaptable Network Intrusion Detection System With Improved Explainability</h3>
<ul>
<li><strong>Authors: </strong>Ayush Kumar, Kar Wai Fok, Vrizlynn L.L. Thing</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20767">https://arxiv.org/abs/2509.20767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20767">https://arxiv.org/pdf/2509.20767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20767]] ExpIDS: A Drift-adaptable Network Intrusion Detection System With Improved Explainability(https://arxiv.org/abs/2509.20767)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, explainability</a></li>
<li><strong>Abstract: </strong>Despite all the advantages associated with Network Intrusion Detection Systems (NIDSs) that utilize machine learning (ML) models, there is a significant reluctance among cyber security experts to implement these models in real-world production settings. This is primarily because of their opaque nature, meaning it is unclear how and why the models make their decisions. In this work, we design a deep learning-based NIDS, ExpIDS to have high decision tree explanation fidelity, i.e., the predictions of decision tree explanation corresponding to ExpIDS should be as close to ExpIDS's predictions as possible. ExpIDS can also adapt to changes in network traffic distribution (drift). With the help of extensive experiments, we verify that ExpIDS achieves higher decision tree explanation fidelity and a malicious traffic detection performance comparable to state-of-the-art NIDSs for common attacks with varying levels of real-world drift.</li>
</ul>

<h3>Title: Measuring LLM Sensitivity in Transformer-based Tabular Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Maria F. Davila R, Azizjon Turaev, Wolfram Wingerath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20768">https://arxiv.org/abs/2509.20768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20768">https://arxiv.org/pdf/2509.20768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20768]] Measuring LLM Sensitivity in Transformer-based Tabular Data Synthesis(https://arxiv.org/abs/2509.20768)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Synthetic tabular data is used for privacy-preserving data sharing and data-driven model development. Its effectiveness, however, depends heavily on the used Tabular Data Synthesis (TDS) tool. Recent studies have shown that Transformer-based models outperform other state-of-the-art models such as Generative Adversarial Networks (GANs) and Diffusion models in terms of data quality. However, Transformer-based models also come with high computational costs, making them sometimes unfeasible for end users with prosumer hardware. This study presents a sensitivity assessment on how the choice of hyperparameters, such as number of layers or hidden dimension affects the quality of the resultant synthetic data and the computational performance. It is performed across two tools, GReaT and REaLTabFormer, evaluating 10 model setups that vary in architecture type and depth. We assess the sensitivity on three dimensions: runtime, machine learning (ML) utility, and similarity to real data distributions. Experiments were conducted on four real-world datasets. Our findings reveal that runtime is proportional to the number of hyperparameters, with shallower configurations completing faster. GReaT consistently achieves lower runtimes than REaLTabFormer, and only on the largest dataset they have comparable runtime. For small datasets, both tools achieve synthetic data with high utility and optimal similarity, but on larger datasets only REaLTabFormer sustains strong utility and similarity. As a result, REaLTabFormer with lightweight LLMs provides the best balance, since it preserves data quality while reducing computational requirements. Nonetheless, its runtime remains higher than that of GReaT and other TDS tools, suggesting that efficiency gains are possible but only up to a certain level.</li>
</ul>

<h3>Title: CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion</h3>
<ul>
<li><strong>Authors: </strong>Maoye Ren, Praneetha Vaddamanu, Jianjin Xu, Fernando De la Torre Frade</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20775">https://arxiv.org/abs/2509.20775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20775">https://arxiv.org/pdf/2509.20775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20775]] CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion(https://arxiv.org/abs/2509.20775)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently remarkable progress has been made in synthesizing realistic human photos using text-to-image diffusion models. However, current approaches face degraded scenes, insufficient control, and suboptimal perceptual identity. We introduce CustomEnhancer, a novel framework to augment existing identity customization models. CustomEnhancer is a zero-shot enhancement pipeline that leverages face swapping techniques, pretrained diffusion model, to obtain additional representations in a zeroshot manner for encoding into personalized models. Through our proposed triple-flow fused PerGeneration approach, which identifies and combines two compatible counter-directional latent spaces to manipulate a pivotal space of personalized model, we unify the generation and reconstruction processes, realizing generation from three flows. Our pipeline also enables comprehensive training-free control over the generation process of personalized models, offering precise controlled personalization for them and eliminating the need for controller retraining for per-model. Besides, to address the high time complexity of null-text inversion (NTI), we introduce ResInversion, a novel inversion method that performs noise rectification via a pre-diffusion mechanism, reducing the inversion time by 129 times. Experiments demonstrate that CustomEnhancer reach SOTA results at scene diversity, identity fidelity, training-free controls, while also showing the efficiency of our ResInversion over NTI. The code will be made publicly available upon paper acceptance.</li>
</ul>

<h3>Title: Towards Atoms of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenhui Hu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20784">https://arxiv.org/abs/2509.20784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20784">https://arxiv.org/pdf/2509.20784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20784]] Towards Atoms of Large Language Models(https://arxiv.org/abs/2509.20784)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The fundamental units of internal representations in large language models (LLMs) remain undefined, limiting further understanding of their mechanisms. Neurons or features are often regarded as such units, yet neurons suffer from polysemy, while features face concerns of unreliable reconstruction and instability. To address this issue, we propose the Atoms Theory, which defines such units as atoms. We introduce the atomic inner product (AIP) to correct representation shifting, formally define atoms, and prove the conditions that atoms satisfy the Restricted Isometry Property (RIP), ensuring stable sparse representations over atom set and linking to compressed sensing. Under stronger conditions, we further establish the uniqueness and exact $\ell_1$ recoverability of the sparse representations, and provide guarantees that single-layer sparse autoencoders (SAEs) with threshold activations can reliably identify the atoms. To validate the Atoms Theory, we train threshold-activated SAEs on Gemma2-2B, Gemma2-9B, and Llama3.1-8B, achieving 99.9% sparse reconstruction across layers on average, and more than 99.8% of atoms satisfy the uniqueness condition, compared to 0.5% for neurons and 68.2% for features, showing that atoms more faithfully capture intrinsic representations of LLMs. Scaling experiments further reveal the link between SAEs size and recovery capacity. Overall, this work systematically introduces and validates Atoms Theory of LLMs, providing a theoretical framework for understanding internal representations and a foundation for mechanistic interpretability. Code available at this https URL.</li>
</ul>

<h3>Title: Dual-supervised Asymmetric Co-training for Semi-supervised Medical Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Jincai Song, Haipeng Chen, Jun Qin, Na Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20785">https://arxiv.org/abs/2509.20785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20785">https://arxiv.org/pdf/2509.20785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20785]] Dual-supervised Asymmetric Co-training for Semi-supervised Medical Domain Generalization(https://arxiv.org/abs/2509.20785)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Semi-supervised domain generalization (SSDG) in medical image segmentation offers a promising solution for generalizing to unseen domains during testing, addressing domain shift challenges and minimizing annotation costs. However, conventional SSDG methods assume labeled and unlabeled data are available for each source domain in the training set, a condition that is not always met in practice. The coexistence of limited annotation and domain shift in the training set is a prevalent issue. Thus, this paper explores a more practical and challenging scenario, cross-domain semi-supervised domain generalization (CD-SSDG), where domain shifts occur between labeled and unlabeled training data, in addition to shifts between training and testing sets. Existing SSDG methods exhibit sub-optimal performance under such domain shifts because of inaccurate pseudolabels. To address this issue, we propose a novel dual-supervised asymmetric co-training (DAC) framework tailored for CD-SSDG. Building upon the co-training paradigm with two sub-models offering cross pseudo supervision, our DAC framework integrates extra feature-level supervision and asymmetric auxiliary tasks for each sub-model. This feature-level supervision serves to address inaccurate pseudo supervision caused by domain shifts between labeled and unlabeled data, utilizing complementary supervision from the rich feature space. Additionally, two distinct auxiliary self-supervised tasks are integrated into each sub-model to enhance domain-invariant discriminative feature learning and prevent model collapse. Extensive experiments on real-world medical image segmentation datasets, i.e., Fundus, Polyp, and SCGM, demonstrate the robust generalizability of the proposed DAC framework.</li>
</ul>

<h3>Title: LiLAW: Lightweight Learnable Adaptive Weighting to Meta-Learn Sample Difficulty and Improve Noisy Training</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Moturu, Anna Goldenberg, Babak Taati</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20786">https://arxiv.org/abs/2509.20786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20786">https://arxiv.org/pdf/2509.20786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20786]] LiLAW: Lightweight Learnable Adaptive Weighting to Meta-Learn Sample Difficulty and Improve Noisy Training(https://arxiv.org/abs/2509.20786)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Training deep neural networks in the presence of noisy labels and data heterogeneity is a major challenge. We introduce Lightweight Learnable Adaptive Weighting (LiLAW), a novel method that dynamically adjusts the loss weight of each training sample based on its evolving difficulty level, categorized as easy, moderate, or hard. Using only three learnable parameters, LiLAW adaptively prioritizes informative samples throughout training by updating these weights using a single mini-batch gradient descent step on the validation set after each training mini-batch, without requiring excessive hyperparameter tuning or a clean validation set. Extensive experiments across multiple general and medical imaging datasets, noise levels and types, loss functions, and architectures with and without pretraining demonstrate that LiLAW consistently enhances performance, even in high-noise environments. It is effective without heavy reliance on data augmentation or advanced regularization, highlighting its practicality. It offers a computationally efficient solution to boost model generalization and robustness in any neural network training setup.</li>
</ul>

<h3>Title: DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Ved Umrajkar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20792">https://arxiv.org/abs/2509.20792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20792">https://arxiv.org/pdf/2509.20792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20792]] DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation(https://arxiv.org/abs/2509.20792)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) are foundational to critical applications like autonomous driving, medical diagnosis, and content moderation. While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA enable their efficient adaptation to specialized tasks, these models remain vulnerable to adversarial attacks that can compromise safety-critical decisions. CLIP, the backbone for numerous downstream VLMs, is a high-value target whose vulnerabilities can cascade across the multimodal AI ecosystem. We propose Dynamic Adversarial Curriculum DAC-LoRA, a novel framework that integrates adversarial training into PEFT. The core principle of our method i.e. an intelligent curriculum of progressively challenging attack, is general and can potentially be applied to any iterative attack method. Guided by the First-Order Stationary Condition (FOSC) and a TRADES-inspired loss, DAC-LoRA achieves substantial improvements in adversarial robustness without significantly compromising clean accuracy. Our work presents an effective, lightweight, and broadly applicable method to demonstrate that the DAC-LoRA framework can be easily integrated into a standard PEFT pipeline to significantly enhance robustness.</li>
</ul>

<h3>Title: FERD: Fairness-Enhanced Data-Free Robustness Distillation</h3>
<ul>
<li><strong>Authors: </strong>Zhengxiao Li, Liming Lu, Xu Zheng, Siyuan Liang, Zhenghan Chen, Yongbin Zhou, Shuchao Pang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20793">https://arxiv.org/abs/2509.20793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20793">https://arxiv.org/pdf/2509.20793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20793]] FERD: Fairness-Enhanced Data-Free Robustness Distillation(https://arxiv.org/abs/2509.20793)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, fair, data-free</a></li>
<li><strong>Abstract: </strong>Data-Free Robustness Distillation (DFRD) aims to transfer the robustness from the teacher to the student without accessing the training data. While existing methods focus on overall robustness, they overlook the robust fairness issues, leading to severe disparity of robustness across different categories. In this paper, we find two key problems: (1) student model distilled with equal class proportion data behaves significantly different across distinct categories; and (2) the robustness of student model is not stable across different attacks target. To bridge these gaps, we present the first Fairness-Enhanced data-free Robustness Distillation (FERD) framework to adjust the proportion and distribution of adversarial examples. For the proportion, FERD adopts a robustness-guided class reweighting strategy to synthesize more samples for the less robust categories, thereby improving robustness of them. For the distribution, FERD generates complementary data samples for advanced robustness distillation. It generates Fairness-Aware Examples (FAEs) by enforcing a uniformity constraint on feature-level predictions, which suppress the dominance of class-specific non-robust features, providing a more balanced representation across all categories. Then, FERD constructs Uniform-Target Adversarial Examples (UTAEs) from FAEs by applying a uniform target class constraint to avoid biased attack directions, which distribute the attack targets across all categories and prevents overfitting to specific vulnerable categories. Extensive experiments on three public datasets show that FERD achieves state-of-the-art worst-class robustness under all adversarial attack (e.g., the worst-class robustness under FGSM and AutoAttack are improved by 15.1\% and 6.4\% using MobileNet-V2 on CIFAR-10), demonstrating superior performance in both robustness and fairness aspects.</li>
</ul>

<h3>Title: Fast Revocable Attribute-Based Encryption with Data Integrity for Internet of Things</h3>
<ul>
<li><strong>Authors: </strong>Yongjiao Li, Liang Zhu, Yalin Deng, Qikun Zhang, Zhenlei Wang, Zhu Cao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20796">https://arxiv.org/abs/2509.20796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20796">https://arxiv.org/pdf/2509.20796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20796]] Fast Revocable Attribute-Based Encryption with Data Integrity for Internet of Things(https://arxiv.org/abs/2509.20796)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Efficient and secure revocable attribute-based encryption (RABE) is vital for ensuring flexible and fine-grained access control and data sharing in cloud storage and outsourced data environments within the Internet of Things (IoT). However, current RABE schemes often struggle to achieve an optimal balance between efficiency, security, dynamic scalability, and other important features, which hampers their practical application. To overcome these limitations, we propose a fast RABE scheme with data integrity for IoT that achieves adaptive security with multiple challenge ciphertexts. Our scheme supports the revocation of authorized users and transfers the computationally heavy revocation processes to the cloud, thereby easing the computational burden on IoT devices. Moreover, it consistently guarantees the integrity and correctness of data. We have demonstrated its adaptive security within the defined security model with multiple challenge ciphertexts and optimized its performance. Experimental results indicate that our scheme provides better performance than existing solutions. Under the same access policy, our scheme reduces computational consumption by 7 to 9 times compared to previous schemes.</li>
</ul>

<h3>Title: Few-Shot and Training-Free Review Generation via Conversational Prompting</h3>
<ul>
<li><strong>Authors: </strong>Genki Kusano</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20805">https://arxiv.org/abs/2509.20805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20805">https://arxiv.org/pdf/2509.20805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20805]] Few-Shot and Training-Free Review Generation via Conversational Prompting(https://arxiv.org/abs/2509.20805)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Personalized review generation helps businesses understand user preferences, yet most existing approaches assume extensive review histories of the target user or require additional model training. Real-world applications often face few-shot and training-free situations, where only a few user reviews are available and fine-tuning is infeasible. It is well known that large language models (LLMs) can address such low-resource settings, but their effectiveness depends on prompt engineering. In this paper, we propose Conversational Prompting, a lightweight method that reformulates user reviews as multi-turn conversations. Its simple variant, Simple Conversational Prompting (SCP), relies solely on the user's own reviews, while the contrastive variant, Contrastive Conversational Prompting (CCP), inserts reviews from other users or LLMs as incorrect replies and then asks the model to correct them, encouraging the model to produce text in the user's style. Experiments on eight product domains and five LLMs showed that the conventional non-conversational prompt often produced reviews similar to those written by random users, based on text-based metrics such as ROUGE-L and BERTScore, and application-oriented tasks like user identity matching and sentiment analysis. In contrast, both SCP and CCP produced reviews much closer to those of the target user, even when each user had only two reviews. CCP brings further improvements when high-quality negative examples are available, whereas SCP remains competitive when such data cannot be collected. These results suggest that conversational prompting offers a practical solution for review generation under few-shot and training-free constraints.</li>
</ul>

<h3>Title: Federated Domain Generalization with Domain-specific Soft Prompts Generation</h3>
<ul>
<li><strong>Authors: </strong>Jianhan Wu, Xiaoyang Qu, Zhangcheng Huang, Jianzong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20807">https://arxiv.org/abs/2509.20807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20807">https://arxiv.org/pdf/2509.20807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20807]] Federated Domain Generalization with Domain-specific Soft Prompts Generation(https://arxiv.org/abs/2509.20807)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, generative</a></li>
<li><strong>Abstract: </strong>Prompt learning has become an efficient paradigm for adapting CLIP to downstream tasks. Compared with traditional fine-tuning, prompt learning optimizes a few parameters yet yields highly competitive results, especially appealing in federated learning for computational efficiency. engendering domain shift among clients and posing a formidable challenge for downstream-task adaptation. Existing federated domain generalization (FDG) methods based on prompt learning typically learn soft prompts from training samples, replacing manually designed prompts to enhance the generalization ability of federated models. However, these learned prompts exhibit limited diversity and tend to ignore information from unknown domains. We propose a novel and effective method from a generative perspective for handling FDG tasks, namely federated domain generalization with domain-specific soft prompts generation (FedDSPG). Specifically, during training, we introduce domain-specific soft prompts (DSPs) for each domain and integrate content and domain knowledge into the generative model among clients. In the inference phase, the generator is utilized to obtain DSPs for unseen target domains, thus guiding downstream tasks in unknown domains. Comprehensive evaluations across several public datasets confirm that our method outperforms existing strong baselines in FDG, achieving state-of-the-art results.</li>
</ul>

<h3>Title: Intelligent Graybox Fuzzing via ATPG-Guided Seed Generation and Submodule Analysis</h3>
<ul>
<li><strong>Authors: </strong>Raghul Saravanan, Sudipta Paria, Aritra Dasgupta, Swarup Bhunia, Sai Manoj P D</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20808">https://arxiv.org/abs/2509.20808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20808">https://arxiv.org/pdf/2509.20808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20808]] Intelligent Graybox Fuzzing via ATPG-Guided Seed Generation and Submodule Analysis(https://arxiv.org/abs/2509.20808)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Hardware Fuzzing emerged as one of the crucial techniques for finding security flaws in modern hardware designs by testing a wide range of input scenarios. One of the main challenges is creating high-quality input seeds that maximize coverage and speed up verification. Coverage-Guided Fuzzing (CGF) methods help explore designs more effectively, but they struggle to focus on specific parts of the hardware. Existing Directed Gray-box Fuzzing (DGF) techniques like DirectFuzz try to solve this by generating targeted tests, but it has major drawbacks, such as supporting only limited hardware description languages, not scaling well to large circuits, and having issues with abstraction mismatches. To address these problems, we introduce a novel framework, PROFUZZ, that follows the DGF approach and combines fuzzing with Automatic Test Pattern Generation (ATPG) for more efficient fuzzing. By leveraging ATPG's structural analysis capabilities, PROFUZZ can generate precise input seeds that target specific design regions more effectively while maintaining high fuzzing throughput. Our experiments show that PROFUZZ scales 30x better than DirectFuzz when handling multiple target sites, improves coverage by 11.66%, and runs 2.76x faster, highlighting its scalability and effectiveness for directed fuzzing in complex hardware systems.</li>
</ul>

<h3>Title: Enrich-on-Graph: Query-Graph Alignment for Complex Reasoning with LLM Enriching</h3>
<ul>
<li><strong>Authors: </strong>Songze Li, Zhiqiang Liu, Zhengke Gui, Huajun Chen, Wen Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20810">https://arxiv.org/abs/2509.20810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20810">https://arxiv.org/pdf/2509.20810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20810]] Enrich-on-Graph: Query-Graph Alignment for Complex Reasoning with LLM Enriching(https://arxiv.org/abs/2509.20810)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit strong reasoning capabilities in complex tasks. However, they still struggle with hallucinations and factual errors in knowledge-intensive scenarios like knowledge graph question answering (KGQA). We attribute this to the semantic gap between structured knowledge graphs (KGs) and unstructured queries, caused by inherent differences in their focuses and structures. Existing methods usually employ resource-intensive, non-scalable workflows reasoning on vanilla KGs, but overlook this gap. To address this challenge, we propose a flexible framework, Enrich-on-Graph (EoG), which leverages LLMs' prior knowledge to enrich KGs, bridge the semantic gap between graphs and queries. EoG enables efficient evidence extraction from KGs for precise and robust reasoning, while ensuring low computational costs, scalability, and adaptability across different methods. Furthermore, we propose three graph quality evaluation metrics to analyze query-graph alignment in KGQA task, supported by theoretical validation of our optimization objectives. Extensive experiments on two KGQA benchmark datasets indicate that EoG can effectively generate high-quality KGs and achieve the state-of-the-art performance. Our code and data are available at this https URL.</li>
</ul>

<h3>Title: Leveraging What's Overfixed: Post-Correction via LLM Grammatical Error Overcorrection</h3>
<ul>
<li><strong>Authors: </strong>Taehee Park, Heejin Do, Gary Geunbae Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20811">https://arxiv.org/abs/2509.20811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20811">https://arxiv.org/pdf/2509.20811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20811]] Leveraging What's Overfixed: Post-Correction via LLM Grammatical Error Overcorrection(https://arxiv.org/abs/2509.20811)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Robust supervised fine-tuned small Language Models (sLMs) often show high reliability but tend to undercorrect. They achieve high precision at the cost of low recall. Conversely, Large Language Models (LLMs) often show the opposite tendency, making excessive overcorrection, leading to low precision. To effectively harness the strengths of LLMs to address the recall challenges in sLMs, we propose Post-Correction via Overcorrection (PoCO), a novel approach that strategically balances recall and precision. PoCO first intentionally triggers overcorrection via LLM to maximize recall by allowing comprehensive revisions, then applies a targeted post-correction step via fine-tuning smaller models to identify and refine erroneous outputs. We aim to harmonize both aspects by leveraging the generative power of LLMs while preserving the reliability of smaller supervised models. Our extensive experiments demonstrate that PoCO effectively balances GEC performance by increasing recall with competitive precision, ultimately improving the overall quality of grammatical error correction.</li>
</ul>

<h3>Title: Revolutionizing Precise Low Back Pain Diagnosis via Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Thanh Binh Le, Hoang Nhat Khang Vo, Tan-Ha Mai, Trong Nhan Phan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20813">https://arxiv.org/abs/2509.20813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20813">https://arxiv.org/pdf/2509.20813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20813]] Revolutionizing Precise Low Back Pain Diagnosis via Contrastive Learning(https://arxiv.org/abs/2509.20813)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Low back pain affects millions worldwide, driving the need for robust diagnostic models that can jointly analyze complex medical images and accompanying text reports. We present LumbarCLIP, a novel multimodal framework that leverages contrastive language-image pretraining to align lumbar spine MRI scans with corresponding radiological descriptions. Built upon a curated dataset containing axial MRI views paired with expert-written reports, LumbarCLIP integrates vision encoders (ResNet-50, Vision Transformer, Swin Transformer) with a BERT-based text encoder to extract dense representations. These are projected into a shared embedding space via learnable projection heads, configurable as linear or non-linear, and normalized to facilitate stable contrastive training using a soft CLIP loss. Our model achieves state-of-the-art performance on downstream classification, reaching up to 95.00% accuracy and 94.75% F1-score on the test set, despite inherent class imbalance. Extensive ablation studies demonstrate that linear projection heads yield more effective cross-modal alignment than non-linear variants. LumbarCLIP offers a promising foundation for automated musculoskeletal diagnosis and clinical decision support.</li>
</ul>

<h3>Title: Distilling Many-Shot In-Context Learning into a Cheat Sheet</h3>
<ul>
<li><strong>Authors: </strong>Ukyo Honda, Soichiro Murakami, Peinan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20820">https://arxiv.org/abs/2509.20820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20820">https://arxiv.org/pdf/2509.20820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20820]] Distilling Many-Shot In-Context Learning into a Cheat Sheet(https://arxiv.org/abs/2509.20820)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) enable effective in-context learning (ICL) with many-shot examples, but at the cost of high computational demand due to longer input tokens. To address this, we propose cheat-sheet ICL, which distills the information from many-shot ICL into a concise textual summary (cheat sheet) used as the context at inference time. Experiments on challenging reasoning tasks show that cheat-sheet ICL achieves comparable or better performance than many-shot ICL with far fewer tokens, and matches retrieval-based ICL without requiring test-time retrieval. These findings demonstrate that cheat-sheet ICL is a practical alternative for leveraging LLMs in downstream tasks.</li>
</ul>

<h3>Title: T2I-Diff: fMRI Signal Generation via Time-Frequency Image Transform and Classifier-Free Denoising Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hwa Hui Tew, Junn Yong Loo, Yee-Fan Tan, Xinyu Tang, Hernando Ombao, Fuad Noman, Raphael C.-W. Phan, Chee-Ming Ting</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20822">https://arxiv.org/abs/2509.20822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20822">https://arxiv.org/pdf/2509.20822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20822]] T2I-Diff: fMRI Signal Generation via Time-Frequency Image Transform and Classifier-Free Denoising Diffusion Models(https://arxiv.org/abs/2509.20822)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Functional Magnetic Resonance Imaging (fMRI) is an advanced neuroimaging method that enables in-depth analysis of brain activity by measuring dynamic changes in the blood oxygenation level-dependent (BOLD) signals. However, the resource-intensive nature of fMRI data acquisition limits the availability of high-fidelity samples required for data-driven brain analysis models. While modern generative models can synthesize fMRI data, they often underperform because they overlook the complex non-stationarity and nonlinear BOLD dynamics. To address these challenges, we introduce T2I-Diff, an fMRI generation framework that leverages time-frequency representation of BOLD signals and classifier-free denoising diffusion. Specifically, our framework first converts BOLD signals into windowed spectrograms via a time-dependent Fourier transform, capturing both the underlying temporal dynamics and spectral evolution. Subsequently, a classifier-free diffusion model is trained to generate class-conditioned frequency spectrograms, which are then reverted to BOLD signals via inverse Fourier transforms. Finally, we validate the efficacy of our approach by demonstrating improved accuracy and generalization in downstream fMRI-based brain network classification.</li>
</ul>

<h3>Title: Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks</h3>
<ul>
<li><strong>Authors: </strong>Yu Liu, Boxiang He, Fanggang Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20835">https://arxiv.org/abs/2509.20835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20835">https://arxiv.org/pdf/2509.20835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20835]] Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks(https://arxiv.org/abs/2509.20835)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>This paper proposes a novel and flexible security-aware semantic-driven integrated sensing and communication (ISAC) framework, namely security semantic ISAC (SS-ISAC). Inspired by the positive impact of the adversarial attack, a pair of pluggable encryption and decryption modules is designed in the proposed SS-ISAC framework. The encryption module is installed after the semantic transmitter, adopting a trainable adversarial residual network (ARN) to create the adversarial attack. Correspondingly, the decryption module before the semantic receiver utilizes another trainable ARN to mitigate the adversarial attack and noise. These two modules can be flexibly assembled considering the system security demands, without drastically modifying the hardware infrastructure. To ensure the sensing and communication (SAC) performance while preventing the eavesdropping threat, the above ARNs are jointly optimized by minimizing a carefully designed loss function that relates to the adversarial attack power, SAC performance, as well as the privacy leakage risk. Simulation results validate the effectiveness of the proposed SS-ISAC framework in terms of both SAC and eavesdropping prevention performance.</li>
</ul>

<h3>Title: Zero-Shot Privacy-Aware Text Rewriting via Iterative Tree Search</h3>
<ul>
<li><strong>Authors: </strong>Shuo Huang, Xingliang Yuan, Gholamreza Haffari, Lizhen Qu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20838">https://arxiv.org/abs/2509.20838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20838">https://arxiv.org/pdf/2509.20838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20838]] Zero-Shot Privacy-Aware Text Rewriting via Iterative Tree Search(https://arxiv.org/abs/2509.20838)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>The increasing adoption of large language models (LLMs) in cloud-based services has raised significant privacy concerns, as user inputs may inadvertently expose sensitive information. Existing text anonymization and de-identification techniques, such as rule-based redaction and scrubbing, often struggle to balance privacy preservation with text naturalness and utility. In this work, we propose a zero-shot, tree-search-based iterative sentence rewriting algorithm that systematically obfuscates or deletes private information while preserving coherence, relevance, and naturalness. Our method incrementally rewrites privacy-sensitive segments through a structured search guided by a reward model, enabling dynamic exploration of the rewriting space. Experiments on privacy-sensitive datasets show that our approach significantly outperforms existing baselines, achieving a superior balance between privacy protection and utility preservation.</li>
</ul>

<h3>Title: Robust Multi-Omics Integration from Incomplete Modalities Significantly Improves Prediction of Alzheimer's Disease</h3>
<ul>
<li><strong>Authors: </strong>Sungjoon Park, Kyungwook Lee, Soorin Yim, Doyeong Hwang, Dongyun Kim, Soonyoung Lee, Amy Dunn, Daniel Gatti, Elissa Chesler, Kristen O'Connell, Kiyoung Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20842">https://arxiv.org/abs/2509.20842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20842">https://arxiv.org/pdf/2509.20842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20842]] Robust Multi-Omics Integration from Incomplete Modalities Significantly Improves Prediction of Alzheimer's Disease(https://arxiv.org/abs/2509.20842)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-omics data capture complex biomolecular interactions and provide insights into metabolism and disease. However, missing modalities hinder integrative analysis across heterogeneous omics. To address this, we present MOIRA (Multi-Omics Integration with Robustness to Absent modalities), an early integration method enabling robust learning from incomplete omics data via representation alignment and adaptive aggregation. MOIRA leverages all samples, including those with missing modalities, by projecting each omics dataset onto a shared embedding space where a learnable weighting mechanism fuses them. Evaluated on the Religious Order Study and Memory and Aging Project (ROSMAP) dataset for Alzheimer's Disease (AD), MOIRA outperformed existing approaches, and further ablation studies confirmed modality-wise contributions. Feature importance analysis revealed AD-related biomarkers consistent with prior literature, highlighting the biological relevance of our approach.</li>
</ul>

<h3>Title: Causal Time Series Generation via Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yutong Xia, Chang Xu, Yuxuan Liang, Qingsong Wen, Roger Zimmermann, Jiang Bian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20846">https://arxiv.org/abs/2509.20846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20846">https://arxiv.org/pdf/2509.20846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20846]] Causal Time Series Generation via Diffusion Models(https://arxiv.org/abs/2509.20846)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Time series generation (TSG) synthesizes realistic sequences and has achieved remarkable success. Among TSG, conditional models generate sequences given observed covariates, however, such models learn observational correlations without considering unobserved confounding. In this work, we propose a causal perspective on conditional TSG and introduce causal time series generation as a new TSG task family, formalized within Pearl's causal ladder, extending beyond observational generation to include interventional and counterfactual settings. To instantiate these tasks, we develop CaTSG, a unified diffusion-based framework with backdoor-adjusted guidance that causally steers sampling toward desired interventions and individual counterfactuals while preserving observational fidelity. Specifically, our method derives causal score functions via backdoor adjustment and the abduction-action-prediction procedure, thus enabling principled support for all three levels of TSG. Extensive experiments on both synthetic and real-world datasets show that CaTSG achieves superior fidelity and also supporting interventional and counterfactual generation that existing baselines cannot handle. Overall, we propose the causal TSG family and instantiate it with CaTSG, providing an initial proof-of-concept and opening a promising direction toward more reliable simulation under interventions and counterfactual generation.</li>
</ul>

<h3>Title: Poisoning Prompt-Guided Sampling in Video Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Cao, Wei Song, Jingling Xue, Jin Song Dong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20851">https://arxiv.org/abs/2509.20851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20851">https://arxiv.org/pdf/2509.20851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20851]] Poisoning Prompt-Guided Sampling in Video Large Language Models(https://arxiv.org/abs/2509.20851)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Video Large Language Models (VideoLLMs) have emerged as powerful tools for understanding videos, supporting tasks such as summarization, captioning, and question answering. Their performance has been driven by advances in frame sampling, progressing from uniform-based to semantic-similarity-based and, most recently, prompt-guided strategies. While vulnerabilities have been identified in earlier sampling strategies, the safety of prompt-guided sampling remains unexplored. We close this gap by presenting PoisonVID, the first black-box poisoning attack that undermines prompt-guided sampling in VideoLLMs. PoisonVID compromises the underlying prompt-guided sampling mechanism through a closed-loop optimization strategy that iteratively optimizes a universal perturbation to suppress harmful frame relevance scores, guided by a depiction set constructed from paraphrased harmful descriptions leveraging a shadow VideoLLM and a lightweight language model, i.e., GPT-4o-mini. Comprehensively evaluated on three prompt-guided sampling strategies and across three advanced VideoLLMs, PoisonVID achieves 82% - 99% attack success rate, highlighting the importance of developing future advanced sampling strategies for VideoLLMs.</li>
</ul>

<h3>Title: FHRFormer: A Self-supervised Transformer Approach for Fetal Heart Rate Inpainting and Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Kjersti Engan, Neel Kanwal, Anita Yeconia, Ladislaus Blacy, Yuda Munyaw, Estomih Mduma, Hege Ersdal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20852">https://arxiv.org/abs/2509.20852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20852">https://arxiv.org/pdf/2509.20852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20852]] FHRFormer: A Self-supervised Transformer Approach for Fetal Heart Rate Inpainting and Forecasting(https://arxiv.org/abs/2509.20852)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Approximately 10\% of newborns require assistance to initiate breathing at birth, and around 5\% need ventilation support. Fetal heart rate (FHR) monitoring plays a crucial role in assessing fetal well-being during prenatal care, enabling the detection of abnormal patterns and supporting timely obstetric interventions to mitigate fetal risks during labor. Applying artificial intelligence (AI) methods to analyze large datasets of continuous FHR monitoring episodes with diverse outcomes may offer novel insights into predicting the risk of needing breathing assistance or interventions. Recent advances in wearable FHR monitors have enabled continuous fetal monitoring without compromising maternal mobility. However, sensor displacement during maternal movement, as well as changes in fetal or maternal position, often lead to signal dropouts, resulting in gaps in the recorded FHR data. Such missing data limits the extraction of meaningful insights and complicates automated (AI-based) analysis. Traditional approaches to handle missing data, such as simple interpolation techniques, often fail to preserve the spectral characteristics of the signals. In this paper, we propose a masked transformer-based autoencoder approach to reconstruct missing FHR signals by capturing both spatial and frequency components of the data. The proposed method demonstrates robustness across varying durations of missing data and can be used for signal inpainting and forecasting. The proposed approach can be applied retrospectively to research datasets to support the development of AI-based risk algorithms. In the future, the proposed method could be integrated into wearable FHR monitoring devices to achieve earlier and more robust risk detection.</li>
</ul>

<h3>Title: Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer</h3>
<ul>
<li><strong>Authors: </strong>Abdur Rehman, S M A Sharif, Md Abdur Rahaman, Mohamed Jismy Aashik Rasool, Seongwan Kim, Jaeho Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20854">https://arxiv.org/abs/2509.20854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20854">https://arxiv.org/pdf/2509.20854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20854]] Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer(https://arxiv.org/abs/2509.20854)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Quantization-aware training (QAT) combined with knowledge distillation (KD) is a promising strategy for compressing Artificial Intelligence (AI) models for deployment on resource-constrained hardware. However, existing QAT-KD methods often struggle to balance task-specific (TS) and distillation losses due to heterogeneous gradient magnitudes, especially under low-bit quantization. We propose Game of Regularizer (GoR), a novel learnable regularization method that adaptively balances TS and KD objectives using only two trainable parameters for dynamic loss weighting. GoR reduces conflict between supervision signals, improves convergence, and boosts the performance of small quantized models (SQMs). Experiments on image classification, object detection (OD), and large language model (LLM) compression show that GoR consistently outperforms state-of-the-art QAT-KD methods. On low-power edge devices, it delivers faster inference while maintaining full-precision accuracy. We also introduce QAT-EKD-GoR, an ensemble distillation framework that uses multiple heterogeneous teacher models. Under optimal conditions, the proposed EKD-GoR can outperform full-precision models, providing a robust solution for real-world deployment.</li>
</ul>

<h3>Title: Plant identification based on noisy web data: the amazing performance of deep learning (LifeCLEF 2017)</h3>
<ul>
<li><strong>Authors: </strong>Herve Goeau, Pierre Bonnet, Alexis Joly</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20856">https://arxiv.org/abs/2509.20856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20856">https://arxiv.org/pdf/2509.20856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20856]] Plant identification based on noisy web data: the amazing performance of deep learning (LifeCLEF 2017)(https://arxiv.org/abs/2509.20856)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The 2017-th edition of the LifeCLEF plant identification challenge is an important milestone towards automated plant identification systems working at the scale of continental floras with 10.000 plant species living mainly in Europe and North America illustrated by a total of 1.1M images. Nowadays, such ambitious systems are enabled thanks to the conjunction of the dazzling recent progress in image classification with deep learning and several outstanding international initiatives, such as the Encyclopedia of Life (EOL), aggregating the visual knowledge on plant species coming from the main national botany institutes. However, despite all these efforts the majority of the plant species still remain without pictures or are poorly illustrated. Outside the institutional channels, a much larger number of plant pictures are available and spread on the web through botanist blogs, plant lovers web-pages, image hosting websites and on-line plant retailers. The LifeCLEF 2017 plant challenge presented in this paper aimed at evaluating to what extent a large noisy training dataset collected through the web and containing a lot of labelling errors can compete with a smaller but trusted training dataset checked by experts. To fairly compare both training strategies, the test dataset was created from a third data source, i.e. the Pl@ntNet mobile application that collects millions of plant image queries all over the world. This paper presents more precisely the resources and assessments of the challenge, summarizes the approaches and systems employed by the participating research groups, and provides an analysis of the main outcomes.</li>
</ul>

<h3>Title: TasselNetV4: A vision foundation model for cross-scene, cross-scale, and cross-species plant counting</h3>
<ul>
<li><strong>Authors: </strong>Xiaonan Hu, Xuebing Li, Jinyu Xu, Abdulkadir Duran Adan, Letian Zhou, Xuhui Zhu, Yanan Li, Wei Guo, Shouyang Liu, Wenzhong Liu, Hao Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20857">https://arxiv.org/abs/2509.20857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20857">https://arxiv.org/pdf/2509.20857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20857]] TasselNetV4: A vision foundation model for cross-scene, cross-scale, and cross-species plant counting(https://arxiv.org/abs/2509.20857)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Accurate plant counting provides valuable information for agriculture such as crop yield prediction, plant density assessment, and phenotype quantification. Vision-based approaches are currently the mainstream solution. Prior art typically uses a detection or a regression model to count a specific plant. However, plants have biodiversity, and new cultivars are increasingly bred each year. It is almost impossible to exhaust and build all species-dependent counting models. Inspired by class-agnostic counting (CAC) in computer vision, we argue that it is time to rethink the problem formulation of plant counting, from what plants to count to how to count plants. In contrast to most daily objects with spatial and temporal invariance, plants are dynamic, changing with time and space. Their non-rigid structure often leads to worse performance than counting rigid instances like heads and cars such that current CAC and open-world detection models are suboptimal to count plants. In this work, we inherit the vein of the TasselNet plant counting model and introduce a new extension, TasselNetV4, shifting from species-specific counting to cross-species counting. TasselNetV4 marries the local counting idea of TasselNet with the extract-and-match paradigm in CAC. It builds upon a plain vision transformer and incorporates novel multi-branch box-aware local counters used to enhance cross-scale robustness. Two challenging datasets, PAC-105 and PAC-Somalia, are harvested. Extensive experiments against state-of-the-art CAC models show that TasselNetV4 achieves not only superior counting performance but also high this http URL results indicate that TasselNetV4 emerges to be a vision foundation model for cross-scene, cross-scale, and cross-species plant counting.</li>
</ul>

<h3>Title: Concise and Sufficient Sub-Sentence Citations for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Guo Chen, Qiuyuan Li, Qiuxian Li, Hongliang Dai, Xiang Chen, Piji Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20859">https://arxiv.org/abs/2509.20859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20859">https://arxiv.org/pdf/2509.20859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20859]] Concise and Sufficient Sub-Sentence Citations for Retrieval-Augmented Generation(https://arxiv.org/abs/2509.20859)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In retrieval-augmented generation (RAG) question answering systems, generating citations for large language model (LLM) outputs enhances verifiability and helps users identify potential hallucinations. However, we observe two problems in the citations produced by existing attribution methods. First, the citations are typically provided at the sentence or even paragraph level. Long sentences or paragraphs may include a substantial amount of irrelevant content. Second, sentence-level citations may omit information that is essential for verifying the output, forcing users to read the surrounding context. In this paper, we propose generating sub-sentence citations that are both concise and sufficient, thereby reducing the effort required by users to confirm the correctness of the generated output. To this end, we first develop annotation guidelines for such citations and construct a corresponding dataset. Then, we propose an attribution framework for generating citations that adhere to our standards. This framework leverages LLMs to automatically generate fine-tuning data for our task and employs a credit model to filter out low-quality examples. Our experiments on the constructed dataset demonstrate that the propose approach can generate high-quality and more readable citations.</li>
</ul>

<h3>Title: FlowXpert: Context-Aware Flow Embedding for Enhanced Traffic Detection in IoT Network</h3>
<ul>
<li><strong>Authors: </strong>Chao Zha, Haolin Pan, Bing Bai, Jiangxing Wu, Ruyun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20861">https://arxiv.org/abs/2509.20861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20861">https://arxiv.org/pdf/2509.20861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20861]] FlowXpert: Context-Aware Flow Embedding for Enhanced Traffic Detection in IoT Network(https://arxiv.org/abs/2509.20861)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, extraction</a></li>
<li><strong>Abstract: </strong>In the Internet of Things (IoT) environment, continuous interaction among a large number of devices generates complex and dynamic network traffic, which poses significant challenges to rule-based detection approaches. Machine learning (ML)-based traffic detection technology, capable of identifying anomalous patterns and potential threats within this traffic, serves as a critical component in ensuring network security. This study first identifies a significant issue with widely adopted feature extraction tools (e.g., CICMeterFlow): the extensive use of time- and length-related features leads to high sparsity, which adversely affects model convergence. Furthermore, existing traffic detection methods generally lack an embedding mechanism capable of efficiently and comprehensively capturing the semantic characteristics of network traffic. To address these challenges, we propose a novel feature extraction tool that eliminates traditional time and length features in favor of context-aware semantic features related to the source host, thus improving the generalizability of the model. In addition, we design an embedding training framework that integrates the unsupervised DBSCAN clustering algorithm with a contrastive learning strategy to effectively capture fine-grained semantic representations of traffic. Extensive empirical evaluations are conducted on the real-world Mawi data set to validate the proposed method in terms of detection accuracy, robustness, and generalization. Comparative experiments against several state-of-the-art (SOTA) models demonstrate the superior performance of our approach. Furthermore, we confirm its applicability and deployability in real-time scenarios.</li>
</ul>

<h3>Title: WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs</h3>
<ul>
<li><strong>Authors: </strong>Guowei Xu, Wenxin Xu, Jiawang Zhao, Kaisheng Ma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20863">https://arxiv.org/abs/2509.20863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20863">https://arxiv.org/pdf/2509.20863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20863]] WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs(https://arxiv.org/abs/2509.20863)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently shown strong potential in language modeling, offering faster generation compared to traditional autoregressive approaches. However, applying supervised fine-tuning (SFT) to diffusion models remains challenging, as they lack precise probability estimates at each denoising step. While the diffusion mechanism enables the model to reason over entire sequences, it also makes the generation process less predictable and often inconsistent. This highlights the importance of controlling key tokens that guide the direction of generation. To address this issue, we propose WeFT, a weighted SFT method for diffusion language models, where tokens are assigned different weights based on their entropy. Derived from diffusion theory, WeFT delivers substantial gains: training on s1K, s1K-1.1, and 3k samples from open-r1, it achieves relative improvements of 39%, 64%, and 83% over standard SFT on four widely used reasoning benchmarks (Sudoku, Countdown, GSM8K, and MATH-500). The code and models will be made publicly available.</li>
</ul>

<h3>Title: SD-RetinaNet: Topologically Constrained Semi-Supervised Retinal Lesion and Layer Segmentation in OCT</h3>
<ul>
<li><strong>Authors: </strong>Botond Fazekas, Guilherme Aresta, Philipp Seeböck, Julia Mai, Ursula Schmidt-Erfurth, Hrvoje Bogunović</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20864">https://arxiv.org/abs/2509.20864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20864">https://arxiv.org/pdf/2509.20864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20864]] SD-RetinaNet: Topologically Constrained Semi-Supervised Retinal Lesion and Layer Segmentation in OCT(https://arxiv.org/abs/2509.20864)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Optical coherence tomography (OCT) is widely used for diagnosing and monitoring retinal diseases, such as age-related macular degeneration (AMD). The segmentation of biomarkers such as layers and lesions is essential for patient diagnosis and follow-up. Recently, semi-supervised learning has shown promise in improving retinal segmentation performance. However, existing methods often produce anatomically implausible segmentations, fail to effectively model layer-lesion interactions, and lack guarantees on topological correctness. To address these limitations, we propose a novel semi-supervised model that introduces a fully differentiable biomarker topology engine to enforce anatomically correct segmentation of lesions and layers. This enables joint learning with bidirectional influence between layers and lesions, leveraging unlabeled and diverse partially labeled datasets. Our model learns a disentangled representation, separating spatial and style factors. This approach enables more realistic layer segmentations and improves lesion segmentation, while strictly enforcing lesion location in their anatomically plausible positions relative to the segmented layers. We evaluate the proposed model on public and internal datasets of OCT scans and show that it outperforms the current state-of-the-art in both lesion and layer segmentation, while demonstrating the ability to generalize layer segmentation to pathological cases using partially annotated training data. Our results demonstrate the potential of using anatomical constraints in semi-supervised learning for accurate, robust, and trustworthy retinal biomarker segmentation.</li>
</ul>

<h3>Title: Single Answer is Not Enough: On Generating Ranked Lists with Medical Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Pittawat Taveekitworachai, Natpatchara Pongjirapat, Krittaphas Chaisutyakorn, Piyalitt Ittichaiwong, Tossaporn Saengja, Kunat Pipatanakul</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20866">https://arxiv.org/abs/2509.20866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20866">https://arxiv.org/pdf/2509.20866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20866]] Single Answer is Not Enough: On Generating Ranked Lists with Medical Reasoning Models(https://arxiv.org/abs/2509.20866)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a systematic study on enabling medical reasoning models (MRMs) to generate ranked lists of answers for open-ended questions. Clinical decision-making rarely relies on a single answer but instead considers multiple options, reducing the risks of narrow perspectives. Yet current MRMs are typically trained to produce only one answer, even in open-ended settings. We propose an alternative format: ranked lists and investigate two approaches: prompting and fine-tuning. While prompting is a cost-effective way to steer an MRM's response, not all MRMs generalize well across different answer formats: choice, short text, and list answers. Based on our prompting findings, we train and evaluate MRMs using supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). SFT teaches a model to imitate annotated responses, and RFT incentivizes exploration through the responses that maximize a reward. We propose new reward functions targeted at ranked-list answer formats, and conduct ablation studies for RFT. Our results show that while some SFT models generalize to certain answer formats, models trained with RFT are more robust across multiple formats. We also present a case study on a modified MedQA with multiple valid answers, finding that although MRMs might fail to select the benchmark's preferred ground truth, they can recognize valid answers. To the best of our knowledge, this is the first systematic investigation of approaches for enabling MRMs to generate answers as ranked lists. We hope this work provides a first step toward developing alternative answer formats that are beneficial beyond single answers in medical domains.</li>
</ul>

<h3>Title: Federated Markov Imputation: Privacy-Preserving Temporal Imputation in Multi-Centric ICU Environments</h3>
<ul>
<li><strong>Authors: </strong>Christoph Düsing, Philipp Cimiano</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20867">https://arxiv.org/abs/2509.20867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20867">https://arxiv.org/pdf/2509.20867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20867]] Federated Markov Imputation: Privacy-Preserving Temporal Imputation in Multi-Centric ICU Environments(https://arxiv.org/abs/2509.20867)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Missing data is a persistent challenge in federated learning on electronic health records, particularly when institutions collect time-series data at varying temporal granularities. To address this, we propose Federated Markov Imputation (FMI), a privacy-preserving method that enables Intensive Care Units (ICUs) to collaboratively build global transition models for temporal imputation. We evaluate FMI on a real-world sepsis onset prediction task using the MIMIC-IV dataset and show that it outperforms local imputation baselines, especially in scenarios with irregular sampling intervals across ICUs.</li>
</ul>

<h3>Title: StyleBench: Evaluating thinking styles in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Junyu Guo, Shangding Gu, Ming Jin, Costas Spanos, Javad Lavaei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20868">https://arxiv.org/abs/2509.20868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20868">https://arxiv.org/pdf/2509.20868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20868]] StyleBench: Evaluating thinking styles in Large Language Models(https://arxiv.org/abs/2509.20868)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in this https URL.</li>
</ul>

<h3>Title: Model-Based Reinforcement Learning under Random Observation Delays</h3>
<ul>
<li><strong>Authors: </strong>Armin Karamzade, Kyungmin Kim, JB Lanier, Davide Corsi, Roy Fox</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20869">https://arxiv.org/abs/2509.20869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20869">https://arxiv.org/pdf/2509.20869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20869]] Model-Based Reinforcement Learning under Random Observation Delays(https://arxiv.org/abs/2509.20869)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Delays frequently occur in real-world environments, yet standard reinforcement learning (RL) algorithms often assume instantaneous perception of the environment. We study random sensor delays in POMDPs, where observations may arrive out-of-sequence, a setting that has not been previously addressed in RL. We analyze the structure of such delays and demonstrate that naive approaches, such as stacking past observations, are insufficient for reliable performance. To address this, we propose a model-based filtering process that sequentially updates the belief state based on an incoming stream of observations. We then introduce a simple delay-aware framework that incorporates this idea into model-based RL, enabling agents to effectively handle random delays. Applying this framework to Dreamer, we compare our approach to delay-aware baselines developed for MDPs. Our method consistently outperforms these baselines and demonstrates robustness to delay distribution shifts during deployment. Additionally, we present experiments on simulated robotic tasks, comparing our method to common practical heuristics and emphasizing the importance of explicitly modeling observation delays.</li>
</ul>

<h3>Title: Plant identification in an open-world (LifeCLEF 2016)</h3>
<ul>
<li><strong>Authors: </strong>Herve Goeau, Pierre Bonnet, Alexis Joly</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20870">https://arxiv.org/abs/2509.20870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20870">https://arxiv.org/pdf/2509.20870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20870]] Plant identification in an open-world (LifeCLEF 2016)(https://arxiv.org/abs/2509.20870)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The LifeCLEF plant identification challenge aims at evaluating plant identification methods and systems at a very large scale, close to the conditions of a real-world biodiversity monitoring scenario. The 2016-th edition was actually conducted on a set of more than 110K images illustrating 1000 plant species living in West Europe, built through a large-scale participatory sensing platform initiated in 2011 and which now involves tens of thousands of contributors. The main novelty over the previous years is that the identification task was evaluated as an open-set recognition problem, i.e. a problem in which the recognition system has to be robust to unknown and never seen categories. Beyond the brute-force classification across the known classes of the training set, the big challenge was thus to automatically reject the false positive classification hits that are caused by the unknown classes. This overview presents more precisely the resources and assessments of the challenge, summarizes the approaches and systems employed by the participating research groups, and provides an analysis of the main outcomes.</li>
</ul>

<h3>Title: SCRA-VQA: Summarized Caption-Rerank for Augmented Large Language Models in Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Yan Zhang, Jiaqing Lin, Miao Zhang, Kui Xiao, Xiaoju Hou, Yue Zhao, Zhifei Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20871">https://arxiv.org/abs/2509.20871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20871">https://arxiv.org/pdf/2509.20871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20871]] SCRA-VQA: Summarized Caption-Rerank for Augmented Large Language Models in Visual Question Answering(https://arxiv.org/abs/2509.20871)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Acquiring high-quality knowledge is a central focus in Knowledge-Based Visual Question Answering (KB-VQA). Recent methods use large language models (LLMs) as knowledge engines for answering. These methods generally employ image captions as visual text descriptions to assist LLMs in interpreting images. However, the captions frequently include excessive noise irrelevant to the question, and LLMs generally do not comprehend VQA tasks, limiting their reasoning capabilities. To address this issue, we propose the Summarized Caption-Rerank Augmented VQA (SCRA-VQA), which employs a pre-trained visual language model to convert images into captions. Moreover, SCRA-VQA generates contextual examples for the captions while simultaneously summarizing and reordering them to exclude unrelated information. The caption-rerank process enables LLMs to understand the image information and questions better, thus enhancing the model's reasoning ability and task adaptability without expensive end-to-end training. Based on an LLM with 6.7B parameters, SCRA-VQA performs excellently on two challenging knowledge-based VQA datasets: OK-VQA and A-OKVQA, achieving accuracies of 38.8% and 34.6%. Our code is available at this https URL.</li>
</ul>

<h3>Title: Distribution-Controlled Client Selection to Improve Federated Learning Strategies</h3>
<ul>
<li><strong>Authors: </strong>Christoph Düsing, Philipp Cimiano</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20877">https://arxiv.org/abs/2509.20877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20877">https://arxiv.org/pdf/2509.20877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20877]] Distribution-Controlled Client Selection to Improve Federated Learning Strategies(https://arxiv.org/abs/2509.20877)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a distributed learning paradigm that allows multiple clients to jointly train a shared model while maintaining data privacy. Despite its great potential for domains with strict data privacy requirements, the presence of data imbalance among clients is a thread to the success of FL, as it causes the performance of the shared model to decrease. To address this, various studies have proposed enhancements to existing FL strategies, particularly through client selection methods that mitigate the detrimental effects of data imbalance. In this paper, we propose an extension to existing FL strategies, which selects active clients that best align the current label distribution with one of two target distributions, namely a balanced distribution or the federations combined label distribution. Subsequently, we empirically verify the improvements through our distribution-controlled client selection on three common FL strategies and two datasets. Our results show that while aligning the label distribution with a balanced distribution yields the greatest improvements facing local imbalance, alignment with the federation's combined label distribution is superior for global imbalance.</li>
</ul>

<h3>Title: The Unanticipated Asymmetry Between Perceptual Optimization and Assessment</h3>
<ul>
<li><strong>Authors: </strong>Jiabei Zhang, Qi Wang, Siyu Wu, Du Chen, Tianhe Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20878">https://arxiv.org/abs/2509.20878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20878">https://arxiv.org/pdf/2509.20878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20878]] The Unanticipated Asymmetry Between Perceptual Optimization and Assessment(https://arxiv.org/abs/2509.20878)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Perceptual optimization is primarily driven by the fidelity objective, which enforces both semantic consistency and overall visual realism, while the adversarial objective provides complementary refinement by enhancing perceptual sharpness and fine-grained detail. Despite their central role, the correlation between their effectiveness as optimization objectives and their capability as image quality assessment (IQA) metrics remains underexplored. In this work, we conduct a systematic analysis and reveal an unanticipated asymmetry between perceptual optimization and assessment: fidelity metrics that excel in IQA are not necessarily effective for perceptual optimization, with this misalignment emerging more distinctly under adversarial training. In addition, while discriminators effectively suppress artifacts during optimization, their learned representations offer only limited benefits when reused as backbone initializations for IQA models. Beyond this asymmetry, our findings further demonstrate that discriminator design plays a decisive role in shaping optimization, with patch-level and convolutional architectures providing more faithful detail reconstruction than vanilla or Transformer-based alternatives. These insights advance the understanding of loss function design and its connection to IQA transferability, paving the way for more principled approaches to perceptual optimization.</li>
</ul>

<h3>Title: A Generalized $χ_n$-Function</h3>
<ul>
<li><strong>Authors: </strong>Cheng Lyu, Mu Yuan, Dabin Zheng, Siwei Sun, Shun Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20880">https://arxiv.org/abs/2509.20880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20880">https://arxiv.org/pdf/2509.20880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20880]] A Generalized $χ_n$-Function(https://arxiv.org/abs/2509.20880)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The mapping $\chi_n$ from $\F_{2}^{n}$ to itself defined by $y=\chi_n(x)$ with $y_i=x_i+x_{i+2}(1+x_{i+1})$, where the indices are computed modulo $n$, has been widely studied for its applications in lightweight cryptography. However, $\chi_n $ is bijective on $\F_2^n$ only when $n$ is odd, restricting its use to odd-dimensional vector spaces over $\F_2$. To address this limitation, we introduce and analyze the generalized mapping $\chi_{n, m}$ defined by $y=\chi_{n,m}(x)$ with $y_i=x_i+x_{i+m} (x_{i+m-1}+1)(x_{i+m-2}+1) \cdots (x_{i+1}+1)$, where $m$ is a fixed integer with $m\nmid n$. To investigate such mappings, we further generalize $\chi_{n,m}$ to $\theta_{m, k}$, where $\theta_{m, k}$ is given by $y_i=x_{i+mk} \prod_{\substack{j=1,\,\, m \nmid j}}^{mk-1} \left(x_{i+j}+1\right), \,\,{\rm for }\,\, i\in \{0,1,\ldots,n-1\}$. We prove that these mappings generate an abelian group isomorphic to the group of units in $\F_2[z]/(z^{\lfloor n/m\rfloor +1})$. This structural insight enables us to construct a broad class of permutations over $\F_2^n$ for any positive integer $n$, along with their inverses. We rigorously analyze algebraic properties of these mappings, including their iterations, fixed points, and cycle structures. Additionally, we provide a comprehensive database of the cryptographic properties for iterates of $\chi_{n,m}$ for small values of $n$ and $m$. Finally, we conduct a comparative security and implementation cost analysis among $\chi_{n,m}$, $\chi_n$, $\chi\chi_n$ (EUROCRYPT 2025 \cite{belkheyar2025chi}) and their variants, and prove Conjecture~1 proposed in~\cite{belkheyar2025chi} as a by-product of our study. Our results lead to generalizations of $\chi_n$, providing alternatives to $\chi_n$ and $\chi\chi_n$.</li>
</ul>

<h3>Title: Integrating Object Interaction Self-Attention and GAN-Based Debiasing for Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Zhifei Li, Feng Qiu, Yiran Wang, Yujing Xia, Kui Xiao, Miao Zhang, Yan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20884">https://arxiv.org/abs/2509.20884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20884">https://arxiv.org/pdf/2509.20884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20884]] Integrating Object Interaction Self-Attention and GAN-Based Debiasing for Visual Question Answering(https://arxiv.org/abs/2509.20884)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual Question Answering (VQA) presents a unique challenge by requiring models to understand and reason about visual content to answer questions accurately. Existing VQA models often struggle with biases introduced by the training data, leading to over-reliance on superficial patterns and inadequate generalization to diverse questions and images. This paper presents a novel model, IOG-VQA, which integrates Object Interaction Self-Attention and GAN-Based Debiasing to enhance VQA model performance. The self-attention mechanism allows our model to capture complex interactions between objects within an image, providing a more comprehensive understanding of the visual context. Meanwhile, the GAN-based debiasing framework generates unbiased data distributions, helping the model to learn more robust and generalizable features. By leveraging these two components, IOG-VQA effectively combines visual and textual information to address the inherent biases in VQA datasets. Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate that our model shows excellent performance compared with the existing methods, particularly in handling biased and imbalanced data distributions highlighting the importance of addressing both object interactions and dataset biases in advancing VQA tasks. Our code is available at this https URL.</li>
</ul>

<h3>Title: Improving Early Sepsis Onset Prediction Through Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Christoph Düsing, Philipp Cimiano</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20885">https://arxiv.org/abs/2509.20885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20885">https://arxiv.org/pdf/2509.20885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20885]] Improving Early Sepsis Onset Prediction Through Federated Learning(https://arxiv.org/abs/2509.20885)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Early and accurate prediction of sepsis onset remains a major challenge in intensive care, where timely detection and subsequent intervention can significantly improve patient outcomes. While machine learning models have shown promise in this domain, their success is often limited by the amount and diversity of training data available to individual hospitals and Intensive Care Units (ICUs). Federated Learning (FL) addresses this issue by enabling collaborative model training across institutions without requiring data sharing, thus preserving patient privacy. In this work, we propose a federated, attention-enhanced Long Short-Term Memory model for sepsis onset prediction, trained on multi-centric ICU data. Unlike existing approaches that rely on fixed prediction windows, our model supports variable prediction horizons, enabling both short- and long-term forecasting in a single unified model. During analysis, we put particular emphasis on the improvements through our approach in terms of early sepsis detection, i.e., predictions with large prediction windows by conducting an in-depth temporal analysis. Our results prove that using FL does not merely improve overall prediction performance (with performance approaching that of a centralized model), but is particularly beneficial for early sepsis onset prediction. Finally, we show that our choice of employing a variable prediction window rather than a fixed window does not hurt performance significantly but reduces computational, communicational, and organizational overhead.</li>
</ul>

<h3>Title: Nuclear Diffusion Models for Low-Rank Background Suppression in Videos</h3>
<ul>
<li><strong>Authors: </strong>Tristan S.W. Stevens, Oisín Nolan, Jean-Luc Robert, Ruud J.G. van Sloun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20886">https://arxiv.org/abs/2509.20886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20886">https://arxiv.org/pdf/2509.20886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20886]] Nuclear Diffusion Models for Low-Rank Background Suppression in Videos(https://arxiv.org/abs/2509.20886)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Video sequences often contain structured noise and background artifacts that obscure dynamic content, posing challenges for accurate analysis and restoration. Robust principal component methods address this by decomposing data into low-rank and sparse components. Still, the sparsity assumption often fails to capture the rich variability present in real video data. To overcome this limitation, a hybrid framework that integrates low-rank temporal modeling with diffusion posterior sampling is proposed. The proposed method, Nuclear Diffusion, is evaluated on a real-world medical imaging problem, namely cardiac ultrasound dehazing, and demonstrates improved dehazing performance compared to traditional RPCA concerning contrast enhancement (gCNR) and signal preservation (KS statistic). These results highlight the potential of combining model-based temporal models with deep generative priors for high-fidelity video restoration.</li>
</ul>

<h3>Title: FerretNet: Efficient Synthetic Image Detection via Local Pixel Dependencies</h3>
<ul>
<li><strong>Authors: </strong>Shuqiao Liang, Jian Liu, Renzhang Chen, Quanlong Guan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20890">https://arxiv.org/abs/2509.20890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20890">https://arxiv.org/pdf/2509.20890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20890]] FerretNet: Efficient Synthetic Image Detection via Local Pixel Dependencies(https://arxiv.org/abs/2509.20890)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>The increasing realism of synthetic images generated by advanced models such as VAEs, GANs, and LDMs poses significant challenges for synthetic image detection. To address this issue, we explore two artifact types introduced during the generation process: (1) latent distribution deviations and (2) decoding-induced smoothing effects, which manifest as inconsistencies in local textures, edges, and color transitions. Leveraging local pixel dependencies (LPD) properties rooted in Markov Random Fields, we reconstruct synthetic images using neighboring pixel information to expose disruptions in texture continuity and edge coherence. Building upon LPD, we propose FerretNet, a lightweight neural network with only 1.1M parameters that delivers efficient and robust synthetic image detection. Extensive experiments demonstrate that FerretNet, trained exclusively on the 4-class ProGAN dataset, achieves an average accuracy of 97.1% on an open-world benchmark comprising across 22 generative models, surpassing state-of-the-art methods by 10.6%.</li>
</ul>

<h3>Title: Deterministic Discrete Denoising</h3>
<ul>
<li><strong>Authors: </strong>Hideyuki Suzuki, Hiroshi Yamashita</a></li>
<li><strong>Subjects: </strong>cs.LG, nlin.CD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20896">https://arxiv.org/abs/2509.20896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20896">https://arxiv.org/pdf/2509.20896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20896]] Deterministic Discrete Denoising(https://arxiv.org/abs/2509.20896)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We propose a deterministic denoising algorithm for discrete-state diffusion models based on Markov chains. The generative reverse process is derandomized by introducing a variant of the herding algorithm with weakly chaotic dynamics, which induces deterministic discrete state transitions. Our approach is a direct replacement for the stochastic denoising process, requiring neither retraining nor continuous state embeddings. We demonstrate consistent improvements in both efficiency and sample quality on text and image generation tasks. Thus, this simple derandomization approach is expected to enhance the significance of discrete diffusion in generative modeling. Furthermore, our results reveal that deterministic reverse processes, well established in continuous diffusion, can also be effective in discrete state spaces.</li>
</ul>

<h3>Title: Concepts in Motion: Temporal Bottlenecks for Interpretable Video Classification</h3>
<ul>
<li><strong>Authors: </strong>Patrick Knab, Sascha Marton, Philipp J. Schubert, Drago Guggiana, Christian Bartelt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20899">https://arxiv.org/abs/2509.20899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20899">https://arxiv.org/pdf/2509.20899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20899]] Concepts in Motion: Temporal Bottlenecks for Interpretable Video Classification(https://arxiv.org/abs/2509.20899)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Conceptual models such as Concept Bottleneck Models (CBMs) have driven substantial progress in improving interpretability for image classification by leveraging human-interpretable concepts. However, extending these models from static images to sequences of images, such as video data, introduces a significant challenge due to the temporal dependencies inherent in videos, which are essential for capturing actions and events. In this work, we introduce MoTIF (Moving Temporal Interpretable Framework), an architectural design inspired by a transformer that adapts the concept bottleneck framework for video classification and handles sequences of arbitrary length. Within the video domain, concepts refer to semantic entities such as objects, attributes, or higher-level components (e.g., 'bow', 'mount', 'shoot') that reoccur across time - forming motifs collectively describing and explaining actions. Our design explicitly enables three complementary perspectives: global concept importance across the entire video, local concept relevance within specific windows, and temporal dependencies of a concept over time. Our results demonstrate that the concept-based modeling paradigm can be effectively transferred to video data, enabling a better understanding of concept contributions in temporal contexts while maintaining competitive performance. Code available at this http URL.</li>
</ul>

<h3>Title: Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization</h3>
<ul>
<li><strong>Authors: </strong>Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20900">https://arxiv.org/abs/2509.20900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20900">https://arxiv.org/pdf/2509.20900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20900]] Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization(https://arxiv.org/abs/2509.20900)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long document summarization remains a significant challenge for current large language models (LLMs), as existing approaches commonly struggle with information loss, factual inconsistencies, and coherence issues when processing excessively long documents. We propose SummQ, a novel adversarial multi-agent framework that addresses these limitations through collaborative intelligence between specialized agents operating in two complementary domains: summarization and quizzing. Our approach employs summary generators and reviewers that work collaboratively to create and evaluate comprehensive summaries, while quiz generators and reviewers create comprehension questions that serve as continuous quality checks for the summarization process. This adversarial dynamic, enhanced by an examinee agent that validates whether the generated summary contains the information needed to answer the quiz questions, enables iterative refinement through multifaceted feedback mechanisms. We evaluate SummQ on three widely used long document summarization benchmarks. Experimental results demonstrate that our framework significantly outperforms existing state-of-the-art methods across ROUGE and BERTScore metrics, as well as in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal the effectiveness of the multi-agent collaboration dynamics, the influence of different agent configurations, and the impact of the quizzing mechanism. This work establishes a new approach for long document summarization that uses adversarial agentic collaboration to improve summarization quality.</li>
</ul>

<h3>Title: FSMODNet: A Closer Look at Few-Shot Detection in Multispectral Data</h3>
<ul>
<li><strong>Authors: </strong>Manuel Nkegoum, Minh-Tan Pham, Élisa Fromont, Bruno Avignon, Sébastien Lefèvre</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20905">https://arxiv.org/abs/2509.20905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20905">https://arxiv.org/pdf/2509.20905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20905]] FSMODNet: A Closer Look at Few-Shot Detection in Multispectral Data(https://arxiv.org/abs/2509.20905)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Few-shot multispectral object detection (FSMOD) addresses the challenge of detecting objects across visible and thermal modalities with minimal annotated data. In this paper, we explore this complex task and introduce a framework named "FSMODNet" that leverages cross-modality feature integration to improve detection performance even with limited labels. By effectively combining the unique strengths of visible and thermal imagery using deformable attention, the proposed method demonstrates robust adaptability in complex illumination and environmental conditions. Experimental results on two public datasets show effective object detection performance in challenging low-data regimes, outperforming several baselines we established from state-of-the-art models. All code, models, and experimental data splits can be found at this https URL.</li>
</ul>

<h3>Title: Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences</h3>
<ul>
<li><strong>Authors: </strong>Julius Pesonen, Arno Solin, Eija Honkavaara</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20906">https://arxiv.org/abs/2509.20906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20906">https://arxiv.org/pdf/2509.20906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20906]] Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences(https://arxiv.org/abs/2509.20906)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>3D object localisation based on a sequence of camera measurements is essential for safety-critical surveillance tasks, such as drone-based wildfire monitoring. Localisation of objects detected with a camera can typically be solved with dense depth estimation or 3D scene reconstruction. However, in the context of distant objects or tasks limited by the amount of available computational resources, neither solution is feasible. In this paper, we show that the task can be solved using particle filters for both single and multiple target scenarios. The method was studied using a 3D simulation and a drone-based image segmentation sequence with global navigation satellite system (GNSS)-based camera pose estimates. The results showed that a particle filter can be used to solve practical localisation tasks based on camera poses and image segments in these situations where other solutions fail. The particle filter is independent of the detection method, making it flexible for new tasks. The study also demonstrates that drone-based wildfire monitoring can be conducted using the proposed method paired with a pre-existing image segmentation model.</li>
</ul>

<h3>Title: MemLens: Uncovering Memorization in LLMs with Activation Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Zirui He, Haiyan Zhao, Ali Payani, Mengnan du</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20909">https://arxiv.org/abs/2509.20909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20909">https://arxiv.org/pdf/2509.20909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20909]] MemLens: Uncovering Memorization in LLMs with Activation Trajectories(https://arxiv.org/abs/2509.20909)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are commonly evaluated on challenging benchmarks such as AIME and Math500, which are susceptible to contamination and risk of being memorized. Existing detection methods, which primarily rely on surface-level lexical overlap and perplexity, demonstrate low generalization and degrade significantly when encountering implicitly contaminated data. In this paper, we propose MemLens (An Activation Lens for Memorization Detection) to detect memorization by analyzing the probability trajectories of numeric tokens during generation. Our method reveals that contaminated samples exhibit ``shortcut'' behaviors, locking onto an answer with high confidence in the model's early layers, whereas clean samples show more gradual evidence accumulation across the model's full depth. We observe that contaminated and clean samples exhibit distinct and well-separated reasoning trajectories. To further validate this, we inject carefully designed samples into the model through LoRA fine-tuning and observe the same trajectory patterns as in naturally contaminated data. These results provide strong evidence that MemLens captures genuine signals of memorization rather than spurious correlations.</li>
</ul>

<h3>Title: SwinMamba: A hybrid local-global mamba framework for enhancing semantic segmentation of remotely sensed images</h3>
<ul>
<li><strong>Authors: </strong>Qinfeng Zhu, Han Li, Liang He, Lei Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20918">https://arxiv.org/abs/2509.20918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20918">https://arxiv.org/pdf/2509.20918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20918]] SwinMamba: A hybrid local-global mamba framework for enhancing semantic segmentation of remotely sensed images(https://arxiv.org/abs/2509.20918)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation of remote sensing imagery is a fundamental task in computer vision, supporting a wide range of applications such as land use classification, urban planning, and environmental monitoring. However, this task is often challenged by the high spatial resolution, complex scene structures, and diverse object scales present in remote sensing data. To address these challenges, various deep learning architectures have been proposed, including convolutional neural networks, Vision Transformers, and the recently introduced Vision Mamba. Vision Mamba features a global receptive field and low computational complexity, demonstrating both efficiency and effectiveness in image segmentation. However, its reliance on global scanning tends to overlook critical local features, such as textures and edges, which are essential for achieving accurate segmentation in remote sensing contexts. To tackle this limitation, we propose SwinMamba, a novel framework inspired by the Swin Transformer. SwinMamba integrates localized Mamba-style scanning within shifted windows with a global receptive field, to enhance the model's perception of both local and global features. Specifically, the first two stages of SwinMamba perform local scanning to capture fine-grained details, while its subsequent two stages leverage global scanning to fuse broader contextual information. In our model, the use of overlapping shifted windows enhances inter-region information exchange, facilitating more robust feature integration across the entire image. Extensive experiments on the LoveDA and ISPRS Potsdam datasets demonstrate that SwinMamba outperforms state-of-the-art methods, underscoring its effectiveness and potential as a superior solution for semantic segmentation of remotely sensed imagery.</li>
</ul>

<h3>Title: RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks</h3>
<ul>
<li><strong>Authors: </strong>Hanbo Huang, Yiran Zhang, Hao Zheng, Xuan Gong, Yihan Li, Lin Liu, Shiyu Liang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20924">https://arxiv.org/abs/2509.20924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20924">https://arxiv.org/pdf/2509.20924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20924]] RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks(https://arxiv.org/abs/2509.20924)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) watermarking has shown promise in detecting AI-generated content and mitigating misuse, with prior work claiming robustness against paraphrasing and text editing. In this paper, we argue that existing evaluations are not sufficiently adversarial, obscuring critical vulnerabilities and overstating the security. To address this, we introduce adaptive robustness radius, a formal metric that quantifies watermark resilience against adaptive adversaries. We theoretically prove that optimizing the attack context and model parameters can substantially reduce this radius, making watermarks highly susceptible to paraphrase attacks. Leveraging this insight, we propose RLCracker, a reinforcement learning (RL)-based adaptive attack that erases watermarks while preserving semantic fidelity. RLCracker requires only limited watermarked examples and zero access to the detector. Despite weak supervision, it empowers a 3B model to achieve 98.5% removal success and an average 0.92 P-SP score on 1,500-token Unigram-marked texts after training on only 100 short samples. This performance dramatically exceeds 6.75% by GPT-4o and generalizes across five model sizes over ten watermarking schemes. Our results confirm that adaptive attacks are broadly effective and pose a fundamental threat to current watermarking defenses.</li>
</ul>

<h3>Title: SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Akihisa Watanabe, Jiawei Ren, Li Siyao, Yichen Peng, Erwin Wu, Edgar Simo-Serra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20927">https://arxiv.org/abs/2509.20927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20927">https://arxiv.org/pdf/2509.20927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20927]] SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation(https://arxiv.org/abs/2509.20927)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating physically plausible human motion is crucial for applications such as character animation and virtual reality. Existing approaches often incorporate a simulator-based motion projection layer to the diffusion process to enforce physical plausibility. However, such methods are computationally expensive due to the sequential nature of the simulator, which prevents parallelization. We show that simulator-based motion projection can be interpreted as a form of guidance, either classifier-based or classifier-free, within the diffusion process. Building on this insight, we propose SimDiff, a Simulator-constrained Diffusion Model that integrates environment parameters (e.g., gravity, wind) directly into the denoising process. By conditioning on these parameters, SimDiff generates physically plausible motions efficiently, without repeated simulator calls at inference, and also provides fine-grained control over different physical coefficients. Moreover, SimDiff successfully generalizes to unseen combinations of environmental parameters, demonstrating compositional generalization.</li>
</ul>

<h3>Title: GenFacts-Generative Counterfactual Explanations for Multi-Variate Time Series</h3>
<ul>
<li><strong>Authors: </strong>Sarah Seifi, Anass Ibrahimi, Tobias Sukianto, Cecilia Carbonelli, Lorenzo Servadei, Robert Wille</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20936">https://arxiv.org/abs/2509.20936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20936">https://arxiv.org/pdf/2509.20936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20936]] GenFacts-Generative Counterfactual Explanations for Multi-Variate Time Series(https://arxiv.org/abs/2509.20936)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Counterfactual explanations aim to enhance model transparency by showing how inputs can be minimally altered to change predictions. For multivariate time series, existing methods often generate counterfactuals that are invalid, implausible, or unintuitive. We introduce GenFacts, a generative framework based on a class-discriminative variational autoencoder. It integrates contrastive and classification-consistency objectives, prototype-based initialization, and realism-constrained optimization. We evaluate GenFacts on radar gesture data as an industrial use case and handwritten letter trajectories as an intuitive benchmark. Across both datasets, GenFacts outperforms state-of-the-art baselines in plausibility (+18.7%) and achieves the highest interpretability scores in a human study. These results highlight that plausibility and user-centered interpretability, rather than sparsity alone, are key to actionable counterfactuals in time series data.</li>
</ul>

<h3>Title: Unlocking Noise-Resistant Vision: Key Architectural Secrets for Robust Models</h3>
<ul>
<li><strong>Authors: </strong>Bum Jun Kim, Makoto Kawano, Yusuke Iwasawa, Yutaka Matsuo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20939">https://arxiv.org/abs/2509.20939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20939">https://arxiv.org/pdf/2509.20939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20939]] Unlocking Noise-Resistant Vision: Key Architectural Secrets for Robust Models(https://arxiv.org/abs/2509.20939)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>While the robustness of vision models is often measured, their dependence on specific architectural design choices is rarely dissected. We investigate why certain vision architectures are inherently more robust to additive Gaussian noise and convert these empirical insights into simple, actionable design rules. Specifically, we performed extensive evaluations on 1,174 pretrained vision models, empirically identifying four consistent design patterns for improved robustness against Gaussian noise: larger stem kernels, smaller input resolutions, average pooling, and supervised vision transformers (ViTs) rather than CLIP ViTs, which yield up to 506 rank improvements and 21.6\%p accuracy gains. We then develop a theoretical analysis that explains these findings, converting observed correlations into causal mechanisms. First, we prove that low-pass stem kernels attenuate noise with a gain that decreases quadratically with kernel size and that anti-aliased downsampling reduces noise energy roughly in proportion to the square of the downsampling factor. Second, we demonstrate that average pooling is unbiased and suppresses noise in proportion to the pooling window area, whereas max pooling incurs a positive bias that grows slowly with window size and yields a relatively higher mean-squared error and greater worst-case sensitivity. Third, we reveal and explain the vulnerability of CLIP ViTs via a pixel-space Lipschitz bound: The smaller normalization standard deviations used in CLIP preprocessing amplify worst-case sensitivity by up to 1.91 times relative to the Inception-style preprocessing common in supervised ViTs. Our results collectively disentangle robustness into interpretable modules, provide a theory that explains the observed trends, and build practical, plug-and-play guidelines for designing vision models more robust against Gaussian noise.</li>
</ul>

<h3>Title: Decoding the Surgical Scene: A Scoping Review of Scene Graphs in Surgery</h3>
<ul>
<li><strong>Authors: </strong>Angelo Henriques, Korab Hoxha, Daniel Zapp, Peter C. Issa, Nassir Navab, M. Ali Nasseri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20941">https://arxiv.org/abs/2509.20941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20941">https://arxiv.org/pdf/2509.20941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20941]] Decoding the Surgical Scene: A Scoping Review of Scene Graphs in Surgery(https://arxiv.org/abs/2509.20941)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Scene graphs (SGs) provide structured relational representations crucial for decoding complex, dynamic surgical environments. This PRISMA-ScR-guided scoping review systematically maps the evolving landscape of SG research in surgery, charting its applications, methodological advancements, and future directions. Our analysis reveals rapid growth, yet uncovers a critical 'data divide': internal-view research (e.g., triplet recognition) almost exclusively uses real-world 2D video, while external-view 4D modeling relies heavily on simulated data, exposing a key translational research gap. Methodologically, the field has advanced from foundational graph neural networks to specialized foundation models that now significantly outperform generalist large vision-language models in surgical contexts. This progress has established SGs as a cornerstone technology for both analysis, such as workflow recognition and automated safety monitoring, and generative tasks like controllable surgical simulation. Although challenges in data annotation and real-time implementation persist, they are actively being addressed through emerging techniques. Surgical SGs are maturing into an essential semantic bridge, enabling a new generation of intelligent systems to improve surgical safety, efficiency, and training.</li>
</ul>

<h3>Title: Why Attention Fails: The Degeneration of Transformers into MLPs in Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Zida Liang, Jiayi Zhu, Weiqiang Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20942">https://arxiv.org/abs/2509.20942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20942">https://arxiv.org/pdf/2509.20942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20942]] Why Attention Fails: The Degeneration of Transformers into MLPs in Time Series Forecasting(https://arxiv.org/abs/2509.20942)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based architectures achieved high performance in natural language processing and computer vision, yet many studies have shown that they have not demonstrated a clear advantage in time series forecasting and even underperform simple linear baselines in some cases. However, most of these studies have not thoroughly explored the reasons behind the failure of transformers. To better understand time-series transformers(TST), we designed a series of experiments, progressively modifying transformers into MLPs to investigate the impact of the attention mechanism. Surprisingly, transformer blocks often degenerate into simple MLPs in existing time-series transformers. We designed a interpretable dataset to investigate the reasons behind the failure of the attention mechanism and revealed that the attention mechanism is not working in the expected way. We theoretically analyzed the reasons behind this phenomenon, demonstrating that the current embedding methods fail to allow transformers to function in a well-structured latent space, and further analyzed the deeper underlying causes of the failure of embedding.</li>
</ul>

<h3>Title: CTI Dataset Construction from Telegram</h3>
<ul>
<li><strong>Authors: </strong>Dincy R. Arikkat, Sneha B. T., Serena Nicolazzo, Antonino Nocera, Vinod P., Rafidha Rehiman K. A., Karthika R</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20943">https://arxiv.org/abs/2509.20943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20943">https://arxiv.org/pdf/2509.20943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20943]] CTI Dataset Construction from Telegram(https://arxiv.org/abs/2509.20943)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Cyber Threat Intelligence (CTI) enables organizations to anticipate, detect, and mitigate evolving cyber threats. Its effectiveness depends on high-quality datasets, which support model development, training, evaluation, and benchmarking. Building such datasets is crucial, as attack vectors and adversary tactics continually evolve. Recently, Telegram has gained prominence as a valuable CTI source, offering timely and diverse threat-related information that can help address these challenges. In this work, we address these challenges by presenting an end-to-end automated pipeline that systematically collects and filters threat-related content from Telegram. The pipeline identifies relevant Telegram channels and scrapes 145,349 messages from 12 curated channels out of 150 identified sources. To accurately filter threat intelligence messages from generic content, we employ a BERT-based classifier, achieving an accuracy of 96.64%. From the filtered messages, we compile a dataset of 86,509 malicious Indicators of Compromise, including domains, IPs, URLs, hashes, and CVEs. This approach not only produces a large-scale, high-fidelity CTI dataset but also establishes a foundation for future research and operational applications in cyber threat detection.</li>
</ul>

<h3>Title: A Real-Time On-Device Defect Detection Framework for Laser Power-Meter Sensors via Unsupervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Dongqi Zheng, Wenjin Fu, Guangzong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20946">https://arxiv.org/abs/2509.20946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20946">https://arxiv.org/pdf/2509.20946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20946]] A Real-Time On-Device Defect Detection Framework for Laser Power-Meter Sensors via Unsupervised Learning(https://arxiv.org/abs/2509.20946)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>We present an automated vision-based system for defect detection and classification of laser power meter sensor coatings. Our approach addresses the critical challenge of identifying coating defects such as thermal damage and scratches that can compromise laser energy measurement accuracy in medical and industrial applications. The system employs an unsupervised anomaly detection framework that trains exclusively on ``good'' sensor images to learn normal coating distribution patterns, enabling detection of both known and novel defect types without requiring extensive labeled defect datasets. Our methodology consists of three key components: (1) a robust preprocessing pipeline using Laplacian edge detection and K-means clustering to segment the area of interest, (2) synthetic data augmentation via StyleGAN2, and (3) a UFlow-based neural network architecture for multi-scale feature extraction and anomaly map generation. Experimental evaluation on 366 real sensor images demonstrates $93.8\%$ accuracy on defective samples and $89.3\%$ accuracy on good samples, with image-level AUROC of 0.957 and pixel-level AUROC of 0.961. The system provides potential annual cost savings through automated quality control and processing times of 0.5 seconds per image in on-device implementation.</li>
</ul>

<h3>Title: Decoupled-Value Attention for Prior-Data Fitted Networks: GP Inference for Physical Equations</h3>
<ul>
<li><strong>Authors: </strong>Kaustubh Sharma, Simardeep Singh, Parikshit Pareek</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20950">https://arxiv.org/abs/2509.20950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20950">https://arxiv.org/pdf/2509.20950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20950]] Decoupled-Value Attention for Prior-Data Fitted Networks: GP Inference for Physical Equations(https://arxiv.org/abs/2509.20950)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Prior-data fitted networks (PFNs) are a promising alternative to time-consuming Gaussian Process (GP) inference for creating fast surrogates of physical systems. PFN reduces the computational burden of GP-training by replacing Bayesian inference in GP with a single forward pass of a learned prediction model. However, with standard Transformer attention, PFNs show limited effectiveness on high-dimensional regression tasks. We introduce Decoupled-Value Attention (DVA)-- motivated by the GP property that the function space is fully characterized by the kernel over inputs and the predictive mean is a weighted sum of training targets. DVA computes similarities from inputs only and propagates labels solely through values. Thus, the proposed DVA mirrors the Gaussian-process update while remaining kernel-free. We demonstrate that the crucial factor for scaling PFNs is the attention rule rather than the architecture itself. Specifically, our results demonstrate that (a) localized attention consistently reduces out-of-sample validation loss in PFNs across different dimensional settings, with validation loss reduced by more than 50% in five- and ten-dimensional cases, and (b) the role of attention is more decisive than the choice of backbone architecture, showing that CNN-based PFNs can perform at par with their Transformer-based counterparts. The proposed PFNs provide 64-dimensional power flow equation approximations with a mean absolute error of the order of 1E-3, while being over 80x faster than exact GP inference.</li>
</ul>

<h3>Title: Flow Matching in the Low-Noise Regime: Pathologies and a Contrastive Remedy</h3>
<ul>
<li><strong>Authors: </strong>Weili Zeng, Yichao Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20952">https://arxiv.org/abs/2509.20952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20952">https://arxiv.org/pdf/2509.20952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20952]] Flow Matching in the Low-Noise Regime: Pathologies and a Contrastive Remedy(https://arxiv.org/abs/2509.20952)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Flow matching has recently emerged as a powerful alternative to diffusion models, providing a continuous-time formulation for generative modeling and representation learning. Yet, we show that this framework suffers from a fundamental instability in the low-noise regime. As noise levels approach zero, arbitrarily small perturbations in the input can induce large variations in the velocity target, causing the condition number of the learning problem to diverge. This ill-conditioning not only slows optimization but also forces the encoder to reallocate its limited Jacobian capacity toward noise directions, thereby degrading semantic representations. We provide the first theoretical analysis of this phenomenon, which we term the low-noise pathology, establishing its intrinsic link to the structure of the flow matching objective. Building on these insights, we propose Local Contrastive Flow (LCF), a hybrid training protocol that replaces direct velocity regression with contrastive feature alignment at small noise levels, while retaining standard flow matching at moderate and high noise. Empirically, LCF not only improves convergence speed but also stabilizes representation quality. Our findings highlight the critical importance of addressing low-noise pathologies to unlock the full potential of flow matching for both generation and representation learning.</li>
</ul>

<h3>Title: Tool Calling for Arabic LLMs: Data Strategies and Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Asim Ersoy, Enes Altinisik, Husrev Taha Sencar, Kareem Darwish</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20957">https://arxiv.org/abs/2509.20957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20957">https://arxiv.org/pdf/2509.20957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20957]] Tool Calling for Arabic LLMs: Data Strategies and Instruction Tuning(https://arxiv.org/abs/2509.20957)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Tool calling is a critical capability that allows Large Language Models (LLMs) to interact with external systems, significantly expanding their utility. However, research and resources for tool calling are predominantly English-centric, leaving a gap in our understanding of how to enable this functionality for other languages, such as Arabic. This paper investigates three key research questions: (1) the necessity of in-language (Arabic) tool-calling data versus relying on cross-lingual transfer, (2) the effect of general-purpose instruction tuning on tool-calling performance, and (3) the value of fine-tuning on specific, high-priority tools. To address these questions, we conduct extensive experiments using base and post-trained variants of an open-weight Arabic LLM. To enable this study, we bridge the resource gap by translating and adapting two open-source tool-calling datasets into Arabic. Our findings provide crucial insights into the optimal strategies for developing robust tool-augmented agents for Arabic.</li>
</ul>

<h3>Title: Unlocking Financial Insights: An advanced Multimodal Summarization with Multimodal Output Framework for Financial Advisory Videos</h3>
<ul>
<li><strong>Authors: </strong>Sarmistha Das, R E Zera Marveen Lyngkhoi, Sriparna Saha, Alka Maurya</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20961">https://arxiv.org/abs/2509.20961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20961">https://arxiv.org/pdf/2509.20961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20961]] Unlocking Financial Insights: An advanced Multimodal Summarization with Multimodal Output Framework for Financial Advisory Videos(https://arxiv.org/abs/2509.20961)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The dynamic propagation of social media has broadened the reach of financial advisory content through podcast videos, yet extracting insights from lengthy, multimodal segments (30-40 minutes) remains challenging. We introduce FASTER (Financial Advisory Summariser with Textual Embedded Relevant images), a modular framework that tackles three key challenges: (1) extracting modality-specific features, (2) producing optimized, concise summaries, and (3) aligning visual keyframes with associated textual points. FASTER employs BLIP for semantic visual descriptions, OCR for textual patterns, and Whisper-based transcription with Speaker diarization as BOS features. A modified Direct Preference Optimization (DPO)-based loss function, equipped with BOS-specific fact-checking, ensures precision, relevance, and factual consistency against the human-aligned summary. A ranker-based retrieval mechanism further aligns keyframes with summarized content, enhancing interpretability and cross-modal coherence. To acknowledge data resource scarcity, we introduce Fin-APT, a dataset comprising 470 publicly accessible financial advisory pep-talk videos for robust multimodal research. Comprehensive cross-domain experiments confirm FASTER's strong performance, robustness, and generalizability when compared to Large Language Models (LLMs) and Vision-Language Models (VLMs). By establishing a new standard for multimodal summarization, FASTER makes financial advisory content more accessible and actionable, thereby opening new avenues for research. The dataset and code are available at: this https URL</li>
</ul>

<h3>Title: Dual-Path Phishing Detection: Integrating Transformer-Based NLP with Structural URL Analysis</h3>
<ul>
<li><strong>Authors: </strong>Ibrahim Altan, Abdulla Bachir, Yousuf Parbhulkar, Abdul Muksith Rizvi, Moshiur Farazi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20972">https://arxiv.org/abs/2509.20972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20972">https://arxiv.org/pdf/2509.20972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20972]] Dual-Path Phishing Detection: Integrating Transformer-Based NLP with Structural URL Analysis(https://arxiv.org/abs/2509.20972)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, transformer</a></li>
<li><strong>Abstract: </strong>Phishing emails pose a persistent and increasingly sophisticated threat, undermining email security through deceptive tactics designed to exploit both semantic and structural vulnerabilities. Traditional detection methods, often based on isolated analysis of email content or embedded URLs, fail to comprehensively address these evolving attacks. In this paper, we propose a dual-path phishing detection framework that integrates transformer-based natural language processing (NLP) with classical machine learning to jointly analyze email text and embedded URLs. Our approach leverages the complementary strengths of semantic analysis using fine-tuned transformer architectures (e.g., DistilBERT) and structural link analysis via character-level TF-IDF vectorization paired with classical classifiers (e.g., Random Forest). Empirical evaluation on representative email and URL datasets demonstrates that this combined approach significantly improves detection accuracy. Specifically, the DistilBERT model achieves a near-optimal balance between accuracy and computational efficiency for textual phishing detection, while Random Forest notably outperforms other classical classifiers in identifying malicious URLs. The modular design allows flexibility for standalone deployment or ensemble integration, facilitating real-world adoption. Collectively, our results highlight the efficacy and practical value of this dual-path approach, establishing a scalable, accurate, and interpretable solution capable of enhancing email security against contemporary phishing threats.</li>
</ul>

<h3>Title: Knowledgeable Language Models as Black-Box Optimizers for Personalized Medicine</h3>
<ul>
<li><strong>Authors: </strong>Michael S. Yao, Osbert Bastani, Alma Andersson, Tommaso Biancalani, Aïcha Bentaieb, Claudia Iriondo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20975">https://arxiv.org/abs/2509.20975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20975">https://arxiv.org/pdf/2509.20975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20975]] Knowledgeable Language Models as Black-Box Optimizers for Personalized Medicine(https://arxiv.org/abs/2509.20975)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The goal of personalized medicine is to discover a treatment regimen that optimizes a patient's clinical outcome based on their personal genetic and environmental factors. However, candidate treatments cannot be arbitrarily administered to the patient to assess their efficacy; we often instead have access to an in silico surrogate model that approximates the true fitness of a proposed treatment. Unfortunately, such surrogate models have been shown to fail to generalize to previously unseen patient-treatment combinations. We hypothesize that domain-specific prior knowledge - such as medical textbooks and biomedical knowledge graphs - can provide a meaningful alternative signal of the fitness of proposed treatments. To this end, we introduce LLM-based Entropy-guided Optimization with kNowledgeable priors (LEON), a mathematically principled approach to leverage large language models (LLMs) as black-box optimizers without any task-specific fine-tuning, taking advantage of their ability to contextualize unstructured domain knowledge to propose personalized treatment plans in natural language. In practice, we implement LEON via 'optimization by prompting,' which uses LLMs as stochastic engines for proposing treatment designs. Experiments on real-world optimization tasks show LEON outperforms both traditional and LLM-based methods in proposing individualized treatments for patients.</li>
</ul>

<h3>Title: CLUE: Conflict-guided Localization for LLM Unlearning Framework</h3>
<ul>
<li><strong>Authors: </strong>Hang Chen, Jiaying Zhu, Xinyu Yang, Wenya Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20977">https://arxiv.org/abs/2509.20977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20977">https://arxiv.org/pdf/2509.20977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20977]] CLUE: Conflict-guided Localization for LLM Unlearning Framework(https://arxiv.org/abs/2509.20977)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The LLM unlearning aims to eliminate the influence of undesirable data without affecting causally unrelated information. This process typically involves using a forget set to remove target information, alongside a retain set to maintain non-target capabilities. While recent localization-based methods demonstrate promise in identifying important neurons to be unlearned, they fail to disentangle neurons responsible for forgetting undesirable knowledge or retaining essential skills, often treating them as a single entangled group. As a result, these methods apply uniform interventions, risking catastrophic over-forgetting or incomplete erasure of the target knowledge. To address this, we turn to circuit discovery, a mechanistic interpretability technique, and propose the Conflict-guided Localization for LLM Unlearning framEwork (CLUE). This framework identifies the forget and retain circuit composed of important neurons, and then the circuits are transformed into conjunctive normal forms (CNF). The assignment of each neuron in the CNF satisfiability solution reveals whether it should be forgotten or retained. We then provide targeted fine-tuning strategies for different categories of neurons. Extensive experiments demonstrate that, compared to existing localization methods, CLUE achieves superior forget efficacy and retain utility through precise neural localization.</li>
</ul>

<h3>Title: Toward Robust and Efficient ML-Based GPU Caching for Modern Inference</h3>
<ul>
<li><strong>Authors: </strong>Peng Chen, Jiaji Zhang, Hailiang Zhao, Yirong Zhang, Jiahong Yu, Xueyan Tang, Yixuan Wang, Hao Li, Jianping Zou, Gang Xiong, Kingsum Chow, Shuibing He, Shuiguang Deng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20979">https://arxiv.org/abs/2509.20979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20979">https://arxiv.org/pdf/2509.20979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20979]] Toward Robust and Efficient ML-Based GPU Caching for Modern Inference(https://arxiv.org/abs/2509.20979)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In modern GPU inference, cache efficiency remains a major bottleneck. In recommendation models, embedding hit rates largely determine throughput, while in large language models, KV-cache misses substantially increase time-to-first-token (TTFT). Heuristic policies such as \textsc{LRU} often struggle under structured access patterns. Learning-based approaches are promising, but in practice face two major limitations: they degrade sharply when predictions are inaccurate, or they gain little even with accurate predictions due to conservative designs. Some also incur high overhead, further limiting practicality. We present \textsc{LCR}, a practical framework for learning-based GPU caching that delivers performance gains while ensuring robustness and efficiency. Its core algorithm, \textsc{LARU}, enhances \textsc{LRU} with machine-learned predictions and dynamically adapts to prediction accuracy through online error estimation. When predictions are accurate, \textsc{LARU} achieves near-optimal performance. With inaccurate predictions, it degrades gracefully to near-\textsc{LRU} performance. With \textsc{LCR}, we bridge the gap between empirical progress and theoretical advances in learning-based caching. Experiments show that \textsc{LCR} delivers consistent gains under realistic conditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\% and reduces P99 TTFT by up to 28.3\%, outperforming widely used inference systems. Even under poor predictions, its performance remains stable, demonstrating practical robustness.</li>
</ul>

<h3>Title: Analysis of instruction-based LLMs' capabilities to score and judge text-input problems in an academic setting</h3>
<ul>
<li><strong>Authors: </strong>Valeria Ramirez-Garcia, David de-Fitero-Dominguez, Antonio Garcia-Cabot, Eva Garcia-Lopez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20982">https://arxiv.org/abs/2509.20982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20982">https://arxiv.org/pdf/2509.20982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20982]] Analysis of instruction-based LLMs' capabilities to score and judge text-input problems in an academic setting(https://arxiv.org/abs/2509.20982)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can act as evaluators, a role studied by methods like LLM-as-a-Judge and fine-tuned judging LLMs. In the field of education, LLMs have been studied as assistant tools for students and teachers. Our research investigates LLM-driven automatic evaluation systems for academic Text-Input Problems using rubrics. We propose five evaluation systems that have been tested on a custom dataset of 110 answers about computer science from higher education students with three models: JudgeLM, Llama-3.1-8B and DeepSeek-R1-Distill-Llama-8B. The evaluation systems include: The JudgeLM evaluation, which uses the model's single answer prompt to obtain a score; Reference Aided Evaluation, which uses a correct answer as a guide aside from the original context of the question; No Reference Evaluation, which ommits the reference answer; Additive Evaluation, which uses atomic criteria; and Adaptive Evaluation, which is an evaluation done with generated criteria fitted to each question. All evaluation methods have been compared with the results of a human evaluator. Results show that the best method to automatically evaluate and score Text-Input Problems using LLMs is Reference Aided Evaluation. With the lowest median absolute deviation (0.945) and the lowest root mean square deviation (1.214) when compared to human evaluation, Reference Aided Evaluation offers fair scoring as well as insightful and complete evaluations. Other methods such as Additive and Adaptive Evaluation fail to provide good results in concise answers, No Reference Evaluation lacks information needed to correctly assess questions and JudgeLM Evaluations have not provided good results due to the model's limitations. As a result, we conclude that Artificial Intelligence-driven automatic evaluation systems, aided with proper methodologies, show potential to work as complementary tools to other academic resources.</li>
</ul>

<h3>Title: SiNGER: A Clearer Voice Distills Vision Transformers Further</h3>
<ul>
<li><strong>Authors: </strong>Geunhyeok Yu, Sunjae Jeong, Yoonyoung Choi, Jaeseung Kim, Hyoseok Hwang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20986">https://arxiv.org/abs/2509.20986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20986">https://arxiv.org/pdf/2509.20986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20986]] SiNGER: A Clearer Voice Distills Vision Transformers Further(https://arxiv.org/abs/2509.20986)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision Transformers are widely adopted as the backbone of vision foundation models, but they are known to produce high-norm artifacts that degrade representation quality. When knowledge distillation transfers these features to students, high-norm artifacts dominate the objective, so students overfit to artifacts and underweight informative signals, diminishing the gains from larger models. Prior work attempted to remove artifacts but encountered an inherent trade-off between artifact suppression and preserving informative signals from teachers. To address this, we introduce Singular Nullspace-Guided Energy Reallocation (SiNGER), a novel distillation framework that suppresses artifacts while preserving informative signals. The key idea is principled teacher feature refinement: during refinement, we leverage the nullspace-guided perturbation to preserve information while suppressing artifacts. Then, the refined teacher's features are distilled to a student. We implement this perturbation efficiently with a LoRA-based adapter that requires minimal structural modification. Extensive experiments show that \oursname consistently improves student models, achieving state-of-the-art performance in multiple downstream tasks and producing clearer and more interpretable representations.</li>
</ul>

<h3>Title: Fast-SEnSeI: Lightweight Sensor-Independent Cloud Masking for On-board Multispectral Sensors</h3>
<ul>
<li><strong>Authors: </strong>Jan Kněžík, Jonáš Herec, Rado Pitoňák</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20991">https://arxiv.org/abs/2509.20991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20991">https://arxiv.org/pdf/2509.20991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20991]] Fast-SEnSeI: Lightweight Sensor-Independent Cloud Masking for On-board Multispectral Sensors(https://arxiv.org/abs/2509.20991)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Cloud segmentation is a critical preprocessing step for many Earth observation tasks, yet most models are tightly coupled to specific sensor configurations and rely on ground-based processing. In this work, we propose Fast-SEnSeI, a lightweight, sensor-independent encoder module that enables flexible, on-board cloud segmentation across multispectral sensors with varying band configurations. Building upon SEnSeI-v2, Fast-SEnSeI integrates an improved spectral descriptor, lightweight architecture, and robust padding-band handling. It accepts arbitrary combinations of spectral bands and their wavelengths, producing fixed-size feature maps that feed into a compact, quantized segmentation model based on a modified U-Net. The module runs efficiently on embedded CPUs using Apache TVM, while the segmentation model is deployed on FPGA, forming a CPU-FPGA hybrid pipeline suitable for space-qualified hardware. Evaluations on Sentinel-2 and Landsat 8 datasets demonstrate accurate segmentation across diverse input configurations.</li>
</ul>

<h3>Title: Binary Autoencoder for Mechanistic Interpretability of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hakaze Cho, Haolin Yang, Brian M. Kurkoski, Naoya Inoue</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20997">https://arxiv.org/abs/2509.20997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20997">https://arxiv.org/pdf/2509.20997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20997]] Binary Autoencoder for Mechanistic Interpretability of Large Language Models(https://arxiv.org/abs/2509.20997)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Existing works are dedicated to untangling atomized numerical components (features) from the hidden states of Large Language Models (LLMs) for interpreting their mechanism. However, they typically rely on autoencoders constrained by some implicit training-time regularization on single training instances (i.e., $L_1$ normalization, top-k function, etc.), without an explicit guarantee of global sparsity among instances, causing a large amount of dense (simultaneously inactive) features, harming the feature sparsity and atomization. In this paper, we propose a novel autoencoder variant that enforces minimal entropy on minibatches of hidden activations, thereby promoting feature independence and sparsity across instances. For efficient entropy calculation, we discretize the hidden activations to 1-bit via a step function and apply gradient estimation to enable backpropagation, so that we term it as Binary Autoencoder (BAE) and empirically demonstrate two major applications: (1) Feature set entropy calculation. Entropy can be reliably estimated on binary hidden activations, which we empirically evaluate and leverage to characterize the inference dynamics of LLMs and In-context Learning. (2) Feature untangling. Similar to typical methods, BAE can extract atomized features from LLM's hidden states. To robustly evaluate such feature extraction capability, we refine traditional feature-interpretation methods to avoid unreliable handling of numerical tokens, and show that BAE avoids dense features while producing the largest number of interpretable ones among baselines, which confirms the effectiveness of BAE serving as a feature extractor.</li>
</ul>

<h3>Title: Lossless Compression: A New Benchmark for Time Series Model Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Meng Wan, Benxi Tian, Jue Wang, Cui Hui, Ningming Nie, Tiantian Liu, Zongguo Wang, Cao Rongqiang, Peng Shi, Yangang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21002">https://arxiv.org/abs/2509.21002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21002">https://arxiv.org/pdf/2509.21002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21002]] Lossless Compression: A New Benchmark for Time Series Model Evaluation(https://arxiv.org/abs/2509.21002)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>The evaluation of time series models has traditionally focused on four canonical tasks: forecasting, imputation, anomaly detection, and classification. While these tasks have driven significant progress, they primarily assess task-specific performance and do not rigorously measure whether a model captures the full generative distribution of the data. We introduce lossless compression as a new paradigm for evaluating time series models, grounded in Shannon's source coding theorem. This perspective establishes a direct equivalence between optimal compression length and the negative log-likelihood, providing a strict and unified information-theoretic criterion for modeling capacity. Then We define a standardized evaluation protocol and metrics. We further propose and open-source a comprehensive evaluation framework TSCom-Bench, which enables the rapid adaptation of time series models as backbones for lossless compression. Experiments across diverse datasets on state-of-the-art models, including TimeXer, iTransformer, and PatchTST, demonstrate that compression reveals distributional weaknesses overlooked by classic benchmarks. These findings position lossless compression as a principled task that complements and extends existing evaluation for time series modeling.</li>
</ul>

<h3>Title: MAIFormer: Multi-Agent Inverted Transformer for Flight Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Seokbin Yoon, Keumjin Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21004">https://arxiv.org/abs/2509.21004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21004">https://arxiv.org/pdf/2509.21004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21004]] MAIFormer: Multi-Agent Inverted Transformer for Flight Trajectory Prediction(https://arxiv.org/abs/2509.21004)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Flight trajectory prediction for multiple aircraft is essential and provides critical insights into how aircraft navigate within current air traffic flows. However, predicting multi-agent flight trajectories is inherently challenging. One of the major difficulties is modeling both the individual aircraft behaviors over time and the complex interactions between flights. Generating explainable prediction outcomes is also a challenge. Therefore, we propose a Multi-Agent Inverted Transformer, MAIFormer, as a novel neural architecture that predicts multi-agent flight trajectories. The proposed framework features two key attention modules: (i) masked multivariate attention, which captures spatio-temporal patterns of individual aircraft, and (ii) agent attention, which models the social patterns among multiple agents in complex air traffic scenes. We evaluated MAIFormer using a real-world automatic dependent surveillance-broadcast flight trajectory dataset from the terminal airspace of Incheon International Airport in South Korea. The experimental results show that MAIFormer achieves the best performance across multiple metrics and outperforms other methods. In addition, MAIFormer produces prediction outcomes that are interpretable from a human perspective, which improves both the transparency of the model and its practical utility in air traffic control.</li>
</ul>

<h3>Title: A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Qinqin He, Jiaqi Weng, Jialing Tao, Hui Xue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21008">https://arxiv.org/abs/2509.21008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21008">https://arxiv.org/pdf/2509.21008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21008]] A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models(https://arxiv.org/abs/2509.21008)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image models exhibit remarkable capabilities in image generation. However, they also pose safety risks of generating harmful content. A key challenge of existing concept erasure methods is the precise removal of target concepts while minimizing degradation of image quality. In this paper, we propose Single Neuron-based Concept Erasure (SNCE), a novel approach that can precisely prevent harmful content generation by manipulating only a single neuron. Specifically, we train a Sparse Autoencoder (SAE) to map text embeddings into a sparse, disentangled latent space, where individual neurons align tightly with atomic semantic concepts. To accurately locate neurons responsible for harmful concepts, we design a novel neuron identification method based on the modulated frequency scoring of activation patterns. By suppressing activations of the harmful concept-specific neuron, SNCE achieves surgical precision in concept erasure with minimal disruption to image quality. Experiments on various benchmarks demonstrate that SNCE achieves state-of-the-art results in target concept erasure, while preserving the model's generation capabilities for non-target concepts. Additionally, our method exhibits strong robustness against adversarial attacks, significantly outperforming existing methods.</li>
</ul>

<h3>Title: ExMolRL: Phenotype-Target Joint Generation of De Novo Molecules via Multi-Objective Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Haotian Guo, Hui Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21010">https://arxiv.org/abs/2509.21010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21010">https://arxiv.org/pdf/2509.21010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21010]] ExMolRL: Phenotype-Target Joint Generation of De Novo Molecules via Multi-Objective Reinforcement Learning(https://arxiv.org/abs/2509.21010)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The generation of high-quality candidate molecules remains a central challenge in AI-driven drug design. Current phenotype-based and target-based strategies each suffer limitations, either incurring high experimental costs or overlook system-level cellular responses. To bridge this gap, we propose ExMoIRL, a novel generative framework that synergistically integrates phenotypic and target-specific cues for de novo molecular generation. The phenotype-guided generator is first pretrained on expansive drug-induced transcriptional profiles and subsequently fine-tuned via multi-objective reinforcement learning (RL). Crucially, the reward function fuses docking affinity and drug-likeness scores, augmented with ranking loss, prior-likelihood regularization, and entropy maximization. The multi-objective RL steers the model toward chemotypes that are simultaneously potent, diverse, and aligned with the specified phenotypic effects. Extensive experiments demonstrate ExMoIRL's superior performance over state-of-the-art phenotype-based and target-based models across multiple well-characterized targets. Our generated molecules exhibit favorable drug-like properties, high target affinity, and inhibitory potency (IC50) against cancer cells. This unified framework showcases the synergistic potential of combining phenotype-guided and target-aware strategies, offering a more effective solution for de novo drug discovery.</li>
</ul>

<h3>Title: Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools</h3>
<ul>
<li><strong>Authors: </strong>Ping He, Changjiang Li, Binbin Zhao, Tianyu Du, Shouling Ji</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21011">https://arxiv.org/abs/2509.21011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21011">https://arxiv.org/pdf/2509.21011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21011]] Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools(https://arxiv.org/abs/2509.21011)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>The remarkable capability of large language models (LLMs) has led to the wide application of LLM-based agents in various domains. To standardize interactions between LLM-based agents and their environments, model context protocol (MCP) tools have become the de facto standard and are now widely integrated into these agents. However, the incorporation of MCP tools introduces the risk of tool poisoning attacks, which can manipulate the behavior of LLM-based agents. Although previous studies have identified such vulnerabilities, their red teaming approaches have largely remained at the proof-of-concept stage, leaving the automatic and systematic red teaming of LLM-based agents under the MCP tool poisoning paradigm an open question. To bridge this gap, we propose AutoMalTool, an automated red teaming framework for LLM-based agents by generating malicious MCP tools. Our extensive evaluation shows that AutoMalTool effectively generates malicious MCP tools capable of manipulating the behavior of mainstream LLM-based agents while evading current detection mechanisms, thereby revealing new security risks in these agents.</li>
</ul>

<h3>Title: Predicting LLM Reasoning Performance with Small Proxy Model</h3>
<ul>
<li><strong>Authors: </strong>Woosung Koh, Juyoung Suk, Sungjun Han, Se-Young Yun, Jay Shin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21013">https://arxiv.org/abs/2509.21013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21013">https://arxiv.org/pdf/2509.21013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21013]] Predicting LLM Reasoning Performance with Small Proxy Model(https://arxiv.org/abs/2509.21013)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Given the prohibitive cost of pre-training large language models, it is essential to leverage smaller proxy models to optimize datasets before scaling up. However, this approach becomes challenging for reasoning capabilities, which exhibit emergent behavior that only appear reliably at larger model sizes, often exceeding 7B parameters. To address this, we introduce rBridge, showing that small proxies ($\leq$1B) can effectively predict large-model reasoning by aligning more closely with (1) the pre-training objective and (2) the target task. rBridge achieves this by weighting negative log-likelihood with task alignment, using reasoning traces from frontier models as gold labels. In our experiments, rBridge (i) reduces dataset ranking costs by over 100x relative to the best baseline, (ii) achieves the strongest correlation across six reasoning benchmarks at 1B to 32B scale, and (iii) zero-shot transfers predictive relationships across pre-training datasets at 1B to 7B scale. These findings indicate that rBridge offers a practical path for exploring reasoning-oriented pre-training at lower cost.</li>
</ul>

<h3>Title: Actor-Critic without Actor</h3>
<ul>
<li><strong>Authors: </strong>Donghyeon Ki, Hee-Jun Ahn, Kyungyoon Kim, Byung-Jun Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21022">https://arxiv.org/abs/2509.21022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21022">https://arxiv.org/pdf/2509.21022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21022]] Actor-Critic without Actor(https://arxiv.org/abs/2509.21022)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Actor-critic methods constitute a central paradigm in reinforcement learning (RL), coupling policy evaluation with policy improvement. While effective across many domains, these methods rely on separate actor and critic networks, which makes training vulnerable to architectural decisions and hyperparameter tuning. Such complexity limits their scalability in settings that require large function approximators. Recently, diffusion models have recently been proposed as expressive policies that capture multi-modal behaviors and improve exploration, but they introduce additional design choices and computational burdens, hindering efficient deployment. We introduce Actor-Critic without Actor (ACA), a lightweight framework that eliminates the explicit actor network and instead generates actions directly from the gradient field of a noise-level critic. This design removes the algorithmic and computational overhead of actor training while keeping policy improvement tightly aligned with the critic's latest value estimates. Moreover, ACA retains the ability to capture diverse, multi-modal behaviors without relying on diffusion-based actors, combining simplicity with expressiveness. Through extensive experiments on standard online RL benchmarks,ACA achieves more favorable learning curves and competitive performance compared to both standard actor-critic and state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.</li>
</ul>

<h3>Title: FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance CorrEction</h3>
<ul>
<li><strong>Authors: </strong>Runqi Lin, Alasdair Paren, Suqin Yuan, Muyang Li, Philip Torr, Adel Bibi, Tongliang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21029">https://arxiv.org/abs/2509.21029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21029">https://arxiv.org/pdf/2509.21029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21029]] FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance CorrEction(https://arxiv.org/abs/2509.21029)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>The integration of new modalities enhances the capabilities of multimodal large language models (MLLMs) but also introduces additional vulnerabilities. In particular, simple visual jailbreaking attacks can manipulate open-source MLLMs more readily than sophisticated textual attacks. However, these underdeveloped attacks exhibit extremely limited cross-model transferability, failing to reliably identify vulnerabilities in closed-source MLLMs. In this work, we analyse the loss landscape of these jailbreaking attacks and find that the generated attacks tend to reside in high-sharpness regions, whose effectiveness is highly sensitive to even minor parameter changes during transfer. To further explain the high-sharpness localisations, we analyse their feature representations in both the intermediate layers and the spectral domain, revealing an improper reliance on narrow layer representations and semantically poor frequency components. Building on this, we propose a Feature Over-Reliance CorrEction (FORCE) method, which guides the attack to explore broader feasible regions across layer features and rescales the influence of frequency features according to their semantic content. By eliminating non-generalizable reliance on both layer and spectral features, our method discovers flattened feasible regions for visual jailbreaking attacks, thereby improving cross-model transferability. Extensive experiments demonstrate that our approach effectively facilitates visual red-teaming evaluations against closed-source MLLMs.</li>
</ul>

<h3>Title: OmniPlantSeg: Species Agnostic 3D Point Cloud Organ Segmentation for High-Resolution Plant Phenotyping Across Modalities</h3>
<ul>
<li><strong>Authors: </strong>Andreas Gilson, Lukas Meyer, Oliver Scholz, Ute Schmid</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21038">https://arxiv.org/abs/2509.21038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21038">https://arxiv.org/pdf/2509.21038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21038]] OmniPlantSeg: Species Agnostic 3D Point Cloud Organ Segmentation for High-Resolution Plant Phenotyping Across Modalities(https://arxiv.org/abs/2509.21038)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate point cloud segmentation for plant organs is crucial for 3D plant phenotyping. Existing solutions are designed problem-specific with a focus on certain plant species or specified sensor-modalities for data acquisition. Furthermore, it is common to use extensive pre-processing and down-sample the plant point clouds to meet hardware or neural network input size requirements. We propose a simple, yet effective algorithm KDSS for sub-sampling of biological point clouds that is agnostic to sensor data and plant species. The main benefit of this approach is that we do not need to down-sample our input data and thus, enable segmentation of the full-resolution point cloud. Combining KD-SS with current state-of-the-art segmentation models shows satisfying results evaluated on different modalities such as photogrammetry, laser triangulation and LiDAR for various plant species. We propose KD-SS as lightweight resolution-retaining alternative to intensive pre-processing and down-sampling methods for plant organ segmentation regardless of used species and sensor modality.</li>
</ul>

<h3>Title: Generative AI for FFRDCs</h3>
<ul>
<li><strong>Authors: </strong>Arun S. Maiya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21040">https://arxiv.org/abs/2509.21040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21040">https://arxiv.org/pdf/2509.21040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21040]] Generative AI for FFRDCs(https://arxiv.org/abs/2509.21040)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Federally funded research and development centers (FFRDCs) face text-heavy workloads, from policy documents to scientific and engineering papers, that are slow to analyze manually. We show how large language models can accelerate summarization, classification, extraction, and sense-making with only a few input-output examples. To enable use in sensitive government contexts, we apply OnPrem$.$LLM, an open-source framework for secure and flexible application of generative AI. Case studies on defense policy documents and scientific corpora, including the National Defense Authorization Act (NDAA) and National Science Foundation (NSF) Awards, demonstrate how this approach enhances oversight and strategic analysis while maintaining auditability and data sovereignty.</li>
</ul>

<h3>Title: Behind RoPE: How Does Causal Mask Encode Positional Information?</h3>
<ul>
<li><strong>Authors: </strong>Junu Kim, Xiao Liu, Zhenghao Lin, Lei Ji, Yeyun Gong, Edward Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21042">https://arxiv.org/abs/2509.21042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21042">https://arxiv.org/pdf/2509.21042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21042]] Behind RoPE: How Does Causal Mask Encode Positional Information?(https://arxiv.org/abs/2509.21042)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>While explicit positional encodings such as RoPE are a primary source of positional information in Transformer decoders, the causal mask also provides positional information. In this work, we prove that the causal mask can induce position-dependent patterns in attention scores, even without parameters or causal dependency in the input. Our theoretical analysis indicates that the induced attention pattern tends to favor nearby query-key pairs, mirroring the behavior of common positional encodings. Empirical analysis confirms that trained models exhibit the same behavior, with learned parameters further amplifying these patterns. Notably, we found that the interaction of causal mask and RoPE distorts RoPE's relative attention score patterns into non-relative ones. We consistently observed this effect in modern large language models, suggesting the importance of considering the causal mask as a source of positional information alongside explicit positional encodings.</li>
</ul>

<h3>Title: Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Honglin Zhang, Qianyue Hao, Fengli Xu, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21044">https://arxiv.org/abs/2509.21044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21044">https://arxiv.org/pdf/2509.21044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21044]] Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs(https://arxiv.org/abs/2509.21044)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) acquire extensive prior knowledge through large-scale pretraining and can be further enhanced via supervised fine-tuning (SFT) or reinforcement learning (RL)-based post-training. A growing body of evidence has shown that RL fine-tuning improves the capability of LLMs beyond what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning is able to enhance the capability of various LLMs with distinct intrinsic characteristics remain underexplored. In this study, we draw inspiration from prior work on edge attribution patching (EAP) to investigate the internal differences of LLMs before and after RL fine-tuning. Our analysis across multiple model families shows two robust effects of online RL post-training: (i) an overall increase in activation intensity, indicating that more internal pathways are engaged and their signals become stronger, and (ii) greater diversity in activation patterns, reflected by higher entropy and less concentrated edge distributions. These changes suggest that RL reshapes information flow to be both more redundant and more flexible, which may explain its advantage in generalization. Notably, models fine-tuned with Direct Preference Optimization (DPO) deviate from these trends, exhibiting substantially weaker or inconsistent internal changes compared to PPO- and GRPO-based training. Together, our findings provide a unified view of how RL fine-tuning systematically alters the internal circuitry of LLMs and highlight the methodological distinctions between online RL and preference-based approaches. Our code is open source at this https URL.</li>
</ul>

<h3>Title: Physics of Learning: A Lagrangian perspective to different learning paradigms</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Guo, Bernhard Schölkopf</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21049">https://arxiv.org/abs/2509.21049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21049">https://arxiv.org/pdf/2509.21049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21049]] Physics of Learning: A Lagrangian perspective to different learning paradigms(https://arxiv.org/abs/2509.21049)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study the problem of building an efficient learning system. Efficient learning processes information in the least time, i.e., building a system that reaches a desired error threshold with the least number of observations. Building upon least action principles from physics, we derive classic learning algorithms, Bellman's optimality equation in reinforcement learning, and the Adam optimizer in generative models from first principles, i.e., the Learning $\textit{Lagrangian}$. We postulate that learning searches for stationary paths in the Lagrangian, and learning algorithms are derivable by seeking the stationary trajectories.</li>
</ul>

<h3>Title: GeoRef: Referring Expressions in Geometry via Task Formulation, Synthetic Supervision, and Reinforced MLLM-based Solutions</h3>
<ul>
<li><strong>Authors: </strong>Bing Liu, Wenqiang Yv, Xuzheng Yang, Shichang Wang, Junzhuo Liu, Peng Wang, Guoqing Wang, Yang Yang, Heng Tao Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21050">https://arxiv.org/abs/2509.21050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21050">https://arxiv.org/pdf/2509.21050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21050]] GeoRef: Referring Expressions in Geometry via Task Formulation, Synthetic Supervision, and Reinforced MLLM-based Solutions(https://arxiv.org/abs/2509.21050)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>AI-driven geometric problem solving is a complex vision-language task that requires accurate diagram interpretation, mathematical reasoning, and robust cross-modal grounding. A foundational yet underexplored capability for this task is the ability to identify and interpret geometric elements based on natural language queries. To address this, we introduce the task of Referring Expression Comprehension (REC) for geometric problems, which evaluates whether models can localize points, shapes, and spatial relations in diagrams in response to textual prompts. We present GeoRef, a benchmark dataset constructed from existing geometric problem corpora, featuring diverse, high-quality annotations and queries. Due to the lack of annotated data for this task, we generate a large-scale synthetic training dataset using a structured geometric formal language, enabling broad coverage of geometric concepts and facilitating model adaptation. We explore two fine-tuning approaches: Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO). Our results show that GRPO significantly outperforms SFT by better aligning model behavior with task-specific rewards. Furthermore, we propose a verify-and-regenerate mechanism that detects incorrect predictions and re-infers answers using contextual reasoning history, further boosting accuracy. Notably, even state-of-the-art Multimodal Large Language Models (MLLMs) struggle with this task, underscoring the necessity of explicitly evaluating and strengthening geometric grounding as a prerequisite for robust geometric problem solving. Moreover, models trained on GeoRef demonstrate measurable improvements on downstream geometric reasoning tasks, highlighting the broader value of REC as a foundation for multimodal mathematical understanding.</li>
</ul>

<h3>Title: When Instructions Multiply: Measuring and Estimating LLM Capabilities of Multiple Instructions Following</h3>
<ul>
<li><strong>Authors: </strong>Keno Harada, Yudai Yamazaki, Masachika Taniguchi, Edison Marrese-Taylor, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21051">https://arxiv.org/abs/2509.21051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21051">https://arxiv.org/pdf/2509.21051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21051]] When Instructions Multiply: Measuring and Estimating LLM Capabilities of Multiple Instructions Following(https://arxiv.org/abs/2509.21051)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly applied to real-world scenarios, it becomes crucial to understand their ability to follow multiple instructions simultaneously. To systematically evaluate these capabilities, we introduce two specialized benchmarks for fundamental domains where multiple instructions following is important: Many Instruction-Following Eval (ManyIFEval) for text generation with up to ten instructions, and Style-aware Mostly Basic Programming Problems (StyleMBPP) for code generation with up to six instructions. Our experiments with the created benchmarks across ten LLMs reveal that performance consistently degrades as the number of instructions increases. Furthermore, given the fact that evaluating all the possible combinations of multiple instructions is computationally impractical in actual use cases, we developed three types of regression models that can estimate performance on both unseen instruction combinations and different numbers of instructions which are not used during training. We demonstrate that a logistic regression model using instruction count as an explanatory variable can predict performance of following multiple instructions with approximately 10% error, even for unseen instruction combinations. We show that relatively modest sample sizes (500 for ManyIFEval and 300 for StyleMBPP) are sufficient for performance estimation, enabling efficient evaluation of LLMs under various instruction combinations.</li>
</ul>

<h3>Title: Background Prompt for Few-Shot Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Songyue Cai, Zongqian Wu, Yujie Mo, Liang Peng, Ping Hu, Xiaoshuang Shi, Xiaofeng Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21055">https://arxiv.org/abs/2509.21055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21055">https://arxiv.org/pdf/2509.21055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21055]] Background Prompt for Few-Shot Out-of-Distribution Detection(https://arxiv.org/abs/2509.21055)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Existing foreground-background (FG-BG) decomposition methods for the few-shot out-of-distribution (FS-OOD) detection often suffer from low robustness due to over-reliance on the local class similarity and a fixed background patch extraction strategy. To address these challenges, we propose a new FG-BG decomposition framework, namely Mambo, for FS-OOD detection. Specifically, we propose to first learn a background prompt to obtain the local background similarity containing both the background and image semantic information, and then refine the local background similarity using the local class similarity. As a result, we use both the refined local background similarity and the local class similarity to conduct background extraction, reducing the dependence of the local class similarity in previous methods. Furthermore, we propose the patch self-calibrated tuning to consider the sample diversity to flexibly select numbers of background patches for different samples, and thus exploring the issue of fixed background extraction strategies in previous methods. Extensive experiments on real-world datasets demonstrate that our proposed Mambo achieves the best performance, compared to SOTA methods in terms of OOD detection and near OOD detection setting. The source code will be released at this https URL.</li>
</ul>

<h3>Title: Stratify or Die: Rethinking Data Splits in Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Naga Venkata Sai Jitin Jami, Thomas Altstidl, Jonas Mueller, Jindong Li, Dario Zanca, Bjoern Eskofier, Heike Leutheuser</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21056">https://arxiv.org/abs/2509.21056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21056">https://arxiv.org/pdf/2509.21056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21056]] Stratify or Die: Rethinking Data Splits in Image Segmentation(https://arxiv.org/abs/2509.21056)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Random splitting of datasets in image segmentation often leads to unrepresentative test sets, resulting in biased evaluations and poor model generalization. While stratified sampling has proven effective for addressing label distribution imbalance in classification tasks, extending these ideas to segmentation remains challenging due to the multi-label structure and class imbalance typically present in such data. Building on existing stratification concepts, we introduce Iterative Pixel Stratification (IPS), a straightforward, label-aware sampling method tailored for segmentation tasks. Additionally, we present Wasserstein-Driven Evolutionary Stratification (WDES), a novel genetic algorithm designed to minimize the Wasserstein distance, thereby optimizing the similarity of label distributions across dataset splits. We prove that WDES is globally optimal given enough generations. Using newly proposed statistical heterogeneity metrics, we evaluate both methods against random sampling and find that WDES consistently produces more representative splits. Applying WDES across diverse segmentation tasks, including street scenes, medical imaging, and satellite imagery, leads to lower performance variance and improved model evaluation. Our results also highlight the particular value of WDES in handling small, imbalanced, and low-diversity datasets, where conventional splitting strategies are most prone to bias.</li>
</ul>

<h3>Title: PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Huo, Shuliang Liu, Bin Wang, Junyan Zhang, Yibo Yan, Aiwei Liu, Xuming Hu, Mingxun Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21057">https://arxiv.org/abs/2509.21057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21057">https://arxiv.org/pdf/2509.21057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21057]] PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints(https://arxiv.org/abs/2509.21057)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Semantic-level watermarking (SWM) for large language models (LLMs) enhances watermarking robustness against text modifications and paraphrasing attacks by treating the sentence as the fundamental unit. However, existing methods still lack strong theoretical guarantees of robustness, and reject-sampling-based generation often introduces significant distribution distortions compared with unwatermarked outputs. In this work, we introduce a new theoretical framework on SWM through the concept of proxy functions (PFs) $\unicode{x2013}$ functions that map sentences to scalar values. Building on this framework, we propose PMark, a simple yet powerful SWM method that estimates the PF median for the next sentence dynamically through sampling while enforcing multiple PF constraints (which we call channels) to strengthen watermark evidence. Equipped with solid theoretical guarantees, PMark achieves the desired distortion-free property and improves the robustness against paraphrasing-style attacks. We also provide an empirically optimized version that further removes the requirement for dynamical median estimation for better sampling efficiency. Experimental results show that PMark consistently outperforms existing SWM baselines in both text quality and robustness, offering a more effective paradigm for detecting machine-generated text. Our code will be released at [this URL](this https URL).</li>
</ul>

<h3>Title: SPREAD: Sampling-based Pareto front Refinement via Efficient Adaptive Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Sedjro Salomon Hotegni, Sebastian Peitz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21058">https://arxiv.org/abs/2509.21058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21058">https://arxiv.org/pdf/2509.21058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21058]] SPREAD: Sampling-based Pareto front Refinement via Efficient Adaptive Diffusion(https://arxiv.org/abs/2509.21058)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Developing efficient multi-objective optimization methods to compute the Pareto set of optimal compromises between conflicting objectives remains a key challenge, especially for large-scale and expensive problems. To bridge this gap, we introduce SPREAD, a generative framework based on Denoising Diffusion Probabilistic Models (DDPMs). SPREAD first learns a conditional diffusion process over points sampled from the decision space and then, at each reverse diffusion step, refines candidates via a sampling scheme that uses an adaptive multiple gradient descent-inspired update for fast convergence alongside a Gaussian RBF-based repulsion term for diversity. Empirical results on multi-objective optimization benchmarks, including offline and Bayesian surrogate-based settings, show that SPREAD matches or exceeds leading baselines in efficiency, scalability, and Pareto front coverage.</li>
</ul>

<h3>Title: SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials</h3>
<ul>
<li><strong>Authors: </strong>Qixin Wan, Zilong Wang, Jingwen Zhou, Wanting Wang, Ziheng Geng, Jiachen Liu, Ran Cao, Minghui Cheng, Lu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21079">https://arxiv.org/abs/2509.21079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21079">https://arxiv.org/pdf/2509.21079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21079]] SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials(https://arxiv.org/abs/2509.21079)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Foundation models have shown remarkable capabilities in various domains, but their performance on complex, multimodal engineering problems remains largely unexplored. We introduce SoM-1K, the first large-scale multimodal benchmark dataset dedicated to evaluating foundation models on problems in the strength of materials (SoM). The dataset, which contains 1,065 annotated SoM problems, mirrors real-world engineering tasks by including both textual problem statements and schematic diagrams. Due to the limited capabilities of current foundation models in understanding complicated visual information, we propose a novel prompting strategy called Descriptions of Images (DoI), which provides rigorous expert-generated text descriptions of the visual diagrams as the context. We evaluate eight representative foundation models, including both large language models (LLMs) and vision language models (VLMs). Our results show that current foundation models struggle significantly with these engineering problems, with the best-performing model achieving only 56.6% accuracy. Interestingly, we found that LLMs, when provided with DoI, often outperform VLMs provided with visual diagrams. A detailed error analysis reveals that DoI plays a crucial role in mitigating visual misinterpretation errors, suggesting that accurate text-based descriptions can be more effective than direct image input for current foundation models. This work establishes a rigorous benchmark for engineering AI and highlights a critical need for developing more robust multimodal reasoning capabilities in foundation models, particularly in scientific and engineering contexts.</li>
</ul>

<h3>Title: Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yixin Wan, Xingrun Chen, Kai-Wei Chang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21080">https://arxiv.org/abs/2509.21080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21080">https://arxiv.org/pdf/2509.21080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21080]] Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs(https://arxiv.org/abs/2509.21080)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have unlocked a wide range of downstream generative applications. However, we found that they also risk perpetuating subtle fairness issues tied to culture, positioning their generations from the perspectives of the mainstream US culture while demonstrating salient externality towards non-mainstream ones. In this work, we identify and systematically investigate this novel culture positioning bias, in which an LLM's default generative stance aligns with a mainstream view and treats other cultures as outsiders. We propose the CultureLens benchmark with 4000 generation prompts and 3 evaluation metrics for quantifying this bias through the lens of a culturally situated interview script generation task, in which an LLM is positioned as an onsite reporter interviewing local people across 10 diverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a stark pattern: while models adopt insider tones in over 88 percent of US-contexted scripts on average, they disproportionately adopt mainly outsider stances for less dominant cultures. To resolve these biases, we propose 2 inference-time mitigation methods: a baseline prompt-based Fairness Intervention Pillars (FIP) method, and a structured Mitigation via Fairness Agents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent) introduces a self-reflection and rewriting loop based on fairness guidelines. (2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized agents: a Planner Agent(initial script generation), a Critique Agent (evaluates initial script against fairness pillars), and a Refinement Agent (incorporates feedback to produce a polished, unbiased script). Empirical results showcase the effectiveness of agent-based methods as a promising direction for mitigating biases in generative LLMs.</li>
</ul>

<h3>Title: Vision Transformers: the threat of realistic adversarial patches</h3>
<ul>
<li><strong>Authors: </strong>Kasper Cools, Clara Maathuis, Alexander M. van Oers, Claudia S. Hübner, Nikos Deligiannis, Marijke Vandewal, Geert De Cubber</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21084">https://arxiv.org/abs/2509.21084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21084">https://arxiv.org/pdf/2509.21084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21084]] Vision Transformers: the threat of realistic adversarial patches(https://arxiv.org/abs/2509.21084)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>The increasing reliance on machine learning systems has made their security a critical concern. Evasion attacks enable adversaries to manipulate the decision-making processes of AI systems, potentially causing security breaches or misclassification of targets. Vision Transformers (ViTs) have gained significant traction in modern machine learning due to increased 1) performance compared to Convolutional Neural Networks (CNNs) and 2) robustness against adversarial perturbations. However, ViTs remain vulnerable to evasion attacks, particularly to adversarial patches, unique patterns designed to manipulate AI classification systems. These vulnerabilities are investigated by designing realistic adversarial patches to cause misclassification in person vs. non-person classification tasks using the Creases Transformation (CT) technique, which adds subtle geometric distortions similar to those occurring naturally when wearing clothing. This study investigates the transferability of adversarial attack techniques used in CNNs when applied to ViT classification models. Experimental evaluation across four fine-tuned ViT models on a binary person classification task reveals significant vulnerability variations: attack success rates ranged from 40.04% (google/vit-base-patch16-224-in21k) to 99.97% (facebook/dino-vitb16), with google/vit-base-patch16-224 achieving 66.40% and facebook/dinov3-vitb16 reaching 65.17%. These results confirm the cross-architectural transferability of adversarial patches from CNNs to ViTs, with pre-training dataset scale and methodology strongly influencing model resilience to adversarial attacks.</li>
</ul>

<h3>Title: UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Guojun Lei, Rong Zhang, Chi Wang, Tianhang Liu, Hong Li, Zhiyuan Ma, Weiwei Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21086">https://arxiv.org/abs/2509.21086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21086">https://arxiv.org/pdf/2509.21086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21086]] UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition(https://arxiv.org/abs/2509.21086)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>We propose a novel architecture UniTransfer, which introduces both spatial and diffusion timestep decomposition in a progressive paradigm, achieving precise and controllable video concept transfer. Specifically, in terms of spatial decomposition, we decouple videos into three key components: the foreground subject, the background, and the motion flow. Building upon this decomposed formulation, we further introduce a dual-to-single-stream DiT-based architecture for supporting fine-grained control over different components in the videos. We also introduce a self-supervised pretraining strategy based on random masking to enhance the decomposed representation learning from large-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning paradigm, we further revisit the denoising diffusion process and propose a Chain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We decompose the denoising process into three stages of different granularity and leverage large language models (LLMs) for stage-specific instructions to guide the generation progressively. We also curate an animal-centric video dataset called OpenAnimal to facilitate the advancement and benchmarking of research in video concept transfer. Extensive experiments demonstrate that our method achieves high-quality and controllable video concept transfer across diverse reference images and scenes, surpassing existing baselines in both visual fidelity and editability. Web Page: this https URL</li>
</ul>

<h3>Title: GraphUniverse: Enabling Systematic Evaluation of Inductive Generalization</h3>
<ul>
<li><strong>Authors: </strong>Louis Van Langendonck, Guillermo Bernárdez, Nina Miolane, Pere Barlet-Ros</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21097">https://arxiv.org/abs/2509.21097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21097">https://arxiv.org/pdf/2509.21097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21097]] GraphUniverse: Enabling Systematic Evaluation of Inductive Generalization(https://arxiv.org/abs/2509.21097)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>A fundamental challenge in graph learning is understanding how models generalize to new, unseen graphs. While synthetic benchmarks offer controlled settings for analysis, existing approaches are confined to single-graph, transductive settings where models train and test on the same graph structure. Addressing this gap, we introduce GraphUniverse, a framework for generating entire families of graphs to enable the first systematic evaluation of inductive generalization at scale. Our core innovation is the generation of graphs with persistent semantic communities, ensuring conceptual consistency while allowing fine-grained control over structural properties like homophily and degree distributions. This enables crucial but underexplored robustness tests, such as performance under controlled distribution shifts. Benchmarking a wide range of architectures -- from GNNs to graph transformers and topological architectures -- reveals that strong transductive performance is a poor predictor of inductive generalization. Furthermore, we find that robustness to distribution shift is highly sensitive not only to model architecture choice but also to the initial graph regime (e.g., high vs. low homophily). Beyond benchmarking, GraphUniverse's flexibility and scalability can facilitate the development of robust and truly generalizable architectures -- including next-generation graph foundation models. An interactive demo is available at this https URL.</li>
</ul>

<h3>Title: VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception</h3>
<ul>
<li><strong>Authors: </strong>Ziang Yan, Xinhao Li, Yinan He, Zhengrong Yue, Xiangyu Zeng, Yali Wang, Yu Qiao, Limin Wang, Yi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21100">https://arxiv.org/abs/2509.21100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21100">https://arxiv.org/pdf/2509.21100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21100]] VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception(https://arxiv.org/abs/2509.21100)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Inducing reasoning in multimodal large language models (MLLMs) is critical for achieving human-level perception and understanding. Existing methods mainly leverage LLM reasoning to analyze parsed visuals, often limited by static perception stages. This paper introduces Visual Test-Time Scaling (VTTS), a novel approach to enhance MLLMs' reasoning via iterative perception during inference. VTTS mimics humans' hierarchical attention by progressively refining focus on high-confidence spatio-temporal regions, guided by updated textual predictions. Specifically, VTTS employs an Iterative Perception (ITP) mechanism, incorporating reinforcement learning with spatio-temporal supervision to optimize reasoning. To support this paradigm, we also present VTTS-80K, a dataset tailored for iterative perception. These designs allows a MLLM to enhance its performance by increasing its perceptual compute. Extensive experiments validate VTTS's effectiveness and generalization across diverse tasks and benchmarks. Our newly introduced Videochat-R1.5 model has achieved remarkable improvements, with an average increase of over 5\%, compared to robust baselines such as Qwen2.5VL-3B and -7B, across more than 15 benchmarks that encompass video conversation, video reasoning, and spatio-temporal perception.</li>
</ul>

<h3>Title: Mammo-CLIP Dissect: A Framework for Analysing Mammography Concepts in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Suaiba Amina Salahuddin, Teresa Dorszewski, Marit Almenning Martiniussen, Tone Hovda, Antonio Portaluri, Solveig Thrun, Michael Kampffmeyer, Elisabeth Wetzer, Kristoffer Wickstrøm, Robert Jenssen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21102">https://arxiv.org/abs/2509.21102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21102">https://arxiv.org/pdf/2509.21102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21102]] Mammo-CLIP Dissect: A Framework for Analysing Mammography Concepts in Vision-Language Models(https://arxiv.org/abs/2509.21102)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Understanding what deep learning (DL) models learn is essential for the safe deployment of artificial intelligence (AI) in clinical settings. While previous work has focused on pixel-based explainability methods, less attention has been paid to the textual concepts learned by these models, which may better reflect the reasoning used by clinicians. We introduce Mammo-CLIP Dissect, the first concept-based explainability framework for systematically dissecting DL vision models trained for mammography. Leveraging a mammography-specific vision-language model (Mammo-CLIP) as a "dissector," our approach labels neurons at specified layers with human-interpretable textual concepts and quantifies their alignment to domain knowledge. Using Mammo-CLIP Dissect, we investigate three key questions: (1) how concept learning differs between DL vision models trained on general image datasets versus mammography-specific datasets; (2) how fine-tuning for downstream mammography tasks affects concept specialisation; and (3) which mammography-relevant concepts remain underrepresented. We show that models trained on mammography data capture more clinically relevant concepts and align more closely with radiologists' workflows than models not trained on mammography data. Fine-tuning for task-specific classification enhances the capture of certain concept categories (e.g., benign calcifications) but can reduce coverage of others (e.g., density-related features), indicating a trade-off between specialisation and generalisation. Our findings show that Mammo-CLIP Dissect provides insights into how convolutional neural networks (CNNs) capture mammography-specific knowledge. By comparing models across training data and fine-tuning regimes, we reveal how domain-specific training and task-specific adaptation shape concept learning. Code and concept set are available: this https URL.</li>
</ul>

<h3>Title: PerHalluEval: Persian Hallucination Evaluation Benchmark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Hosseini, Kimia Hosseini, Shayan Bali, Zahra Zanjani, Saeedeh Momtazi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21104">https://arxiv.org/abs/2509.21104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21104">https://arxiv.org/pdf/2509.21104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21104]] PerHalluEval: Persian Hallucination Evaluation Benchmark for Large Language Models(https://arxiv.org/abs/2509.21104)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hallucination is a persistent issue affecting all large language Models (LLMs), particularly within low-resource languages such as Persian. PerHalluEval (Persian Hallucination Evaluation) is the first dynamic hallucination evaluation benchmark tailored for the Persian language. Our benchmark leverages a three-stage LLM-driven pipeline, augmented with human validation, to generate plausible answers and summaries regarding QA and summarization tasks, focusing on detecting extrinsic and intrinsic hallucinations. Moreover, we used the log probabilities of generated tokens to select the most believable hallucinated instances. In addition, we engaged human annotators to highlight Persian-specific contexts in the QA dataset in order to evaluate LLMs' performance on content specifically related to Persian culture. Our evaluation of 12 LLMs, including open- and closed-source models using PerHalluEval, revealed that the models generally struggle in detecting hallucinated Persian text. We showed that providing external knowledge, i.e., the original document for the summarization task, could mitigate hallucination partially. Furthermore, there was no significant difference in terms of hallucination when comparing LLMs specifically trained for Persian with others.</li>
</ul>

<h3>Title: BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback</h3>
<ul>
<li><strong>Authors: </strong>Hyunseo Kim, Sangam Lee, Kwangwook Seo, Dongha Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21106">https://arxiv.org/abs/2509.21106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21106">https://arxiv.org/pdf/2509.21106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21106]] BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback(https://arxiv.org/abs/2509.21106)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Search-augmented large language models (LLMs) have advanced information-seeking tasks by integrating retrieval into generation, reducing users' cognitive burden compared to traditional search systems. Yet they remain insufficient for fully addressing diverse user needs, which requires recognizing how the same query can reflect different intents across users and delivering information in preferred forms. While recent systems such as ChatGPT and Gemini attempt personalization by leveraging user histories, systematic evaluation of such personalization is under-explored. To address this gap, we propose BESPOKE, the realistic benchmark for evaluating personalization in search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting authentic chat and search histories directly from humans, and diagnostic, by pairing responses with fine-grained preference scores and feedback. The benchmark is constructed through long-term, deeply engaged human annotation, where human annotators contributed their own histories, authored queries with detailed information needs, and evaluated responses with scores and diagnostic feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key requirements for effective personalization in information-seeking tasks, providing a foundation for fine-grained evaluation of personalized search-augmented LLMs. Our code and data are available at this https URL.</li>
</ul>

<h3>Title: MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Sicheng Tao, Jungang Li, Yibo Yan, Junyan Zhang, Yubo Gao, Hanqian Li, ShuHang Xun, Yuxuan Fan, Hong Chen, Jianxiang He, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21113">https://arxiv.org/abs/2509.21113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21113">https://arxiv.org/pdf/2509.21113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21113]] MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning(https://arxiv.org/abs/2509.21113)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Video reasoning has emerged as a critical capability for multimodal large language models (MLLMs), requiring models to move beyond static perception toward coherent understanding of temporal dynamics in complex scenes. Yet existing MLLMs often exhibit process inconsistency, where intermediate reasoning drifts from video dynamics even when the final answer is correct, undermining interpretability and robustness. To address this issue, we introduce MOSS-ChatV, a reinforcement learning framework with a Dynamic Time Warping (DTW)-based process reward. This rule-based reward aligns reasoning traces with temporally grounded references, enabling efficient process supervision without auxiliary reward models. We further identify dynamic state prediction as a key measure of video reasoning and construct MOSS-Video, a benchmark with annotated reasoning traces, where the training split is used to fine-tune MOSS-ChatV and the held-out split is reserved for evaluation. MOSS-ChatV achieves 87.2\% on MOSS-Video (test) and improves performance on general video benchmarks such as MVBench and MMVU. The framework consistently yields gains across different architectures, including Qwen2.5-VL and Phi-2, confirming its broad applicability. Evaluations with GPT-4o-as-judge further show that MOSS-ChatV produces more consistent and stable reasoning traces.</li>
</ul>

<h3>Title: MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Guojun Lei, Chi Wang, Yikai Wang, Hong Li, Ying Song, Weiwei Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21119">https://arxiv.org/abs/2509.21119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21119">https://arxiv.org/pdf/2509.21119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21119]] MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation(https://arxiv.org/abs/2509.21119)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating videos guided by camera trajectories poses significant challenges in achieving consistency and generalizability, particularly when both camera and object motions are present. Existing approaches often attempt to learn these motions separately, which may lead to confusion regarding the relative motion between the camera and the objects. To address this challenge, we propose a novel approach that integrates both camera and object motions by converting them into the motion of corresponding pixels. Utilizing a stable diffusion network, we effectively learn reference motion maps in relation to the specified camera trajectory. These maps, along with an extracted semantic object prior, are then fed into an image-to-video network to generate the desired video that can accurately follow the designated camera trajectory while maintaining consistent object motions. Extensive experiments verify that our model outperforms SOTA methods by a large margin.</li>
</ul>

<h3>Title: Acoustic-based Gender Differentiation in Speech-aware Language Models</h3>
<ul>
<li><strong>Authors: </strong>Junhyuk Choi, Jihwan Seol, Nayeon Kim, Chanhee Cho, EunBin Cho, Bugeun Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21125">https://arxiv.org/abs/2509.21125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21125">https://arxiv.org/pdf/2509.21125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21125]] Acoustic-based Gender Differentiation in Speech-aware Language Models(https://arxiv.org/abs/2509.21125)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Speech-aware Language Models (SpeechLMs) have fundamentally transformed human-AI interaction by enabling voice-based communication, yet they may exhibit acoustic-based gender differentiation where identical questions lead to different responses based on the speaker's gender. This paper propose a new dataset that enables systematic analysis of this phenomenon, containing 9,208 speech samples across three categories: Gender-Independent, Gender-Stereotypical, and Gender-Dependent. We further evaluated LLaMA-Omni series and discovered a paradoxical pattern; while overall responses seems identical regardless of gender, the pattern is far from unbiased responses. Specifically, in Gender-Stereotypical questions, all models consistently exhibited male-oriented responses; meanwhile, in Gender-Dependent questions where gender differentiation would be contextually appropriate, models exhibited responses independent to gender instead. We also confirm that this pattern does not result from neutral options nor perceived gender of a voice. When we allow neutral response, models tends to respond neutrally also in Gender-Dependent questions. The paradoxical pattern yet retains when we applied gender neutralization methods on speech. Through comparison between SpeechLMs with corresponding backbone LLMs, we confirmed that these paradoxical patterns primarily stem from Whisper speech encoders, which generates male-oriented acoustic tokens. These findings reveal that current SpeechLMs may not successfully remove gender biases though they prioritized general fairness principles over contextual appropriateness, highlighting the need for more sophisticated techniques to utilize gender information properly in speech technology.</li>
</ul>

<h3>Title: EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense</h3>
<ul>
<li><strong>Authors: </strong>Wei Huang, De-Tian Chu, Lin-Yuan Bai, Wei Kang, Hai-Tao Zhang, Bo Li, Zhi-Mo Han, Jing Ge, Hai-Feng Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21129">https://arxiv.org/abs/2509.21129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21129">https://arxiv.org/pdf/2509.21129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21129]] EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense(https://arxiv.org/abs/2509.21129)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Modern email spam and phishing attacks have evolved far beyond keyword blacklists or simple heuristics. Adversaries now craft multi-modal campaigns that combine natural-language text with obfuscated URLs, forged headers, and malicious attachments, adapting their strategies within days to bypass filters. Traditional spam detection systems, which rely on static rules or single-modality models, struggle to integrate heterogeneous signals or to continuously adapt, leading to rapid performance degradation. We propose EvoMail, a self-evolving cognitive agent framework for robust detection of spam and phishing. EvoMail first constructs a unified heterogeneous email graph that fuses textual content, metadata (headers, senders, domains), and embedded resources (URLs, attachments). A Cognitive Graph Neural Network enhanced by a Large Language Model (LLM) performs context-aware reasoning across these sources to identify coordinated spam campaigns. Most critically, EvoMail engages in an adversarial self-evolution loop: a ''red-team'' agent generates novel evasion tactics -- such as character obfuscation or AI-generated phishing text -- while the ''blue-team'' detector learns from failures, compresses experiences into a memory module, and reuses them for future reasoning. Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam, SpamAssassin, and TREC) and synthetic adversarial variants demonstrate that EvoMail consistently outperforms state-of-the-art baselines in detection accuracy, adaptability to evolving spam tactics, and interpretability of reasoning traces. These results highlight EvoMail's potential as a resilient and explainable defense framework against next-generation spam and phishing threats.</li>
</ul>

<h3>Title: Sparse Representations Improve Adversarial Robustness of Neural Network Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Killian Steunou, Sigurd Saue, Théo Druilhe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21130">https://arxiv.org/abs/2509.21130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21130">https://arxiv.org/pdf/2509.21130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21130]] Sparse Representations Improve Adversarial Robustness of Neural Network Classifiers(https://arxiv.org/abs/2509.21130)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks perform remarkably well on image classification tasks but remain vulnerable to carefully crafted adversarial perturbations. This work revisits linear dimensionality reduction as a simple, data-adapted defense. We empirically compare standard Principal Component Analysis (PCA) with its sparse variant (SPCA) as front-end feature extractors for downstream classifiers, and we complement these experiments with a theoretical analysis. On the theory side, we derive exact robustness certificates for linear heads applied to SPCA features: for both $\ell_\infty$ and $\ell_2$ threat models (binary and multiclass), the certified radius grows as the dual norms of $W^\top u$ shrink, where $W$ is the projection and $u$ the head weights. We further show that for general (non-linear) heads, sparsity reduces operator-norm bounds through a Lipschitz composition argument, predicting lower input sensitivity. Empirically, with a small non-linear network after the projection, SPCA consistently degrades more gracefully than PCA under strong white-box and black-box attacks while maintaining competitive clean accuracy. Taken together, the theory identifies the mechanism (sparser projections reduce adversarial leverage) and the experiments verify that this benefit persists beyond the linear setting. Our code is available at this https URL.</li>
</ul>

<h3>Title: The Unwinnable Arms Race of AI Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Till Aczel, Lorenzo Vettor, Andreas Plesner, Roger Wattenhofer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21135">https://arxiv.org/abs/2509.21135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21135">https://arxiv.org/pdf/2509.21135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21135]] The Unwinnable Arms Race of AI Image Detection(https://arxiv.org/abs/2509.21135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid progress of image generative AI has blurred the boundary between synthetic and real images, fueling an arms race between generators and discriminators. This paper investigates the conditions under which discriminators are most disadvantaged in this competition. We analyze two key factors: data dimensionality and data complexity. While increased dimensionality often strengthens the discriminators ability to detect subtle inconsistencies, complexity introduces a more nuanced effect. Using Kolmogorov complexity as a measure of intrinsic dataset structure, we show that both very simple and highly complex datasets reduce the detectability of synthetic images; generators can learn simple datasets almost perfectly, whereas extreme diversity masks imperfections. In contrast, intermediate-complexity datasets create the most favorable conditions for detection, as generators fail to fully capture the distribution and their errors remain visible.</li>
</ul>

<h3>Title: Emerging Paradigms for Securing Federated Learning Systems</h3>
<ul>
<li><strong>Authors: </strong>Amr Akmal Abouelmagd, Amr Hilal</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21147">https://arxiv.org/abs/2509.21147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21147">https://arxiv.org/pdf/2509.21147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21147]] Emerging Paradigms for Securing Federated Learning Systems(https://arxiv.org/abs/2509.21147)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) facilitates collaborative model training while keeping raw data decentralized, making it a conduit for leveraging the power of IoT devices while maintaining privacy of the locally collected data. However, existing privacy- preserving techniques present notable hurdles. Methods such as Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential Privacy (DP) often incur high compu- tational costs and suffer from limited scalability. This survey examines emerging approaches that hold promise for enhancing both privacy and efficiency in FL, including Trusted Execution Environments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing (QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm Intelligence (SI). For each paradigm, we assess its relevance to the FL pipeline, outlining its strengths, limitations, and practical considerations. We conclude by highlighting open challenges and prospective research avenues, offering a detailed roadmap for advancing secure and scalable FL systems.</li>
</ul>

<h3>Title: LAVA: Explainability for Unsupervised Latent Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Ivan Stresec, Joana P. Gonçalves</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21149">https://arxiv.org/abs/2509.21149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21149">https://arxiv.org/pdf/2509.21149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21149]] LAVA: Explainability for Unsupervised Latent Embeddings(https://arxiv.org/abs/2509.21149)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Unsupervised black-box models can be drivers of scientific discovery, but remain difficult to interpret. Crucially, discovery hinges on understanding the model output, which is often a multi-dimensional latent embedding rather than a well-defined target. While explainability for supervised learning usually seeks to uncover how input features are used to predict a target, its unsupervised counterpart should relate input features to the structure of the learned latent space. Adaptations of supervised model explainability for unsupervised learning provide either single-sample or dataset-wide summary explanations. However, without automated strategies of relating similar samples to one another guided by their latent proximity, explanations remain either too fine-grained or too reductive to be meaningful. This is especially relevant for manifold learning methods that produce no mapping function, leaving us only with the relative spatial organization of their embeddings. We introduce Locality-Aware Variable Associations (LAVA), a post-hoc model-agnostic method designed to explain local embedding organization through its relationship with the input features. To achieve this, LAVA represents the latent space as a series of localities (neighborhoods) described in terms of correlations between the original features, and then reveals reoccurring patterns of correlations across the entire latent space. Based on UMAP embeddings of MNIST and a single-cell kidney dataset, we show that LAVA captures relevant feature associations, with visually and biologically relevant local patterns shared among seemingly distant regions of the latent spaces.</li>
</ul>

<h3>Title: CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Ruiyu Wang, Shizhao Sun, Weijian Ma, Jiang Bian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21150">https://arxiv.org/abs/2509.21150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21150">https://arxiv.org/pdf/2509.21150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21150]] CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific Tokenization(https://arxiv.org/abs/2509.21150)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Computer-Aided Design (CAD) is a foundational component of industrial prototyping, where models are defined not by raw coordinates but by construction sequences such as sketches and extrusions. This sequential structure enables both efficient prototype initialization and subsequent editing. Text-guided CAD prototyping, which unifies Text-to-CAD generation and CAD editing, has the potential to streamline the entire design pipeline. However, prior work has not explored this setting, largely because standard large language model (LLM) tokenizers decompose CAD sequences into natural-language word pieces, failing to capture primitive-level CAD semantics and hindering attention modules from modeling geometric structure. We conjecture that a multimodal tokenization strategy, aligned with CAD's primitive and structural nature, can provide more effective representations. To this end, we propose CAD-Tokenizer, a framework that represents CAD data with modality-specific tokens using a sequence-based VQ-VAE with primitive-level pooling and constrained decoding. This design produces compact, primitive-aware representations that align with CAD's structural nature. Applied to unified text-guided CAD prototyping, CAD-Tokenizer significantly improves instruction following and generation quality, achieving better quantitative and qualitative performance over both general-purpose LLMs and task-specific baselines.</li>
</ul>

<h3>Title: Retrieval over Classification: Integrating Relation Semantics for Multimodal Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Lei Hei, Tingjing Liao, Yingxin Pei, Yiyang Qi, Jiaqi Wang, Ruiting Li, Feiliang Ren</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21151">https://arxiv.org/abs/2509.21151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21151">https://arxiv.org/pdf/2509.21151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21151]] Retrieval over Classification: Integrating Relation Semantics for Multimodal Relation Extraction(https://arxiv.org/abs/2509.21151)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Relation extraction (RE) aims to identify semantic relations between entities in unstructured text. Although recent work extends traditional RE to multimodal scenarios, most approaches still adopt classification-based paradigms with fused multimodal features, representing relations as discrete labels. This paradigm has two significant limitations: (1) it overlooks structural constraints like entity types and positional cues, and (2) it lacks semantic expressiveness for fine-grained relation understanding. We propose \underline{R}etrieval \underline{O}ver \underline{C}lassification (ROC), a novel framework that reformulates multimodal RE as a retrieval task driven by relation semantics. ROC integrates entity type and positional information through a multimodal encoder, expands relation labels into natural language descriptions using a large language model, and aligns entity-relation pairs via semantic similarity-based contrastive learning. Experiments show that our method achieves state-of-the-art performance on the benchmark datasets MNRE and MORE and exhibits stronger robustness and interpretability.</li>
</ul>

<h3>Title: Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say</h3>
<ul>
<li><strong>Authors: </strong>Jacob Fein-Ashley, Dhruv Parikh, Rajgopal Kannan, Viktor Prasanna</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21164">https://arxiv.org/abs/2509.21164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21164">https://arxiv.org/pdf/2509.21164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21164]] Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say(https://arxiv.org/abs/2509.21164)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Open-source Large Language Models (LLMs) increasingly specialize by domain (e.g., math, code, general reasoning), motivating systems that leverage complementary strengths across models. Prior multi-LLM approaches either (i) route a query to one or a few experts and generate independently, (ii) aggregate outputs from each model via costly multi-turn exchanges, or (iii) fuse weights into a single model-typically requiring architectural homogeneity. We introduce Mixture of Thoughts (MoT), a simple method for latent-level collaboration among heterogeneous experts under a global routing scheme. For each query, a lightweight router selects top-$K$ experts and designates a primary expert; uniformly placed interaction layers project hidden states into a shared latent space where the primary expert performs cross-attention over its active (selected) peers. Pre-trained experts remain frozen; only the router and the lightweight interaction layers are trained with a novel joint training objective that improves both the expert selection and inter-expert collaboration. Across five in-distribution (ID) and three out-of-distribution (OOD) benchmarks, MoT surpasses the current routing and aggregation-based state-of-the-art, Avengers, by $+0.38\%$ and $+2.92\%$, respectively. Further, MoT significantly outperforms the best-performing single model. It achieves this with single-pass inference, runtime comparable to routing baselines, and none of the overheads of iterative aggregation. MoT offers a simple latent-space mechanism for combining heterogeneous LLMs, a practical step toward broader multi-LLM collaboration. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: A Unified Framework for Diffusion Model Unlearning with f-Divergence</h3>
<ul>
<li><strong>Authors: </strong>Nicola Novello, Federico Fontana, Luigi Cinque, Deniz Gunduz, Andrea M. Tonello</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21167">https://arxiv.org/abs/2509.21167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21167">https://arxiv.org/pdf/2509.21167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21167]] A Unified Framework for Diffusion Model Unlearning with f-Divergence(https://arxiv.org/abs/2509.21167)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Machine unlearning aims to remove specific knowledge from a trained model. While diffusion models (DMs) have shown remarkable generative capabilities, existing unlearning methods for text-to-image (T2I) models often rely on minimizing the mean squared error (MSE) between the output distribution of a target and an anchor concept. We show that this MSE-based approach is a special case of a unified $f$-divergence-based framework, in which any $f$-divergence can be utilized. We analyze the benefits of using different $f$-divergences, that mainly impact the convergence properties of the algorithm and the quality of unlearning. The proposed unified framework offers a flexible paradigm that allows to select the optimal divergence for a specific application, balancing different trade-offs between aggressive unlearning and concept preservation.</li>
</ul>

<h3>Title: Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Aymen Bouguerra, Daniel Montoya, Alexandra Gomez-Villa, Fabio Arnez, Chokri Mraidha</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21173">https://arxiv.org/abs/2509.21173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21173">https://arxiv.org/pdf/2509.21173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21173]] Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy(https://arxiv.org/abs/2509.21173)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The powerful zero-shot generalization capabilities of vision-language models (VLMs) like CLIP have enabled new paradigms for safety-related tasks such as out-of-distribution (OOD) detection. However, additional aspects crucial for the computationally efficient and reliable deployment of CLIP are still overlooked. In particular, the impact of quantization on CLIP's performance beyond accuracy remains underexplored. This work presents a large-scale evaluation of quantization on CLIP models, assessing not only in-distribution accuracy but a comprehensive suite of reliability metrics and revealing counterintuitive results driven by pre-training source. We demonstrate that quantization consistently improves calibration for typically underconfident pre-trained models, while often degrading it for overconfident variants. Intriguingly, this degradation in calibration does not preclude gains in other reliability metrics; we find that OOD detection can still improve for these same poorly calibrated models. Furthermore, we identify specific quantization-aware training (QAT) methods that yield simultaneous gains in zero-shot accuracy, calibration, and OOD robustness, challenging the view of a strict efficiency-performance trade-off. These findings offer critical insights for navigating the multi-objective problem of deploying efficient, reliable, and robust VLMs by utilizing quantization beyond its conventional role.</li>
</ul>

<h3>Title: Who's Laughing Now? An Overview of Computational Humour Generation and Explanation</h3>
<ul>
<li><strong>Authors: </strong>Tyler Loakman, William Thorne, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21175">https://arxiv.org/abs/2509.21175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21175">https://arxiv.org/pdf/2509.21175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21175]] Who's Laughing Now? An Overview of Computational Humour Generation and Explanation(https://arxiv.org/abs/2509.21175)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The creation and perception of humour is a fundamental human trait, positioning its computational understanding as one of the most challenging tasks in natural language processing (NLP). As an abstract, creative, and frequently context-dependent construct, humour requires extensive reasoning to understand and create, making it a pertinent task for assessing the common-sense knowledge and reasoning abilities of modern large language models (LLMs). In this work, we survey the landscape of computational humour as it pertains to the generative tasks of creation and explanation. We observe that, despite the task of understanding humour bearing all the hallmarks of a foundational NLP task, work on generating and explaining humour beyond puns remains sparse, while state-of-the-art models continue to fall short of human capabilities. We bookend our literature survey by motivating the importance of computational humour processing as a subdiscipline of NLP and presenting an extensive discussion of future directions for research in the area that takes into account the subjective and ethically ambiguous nature of humour.</li>
</ul>

<h3>Title: Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy</h3>
<ul>
<li><strong>Authors: </strong>Tian Lan, Hao Duong Le, Jinbo Li, Wenjun He, Meng Wang, Chenghao Liu, Chen Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21190">https://arxiv.org/abs/2509.21190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21190">https://arxiv.org/pdf/2509.21190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21190]] Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy(https://arxiv.org/abs/2509.21190)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection (TSAD) is a critical task, but developing models that generalize to unseen data in a zero-shot manner remains a major challenge. Prevailing foundation models for TSAD predominantly rely on reconstruction-based objectives, which suffer from a fundamental objective mismatch: they struggle to identify subtle anomalies while often misinterpreting complex normal patterns, leading to high rates of false negatives and positives. To overcome these limitations, we introduce \texttt{TimeRCD}, a novel foundation model for TSAD built upon a new pre-training paradigm: Relative Context Discrepancy (RCD). Instead of learning to reconstruct inputs, \texttt{TimeRCD} is explicitly trained to identify anomalies by detecting significant discrepancies between adjacent time windows. This relational approach, implemented with a standard Transformer architecture, enables the model to capture contextual shifts indicative of anomalies that reconstruction-based methods often miss. To facilitate this paradigm, we develop a large-scale, diverse synthetic corpus with token-level anomaly labels, providing the rich supervisory signal necessary for effective pre-training. Extensive experiments demonstrate that \texttt{TimeRCD} significantly outperforms existing general-purpose and anomaly-specific foundation models in zero-shot TSAD across diverse datasets. Our results validate the superiority of the RCD paradigm and establish a new, effective path toward building robust and generalizable foundation models for time series anomaly detection.</li>
</ul>

<h3>Title: GEP: A GCG-Based method for extracting personally identifiable information from chatbots built on small language models</h3>
<ul>
<li><strong>Authors: </strong>Jieli Zhu, Vi Ngoc-Nha Tran</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21192">https://arxiv.org/abs/2509.21192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21192">https://arxiv.org/pdf/2509.21192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21192]] GEP: A GCG-Based method for extracting personally identifiable information from chatbots built on small language models(https://arxiv.org/abs/2509.21192)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Small language models (SLMs) become unprecedentedly appealing due to their approximately equivalent performance compared to large language models (LLMs) in certain fields with less energy and time consumption during training and inference. However, the personally identifiable information (PII) leakage of SLMs for downstream tasks has yet to be explored. In this study, we investigate the PII leakage of the chatbot based on SLM. We first finetune a new chatbot, i.e., ChatBioGPT based on the backbone of BioGPT using medical datasets Alpaca and HealthCareMagic. It shows a matchable performance in BERTscore compared with previous studies of ChatDoctor and ChatGPT. Based on this model, we prove that the previous template-based PII attacking methods cannot effectively extract the PII in the dataset for leakage detection under the SLM condition. We then propose GEP, which is a greedy coordinate gradient-based (GCG) method specifically designed for PII extraction. We conduct experimental studies of GEP and the results show an increment of up to 60$\times$ more leakage compared with the previous template-based methods. We further expand the capability of GEP in the case of a more complicated and realistic situation by conducting free-style insertion where the inserted PII in the dataset is in the form of various syntactic expressions instead of fixed templates, and GEP is still able to reveal a PII leakage rate of up to 4.53%.</li>
</ul>

<h3>Title: Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xiangru Tang, Wanghan Xu, Yujie Wang, Zijie Guo, Daniel Shao, Jiapeng Chen, Cixuan Zhang, Ziyi Wang, Lixin Zhang, Guancheng Wan, Wenlong Zhang, Lei Bai, Zhenfei Yin, Philip Torr, Hanrui Wang, Di Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21193">https://arxiv.org/abs/2509.21193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21193">https://arxiv.org/pdf/2509.21193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21193]] Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning(https://arxiv.org/abs/2509.21193)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently shown strong progress on scientific reasoning, yet two major bottlenecks remain. First, explicit retrieval fragments reasoning, imposing a hidden "tool tax" of extra tokens and steps. Second, multi-agent pipelines often dilute strong solutions by averaging across all candidates. We address these challenges with a unified framework that combines implicit retrieval and structured collaboration. At its foundation, a Monitor-based retrieval module operates at the token level, integrating external knowledge with minimal disruption to reasoning. On top of this substrate, Hierarchical Solution Refinement (HSR) iteratively designates each candidate as an anchor to be repaired by its peers, while Quality-Aware Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\% accuracy -- the highest reported to date, surpassing the strongest agent baseline by 13.4 points and leading frontier LLMs by up to 18.1 points, while simultaneously reducing token usage by 53.5\% and agent steps by 43.7\%. Results on SuperGPQA and TRQA confirm robustness across domains. Error analysis shows that reasoning failures and knowledge gaps co-occur in over 85\% of cases, while diversity analysis reveals a clear dichotomy: retrieval tasks benefit from solution variety, whereas reasoning tasks favor consensus. Together, these findings demonstrate how implicit augmentation and structured refinement overcome the inefficiencies of explicit tool use and uniform aggregation. Code is available at: this https URL.</li>
</ul>

<h3>Title: Differential-Integral Neural Operator for Long-Term Turbulence Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Hao Wu, Yuan Gao, Fan Xu, Fan Zhang, Qingsong Wen, Kun Wang, Xiaomeng Huang, Xian Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21196">https://arxiv.org/abs/2509.21196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21196">https://arxiv.org/pdf/2509.21196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21196]] Differential-Integral Neural Operator for Long-Term Turbulence Forecasting(https://arxiv.org/abs/2509.21196)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Accurately forecasting the long-term evolution of turbulence represents a grand challenge in scientific computing and is crucial for applications ranging from climate modeling to aerospace engineering. Existing deep learning methods, particularly neural operators, often fail in long-term autoregressive predictions, suffering from catastrophic error accumulation and a loss of physical fidelity. This failure stems from their inability to simultaneously capture the distinct mathematical structures that govern turbulent dynamics: local, dissipative effects and global, non-local interactions. In this paper, we propose the {\textbf{\underline{D}}}ifferential-{\textbf{\underline{I}}}ntegral {\textbf{\underline{N}}}eural {\textbf{\underline{O}}}perator (\method{}), a novel framework designed from a first-principles approach of operator decomposition. \method{} explicitly models the turbulent evolution through parallel branches that learn distinct physical operators: a local differential operator, realized by a constrained convolutional network that provably converges to a derivative, and a global integral operator, captured by a Transformer architecture that learns a data-driven global kernel. This physics-based decomposition endows \method{} with exceptional stability and robustness. Through extensive experiments on the challenging 2D Kolmogorov flow benchmark, we demonstrate that \method{} significantly outperforms state-of-the-art models in long-term forecasting. It successfully suppresses error accumulation over hundreds of timesteps, maintains high fidelity in both the vorticity fields and energy spectra, and establishes a new benchmark for physically consistent, long-range turbulence forecast.</li>
</ul>

<h3>Title: TABLET: A Large-Scale Dataset for Robust Visual Table Understanding</h3>
<ul>
<li><strong>Authors: </strong>Iñigo Alonso, Imanol Miranda, Eneko Agirre, Mirella Lapata</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21205">https://arxiv.org/abs/2509.21205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21205">https://arxiv.org/pdf/2509.21205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21205]] TABLET: A Large-Scale Dataset for Robust Visual Table Understanding(https://arxiv.org/abs/2509.21205)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While table understanding increasingly relies on pixel-only settings where tables are processed as visual representations, current benchmarks predominantly use synthetic renderings that lack the complexity and visual diversity of real-world tables. Additionally, existing visual table understanding (VTU) datasets offer fixed examples with single visualizations and pre-defined instructions, providing no access to underlying serialized data for reformulation. We introduce TABLET, a large-scale VTU dataset with 4 million examples across 20 tasks, grounded in 2 million unique tables where 88% preserve original visualizations. Each example includes paired image-HTML representations, comprehensive metadata, and provenance information linking back to the source datasets. Fine-tuning vision-language models like Qwen2.5-VL-7B on TABLET improves performance on seen and unseen VTU tasks while increasing robustness on real-world table visualizations. By preserving original visualizations and maintaining example traceability in a unified large-scale collection, TABLET establishes a foundation for robust training and extensible evaluation of future VTU models.</li>
</ul>

<h3>Title: CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained Corpus and Reasoning Analysis</h3>
<ul>
<li><strong>Authors: </strong>Xinzhe Xu, Liang Zhao, Hongshen Xu, Chen Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21208">https://arxiv.org/abs/2509.21208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21208">https://arxiv.org/pdf/2509.21208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21208]] CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained Corpus and Reasoning Analysis(https://arxiv.org/abs/2509.21208)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly tasked with analyzing legal texts and citing relevant statutes, yet their reliability is often compromised by general pre-training that ingests legal texts without specialized focus, obscuring the true depth of their legal knowledge. This paper introduces CLaw, a novel benchmark specifically engineered to meticulously evaluate LLMs on Chinese legal knowledge and its application in reasoning. CLaw comprises two key components: (1) a comprehensive, fine-grained corpus of all 306 Chinese national statutes, segmented to the subparagraph level and incorporating precise historical revision timesteps for rigorous recall evaluation (64,849 entries), and (2) a challenging set of 254 case-based reasoning instances derived from China Supreme Court curated materials to assess the practical application of legal knowledge. Our empirical evaluation reveals that most contemporary LLMs significantly struggle to faithfully reproduce legal provisions. As accurate retrieval and citation of legal provisions form the basis of legal reasoning, this deficiency critically undermines the reliability of their responses. We contend that achieving trustworthy legal reasoning in LLMs requires a robust synergy of accurate knowledge retrieval--potentially enhanced through supervised fine-tuning (SFT) or retrieval-augmented generation (RAG)--and strong general reasoning capabilities. This work provides an essential benchmark and critical insights for advancing domain-specific LLM reasoning, particularly within the complex legal sphere.</li>
</ul>

<h3>Title: Learning Conformal Explainers for Image Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Amr Alkhatib, Stephanie Lowry</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21209">https://arxiv.org/abs/2509.21209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21209">https://arxiv.org/pdf/2509.21209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21209]] Learning Conformal Explainers for Image Classifiers(https://arxiv.org/abs/2509.21209)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Feature attribution methods are widely used for explaining image-based predictions, as they provide feature-level insights that can be intuitively visualized. However, such explanations often vary in their robustness and may fail to faithfully reflect the reasoning of the underlying black-box model. To address these limitations, we propose a novel conformal prediction-based approach that enables users to directly control the fidelity of the generated explanations. The method identifies a subset of salient features that is sufficient to preserve the model's prediction, regardless of the information carried by the excluded features, and without demanding access to ground-truth explanations for calibration. Four conformity functions are proposed to quantify the extent to which explanations conform to the model's predictions. The approach is empirically evaluated using five explainers across six image datasets. The empirical results demonstrate that FastSHAP consistently outperforms the competing methods in terms of both fidelity and informational efficiency, the latter measured by the size of the explanation regions. Furthermore, the results reveal that conformity measures based on super-pixels are more effective than their pixel-wise counterparts.</li>
</ul>

<h3>Title: SGMem: Sentence Graph Memory for Long-Term Conversational Agents</h3>
<ul>
<li><strong>Authors: </strong>Yaxiong Wu, Yongyue Zhang, Sheng Liang, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21212">https://arxiv.org/abs/2509.21212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21212">https://arxiv.org/pdf/2509.21212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21212]] SGMem: Sentence Graph Memory for Long-Term Conversational Agents(https://arxiv.org/abs/2509.21212)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Long-term conversational agents require effective memory management to handle dialogue histories that exceed the context window of large language models (LLMs). Existing methods based on fact extraction or summarization reduce redundancy but struggle to organize and retrieve relevant information across different granularities of dialogue and generated memory. We introduce SGMem (Sentence Graph Memory), which represents dialogue as sentence-level graphs within chunked units, capturing associations across turn-, round-, and session-level contexts. By combining retrieved raw dialogue with generated memory such as summaries, facts and insights, SGMem supplies LLMs with coherent and relevant context for response generation. Experiments on LongMemEval and LoCoMo show that SGMem consistently improves accuracy and outperforms strong baselines in long-term conversational question answering.</li>
</ul>

<h3>Title: Go With The Flow: Churn-Tolerant Decentralized Training of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nikolay Blagoev, Bart Cox, Jérémie Decouchant, Lydia Y. Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21221">https://arxiv.org/abs/2509.21221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21221">https://arxiv.org/pdf/2509.21221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21221]] Go With The Flow: Churn-Tolerant Decentralized Training of Large Language Models(https://arxiv.org/abs/2509.21221)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, large language model</a></li>
<li><strong>Abstract: </strong>Motivated by the emergence of large language models (LLMs) and the importance of democratizing their training, we propose GWTF, the first crash tolerant practical decentralized training framework for LLMs. Differently from existing distributed and federated training frameworks, GWTF enables the efficient collaborative training of a LLM on heterogeneous clients that volunteer their resources. In addition, GWTF addresses node churn, i.e., clients joining or leaving the system at any time, and network instabilities, i.e., network links becoming unstable or unreliable. The core of GWTF is a novel decentralized flow algorithm that finds the most effective routing that maximizes the number of microbatches trained with the lowest possible delay. We extensively evaluate GWTF on GPT-like and LLaMa-like models and compare it against the prior art. Our results indicate that GWTF reduces the training time by up to 45% in realistic and challenging scenarios that involve heterogeneous client nodes distributed over 10 different geographic locations with a high node churn rate.</li>
</ul>

<h3>Title: Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Muxin Pu, Mei Kuan Lim, Chun Yong Chong, Chen Change Loy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21223">https://arxiv.org/abs/2509.21223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21223">https://arxiv.org/pdf/2509.21223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21223]] Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding(https://arxiv.org/abs/2509.21223)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Pre-training has proven effective for learning transferable features in sign language understanding (SLU) tasks. Recently, skeleton-based methods have gained increasing attention because they can robustly handle variations in subjects and backgrounds without being affected by appearance or environmental factors. Current SLU methods continue to face three key limitations: 1) weak semantic grounding, as models often capture low-level motion patterns from skeletal data but struggle to relate them to linguistic meaning; 2) imbalance between local details and global context, with models either focusing too narrowly on fine-grained cues or overlooking them for broader context; and 3) inefficient cross-modal learning, as constructing semantically aligned representations across modalities remains difficult. To address these, we propose Sigma, a unified skeleton-based SLU framework featuring: 1) a sign-aware early fusion mechanism that facilitates deep interaction between visual and textual modalities, enriching visual features with linguistic context; 2) a hierarchical alignment learning strategy that jointly maximises agreements across different levels of paired features from different modalities, effectively capturing both fine-grained details and high-level semantic relationships; and 3) a unified pre-training framework that combines contrastive learning, text matching and language modelling to promote semantic consistency and generalisation. Sigma achieves new state-of-the-art results on isolated sign language recognition, continuous sign language recognition, and gloss-free sign language translation on multiple benchmarks spanning different sign and spoken languages, demonstrating the impact of semantically informative pre-training and the effectiveness of skeletal data as a stand-alone solution for SLU.</li>
</ul>

<h3>Title: AbideGym: Turning Static RL Worlds into Adaptive Challenges</h3>
<ul>
<li><strong>Authors: </strong>Abi Aryan, Zac Liu, Aaron Childress</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21234">https://arxiv.org/abs/2509.21234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21234">https://arxiv.org/pdf/2509.21234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21234]] AbideGym: Turning Static RL Worlds into Adaptive Challenges(https://arxiv.org/abs/2509.21234)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Agents trained with reinforcement learning often develop brittle policies that fail when dynamics shift, a problem amplified by static benchmarks. AbideGym, a dynamic MiniGrid wrapper, introduces agent-aware perturbations and scalable complexity to enforce intra-episode adaptation. By exposing weaknesses in static policies and promoting resilience, AbideGym provides a modular, reproducible evaluation framework for advancing research in curriculum learning, continual learning, and robust generalization.</li>
</ul>

<h3>Title: Query-Centric Graph Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Yaxiong Wu, Jianyuan Bo, Yongyue Zhang, Sheng Liang, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21237">https://arxiv.org/abs/2509.21237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21237">https://arxiv.org/pdf/2509.21237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21237]] Query-Centric Graph Retrieval Augmented Generation(https://arxiv.org/abs/2509.21237)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Graph-based retrieval-augmented generation (RAG) enriches large language models (LLMs) with external knowledge for long-context understanding and multi-hop reasoning, but existing methods face a granularity dilemma: fine-grained entity-level graphs incur high token costs and lose context, while coarse document-level graphs fail to capture nuanced relations. We introduce QCG-RAG, a query-centric graph RAG framework that enables query-granular indexing and multi-hop chunk retrieval. Our query-centric approach leverages Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with controllable granularity, improving graph quality and interpretability. A tailored multi-hop retrieval mechanism then selects relevant chunks via the generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG consistently outperforms prior chunk-based and graph-based RAG methods in question answering accuracy, establishing a new paradigm for multi-hop reasoning.</li>
</ul>

<h3>Title: Tree Search for LLM Agent Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, Liaoni Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21240">https://arxiv.org/abs/2509.21240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21240">https://arxiv.org/pdf/2509.21240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21240]] Tree Search for LLM Agent Reinforcement Learning(https://arxiv.org/abs/2509.21240)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.</li>
</ul>

<h3>Title: Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven Framework</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Wang, Ziyang Chen, Md Faisal Kabir</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21241">https://arxiv.org/abs/2509.21241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21241">https://arxiv.org/pdf/2509.21241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21241]] Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven Framework(https://arxiv.org/abs/2509.21241)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The widespread adoption of Low-Rank Adaptation (LoRA) has enabled large language models (LLMs) to acquire domain-specific knowledge with remarkable efficiency. However, understanding how such a fine-tuning mechanism alters a model's structural reasoning and semantic behavior remains an open challenge. This work introduces a novel framework that explains fine-tuned LLMs via counterfactuals grounded in knowledge graphs. Specifically, we construct BioToolKG, a domain-specific heterogeneous knowledge graph in bioinformatics tools and design a counterfactual-based fine-tuned LLMs explainer (CFFTLLMExplainer) that learns soft masks over graph nodes and edges to generate minimal structural perturbations that induce maximum semantic divergence. Our method jointly optimizes structural sparsity and semantic divergence while enforcing interpretability preserving constraints such as entropy regularization and edge smoothness. We apply this framework to a fine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes the model's structural dependencies and aligns with LoRA-induced parameter shifts. This work provides new insights into the internal mechanisms of fine-tuned LLMs and highlights counterfactual graphs as a potential tool for interpretable AI.</li>
</ul>

<h3>Title: Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets</h3>
<ul>
<li><strong>Authors: </strong>Team Hunyuan3D: Bowen Zhang, Chunchao Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jingwei Huang, Junlin Yu, Kunhong Li, Linus, Penghao Wang, Qingxiang Lin, Sicong Liu, Xianghui Yang, Yixuan Tang, Yunfei Zhao, Zeqiang Lai, Zhihao Liang, Zibo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21245">https://arxiv.org/abs/2509.21245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21245">https://arxiv.org/pdf/2509.21245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21245]] Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets(https://arxiv.org/abs/2509.21245)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D-native generative models have accelerated asset creation for games, film, and design. However, most methods still rely primarily on image or text conditioning and lack fine-grained, cross-modal controls, which limits controllability and practical adoption. To address this gap, we present Hunyuan3D-Omni, a unified framework for fine-grained, controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images, Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose priors as conditioning signals, enabling precise control over geometry, topology, and pose. Instead of separate heads for each modality, our model unifies all signals in a single cross-modal architecture. We train with a progressive, difficulty-aware sampling strategy that selects one control modality per example and biases sampling toward harder signals (e.g., skeletal pose) while downweighting easier ones (e.g., point clouds), encouraging robust multi-modal fusion and graceful handling of missing inputs. Experiments show that these additional controls improve generation accuracy, enable geometry-aware transformations, and increase robustness for production workflows.</li>
</ul>

<h3>Title: Learning to Look: Cognitive Attention Alignment with Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ryan L. Yang, Dipkamal Bhusal, Nidhi Rastogi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21247">https://arxiv.org/abs/2509.21247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21247">https://arxiv.org/pdf/2509.21247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21247]] Learning to Look: Cognitive Attention Alignment with Vision-Language Models(https://arxiv.org/abs/2509.21247)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Convolutional Neural Networks (CNNs) frequently "cheat" by exploiting superficial correlations, raising concerns about whether they make predictions for the right reasons. Inspired by cognitive science, which highlights the role of attention in robust human perception, recent methods have sought to guide model attention using concept-based supervision and explanation regularization. However, these techniques depend on labor-intensive, expert-provided annotations, limiting their scalability. We propose a scalable framework that leverages vision-language models to automatically generate semantic attention maps using natural language prompts. By introducing an auxiliary loss that aligns CNN attention with these language-guided maps, our approach promotes more reliable and cognitively plausible decision-making without manual annotation. Experiments on challenging datasets, ColoredMNIST and DecoyMNIST, show that our method achieves state-of-the-art performance on ColorMNIST and remains competitive with annotation-heavy baselines on DecoyMNIST, demonstrating improved generalization, reduced shortcut reliance, and model attention that better reflects human intuition.</li>
</ul>

<h3>Title: Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations</h3>
<ul>
<li><strong>Authors: </strong>Zhijian Yang, Noel DSouza, Istvan Megyeri, Xiaojian Xu, Amin Honarmandi Shandiz, Farzin Haddadpour, Krisztian Koos, Laszlo Rusko, Emanuele Valeriano, Bharadwaj Swaninathan, Lei Wu, Parminder Bhatia, Taha Kass-Hout, Erhan Bas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21249">https://arxiv.org/abs/2509.21249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21249">https://arxiv.org/pdf/2509.21249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21249]] Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations(https://arxiv.org/abs/2509.21249)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Magnetic Resonance Imaging (MRI) is a critical medical imaging modality in clinical diagnosis and research, yet its complexity and heterogeneity pose challenges for automated analysis, particularly in scalable and generalizable machine learning applications. While foundation models have revolutionized natural language and vision tasks, their application to MRI remains limited due to data scarcity and narrow anatomical focus. In this work, we present Decipher-MR, a 3D MRI-specific vision-language foundation model trained on a large-scale dataset comprising 200,000 MRI series from over 22,000 studies spanning diverse anatomical regions, sequences, and pathologies. Decipher-MR integrates self-supervised vision learning with report-guided text supervision to build robust, generalizable representations, enabling effective adaptation across broad applications. To enable robust and diverse clinical tasks with minimal computational overhead, Decipher-MR supports a modular design that enables tuning of lightweight, task-specific decoders attached to a frozen pretrained encoder. Following this setting, we evaluate Decipher-MR across diverse benchmarks including disease classification, demographic prediction, anatomical localization, and cross-modal retrieval, demonstrating consistent performance gains over existing foundation models and task-specific approaches. Our results establish Decipher-MR as a scalable and versatile foundation for MRI-based AI, facilitating efficient development across clinical and research domains.</li>
</ul>

<h3>Title: Federated Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Zifan Wang, Anqi Dong, Mahmoud Selim, Michael M. Zavlanos, Karl H. Johansson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21250">https://arxiv.org/abs/2509.21250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21250">https://arxiv.org/pdf/2509.21250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21250]] Federated Flow Matching(https://arxiv.org/abs/2509.21250)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, generative</a></li>
<li><strong>Abstract: </strong>Data today is decentralized, generated and stored across devices and institutions where privacy, ownership, and regulation prevent centralization. This motivates the need to train generative models directly from distributed data locally without central aggregation. In this paper, we introduce Federated Flow Matching (FFM), a framework for training flow matching models under privacy constraints. Specifically, we first examine FFM-vanilla, where each client trains locally with independent source and target couplings, preserving privacy but yielding curved flows that slow inference. We then develop FFM-LOT, which employs local optimal transport couplings to improve straightness within each client but lacks global consistency under heterogeneous data. Finally, we propose FFM-GOT, a federated strategy based on the semi-dual formulation of optimal transport, where a shared global potential function coordinates couplings across clients. Experiments on synthetic and image datasets show that FFM enables privacy-preserving training while enhancing both the flow straightness and sample quality in federated settings, with performance comparable to the centralized baseline.</li>
</ul>

<h3>Title: Instruction-tuned Self-Questioning Framework for Multimodal Reasoning</h3>
<ul>
<li><strong>Authors: </strong>You-Won Jang, Yu-Jung Heo, Jaeseok Kim, Minsu Lee, Du-Seong Chang, Byoung-Tak Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21251">https://arxiv.org/abs/2509.21251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21251">https://arxiv.org/pdf/2509.21251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21251]] Instruction-tuned Self-Questioning Framework for Multimodal Reasoning(https://arxiv.org/abs/2509.21251)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The field of vision-language understanding has been actively researched in recent years, thanks to the development of Large Language Models~(LLMs). However, it still needs help with problems requiring multi-step reasoning, even for very simple questions. Recent studies adopt LLMs to tackle this problem by iteratively generating sub-questions and answers. However, there are disadvantages such as 1) the fine-grained visual contents of images are not available using LLMs that cannot read visual information, 2) internal mechanisms are inaccessible and difficult to reproduce by using black-box LLMs. To solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP, which improves inference performance by generating image-aware informative sub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists of a Questioner, Answerer, and Reasoner that share the same architecture. Questioner and Answerer generate sub-questions and sub-answers to help infer the main-question, and Reasoner performs reasoning on the main-question considering the generated sub-question information. Our experiments show that the proposed method SQ-InstructBLIP, which uses the generated sub-questions as additional information when solving the VQA task, performs more accurate reasoning than the previous works.</li>
</ul>

<h3>Title: humancompatible.train: Implementing Optimization Algorithms for Stochastically-Constrained Stochastic Optimization Problems</h3>
<ul>
<li><strong>Authors: </strong>Andrii Kliachkin, Jana Lepšová, Gilles Bareilles, Jakub Mareček</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21254">https://arxiv.org/abs/2509.21254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21254">https://arxiv.org/pdf/2509.21254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21254]] humancompatible.train: Implementing Optimization Algorithms for Stochastically-Constrained Stochastic Optimization Problems(https://arxiv.org/abs/2509.21254)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>There has been a considerable interest in constrained training of deep neural networks (DNNs) recently for applications such as fairness and safety. Several toolkits have been proposed for this task, yet there is still no industry standard. We present this http URL (this https URL), an easily-extendable PyTorch-based Python package for training DNNs with stochastic constraints. We implement multiple previously unimplemented algorithms for stochastically constrained stochastic optimization. We demonstrate the toolkit use by comparing two algorithms on a deep learning task with fairness constraints.</li>
</ul>

<h3>Title: Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Seyed Amir Kasaei, Mohammad Hossein Rohban</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21257">https://arxiv.org/abs/2509.21257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21257">https://arxiv.org/pdf/2509.21257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21257]] Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation(https://arxiv.org/abs/2509.21257)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In language and vision-language models, hallucination is broadly understood as content generated from a model's prior knowledge or biases rather than from the given input. While this phenomenon has been studied in those domains, it has not been clearly framed for text-to-image (T2I) generative models. Existing evaluations mainly focus on alignment, checking whether prompt-specified elements appear, but overlook what the model generates beyond the prompt. We argue for defining hallucination in T2I as bias-driven deviations and propose a taxonomy with three categories: attribute, relation, and object hallucinations. This framing introduces an upper bound for evaluation and surfaces hidden biases, providing a foundation for richer assessment of T2I models.</li>
</ul>

<h3>Title: Every Subtlety Counts: Fine-grained Person Independence Micro-Action Recognition via Distributionally Robust Optimization</h3>
<ul>
<li><strong>Authors: </strong>Feng-Qi Cui, Jinyang Huang, Anyang Tong, Ziyu Jia, Jie Zhang, Zhi Liu, Dan Guo, Jianwei Lu, Meng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21261">https://arxiv.org/abs/2509.21261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21261">https://arxiv.org/pdf/2509.21261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21261]] Every Subtlety Counts: Fine-grained Person Independence Micro-Action Recognition via Distributionally Robust Optimization(https://arxiv.org/abs/2509.21261)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Micro-action Recognition is vital for psychological assessment and human-computer interaction. However, existing methods often fail in real-world scenarios because inter-person variability causes the same action to manifest differently, hindering robust generalization. To address this, we propose the Person Independence Universal Micro-action Recognition Framework, which integrates Distributionally Robust Optimization principles to learn person-agnostic representations. Our framework contains two plug-and-play components operating at the feature and loss levels. At the feature level, the Temporal-Frequency Alignment Module normalizes person-specific motion characteristics with a dual-branch design: the temporal branch applies Wasserstein-regularized alignment to stabilize dynamic trajectories, while the frequency branch introduces variance-guided perturbations to enhance robustness against person-specific spectral differences. A consistency-driven fusion mechanism integrates both branches. At the loss level, the Group-Invariant Regularized Loss partitions samples into pseudo-groups to simulate unseen person-specific distributions. By up-weighting boundary cases and regularizing subgroup variance, it forces the model to generalize beyond easy or frequent samples, thus enhancing robustness to difficult variations. Experiments on the large-scale MA-52 dataset demonstrate that our framework outperforms existing methods in both accuracy and robustness, achieving stable generalization under fine-grained conditions.</li>
</ul>

<h3>Title: Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication</h3>
<ul>
<li><strong>Authors: </strong>Evgeny Kaskov, Elizaveta Petrova, Petr Surovtsev, Anna Kostikova, Ilya Mistiurin, Alexander Kapitanov, Alexander Nagaev</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21262">https://arxiv.org/abs/2509.21262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21262">https://arxiv.org/pdf/2509.21262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21262]] Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication(https://arxiv.org/abs/2509.21262)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Homonyms are words with identical spelling but distinct meanings, which pose challenges for many generative models. When a homonym appears in a prompt, diffusion models may generate multiple senses of the word simultaneously, which is known as homonym duplication. This issue is further complicated by an Anglocentric bias, which includes an additional translation step before the text-to-image model pipeline. As a result, even words that are not homonymous in the original language may become homonyms and lose their meaning after translation into English. In this paper, we introduce a method for measuring duplication rates and conduct evaluations of different diffusion models using both automatic evaluation utilizing Vision-Language Models (VLM) and human evaluation. Additionally, we investigate methods to mitigate the homonym duplication problem through prompt expansion, demonstrating that this approach also effectively reduces duplication related to Anglocentric bias. The code for the automatic evaluation pipeline is publicly available.</li>
</ul>

<h3>Title: Dense Semantic Matching with VGGT Prior</h3>
<ul>
<li><strong>Authors: </strong>Songlin Yang, Tianyi Wei, Yushi Lan, Zeqi Xiao, Anyi Rao, Xingang Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21263">https://arxiv.org/abs/2509.21263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21263">https://arxiv.org/pdf/2509.21263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21263]] Dense Semantic Matching with VGGT Prior(https://arxiv.org/abs/2509.21263)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Semantic matching aims to establish pixel-level correspondences between instances of the same category and represents a fundamental task in computer vision. Existing approaches suffer from two limitations: (i) Geometric Ambiguity: Their reliance on 2D foundation model features (e.g., Stable Diffusion, DINO) often fails to disambiguate symmetric structures, requiring extra fine-tuning yet lacking generalization; (ii) Nearest-Neighbor Rule: Their pixel-wise matching ignores cross-image invisibility and neglects manifold preservation. These challenges call for geometry-aware pixel descriptors and holistic dense correspondence mechanisms. Inspired by recent advances in 3D geometric foundation models, we turn to VGGT, which provides geometry-grounded features and holistic dense matching capabilities well aligned with these needs. However, directly transferring VGGT is challenging, as it was originally designed for geometry matching within cross views of a single instance, misaligned with cross-instance semantic matching, and further hindered by the scarcity of dense semantic annotations. To address this, we propose an approach that (i) retains VGGT's intrinsic strengths by reusing early feature stages, fine-tuning later ones, and adding a semantic head for bidirectional correspondences; and (ii) adapts VGGT to the semantic matching scenario under data scarcity through cycle-consistent training strategy, synthetic data augmentation, and progressive training recipe with aliasing artifact mitigation. Extensive experiments demonstrate that our approach achieves superior geometry awareness, matching reliability, and manifold preservation, outperforming previous baselines.</li>
</ul>

<h3>Title: LLM Output Homogenization is Task Dependent</h3>
<ul>
<li><strong>Authors: </strong>Shomik Jain, Jack Lanchantin, Maximilian Nickel, Karen Ullrich, Ashia Wilson, Jamelle Watson-Daniels</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21267">https://arxiv.org/abs/2509.21267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21267">https://arxiv.org/pdf/2509.21267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21267]] LLM Output Homogenization is Task Dependent(https://arxiv.org/abs/2509.21267)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A large language model can be less helpful if it exhibits output response homogenization. But whether two responses are considered homogeneous, and whether such homogenization is problematic, both depend on the task category. For instance, in objective math tasks, we often expect no variation in the final answer but anticipate variation in the problem-solving strategy. Whereas, for creative writing tasks, we may expect variation in key narrative components (e.g. plot, genre, setting, etc), beyond the vocabulary or embedding diversity produced by temperature-sampling. Previous work addressing output homogenization often fails to conceptualize diversity in a task-dependent way. We address this gap in the literature directly by making the following contributions. (1) We present a task taxonomy comprised of eight task categories that each have distinct conceptualizations of output homogenization. (2) We introduce task-anchored functional diversity to better evaluate output homogenization. (3) We propose a task-anchored sampling technique that increases functional diversity for task categories where homogenization is undesired, while preserving homogenization where it is desired. (4) We challenge the perceived existence of a diversity-quality trade-off by increasing functional diversity while maintaining response quality. Overall, we demonstrate how task dependence improves the evaluation and mitigation of output homogenization.</li>
</ul>

<h3>Title: LLMTrace: A Corpus for Classification and Fine-Grained Localization of AI-Written Text</h3>
<ul>
<li><strong>Authors: </strong>Irina Tolstykh, Aleksandra Tsybina, Sergey Yakubson, Maksim Kuprashevich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21269">https://arxiv.org/abs/2509.21269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21269">https://arxiv.org/pdf/2509.21269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21269]] LLMTrace: A Corpus for Classification and Fine-Grained Localization of AI-Written Text(https://arxiv.org/abs/2509.21269)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The widespread use of human-like text from Large Language Models (LLMs) necessitates the development of robust detection systems. However, progress is limited by a critical lack of suitable training data; existing datasets are often generated with outdated models, are predominantly in English, and fail to address the increasingly common scenario of mixed human-AI authorship. Crucially, while some datasets address mixed authorship, none provide the character-level annotations required for the precise localization of AI-generated segments within a text. To address these gaps, we introduce LLMTrace, a new large-scale, bilingual (English and Russian) corpus for AI-generated text detection. Constructed using a diverse range of modern proprietary and open-source LLMs, our dataset is designed to support two key tasks: traditional full-text binary classification (human vs. AI) and the novel task of AI-generated interval detection, facilitated by character-level annotations. We believe LLMTrace will serve as a vital resource for training and evaluating the next generation of more nuanced and practical AI detection models. The project page is available at \href{this https URL}{iitolstykh/LLMTrace}.</li>
</ul>

<h3>Title: A Sentinel-3 foundation model for ocean colour</h3>
<ul>
<li><strong>Authors: </strong>Geoffrey Dawson, Remy Vandaele, Andrew Taylor, David Moffat, Helen Tamura-Wicks, Sarah Jackson, Rosie Lickorish, Paolo Fraccaro, Hywel Williams, Chunbo Luo, Anne Jones</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21273">https://arxiv.org/abs/2509.21273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21273">https://arxiv.org/pdf/2509.21273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21273]] A Sentinel-3 foundation model for ocean colour(https://arxiv.org/abs/2509.21273)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI) Foundation models (FMs), pre-trained on massive unlabelled datasets, have the potential to drastically change AI applications in ocean science, where labelled data are often sparse and expensive to collect. In this work, we describe a new foundation model using the Prithvi-EO Vision Transformer architecture which has been pre-trained to reconstruct data from the Sentinel-3 Ocean and Land Colour Instrument (OLCI). We evaluate the model by fine-tuning on two downstream marine earth observation tasks. We first assess model performance compared to current baseline models used to quantify chlorophyll concentration. We then evaluate the FMs ability to refine remote sensing-based estimates of ocean primary production. Our results demonstrate the utility of self-trained FMs for marine monitoring, in particular for making use of small amounts of high quality labelled data and in capturing detailed spatial patterns of ocean colour whilst matching point observations. We conclude that this new generation of geospatial AI models has the potential to provide more robust, data-driven insights into ocean ecosystems and their role in global climate processes.</li>
</ul>

<h3>Title: Does FLUX Already Know How to Perform Physically Plausible Image Composition?</h3>
<ul>
<li><strong>Authors: </strong>Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, Adams Wai-Kin Kong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21278">https://arxiv.org/abs/2509.21278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21278">https://arxiv.org/pdf/2509.21278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21278]] Does FLUX Already Know How to Perform Physically Plausible Image Composition?(https://arxiv.org/abs/2509.21278)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image composition aims to seamlessly insert a user-specified object into a new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack a framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, a training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Degradation-suppression guidance and adaptive background blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication.</li>
</ul>

<h3>Title: It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL</h3>
<ul>
<li><strong>Authors: </strong>Madeleine Dwyer, Adam Sobey, Adriane Chapman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21282">https://arxiv.org/abs/2509.21282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21282">https://arxiv.org/pdf/2509.21282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21282]] It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL(https://arxiv.org/abs/2509.21282)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training large language models (LLMs) with reinforcement learning (RL) methods such as PPO and GRPO commonly relies on ratio clipping to stabilise updates. While effective at preventing instability, clipping discards information and introduces gradient discontinuities. We propose Probability Smoothing Policy Optimisation (PSPO), which smooths the current policy's probabilities toward the old (behaviour) policy before computing the importance ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient signal, while interpolation toward the old policy creates a soft trust region that discourages large, destabilising updates, with formal guarantees. We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO (single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar performance but improves the reasoning leading to clearer and more concise responses which are more logical. Compared to clipped GRPO, GR-PSPO substantially improves performance both the 0.5B and 1.5B models, with a boost of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).</li>
</ul>

<h3>Title: Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21284">https://arxiv.org/abs/2509.21284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21284">https://arxiv.org/pdf/2509.21284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21284]] Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond(https://arxiv.org/abs/2509.21284)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Existing research indicates that the output of Chain-of-Thought (CoT) is significantly affected by input perturbations. Although many methods aim to mitigate such impact by optimizing prompts, a theoretical explanation of how these perturbations influence CoT outputs remains an open area of research. This gap limits our in-depth understanding of how input perturbations propagate during the reasoning process and hinders further improvements in prompt optimization methods. Therefore, in this paper, we theoretically analyze the effect of input perturbations on the fluctuation of CoT outputs. We first derive an upper bound for input perturbations under the condition that the output fluctuation is within an acceptable range, based on which we prove that: (i) This upper bound is positively correlated with the number of reasoning steps in the CoT; (ii) Even an infinitely long reasoning process cannot eliminate the impact of input perturbations. We then apply these conclusions to the Linear Self-Attention (LSA) model, which can be viewed as a simplified version of the Transformer. For the LSA model, we prove that the upper bound for input perturbation is negatively correlated with the norms of the input embedding and hidden state vectors. To validate this theoretical analysis, we conduct experiments on three mainstream datasets and four mainstream models. The experimental results align with our theoretical analysis, empirically demonstrating the correctness of our findings.</li>
</ul>

<h3>Title: DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Kin Ian Lo, Hala Hawashin, Mina Abbaszadeh, Tilen Limback-Stokin, Hadi Wazni, Mehrnoosh Sadrzadeh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21287">https://arxiv.org/abs/2509.21287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21287">https://arxiv.org/pdf/2509.21287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21287]] DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding(https://arxiv.org/abs/2509.21287)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent vision-language models excel at large-scale image-text alignment but often neglect the compositional structure of language, leading to failures on tasks that hinge on word order and predicate-argument structure. We introduce DisCoCLIP, a multimodal encoder that combines a frozen CLIP vision transformer with a novel tensor network text encoder that explicitly encodes syntactic structure. Sentences are parsed with a Combinatory Categorial Grammar parser to yield distributional word tensors whose contractions mirror the sentence's grammatical derivation. To keep the model efficient, high-order tensors are factorized with tensor decompositions, reducing parameter count from tens of millions to under one million. Trained end-to-end with a self-supervised contrastive loss, DisCoCLIP markedly improves sensitivity to verb semantics and word order: it raises CLIP's SVO-Probes verb accuracy from 77.6% to 82.4%, boosts ARO attribution and relation scores by over 9% and 4%, and achieves 93.7% on a newly introduced SVO-Swap benchmark. These results demonstrate that embedding explicit linguistic structure via tensor networks yields interpretable, parameter-efficient representations that substantially improve compositional reasoning in vision-language tasks.</li>
</ul>

<h3>Title: Optimal Robust Recourse with $L^p$-Bounded Model Change</h3>
<ul>
<li><strong>Authors: </strong>Phone Kyaw, Kshitij Kayastha, Shahin Jabbari</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21293">https://arxiv.org/abs/2509.21293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21293">https://arxiv.org/pdf/2509.21293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21293]] Optimal Robust Recourse with $L^p$-Bounded Model Change(https://arxiv.org/abs/2509.21293)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recourse provides individuals who received undesirable labels (e.g., denied a loan) from algorithmic decision-making systems with a minimum-cost improvement suggestion to achieve the desired outcome. However, in practice, models often get updated to reflect changes in the data distribution or environment, invalidating the recourse recommendations (i.e., following the recourse will not lead to the desirable outcome). The robust recourse literature addresses this issue by providing a framework for computing recourses whose validity is resilient to slight changes in the model. However, since the optimization problem of computing robust recourse is non-convex (even for linear models), most of the current approaches do not have any theoretical guarantee on the optimality of the recourse. Recent work by Kayastha et. al. provides the first provably optimal algorithm for robust recourse with respect to generalized linear models when the model changes are measured using the $L^{\infty}$ norm. However, using the $L^{\infty}$ norm can lead to recourse solutions with a high price. To address this shortcoming, we consider more constrained model changes defined by the $L^p$ norm, where $p\geq 1$ but $p\neq \infty$, and provide a new algorithm that provably computes the optimal robust recourse for generalized linear models. Empirically, for both linear and non-linear models, we demonstrate that our algorithm achieves a significantly lower price of recourse (up to several orders of magnitude) compared to prior work and also exhibits a better trade-off between the implementation cost of recourse and its validity. Our empirical analysis also illustrates that our approach provides more sparse recourses compared to prior work and remains resilient to post-processing approaches that guarantee feasibility.</li>
</ul>

<h3>Title: The role of synthetic data in Multilingual, Multi-cultural AI systems: Lessons from Indic Languages</h3>
<ul>
<li><strong>Authors: </strong>Pranjal A. Chitale, Varun Gumma, Sanchit Ahuja, Prashant Kodali, Manan Uppadhyay, Deepthi Sudharsan, Sunayana Sitaram</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21294">https://arxiv.org/abs/2509.21294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21294">https://arxiv.org/pdf/2509.21294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21294]] The role of synthetic data in Multilingual, Multi-cultural AI systems: Lessons from Indic Languages(https://arxiv.org/abs/2509.21294)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Developing AI systems that operate effectively across languages while remaining culturally grounded is a long-standing challenge, particularly in low-resource settings. Synthetic data provides a promising avenue, yet its effectiveness in multilingual and multicultural contexts remains underexplored. We investigate the creation and impact of synthetic, culturally contextualized datasets for Indian languages through a bottom-up generation strategy that prompts large open-source LLMs (>= 235B parameters) to ground data generation in language-specific Wikipedia content. This approach complements the dominant top-down paradigm of translating synthetic datasets from high-resource languages such as English. We introduce Updesh, a high-quality large-scale synthetic instruction-following dataset comprising 9.5M data points across 13 Indian languages, encompassing diverse reasoning and generative tasks with an emphasis on long-context, multi-turn capabilities, and alignment with Indian cultural contexts. A comprehensive evaluation incorporating both automated metrics and human annotation across 10k assessments indicates that generated data is high quality; though, human evaluation highlights areas for further improvement. Additionally, we perform downstream evaluations by fine-tuning models on our dataset and assessing the performance across 15 diverse multilingual datasets. Models trained on Updesh consistently achieve significant gains on generative tasks and remain competitive on multiple-choice style NLU tasks. Notably, relative improvements are most pronounced in low and medium-resource languages, narrowing their gap with high-resource languages. These findings provide empirical evidence that effective multilingual AI requires multi-faceted data curation and generation strategies that incorporate context-aware, culturally grounded methodologies.</li>
</ul>

<h3>Title: No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Yehonatan Refael, Guy Smorodinsky, Ofir Lindenbaum, Itay Safran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21296">https://arxiv.org/abs/2509.21296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21296">https://arxiv.org/pdf/2509.21296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21296]] No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks(https://arxiv.org/abs/2509.21296)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>The memorization of training data by neural networks raises pressing concerns for privacy and security. Recent work has shown that, under certain conditions, portions of the training set can be reconstructed directly from model parameters. Some of these methods exploit implicit bias toward margin maximization, suggesting that properties often regarded as beneficial for generalization may actually compromise privacy. Yet despite striking empirical demonstrations, the reliability of these attacks remains poorly understood and lacks a solid theoretical foundation. In this work, we take a complementary perspective: rather than designing stronger attacks, we analyze the inherent weaknesses and limitations of existing reconstruction methods and identify conditions under which they fail. We rigorously prove that, without incorporating prior knowledge about the data, there exist infinitely many alternative solutions that may lie arbitrarily far from the true training set, rendering reconstruction fundamentally unreliable. Empirically, we further demonstrate that exact duplication of training examples occurs only by chance. Our results refine the theoretical understanding of when training set leakage is possible and offer new insights into mitigating reconstruction attacks. Remarkably, we demonstrate that networks trained more extensively, and therefore satisfying implicit bias conditions more strongly -- are, in fact, less susceptible to reconstruction attacks, reconciling privacy with the need for strong generalization in this setting.</li>
</ul>

<h3>Title: Quantized Visual Geometry Grounded Transformer</h3>
<ul>
<li><strong>Authors: </strong>Weilun Feng, Haotong Qin, Mingqiang Wu, Chuanguang Yang, Yuqi Li, Xiangqi Li, Zhulin An, Libo Huang, Yulun Zhang, Michele Magno, Yongjun Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21302">https://arxiv.org/abs/2509.21302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21302">https://arxiv.org/pdf/2509.21302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21302]] Quantized Visual Geometry Grounded Transformer(https://arxiv.org/abs/2509.21302)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Learning-based 3D reconstruction models, represented by Visual Geometry Grounded Transformers (VGGTs), have made remarkable progress with the use of large-scale transformers. Their prohibitive computational and memory costs severely hinder real-world deployment. Post-Training Quantization (PTQ) has become a common practice for compressing and accelerating models. However, we empirically observe that PTQ faces unique obstacles when compressing billion-scale VGGTs: the data-independent special tokens induce heavy-tailed activation distributions, while the multi-view nature of 3D data makes calibration sample selection highly unstable. This paper proposes the first Quantization framework for VGGTs, namely QuantVGGT. This mainly relies on two technical contributions: First, we introduce Dual-Smoothed Fine-Grained Quantization, which integrates pre-global Hadamard rotation and post-local channel smoothing to mitigate heavy-tailed distributions and inter-channel variance robustly. Second, we design Noise-Filtered Diverse Sampling, which filters outliers via deep-layer statistics and constructs frame-aware diverse calibration clusters to ensure stable quantization ranges. Comprehensive experiments demonstrate that QuantVGGT achieves the state-of-the-art results across different benchmarks and bit-width, surpassing the previous state-of-the-art generic quantization method with a great margin. We highlight that our 4-bit QuantVGGT can deliver a 3.7$\times$ memory reduction and 2.5$\times$ acceleration in real-hardware inference, while maintaining reconstruction accuracy above 98\% of its full-precision counterpart. This demonstrates the vast advantages and practicality of QuantVGGT in resource-constrained scenarios. Our code is released in this https URL.</li>
</ul>

<h3>Title: Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Daniel Vennemeyer, Phan Anh Duong, Tiffany Zhan, Tianyu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21305">https://arxiv.org/abs/2509.21305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21305">https://arxiv.org/pdf/2509.21305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21305]] Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors in LLMs(https://arxiv.org/abs/2509.21305)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often exhibit sycophantic behaviors -- such as excessive agreement with or flattery of the user -- but it is unclear whether these behaviors arise from a single mechanism or multiple distinct processes. We decompose sycophancy into sycophantic agreement and sycophantic praise, contrasting both with genuine agreement. Using difference-in-means directions, activation additions, and subspace geometry across multiple models and datasets, we show that: (1) the three behaviors are encoded along distinct linear directions in latent space; (2) each behavior can be independently amplified or suppressed without affecting the others; and (3) their representational structure is consistent across model families and scales. These results suggest that sycophantic behaviors correspond to distinct, independently steerable representations.</li>
</ul>

<h3>Title: SD3.5-Flash: Distribution-Guided Distillation of Generative Flows</h3>
<ul>
<li><strong>Authors: </strong>Hmrishav Bandyopadhyay, Rahim Entezari, Jim Scott, Reshinth Adithyan, Yi-Zhe Song, Varun Jampani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21318">https://arxiv.org/abs/2509.21318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21318">https://arxiv.org/pdf/2509.21318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21318]] SD3.5-Flash: Distribution-Guided Distillation of Generative Flows(https://arxiv.org/abs/2509.21318)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present SD3.5-Flash, an efficient few-step distillation framework that brings high-quality image generation to accessible consumer devices. Our approach distills computationally prohibitive rectified flow models through a reformulated distribution matching objective tailored specifically for few-step generation. We introduce two key innovations: "timestep sharing" to reduce gradient noise and "split-timestep fine-tuning" to improve prompt alignment. Combined with comprehensive pipeline optimizations like text encoder restructuring and specialized quantization, our system enables both rapid generation and memory-efficient deployment across different hardware configurations. This democratizes access across the full spectrum of devices, from mobile phones to desktop computers. Through extensive evaluation including large-scale user studies, we demonstrate that SD3.5-Flash consistently outperforms existing few-step methods, making advanced generative AI truly accessible for practical deployment.</li>
</ul>

<h3>Title: RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards</h3>
<ul>
<li><strong>Authors: </strong>Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Ellie Evans, Daniel Egert, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21319">https://arxiv.org/abs/2509.21319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21319">https://arxiv.org/pdf/2509.21319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21319]] RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards(https://arxiv.org/abs/2509.21319)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM post-training, each offering distinct advantages. However, RLHF struggles with interpretability and reward hacking because it relies on human judgments that usually lack explicit criteria, whereas RLVR is limited in scope by its focus on correctness-based verifiers. We propose Reinforcement Learning with Binary Flexible Feedback (RLBFF), which combines the versatility of human-driven preferences with the precision of rule-based verification, enabling reward models to capture nuanced aspects of response quality beyond mere correctness. RLBFF extracts principles that can be answered in a binary fashion (e.g. accuracy of information: yes, or code readability: no) from natural language feedback. Such principles can then be used to ground Reward Model training as an entailment task (response satisfies or does not satisfy an arbitrary principle). We show that Reward Models trained in this manner can outperform Bradley-Terry models when matched for data and achieve top performance on RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24, 2025). Additionally, users can specify principles of interest at inference time to customize the focus of our reward models, in contrast to Bradley-Terry models. Finally, we present a fully open source recipe (including data) to align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the performance of o3-mini and DeepSeek R1 on general alignment benchmarks of MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).</li>
</ul>

<h3>Title: SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines</h3>
<ul>
<li><strong>Authors: </strong>Yizhou Wang, Chen Tang, Han Deng, Jiabei Xiao, Jiaqi Liu, Jianyu Wu, Jun Yao, Pengze Li, Encheng Su, Lintao Wang, Guohang Zhuang, Yuchen Ren, Ben Fei, Ming Hu, Xin Chen, Dongzhan Zhou, Junjun He, Xiangyu Yue, Zhenfei Yin, Jiamin Wu, Qihao Zheng, Yuhao Zhou, Huihui Xu, Chenglong Ma, Yan Lu, Wenlong Zhang, Chunfeng Song, Philip Torr, Shixiang Tang, Xinzhu Ma, Wanli Ouyang, Lei Bai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21320">https://arxiv.org/abs/2509.21320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21320">https://arxiv.org/pdf/2509.21320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21320]] SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines(https://arxiv.org/abs/2509.21320)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text/knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at this https URL and this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
