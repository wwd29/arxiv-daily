<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: DReS-FL: Dropout-Resilient Secure Federated Learning for Non-IID Clients via Secret Data Sharing. (arXiv:2210.02680v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02680">http://arxiv.org/abs/2210.02680</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02680] DReS-FL: Dropout-Resilient Secure Federated Learning for Non-IID Clients via Secret Data Sharing](http://arxiv.org/abs/2210.02680)</code></li>
<li>Summary: <p>Federated learning (FL) strives to enable collaborative training of machine
learning models without centrally collecting clients' private data. Different
from centralized training, the local datasets across clients in FL are
non-independent and identically distributed (non-IID). In addition, the
data-owning clients may drop out of the training process arbitrarily. These
characteristics will significantly degrade the training performance. This paper
proposes a Dropout-Resilient Secure Federated Learning (DReS-FL) framework
based on Lagrange coded computing (LCC) to tackle both the non-IID and dropout
problems. The key idea is to utilize Lagrange coding to secretly share the
private datasets among clients so that each client receives an encoded version
of the global dataset, and the local gradient computation over this dataset is
unbiased. To correctly decode the gradient at the server, the gradient function
has to be a polynomial in a finite field, and thus we construct polynomial
integer neural networks (PINNs) to enable our framework. Theoretical analysis
shows that DReS-FL is resilient to client dropouts and provides privacy
protection for the local datasets. Furthermore, we experimentally demonstrate
that DReS-FL consistently leads to significant performance gains over baseline
methods.
</p></li>
</ul>

<h3>Title: Cyber-Resilient Privacy Preservation and Secure Billing Approach for Smart Energy Metering Devices. (arXiv:2210.02760v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02760">http://arxiv.org/abs/2210.02760</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02760] Cyber-Resilient Privacy Preservation and Secure Billing Approach for Smart Energy Metering Devices](http://arxiv.org/abs/2210.02760)</code></li>
<li>Summary: <p>Most of the smart applications, such as smart energy metering devices, demand
strong privacy preservation to strengthen data privacy. However, it is
difficult to protect the privacy of the smart device data, especially on the
client side. It is mainly because payment for billing is computed by the server
deployed at the client's side, and it is highly challenging to prevent the
leakage of client's information to unauthorised users. Various researchers have
discussed this problem and have proposed different privacy preservation
techniques. Conventional techniques suffer from the problem of high
computational and communication overload on the client side. In addition, the
performance of these techniques deteriorates due to computational complexity
and their inability to handle the security of large-scale data. Due to these
limitations, it becomes easy for the attackers to introduce malicious attacks
on the server, posing a significant threat to data security. In this context,
this proposal intends to design novel privacy preservation and secure billing
framework using deep learning techniques to ensure data security in smart
energy metering devices. This research aims to overcome the limitations of the
existing techniques to achieve robust privacy preservation in smart devices and
increase the cyber resilience of these devices.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Effective Metaheuristic Based Classifiers for Multiclass Intrusion Detection. (arXiv:2210.02678v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02678">http://arxiv.org/abs/2210.02678</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02678] Effective Metaheuristic Based Classifiers for Multiclass Intrusion Detection](http://arxiv.org/abs/2210.02678)</code></li>
<li>Summary: <p>Network security has become the biggest concern in the area of cyber security
because of the exponential growth in computer networks and applications.
Intrusion detection plays an important role in the security of information
systems or networks devices. The purpose of an intrusion detection system (IDS)
is to detect malicious activities and then generate an alarm against these
activities. Having a large amount of data is one of the key problems in
detecting attacks. Most of the intrusion detection systems use all features of
datasets to evaluate the models and result in is, low detection rate, high
computational time and uses of many computer resources. For fast attacks
detection IDS needs a lightweight data. A feature selection method plays a key
role to select best features to achieve maximum accuracy. This research work
conduct experiments by considering on two updated attacks datasets, UNSW-NB15
and CICDDoS2019. This work suggests a wrapper based Genetic Algorithm (GA)
features selection method with ensemble classifiers. GA select the best feature
subsets and achieve high accuracy, detection rate (DR) and low false alarm rate
(FAR) compared to existing approaches. This research focuses on multi-class
classification. Implements two ensemble methods: stacking and bagging to detect
different types of attacks. The results show that GA improve the accuracy
significantly with stacking ensemble classifier.
</p></li>
</ul>

<h3>Title: Microsoft Defender Will Be Defended: MemoryRanger Prevents Blinding Windows AV. (arXiv:2210.02821v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02821">http://arxiv.org/abs/2210.02821</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02821] Microsoft Defender Will Be Defended: MemoryRanger Prevents Blinding Windows AV](http://arxiv.org/abs/2210.02821)</code></li>
<li>Summary: <p>Windows OS is facing a huge rise in kernel attacks. An overview of popular
techniques that result in loading kernel drivers will be presented. One of the
key targets of modern threats is disabling and blinding Microsoft Defender, a
default Windows AV. The analysis of recent driver-based attacks will be given,
the challenge is to block them. The survey of user- and kernel-level attacks on
Microsoft Defender will be given. One of the recently published attackers
techniques abuses Mandatory Integrity Control (MIC) and Security Reference
Monitor (SRM) by modifying Integrity Level and Debug Privileges for the
Microsoft Defender via syscalls. However, this user-mode attack can be blocked
via the Windows 'trust labels' mechanism. The presented paper discovered the
internals of MIC and SRM, including the analysis of Microsoft Defender during
malware detection. We show how attackers can attack Microsoft Defender using a
kernel-mode driver. This driver modifies the fields of the Token structure
allocated for the Microsoft Defender application. The presented attack resulted
in disabling Microsoft Defender, without terminating any of its processes and
without triggering any Windows security features, such as PatchGuard. The
customized hypervisor-based solution named MemoryRanger was used to protect the
Windows Defender kernel structures. The experiments show that MemoryRanger
successfully restricts access to the sensitive kernel data from illegal access
attempts with affordable performance degradation.
</p></li>
</ul>

<h3>Title: Detecting Irregular Network Activity with Adversarial Learning and Expert Feedback. (arXiv:2210.02841v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02841">http://arxiv.org/abs/2210.02841</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02841] Detecting Irregular Network Activity with Adversarial Learning and Expert Feedback](http://arxiv.org/abs/2210.02841)</code></li>
<li>Summary: <p>Anomaly detection is a ubiquitous and challenging task relevant across many
disciplines. With the vital role communication networks play in our daily
lives, the security of these networks is imperative for smooth functioning of
society. To this end, we propose a novel self-supervised deep learning
framework CAAD for anomaly detection in wireless communication systems.
Specifically, CAAD employs contrastive learning in an adversarial setup to
learn effective representations of normal and anomalous behavior in wireless
networks. We conduct rigorous performance comparisons of CAAD with several
state-of-the-art anomaly detection techniques and verify that CAAD yields a
mean performance improvement of 92.84%. Additionally, we also augment CAAD
enabling it to systematically incorporate expert feedback through a novel
contrastive learning feedback loop to improve the learned representations and
thereby reduce prediction uncertainty (CAAD-EF). We view CAAD-EF as a novel,
holistic and widely applicable solution to anomaly detection.
</p></li>
</ul>

<h3>Title: NeuDep: Neural Binary Memory Dependence Analysis. (arXiv:2210.02853v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02853">http://arxiv.org/abs/2210.02853</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02853] NeuDep: Neural Binary Memory Dependence Analysis](http://arxiv.org/abs/2210.02853)</code></li>
<li>Summary: <p>Determining whether multiple instructions can access the same memory location
is a critical task in binary analysis. It is challenging as statically
computing precise alias information is undecidable in theory. The problem
aggravates at the binary level due to the presence of compiler optimizations
and the absence of symbols and types. Existing approaches either produce
significant spurious dependencies due to conservative analysis or scale poorly
to complex binaries.
</p></li>
</ul>

<p>We present a new machine-learning-based approach to predict memory
dependencies by exploiting the model's learned knowledge about how binary
programs execute. Our approach features (i) a self-supervised procedure that
pretrains a neural net to reason over binary code and its dynamic value flows
through memory addresses, followed by (ii) supervised finetuning to infer the
memory dependencies statically. To facilitate efficient learning, we develop
dedicated neural architectures to encode the heterogeneous inputs (i.e., code,
data values, and memory addresses from traces) with specific modules and fuse
them with a composition learning strategy.
</p>
<p>We implement our approach in NeuDep and evaluate it on 41 popular software
projects compiled by 2 compilers, 4 optimizations, and 4 obfuscation passes. We
demonstrate that NeuDep is more precise (1.5x) and faster (3.5x) than the
current state-of-the-art. Extensive probing studies on security-critical
reverse engineering tasks suggest that NeuDep understands memory access
patterns, learns function signatures, and is able to match indirect calls. All
these tasks either assist or benefit from inferring memory dependencies.
Notably, NeuDep also outperforms the current state-of-the-art on these tasks.
</p>

<h3>Title: Model-Driven Engineering for Formal Verification and Security Testing of Authentication Protocols. (arXiv:2210.03020v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.03020">http://arxiv.org/abs/2210.03020</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.03020] Model-Driven Engineering for Formal Verification and Security Testing of Authentication Protocols](http://arxiv.org/abs/2210.03020)</code></li>
<li>Summary: <p>Even if the verification of authentication protocols can be achieved by means
of formal analysis, the modelling of such an activity is an error-prone task
due to the lack of automated and integrated processes. This paper proposes a
comprehensive approach, based on the Unified Modeling Language (UML) profiling
technique and on model-transformation, to enable automatic analysis of
authentication protocols starting from high-level models. In particular, a
UML-based approach is able to generate an annotated model of communication
protocols from which formal notations (e.g., AnBx, Tamarin) can be generated.
Such models in lower-level languages can be analysed with existing solvers
and/or with traditional testing techniques by means of test case generation
approaches. The industrial impact of the research is high due to the growing
need of security and the necessity to connect industrial processes and
equipment to virtualised computing infrastructures. The research is conducted
on two case studies: railway signalling systems and blockchain based
applications.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: FedGraph: an Aggregation Method from Graph Perspective. (arXiv:2210.02733v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02733">http://arxiv.org/abs/2210.02733</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02733] FedGraph: an Aggregation Method from Graph Perspective](http://arxiv.org/abs/2210.02733)</code></li>
<li>Summary: <p>With the increasingly strengthened data privacy act and the difficult data
centralization, Federated Learning (FL) has become an effective solution to
collaboratively train the model while preserving each client's privacy. FedAvg
is a standard aggregation algorithm that makes the proportion of dataset size
of each client as aggregation weight. However, it can't deal with
non-independent and identically distributed (non-i.i.d) data well because of
its fixed aggregation weights and the neglect of data distribution. In this
paper, we propose an aggregation strategy that can effectively deal with
non-i.i.d dataset, namely FedGraph, which can adjust the aggregation weights
adaptively according to the training condition of local models in whole
training process. The FedGraph takes three factors into account from coarse to
fine: the proportion of each local dataset size, the topology factor of model
graphs, and the model weights. We calculate the gravitational force between
local models by transforming the local models into topology graphs. The
FedGraph can explore the internal correlation between local models better
through the weighted combination of the proportion each local dataset, topology
structure, and model weights. The proposed FedGraph has been applied to the
MICCAI Federated Tumor Segmentation Challenge 2021 (FeTS) datasets, and the
validation results show that our method surpasses the previous state-of-the-art
by 2.76 mean Dice Similarity Score. The source code will be available at
Github.
</p></li>
</ul>

<h3>Title: Privacy-Preserving Text Classification on BERT Embeddings with Homomorphic Encryption. (arXiv:2210.02574v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02574">http://arxiv.org/abs/2210.02574</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02574] Privacy-Preserving Text Classification on BERT Embeddings with Homomorphic Encryption](http://arxiv.org/abs/2210.02574)</code></li>
<li>Summary: <p>Embeddings, which compress information in raw text into semantics-preserving
low-dimensional vectors, have been widely adopted for their efficacy. However,
recent research has shown that embeddings can potentially leak private
information about sensitive attributes of the text, and in some cases, can be
inverted to recover the original input text. To address these growing privacy
challenges, we propose a privatization mechanism for embeddings based on
homomorphic encryption, to prevent potential leakage of any piece of
information in the process of text classification. In particular, our method
performs text classification on the encryption of embeddings from
state-of-the-art models like BERT, supported by an efficient GPU implementation
of CKKS encryption scheme. We show that our method offers encrypted protection
of BERT embeddings, while largely preserving their utility on downstream text
classification tasks.
</p></li>
</ul>

<h3>Title: PrivacyCube: A Tangible Device for Improving Privacy Awareness in IoT. (arXiv:2210.02650v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02650">http://arxiv.org/abs/2210.02650</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02650] PrivacyCube: A Tangible Device for Improving Privacy Awareness in IoT](http://arxiv.org/abs/2210.02650)</code></li>
<li>Summary: <p>Consumers increasingly bring IoT devices into their living spaces without
understanding how their data is collected, processed, and used. We present
PrivacyCube, a novel tangible device designed to explore the extent to which
privacy awareness in smart homes can be elevated. PrivacyCube visualises IoT
devices' data consumption displaying privacy-related notices. PrivacyCube aims
at assisting families to (i) understand key privacy aspects better and (ii)
have conversations around data management practices of IoT devices. Thus,
families can learn and make informed privacy decisions collectively.
</p></li>
</ul>

<h3>Title: Federated Boosted Decision Trees with Differential Privacy. (arXiv:2210.02910v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02910">http://arxiv.org/abs/2210.02910</a></li>
<li>Code URL: <a href="https://github.com/samuel-maddock/federated-boosted-dp-trees">https://github.com/samuel-maddock/federated-boosted-dp-trees</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02910] Federated Boosted Decision Trees with Differential Privacy](http://arxiv.org/abs/2210.02910)</code></li>
<li>Summary: <p>There is great demand for scalable, secure, and efficient privacy-preserving
machine learning models that can be trained over distributed data. While deep
learning models typically achieve the best results in a centralized non-secure
setting, different models can excel when privacy and communication constraints
are imposed. Instead, tree-based approaches such as XGBoost have attracted much
attention for their high performance and ease of use; in particular, they often
achieve state-of-the-art results on tabular data. Consequently, several recent
works have focused on translating Gradient Boosted Decision Tree (GBDT) models
like XGBoost into federated settings, via cryptographic mechanisms such as
Homomorphic Encryption (HE) and Secure Multi-Party Computation (MPC). However,
these do not always provide formal privacy guarantees, or consider the full
range of hyperparameters and implementation settings. In this work, we
implement the GBDT model under Differential Privacy (DP). We propose a general
framework that captures and extends existing approaches for differentially
private decision trees. Our framework of methods is tailored to the federated
setting, and we show that with a careful choice of techniques it is possible to
achieve very high utility while maintaining strong levels of privacy.
</p></li>
</ul>

<h3>Title: CANIFE: Crafting Canaries for Empirical Privacy Measurement in Federated Learning. (arXiv:2210.02912v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02912">http://arxiv.org/abs/2210.02912</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02912] CANIFE: Crafting Canaries for Empirical Privacy Measurement in Federated Learning](http://arxiv.org/abs/2210.02912)</code></li>
<li>Summary: <p>Federated Learning (FL) is a setting for training machine learning models in
distributed environments where the clients do not share their raw data but
instead send model updates to a server. However, model updates can be subject
to attacks and leak private information. Differential Privacy (DP) is a leading
mitigation strategy which involves adding noise to clipped model updates,
trading off performance for strong theoretical privacy guarantees. Previous
work has shown that the threat model of DP is conservative and that the
obtained guarantees may be vacuous or may not directly translate to information
leakage in practice. In this paper, we aim to achieve a tighter measurement of
the model exposure by considering a realistic threat model. We propose a novel
method, CANIFE, that uses canaries - carefully crafted samples by a strong
adversary to evaluate the empirical privacy of a training round. We apply this
attack to vision models trained on CIFAR-10 and CelebA and to language models
trained on Sent140 and Shakespeare. In particular, in realistic FL scenarios,
we demonstrate that the empirical epsilon obtained with CANIFE is 2-7x lower
than the theoretical bound.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Token Classification for Disambiguating Medical Abbreviations. (arXiv:2210.02487v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02487">http://arxiv.org/abs/2210.02487</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02487] Token Classification for Disambiguating Medical Abbreviations](http://arxiv.org/abs/2210.02487)</code></li>
<li>Summary: <p>Abbreviations are unavoidable yet critical parts of the medical text. Using
abbreviations, especially in clinical patient notes, can save time and space,
protect sensitive information, and help avoid repetitions. However, most
abbreviations might have multiple senses, and the lack of a standardized
mapping system makes disambiguating abbreviations a difficult and
time-consuming task. The main objective of this study is to examine the
feasibility of token classification methods for medical abbreviation
disambiguation. Specifically, we explore the capability of token classification
methods to deal with multiple unique abbreviations in a single text. We use two
public datasets to compare and contrast the performance of several transformer
models pre-trained on different scientific and medical corpora. Our proposed
token classification approach outperforms the more commonly used text
classification models for the abbreviation disambiguation task. In particular,
the SciBERT model shows a strong performance for both token and text
classification tasks over the two considered datasets. Furthermore, we find
that abbreviation disambiguation performance for the text classification models
becomes comparable to that of token classification only when postprocessing is
applied to their predictions, which involves filtering possible labels for an
abbreviation based on the training data.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: How Far Are We from Real Synonym Substitution Attacks?. (arXiv:2210.02844v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02844">http://arxiv.org/abs/2210.02844</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02844] How Far Are We from Real Synonym Substitution Attacks?](http://arxiv.org/abs/2210.02844)</code></li>
<li>Summary: <p>In this paper, we explore the following question: how far are we from real
synonym substitution attacks (SSAs). We approach this question by examining how
SSAs replace words in the original sentence and show that there are still
unresolved obstacles that make current SSAs generate invalid adversarial
samples. We reveal that four widely used word substitution methods generate a
large fraction of invalid substitution words that are ungrammatical or do not
preserve the original sentence's semantics. Next, we show that the semantic and
grammatical constraints used in SSAs for detecting invalid word replacements
are highly insufficient in detecting invalid adversarial samples. Our work is
an important stepping stone to constructing better SSAs in the future.
</p></li>
</ul>

<h3>Title: Practical Adversarial Attacks on Spatiotemporal Traffic Forecasting Models. (arXiv:2210.02447v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02447">http://arxiv.org/abs/2210.02447</a></li>
<li>Code URL: <a href="https://github.com/kdd-hkust/adv-st">https://github.com/kdd-hkust/adv-st</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02447] Practical Adversarial Attacks on Spatiotemporal Traffic Forecasting Models](http://arxiv.org/abs/2210.02447)</code></li>
<li>Summary: <p>Machine learning based traffic forecasting models leverage sophisticated
spatiotemporal auto-correlations to provide accurate predictions of city-wide
traffic states. However, existing methods assume a reliable and unbiased
forecasting environment, which is not always available in the wild. In this
work, we investigate the vulnerability of spatiotemporal traffic forecasting
models and propose a practical adversarial spatiotemporal attack framework.
Specifically, instead of simultaneously attacking all geo-distributed data
sources, an iterative gradient-guided node saliency method is proposed to
identify the time-dependent set of victim nodes. Furthermore, we devise a
spatiotemporal gradient descent based scheme to generate real-valued
adversarial traffic states under a perturbation constraint. Meanwhile, we
theoretically demonstrate the worst performance bound of adversarial traffic
forecasting attacks. Extensive experiments on two real-world datasets show that
the proposed two-step framework achieves up to $67.8\%$ performance degradation
on various advanced spatiotemporal forecasting models. Remarkably, we also show
that adversarial training with our proposed attacks can significantly improve
the robustness of spatiotemporal traffic forecasting models. Our code is
available in \url{https://github.com/luckyfan-cs/ASTFA}.
</p></li>
</ul>

<h3>Title: From Threat Reports to Continuous Threat Intelligence: A Comparison of Attack Technique Extraction Methods from Textual Artifacts. (arXiv:2210.02601v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02601">http://arxiv.org/abs/2210.02601</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02601] From Threat Reports to Continuous Threat Intelligence: A Comparison of Attack Technique Extraction Methods from Textual Artifacts](http://arxiv.org/abs/2210.02601)</code></li>
<li>Summary: <p>The cyberthreat landscape is continuously evolving. Hence, continuous
monitoring and sharing of threat intelligence have become a priority for
organizations. Threat reports, published by cybersecurity vendors, contain
detailed descriptions of attack Tactics, Techniques, and Procedures (TTP)
written in an unstructured text format. Extracting TTP from these reports aids
cybersecurity practitioners and researchers learn and adapt to evolving attacks
and in planning threat mitigation. Researchers have proposed TTP extraction
methods in the literature, however, not all of these proposed methods are
compared to one another or to a baseline. \textit{The goal of this study is to
aid cybersecurity researchers and practitioners choose attack technique
extraction methods for monitoring and sharing threat intelligence by comparing
the underlying methods from the TTP extraction studies in the literature.} In
this work, we identify ten existing TTP extraction studies from the literature
and implement five methods from the ten studies. We find two methods, based on
Term Frequency-Inverse Document Frequency(TFIDF) and Latent Semantic Indexing
(LSI), outperform the other three methods with a F1 score of 84\% and 83\%,
respectively. We observe the performance of all methods in F1 score drops in
the case of increasing the class labels exponentially. We also implement and
evaluate an oversampling strategy to mitigate class imbalance issues.
Furthermore, oversampling improves the classification performance of TTP
extraction. We provide recommendations from our findings for future
cybersecurity researchers, such as the construction of a benchmark dataset from
a large corpus; and the selection of textual features of TTP. Our work, along
with the dataset and implementation source code, can work as a baseline for
cybersecurity researchers to test and compare the performance of future TTP
extraction methods.
</p></li>
</ul>

<h3>Title: On Optimal Learning Under Targeted Data Poisoning. (arXiv:2210.02713v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02713">http://arxiv.org/abs/2210.02713</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02713] On Optimal Learning Under Targeted Data Poisoning](http://arxiv.org/abs/2210.02713)</code></li>
<li>Summary: <p>Consider the task of learning a hypothesis class $\mathcal{H}$ in the
presence of an adversary that can replace up to an $\eta$ fraction of the
examples in the training set with arbitrary adversarial examples. The adversary
aims to fail the learner on a particular target test point $x$ which is known
to the adversary but not to the learner. In this work we aim to characterize
the smallest achievable error $\epsilon=\epsilon(\eta)$ by the learner in the
presence of such an adversary in both realizable and agnostic settings. We
fully achieve this in the realizable setting, proving that
$\epsilon=\Theta(\mathtt{VC}(\mathcal{H})\cdot \eta)$, where
$\mathtt{VC}(\mathcal{H})$ is the VC dimension of $\mathcal{H}$. Remarkably, we
show that the upper bound can be attained by a deterministic learner. In the
agnostic setting we reveal a more elaborate landscape: we devise a
deterministic learner with a multiplicative regret guarantee of $\epsilon \leq
C\cdot\mathtt{OPT} + O(\mathtt{VC}(\mathcal{H})\cdot \eta)$, where $C > 1$ is a
universal numerical constant. We complement this by showing that for any
deterministic learner there is an attack which worsens its error to at least
$2\cdot \mathtt{OPT}$. This implies that a multiplicative deterioration in the
regret is unavoidable in this case. Finally, the algorithms we develop for
achieving the optimal rates are inherently improper. Nevertheless, we show that
for a variety of natural concept classes, such as linear classifiers, it is
possible to retain the dependence $\epsilon=\Theta_{\mathcal{H}}(\eta)$ by a
proper algorithm in the realizable setting. Here $\Theta_{\mathcal{H}}$
conceals a polynomial dependence on $\mathtt{VC}(\mathcal{H})$.
</p></li>
</ul>

<h3>Title: Deep Reinforcement Learning based Evasion Generative Adversarial Network for Botnet Detection. (arXiv:2210.02840v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02840">http://arxiv.org/abs/2210.02840</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02840] Deep Reinforcement Learning based Evasion Generative Adversarial Network for Botnet Detection](http://arxiv.org/abs/2210.02840)</code></li>
<li>Summary: <p>Botnet detectors based on machine learning are potential targets for
adversarial evasion attacks. Several research works employ adversarial training
with samples generated from generative adversarial nets (GANs) to make the
botnet detectors adept at recognising adversarial evasions. However, the
synthetic evasions may not follow the original semantics of the input samples.
This paper proposes a novel GAN model leveraged with deep reinforcement
learning (DRL) to explore semantic aware samples and simultaneously harden its
detection. A DRL agent is used to attack the discriminator of the GAN that acts
as a botnet detector. The discriminator is trained on the crafted perturbations
by the agent during the GAN training, which helps the GAN generator converge
earlier than the case without DRL. We name this model RELEVAGAN, i.e. ["relive
a GAN" or deep REinforcement Learning-based Evasion Generative Adversarial
Network] because, with the help of DRL, it minimises the GAN's job by letting
its generator explore the evasion samples within the semantic limits. During
the GAN training, the attacks are conducted to adjust the discriminator weights
for learning crafted perturbations by the agent. RELEVAGAN does not require
adversarial training for the ML classifiers since it can act as an adversarial
semantic-aware botnet detection model. Code will be available at
https://github.com/rhr407/RELEVAGAN.
</p></li>
</ul>

<h3>Title: Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning. (arXiv:2210.02873v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02873">http://arxiv.org/abs/2210.02873</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02873] Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning](http://arxiv.org/abs/2210.02873)</code></li>
<li>Summary: <p>Federated Learning (FL) is a machine learning technique that addresses the
privacy challenges in terms of access rights of local datasets by enabling the
training of a model across nodes holding their data samples locally. To achieve
decentralized federated learning, blockchain-based FL was proposed as a
distributed FL architecture. In decentralized FL, the chief is eliminated from
the learning process as workers collaborate between each other to train the
global model. Decentralized FL applications need to account for the additional
delay incurred by blockchain-based FL deployments. Particularly in this
setting, to detect targeted/untargeted poisoning attacks, we investigate the
end-to-end learning completion latency of a realistic decentralized FL process
protected against poisoning attacks. We propose a technique which consists in
decoupling the monitoring phase from the detection phase in defenses against
poisoning attacks in a decentralized federated learning deployment that aim at
monitoring the behavior of the workers. We demonstrate that our proposed
blockchain-based monitoring improved network scalability, robustness and time
efficiency. The parallelization of operations results in minimized latency over
the end-to-end communication, computation, and consensus delays incurred during
the FL and blockchain operations.
</p></li>
</ul>

<h3>Title: EvilScreen Attack: Smart TV Hijacking via Multi-channel Remote Control Mimicry. (arXiv:2210.03014v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.03014">http://arxiv.org/abs/2210.03014</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.03014] EvilScreen Attack: Smart TV Hijacking via Multi-channel Remote Control Mimicry](http://arxiv.org/abs/2210.03014)</code></li>
<li>Summary: <p>Modern smart TVs often communicate with their remote controls (including
those smart phone simulated ones) using multiple wireless channels (e.g.,
Infrared, Bluetooth, and Wi-Fi). However, this multi-channel remote control
communication introduces a new attack surface. An inherent security flaw is
that remote controls of most smart TVs are designed to work in a benign
environment rather than an adversarial one, and thus wireless communications
between a smart TV and its remote controls are not strongly protected.
Attackers could leverage such flaw to abuse the remote control communication
and compromise smart TV systems. In this paper, we propose EvilScreen, a novel
attack that exploits ill-protected remote control communications to access
protected resources of a smart TV or even control the screen. EvilScreen
exploits a multi-channel remote control mimicry vulnerability present in today
smart TVs. Unlike other attacks, which compromise the TV system by exploiting
code vulnerabilities or malicious third-party apps, EvilScreen directly reuses
commands of different remote controls, combines them together to circumvent
deployed authentication and isolation policies, and finally accesses or
controls TV resources remotely. We evaluated eight mainstream smart TVs and
found that they are all vulnerable to EvilScreen attacks, including a Samsung
product adopting the ISO/IEC security specification.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: On Adversarial Robustness of Deep Image Deblurring. (arXiv:2210.02502v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02502">http://arxiv.org/abs/2210.02502</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02502] On Adversarial Robustness of Deep Image Deblurring](http://arxiv.org/abs/2210.02502)</code></li>
<li>Summary: <p>Recent approaches employ deep learning-based solutions for the recovery of a
sharp image from its blurry observation. This paper introduces adversarial
attacks against deep learning-based image deblurring methods and evaluates the
robustness of these neural networks to untargeted and targeted attacks. We
demonstrate that imperceptible distortion can significantly degrade the
performance of state-of-the-art deblurring networks, even producing drastically
different content in the output, indicating the strong need to include
adversarially robust training not only in classification but also for image
recovery.
</p></li>
</ul>

<h3>Title: TartanCalib: Iterative Wide-Angle Lens Calibration using Adaptive SubPixel Refinement of AprilTags. (arXiv:2210.02511v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02511">http://arxiv.org/abs/2210.02511</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02511] TartanCalib: Iterative Wide-Angle Lens Calibration using Adaptive SubPixel Refinement of AprilTags](http://arxiv.org/abs/2210.02511)</code></li>
<li>Summary: <p>Wide-angle cameras are uniquely positioned for mobile robots, by virtue of
the rich information they provide in a small, light, and cost-effective form
factor. An accurate calibration of the intrinsics and extrinsics is a critical
pre-requisite for using the edge of a wide-angle lens for depth perception and
odometry. Calibrating wide-angle lenses with current state-of-the-art
techniques yields poor results due to extreme distortion at the edge, as most
algorithms assume a lens with low to medium distortion closer to a pinhole
projection. In this work we present our methodology for accurate wide-angle
calibration. Our pipeline generates an intermediate model, and leverages it to
iteratively improve feature detection and eventually the camera parameters. We
test three key methods to utilize intermediate camera models: (1) undistorting
the image into virtual pinhole cameras, (2) reprojecting the target into the
image frame, and (3) adaptive subpixel refinement. Combining adaptive subpixel
refinement and feature reprojection significantly improves reprojection errors
by up to 26.59 %, helps us detect up to 42.01 % more features, and improves
performance in the downstream task of dense depth mapping. Finally, TartanCalib
is open-source and implemented into an easy-to-use calibration toolbox. We also
provide a translation layer with other state-of-the-art works, which allows for
regressing generic models with thousands of parameters or using a more robust
solver. To this end, TartanCalib is the tool of choice for wide-angle
calibration. Project website and code: <a href="http://tartancalib.com.">this http URL</a>
</p></li>
</ul>

<h3>Title: AOE-Net: Entities Interactions Modeling with Adaptive Attention Mechanism for Temporal Action Proposals Generation. (arXiv:2210.02578v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02578">http://arxiv.org/abs/2210.02578</a></li>
<li>Code URL: <a href="https://github.com/uark-aicv/aoe-net">https://github.com/uark-aicv/aoe-net</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02578] AOE-Net: Entities Interactions Modeling with Adaptive Attention Mechanism for Temporal Action Proposals Generation](http://arxiv.org/abs/2210.02578)</code></li>
<li>Summary: <p>Temporal action proposal generation (TAPG) is a challenging task, which
requires localizing action intervals in an untrimmed video. Intuitively, we as
humans, perceive an action through the interactions between actors, relevant
objects, and the surrounding environment. Despite the significant progress of
TAPG, a vast majority of existing methods ignore the aforementioned principle
of the human perceiving process by applying a backbone network into a given
video as a black-box. In this paper, we propose to model these interactions
with a multi-modal representation network, namely, Actors-Objects-Environment
Interaction Network (AOE-Net). Our AOE-Net consists of two modules, i.e.,
perception-based multi-modal representation (PMR) and boundary-matching module
(BMM). Additionally, we introduce adaptive attention mechanism (AAM) in PMR to
focus only on main actors (or relevant objects) and model the relationships
among them. PMR module represents each video snippet by a visual-linguistic
feature, in which main actors and surrounding environment are represented by
visual information, whereas relevant objects are depicted by linguistic
features through an image-text model. BMM module processes the sequence of
visual-linguistic features as its input and generates action proposals.
Comprehensive experiments and extensive ablation studies on ActivityNet-1.3 and
THUMOS-14 datasets show that our proposed AOE-Net outperforms previous
state-of-the-art methods with remarkable performance and generalization for
both TAPG and temporal action detection. To prove the robustness and
effectiveness of AOE-Net, we further conduct an ablation study on egocentric
videos, i.e. EPIC-KITCHENS 100 dataset. Source code is available upon
acceptance.
</p></li>
</ul>

<h3>Title: Dynamic Stochastic Ensemble with Adversarial Robust Lottery Ticket Subnetworks. (arXiv:2210.02618v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02618">http://arxiv.org/abs/2210.02618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02618] Dynamic Stochastic Ensemble with Adversarial Robust Lottery Ticket Subnetworks](http://arxiv.org/abs/2210.02618)</code></li>
<li>Summary: <p>Adversarial attacks are considered the intrinsic vulnerability of CNNs.
Defense strategies designed for attacks have been stuck in the adversarial
attack-defense arms race, reflecting the imbalance between attack and defense.
Dynamic Defense Framework (DDF) recently changed the passive safety status quo
based on the stochastic ensemble model. The diversity of subnetworks, an
essential concern in the DDF, can be effectively evaluated by the adversarial
transferability between different networks. Inspired by the poor adversarial
transferability between subnetworks of scratch tickets with various remaining
ratios, we propose a method to realize the dynamic stochastic ensemble defense
strategy. We discover the adversarial transferable diversity between robust
lottery ticket subnetworks drawn from different basic structures and sparsity.
The experimental results suggest that our method achieves better robust and
clean recognition accuracy by adversarial transferable diversity, which would
decrease the reliability of attacks.
</p></li>
</ul>

<h3>Title: Domain Generalization via Contrastive Causal Learning. (arXiv:2210.02655v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02655">http://arxiv.org/abs/2210.02655</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02655] Domain Generalization via Contrastive Causal Learning](http://arxiv.org/abs/2210.02655)</code></li>
<li>Summary: <p>Domain Generalization (DG) aims to learn a model that can generalize well to
unseen target domains from a set of source domains. With the idea of invariant
causal mechanism, a lot of efforts have been put into learning robust causal
effects which are determined by the object yet insensitive to the domain
changes. Despite the invariance of causal effects, they are difficult to be
quantified and optimized. Inspired by the ability that humans adapt to new
environments by prior knowledge, We develop a novel Contrastive Causal Model
(CCM) to transfer unseen images to taught knowledge which are the features of
seen images, and quantify the causal effects based on taught knowledge.
Considering the transfer is affected by domain shifts in DG, we propose a more
inclusive causal graph to describe DG task. Based on this causal graph, CCM
controls the domain factor to cut off excess causal paths and uses the
remaining part to calculate the causal effects of images to labels via the
front-door criterion. Specifically, CCM is composed of three components: (i)
domain-conditioned supervised learning which teaches CCM the correlation
between images and labels, (ii) causal effect learning which helps CCM measure
the true causal effects of images to labels, (iii) contrastive similarity
learning which clusters the features of images that belong to the same class
and provides the quantification of similarity. Finally, we test the performance
of CCM on multiple datasets including PACS, OfficeHome, and TerraIncognita. The
extensive experiments demonstrate that CCM surpasses the previous DG methods
with clear margins.
</p></li>
</ul>

<h3>Title: CLAD: A Contrastive Learning based Approach for Background Debiasing. (arXiv:2210.02748v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02748">http://arxiv.org/abs/2210.02748</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02748] CLAD: A Contrastive Learning based Approach for Background Debiasing](http://arxiv.org/abs/2210.02748)</code></li>
<li>Summary: <p>Convolutional neural networks (CNNs) have achieved superhuman performance in
multiple vision tasks, especially image classification. However, unlike humans,
CNNs leverage spurious features, such as background information to make
decisions. This tendency creates different problems in terms of robustness or
weak generalization performance. Through our work, we introduce a contrastive
learning-based approach (CLAD) to mitigate the background bias in CNNs. CLAD
encourages semantic focus on object foregrounds and penalizes learning features
from irrelavant backgrounds. Our method also introduces an efficient way of
sampling negative samples. We achieve state-of-the-art results on the
Background Challenge dataset, outperforming the previous benchmark with a
margin of 4.1\%. Our paper shows how CLAD serves as a proof of concept for
debiasing of spurious features, such as background and texture (in
supplementary material).
</p></li>
</ul>

<h3>Title: Robust Double-Encoder Network for RGB-D Panoptic Segmentation. (arXiv:2210.02834v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02834">http://arxiv.org/abs/2210.02834</a></li>
<li>Code URL: <a href="https://github.com/prbonn/ps-res-excite">https://github.com/prbonn/ps-res-excite</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02834] Robust Double-Encoder Network for RGB-D Panoptic Segmentation](http://arxiv.org/abs/2210.02834)</code></li>
<li>Summary: <p>Perception is crucial for robots that act in real-world environments, as
autonomous systems need to see and understand the world around them to act
appropriately. Panoptic segmentation provides an interpretation of the scene by
computing a pixel-wise semantic label together with instance IDs. In this
paper, we address panoptic segmentation using RGB-D data of indoor scenes. We
propose a novel encoder-decoder neural network that processes RGB and depth
separately through two encoders. The features of the individual encoders are
progressively merged at different resolutions, such that the RGB features are
enhanced using complementary depth information. We propose a novel merging
approach called ResidualExcite, which reweighs each entry of the feature map
according to its importance. With our double-encoder architecture, we are
robust to missing cues. In particular, the same model can train and infer on
RGB-D, RGB-only, and depth-only input data, without the need to train
specialized models. We evaluate our method on publicly available datasets and
show that our approach achieves superior results compared to other common
approaches for panoptic segmentation.
</p></li>
</ul>

<h3>Title: Cross-Modality Domain Adaptation for Freespace Detection: A Simple yet Effective Baseline. (arXiv:2210.02991v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02991">http://arxiv.org/abs/2210.02991</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02991] Cross-Modality Domain Adaptation for Freespace Detection: A Simple yet Effective Baseline](http://arxiv.org/abs/2210.02991)</code></li>
<li>Summary: <p>As one of the fundamental functions of autonomous driving system, freespace
detection aims at classifying each pixel of the image captured by the camera as
drivable or non-drivable. Current works of freespace detection heavily rely on
large amount of densely labeled training data for accuracy and robustness,
which is time-consuming and laborious to collect and annotate. To the best of
our knowledge, we are the first work to explore unsupervised domain adaptation
for freespace detection to alleviate the data limitation problem with synthetic
data. We develop a cross-modality domain adaptation framework which exploits
both RGB images and surface normal maps generated from depth images. A
Collaborative Cross Guidance (CCG) module is proposed to leverage the context
information of one modality to guide the other modality in a cross manner, thus
realizing inter-modality intra-domain complement. To better bridge the domain
gap between source domain (synthetic data) and target domain (real-world data),
we also propose a Selective Feature Alignment (SFA) module which only aligns
the features of consistent foreground area between the two domains, thus
realizing inter-domain intra-modality adaptation. Extensive experiments are
conducted by adapting three different synthetic datasets to one real-world
dataset for freespace detection respectively. Our method performs closely to
fully supervised freespace detection methods (93.08 v.s. 97.50 F1 score) and
outperforms other general unsupervised domain adaptation methods for semantic
segmentation with large margins, which shows the promising potential of domain
adaptation for freespace detection.
</p></li>
</ul>

<h3>Title: Feature-Realistic Neural Fusion for Real-Time, Open Set Scene Understanding. (arXiv:2210.03043v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.03043">http://arxiv.org/abs/2210.03043</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.03043] Feature-Realistic Neural Fusion for Real-Time, Open Set Scene Understanding](http://arxiv.org/abs/2210.03043)</code></li>
<li>Summary: <p>General scene understanding for robotics requires flexible semantic
representation, so that novel objects and structures which may not have been
known at training time can be identified, segmented and grouped. We present an
algorithm which fuses general learned features from a standard pre-trained
network into a highly efficient 3D geometric neural field representation during
real-time SLAM. The fused 3D feature maps inherit the coherence of the neural
field's geometry representation. This means that tiny amounts of human
labelling interacting at runtime enable objects or even parts of objects to be
robustly and accurately segmented in an open set manner.
</p></li>
</ul>

<h3>Title: Ambiguous Images With Human Judgments for Robust Visual Event Classification. (arXiv:2210.03102v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.03102">http://arxiv.org/abs/2210.03102</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.03102] Ambiguous Images With Human Judgments for Robust Visual Event Classification](http://arxiv.org/abs/2210.03102)</code></li>
<li>Summary: <p>Contemporary vision benchmarks predominantly consider tasks on which humans
can achieve near-perfect performance. However, humans are frequently presented
with visual data that they cannot classify with 100% certainty, and models
trained on standard vision benchmarks achieve low performance when evaluated on
this data. To address this issue, we introduce a procedure for creating
datasets of ambiguous images and use it to produce SQUID-E ("Squidy"), a
collection of noisy images extracted from videos. All images are annotated with
ground truth values and a test set is annotated with human uncertainty
judgments. We use this dataset to characterize human uncertainty in vision
tasks and evaluate existing visual event classification models. Experimental
results suggest that existing vision models are not sufficiently equipped to
provide meaningful outputs for ambiguous images and that datasets of this
nature can be used to assess and improve such models through model training and
direct evaluation of model calibration. These findings motivate large-scale
ambiguous dataset creation and further research focusing on noisy visual data.
</p></li>
</ul>

<h3>Title: SimPer: Simple Self-Supervised Learning of Periodic Targets. (arXiv:2210.03115v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.03115">http://arxiv.org/abs/2210.03115</a></li>
<li>Code URL: <a href="https://github.com/yyzharry/simper">https://github.com/yyzharry/simper</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.03115] SimPer: Simple Self-Supervised Learning of Periodic Targets](http://arxiv.org/abs/2210.03115)</code></li>
<li>Summary: <p>From human physiology to environmental evolution, important processes in
nature often exhibit meaningful and strong periodic or quasi-periodic changes.
Due to their inherent label scarcity, learning useful representations for
periodic tasks with limited or no supervision is of great benefit. Yet,
existing self-supervised learning (SSL) methods overlook the intrinsic
periodicity in data, and fail to learn representations that capture periodic or
frequency attributes. In this paper, we present SimPer, a simple contrastive
SSL regime for learning periodic information in data. To exploit the periodic
inductive bias, SimPer introduces customized augmentations, feature similarity
measures, and a generalized contrastive loss for learning efficient and robust
periodic representations. Extensive experiments on common real-world tasks in
human behavior analysis, environmental sensing, and healthcare domains verify
the superior performance of SimPer compared to state-of-the-art SSL methods,
highlighting its intriguing properties including better data efficiency,
robustness to spurious correlations, and generalization to distribution shifts.
Code and data are available at: https://github.com/YyzHarry/SimPer.
</p></li>
</ul>

<h3>Title: CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised learning of speech representations. (arXiv:2210.02592v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02592">http://arxiv.org/abs/2210.02592</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02592] CCC-wav2vec 2](http://arxiv.org/abs/2210.02592)</code></li>
<li>Summary: <p>While Self-Supervised Learning has helped reap the benefit of the scale from
the available unlabeled data, the learning paradigms are continuously being
bettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which
uses clustering and an augmentation-based cross-contrastive loss as its
self-supervised objective. Through the clustering module, we scale down the
influence of those negative examples that are highly similar to the positive.
The Cross-Contrastive loss is computed between the encoder output of the
original sample and the quantizer output of its augmentation and vice-versa,
bringing robustness to the pre-training strategy. ccc-wav2vec 2.0 achieves up
to 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on
the test-clean and test-other sets, respectively, of LibriSpeech, without the
use of any language model. The proposed method also achieves up to 14.9%
relative WER improvement over the baseline wav2vec 2.0 when fine-tuned on
Switchboard data. We make all our codes publicly available on GitHub.
</p></li>
</ul>

<h3>Title: Time Will Change Things: An Empirical Study on Dynamic Language Understanding in Social Media Classification. (arXiv:2210.02857v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02857">http://arxiv.org/abs/2210.02857</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02857] Time Will Change Things: An Empirical Study on Dynamic Language Understanding in Social Media Classification](http://arxiv.org/abs/2210.02857)</code></li>
<li>Summary: <p>Language features are ever-evolving in the real-world social media
environment. Many trained models in natural language understanding (NLU),
ineffective in semantic inference for unseen features, might consequently
struggle with the deteriorating performance in dynamicity. To address this
challenge, we empirically study social media NLU in a dynamic setup, where
models are trained on the past data and test on the future. It better reflects
the realistic practice compared to the commonly-adopted static setup of random
data split. To further analyze model adaption to the dynamicity, we explore the
usefulness of leveraging some unlabeled data created after a model is trained.
The performance of unsupervised domain adaption baselines based on
auto-encoding and pseudo-labeling and a joint framework coupling them both are
examined in the experiments. Substantial results on four social media tasks
imply the universally negative effects of evolving environments over
classification accuracy, while auto-encoding and pseudo-labeling
collaboratively show the best robustness in dynamicity.
</p></li>
</ul>

<h3>Title: Binding Language Models in Symbolic Languages. (arXiv:2210.02875v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02875">http://arxiv.org/abs/2210.02875</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02875] Binding Language Models in Symbolic Languages](http://arxiv.org/abs/2210.02875)</code></li>
<li>Summary: <p>Though end-to-end neural approaches have recently been dominating NLP tasks
in both performance and ease-of-use, they lack interpretability and robustness.
We propose Binder, a training-free neural-symbolic framework that maps the task
input to a program, which (1) allows binding a unified API of language model
(LM) functionalities to a programming language (e.g., SQL, Python) to extend
its grammar coverage and thus tackle more diverse questions, (2) adopts an LM
as both the program parser and the underlying model called by the API during
execution, and (3) requires only a few in-context exemplar annotations.
Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only
a few in-context exemplars, Codex is able to identify the part of the task
input that cannot be answerable by the original programming language, correctly
generate API calls to prompt Codex to solve the unanswerable part, and identify
where to place the API calls while being compatible with the original grammar.
In the execution stage, Codex can perform versatile functionalities (e.g.,
commonsense QA, information extraction) given proper prompts in the API calls.
Binder achieves state-of-the-art results on WikiTableQuestions and TabFact
datasets, with explicit output programs that benefit human debugging. Note that
previous best systems are all finetuned on tens of thousands of task-specific
samples, while Binder only uses dozens of annotations as in-context exemplars
without any training. Our code is available at https://github.com/HKUNLP/Binder .
</p></li>
</ul>

<h3>Title: Detecting Narrative Elements in Informational Text. (arXiv:2210.03028v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.03028">http://arxiv.org/abs/2210.03028</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.03028] Detecting Narrative Elements in Informational Text](http://arxiv.org/abs/2210.03028)</code></li>
<li>Summary: <p>Automatic extraction of narrative elements from text, combining narrative
theories with computational models, has been receiving increasing attention
over the last few years. Previous works have utilized the oral narrative theory
by Labov and Waletzky to identify various narrative elements in personal
stories texts. Instead, we direct our focus to informational texts,
specifically news stories. We introduce NEAT (Narrative Elements AnnoTation) -
a novel NLP task for detecting narrative elements in raw text. For this
purpose, we designed a new multi-label narrative annotation scheme, better
suited for informational text (e.g. news media), by adapting elements from the
narrative theory of Labov and Waletzky (Complication and Resolution) and adding
a new narrative element of our own (Success). We then used this scheme to
annotate a new dataset of 2,209 sentences, compiled from 46 news articles from
various category domains. We trained a number of supervised models in several
different setups over the annotated dataset to identify the different narrative
elements, achieving an average F1 score of up to 0.77. The results demonstrate
the holistic nature of our annotation scheme as well as its robustness to
domain category.
</p></li>
</ul>

<h3>Title: Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization. (arXiv:2210.03029v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.03029">http://arxiv.org/abs/2210.03029</a></li>
<li>Code URL: <a href="https://github.com/seonghyeonye/rospr">https://github.com/seonghyeonye/rospr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.03029] Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization](http://arxiv.org/abs/2210.03029)</code></li>
<li>Summary: <p>During zero-shot inference with language models (LMs), using hard prompts
alone may not be able to fully describe the target task. In this paper, we
explore how the retrieval of soft prompts obtained through prompt tuning can
assist hard prompts in zero-shot task generalization. Specifically, we train
soft prompt embeddings for each prompt through prompt tuning, store the samples
of the training instances (hard prompt + input instances) mapped with the
prompt embeddings, and retrieve the corresponding prompt embedding of the
training instance closest to the query instance during inference. Results show
this simple approach enhances the performance of T0 on unseen tasks by
outperforming it on 10 out of 11 datasets as well as improving the mean
accuracy of T0 on BIG-bench benchmark by 2.39% points while adding only 0.007%
additional parameters. Also, using interpolation of multiple embeddings and
variance-based ranking further improve accuracy and robustness to different
evaluation prompts, widening the performance gap. Finally, we find that
retrieving source embeddings trained on similar answer choice formats is more
important than those on similar task types. Model checkpoints and code
implementation are available at https://github.com/seonghyeonye/RoSPr.
</p></li>
</ul>

<h3>Title: Toxicity in Multilingual Machine Translation at Scale. (arXiv:2210.03070v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.03070">http://arxiv.org/abs/2210.03070</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.03070] Toxicity in Multilingual Machine Translation at Scale](http://arxiv.org/abs/2210.03070)</code></li>
<li>Summary: <p>Machine Translation systems can produce different types of errors, some of
which get characterized as critical or catastrophic due to the specific
negative impact they can have on users. Automatic or human evaluation metrics
do not necessarily differentiate between such critical errors and more
innocuous ones. In this paper we focus on one type of critical error: added
toxicity. We evaluate and analyze added toxicity when translating a large
evaluation dataset (HOLISTICBIAS, over 472k sentences, covering 13 demographic
axes) from English into 164 languages. The toxicity automatic evaluation shows
that added toxicity across languages varies from 0% to 5%. The output languages
with the most added toxicity tend to be low-resource ones, and the demographic
axes with the most added toxicity include sexual orientation, gender and sex,
and ability. We also perform human evaluation on a subset of 8 directions,
confirming the prevalence of true added toxicity.
</p></li>
</ul>

<p>We use a measurement of the amount of source contribution to the translation,
where a low source contribution implies hallucination, to interpret what causes
toxicity. We observe that the source contribution is somewhat correlated with
toxicity but that 45.6% of added toxic words have a high source contribution,
suggesting that much of the added toxicity may be due to mistranslations.
Combining the signal of source contribution level with a measurement of
translation robustness allows us to flag 22.3% of added toxicity, suggesting
that added toxicity may be related to both hallucination and the stability of
translations in different contexts. Given these findings, our recommendations
to reduce added toxicity are to curate training data to avoid mistranslations,
mitigate hallucination and check unstable translations.
</p>

<h3>Title: A Closer Look at Robustness to L-infinity and Spatial Perturbations and their Composition. (arXiv:2210.02577v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02577">http://arxiv.org/abs/2210.02577</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02577] A Closer Look at Robustness to L-infinity and Spatial Perturbations and their Composition](http://arxiv.org/abs/2210.02577)</code></li>
<li>Summary: <p>In adversarial machine learning, the popular $\ell_\infty$ threat model has
been the focus of much previous work. While this mathematical definition of
imperceptibility successfully captures an infinite set of additive image
transformations that a model should be robust to, this is only a subset of all
transformations which leave the semantic label of an image unchanged. Indeed,
previous work also considered robustness to spatial attacks as well as other
semantic transformations; however, designing defense methods against the
composition of spatial and $\ell_{\infty}$ perturbations remains relatively
underexplored. In the following, we improve the understanding of this seldom
investigated compositional setting. We prove theoretically that no linear
classifier can achieve more than trivial accuracy against a composite adversary
in a simple statistical setting, illustrating its difficulty. We then
investigate how state-of-the-art $\ell_{\infty}$ defenses can be adapted to
this novel threat model and study their performance against compositional
attacks. We find that our newly proposed TRADES$<em>{\text{All}}$ strategy
performs the strongest of all. Analyzing its logit's Lipschitz constant for RT
transformations of different sizes, we find that TRADES$</em>{\text{All}}$ remains
stable over a wide range of RT transformations with and without $\ell_\infty$
perturbations.
</p></li>
</ul>

<h3>Title: Flow Matching for Generative Modeling. (arXiv:2210.02747v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02747">http://arxiv.org/abs/2210.02747</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02747] Flow Matching for Generative Modeling](http://arxiv.org/abs/2210.02747)</code></li>
<li>Summary: <p>We introduce a new paradigm for generative modeling built on Continuous
Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale.
Specifically, we present the notion of Flow Matching (FM), a simulation-free
approach for training CNFs based on regressing vector fields of fixed
conditional probability paths. Flow Matching is compatible with a general
family of Gaussian probability paths for transforming between noise and data
samples -- which subsumes existing diffusion paths as specific instances.
Interestingly, we find that employing FM with diffusion paths results in a more
robust and stable alternative for training diffusion models. Furthermore, Flow
Matching opens the door to training CNFs with other, non-diffusion probability
paths. An instance of particular interest is using Optimal Transport (OT)
displacement interpolation to define the conditional probability paths. These
paths are more efficient than diffusion paths, provide faster training and
sampling, and result in better generalization. Training CNFs using Flow
Matching on ImageNet leads to state-of-the-art performance in terms of both
likelihood and sample quality, and allows fast and reliable sample generation
using off-the-shelf numerical ODE solvers.
</p></li>
</ul>

<h3>Title: Communication-Efficient and Drift-Robust Federated Learning via Elastic Net. (arXiv:2210.02940v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02940">http://arxiv.org/abs/2210.02940</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02940] Communication-Efficient and Drift-Robust Federated Learning via Elastic Net](http://arxiv.org/abs/2210.02940)</code></li>
<li>Summary: <p>Federated learning (FL) is a distributed method to train a global model over
a set of local clients while keeping data localized. It reduces the risks of
privacy and security but faces important challenges including expensive
communication costs and client drift issues. To address these issues, we
propose FedElasticNet, a communication-efficient and drift-robust FL framework
leveraging the elastic net. It repurposes two types of the elastic net
regularizers (i.e., $\ell_1$ and $\ell_2$ penalties on the local model
updates): (1) the $\ell_1$-norm regularizer sparsifies the local updates to
reduce the communication costs and (2) the $\ell_2$-norm regularizer resolves
the client drift problem by limiting the impact of drifting local updates due
to data heterogeneity. FedElasticNet is a general framework for FL; hence,
without additional costs, it can be integrated into prior FL techniques, e.g.,
FedAvg, FedProx, SCAFFOLD, and FedDyn. We show that our framework effectively
resolves the communication cost and client drift problems simultaneously.
</p></li>
</ul>

<h3>Title: Unmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?. (arXiv:2210.03044v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.03044">http://arxiv.org/abs/2210.03044</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.03044] Unmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?](http://arxiv.org/abs/2210.03044)</code></li>
<li>Summary: <p>Modern deep learning involves training costly, highly overparameterized
networks, thus motivating the search for sparser networks that can still be
trained to the same accuracy as the full network (i.e. matching). Iterative
magnitude pruning (IMP) is a state of the art algorithm that can find such
highly sparse matching subnetworks, known as winning tickets. IMP operates by
iterative cycles of training, masking smallest magnitude weights, rewinding
back to an early training point, and repeating. Despite its simplicity, the
underlying principles for when and how IMP finds winning tickets remain
elusive. In particular, what useful information does an IMP mask found at the
end of training convey to a rewound network near the beginning of training? How
does SGD allow the network to extract this information? And why is iterative
pruning needed? We develop answers in terms of the geometry of the error
landscape. First, we find that$\unicode{x2014}$at higher
sparsities$\unicode{x2014}$pairs of pruned networks at successive pruning
iterations are connected by a linear path with zero error barrier if and only
if they are matching. This indicates that masks found at the end of training
convey the identity of an axial subspace that intersects a desired linearly
connected mode of a matching sublevel set. Second, we show SGD can exploit this
information due to a strong form of robustness: it can return to this mode
despite strong perturbations early in training. Third, we show how the flatness
of the error landscape at the end of training determines a limit on the
fraction of weights that can be pruned at each iteration of IMP. Finally, we
show that the role of retraining in IMP is to find a network with new small
weights to prune. Overall, these results make progress toward demystifying the
existence of winning tickets by revealing the fundamental role of error
landscape geometry.
</p></li>
</ul>

<h3>Title: Distributionally Adaptive Meta Reinforcement Learning. (arXiv:2210.03104v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.03104">http://arxiv.org/abs/2210.03104</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.03104] Distributionally Adaptive Meta Reinforcement Learning](http://arxiv.org/abs/2210.03104)</code></li>
<li>Summary: <p>Meta-reinforcement learning algorithms provide a data-driven way to acquire
policies that quickly adapt to many tasks with varying rewards or dynamics
functions. However, learned meta-policies are often effective only on the exact
task distribution on which they were trained and struggle in the presence of
distribution shift of test-time rewards or transition dynamics. In this work,
we develop a framework for meta-RL algorithms that are able to behave
appropriately under test-time distribution shifts in the space of tasks. Our
framework centers on an adaptive approach to distributional robustness that
trains a population of meta-policies to be robust to varying levels of
distribution shift. When evaluated on a potentially shifted test-time
distribution of tasks, this allows us to choose the meta-policy with the most
appropriate level of robustness, and use it to perform fast adaptation. We
formally show how our framework allows for improved regret under distribution
shift, and empirically show its efficacy on simulated robotics problems under a
wide range of distribution shifts.
</p></li>
</ul>

<h3>Title: TgDLF2.0: Theory-guided deep-learning for electrical load forecasting via Transformer and transfer learning. (arXiv:2210.02448v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02448">http://arxiv.org/abs/2210.02448</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02448] TgDLF2](http://arxiv.org/abs/2210.02448)</code></li>
<li>Summary: <p>Electrical energy is essential in today's society. Accurate electrical load
forecasting is beneficial for better scheduling of electricity generation and
saving electrical energy. In this paper, we propose theory-guided deep-learning
load forecasting 2.0 (TgDLF2.0) to solve this issue, which is an improved
version of the theory-guided deep-learning framework for load forecasting via
ensemble long short-term memory (TgDLF). TgDLF2.0 introduces the deep-learning
model Transformer and transfer learning on the basis of dividing the electrical
load into dimensionless trends and local fluctuations, which realizes the
utilization of domain knowledge, captures the long-term dependency of the load
series, and is more appropriate for realistic scenarios with scarce samples.
Cross-validation experiments on different districts show that TgDLF2.0 is
approximately 16% more accurate than TgDLF and saves more than half of the
training time. TgDLF2.0 with 50% weather noise has the same accuracy as TgDLF
without noise, which proves its robustness. We also preliminarily mine the
interpretability of Transformer in TgDLF2.0, which may provide future potential
for better theory guidance. Furthermore, experiments demonstrate that transfer
learning can accelerate convergence of the model in half the number of training
epochs and achieve better performance.
</p></li>
</ul>

<h3>Title: Bi-Stride Multi-Scale Graph Neural Network for Mesh-Based Physical Simulation. (arXiv:2210.02573v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02573">http://arxiv.org/abs/2210.02573</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02573] Bi-Stride Multi-Scale Graph Neural Network for Mesh-Based Physical Simulation](http://arxiv.org/abs/2210.02573)</code></li>
<li>Summary: <p>Learning physical systems on unstructured meshes by flat Graph neural
networks (GNNs) faces the challenge of modeling the long-range interactions due
to the scaling complexity w.r.t. the number of nodes, limiting the
generalization under mesh refinement. On regular grids, the convolutional
neural networks (CNNs) with a U-net structure can resolve this challenge by
efficient stride, pooling, and upsampling operations. Nonetheless, these tools
are much less developed for graph neural networks (GNNs), especially when GNNs
are employed for learning large-scale mesh-based physics. The challenges arise
from the highly irregular meshes and the lack of effective ways to construct
the multi-level structure without losing connectivity. Inspired by the
bipartite graph determination algorithm, we introduce Bi-Stride Multi-Scale
Graph Neural Network (BSMS-GNN) by proposing \textit{bi-stride} as a simple
pooling strategy for building the multi-level GNN. \textit{Bi-stride} pools
nodes by striding every other BFS frontier; it 1) works robustly on any
challenging mesh in the wild, 2) avoids using a mesh generator at coarser
levels, 3) avoids the spatial proximity for building coarser levels, and 4)
uses non-parametrized aggregating/returning instead of MLPs during pooling and
unpooling. Experiments show that our framework significantly outperforms the
state-of-the-art method's computational efficiency in representative
physics-based simulation cases.
</p></li>
</ul>

<h3>Title: Uncertainty Estimation for Multi-view Data: The Power of Seeing the Whole Picture. (arXiv:2210.02676v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02676">http://arxiv.org/abs/2210.02676</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02676] Uncertainty Estimation for Multi-view Data: The Power of Seeing the Whole Picture](http://arxiv.org/abs/2210.02676)</code></li>
<li>Summary: <p>Uncertainty estimation is essential to make neural networks trustworthy in
real-world applications. Extensive research efforts have been made to quantify
and reduce predictive uncertainty. However, most existing works are designed
for unimodal data, whereas multi-view uncertainty estimation has not been
sufficiently investigated. Therefore, we propose a new multi-view
classification framework for better uncertainty estimation and out-of-domain
sample detection, where we associate each view with an uncertainty-aware
classifier and combine the predictions of all the views in a principled way.
The experimental results with real-world datasets demonstrate that our proposed
approach is an accurate, reliable, and well-calibrated classifier, which
predominantly outperforms the multi-view baselines tested in terms of expected
calibration error, robustness to noise, and accuracy for the in-domain sample
classification and the out-of-domain sample detection tasks.
</p></li>
</ul>

<h3>Title: Paging with Succinct Predictions. (arXiv:2210.02775v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02775">http://arxiv.org/abs/2210.02775</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02775] Paging with Succinct Predictions](http://arxiv.org/abs/2210.02775)</code></li>
<li>Summary: <p>Paging is a prototypical problem in the area of online algorithms. It has
also played a central role in the development of learning-augmented algorithms
-- a recent line of research that aims to ameliorate the shortcomings of
classical worst-case analysis by giving algorithms access to predictions. Such
predictions can typically be generated using a machine learning approach, but
they are inherently imperfect. Previous work on learning-augmented paging has
investigated predictions on (i) when the current page will be requested again
(reoccurrence predictions), (ii) the current state of the cache in an optimal
algorithm (state predictions), (iii) all requests until the current page gets
requested again, and (iv) the relative order in which pages are requested.
</p></li>
</ul>

<p>We study learning-augmented paging from the new perspective of requiring the
least possible amount of predicted information. More specifically, the
predictions obtained alongside each page request are limited to one bit only.
We consider two natural such setups: (i) discard predictions, in which the
predicted bit denotes whether or not it is ``safe'' to evict this page, and
(ii) phase predictions, where the bit denotes whether the current page will be
requested in the next phase (for an appropriate partitioning of the input into
phases). We develop algorithms for each of the two setups that satisfy all
three desirable properties of learning-augmented algorithms -- that is, they
are consistent, robust and smooth -- despite being limited to a one-bit
prediction per request. We also present lower bounds establishing that our
algorithms are essentially best possible.
</p>

<h3>Title: SynBench: Task-Agnostic Benchmarking of Pretrained Representations using Synthetic Data. (arXiv:2210.02989v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02989">http://arxiv.org/abs/2210.02989</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02989] SynBench: Task-Agnostic Benchmarking of Pretrained Representations using Synthetic Data](http://arxiv.org/abs/2210.02989)</code></li>
<li>Summary: <p>Recent success in fine-tuning large models, that are pretrained on broad data
at scale, on downstream tasks has led to a significant paradigm shift in deep
learning, from task-centric model design to task-agnostic representation
learning and task-specific fine-tuning. As the representations of pretrained
models are used as a foundation for different downstream tasks, this paper
proposes a new task-agnostic framework, \textit{SynBench}, to measure the
quality of pretrained representations using synthetic data. We set up a
reference by a theoretically-derived robustness-accuracy tradeoff of the class
conditional Gaussian mixture. Given a pretrained model, the representations of
data synthesized from the Gaussian mixture are used to compare with our
reference to infer the quality.By comparing the ratio of area-under-curve
between the raw data and their representations, SynBench offers a quantifiable
score for robustness-accuracy performance benchmarking. Our framework applies
to a wide range of pretrained models taking continuous data inputs and is
independent of the downstream tasks and datasets. Evaluated with several
pretrained vision transformer models, the experimental results show that our
SynBench score well matches the actual linear probing performance of the
pre-trained model when fine-tuned on downstream tasks. Moreover, our framework
can be used to inform the design of robust linear probing on pretrained
representations to mitigate the robustness-accuracy tradeoff in downstream
tasks.
</p></li>
</ul>

<h3>Title: A Better Way to Decay: Proximal Gradient Training Algorithms for Neural Nets. (arXiv:2210.03069v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.03069">http://arxiv.org/abs/2210.03069</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.03069] A Better Way to Decay: Proximal Gradient Training Algorithms for Neural Nets](http://arxiv.org/abs/2210.03069)</code></li>
<li>Summary: <p>Weight decay is one of the most widely used forms of regularization in deep
learning, and has been shown to improve generalization and robustness. The
optimization objective driving weight decay is a sum of losses plus a term
proportional to the sum of squared weights. This paper argues that stochastic
gradient descent (SGD) may be an inefficient algorithm for this objective. For
neural networks with ReLU activations, solutions to the weight decay objective
are equivalent to those of a different objective in which the regularization
term is instead a sum of products of $\ell_2$ (not squared) norms of the input
and output weights associated each ReLU. This alternative (and effectively
equivalent) regularization suggests a novel proximal gradient algorithm for
network training. Theory and experiments support the new training approach,
showing that it can converge much faster to the sparse solutions it shares with
standard weight decay training.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: IJCB 2022 Mobile Behavioral Biometrics Competition (MobileB2C). (arXiv:2210.03072v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.03072">http://arxiv.org/abs/2210.03072</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.03072] IJCB 2022 Mobile Behavioral Biometrics Competition (MobileB2C)](http://arxiv.org/abs/2210.03072)</code></li>
<li>Summary: <p>This paper describes the experimental framework and results of the IJCB 2022
Mobile Behavioral Biometrics Competition (MobileB2C). The aim of MobileB2C is
benchmarking mobile user authentication systems based on behavioral biometric
traits transparently acquired by mobile devices during ordinary Human-Computer
Interaction (HCI), using a novel public database, BehavePassDB, and a standard
experimental protocol. The competition is divided into four tasks corresponding
to typical user activities: keystroke, text reading, gallery swiping, and
tapping. The data are composed of touchscreen data and several background
sensor data simultaneously acquired. "Random" (different users with different
devices) and "skilled" (different user on the same device attempting to imitate
the legitimate one) impostor scenarios are considered. The results achieved by
the participants show the feasibility of user authentication through behavioral
biometrics, although this proves to be a non-trivial challenge. MobileB2C will
be established as an on-going competition.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: U3E: Unsupervised and Erasure-based Evidence Extraction for Machine Reading Comprehension. (arXiv:2210.02621v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02621">http://arxiv.org/abs/2210.02621</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02621] U3E: Unsupervised and Erasure-based Evidence Extraction for Machine Reading Comprehension](http://arxiv.org/abs/2210.02621)</code></li>
<li>Summary: <p>More tasks in Machine Reading Comprehension(MRC) require, in addition to
answer prediction, the extraction of evidence sentences that support the
answer. However, the annotation of supporting evidence sentences is usually
time-consuming and labor-intensive. In this paper, to address this issue and
considering that most of the existing extraction methods are semi-supervised,
we propose an unsupervised evidence extraction method (U3E). U3E takes the
changes after sentence-level feature erasure in the document as input,
simulating the decline in problem-solving ability caused by human memory
decline. In order to make selections on the basis of fully understanding the
semantics of the original text, we also propose metrics to quickly select the
optimal memory model for this input changes. To compare U3E with typical
evidence extraction methods and investigate its effectiveness in evidence
extraction, we conduct experiments on different datasets. Experimental results
show that U3E is simple but effective, not only extracting evidence more
accurately, but also significantly improving model performance.
</p></li>
</ul>

<h3>Title: Look Ma, Only 400 Samples! Revisiting the Effectiveness of Automatic N-Gram Rule Generation for Spelling Normalization in Filipino. (arXiv:2210.02675v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02675">http://arxiv.org/abs/2210.02675</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02675] Look Ma, Only 400 Samples! Revisiting the Effectiveness of Automatic N-Gram Rule Generation for Spelling Normalization in Filipino](http://arxiv.org/abs/2210.02675)</code></li>
<li>Summary: <p>With 84.75 million Filipinos online, the ability for models to process online
text is crucial for developing Filipino NLP applications. To this end, spelling
correction is a crucial preprocessing step for downstream processing. However,
the lack of data prevents the use of language models for this task. In this
paper, we propose an N-Gram + Damerau Levenshtein distance model with automatic
rule extraction. We train the model on 300 samples, and show that despite
limited training data, it achieves good performance and outperforms other deep
learning approaches in terms of accuracy and edit distance. Moreover, the model
(1) requires little compute power, (2) trains in little time, thus allowing for
retraining, and (3) is easily interpretable, allowing for direct
troubleshooting, highlighting the success of traditional approaches over more
complex deep learning models in settings where data is unavailable.
</p></li>
</ul>

<h3>Title: Geodesic Graph Neural Network for Efficient Graph Representation Learning. (arXiv:2210.02636v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02636">http://arxiv.org/abs/2210.02636</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02636] Geodesic Graph Neural Network for Efficient Graph Representation Learning](http://arxiv.org/abs/2210.02636)</code></li>
<li>Summary: <p>Recently, Graph Neural Networks (GNNs) have been applied to graph learning
tasks and achieved state-of-the-art results. However, many competitive methods
employ preprocessing on the target nodes, such as subgraph extraction and
customized labeling, to capture some information that is hard to be learned by
normal GNNs. Such operations are time-consuming and do not scale to large
graphs. In this paper, we propose an efficient GNN framework called Geodesic
GNN (GDGNN). It injects conditional relationships between nodes into the model
without labeling. Specifically, we view the shortest paths between two nodes as
the spatial graph context of the neighborhood around them. The GNN embeddings
of nodes on the shortest paths are used to generate geodesic representations.
Conditioned on the geodesic representations, GDGNN is able to generate node,
link, and graph representations that carry much richer structural information
than plain GNNs. We theoretically prove that GDGNN is more powerful than plain
GNNs, and present experimental results to show that GDGNN achieves highly
competitive performance with state-of-the-art GNN models on link prediction and
graph classification tasks while taking significantly less time.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Learning with Server Learning: Enhancing Performance for Non-IID Data. (arXiv:2210.02614v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02614">http://arxiv.org/abs/2210.02614</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02614] Federated Learning with Server Learning: Enhancing Performance for Non-IID Data](http://arxiv.org/abs/2210.02614)</code></li>
<li>Summary: <p>Federated learning (FL) has become a popular means for distributed learning
at clients using local data samples. However, recent studies have shown that FL
may experience slow learning and poor performance when client data are not
independent and identically distributed (IID). This paper proposes a new
federated learning algorithm, where the central server has access to a small
dataset, learns from it, and fuses the knowledge into the global model through
the federated learning process. This new approach, referred to as Federated
learning with Server Learning or FSL, is complementary to and can be combined
with other FL learning algorithms. We prove the convergence of FSL and
demonstrate its benefits through analysis and simulations. We also reveal an
inherent trade-off: when the current model is far from any local minimizer,
server learning can significantly improve and accelerate FL. On the other hand,
when the model is close to a local minimizer, server learning could potentially
affect the convergence neighborhood of FL due to variances in the estimated
gradient used by the server. We show via simulations that such trade-off can be
tuned easily to provide significant benefits, even when the server dataset is
very small.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: A Review of Uncertainty Calibration in Pretrained Object Detectors. (arXiv:2210.02935v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02935">http://arxiv.org/abs/2210.02935</a></li>
<li>Code URL: <a href="https://github.com/ies-research/uncertainty-object-detection">https://github.com/ies-research/uncertainty-object-detection</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02935] A Review of Uncertainty Calibration in Pretrained Object Detectors](http://arxiv.org/abs/2210.02935)</code></li>
<li>Summary: <p>In the field of deep learning based computer vision, the development of deep
object detection has led to unique paradigms (e.g., two-stage or set-based) and
architectures (e.g., Faster-RCNN or DETR) which enable outstanding performance
on challenging benchmark datasets. Despite this, the trained object detectors
typically do not reliably assess uncertainty regarding their own knowledge, and
the quality of their probabilistic predictions is usually poor. As these are
often used to make subsequent decisions, such inaccurate probabilistic
predictions must be avoided. In this work, we investigate the uncertainty
calibration properties of different pretrained object detection architectures
in a multi-class setting. We propose a framework to ensure a fair, unbiased,
and repeatable evaluation and conduct detailed analyses assessing the
calibration under distributional changes (e.g., distributional shift and
application to out-of-distribution data). Furthermore, by investigating the
influence of different detector paradigms, post-processing steps, and suitable
choices of metrics, we deliver novel insights into why poor detector
calibration emerges. Based on these insights, we are able to improve the
calibration of a detector by simply finetuning its last layer.
</p></li>
</ul>

<h3>Title: Reinforcement Learning with Large Action Spaces for Neural Machine Translation. (arXiv:2210.03053v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.03053">http://arxiv.org/abs/2210.03053</a></li>
<li>Code URL: <a href="https://github.com/AsafYehudai/Reinforcement-Learning-with-Large-Action-Spaces-for-Neural-Machine-Translation">https://github.com/AsafYehudai/Reinforcement-Learning-with-Large-Action-Spaces-for-Neural-Machine-Translation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.03053] Reinforcement Learning with Large Action Spaces for Neural Machine Translation](http://arxiv.org/abs/2210.03053)</code></li>
<li>Summary: <p>Applying Reinforcement learning (RL) following maximum likelihood estimation
(MLE) pre-training is a versatile method for enhancing neural machine
translation (NMT) performance. However, recent work has argued that the gains
produced by RL for NMT are mostly due to promoting tokens that have already
received a fairly high probability in pre-training. We hypothesize that the
large action space is a main obstacle to RL's effectiveness in MT, and conduct
two sets of experiments that lend support to our hypothesis. First, we find
that reducing the size of the vocabulary improves RL's effectiveness. Second,
we find that effectively reducing the dimension of the action space without
changing the vocabulary also yields notable improvement as evaluated by BLEU,
semantic similarity, and human evaluation. Indeed, by initializing the
network's final fully connected layer (that maps the network's internal
dimension to the vocabulary dimension), with a layer that generalizes over
similar actions, we obtain a substantial improvement in RL performance: 1.5
BLEU points on average.
</p></li>
</ul>

<h3>Title: A Human Rights-Based Approach to Responsible AI. (arXiv:2210.02667v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02667">http://arxiv.org/abs/2210.02667</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02667] A Human Rights-Based Approach to Responsible AI](http://arxiv.org/abs/2210.02667)</code></li>
<li>Summary: <p>Research on fairness, accountability, transparency and ethics of AI-based
interventions in society has gained much-needed momentum in recent years.
However it lacks an explicit alignment with a set of normative values and
principles that guide this research and interventions. Rather, an implicit
consensus is often assumed to hold for the values we impart into our models -
something that is at odds with the pluralistic world we live in. In this paper,
we put forth the doctrine of universal human rights as a set of globally
salient and cross-culturally recognized set of values that can serve as a
grounding framework for explicit value alignment in responsible AI - and
discuss its efficacy as a framework for civil society partnership and
participation. We argue that a human rights framework orients the research in
this space away from the machines and the risks of their biases, and towards
humans and the risks to their rights, essentially helping to center the
conversation around who is harmed, what harms they face, and how those harms
may be mitigated.
</p></li>
</ul>

<h3>Title: Equalizing Credit Opportunity in Algorithms: Aligning Algorithmic Fairness Research with U.S. Fair Lending Regulation. (arXiv:2210.02516v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02516">http://arxiv.org/abs/2210.02516</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02516] Equalizing Credit Opportunity in Algorithms: Aligning Algorithmic Fairness Research with U](http://arxiv.org/abs/2210.02516)</code></li>
<li>Summary: <p>Credit is an essential component of financial wellbeing in America, and
unequal access to it is a large factor in the economic disparities between
demographic groups that exist today. Today, machine learning algorithms,
sometimes trained on alternative data, are increasingly being used to determine
access to credit, yet research has shown that machine learning can encode many
different versions of "unfairness," thus raising the concern that banks and
other financial institutions could -- potentially unwittingly -- engage in
illegal discrimination through the use of this technology. In the US, there are
laws in place to make sure discrimination does not happen in lending and
agencies charged with enforcing them. However, conversations around fair credit
models in computer science and in policy are often misaligned: fair machine
learning research often lacks legal and practical considerations specific to
existing fair lending policy, and regulators have yet to issue new guidance on
how, if at all, credit risk models should be utilizing practices and techniques
from the research community. This paper aims to better align these sides of the
conversation. We describe the current state of credit discrimination regulation
in the United States, contextualize results from fair ML research to identify
the specific fairness concerns raised by the use of machine learning in
lending, and discuss regulatory opportunities to address these concerns.
</p></li>
</ul>

<h3>Title: Uncovering the Structural Fairness in Graph Contrastive Learning. (arXiv:2210.03011v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.03011">http://arxiv.org/abs/2210.03011</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.03011] Uncovering the Structural Fairness in Graph Contrastive Learning](http://arxiv.org/abs/2210.03011)</code></li>
<li>Summary: <p>Recent studies show that graph convolutional network (GCN) often performs
worse for low-degree nodes, exhibiting the so-called structural unfairness for
graphs with long-tailed degree distributions prevalent in the real world. Graph
contrastive learning (GCL), which marries the power of GCN and contrastive
learning, has emerged as a promising self-supervised approach for learning node
representations. How does GCL behave in terms of structural fairness?
Surprisingly, we find that representations obtained by GCL methods are already
fairer to degree bias than those learned by GCN. We theoretically show that
this fairness stems from intra-community concentration and inter-community
scatter properties of GCL, resulting in a much clear community structure to
drive low-degree nodes away from the community boundary. Based on our
theoretical analysis, we further devise a novel graph augmentation method,
called GRAph contrastive learning for DEgree bias (GRADE), which applies
different strategies to low- and high-degree nodes. Extensive experiments on
various benchmarks and evaluation protocols validate the effectiveness of the
proposed method.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Just ClozE! A Fast and Simple Method for Evaluating the Factual Consistency in Abstractive Summarization. (arXiv:2210.02804v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02804">http://arxiv.org/abs/2210.02804</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02804] Just ClozE! A Fast and Simple Method for Evaluating the Factual Consistency in Abstractive Summarization](http://arxiv.org/abs/2210.02804)</code></li>
<li>Summary: <p>The issue of factual consistency in abstractive summarization has attracted
much attention in recent years, and the evaluation of factual consistency
between summary and document has become an important and urgent task. Most of
the current evaluation metrics are adopted from the question answering (QA).
However, the application of QA-based metrics is extremely time-consuming in
practice, causing the iteration cycle of abstractive summarization research to
be severely prolonged. In this paper, we propose a new method called ClozE to
evaluate factual consistency by cloze model, instantiated based on masked
language model(MLM), with strong interpretability and substantially higher
speed. We demonstrate that ClozE can reduce the evaluation time by nearly
96$\%$ relative to QA-based metrics while retaining their interpretability and
performance through experiments on six human-annotated datasets and a
meta-evaluation benchmark GO FIGURE \citep{gabriel2020go}. We also implement
experiments to further demonstrate more characteristics of ClozE in terms of
performance and speed. In addition, we conduct an experimental analysis of the
limitations of ClozE, which suggests future research directions. The code and
models for ClozE will be released upon the paper acceptance.
</p></li>
</ul>

<h3>Title: Learning Disentangled Representations for Natural Language Definitions. (arXiv:2210.02898v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02898">http://arxiv.org/abs/2210.02898</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02898] Learning Disentangled Representations for Natural Language Definitions](http://arxiv.org/abs/2210.02898)</code></li>
<li>Summary: <p>Disentangling the encodings of neural models is a fundamental aspect for
improving interpretability, semantic control and downstream task performance in
Natural Language Processing. Currently, most disentanglement methods are
unsupervised or rely on synthetic datasets with known generative factors. We
argue that recurrent syntactic and semantic regularities in textual data can be
used to provide the models with both structural biases and generative factors.
We leverage the semantic structures present in a representative and
semantically dense category of sentence types, definitional sentences, for
training a Variational Autoencoder to learn disentangled representations. Our
experimental results show that the proposed model outperforms unsupervised
baselines on several qualitative and quantitative benchmarks for
disentanglement, and it also improves the results in the downstream task of
definition modeling.
</p></li>
</ul>

<h3>Title: Explainable Verbal Deception Detection using Transformers. (arXiv:2210.03080v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.03080">http://arxiv.org/abs/2210.03080</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.03080] Explainable Verbal Deception Detection using Transformers](http://arxiv.org/abs/2210.03080)</code></li>
<li>Summary: <p>People are regularly confronted with potentially deceptive statements (e.g.,
fake news, misleading product reviews, or lies about activities). Only few
works on automated text-based deception detection have exploited the potential
of deep learning approaches. A critique of deep-learning methods is their lack
of interpretability, preventing us from understanding the underlying
(linguistic) mechanisms involved in deception. However, recent advancements
have made it possible to explain some aspects of such models. This paper
proposes and evaluates six deep-learning models, including combinations of BERT
(and RoBERTa), MultiHead Attention, co-attentions, and transformers. To
understand how the models reach their decisions, we then examine the model's
predictions with LIME. We then zoom in on vocabulary uniqueness and the
correlation of LIWC categories with the outcome class (truthful vs deceptive).
The findings suggest that our transformer-based models can enhance automated
deception detection performances (+2.11% in accuracy) and show significant
differences pertinent to the usage of LIWC features in truthful and deceptive
statements.
</p></li>
</ul>

<h3>Title: Fault Diagnosis using eXplainable AI: a Transfer Learning-based Approach for Rotating Machinery exploiting Augmented Synthetic Data. (arXiv:2210.02974v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02974">http://arxiv.org/abs/2210.02974</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02974] Fault Diagnosis using eXplainable AI: a Transfer Learning-based Approach for Rotating Machinery exploiting Augmented Synthetic Data](http://arxiv.org/abs/2210.02974)</code></li>
<li>Summary: <p>Artificial Intelligence (AI) is one of the approaches that has been proposed
to analyze the collected data (e.g., vibration signals) providing a diagnosis
of the asset's operating condition. It is known that models trained with
labeled data (supervised) achieve excellent results, but two main problems make
their application in production processes difficult: (i) impossibility or long
time to obtain a sample of all operational conditions (since faults seldom
happen) and (ii) high cost of experts to label all acquired data. Another
limitating factor for the applicability of AI approaches in this context is the
lack of interpretability of the models (black-boxes), which reduces the
confidence of the diagnosis and trust/adoption from users. To overcome these
problems, a new generic and interpretable approach for classifying faults in
rotating machinery based on transfer learning from augmented synthetic data to
real rotating machinery is here proposed, namelly FaultD-XAI (Fault Diagnosis
using eXplainable AI). To provide scalability using transfer learning,
synthetic vibration signals are created mimicking the characteristic behavior
of failures in operation. The application of Gradient-weighted Class Activation
Mapping (Grad-CAM) with 1D Convolutional Neural Network (1D CNN) allows the
interpretation of results, supporting the user in decision making and
increasing diagnostic confidence. The proposed approach not only obtained
promising diagnostic performance, but was also able to learn characteristics
used by experts to identify conditions in a source domain and apply them in
another target domain. The experimental results suggest a promising approach on
exploiting transfer learning, synthetic data and explainable artificial
intelligence for fault diagnosis. Lastly, to guarantee reproducibility and
foster research in the field, the developed dataset is made publicly available.
</p></li>
</ul>

<h3>Title: Transformers Implement First-Order Logic with Majority Quantifiers. (arXiv:2210.02671v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02671">http://arxiv.org/abs/2210.02671</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02671] Transformers Implement First-Order Logic with Majority Quantifiers](http://arxiv.org/abs/2210.02671)</code></li>
<li>Summary: <p>Characterizing the implicit structure of the computation within neural
networks is a foundational problem in the area of deep learning
interpretability. Can their inner decision process be captured symbolically in
some familiar logic? We show that any transformer neural network can be
translated into an equivalent fixed-size first-order logic formula which may
also use majority quantifiers. The idea is to simulate transformers with highly
uniform threshold circuits and leverage known theoretical connections between
circuits and logic. Our findings also reveal the surprising fact that the
entire transformer computation can be reduced merely to the division of two
(large) integers. While our results are most pertinent for transformers, they
apply equally to a broader class of neural network architectures, namely those
with a fixed-depth uniform computation graph made up of standard neural net
components, which includes feedforward and convolutional networks.
</p></li>
</ul>

<h3>Title: Continuous Diagnosis and Prognosis by Controlling the Update Process of Deep Neural Networks. (arXiv:2210.02719v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.02719">http://arxiv.org/abs/2210.02719</a></li>
<li>Code URL: <a href="https://github.com/scxsunchenxi/ccts">https://github.com/scxsunchenxi/ccts</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.02719] Continuous Diagnosis and Prognosis by Controlling the Update Process of Deep Neural Networks](http://arxiv.org/abs/2210.02719)</code></li>
<li>Summary: <p>Continuous diagnosis and prognosis are essential for intensive care patients.
It can provide more opportunities for timely treatment and rational resource
allocation, especially for sepsis, a main cause of death in ICU, and COVID-19,
a new worldwide epidemic. Although deep learning methods have shown their great
superiority in many medical tasks, they tend to catastrophically forget, over
fit, and get results too late when performing diagnosis and prognosis in the
continuous mode. In this work, we summarized the three requirements of this
task, proposed a new concept, continuous classification of time series (CCTS),
and designed a novel model training method, restricted update strategy of
neural networks (RU). In the context of continuous prognosis, our method
outperformed all baselines and achieved the average accuracy of 90%, 97%, and
85% on sepsis prognosis, COVID-19 mortality prediction, and eight diseases
classification. Superiorly, our method can also endow deep learning with
interpretability, having the potential to explore disease mechanisms and
provide a new horizon for medical research. We have achieved disease staging
for sepsis and COVID-19, discovering four stages and three stages with their
typical biomarkers respectively. Further, our method is a data-agnostic and
model-agnostic plug-in, it can be used to continuously prognose other diseases
with staging and even implement CCTS in other fields.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
