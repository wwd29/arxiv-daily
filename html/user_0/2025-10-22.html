<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-22</h1>
<h3>Title: From Noise to Laws: Regularized Time-Series Forecasting via Denoised Dynamic Graphs</h3>
<ul>
<li><strong>Authors: </strong>Hongwei Ma, Junbin Gao, Minh-ngoc Tran</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17817">https://arxiv.org/abs/2510.17817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17817">https://arxiv.org/pdf/2510.17817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17817]] From Noise to Laws: Regularized Time-Series Forecasting via Denoised Dynamic Graphs(https://arxiv.org/abs/2510.17817)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Long-horizon multivariate time-series forecasting is challenging because realistic predictions must (i) denoise heterogeneous signals, (ii) track time-varying cross-series dependencies, and (iii) remain stable and physically plausible over long rollout horizons. We present PRISM, which couples a score-based diffusion preconditioner with a dynamic, correlation-thresholded graph encoder and a forecast head regularized by generic physics penalties. We prove contraction of the induced horizon dynamics under mild conditions and derive Lipschitz bounds for graph blocks, explaining the model's robustness. On six standard benchmarks , PRISM achieves consistent SOTA with strong MSE and MAE gains.</li>
</ul>

<h3>Title: GRETEL: A Goal-driven Retrieval and Execution-based Trial Framework for LLM Tool Selection Enhancing</h3>
<ul>
<li><strong>Authors: </strong>Zongze Wu, Yani Guo, Churong Liang, Runnan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17843">https://arxiv.org/abs/2510.17843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17843">https://arxiv.org/pdf/2510.17843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17843]] GRETEL: A Goal-driven Retrieval and Execution-based Trial Framework for LLM Tool Selection Enhancing(https://arxiv.org/abs/2510.17843)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Despite remarkable advances in Large Language Model capabilities, tool retrieval for agent-based systems remains fundamentally limited by reliance on semantic similarity, which fails to capture functional viability. Current methods often retrieve textually relevant but functionally inoperative tools due to parameter mismatches, authentication failures, and execution constraints--a phenomenon we term the semantic-functional gap. We introduce GRETEL, to address this gap through systematic empirical validation. GRETEL implements an agentic workflow that processes semantically retrieved candidates through sandboxed plan-execute-evaluate cycles, generating execution-grounded evidence to distinguish truly functional tools from merely descriptive matches. Our comprehensive evaluation on the ToolBench benchmark demonstrates substantial improvements across all metrics: Pass Rate (at 10) increases from 0.690 to 0.826, Recall (at 10) improves from 0.841 to 0.867, and NDCG (at 10) rises from 0.807 to 0.857.. These results establish that execution-based validation provides a more reliable foundation for tool selection than semantic similarity alone, enabling more robust agent performance in real-world applications.</li>
</ul>

<h3>Title: Modeling Layered Consciousness with Multi-Agent Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sang Hun Kim, Jongmin Lee, Dongkyu Park, So Young Lee, Yosep Chong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17844">https://arxiv.org/abs/2510.17844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17844">https://arxiv.org/pdf/2510.17844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17844]] Modeling Layered Consciousness with Multi-Agent Large Language Models(https://arxiv.org/abs/2510.17844)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose a multi-agent framework for modeling artificial consciousness in large language models (LLMs), grounded in psychoanalytic theory. Our \textbf{Psychodynamic Model} simulates self-awareness, preconsciousness, and unconsciousness through agent interaction, guided by a Personalization Module combining fixed traits and dynamic needs. Using parameter-efficient fine-tuning on emotionally rich dialogues, the system was evaluated across eight personalized conditions. An LLM as a judge approach showed a 71.2\% preference for the fine-tuned model, with improved emotional depth and reduced output variance, demonstrating its potential for adaptive, personalized cognition.</li>
</ul>

<h3>Title: MAT-Agent: Adaptive Multi-Agent Training Optimization</h3>
<ul>
<li><strong>Authors: </strong>Jusheng Zhang, Kaitong Cai, Yijia Fan, Ningyuan Liu, Keze Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17845">https://arxiv.org/abs/2510.17845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17845">https://arxiv.org/pdf/2510.17845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17845]] MAT-Agent: Adaptive Multi-Agent Training Optimization(https://arxiv.org/abs/2510.17845)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-label image classification demands adaptive training strategies to navigate complex, evolving visual-semantic landscapes, yet conventional methods rely on static configurations that falter in dynamic settings. We propose MAT-Agent, a novel multi-agent framework that reimagines training as a collaborative, real-time optimization process. By deploying autonomous agents to dynamically tune data augmentation, optimizers, learning rates, and loss functions, MAT-Agent leverages non-stationary multi-armed bandit algorithms to balance exploration and exploitation, guided by a composite reward harmonizing accuracy, rare-class performance, and training stability. Enhanced with dual-rate exponential moving average smoothing and mixed-precision training, it ensures robustness and efficiency. Extensive experiments across Pascal VOC, COCO, and VG-256 demonstrate MAT-Agent's superiority: it achieves an mAP of 97.4 (vs. 96.2 for PAT-T), OF1 of 92.3, and CF1 of 91.4 on Pascal VOC; an mAP of 92.8 (vs. 92.0 for HSQ-CvN), OF1 of 88.2, and CF1 of 87.1 on COCO; and an mAP of 60.9, OF1 of 70.8, and CF1 of 61.1 on VG-256. With accelerated convergence and robust cross-domain generalization, MAT-Agent offers a scalable, intelligent solution for optimizing complex visual models, paving the way for adaptive deep learning advancements.</li>
</ul>

<h3>Title: CARLE: A Hybrid Deep-Shallow Learning Framework for Robust and Explainable RUL Estimation of Rolling Element Bearings</h3>
<ul>
<li><strong>Authors: </strong>Waleed Razzaq, Yun-Bo Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17846">https://arxiv.org/abs/2510.17846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17846">https://arxiv.org/pdf/2510.17846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17846]] CARLE: A Hybrid Deep-Shallow Learning Framework for Robust and Explainable RUL Estimation of Rolling Element Bearings(https://arxiv.org/abs/2510.17846)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Prognostic Health Management (PHM) systems monitor and predict equipment health. A key task is Remaining Useful Life (RUL) estimation, which predicts how long a component, such as a rolling element bearing, will operate before failure. Many RUL methods exist but often lack generalizability and robustness under changing operating conditions. This paper introduces CARLE, a hybrid AI framework that combines deep and shallow learning to address these challenges. CARLE uses Res-CNN and Res-LSTM blocks with multi-head attention and residual connections to capture spatial and temporal degradation patterns, and a Random Forest Regressor (RFR) for stable, accurate RUL prediction. A compact preprocessing pipeline applies Gaussian filtering for noise reduction and Continuous Wavelet Transform (CWT) for time-frequency feature extraction. We evaluate CARLE on the XJTU-SY and PRONOSTIA bearing datasets. Ablation studies measure each component's contribution, while noise and cross-domain experiments test robustness and generalization. Comparative results show CARLE outperforms several state-of-the-art methods, especially under dynamic conditions. Finally, we analyze model interpretability with LIME and SHAP to assess transparency and trustworthiness.</li>
</ul>

<h3>Title: CoIDO: Efficient Data Selection for Visual Instruction Tuning via Coupled Importance-Diversity Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yichen Yan, Ming Zhong, Qi Zhu, Xiaoling Gu, Jinpeng Chen, Huan Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17847">https://arxiv.org/abs/2510.17847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17847">https://arxiv.org/pdf/2510.17847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17847]] CoIDO: Efficient Data Selection for Visual Instruction Tuning via Coupled Importance-Diversity Optimization(https://arxiv.org/abs/2510.17847)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) rely heavily on instruction tuning to align vision and language capabilities, yet the computational cost of training on large-scale datasets remains a major bottleneck. Existing data selection methods aim to mitigate this by selecting important and diverse subsets, but they often suffer from two critical drawbacks: high computational overhead from processing the entire dataset and suboptimal data selection due to separate treatment of importance and diversity. We introduce CoIDO, a novel dual-objective framework that jointly optimizes data importance and diversity to overcome these challenges. Unlike existing approaches that require costly evaluations across the whole dataset, CoIDO employs a lightweight plug-in scorer. This scorer is trained on just a small random sample of data to learn the distribution of the candidate set, drastically reducing computational demands. By leveraging a homoscedastic uncertainty-based formulation, CoIDO effectively balances importance and diversity during training, enabling efficient and scalable data selection. In our experiments, we trained the CoIDO scorer using only 20 percent of randomly sampled data. Once trained, CoIDO was applied to the entire dataset to select a 20 percent subset for instruction tuning. On the widely used LLaVA-1.5-7B model across ten downstream tasks, this selected subset achieved an impressive 98.2 percent of the performance of full-data fine-tuning, on average.</li>
</ul>

<h3>Title: RiskTagger: An LLM-based Agent for Automatic Annotation of Web3 Crypto Money Laundering Behaviors</h3>
<ul>
<li><strong>Authors: </strong>Dan Lin, Yanli Ding, Weipeng Zou, Jiachi Chen, Xiapu Luo, Jiajing Wu, Zibin Zheng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17848">https://arxiv.org/abs/2510.17848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17848">https://arxiv.org/pdf/2510.17848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17848]] RiskTagger: An LLM-based Agent for Automatic Annotation of Web3 Crypto Money Laundering Behaviors(https://arxiv.org/abs/2510.17848)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>While the rapid growth of Web3 has driven the development of decentralized finance, user anonymity and cross-chain asset flows make on-chain laundering behaviors more covert and complex. In this context, constructing high-quality anti-money laundering(AML) datasets has become essential for risk-control systems and on-chain forensic analysis, yet current practices still rely heavily on manual efforts with limited efficiency and coverage. In this paper, we introduce RiskTagger, a large-language-model-based agent for the automatic annotation of crypto laundering behaviors in Web3. RiskTagger is designed to replace or complement human annotators by addressing three key challenges: extracting clues from complex unstructured reports, reasoning over multichain transaction paths, and producing auditor-friendly explanations. RiskTagger implements an end-to-end multi-module agent, integrating a key-clue extractor, a multichain fetcher with a laundering-behavior reasoner, and a data explainer, forming a data annotation pipeline. Experiments on the real case Bybit Hack (with the highest stolen asset value) demonstrate that RiskTagger achieves 100% accuracy in clue extraction, 84.1% consistency with expert judgment, and 90% coverage in explanation generation. Overall, RiskTagger automates laundering behavior annotation while improving transparency and scalability in AML research.</li>
</ul>

<h3>Title: Pre to Post-Treatment Glioblastoma MRI Prediction using a Latent Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Alexandre G. Leclercq, Sébastien Bougleux, Noémie N. Moreau, Alexis Desmonts, Romain Hérault, Aurélien Corroyer-Dulmont</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17851">https://arxiv.org/abs/2510.17851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17851">https://arxiv.org/pdf/2510.17851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17851]] Pre to Post-Treatment Glioblastoma MRI Prediction using a Latent Diffusion Model(https://arxiv.org/abs/2510.17851)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Glioblastoma (GBM) is an aggressive primary brain tumor with a median survival of approximately 15 months. In clinical practice, the Stupp protocol serves as the standard first-line treatment. However, patients exhibit highly heterogeneous therapeutic responses which required at least two months before first visual impact can be observed, typically with MRI. Early prediction treatment response is crucial for advancing personalized medicine. Disease Progression Modeling (DPM) aims to capture the trajectory of disease evolution, while Treatment Response Prediction (TRP) focuses on assessing the impact of therapeutic interventions. Whereas most TRP approaches primarly rely on timeseries data, we consider the problem of early visual TRP as a slice-to-slice translation model generating post-treatment MRI from a pre-treatment MRI, thus reflecting the tumor evolution. To address this problem we propose a Latent Diffusion Model with a concatenation-based conditioning from the pre-treatment MRI and the tumor localization, and a classifier-free guidance to enhance generation quality using survival information, in particular post-treatment tumor evolution. Our model were trained and tested on a local dataset consisting of 140 GBM patients collected at Centre François Baclesse. For each patient we collected pre and post T1-Gd MRI, tumor localization manually delineated in the pre-treatment MRI by medical experts, and survival information.</li>
</ul>

<h3>Title: Provenance of AI-Generated Images: A Vector Similarity and Blockchain-based Approach</h3>
<ul>
<li><strong>Authors: </strong>Jitendra Sharma, Arthur Carvalho, Suman Bhunia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17854">https://arxiv.org/abs/2510.17854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17854">https://arxiv.org/pdf/2510.17854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17854]] Provenance of AI-Generated Images: A Vector Similarity and Blockchain-based Approach(https://arxiv.org/abs/2510.17854)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Rapid advancement in generative AI and large language models (LLMs) has enabled the generation of highly realistic and contextually relevant digital content. LLMs such as ChatGPT with DALL-E integration and Stable Diffusion techniques can produce images that are often indistinguishable from those created by humans, which poses challenges for digital content authentication. Verifying the integrity and origin of digital data to ensure it remains unaltered and genuine is crucial to maintaining trust and legality in digital media. In this paper, we propose an embedding-based AI image detection framework that utilizes image embeddings and a vector similarity to distinguish AI-generated images from real (human-created) ones. Our methodology is built on the hypothesis that AI-generated images demonstrate closer embedding proximity to other AI-generated content, while human-created images cluster similarly within their domain. To validate this hypothesis, we developed a system that processes a diverse dataset of AI and human-generated images through five benchmark embedding models. Extensive experimentation demonstrates the robustness of our approach, and our results confirm that moderate to high perturbations minimally impact the embedding signatures, with perturbed images maintaining close similarity matches to their original versions. Our solution provides a generalizable framework for AI-generated image detection that balances accuracy with computational efficiency.</li>
</ul>

<h3>Title: Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch</h3>
<ul>
<li><strong>Authors: </strong>Xu Cai, Yang Wu, Qianli Chen, Haoran Wu, Lichuan Xiang, Hongkai Wen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17858">https://arxiv.org/abs/2510.17858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17858">https://arxiv.org/pdf/2510.17858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17858]] Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch(https://arxiv.org/abs/2510.17858)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present an ultra-efficient post-training method for shortcutting large-scale pre-trained flow matching diffusion models into efficient few-step samplers, enabled by novel velocity field self-distillation. While shortcutting in flow matching, originally introduced by shortcut models, offers flexible trajectory-skipping capabilities, it requires a specialized step-size embedding incompatible with existing models unless retraining from scratch$\unicode{x2013}$a process nearly as costly as pretraining itself. Our key contribution is thus imparting a more aggressive shortcut mechanism to standard flow matching models (e.g., Flux), leveraging a unique distillation principle that obviates the need for step-size embedding. Working on the velocity field rather than sample space and learning rapidly from self-guided distillation in an online manner, our approach trains efficiently, e.g., producing a 3-step Flux less than one A100 day. Beyond distillation, our method can be incorporated into the pretraining stage itself, yielding models that inherently learn efficient, few-step flows without compromising quality. This capability also enables, to our knowledge, the first few-shot distillation method (e.g., 10 text-image pairs) for dozen-billion-parameter diffusion models, delivering state-of-the-art performance at almost free cost.</li>
</ul>

<h3>Title: When "Correct" Is Not Safe: Can We Trust Functionally Correct Patches Generated by Code Agents?</h3>
<ul>
<li><strong>Authors: </strong>Yibo Peng, James Song, Lei Li, Xinyu Yang, Mihai Christodorescu, Ravi Mangal, Corina Pasareanu, Haizhong Zheng, Beidi Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17862">https://arxiv.org/abs/2510.17862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17862">https://arxiv.org/pdf/2510.17862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17862]] When "Correct" Is Not Safe: Can We Trust Functionally Correct Patches Generated by Code Agents?(https://arxiv.org/abs/2510.17862)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Code agents are increasingly trusted to autonomously fix bugs on platforms such as GitHub, yet their security evaluation focuses almost exclusively on functional correctness. In this paper, we reveal a novel type of threat to real-world code agents: Functionally Correct yet Vulnerable (FCV) patches, which pass all test cases but contain vulnerable code. With our proposed FCV-Attack, which can be deliberately crafted by malicious attackers or implicitly introduced by benign developers, we show that SOTA LLMs (e.g., ChatGPT and Claude) and agent scaffolds (e.g., SWE-agent and OpenHands) are all vulnerable to this FCV threat; across 12 agent-model combinations on SWE-Bench, the attack only requires black-box access and a single query to the code agent to perform the attack. For example, for CWE-538 (information exposure vulnerability), the FCV-Attack attains an attack success rate of $40.7\%$ on GPT-5 Mini + OpenHands. Our results reveal an important security threat overlooked by current evaluation paradigms and urge the development of security-aware defenses for code agents.</li>
</ul>

<h3>Title: MUSE: Model-based Uncertainty-aware Similarity Estimation for zero-shot 2D Object Detection and Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Sungmin Cho, Sungbum Park, Insoo Oh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17866">https://arxiv.org/abs/2510.17866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17866">https://arxiv.org/pdf/2510.17866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17866]] MUSE: Model-based Uncertainty-aware Similarity Estimation for zero-shot 2D Object Detection and Segmentation(https://arxiv.org/abs/2510.17866)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In this work, we introduce MUSE (Model-based Uncertainty-aware Similarity Estimation), a training-free framework designed for model-based zero-shot 2D object detection and segmentation. MUSE leverages 2D multi-view templates rendered from 3D unseen objects and 2D object proposals extracted from input query images. In the embedding stage, it integrates class and patch embeddings, where the patch embeddings are normalized using generalized mean pooling (GeM) to capture both global and local representations efficiently. During the matching stage, MUSE employs a joint similarity metric that combines absolute and relative similarity scores, enhancing the robustness of matching under challenging scenarios. Finally, the similarity score is refined through an uncertainty-aware object prior that adjusts for proposal reliability. Without any additional training or fine-tuning, MUSE achieves state-of-the-art performance on the BOP Challenge 2025, ranking first across the Classic Core, H3, and Industrial tracks. These results demonstrate that MUSE offers a powerful and generalizable framework for zero-shot 2D object detection and segmentation.</li>
</ul>

<h3>Title: GAN-based Content-Conditioned Generation of Handwritten Musical Symbols</h3>
<ul>
<li><strong>Authors: </strong>Gerard Asbert, Pau Torras, Lei Kang, Alicia Fornés, Josep Lladós</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17869">https://arxiv.org/abs/2510.17869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17869">https://arxiv.org/pdf/2510.17869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17869]] GAN-based Content-Conditioned Generation of Handwritten Musical Symbols(https://arxiv.org/abs/2510.17869)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The field of Optical Music Recognition (OMR) is currently hindered by the scarcity of real annotated data, particularly when dealing with handwritten historical musical scores. In similar fields, such as Handwritten Text Recognition, it was proven that synthetic examples produced with image generation techniques could help to train better-performing recognition architectures. This study explores the generation of realistic, handwritten-looking scores by implementing a music symbol-level Generative Adversarial Network (GAN) and assembling its output into a full score using the Smashcima engraving software. We have systematically evaluated the visual fidelity of these generated samples, concluding that the generated symbols exhibit a high degree of realism, marking significant progress in synthetic score generation.</li>
</ul>

<h3>Title: Auditing and Mitigating Bias in Gender Classification Algorithms: A Data-Centric Approach</h3>
<ul>
<li><strong>Authors: </strong>Tadesse K Bahiru, Natnael Tilahun Sinshaw, Teshager Hailemariam Moges, Dheeraj Kumar Singh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17873">https://arxiv.org/abs/2510.17873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17873">https://arxiv.org/pdf/2510.17873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17873]] Auditing and Mitigating Bias in Gender Classification Algorithms: A Data-Centric Approach(https://arxiv.org/abs/2510.17873)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Gender classification systems often inherit and amplify demographic imbalances in their training data. We first audit five widely used gender classification datasets, revealing that all suffer from significant intersectional underrepresentation. To measure the downstream impact of these flaws, we train identical MobileNetV2 classifiers on the two most balanced of these datasets, UTKFace and FairFace. Our fairness evaluation shows that even these models exhibit significant bias, misclassifying female faces at a higher rate than male faces and amplifying existing racial skew. To counter these data-induced biases, we construct BalancedFace, a new public dataset created by blending images from FairFace and UTKFace, supplemented with images from other collections to fill missing demographic gaps. It is engineered to equalize subgroup shares across 189 intersections of age, race, and gender using only real, unedited images. When a standard classifier is trained on BalancedFace, it reduces the maximum True Positive Rate gap across racial subgroups by over 50% and brings the average Disparate Impact score 63% closer to the ideal of 1.0 compared to the next-best dataset, all with a minimal loss of overall accuracy. These results underline the profound value of data-centric interventions and provide an openly available resource for fair gender classification research.</li>
</ul>

<h3>Title: 3D Weakly Supervised Semantic Segmentation via Class-Aware and Geometry-Guided Pseudo-Label Refinement</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxu Xu, Xuexun Liu, Jinlong Li, Yitian Yuan, Qiudan Zhang, Lin Ma, Nicu Sebe, Xu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17875">https://arxiv.org/abs/2510.17875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17875">https://arxiv.org/pdf/2510.17875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17875]] 3D Weakly Supervised Semantic Segmentation via Class-Aware and Geometry-Guided Pseudo-Label Refinement(https://arxiv.org/abs/2510.17875)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>3D weakly supervised semantic segmentation (3D WSSS) aims to achieve semantic segmentation by leveraging sparse or low-cost annotated data, significantly reducing reliance on dense point-wise annotations. Previous works mainly employ class activation maps or pre-trained vision-language models to address this challenge. However, the low quality of pseudo-labels and the insufficient exploitation of 3D geometric priors jointly create significant technical bottlenecks in developing high-performance 3D WSSS models. In this paper, we propose a simple yet effective 3D weakly supervised semantic segmentation method that integrates 3D geometric priors into a class-aware guidance mechanism to generate high-fidelity pseudo labels. Concretely, our designed methodology first employs Class-Aware Label Refinement module to generate more balanced and accurate pseudo labels for semantic categrories. This initial refinement stage focuses on enhancing label quality through category-specific optimization. Subsequently, the Geometry-Aware Label Refinement component is developed, which strategically integrates implicit 3D geometric constraints to effectively filter out low-confidence pseudo labels that fail to comply with geometric plausibility. Moreover, to address the challenge of extensive unlabeled regions, we propose a Label Update strategy that integrates Self-Training to propagate labels into these areas. This iterative process continuously enhances pseudo-label quality while expanding label coverage, ultimately fostering the development of high-performance 3D WSSS models. Comprehensive experimental validation reveals that our proposed methodology achieves state-of-the-art performance on both ScanNet and S3DIS benchmarks while demonstrating remarkable generalization capability in unsupervised settings, maintaining competitive accuracy through its robust design.</li>
</ul>

<h3>Title: Outraged AI: Large language models prioritise emotion over cost in fairness enforcement</h3>
<ul>
<li><strong>Authors: </strong>Hao Liu, Yiqing Dai, Haotian Tan, Yu Lei, Yujia Zhou, Zhen Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17880">https://arxiv.org/abs/2510.17880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17880">https://arxiv.org/pdf/2510.17880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17880]] Outraged AI: Large language models prioritise emotion over cost in fairness enforcement(https://arxiv.org/abs/2510.17880)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Emotions guide human decisions, but whether large language models (LLMs) use emotion similarly remains unknown. We tested this using altruistic third-party punishment, where an observer incurs a personal cost to enforce fairness, a hallmark of human morality and often driven by negative emotion. In a large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100 decisions, LLMs used emotion to guide punishment, sometimes even more strongly than humans did: Unfairness elicited stronger negative emotion that led to more punishment; punishing unfairness produced more positive emotion than accepting; and critically, prompting self-reports of emotion causally increased punishment. However, mechanisms diverged: LLMs prioritized emotion over cost, enforcing norms in an almost all-or-none manner with reduced cost sensitivity, whereas humans balanced fairness and cost. Notably, reasoning models (o3-mini, DeepSeek-R1) were more cost-sensitive and closer to human behavior than foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven. These findings provide the first causal evidence of emotion-guided moral decisions in LLMs and reveal deficits in cost calibration and nuanced fairness judgements, reminiscent of early-stage human responses. We propose that LLMs progress along a trajectory paralleling human development; future models should integrate emotion with context-sensitive reasoning to achieve human-like emotional intelligence.</li>
</ul>

<h3>Title: POPI: Personalizing LLMs via Optimized Natural Language Preference Inference</h3>
<ul>
<li><strong>Authors: </strong>Yizhuo Chen, Xin Liu, Ruijie Wang, Zheng Li, Pei Chen, Changlong Yu, Priyanka Nigam, Meng Jiang, Bing Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17881">https://arxiv.org/abs/2510.17881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17881">https://arxiv.org/pdf/2510.17881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17881]] POPI: Personalizing LLMs via Optimized Natural Language Preference Inference(https://arxiv.org/abs/2510.17881)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve strong benchmark performance, yet user experiences remain inconsistent due to diverse preferences in style, tone, and reasoning mode. Nevertheless, existing alignment techniques such as reinforcement learning from human feedback (RLHF) or Direct Preference Optimization (DPO) largely optimize toward population-level averages and overlook individual variation. Naive personalization strategies like per-user fine-tuning are computationally prohibitive, and in-context approaches that prepend raw user signals often suffer from inefficiency and noise. To address these challenges, we propose POPI, a general framework that introduces a preference inference model to distill heterogeneous user signals into concise natural language summaries. These summaries act as transparent, compact, and transferable personalization representations that condition a shared generation model to produce personalized responses. POPI jointly optimizes both preference inference and personalized generation under a unified objective using reinforcement learning, ensuring summaries maximally encode useful preference information. Extensive experiments across four personalization benchmarks demonstrate that POPI consistently improves personalization accuracy while reducing context overhead by a large margin. Moreover, optimized summaries seamlessly transfer to frozen off-the-shelf LLMs, enabling plug-and-play personalization without weight updates.</li>
</ul>

<h3>Title: From Flows to Words: Can Zero-/Few-Shot LLMs Detect Network Intrusions? A Grammar-Constrained, Calibrated Evaluation on UNSW-NB15</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Abdul Rehman, Syed Imad Ali Shah, Abbas n=Anwar, Noor Islam</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17883">https://arxiv.org/abs/2510.17883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17883">https://arxiv.org/pdf/2510.17883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17883]] From Flows to Words: Can Zero-/Few-Shot LLMs Detect Network Intrusions? A Grammar-Constrained, Calibrated Evaluation on UNSW-NB15(https://arxiv.org/abs/2510.17883)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can reason over natural-language inputs, but their role in intrusion detection without fine-tuning remains uncertain. This study evaluates a prompt-only approach on UNSW-NB15 by converting each network flow to a compact textual record and augmenting it with lightweight, domain-inspired boolean flags (asymmetry, burst rate, TTL irregularities, timer anomalies, rare service/state, short bursts). To reduce output drift and support measurement, the model is constrained to produce structured, grammar-valid responses, and a single decision threshold is calibrated on a small development split. We compare zero-shot, instruction-guided, and few-shot prompting to strong tabular and neural baselines under identical splits, reporting accuracy, precision, recall, F1, and macro scores. Empirically, unguided prompting is unreliable, while instructions plus flags substantially improve detection quality; adding calibrated scoring further stabilizes results. On a balanced subset of two hundred flows, a 7B instruction-tuned model with flags reaches macro-F1 near 0.78; a lighter 3B model with few-shot cues and calibration attains F1 near 0.68 on one thousand examples. As the evaluation set grows to two thousand flows, decision quality decreases, revealing sensitivity to coverage and prompting. Tabular baselines remain more stable and faster, yet the prompt-only pipeline requires no gradient training, produces readable artifacts, and adapts easily through instructions and flags. Contributions include a flow-to-text protocol with interpretable cues, a calibration method for thresholding, a systematic baseline comparison, and a reproducibility bundle with prompts, grammar, metrics, and figures.</li>
</ul>

<h3>Title: When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Abdul Rehman, Syed Imad Ali Shah, Abbas Anwar, Noor Islam</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17884">https://arxiv.org/abs/2510.17884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17884">https://arxiv.org/pdf/2510.17884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17884]] When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking(https://arxiv.org/abs/2510.17884)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>The remarkable capabilities of Large Language Models (LLMs) in natural language understanding and generation have sparked interest in their potential for cybersecurity applications, including password guessing. In this study, we conduct an empirical investigation into the efficacy of pre-trained LLMs for password cracking using synthetic user profiles. Specifically, we evaluate the performance of state-of-the-art open-source LLMs such as TinyLLaMA, Falcon-RW-1B, and Flan-T5 by prompting them to generate plausible passwords based on structured user attributes (e.g., name, birthdate, hobbies). Our results, measured using Hit@1, Hit@5, and Hit@10 metrics under both plaintext and SHA-256 hash comparisons, reveal consistently poor performance, with all models achieving less than 1.5% accuracy at Hit@10. In contrast, traditional rule-based and combinator-based cracking methods demonstrate significantly higher success rates. Through detailed analysis and visualization, we identify key limitations in the generative reasoning of LLMs when applied to the domain-specific task of password guessing. Our findings suggest that, despite their linguistic prowess, current LLMs lack the domain adaptation and memorization capabilities required for effective password inference, especially in the absence of supervised fine-tuning on leaked password datasets. This study provides critical insights into the limitations of LLMs in adversarial contexts and lays the groundwork for future efforts in secure, privacy-preserving, and robust password modeling.</li>
</ul>

<h3>Title: Advances in Pre-trained Language Models for Domain-Specific Text Classification: A Systematic Review</h3>
<ul>
<li><strong>Authors: </strong>Zhyar Rzgar K. Rostam, Gábor Kertész</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17892">https://arxiv.org/abs/2510.17892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17892">https://arxiv.org/pdf/2510.17892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17892]] Advances in Pre-trained Language Models for Domain-Specific Text Classification: A Systematic Review(https://arxiv.org/abs/2510.17892)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The exponential increase in scientific literature and online information necessitates efficient methods for extracting knowledge from textual data. Natural language processing (NLP) plays a crucial role in addressing this challenge, particularly in text classification tasks. While large language models (LLMs) have achieved remarkable success in NLP, their accuracy can suffer in domain-specific contexts due to specialized vocabulary, unique grammatical structures, and imbalanced data distributions. In this systematic literature review (SLR), we investigate the utilization of pre-trained language models (PLMs) for domain-specific text classification. We systematically review 41 articles published between 2018 and January 2024, adhering to the PRISMA statement (preferred reporting items for systematic reviews and meta-analyses). This review methodology involved rigorous inclusion criteria and a multi-step selection process employing AI-powered tools. We delve into the evolution of text classification techniques and differentiate between traditional and modern approaches. We emphasize transformer-based models and explore the challenges and considerations associated with using LLMs for domain-specific text classification. Furthermore, we categorize existing research based on various PLMs and propose a taxonomy of techniques used in the field. To validate our findings, we conducted a comparative experiment involving BERT, SciBERT, and BioBERT in biomedical sentence classification. Finally, we present a comparative study on the performance of LLMs in text classification tasks across different domains. In addition, we examine recent advancements in PLMs for domain-specific text classification and offer insights into future directions and limitations in this rapidly evolving domain.</li>
</ul>

<h3>Title: Hierarchical Federated Unlearning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yisheng Zhong, Zhengbang Yang, Zhuangdi Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17895">https://arxiv.org/abs/2510.17895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17895">https://arxiv.org/pdf/2510.17895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17895]] Hierarchical Federated Unlearning for Large Language Models(https://arxiv.org/abs/2510.17895)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, robust, federate, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly integrated into real-world applications, raising concerns about privacy, security and the need to remove undesirable knowledge. Machine Unlearning has emerged as a promising solution, yet faces two key challenges: (1) practical unlearning needs are often continuous and heterogeneous, and (2) they involve decentralized, sensitive data with asymmetric access. These factors result in inter-domain and intra-domain interference, which further amplifies the dilemma of unbalanced forgetting and retaining performance. In response, we propose a federated unlearning approach for LLMs that is scalable and privacy preserving. Our method decouples unlearning and retention via task-specific adapter learning and employs a hierarchical merging strategy to mitigate conflicting objectives and enables robust, adaptable unlearning updates. Comprehensive experiments on benchmarks of WMDP, MUSE, and TOFU showed that our approach effectively handles heterogeneous unlearning requests while maintaining strong LLM utility compared with baseline methods.</li>
</ul>

<h3>Title: Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism</h3>
<ul>
<li><strong>Authors: </strong>Tao Bu, Qiangang Wang, Bowen Zeng, Hanwen Sun, Yunpeng Huang, Chun Cao, Jingwei Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17896">https://arxiv.org/abs/2510.17896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17896">https://arxiv.org/pdf/2510.17896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17896]] Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism(https://arxiv.org/abs/2510.17896)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based large language models (LLMs) have achieved remarkable success, yet their standard attention mechanism incurs quadratic computation and memory costs with respect to sequence length, posing a major bottleneck for long-context training. Prior work tackles this challenge along two directions: (1) kernel-level optimizations, which accelerate dense and sparse attention operators; and (2) module-level strategies, often referred to as distributed attention or context parallel training, which scale attention across multiple devices. However, systematic evaluation still remains limited: operator-level comparisons are often incomplete, while context parallel strategies are typically framework-specific, with unclear performance analysis across contexts. To address these gaps, we propose a unified benchmark that integrates representative attention kernels and context parallel mechanisms with a modular and extensible interface for evaluation. The benchmark evaluates methods along two critical dimensions: (1) attention mask patterns, which strongly affect efficiency, scalability, and usability, and (2) sequence length and distributed scale, which determine performance under extreme long-context training. Through comprehensive experiments on the cluster of up to 96 GPUs, our benchmark enables reproducible comparisons, highlights method-specific trade-offs, and provides practical guidance for designing and deploying attention mechanisms in long-context LLM training.</li>
</ul>

<h3>Title: L-MoE: End-to-End Training of a Lightweight Mixture of Low-Rank Adaptation Experts</h3>
<ul>
<li><strong>Authors: </strong>Shihao Ji, Zihui Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17898">https://arxiv.org/abs/2510.17898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17898">https://arxiv.org/pdf/2510.17898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17898]] L-MoE: End-to-End Training of a Lightweight Mixture of Low-Rank Adaptation Experts(https://arxiv.org/abs/2510.17898)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Mixture of Experts (MoE) architecture enables the scaling of Large Language Models (LLMs) to trillions of parameters by activating a sparse subset of weights for each input, maintaining constant computational cost during inference. Concurrently, Low-Rank Adaptation (LoRA) has emerged as a dominant technique for parameter-efficiently fine-tuning LLMs on specialized tasks. In this work, we unify these two paradigms into a novel, end-to-end trainable framework named L-MoE: a Lightweight Mixture of LoRA Experts. L-MoE redefines MoE experts not as dense feed-forward networks, but as a collection of task-specialized, low-rank adapters. A lightweight gating network, trained jointly with the experts, learns to dynamically compose these LoRA adapters by computing a weighted average of their parameters for each input token. This composition is fully differentiable, allowing gradients from a standard auto-regressive language modeling objective to flow back through the entire architecture, simultaneously refining both the expert adapters and the routing strategy. This approach creates a highly parameter-efficient MoE model that is modular by design, allows for dynamic skill composition, and is trainable from end-to-end. We present the formal mathematical framework for L-MoE, detailing the differentiable routing mechanism and the joint optimization objective, thereby providing a new path toward building more efficient, scalable, and specialized language models.</li>
</ul>

<h3>Title: Automated Algorithm Design for Auto-Tuning Optimizers</h3>
<ul>
<li><strong>Authors: </strong>Floris-Jan Willemsen, Niki van Stein, Ben van Werkhoven</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17899">https://arxiv.org/abs/2510.17899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17899">https://arxiv.org/pdf/2510.17899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17899]] Automated Algorithm Design for Auto-Tuning Optimizers(https://arxiv.org/abs/2510.17899)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic performance tuning (auto-tuning) is essential for optimizing high-performance applications, where vast and irregular parameter spaces make manual exploration infeasible. Traditionally, auto-tuning relies on well-established optimization algorithms such as evolutionary algorithms, annealing methods, or surrogate model-based optimizers to efficiently find near-optimal configurations. However, designing effective optimizers remains challenging, as no single method performs best across all tuning tasks. In this work, we explore a new paradigm: using large language models (LLMs) to automatically generate optimization algorithms tailored to auto-tuning problems. We introduce a framework that prompts LLMs with problem descriptions and search-space characteristics results to produce specialized optimization strategies, which are iteratively examined and improved. These generated algorithms are evaluated on four real-world auto-tuning applications across six hardware platforms and compared against the state-of-the-art in optimization algorithms of two contemporary auto-tuning frameworks. The evaluation demonstrates that providing additional application- and search space-specific information in the generation stage results in an average performance improvement of 30.7\% and 14.6\%, respectively. In addition, our results show that LLM-generated optimizers can rival, and in various cases outperform, existing human-designed algorithms, with our best-performing generated optimization algorithms achieving, on average, 72.4\% improvement over state-of-the-art optimizers for auto-tuning.</li>
</ul>

<h3>Title: The Sherpa.ai Blind Vertical Federated Learning Paradigm to Minimize the Number of Communications</h3>
<ul>
<li><strong>Authors: </strong>Alex Acero, Daniel M. Jimenez-Gutierrez, Dario Pighin, Enrique Zuazua, Joaquin Del Rio, Xabi Uribe-Etxebarria</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17901">https://arxiv.org/abs/2510.17901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17901">https://arxiv.org/pdf/2510.17901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17901]] The Sherpa.ai Blind Vertical Federated Learning Paradigm to Minimize the Number of Communications(https://arxiv.org/abs/2510.17901)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative decentralized training across multiple parties (nodes) while keeping raw data private. There are two main paradigms in FL: Horizontal FL (HFL), where all participant nodes share the same feature space but hold different samples, and Vertical FL (VFL), where participants hold complementary features for the same samples. While HFL is widely adopted, VFL is employed in domains where nodes hold complementary features about the same samples. Still, VFL presents a significant limitation: the vast number of communications required during training. This compromises privacy and security, and can lead to high energy consumption, and in some cases, make model training unfeasible due to the high number of communications. In this paper, we introduce this http URL Blind Vertical Federated Learning (SBVFL), a novel paradigm that leverages a distributed training mechanism enhanced for privacy and security. Decoupling the vast majority of node updates from the server dramatically reduces node-server communication. Experiments show that SBVFL reduces communication by ~99% compared to standard VFL while maintaining accuracy and robustness. Therefore, SBVFL enables practical, privacy-preserving VFL across sensitive domains, including healthcare, finance, manufacturing, aerospace, cybersecurity, and the defense industry.</li>
</ul>

<h3>Title: BreakFun: Jailbreaking LLMs via Schema Exploitation</h3>
<ul>
<li><strong>Authors: </strong>Amirkia Rafiei Oskooei, Mehmet S. Aktas</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17904">https://arxiv.org/abs/2510.17904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17904">https://arxiv.org/pdf/2510.17904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17904]] BreakFun: Jailbreaking LLMs via Schema Exploitation(https://arxiv.org/abs/2510.17904)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The proficiency of Large Language Models (LLMs) in processing structured data and adhering to syntactic rules is a capability that drives their widespread adoption but also makes them paradoxically vulnerable. In this paper, we investigate this vulnerability through BreakFun, a jailbreak methodology that weaponizes an LLM's adherence to structured schemas. BreakFun employs a three-part prompt that combines an innocent framing and a Chain-of-Thought distraction with a core "Trojan Schema"--a carefully crafted data structure that compels the model to generate harmful content, exploiting the LLM's strong tendency to follow structures and schemas. We demonstrate this vulnerability is highly transferable, achieving an average success rate of 89% across 13 foundational and proprietary models on JailbreakBench, and reaching a 100% Attack Success Rate (ASR) on several prominent models. A rigorous ablation study confirms this Trojan Schema is the attack's primary causal factor. To counter this, we introduce the Adversarial Prompt Deconstruction guardrail, a defense that utilizes a secondary LLM to perform a "Literal Transcription"--extracting all human-readable text to isolate and reveal the user's true harmful intent. Our proof-of-concept guardrail demonstrates high efficacy against the attack, validating that targeting the deceptive schema is a viable mitigation strategy. Our work provides a look into how an LLM's core strengths can be turned into critical weaknesses, offering a fresh perspective for building more robustly aligned models.</li>
</ul>

<h3>Title: Atomic Literary Styling: Mechanistic Manipulation of Prose Generation in Neural Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tsogt-Ochir Enkhbayar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17909">https://arxiv.org/abs/2510.17909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17909">https://arxiv.org/pdf/2510.17909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17909]] Atomic Literary Styling: Mechanistic Manipulation of Prose Generation in Neural Language Models(https://arxiv.org/abs/2510.17909)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We present a mechanistic analysis of literary style in GPT-2, identifying individual neurons that discriminate between exemplary prose and rigid AI-generated text. Using Herman Melville's Bartleby, the Scrivener as a corpus, we extract activation patterns from 355 million parameters across 32,768 neurons in late layers. We find 27,122 statistically significant discriminative neurons ($p < 0.05$), with effect sizes up to $|d| = 1.4$. Through systematic ablation studies, we discover a paradoxical result: while these neurons correlate with literary text during analysis, removing them often improves rather than degrades generated prose quality. Specifically, ablating 50 high-discriminating neurons yields a 25.7% improvement in literary style metrics. This demonstrates a critical gap between observational correlation and causal necessity in neural networks. Our findings challenge the assumption that neurons which activate on desirable inputs will produce those outputs during generation, with implications for mechanistic interpretability research and AI alignment.</li>
</ul>

<h3>Title: Data Unlearning Beyond Uniform Forgetting via Diffusion Time and Frequency Selection</h3>
<ul>
<li><strong>Authors: </strong>Jinseong Park, Mijung Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17917">https://arxiv.org/abs/2510.17917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17917">https://arxiv.org/pdf/2510.17917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17917]] Data Unlearning Beyond Uniform Forgetting via Diffusion Time and Frequency Selection(https://arxiv.org/abs/2510.17917)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Data unlearning aims to remove the influence of specific training samples from a trained model without requiring full retraining. Unlike concept unlearning, data unlearning in diffusion models remains underexplored and often suffers from quality degradation or incomplete forgetting. To address this, we first observe that most existing methods attempt to unlearn the samples at all diffusion time steps equally, leading to poor-quality generation. We argue that forgetting occurs disproportionately across time and frequency, depending on the model and scenarios. By selectively focusing on specific time-frequency ranges during training, we achieve samples with higher aesthetic quality and lower noise. We validate this improvement by applying our time-frequency selective approach to diverse settings, including gradient-based and preference optimization objectives, as well as both image-level and text-to-image tasks. Finally, to evaluate both deletion and quality of unlearned data samples, we propose a simple normalized version of SSCD. Together, our analysis and methods establish a clearer understanding of the unique challenges in data unlearning for diffusion models, providing practical strategies to improve both evaluation and unlearning performance.</li>
</ul>

<h3>Title: JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Junlan Feng, Fanyu Meng, Chong Long, Pengyu Cong, Duqing Wang, Yan Zheng, Yuyao Zhang, Xuanchang Gao, Ye Yuan, Yunfei Ma, Zhijie Ren, Fan Yang, Na Wu, Di Jin, Chao Deng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17918">https://arxiv.org/abs/2510.17918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17918">https://arxiv.org/pdf/2510.17918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17918]] JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs(https://arxiv.org/abs/2510.17918)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The hallucination and credibility concerns of large language models (LLMs) are global challenges that the industry is collectively addressing. Recently, a significant amount of advances have been made on post-training and inference techniques to mitigate these challenges. However, it is widely agreed that unsafe and hallucinations of LLMs intrinsically originate from pre-training, involving pre-training data and the next-token prediction learning mechanism. In this paper, we focus on enhancing pre-training data to improve the trustworthiness and safety of LLMs. Since the data is vast, it's almost impossible to entirely purge the data of factual errors, logical inconsistencies, or distributional biases. Moreover, the pre-training data lack grounding in real-world knowledge. Each piece of data is treated as a sequence of tokens rather than as a representation of a part of the world. To overcome these issues, we propose approaches to enhancing our pre-training data with its context in the world and increasing a substantial amount of data reflecting industrial scenarios. We argue that most source data are created by the authors for specific purposes in a certain spatial-temporal context. They have played a role in the real world. By incorporating related world context information, we aim to better anchor pre-training data within real-world scenarios, thereby reducing uncertainty in model training and enhancing the model's safety and trustworthiness. We refer to our Data with World Context as DWC. We continue pre-training an earlier checkpoint of JT-35B-Base with 1.5 trillion of DWC tokens. We introduce our post-training procedures to activate the potentials of DWC. Compared with the Qwen model of a similar scale, JT-Safe-35B achieves an average performance improvement of 1.79% on the Safety and Trustworthy evaluation benchmarks, while being pretrained with only 6.2 trillion tokens.</li>
</ul>

<h3>Title: ParaVul: A Parallel Large Language Model and Retrieval-Augmented Framework for Smart Contract Vulnerability Detection</h3>
<ul>
<li><strong>Authors: </strong>Tenghui Huang, Jinbo Wen, Jiawen Kang, Siyong Chen, Zhengtao Li, Tao Zhang, Dongning Liu, Jiacheng Wang, Chengjun Cai, Yinqiu Liu, Dusit Niyato</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17919">https://arxiv.org/abs/2510.17919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17919">https://arxiv.org/pdf/2510.17919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17919]] ParaVul: A Parallel Large Language Model and Retrieval-Augmented Framework for Smart Contract Vulnerability Detection(https://arxiv.org/abs/2510.17919)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Smart contracts play a significant role in automating blockchain services. Nevertheless, vulnerabilities in smart contracts pose serious threats to blockchain security. Currently, traditional detection methods primarily rely on static analysis and formal verification, which can result in high false-positive rates and poor scalability. Large Language Models (LLMs) have recently made significant progress in smart contract vulnerability detection. However, they still face challenges such as high inference costs and substantial computational overhead. In this paper, we propose ParaVul, a parallel LLM and retrieval-augmented framework to improve the reliability and accuracy of smart contract vulnerability detection. Specifically, we first develop Sparse Low-Rank Adaptation (SLoRA) for LLM fine-tuning. SLoRA introduces sparsification by incorporating a sparse matrix into quantized LoRA-based LLMs, thereby reducing computational overhead and resource requirements while enhancing their ability to understand vulnerability-related issues. We then construct a vulnerability contract dataset and develop a hybrid Retrieval-Augmented Generation (RAG) system that integrates dense retrieval with Best Matching 25 (BM25), assisting in verifying the results generated by the LLM. Furthermore, we propose a meta-learning model to fuse the outputs of the RAG system and the LLM, thereby generating the final detection results. After completing vulnerability detection, we design chain-of-thought prompts to guide LLMs to generate comprehensive vulnerability detection reports. Simulation results demonstrate the superiority of ParaVul, especially in terms of F1 scores, achieving 0.9398 for single-label detection and 0.9330 for multi-label detection.</li>
</ul>

<h3>Title: CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections</h3>
<ul>
<li><strong>Authors: </strong>Keuntae Kim, Eunhye Jeong, Sehyeon Lee, Seohee Yoon, Yong Suk Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17921">https://arxiv.org/abs/2510.17921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17921">https://arxiv.org/pdf/2510.17921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17921]] CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections(https://arxiv.org/abs/2510.17921)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in enhancing the reasoning ability of large language models (LLMs) have been remarkably successful. LLMs trained with reinforcement learning (RL) for reasoning demonstrate strong performance in challenging tasks such as mathematics and coding, even with relatively small model sizes. However, despite these improvements in task accuracy, the assessment of creativity in LLM generations has been largely overlooked in reasoning tasks, in contrast to writing tasks. The lack of research on creativity assessment in reasoning primarily stems from two challenges: (1) the difficulty of defining the range of creativity, and (2) the necessity of human evaluation in the assessment process. To address these challenges, we propose CLAWS, a method that defines and classifies mathematical solutions into typical, creative, and hallucinated categories without human evaluation, by leveraging attention weights across prompt sections and output. CLAWS outperforms five existing white-box detection methods (Perplexity, Logit Entropy, Window Entropy, Hidden Score, and Attention Score) on five 7-8B math RL models (DeepSeek, Qwen, Mathstral, OpenMath2, and Oreal). We validate CLAWS on 4545 math problems collected from 181 math contests (AJHSME, AMC, AIME).</li>
</ul>

<h3>Title: Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shuodi Liu, Yingzhuo Liu, Zi Wang, Yusheng Wang, Huijia Wu, Liuyu Xiang, Zhaofeng He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17922">https://arxiv.org/abs/2510.17922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17922">https://arxiv.org/pdf/2510.17922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17922]] Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models(https://arxiv.org/abs/2510.17922)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable reasoning and planning capabilities, driving extensive research into task decomposition. Existing task decomposition methods focus primarily on memory, tool usage, and feedback mechanisms, achieving notable success in specific domains, but they often overlook the trade-off between performance and cost. In this study, we first conduct a comprehensive investigation on task decomposition, identifying six categorization schemes. Then, we perform an empirical analysis of three factors that influence the performance and cost of task decomposition: categories of approaches, characteristics of tasks, and configuration of decomposition and execution models, uncovering three critical insights and summarizing a set of practical principles. Building on this analysis, we propose the Select-Then-Decompose strategy, which establishes a closed-loop problem-solving process composed of three stages: selection, execution, and verification. This strategy dynamically selects the most suitable decomposition approach based on task characteristics and enhances the reliability of the results through a verification module. Comprehensive evaluations across multiple benchmarks show that the Select-Then-Decompose consistently lies on the Pareto frontier, demonstrating an optimal balance between performance and cost. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Chenwei Tang, Jingyu Xing, Xinyu Liu, Wei Ju, Jiancheng Lv, Deng Xiong, Ziyue Qiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17923">https://arxiv.org/abs/2510.17923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17923">https://arxiv.org/pdf/2510.17923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17923]] Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning(https://arxiv.org/abs/2510.17923)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing Large Language Models (LLMs), achieving remarkable performance in complex reasoning domains such as mathematics and code generation. However, current RL methods face a fundamental scalability bottleneck due to their heavy reliance on human-curated preference data or labeled datasets for reward modeling. To overcome this limitation, we explore RL on unlabeled data where models learn autonomously from continuous experience streams. The core challenge in this setting lies in reliable reward estimation without ground-truth supervision. Existing approaches like Test-Time RL address this through self-consistent consensus, but risk reinforcing incorrect pseudo-labels derived from majority voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel test-time reward mechanism that operates without external supervision. COMPASS integrates two complementary components: the Dual-Calibration Answer Reward (DCAR), which stabilizes training by establishing trustworthy pseudo-labels through confidence and credibility calibration, and the Decisive Path Reward (DPR), which directly optimizes the reasoning process quality beyond mere outcome supervision. By jointly reinforcing trustworthy consensus answers and highly decisive reasoning chains, the COMPASS systematically enhances the model's analytical capabilities. Extensive experiments show that COMPASS achieves significant and consistent performance gains across diverse reasoning tasks and model architectures, advancing a more scalable direction for LLMs to learn from continuous experience.</li>
</ul>

<h3>Title: Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yehor Tereshchenko, Mika Hämäläinen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17924">https://arxiv.org/abs/2510.17924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17924">https://arxiv.org/pdf/2510.17924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17924]] Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs(https://arxiv.org/abs/2510.17924)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive comparative analysis of Natural Language Processing (NLP) methods for automated toxicity detection in online gaming chats. Traditional machine learning models with embeddings, large language models (LLMs) with zero-shot and few-shot prompting, fine-tuned transformer models, and retrieval-augmented generation (RAG) approaches are evaluated. The evaluation framework assesses three critical dimensions: classification accuracy, processing speed, and computational costs. A hybrid moderation system architecture is proposed that optimizes human moderator workload through automated detection and incorporates continuous learning mechanisms. The experimental results demonstrate significant performance variations across methods, with fine-tuned DistilBERT achieving optimal accuracy-cost trade-offs. The findings provide empirical evidence for deploying cost-effective, efficient content moderation systems in dynamic online gaming environments.</li>
</ul>

<h3>Title: EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning</h3>
<ul>
<li><strong>Authors: </strong>He Du, Bowen Li, Aijun Yang, Siyang He, Qipeng Guo, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17928">https://arxiv.org/abs/2510.17928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17928">https://arxiv.org/pdf/2510.17928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17928]] EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning(https://arxiv.org/abs/2510.17928)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reliable verifiable data has become a key driver of capability gains in modern language models, enabling stable reinforcement learning with verifiable rewards and effective distillation that transfers competence across math, coding, and agentic tasks. Yet constructing generalizable synthetic verifiable data remains difficult due to hallucination-prone generation, and weak or trivial verification artifacts that fail to separate strong from weak solutions. Existing approaches often rely on task-specific heuristics or post-hoc filters that do not transfer across domains and lack a principled, universal evaluator of verifiability. In this work, we introduce an evolutionary, task-agnostic, strategy-guided, executably-checkable data synthesis framework that, from minimal seed supervision, jointly synthesizes problems, diverse candidate solutions, and verification artifacts, and iteratively discovers strategies via a consistency-based evaluator that enforces agreement between human-annotated and strategy-induced checks. This pipeline upgrades filtering into principled synthesis: it reliably assembles coherent, verifiable training instances and generalizes without domain-specific rules. Our experiments demonstrate the effectiveness of the proposed approach under both RLVR and model distillation training paradigms. The results show that training with our synthesized data yields significant improvements on both the LiveCodeBench and AgentBench-OS tasks, highlighting the robust generalization of our framework.</li>
</ul>

<h3>Title: From Observations to Parameters: Detecting Changepoint in Nonlinear Dynamics with Simulation-based Inference</h3>
<ul>
<li><strong>Authors: </strong>Xiangbo Deng, Cheng Chen, Peng Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17933">https://arxiv.org/abs/2510.17933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17933">https://arxiv.org/pdf/2510.17933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17933]] From Observations to Parameters: Detecting Changepoint in Nonlinear Dynamics with Simulation-based Inference(https://arxiv.org/abs/2510.17933)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Detecting regime shifts in chaotic time series is hard because observation-space signals are entangled with intrinsic variability. We propose Parameter--Space Changepoint Detection (Param--CPD), a two--stage framework that first amortizes Bayesian inference of governing parameters with a neural posterior estimator trained by simulation-based inference, and then applies a standard CPD algorithm to the resulting parameter trajectory. On Lorenz--63 with piecewise-constant parameters, Param--CPD improves F1, reduces localization error, and lowers false positives compared to observation--space baselines. We further verify identifiability and calibration of the inferred posteriors on stationary trajectories, explaining why parameter space offers a cleaner detection signal. Robustness analyses over tolerance, window length, and noise indicate consistent gains. Our results show that operating in a physically interpretable parameter space enables accurate and interpretable changepoint detection in nonlinear dynamical systems.</li>
</ul>

<h3>Title: AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB VRAM</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Huang, Hong Ting Tsang, Jiaxin Bai, Xi Peng, Gong Zhang, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17934">https://arxiv.org/abs/2510.17934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17934">https://arxiv.org/pdf/2510.17934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17934]] AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB VRAM(https://arxiv.org/abs/2510.17934)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) has shown some success in augmenting large language models (LLMs) with external knowledge. However, as a non-parametric knowledge integration paradigm for LLMs, RAG methods heavily rely on external retrieval modules and the retrieved textual context prior. Especially for very large scale knowledge augmentation, they would introduce substantial inference latency due to expensive searches and much longer relevant context. In this paper, we propose a parametric knowledge integration method, called \textbf{AtlasKV}, a scalable, effective, and general way to augment LLMs with billion-scale knowledge graphs (KGs) (e.g. 1B triples) using very little GPU memory cost (e.g. less than 20GB VRAM). In AtlasKV, we introduce KG2KV and HiKVP to integrate KG triples into LLMs at scale with sub-linear time and memory complexity. It maintains strong knowledge grounding and generalization performance using the LLMs' inherent attention mechanism, and requires no external retrievers, long context priors, or retraining when adapting to new knowledge.</li>
</ul>

<h3>Title: UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts</h3>
<ul>
<li><strong>Authors: </strong>Fu-Yun Wang, Han Zhang, Michael Gharbi, Hongsheng Li, Taesung Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17937">https://arxiv.org/abs/2510.17937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17937">https://arxiv.org/pdf/2510.17937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17937]] UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts(https://arxiv.org/abs/2510.17937)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present UniRL-Zero, a unified reinforcement learning (RL) framework that boosts, multimodal language model understanding and reasoning, diffusion model multimedia generation, and their beneficial interaction capabilities within a unified model. Our work defines six scenarios for unified model reinforcement learning, providing systematic baselines for reinforcement learning of unified understanding and generation model. Our code is available at this https URL.</li>
</ul>

<h3>Title: Believe It or Not: How Deeply do LLMs Believe Implanted Facts?</h3>
<ul>
<li><strong>Authors: </strong>Stewart Slocum, Julian Minder, Clément Dumas, Henry Sleight, Ryan Greenblatt, Samuel Marks, Rowan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17941">https://arxiv.org/abs/2510.17941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17941">https://arxiv.org/pdf/2510.17941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17941]] Believe It or Not: How Deeply do LLMs Believe Implanted Facts?(https://arxiv.org/abs/2510.17941)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge editing techniques promise to implant new factual knowledge into large language models (LLMs). But do LLMs really believe these facts? We develop a framework to measure belief depth and use it to evaluate the success of knowledge editing techniques. We operationalize belief depth as the extent to which implanted knowledge 1) generalizes to related contexts (e.g. Fermi estimates several logical steps removed), 2) is robust to self-scrutiny and direct challenge, and 3) is represented similarly to genuine knowledge (as measured by linear probes). Our evaluations show that simple prompting and mechanistic editing techniques fail to implant knowledge deeply. In contrast, Synthetic Document Finetuning (SDF) - where models are trained on LLM-generated documents consistent with a fact - often succeeds at implanting beliefs that behave similarly to genuine knowledge. However, SDF's success is not universal, as implanted beliefs that contradict basic world knowledge are brittle and representationally distinct from genuine knowledge. Overall, our work introduces measurable criteria for belief depth and enables the rigorous evaluation necessary for deploying knowledge editing in real-world applications.</li>
</ul>

<h3>Title: PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits</h3>
<ul>
<li><strong>Authors: </strong>Neeladri Bhuiya, Madhav Aggarwal, Diptanshu Purwar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17947">https://arxiv.org/abs/2510.17947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17947">https://arxiv.org/pdf/2510.17947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17947]] PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits(https://arxiv.org/abs/2510.17947)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are improving at an exceptional rate. With the advent of agentic workflows, multi-turn dialogue has become the de facto mode of interaction with LLMs for completing long and complex tasks. While LLM capabilities continue to improve, they remain increasingly susceptible to jailbreaking, especially in multi-turn scenarios where harmful intent can be subtly injected across the conversation to produce nefarious outcomes. While single-turn attacks have been extensively explored, adaptability, efficiency and effectiveness continue to remain key challenges for their multi-turn counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play framework for designing multi-turn attacks inspired by lifelong-learning agents. PLAGUE dissects the lifetime of a multi-turn attack into three carefully designed phases (Primer, Planner and Finisher) that enable a systematic and information-rich exploration of the multi-turn attack family. Evaluations show that red-teaming agents designed using PLAGUE achieve state-of-the-art jailbreaking results, improving attack success rates (ASR) by more than 30% across leading models in a lesser or comparable query budget. Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered highly resistant to jailbreaks in safety literature. Our work offers tools and insights to understand the importance of plan initialization, context optimization and lifelong learning in crafting multi-turn attacks for a comprehensive model vulnerability evaluation.</li>
</ul>

<h3>Title: Demystifying Transition Matching: When and Why It Can Beat Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Jaihoon Kim, Rajarshi Saha, Minhyuk Sung, Youngsuk Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17991">https://arxiv.org/abs/2510.17991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17991">https://arxiv.org/pdf/2510.17991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17991]] Demystifying Transition Matching: When and Why It Can Beat Flow Matching(https://arxiv.org/abs/2510.17991)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Flow Matching (FM) underpins many state-of-the-art generative models, yet recent results indicate that Transition Matching (TM) can achieve higher quality with fewer sampling steps. This work answers the question of when and why TM outperforms FM. First, when the target is a unimodal Gaussian distribution, we prove that TM attains strictly lower KL divergence than FM for finite number of steps. The improvement arises from stochastic difference latent updates in TM, which preserve target covariance that deterministic FM underestimates. We then characterize convergence rates, showing that TM achieves faster convergence than FM under a fixed compute budget, establishing its advantage in the unimodal Gaussian setting. Second, we extend the analysis to Gaussian mixtures and identify local-unimodality regimes in which the sampling dynamics approximate the unimodal case, where TM can outperform FM. The approximation error decreases as the minimal distance between component means increases, highlighting that TM is favored when the modes are well separated. However, when the target variance approaches zero, each TM update converges to the FM update, and the performance advantage of TM diminishes. In summary, we show that TM outperforms FM when the target distribution has well-separated modes and non-negligible variances. We validate our theoretical results with controlled experiments on Gaussian distributions, and extend the comparison to real-world applications in image and video generation.</li>
</ul>

<h3>Title: Investigating Demographic Bias in Brain MRI Segmentation: A Comparative Study of Deep-Learning and Non-Deep-Learning Methods</h3>
<ul>
<li><strong>Authors: </strong>Ghazal Danaee, Marc Niethammer, Jarrett Rushmore, Sylvain Bouix</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17999">https://arxiv.org/abs/2510.17999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17999">https://arxiv.org/pdf/2510.17999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17999]] Investigating Demographic Bias in Brain MRI Segmentation: A Comparative Study of Deep-Learning and Non-Deep-Learning Methods(https://arxiv.org/abs/2510.17999)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, segmentation</a></li>
<li><strong>Abstract: </strong>Deep-learning-based segmentation algorithms have substantially advanced the field of medical image analysis, particularly in structural delineations in MRIs. However, an important consideration is the intrinsic bias in the data. Concerns about unfairness, such as performance disparities based on sensitive attributes like race and sex, are increasingly urgent. In this work, we evaluate the results of three different segmentation models (UNesT, nnU-Net, and CoTr) and a traditional atlas-based method (ANTs), applied to segment the left and right nucleus accumbens (NAc) in MRI images. We utilize a dataset including four demographic subgroups: black female, black male, white female, and white male. We employ manually labeled gold-standard segmentations to train and test segmentation models. This study consists of two parts: the first assesses the segmentation performance of models, while the second measures the volumes they produce to evaluate the effects of race, sex, and their interaction. Fairness is quantitatively measured using a metric designed to quantify fairness in segmentation performance. Additionally, linear mixed models analyze the impact of demographic variables on segmentation accuracy and derived volumes. Training on the same race as the test subjects leads to significantly better segmentation accuracy for some models. ANTs and UNesT show notable improvements in segmentation accuracy when trained and tested on race-matched data, unlike nnU-Net, which demonstrates robust performance independent of demographic matching. Finally, we examine sex and race effects on the volume of the NAc using segmentations from the manual rater and from our biased models. Results reveal that the sex effects observed with manual segmentation can also be observed with biased models, whereas the race effects disappear in all but one model.</li>
</ul>

<h3>Title: BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?</h3>
<ul>
<li><strong>Authors: </strong>Fengqing Jiang, Yichen Feng, Yuetai Li, Luyao Niu, Basel Alomair, Radha Poovendran</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18003">https://arxiv.org/abs/2510.18003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18003">https://arxiv.org/pdf/2510.18003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18003]] BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?(https://arxiv.org/abs/2510.18003)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense</a></li>
<li><strong>Abstract: </strong>The convergence of LLM-powered research assistants and AI-based peer review systems creates a critical vulnerability: fully automated publication loops where AI-generated research is evaluated by AI reviewers without human oversight. We investigate this through \textbf{BadScientist}, a framework that evaluates whether fabrication-oriented paper generation agents can deceive multi-model LLM review systems. Our generator employs presentation-manipulation strategies requiring no real experiments. We develop a rigorous evaluation framework with formal error guarantees (concentration bounds and calibration analysis), calibrated on real data. Our results reveal systematic vulnerabilities: fabricated papers achieve acceptance rates up to . Critically, we identify \textit{concern-acceptance conflict} -- reviewers frequently flag integrity issues yet assign acceptance-level scores. Our mitigation strategies show only marginal improvements, with detection accuracy barely exceeding random chance. Despite provably sound aggregation mathematics, integrity checking systematically fails, exposing fundamental limitations in current AI-driven review systems and underscoring the urgent need for defense-in-depth safeguards in scientific publishing.</li>
</ul>

<h3>Title: Attention-Guided Deep Adversarial Temporal Subspace Clustering (A-DATSC) Model for multivariate spatiotemporal data</h3>
<ul>
<li><strong>Authors: </strong>Francis Ndikum Nji, Vandana Janeja, Jianwu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18004">https://arxiv.org/abs/2510.18004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18004">https://arxiv.org/pdf/2510.18004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18004]] Attention-Guided Deep Adversarial Temporal Subspace Clustering (A-DATSC) Model for multivariate spatiotemporal data(https://arxiv.org/abs/2510.18004)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deep subspace clustering models are vital for applications such as snowmelt detection, sea ice tracking, crop health monitoring, infectious disease modeling, network load prediction, and land-use planning, where multivariate spatiotemporal data exhibit complex temporal dependencies and reside on multiple nonlinear manifolds beyond the capability of traditional clustering methods. These models project data into a latent space where samples lie in linear subspaces and exploit the self-expressiveness property to uncover intrinsic relationships. Despite their success, existing methods face major limitations: they use shallow autoencoders that ignore clustering errors, emphasize global features while neglecting local structure, fail to model long-range dependencies and positional information, and are rarely applied to 4D spatiotemporal data. To address these issues, we propose A-DATSC (Attention-Guided Deep Adversarial Temporal Subspace Clustering), a model combining a deep subspace clustering generator and a quality-verifying discriminator. The generator, inspired by U-Net, preserves spatial and temporal integrity through stacked TimeDistributed ConvLSTM2D layers, reducing parameters and enhancing generalization. A graph attention transformer based self-expressive network captures local spatial relationships, global dependencies, and both short- and long-range correlations. Experiments on three real-world multivariate spatiotemporal datasets show that A-DATSC achieves substantially superior clustering performance compared to state-of-the-art deep subspace clustering models.</li>
</ul>

<h3>Title: ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues</h3>
<ul>
<li><strong>Authors: </strong>Prateek Gothwal, Deeptimaan Banerjee, Ashis Kumer Biswas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18016">https://arxiv.org/abs/2510.18016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18016">https://arxiv.org/pdf/2510.18016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18016]] ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues(https://arxiv.org/abs/2510.18016)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Engagement detection in online learning environments is vital for improving student outcomes and personalizing instruction. We present ViBED-Net (Video-Based Engagement Detection Network), a novel deep learning framework designed to assess student engagement from video data using a dual-stream architecture. ViBED-Net captures both facial expressions and full-scene context by processing facial crops and entire video frames through EfficientNetV2 for spatial feature extraction. These features are then analyzed over time using two temporal modeling strategies: Long Short-Term Memory (LSTM) networks and Transformer encoders. Our model is evaluated on the DAiSEE dataset, a large-scale benchmark for affective state recognition in e-learning. To enhance performance on underrepresented engagement classes, we apply targeted data augmentation techniques. Among the tested variants, ViBED-Net with LSTM achieves 73.43\% accuracy, outperforming existing state-of-the-art approaches. ViBED-Net demonstrates that combining face-aware and scene-aware spatiotemporal cues significantly improves engagement detection accuracy. Its modular design allows flexibility for application across education, user experience research, and content personalization. This work advances video-based affective computing by offering a scalable, high-performing solution for real-world engagement analysis. The source code for this project is available on this https URL .</li>
</ul>

<h3>Title: Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation Solution</h3>
<ul>
<li><strong>Authors: </strong>Asim Mohamed, Martin Gubri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18019">https://arxiv.org/abs/2510.18019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18019">https://arxiv.org/pdf/2510.18019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18019]] Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation Solution(https://arxiv.org/abs/2510.18019)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, fair, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Multilingual watermarking aims to make large language model (LLM) outputs traceable across languages, yet current methods still fall short. Despite claims of cross-lingual robustness, they are evaluated only on high-resource languages. We show that existing multilingual watermarking methods are not truly multilingual: they fail to remain robust under translation attacks in medium- and low-resource languages. We trace this failure to semantic clustering, which fails when the tokenizer vocabulary contains too few full-word tokens for a given language. To address this, we introduce STEAM, a back-translation-based detection method that restores watermark strength lost through translation. STEAM is compatible with any watermarking method, robust across different tokenizers and languages, non-invasive, and easily extendable to new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17 languages, STEAM provides a simple and robust path toward fairer watermarking across diverse languages.</li>
</ul>

<h3>Title: From Local to Global: Revisiting Structured Pruning Paradigms for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ziyan Wang, Enmao Diao, Qi Le, Pu Wang, Minwoo Lee, Shu-ping Yeh, Evgeny Stupachenko, Hao Feng, Li Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18030">https://arxiv.org/abs/2510.18030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18030">https://arxiv.org/pdf/2510.18030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18030]] From Local to Global: Revisiting Structured Pruning Paradigms for Large Language Models(https://arxiv.org/abs/2510.18030)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Structured pruning is a practical approach to deploying large language models (LLMs) efficiently, as it yields compact, hardware-friendly architectures. However, the dominant local paradigm is task-agnostic: by optimizing layer-wise reconstruction rather than task objectives, it tends to preserve perplexity or generic zero-shot behavior but fails to capitalize on modest task-specific calibration signals, often yielding limited downstream gains. We revisit global structured pruning and present GISP-Global Iterative Structured Pruning-a post-training method that removes attention heads and MLP channels using first-order, loss-based important weights aggregated at the structure level with block-wise normalization. An iterative schedule, rather than one-shot pruning, stabilizes accuracy at higher sparsity and mitigates perplexity collapse without requiring intermediate fine-tuning; the pruning trajectory also forms nested subnetworks that support a "prune-once, deploy-many" workflow. Furthermore, because importance is defined by a model-level loss, GISP naturally supports task-specific objectives; we instantiate perplexity for language modeling and a margin-based objective for decision-style tasks. Extensive experiments show that across Llama2-7B/13B, Llama3-8B, and Mistral-0.3-7B, GISP consistently lowers WikiText-2 perplexity and improves downstream accuracy, with especially strong gains at 40-50% sparsity; on DeepSeek-R1-Distill-Llama-3-8B with GSM8K, task-aligned calibration substantially boosts exact-match accuracy.</li>
</ul>

<h3>Title: SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection</h3>
<ul>
<li><strong>Authors: </strong>Roberto Brusnicki, David Pop, Yuan Gao, Mattia Piccinini, Johannes Betz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18034">https://arxiv.org/abs/2510.18034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18034">https://arxiv.org/pdf/2510.18034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18034]] SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection(https://arxiv.org/abs/2510.18034)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Autonomous driving systems remain critically vulnerable to the long-tail of rare, out-of-distribution scenarios with semantic anomalies. While Vision Language Models (VLMs) offer promising reasoning capabilities, naive prompting approaches yield unreliable performance and depend on expensive proprietary models, limiting practical deployment. We introduce SAVANT (Semantic Analysis with Vision-Augmented Anomaly deTection), a structured reasoning framework that achieves high accuracy and recall in detecting anomalous driving scenarios from input images through layered scene analysis and a two-phase pipeline: structured scene description extraction followed by multi-modal evaluation. Our approach transforms VLM reasoning from ad-hoc prompting to systematic analysis across four semantic layers: Street, Infrastructure, Movable Objects, and Environment. SAVANT achieves 89.6% recall and 88.0% accuracy on real-world driving scenarios, significantly outperforming unstructured baselines. More importantly, we demonstrate that our structured framework enables a fine-tuned 7B parameter open-source model (Qwen2.5VL) to achieve 90.8% recall and 93.8% accuracy - surpassing all models evaluated while enabling local deployment at near-zero cost. By automatically labeling over 9,640 real-world images with high accuracy, SAVANT addresses the critical data scarcity problem in anomaly detection and provides a practical path toward reliable, accessible semantic monitoring for autonomous systems.</li>
</ul>

<h3>Title: Language Models as Semantic Augmenters for Sequential Recommenders</h3>
<ul>
<li><strong>Authors: </strong>Mahsa Valizadeh, Xiangjue Dong, Rui Tuo, James Caverlee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18046">https://arxiv.org/abs/2510.18046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18046">https://arxiv.org/pdf/2510.18046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18046]] Language Models as Semantic Augmenters for Sequential Recommenders(https://arxiv.org/abs/2510.18046)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel at capturing latent semantics and contextual relationships across diverse modalities. However, in modeling user behavior from sequential interaction data, performance often suffers when such semantic context is limited or absent. We introduce LaMAR, a LLM-driven semantic enrichment framework designed to enrich such sequences automatically. LaMAR leverages LLMs in a few-shot setting to generate auxiliary contextual signals by inferring latent semantic aspects of a user's intent and item relationships from existing metadata. These generated signals, such as inferred usage scenarios, item intents, or thematic summaries, augment the original sequences with greater contextual depth. We demonstrate the utility of this generated resource by integrating it into benchmark sequential modeling tasks, where it consistently improves performance. Further analysis shows that LLM-generated signals exhibit high semantic novelty and diversity, enhancing the representational capacity of the downstream models. This work represents a new data-centric paradigm where LLMs serve as intelligent context generators, contributing a new method for the semi-automatic creation of training data and language resources.</li>
</ul>

<h3>Title: Measure-Theoretic Anti-Causal Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Arman Behnam, Binghui Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18052">https://arxiv.org/abs/2510.18052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18052">https://arxiv.org/pdf/2510.18052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18052]] Measure-Theoretic Anti-Causal Representation Learning(https://arxiv.org/abs/2510.18052)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Causal representation learning in the anti-causal setting (labels cause features rather than the reverse) presents unique challenges requiring specialized approaches. We propose Anti-Causal Invariant Abstractions (ACIA), a novel measure-theoretic framework for anti-causal representation learning. ACIA employs a two-level design, low-level representations capture how labels generate observations, while high-level representations learn stable causal patterns across environment-specific variations. ACIA addresses key limitations of existing approaches by accommodating prefect and imperfect interventions through interventional kernels, eliminating dependency on explicit causal structures, handling high-dimensional data effectively, and providing theoretical guarantees for out-of-distribution generalization. Experiments on synthetic and real-world medical datasets demonstrate that ACIA consistently outperforms state-of-the-art methods in both accuracy and invariance metrics. Furthermore, our theoretical results establish tight bounds on performance gaps between training and unseen environments, confirming the efficacy of our approach for robust anti-causal learning.</li>
</ul>

<h3>Title: Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Fan, Tong Wei, Chaoran Cheng, Yuxin Chen, Ge Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18053">https://arxiv.org/abs/2510.18053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18053">https://arxiv.org/pdf/2510.18053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18053]] Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models(https://arxiv.org/abs/2510.18053)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Balancing exploration and exploitation during reinforcement learning fine-tuning of generative models presents a critical challenge, as existing approaches rely on fixed divergence regularization that creates an inherent dilemma: strong regularization preserves model capabilities but limits reward optimization, while weak regularization enables greater alignment but risks instability or reward hacking. We introduce Adaptive Divergence Regularized Policy Optimization (ADRPO), which automatically adjusts regularization strength based on advantage estimates-reducing regularization for high-value samples while applying stronger regularization to poor samples, enabling policies to navigate between exploration and aggressive exploitation according to data quality. Our implementation with Wasserstein-2 regularization for flow matching generative models achieves remarkable results on text-to-image generation, achieving better semantic alignment and diversity than offline methods like DPO and online methods with fixed regularization like ORW-CFM-W2. ADRPO enables a 2B parameter SD3 model to surpass much larger models with 4.8B and 12B parameters in attribute binding, semantic consistency, artistic style transfer, and compositional control while maintaining generation diversity. ADRPO generalizes to KL-regularized fine-tuning of both text-only LLMs and multi-modal reasoning models, enhancing existing online RL methods like GRPO. In LLM fine-tuning, ADRPO demonstrates an emergent ability to escape local optima through active exploration, while in multi-modal audio reasoning, it outperforms GRPO through superior step-by-step reasoning, enabling a 7B model to outperform substantially larger commercial models including Gemini 2.5 Pro and GPT-4o Audio, offering an effective plug-and-play solution to the exploration-exploitation challenge across diverse generative architectures and modalities.</li>
</ul>

<h3>Title: HouseTour: A Virtual Real Estate A(I)gent</h3>
<ul>
<li><strong>Authors: </strong>Ata Çelen, Marc Pollefeys, Daniel Barath, Iro Armeni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18054">https://arxiv.org/abs/2510.18054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18054">https://arxiv.org/pdf/2510.18054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18054]] HouseTour: A Virtual Real Estate A(I)gent(https://arxiv.org/abs/2510.18054)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce HouseTour, a method for spatially-aware 3D camera trajectory and natural language summary generation from a collection of images depicting an existing 3D space. Unlike existing vision-language models (VLMs), which struggle with geometric reasoning, our approach generates smooth video trajectories via a diffusion process constrained by known camera poses and integrates this information into the VLM for 3D-grounded descriptions. We synthesize the final video using 3D Gaussian splatting to render novel views along the trajectory. To support this task, we present the HouseTour dataset, which includes over 1,200 house-tour videos with camera poses, 3D reconstructions, and real estate descriptions. Experiments demonstrate that incorporating 3D camera trajectories into the text generation process improves performance over methods handling each task independently. We evaluate both individual and end-to-end performance, introducing a new joint metric. Our work enables automated, professional-quality video creation for real estate and touristic applications without requiring specialized expertise or equipment.</li>
</ul>

<h3>Title: SPACeR: Self-Play Anchoring with Centralized Reference Models</h3>
<ul>
<li><strong>Authors: </strong>Wei-Jer Chang, Akshay Rangesh, Kevin Joseph, Matthew Strong, Masayoshi Tomizuka, Yihan Hu, Wei Zhan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18060">https://arxiv.org/abs/2510.18060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18060">https://arxiv.org/pdf/2510.18060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18060]] SPACeR: Self-Play Anchoring with Centralized Reference Models(https://arxiv.org/abs/2510.18060)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Developing autonomous vehicles (AVs) requires not only safety and efficiency, but also realistic, human-like behaviors that are socially aware and predictable. Achieving this requires sim agent policies that are human-like, fast, and scalable in multi-agent settings. Recent progress in imitation learning with large diffusion-based or tokenized models has shown that behaviors can be captured directly from human driving data, producing realistic policies. However, these models are computationally expensive, slow during inference, and struggle to adapt in reactive, closed-loop scenarios. In contrast, self-play reinforcement learning (RL) scales efficiently and naturally captures multi-agent interactions, but it often relies on heuristics and reward shaping, and the resulting policies can diverge from human norms. We propose SPACeR, a framework that leverages a pretrained tokenized autoregressive motion model as a centralized reference policy to guide decentralized self-play. The reference model provides likelihood rewards and KL divergence, anchoring policies to the human driving distribution while preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our method achieves competitive performance with imitation-learned policies while being up to 10x faster at inference and 50x smaller in parameter size than large generative models. In addition, we demonstrate in closed-loop ego planning evaluation tasks that our sim agents can effectively measure planner quality with fast and scalable traffic simulation, establishing a new paradigm for testing autonomous driving policies.</li>
</ul>

<h3>Title: Fine-tuning Flow Matching Generative Models with Intermediate Feedback</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Fan, Chaoran Cheng, Shuaike Shen, Xiangxin Zhou, Ge Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18072">https://arxiv.org/abs/2510.18072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18072">https://arxiv.org/pdf/2510.18072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18072]] Fine-tuning Flow Matching Generative Models with Intermediate Feedback(https://arxiv.org/abs/2510.18072)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Flow-based generative models have shown remarkable success in text-to-image generation, yet fine-tuning them with intermediate feedback remains challenging, especially for continuous-time flow matching models. Most existing approaches solely learn from outcome rewards, struggling with the credit assignment problem. Alternative methods that attempt to learn a critic via direct regression on cumulative rewards often face training instabilities and model collapse in online settings. We present AC-Flow, a robust actor-critic framework that addresses these challenges through three key innovations: (1) reward shaping that provides well-normalized learning signals to enable stable intermediate value learning and gradient control, (2) a novel dual-stability mechanism that combines advantage clipping to prevent destructive policy updates with a warm-up phase that allows the critic to mature before influencing the actor, and (3) a scalable generalized critic weighting scheme that extends traditional reward-weighted methods while preserving model diversity through Wasserstein regularization. Through extensive experiments on Stable Diffusion 3, we demonstrate that AC-Flow achieves state-of-the-art performance in text-to-image alignment tasks and generalization to unseen human preference models. Our results demonstrate that even with a computationally efficient critic model, we can robustly finetune flow models without compromising generative quality, diversity, or stability.</li>
</ul>

<h3>Title: Chain-of-Thought Reasoning Improves Context-Aware Translation with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shabnam Ataee, Andrei Popescu-Belis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18077">https://arxiv.org/abs/2510.18077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18077">https://arxiv.org/pdf/2510.18077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18077]] Chain-of-Thought Reasoning Improves Context-Aware Translation with Large Language Models(https://arxiv.org/abs/2510.18077)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper assesses the capacity of large language models (LLMs) to translate texts that include inter-sentential dependencies. We use the English-French DiscEvalMT benchmark (Bawden et al., 2018) with pairs of sentences containing translation challenges either for pronominal anaphora or for lexical cohesion. We evaluate 12 LLMs from the DeepSeek-R1, GPT, Llama, Mistral and Phi families on two tasks: (1) distinguishing a correct translation from a wrong but plausible one; (2) generating a correct translation. We compare prompts that encourage chain-of-thought reasoning with those that do not. The best models take advantage of reasoning and reach about 90% accuracy on the first task, and COMET scores of about 92% on the second task, with GPT-4, GPT-4o and Phi standing out. Moreover, we observe a "wise get wiser" effect: the improvements through reasoning are positively correlated with the scores of the models without reasoning.</li>
</ul>

<h3>Title: MEG-GPT: A transformer-based foundation model for magnetoencephalography data</h3>
<ul>
<li><strong>Authors: </strong>Rukuang Huang, Sungjun Cho, Chetan Gohil, Oiwi Parker Jones, Mark Woolrich</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18080">https://arxiv.org/abs/2510.18080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18080">https://arxiv.org/pdf/2510.18080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18080]] MEG-GPT: A transformer-based foundation model for magnetoencephalography data(https://arxiv.org/abs/2510.18080)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Modelling the complex spatiotemporal patterns of large-scale brain dynamics is crucial for neuroscience, but traditional methods fail to capture the rich structure in modalities such as magnetoencephalography (MEG). Recent advances in deep learning have enabled significant progress in other domains, such as language and vision, by using foundation models at scale. Here, we introduce MEG-GPT, a transformer based foundation model that uses time-attention and next time-point prediction. To facilitate this, we also introduce a novel data-driven tokeniser for continuous MEG data, which preserves the high temporal resolution of continuous MEG signals without lossy transformations. We trained MEG-GPT on tokenised brain region time-courses extracted from a large-scale MEG dataset (N=612, eyes-closed rest, Cam-CAN data), and show that the learnt model can generate data with realistic spatio-spectral properties, including transient events and population variability. Critically, it performs well in downstream decoding tasks, improving downstream supervised prediction task, showing improved zero-shot generalisation across sessions (improving accuracy from 0.54 to 0.59) and subjects (improving accuracy from 0.41 to 0.49) compared to a baseline methods. Furthermore, we show the model can be efficiently fine-tuned on a smaller labelled dataset to boost performance in cross-subject decoding scenarios. This work establishes a powerful foundation model for electrophysiological data, paving the way for applications in computational neuroscience and neural decoding.</li>
</ul>

<h3>Title: Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Zhang, Andrew Estornell, David D. Baek, Bo Li, Xiaojun Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18081">https://arxiv.org/abs/2510.18081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18081">https://arxiv.org/pdf/2510.18081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18081]] Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth(https://arxiv.org/abs/2510.18081)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit strong but shallow alignment: they directly refuse harmful queries when a refusal is expected at the very start of an assistant turn, yet this protection collapses once a harmful continuation is underway (either through the adversarial attacks or via harmful assistant-prefill attacks). This raises a fundamental question: Can the innate shallow alignment in LLMs be unlocked to ensure safety at arbitrary generation depths? To achieve this goal, we propose Any-Depth Alignment (ADA), an effective inference-time defense with negligible overhead. ADA is built based on our observation that alignment is concentrated in the assistant header tokens through repeated use in shallow-refusal training, and these tokens possess the model's strong alignment priors. By reintroducing these tokens mid-stream, ADA induces the model to reassess harmfulness and recover refusals at any point in generation. Across diverse open-source model families (Llama, Gemma, Mistral, Qwen, DeepSeek, and gpt-oss), ADA achieves robust safety performance without requiring any changes to the base model's parameters. It secures a near-100% refusal rate against challenging adversarial prefill attacks ranging from dozens to thousands of tokens. Furthermore, ADA reduces the average success rate of prominent adversarial prompt attacks (such as GCG, AutoDAN, PAIR, and TAP) to below 3%. This is all accomplished while preserving utility on benign tasks with minimal over-refusal. ADA maintains this resilience even after the base model undergoes subsequent instruction tuning (benign or adversarial).</li>
</ul>

<h3>Title: Chimera: Compositional Image Generation using Part-based Concepting</h3>
<ul>
<li><strong>Authors: </strong>Shivam Singh, Yiming Chen, Agneet Chatterjee, Amit Raj, James Hays, Yezhou Yang, Chitra Baral</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18083">https://arxiv.org/abs/2510.18083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18083">https://arxiv.org/pdf/2510.18083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18083]] Chimera: Compositional Image Generation using Part-based Concepting(https://arxiv.org/abs/2510.18083)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Personalized image generative models are highly proficient at synthesizing images from text or a single image, yet they lack explicit control for composing objects from specific parts of multiple source images without user specified masks or annotations. To address this, we introduce Chimera, a personalized image generation model that generates novel objects by combining specified parts from different source images according to textual instructions. To train our model, we first construct a dataset from a taxonomy built on 464 unique (part, subject) pairs, which we term semantic atoms. From this, we generate 37k prompts and synthesize the corresponding images with a high-fidelity text-to-image model. We train a custom diffusion prior model with part-conditional guidance, which steers the image-conditioning features to enforce both semantic identity and spatial layout. We also introduce an objective metric PartEval to assess the fidelity and compositional accuracy of generation pipelines. Human evaluations and our proposed metric show that Chimera outperforms other baselines by 14% in part alignment and compositional accuracy and 21% in visual quality.</li>
</ul>

<h3>Title: RL-Driven Security-Aware Resource Allocation Framework for UAV-Assisted O-RAN</h3>
<ul>
<li><strong>Authors: </strong>Zaineh Abughazzah, Emna Baccour, Loay Ismail, Amr Mohamed, Mounir Hamdi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18084">https://arxiv.org/abs/2510.18084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18084">https://arxiv.org/pdf/2510.18084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18084]] RL-Driven Security-Aware Resource Allocation Framework for UAV-Assisted O-RAN(https://arxiv.org/abs/2510.18084)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>The integration of Unmanned Aerial Vehicles (UAVs) into Open Radio Access Networks (O-RAN) enhances communication in disaster management and Search and Rescue (SAR) operations by ensuring connectivity when infrastructure fails. However, SAR scenarios demand stringent security and low-latency communication, as delays or breaches can compromise mission success. While UAVs serve as mobile relays, they introduce challenges in energy consumption and resource management, necessitating intelligent allocation strategies. Existing UAV-assisted O-RAN approaches often overlook the joint optimization of security, latency, and energy efficiency in dynamic environments. This paper proposes a novel Reinforcement Learning (RL)-based framework for dynamic resource allocation in UAV relays, explicitly addressing these trade-offs. Our approach formulates an optimization problem that integrates security-aware resource allocation, latency minimization, and energy efficiency, which is solved using RL. Unlike heuristic or static methods, our framework adapts in real-time to network dynamics, ensuring robust communication. Simulations demonstrate superior performance compared to heuristic baselines, achieving enhanced security and energy efficiency while maintaining ultra-low latency in SAR scenarios.</li>
</ul>

<h3>Title: Accelerating Vision Transformers with Adaptive Patch Sizes</h3>
<ul>
<li><strong>Authors: </strong>Rohan Choudhury, JungEun Kim, Jinhyung Park, Eunho Yang, László A. Jeni, Kris M. Kitani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18091">https://arxiv.org/abs/2510.18091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18091">https://arxiv.org/pdf/2510.18091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18091]] Accelerating Vision Transformers with Adaptive Patch Sizes(https://arxiv.org/abs/2510.18091)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) partition input images into uniformly sized patches regardless of their content, resulting in long input sequence lengths for high-resolution images. We present Adaptive Patch Transformers (APT), which addresses this by using multiple different patch sizes within the same image. APT reduces the total number of input tokens by allocating larger patch sizes in more homogeneous areas and smaller patches in more complex ones. APT achieves a drastic speedup in ViT inference and training, increasing throughput by 40% on ViT-L and 50% on ViT-H while maintaining downstream performance, and can be applied to a previously fine-tuned ViT, converging in as little as 1 epoch. It also significantly reduces training and inference time without loss of performance in high-resolution dense visual tasks, achieving up to 30\% faster training and inference in visual QA, object detection, and semantic segmentation.</li>
</ul>

<h3>Title: PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces</h3>
<ul>
<li><strong>Authors: </strong>Wan Ki Wong, Sahel Torkamani, Michele Ciampi, Rik Sarkar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18109">https://arxiv.org/abs/2510.18109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18109">https://arxiv.org/pdf/2510.18109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18109]] PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces(https://arxiv.org/abs/2510.18109)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, fair</a></li>
<li><strong>Abstract: </strong>Evaluating the relevance of data is a critical task for model builders seeking to acquire datasets that enhance model performance. Ideally, such evaluation should allow the model builder to assess the utility of candidate data without exposing proprietary details of the model. At the same time, data providers must be assured that no information about their data - beyond the computed utility score - is disclosed to the model builder. In this paper, we present PrivaDE, a cryptographic protocol for privacy-preserving utility scoring and selection of data for machine learning. While prior works have proposed data evaluation protocols, our approach advances the state of the art through a practical, blockchain-centric design. Leveraging the trustless nature of blockchains, PrivaDE enforces malicious-security guarantees and ensures strong privacy protection for both models and datasets. To achieve efficiency, we integrate several techniques - including model distillation, model splitting, and cut-and-choose zero-knowledge proofs - bringing the runtime to a practical level. Furthermore, we propose a unified utility scoring function that combines empirical loss, predictive entropy, and feature-space diversity, and that can be seamlessly integrated into active-learning workflows. Evaluation shows that PrivaDE performs data evaluation effectively, achieving online runtimes within 15 minutes even for models with millions of parameters. Our work lays the foundation for fair and automated data marketplaces in decentralized machine learning ecosystems.</li>
</ul>

<h3>Title: Does Reasoning Help LLM Agents Play Dungeons and Dragons? A Prompt Engineering Experiment</h3>
<ul>
<li><strong>Authors: </strong>Patricia Delafuente, Arya Honraopatil, Lara J. Martin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18112">https://arxiv.org/abs/2510.18112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18112">https://arxiv.org/pdf/2510.18112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18112]] Does Reasoning Help LLM Agents Play Dungeons and Dragons? A Prompt Engineering Experiment(https://arxiv.org/abs/2510.18112)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper explores the application of Large Language Models (LLMs) and reasoning to predict Dungeons & Dragons (DnD) player actions and format them as Avrae Discord bot commands. Using the FIREBALL dataset, we evaluated a reasoning model, DeepSeek-R1-Distill-LLaMA-8B, and an instruct model, LLaMA-3.1-8B-Instruct, for command generation. Our findings highlight the importance of providing specific instructions to models, that even single sentence changes in prompts can greatly affect the output of models, and that instruct models are sufficient for this task compared to reasoning models.</li>
</ul>

<h3>Title: Investigating the Impact of Dark Patterns on LLM-Based Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Devin Ersoy (1), Brandon Lee (1), Ananth Shreekumar (1), Arjun Arunasalam (2), Muhammad Ibrahim (3), Antonio Bianchi (1), Z. Berkay Celik (1) ((1) Purdue University, (2) Florida International University, (3) Georgia Institute of Technology)</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18113">https://arxiv.org/abs/2510.18113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18113">https://arxiv.org/pdf/2510.18113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18113]] Investigating the Impact of Dark Patterns on LLM-Based Web Agents(https://arxiv.org/abs/2510.18113)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, large language model</a></li>
<li><strong>Abstract: </strong>As users increasingly turn to large language model (LLM) based web agents to automate online tasks, agents may encounter dark patterns: deceptive user interface designs that manipulate users into making unintended decisions. Although dark patterns primarily target human users, their potentially harmful impacts on LLM-based generalist web agents remain unexplored. In this paper, we present the first study that investigates the impact of dark patterns on the decision-making process of LLM-based generalist web agents. To achieve this, we introduce LiteAgent, a lightweight framework that automatically prompts agents to execute tasks while capturing comprehensive logs and screen-recordings of their interactions. We also present TrickyArena, a controlled environment comprising web applications from domains such as e-commerce, streaming services, and news platforms, each containing diverse and realistic dark patterns that can be selectively enabled or disabled. Using LiteAgent and TrickyArena, we conduct multiple experiments to assess the impact of both individual and combined dark patterns on web agent behavior. We evaluate six popular LLM-based generalist web agents across three LLMs and discover that when there is a single dark pattern present, agents are susceptible to it an average of 41% of the time. We also find that modifying dark pattern UI attributes through visual design changes or HTML code adjustments and introducing multiple dark patterns simultaneously can influence agent susceptibility. This study emphasizes the need for holistic defense mechanisms in web agents, encompassing both agent-specific protections and broader web safety measures.</li>
</ul>

<h3>Title: Latent Discrete Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Dario Shariatian, Alain Durmus, Stefano Peluchetti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18114">https://arxiv.org/abs/2510.18114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18114">https://arxiv.org/pdf/2510.18114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18114]] Latent Discrete Diffusion Models(https://arxiv.org/abs/2510.18114)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We study discrete diffusion for language and other categorical data and focus on a common limitation of masked denoisers: reverse transitions typically factorize across positions, which can weaken joint structure and degrade quality in few-step generation. We propose \emph{Latent Discrete Diffusion Models} (LDDMs), which couple a masked discrete diffusion over tokens with a continuous diffusion over latent embeddings. The latent channel provides a softer signal and carries cross-token dependencies that help resolve ambiguities. We present two instantiations: (i) FUJI-LDDMs, which perform fully joint denoising of tokens and latents, and (ii) SEQ-LDDMs, which sequentially resolve the latent and then the discrete chain conditionally on it. For both variants we derive ELBO-style objectives and discuss design choices to learn informative latents yet amenable to diffusoin modeling. In experiments, LDDMs yield improvements on unconditional generation metrics as compared to state-of-the-art masked discrete diffusion baselines, and are effective at lower sampling budgets, where unmasking many tokens per step is desirable.</li>
</ul>

<h3>Title: Gradient Variance Reveals Failure Modes in Flow-Based Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Teodora Reu, Sixtine Dromigny, Michael Bronstein, Francisco Vargas</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18118">https://arxiv.org/abs/2510.18118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18118">https://arxiv.org/pdf/2510.18118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18118]] Gradient Variance Reveals Failure Modes in Flow-Based Generative Models(https://arxiv.org/abs/2510.18118)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rectified Flows learn ODE vector fields whose trajectories are straight between source and target distributions, enabling near one-step inference. We show that this straight-path objective conceals fundamental failure modes: under deterministic training, low gradient variance drives memorization of arbitrary training pairings, even when interpolant lines between pairs intersect. To analyze this mechanism, we study Gaussian-to-Gaussian transport and use the loss gradient variance across stochastic and deterministic regimes to characterize which vector fields optimization favors in each setting. We then show that, in a setting where all interpolating lines intersect, applying Rectified Flow yields the same specific pairings at inference as during training. More generally, we prove that a memorizing vector field exists even when training interpolants intersect, and that optimizing the straight-path objective converges to this ill-defined field. At inference, deterministic integration reproduces the exact training pairings. We validate our findings empirically on the CelebA dataset, confirming that deterministic interpolants induce memorization, while the injection of small noise restores generalization.</li>
</ul>

<h3>Title: Efficient Long-context Language Model Training by Core Attention Disaggregation</h3>
<ul>
<li><strong>Authors: </strong>Yonghao Zhuang, Junda Chen, Bo Pang, Yi Gu, Yibo Zhu, Yimin Jiang, Ion Stoica, Eric Xing, Hao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18121">https://arxiv.org/abs/2510.18121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18121">https://arxiv.org/pdf/2510.18121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18121]] Efficient Long-context Language Model Training by Core Attention Disaggregation(https://arxiv.org/abs/2510.18121)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present core attention disaggregation (CAD), a technique that improves long-context large language model training by decoupling the core attention computation, softmax(QK^T)V, from the rest of the model and executing it on a separate pool of devices. In existing systems, core attention is colocated with other layers; at long context lengths, its quadratic compute growth compared to the near-linear growth of other components causes load imbalance and stragglers across data and pipeline parallel groups. CAD is enabled by two observations. First, core attention is stateless: it has no trainable parameters and only minimal transient data, so balancing reduces to scheduling compute-bound tasks. Second, it is composable: modern attention kernels retain high efficiency when processing fused batches of token-level shards with arbitrary lengths. CAD partitions core attention into token-level tasks and dispatches them to dedicated attention servers, which dynamically rebatch tasks to equalize compute without sacrificing kernel efficiency. We implement CAD in a system called DistCA, which uses a ping-pong execution scheme to fully overlap communication with computation and in-place execution on attention servers to reduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens, DistCA improves end-to-end training throughput by up to 1.35x, eliminates data and pipeline parallel stragglers, and achieves near-perfect compute and memory balance.</li>
</ul>

<h3>Title: HyperDiffusionFields (HyDiF): Diffusion-Guided Hypernetworks for Learning Implicit Molecular Neural Fields</h3>
<ul>
<li><strong>Authors: </strong>Sudarshan Babu, Phillip Lo, Xiao Zhang, Aadi Srivastava, Ali Davariashtiyani, Jason Perera, Michael Maire, Aly A. Khan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18122">https://arxiv.org/abs/2510.18122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18122">https://arxiv.org/pdf/2510.18122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18122]] HyperDiffusionFields (HyDiF): Diffusion-Guided Hypernetworks for Learning Implicit Molecular Neural Fields(https://arxiv.org/abs/2510.18122)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce HyperDiffusionFields (HyDiF), a framework that models 3D molecular conformers as continuous fields rather than discrete atomic coordinates or graphs. At the core of our approach is the Molecular Directional Field (MDF), a vector field that maps any point in space to the direction of the nearest atom of a particular type. We represent MDFs using molecule-specific neural implicit fields, which we call Molecular Neural Fields (MNFs). To enable learning across molecules and facilitate generalization, we adopt an approach where a shared hypernetwork, conditioned on a molecule, generates the weights of the given molecule's MNF. To endow the model with generative capabilities, we train the hypernetwork as a denoising diffusion model, enabling sampling in the function space of molecular fields. Our design naturally extends to a masked diffusion mechanism to support structure-conditioned generation tasks, such as molecular inpainting, by selectively noising regions of the field. Beyond generation, the localized and continuous nature of MDFs enables spatially fine-grained feature extraction for molecular property prediction, something not easily achievable with graph or point cloud based methods. Furthermore, we demonstrate that our approach scales to larger biomolecules, illustrating a promising direction for field-based molecular modeling.</li>
</ul>

<h3>Title: SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving</h3>
<ul>
<li><strong>Authors: </strong>Xiangbo Gao, Tzu-Hsiang Lin, Ruojing Song, Yuheng Wu, Kuan-Ru Huang, Zicheng Jin, Fangzhou Lin, Shinan Liu, Zhengzhong Tu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18123">https://arxiv.org/abs/2510.18123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18123">https://arxiv.org/pdf/2510.18123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18123]] SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving(https://arxiv.org/abs/2510.18123)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>Collaborative driving systems leverage vehicle-to-everything (V2X) communication across multiple agents to enhance driving safety and efficiency. Traditional V2X systems take raw sensor data, neural features, or perception results as communication media, which face persistent challenges, including high bandwidth demands, semantic loss, and interoperability issues. Recent advances investigate natural language as a promising medium, which can provide semantic richness, decision-level reasoning, and human-machine interoperability at significantly lower bandwidth. Despite great promise, this paradigm shift also introduces new vulnerabilities within language communication, including message loss, hallucinations, semantic manipulation, and adversarial attacks. In this work, we present the first systematic study of full-stack safety and security issues in natural-language-based collaborative driving. Specifically, we develop a comprehensive taxonomy of attack strategies, including connection disruption, relay/replay interference, content spoofing, and multi-connection forgery. To mitigate these risks, we introduce an agentic defense pipeline, which we call SafeCoop, that integrates a semantic firewall, language-perception consistency checks, and multi-source consensus, enabled by an agentic transformation function for cross-frame spatial alignment. We systematically evaluate SafeCoop in closed-loop CARLA simulation across 32 critical scenarios, achieving 69.15% driving score improvement under malicious attacks and up to 67.32% F1 score for malicious detection. This study provides guidance for advancing research on safe, secure, and trustworthy language-driven collaboration in transportation systems. Our project page is this https URL.</li>
</ul>

<h3>Title: Rethinking PCA Through Duality</h3>
<ul>
<li><strong>Authors: </strong>Jan Quan, Johan Suykens, Panagiotis Patrinos</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18130">https://arxiv.org/abs/2510.18130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18130">https://arxiv.org/pdf/2510.18130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18130]] Rethinking PCA Through Duality(https://arxiv.org/abs/2510.18130)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Motivated by the recently shown connection between self-attention and (kernel) principal component analysis (PCA), we revisit the fundamentals of PCA. Using the difference-of-convex (DC) framework, we present several novel formulations and provide new theoretical insights. In particular, we show the kernelizability and out-of-sample applicability for a PCA-like family of problems. Moreover, we uncover that simultaneous iteration, which is connected to the classical QR algorithm, is an instance of the difference-of-convex algorithm (DCA), offering an optimization perspective on this longstanding method. Further, we describe new algorithms for PCA and empirically compare them with state-of-the-art methods. Lastly, we introduce a kernelizable dual formulation for a robust variant of PCA that minimizes the $l_1$ deviation of the reconstruction errors.</li>
</ul>

<h3>Title: World-in-World: World Models in a Closed-Loop World</h3>
<ul>
<li><strong>Authors: </strong>Jiahan Zhang, Muqing Jiang, Nanru Dai, Taiming Lu, Arda Uzunoglu, Shunchi Zhang, Yana Wei, Jiahao Wang, Vishal M. Patel, Paul Pu Liang, Daniel Khashabi, Cheng Peng, Rama Chellappa, Tianmin Shu, Alan Yuille, Yilun Du, Jieneng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18135">https://arxiv.org/abs/2510.18135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18135">https://arxiv.org/pdf/2510.18135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18135]] World-in-World: World Models in a Closed-Loop World(https://arxiv.org/abs/2510.18135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative world models (WMs) can now simulate worlds with striking visual realism, which naturally raises the question of whether they can endow embodied agents with predictive perception for decision making. Progress on this question has been limited by fragmented evaluation: most existing benchmarks adopt open-loop protocols that emphasize visual quality in isolation, leaving the core issue of embodied utility unresolved, i.e., do WMs actually help agents succeed at embodied tasks? To address this gap, we introduce World-in-World, the first open platform that benchmarks WMs in a closed-loop world that mirrors real agent-environment interactions. World-in-World provides a unified online planning strategy and a standardized action API, enabling heterogeneous WMs for decision making. We curate four closed-loop environments that rigorously evaluate diverse WMs, prioritize task success as the primary metric, and move beyond the common focus on visual quality; we also present the first data scaling law for world models in embodied settings. Our study uncovers three surprises: (1) visual quality alone does not guarantee task success, controllability matters more; (2) scaling post-training with action-observation data is more effective than upgrading the pretrained video generators; and (3) allocating more inference-time compute allows WMs to substantially improve closed-loop performance.</li>
</ul>

<h3>Title: LLMs Encode How Difficult Problems Are</h3>
<ul>
<li><strong>Authors: </strong>William Lugoloobi, Chris Russell</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18147">https://arxiv.org/abs/2510.18147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18147">https://arxiv.org/pdf/2510.18147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18147]] LLMs Encode How Difficult Problems Are(https://arxiv.org/abs/2510.18147)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models exhibit a puzzling inconsistency: they solve complex problems yet frequently fail on seemingly simpler ones. We investigate whether LLMs internally encode problem difficulty in a way that aligns with human judgment, and whether this representation tracks generalization during reinforcement learning post-training. We train linear probes across layers and token positions on 60 models, evaluating on mathematical and coding subsets of Easy2HardBench. We find that human-labeled difficulty is strongly linearly decodable (AMC: $\rho \approx 0.88$) and exhibits clear model-size scaling, whereas LLM-derived difficulty is substantially weaker and scales poorly. Steering along the difficulty direction reveals that pushing models toward "easier" representations reduces hallucination and improves accuracy. During GRPO training on Qwen2.5-Math-1.5B, the human-difficulty probe strengthens and positively correlates with test accuracy across training steps, while the LLM-difficulty probe degrades and negatively correlates with performance. These results suggest that human annotations provide a stable difficulty signal that RL amplifies, while automated difficulty estimates derived from model performance become misaligned precisely as models improve. We release probe code and evaluation scripts to facilitate replication.</li>
</ul>

<h3>Title: Extracting Rule-based Descriptions of Attention Features in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Dan Friedman, Adithya Bhaskar, Alexander Wettig, Danqi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18148">https://arxiv.org/abs/2510.18148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18148">https://arxiv.org/pdf/2510.18148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18148]] Extracting Rule-based Descriptions of Attention Features in Transformers(https://arxiv.org/abs/2510.18148)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Mechanistic interpretability strives to explain model behavior in terms of bottom-up primitives. The leading paradigm is to express hidden states as a sparse linear combination of basis vectors, called features. However, this only identifies which text sequences (exemplars) activate which features; the actual interpretation of features requires subjective inspection of these exemplars. This paper advocates for a different solution: rule-based descriptions that match token patterns in the input and correspondingly increase or decrease the likelihood of specific output tokens. Specifically, we extract rule-based descriptions of SAE features trained on the outputs of attention layers. While prior work treats the attention layers as an opaque box, we describe how it may naturally be expressed in terms of interactions between input and output features, of which we study three types: (1) skip-gram rules of the form "[Canadian city]... speaks --> English", (2) absence rules of the form "[Montreal]... speaks -/-> English," and (3) counting rules that toggle only when the count of a word exceeds a certain value or the count of another word. Absence and counting rules are not readily discovered by inspection of exemplars, where manual and automatic descriptions often identify misleading or incomplete explanations. We then describe a simple approach to extract these types of rules automatically from a transformer, and apply it to GPT-2 small. We find that a majority of features may be described well with around 100 skip-gram rules, though absence rules are abundant even as early as the first layer (in over a fourth of features). We also isolate a few examples of counting rules. This paper lays the groundwork for future research into rule-based descriptions of features by defining them, showing how they may be extracted, and providing a preliminary taxonomy of some of the behaviors they represent.</li>
</ul>

<h3>Title: Black-Box Evasion Attacks on Data-Driven Open RAN Apps: Tailored Design and Experimental Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Pranshav Gajjar, Molham Khoja, Abiodun Ganiyu, Marc Juarez, Mahesh K. Marina, Andrew Lehane, Vijay K. Shah</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18160">https://arxiv.org/abs/2510.18160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18160">https://arxiv.org/pdf/2510.18160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18160]] Black-Box Evasion Attacks on Data-Driven Open RAN Apps: Tailored Design and Experimental Evaluation(https://arxiv.org/abs/2510.18160)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>The impending adoption of Open Radio Access Network (O-RAN) is fueling innovation in the RAN towards data-driven operation. Unlike traditional RAN where the RAN data and its usage is restricted within proprietary and monolithic RAN equipment, the O-RAN architecture opens up access to RAN data via RAN intelligent controllers (RICs), to third-party machine learning (ML) powered applications - rApps and xApps - to optimize RAN operations. Consequently, a major focus has been placed on leveraging RAN data to unlock greater efficiency gains. However, there is an increasing recognition that RAN data access to apps could become a source of vulnerability and be exploited by malicious actors. Motivated by this, we carry out a comprehensive investigation of data vulnerabilities on both xApps and rApps, respectively hosted in Near- and Non-real-time (RT) RIC components of O-RAN. We qualitatively analyse the O-RAN security mechanisms and limitations for xApps and rApps, and consider a threat model informed by this analysis. We design a viable and effective black-box evasion attack strategy targeting O-RAN RIC Apps while accounting for the stringent timing constraints and attack effectiveness. The strategy employs four key techniques: the model cloning algorithm, input-specific perturbations, universal adversarial perturbations (UAPs), and targeted UAPs. This strategy targets ML models used by both xApps and rApps within the O-RAN system, aiming to degrade network performance. We validate the effectiveness of the designed evasion attack strategy and quantify the scale of performance degradation using a real-world O-RAN testbed and emulation environments. Evaluation is conducted using the Interference Classification xApp and the Power Saving rApp as representatives for near-RT and non-RT RICs. We also show that the attack strategy is effective against prominent defense techniques for adversarial ML.</li>
</ul>

<h3>Title: Automatic Prompt Generation via Adaptive Selection of Prompting Techniques</h3>
<ul>
<li><strong>Authors: </strong>Yohei Ikenoue, Hitomi Tashiro, Shigeru Kuroyanagi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18162">https://arxiv.org/abs/2510.18162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18162">https://arxiv.org/pdf/2510.18162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18162]] Automatic Prompt Generation via Adaptive Selection of Prompting Techniques(https://arxiv.org/abs/2510.18162)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompt engineering is crucial for achieving reliable and effective outputs from large language models (LLMs), but its design requires specialized knowledge of prompting techniques and a deep understanding of target tasks. To address this challenge, we propose a novel method that adaptively selects task-appropriate prompting techniques based on users' abstract task descriptions and automatically generates high-quality prompts without relying on pre-existing templates or frameworks. The proposed method constructs a knowledge base that associates task clusters, characterized by semantic similarity across diverse tasks, with their corresponding prompting techniques. When users input task descriptions, the system assigns them to the most relevant task cluster and dynamically generates prompts by integrating techniques drawn from the knowledge base. An experimental evaluation of the proposed method on 23 tasks from BIG-Bench Extra Hard (BBEH) demonstrates superior performance compared with standard prompts and existing automatic prompt-generation tools, as measured by both arithmetic and harmonic mean scores. This research establishes a foundation for streamlining and standardizing prompt creation, enabling non-experts to effectively leverage LLMs.</li>
</ul>

<h3>Title: Adapting Stereo Vision From Objects To 3D Lunar Surface Reconstruction with the StereoLunar Dataset</h3>
<ul>
<li><strong>Authors: </strong>Clementine Grethen, Simone Gasparini, Geraldine Morin, Jeremy Lebreton, Lucas Marti, Manuel Sanchez-Gestido</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18172">https://arxiv.org/abs/2510.18172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18172">https://arxiv.org/pdf/2510.18172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18172]] Adapting Stereo Vision From Objects To 3D Lunar Surface Reconstruction with the StereoLunar Dataset(https://arxiv.org/abs/2510.18172)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate 3D reconstruction of lunar surfaces is essential for space exploration. However, existing stereo vision reconstruction methods struggle in this context due to the Moon's lack of texture, difficult lighting variations, and atypical orbital trajectories. State-of-the-art deep learning models, trained on human-scale datasets, have rarely been tested on planetary imagery and cannot be transferred directly to lunar conditions. To address this issue, we introduce LunarStereo, the first open dataset of photorealistic stereo image pairs of the Moon, simulated using ray tracing based on high-resolution topography and reflectance models. It covers diverse altitudes, lighting conditions, and viewing angles around the lunar South Pole, offering physically grounded supervision for 3D reconstruction tasks. Based on this dataset, we adapt the MASt3R model to the lunar domain through fine-tuning on LunarStereo. We validate our approach through extensive qualitative and quantitative experiments on both synthetic and real lunar data, evaluating 3D surface reconstruction and relative pose estimation. Extensive experiments on synthetic and real lunar data validate the approach, demonstrating significant improvements over zero-shot baselines and paving the way for robust cross-scale generalization in extraterrestrial environments.</li>
</ul>

<h3>Title: CMT-Bench: Cricket Multi-Table Generation Benchmark for Probing Robustness in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ritam Upadhyay, Naman Ahuja, Rishabh Baral, Aparna Garimella, Vivek Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18173">https://arxiv.org/abs/2510.18173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18173">https://arxiv.org/pdf/2510.18173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18173]] CMT-Bench: Cricket Multi-Table Generation Benchmark for Probing Robustness in Large Language Models(https://arxiv.org/abs/2510.18173)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>LLM Driven text-to-table (T2T) systems often rely on extensive prompt-engineering or iterative event extraction in code-parsable formats, which boosts scores but are computationally expensive and obscure how models actually reason over temporal evolving narratives to summarise key information. We present CMT-Bench, a diagnostic benchmark built from live cricket commentary that requires dynamic table generation across two evolving schemas under a dense, rule-governed policy. CMT-Bench is designed to probe robustness via three semantics-preserving dimensions: (i) extractive-cue ablation to separate extractive shortcuts from state tracking, (ii) temporal prefixing to test long-context stability, and (iii) entity-form perturbations (anonymization, outof-distribution substitutions, role-entangling paraphrases) to assess sensitivity to surface variation. Across diverse long-context stateof-the-art LLMs, we find large drops without extractive summaries, monotonic degradation with input length, and consistent accuracy drop under entity-form changes. Complementary distributional tests confirm significant shifts in numeric error patterns, indicating drift in reasoning rather than mere noise. Our results show that current LLMs are brittle in dynamic Textto-table generation, motivating robustness-first evaluation as a prerequisite for developing efficient and scalable approaches for this task.</li>
</ul>

<h3>Title: Nash Policy Gradient: A Policy Gradient Method with Iteratively Refined Regularization for Finding Nash Equilibria</h3>
<ul>
<li><strong>Authors: </strong>Eason Yu, Tzu Hao Liu, Yunke Wang, Clément L. Canonne, Nguyen H. Tran, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18183">https://arxiv.org/abs/2510.18183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18183">https://arxiv.org/pdf/2510.18183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18183]] Nash Policy Gradient: A Policy Gradient Method with Iteratively Refined Regularization for Finding Nash Equilibria(https://arxiv.org/abs/2510.18183)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Finding Nash equilibria in imperfect-information games remains a central challenge in multi-agent reinforcement learning. While regularization-based methods have recently achieved last-iteration convergence to a regularized equilibrium, they require the regularization strength to shrink toward zero to approximate a Nash equilibrium, often leading to unstable learning in practice. Instead, we fix the regularization strength at a large value for robustness and achieve convergence by iteratively refining the reference policy. Our main theoretical result shows that this procedure guarantees strictly monotonic improvement and convergence to an exact Nash equilibrium in two-player zero-sum games, without requiring a uniqueness assumption. Building on this framework, we develop a practical algorithm, Nash Policy Gradient (NashPG), which preserves the generalizability of policy gradient methods while relying solely on the current and reference policies. Empirically, NashPG achieves comparable or lower exploitability than prior model-free methods on classic benchmark games and scales to large domains such as Battleship and No-Limit Texas Hold'em, where NashPG consistently attains higher Elo ratings.</li>
</ul>

<h3>Title: ActivationReasoning: Logical Reasoning in Latent Activation Spaces</h3>
<ul>
<li><strong>Authors: </strong>Lukas Helff, Ruben Härle, Wolfgang Stammer, Felix Friedrich, Manuel Brack, Antonia Wüst, Hikaru Shindo, Patrick Schramowski, Kristian Kersting</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18184">https://arxiv.org/abs/2510.18184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18184">https://arxiv.org/pdf/2510.18184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18184]] ActivationReasoning: Logical Reasoning in Latent Activation Spaces(https://arxiv.org/abs/2510.18184)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at generating fluent text, but their internal reasoning remains opaque and difficult to control. Sparse autoencoders (SAEs) make hidden activations more interpretable by exposing latent features that often align with human concepts. Yet, these features are fragile and passive, offering no mechanism for systematic reasoning or model control. To address this, we introduce ActivationReasoning (AR), a framework that embeds explicit logical reasoning into the latent space of LLMs. It proceeds in three stages: (1) Finding latent representations, first latent concept representations are identified (e.g., via SAEs) and organized into a dictionary; (2) Activating propositions, at inference time AR detects activating concepts and maps them to logical propositions; and (3)Logical reasoning, applying logical rules over these propositions to infer higher-order structures, compose new concepts, and steer model behavior. We evaluate AR on multi-hop reasoning (PrOntoQA), abstraction and robustness to indirect concept cues (Rail2Country), reasoning over natural and diverse language (ProverQA), and context-sensitive safety (BeaverTails). Across all tasks, AR scales robustly with reasoning complexity, generalizes to abstract and context-sensitive tasks, and transfers across model backbones. These results demonstrate that grounding logical structure in latent activations not only improves transparency but also enables structured reasoning, reliable control, and alignment with desired behaviors, providing a path toward more reliable and auditable AI.</li>
</ul>

<h3>Title: RadDiagSeg-M: A Vision Language Model for Joint Diagnosis and Multi-Target Segmentation in Radiology</h3>
<ul>
<li><strong>Authors: </strong>Chengrun Li, Corentin Royer, Haozhe Luo, Bastian Wittmann, Xia Li, Ibrahim Hamamci, Sezgin Er, Anjany Sekuboyina, Bjoern Menze</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18188">https://arxiv.org/abs/2510.18188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18188">https://arxiv.org/pdf/2510.18188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18188]] RadDiagSeg-M: A Vision Language Model for Joint Diagnosis and Multi-Target Segmentation in Radiology(https://arxiv.org/abs/2510.18188)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Most current medical vision language models struggle to jointly generate diagnostic text and pixel-level segmentation masks in response to complex visual questions. This represents a major limitation towards clinical application, as assistive systems that fail to provide both modalities simultaneously offer limited value to medical practitioners. To alleviate this limitation, we first introduce RadDiagSeg-D, a dataset combining abnormality detection, diagnosis, and multi-target segmentation into a unified and hierarchical task. RadDiagSeg-D covers multiple imaging modalities and is precisely designed to support the development of models that produce descriptive text and corresponding segmentation masks in tandem. Subsequently, we leverage the dataset to propose a novel vision-language model, RadDiagSeg-M, capable of joint abnormality detection, diagnosis, and flexible segmentation. RadDiagSeg-M provides highly informative and clinically useful outputs, effectively addressing the need to enrich contextual information for assistive diagnosis. Finally, we benchmark RadDiagSeg-M and showcase its strong performance across all components involved in the task of multi-target text-and-mask generation, establishing a robust and competitive baseline.</li>
</ul>

<h3>Title: TaintSentinel: Path-Level Randomness Vulnerability Detection for Ethereum Smart Contracts</h3>
<ul>
<li><strong>Authors: </strong>Hadis Rezaei, Ahmed Afif Monrat, Karl Andersson, Francesco Flammini</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18192">https://arxiv.org/abs/2510.18192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18192">https://arxiv.org/pdf/2510.18192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18192]] TaintSentinel: Path-Level Randomness Vulnerability Detection for Ethereum Smart Contracts(https://arxiv.org/abs/2510.18192)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>The inherent determinism of blockchain technology poses a significant challenge to generating secure random numbers within smart contracts, leading to exploitable vulnerabilities, particularly in decentralized finance (DeFi) ecosystems and blockchain-based gaming applications. From our observations, the current state-of-the-art detection tools suffer from inadequate precision while dealing with random number vulnerabilities. To address this problem, we propose TaintSentinel, a novel path sensitive vulnerability detection system designed to analyze smart contracts at the execution path level and gradually analyze taint with domain-specific rules. This paper discusses a solution that incorporates a multi-faceted approach, integrating rule-based taint analysis to track data flow, a dual stream neural network to identify complex vulnerability signatures, and evidence-based parameter initialization to minimize false positives. The system's two-phase operation involves semantic graph construction and taint propagation analysis, followed by pattern recognition using PathGNN and global structural analysis via GlobalGCN. Our experiments on 4,844 contracts demonstrate the superior performance of TaintSentinel relative to existing tools, yielding an F1-score of 0.892, an AUC-ROC of 0.94, and a PRA accuracy of 97%.</li>
</ul>

<h3>Title: Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge</h3>
<ul>
<li><strong>Authors: </strong>Yoshinari Fujinuma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18196">https://arxiv.org/abs/2510.18196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18196">https://arxiv.org/pdf/2510.18196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18196]] Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge(https://arxiv.org/abs/2510.18196)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are commonly used as evaluators in various applications, but the reliability of the outcomes remains a challenge. One such challenge is using LLMs-as-judges for direct assessment, i.e., assigning scores from a specified range without any references. We first show that this challenge stems from LLM judge outputs being associated with score range bias, i.e., LLM judge outputs are highly sensitive to pre-defined score ranges, preventing the search for optimal score ranges. We also show that similar biases exist among models from the same family. We then mitigate this bias through contrastive decoding, achieving up to 11.3% relative improvement on average in Spearman correlation with human judgments across different score ranges.</li>
</ul>

<h3>Title: RESCUE: Retrieval Augmented Secure Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Shi, Tianyi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18204">https://arxiv.org/abs/2510.18204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18204">https://arxiv.org/pdf/2510.18204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18204]] RESCUE: Retrieval Augmented Secure Code Generation(https://arxiv.org/abs/2510.18204)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, large language model</a></li>
<li><strong>Abstract: </strong>Despite recent advances, Large Language Models (LLMs) still generate vulnerable code. Retrieval-Augmented Generation (RAG) has the potential to enhance LLMs for secure code generation by incorporating external security knowledge. However, the conventional RAG design struggles with the noise of raw security-related documents, and existing retrieval methods overlook the significant security semantics implicitly embedded in task descriptions. To address these issues, we propose RESCUE, a new RAG framework for secure code generation with two key innovations. First, we propose a hybrid knowledge base construction method that combines LLM-assisted cluster-then-summarize distillation with program slicing, producing both high-level security guidelines and concise, security-focused code examples. Second, we design a hierarchical multi-faceted retrieval to traverse the constructed knowledge base from top to bottom and integrates multiple security-critical facts at each hierarchical level, ensuring comprehensive and accurate retrieval. We evaluated RESCUE on four benchmarks and compared it with five state-of-the-art secure code generation methods on six LLMs. The results demonstrate that RESCUE improves the SecurePass@1 metric by an average of 4.8 points, establishing a new state-of-the-art performance for security. Furthermore, we performed in-depth analysis and ablation studies to rigorously validate the effectiveness of individual components in RESCUE.</li>
</ul>

<h3>Title: EMA-SAM: Exponential Moving-average for SAM-based PTMC Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Maryam Dialameh, Hossein Rajabzadeh, Jung Suk Sim, Hyock Ju Kwon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18213">https://arxiv.org/abs/2510.18213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18213">https://arxiv.org/pdf/2510.18213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18213]] EMA-SAM: Exponential Moving-average for SAM-based PTMC Segmentation(https://arxiv.org/abs/2510.18213)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Papillary thyroid microcarcinoma (PTMC) is increasingly managed with radio-frequency ablation (RFA), yet accurate lesion segmentation in ultrasound videos remains difficult due to low contrast, probe-induced motion, and heat-related artifacts. The recent Segment Anything Model 2 (SAM-2) generalizes well to static images, but its frame-independent design yields unstable predictions and temporal drift in interventional ultrasound. We introduce \textbf{EMA-SAM}, a lightweight extension of SAM-2 that incorporates a confidence-weighted exponential moving average pointer into the memory bank, providing a stable latent prototype of the tumour across frames. This design preserves temporal coherence through probe pressure and bubble occlusion while rapidly adapting once clear evidence reappears. On our curated PTMC-RFA dataset (124 minutes, 13 patients), EMA-SAM improves \emph{maxDice} from 0.82 (SAM-2) to 0.86 and \emph{maxIoU} from 0.72 to 0.76, while reducing false positives by 29\%. On external benchmarks, including VTUS and colonoscopy video polyp datasets, EMA-SAM achieves consistent gains of 2--5 Dice points over SAM-2. Importantly, the EMA pointer adds \textless0.1\% FLOPs, preserving real-time throughput of $\sim$30\,FPS on a single A100 GPU. These results establish EMA-SAM as a robust and efficient framework for stable tumour tracking, bridging the gap between foundation models and the stringent demands of interventional ultrasound. Codes are available here \hyperref[code {this https URL}.</li>
</ul>

<h3>Title: VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</h3>
<ul>
<li><strong>Authors: </strong>Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18214">https://arxiv.org/abs/2510.18214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18214">https://arxiv.org/pdf/2510.18214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18214]] VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety(https://arxiv.org/abs/2510.18214)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Safety evaluation of multimodal foundation models often treats vision and language inputs separately, missing risks from joint interpretation where benign content becomes harmful in combination. Existing approaches also fail to distinguish clearly unsafe content from borderline cases, leading to problematic over-blocking or under-refusal of genuinely harmful content. We present Vision Language Safety Understanding (VLSU), a comprehensive framework to systematically evaluate multimodal safety through fine-grained severity classification and combinatorial analysis across 17 distinct safety patterns. Using a multi-stage pipeline with real-world images and human annotation, we construct a large-scale benchmark of 8,187 samples spanning 15 harm categories. Our evaluation of eleven state-of-the-art models reveals systematic joint understanding failures: while models achieve 90%-plus accuracy on clear unimodal safety signals, performance degrades substantially to 20-55% when joint image-text reasoning is required to determine the safety label. Most critically, 34% of errors in joint image-text safety classification occur despite correct classification of the individual modalities, further demonstrating absent compositional reasoning capabilities. Additionally, we find that models struggle to balance refusing unsafe content while still responding to borderline cases that deserve engagement. For example, we find that instruction framing can reduce the over-blocking rate on borderline content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of under-refusing on unsafe content with refusal rate dropping from 90.8% to 53.9%. Overall, our framework exposes weaknesses in joint image-text understanding and alignment gaps in current models, and provides a critical test bed to enable the next milestones in research on robust vision-language safety.</li>
</ul>

<h3>Title: Joint Optimization of Cooperation Efficiency and Communication Covertness for Target Detection with AUVs</h3>
<ul>
<li><strong>Authors: </strong>Xueyao Zhang, Bo Yang, Zhiwen Yu, Xuelin Cao, Wei Xiang, Bin Guo, Liang Wang, Billy Pik Lik Lau, George C. Alexandropoulos, Jun Luo, Mérouane Debbah, Zhu Han, Chau Yuen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18225">https://arxiv.org/abs/2510.18225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18225">https://arxiv.org/pdf/2510.18225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18225]] Joint Optimization of Cooperation Efficiency and Communication Covertness for Target Detection with AUVs(https://arxiv.org/abs/2510.18225)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>This paper investigates underwater cooperative target detection using autonomous underwater vehicles (AUVs), with a focus on the critical trade-off between cooperation efficiency and communication covertness. To tackle this challenge, we first formulate a joint trajectory and power control optimization problem, and then present an innovative hierarchical action management framework to solve it. According to the hierarchical formulation, at the macro level, the master AUV models the agent selection process as a Markov decision process and deploys the proximal policy optimization algorithm for strategic task allocation. At the micro level, each selected agent's decentralized decision-making is modeled as a partially observable Markov decision process, and a multi-agent proximal policy optimization algorithm is used to dynamically adjust its trajectory and transmission power based on its local observations. Under the centralized training and decentralized execution paradigm, our target detection framework enables adaptive covert cooperation while satisfying both energy and mobility constraints. By comprehensively modeling the considered system, the involved signals and tasks, as well as energy consumption, theoretical insights and practical solutions for the efficient and secure operation of multiple AUVs are provided, offering significant implications for the execution of underwater covert communication tasks.</li>
</ul>

<h3>Title: Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with Projected Gradient-Aligned Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Zhendong Mi, Qitao Tan, Grace Li Zhang, Zhaozhuo Xu, Geng Yuan, Shaoyi Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18228">https://arxiv.org/abs/2510.18228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18228">https://arxiv.org/pdf/2510.18228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18228]] Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with Projected Gradient-Aligned Perturbations(https://arxiv.org/abs/2510.18228)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) using zeroth-order (ZO) optimization has emerged as a promising alternative to traditional gradient-based methods due to its reduced memory footprint requirement. However, existing ZO methods suffer from high variance in gradient estimation, leading to slow convergence and suboptimal performance on large-scale models. In this work, we propose P-GAP, a fast LLM fine-tuning approach through zeroth-order optimization with Projected Gradient-Aligned Perturbations. Specifically, we first estimate a low-dimensional gradient space and then align perturbations in projected gradients' direction within the space. This approach enables reduced the number of perturbed parameters and decreased variance, therefore accelerated convergence for LLM fine-tuning. Experiments on LLMs show that P-GAP consistently surpasses the baselines, achieving up to 6% increase in accuracy on classification tasks and up to 12% higher accuracy on generation tasks, with up to about 81% less training iterations and 70% less GPU hours. These results demonstrate that P-GAP enables fast, scalable, and resource-efficient ZO LLM fine-tuning.</li>
</ul>

<h3>Title: Beyond Frequency: Scoring-Driven Debiasing for Object Detection via Blueprint-Prompted Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Xinhao Cai, Liulei Li, Gensheng Pei, Tao Chen, Jinshan Pan, Yazhou Yao, Wenguan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18229">https://arxiv.org/abs/2510.18229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18229">https://arxiv.org/pdf/2510.18229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18229]] Beyond Frequency: Scoring-Driven Debiasing for Object Detection via Blueprint-Prompted Image Synthesis(https://arxiv.org/abs/2510.18229)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents a generation-based debiasing framework for object detection. Prior debiasing methods are often limited by the representation diversity of samples, while naive generative augmentation often preserves the biases it aims to solve. Moreover, our analysis reveals that simply generating more data for rare classes is suboptimal due to two core issues: i) instance frequency is an incomplete proxy for the true data needs of a model, and ii) current layout-to-image synthesis lacks the fidelity and control to generate high-quality, complex scenes. To overcome this, we introduce the representation score (RS) to diagnose representational gaps beyond mere frequency, guiding the creation of new, unbiased layouts. To ensure high-quality synthesis, we replace ambiguous text prompts with a precise visual blueprint and employ a generative alignment strategy, which fosters communication between the detector and generator. Our method significantly narrows the performance gap for underrepresented object groups, \eg, improving large/rare instances by 4.4/3.6 mAP over the baseline, and surpassing prior L2I synthesis models by 15.9 mAP for layout accuracy in generated images.</li>
</ul>

<h3>Title: ACTG-ARL: Differentially Private Conditional Text Generation with RL-Boosted Control</h3>
<ul>
<li><strong>Authors: </strong>Yuzheng Hu, Ryan McKenna, Da Yu, Shanshan Wu, Han Zhao, Zheng Xu, Peter Kairouz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18232">https://arxiv.org/abs/2510.18232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18232">https://arxiv.org/pdf/2510.18232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18232]] ACTG-ARL: Differentially Private Conditional Text Generation with RL-Boosted Control(https://arxiv.org/abs/2510.18232)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Generating high-quality synthetic text under differential privacy (DP) is critical for training and evaluating language models without compromising user privacy. Prior work on synthesizing DP datasets often fail to preserve key statistical attributes, suffer utility loss from the noise required by DP, and lack fine-grained control over generation. To address these challenges, we make two contributions. First, we introduce a hierarchical framework that decomposes DP synthetic text generation into two subtasks: feature learning and conditional text generation. This design explicitly incorporates learned features into the generation process and simplifies the end-to-end synthesis task. Through systematic ablations, we identify the most effective configuration: a rich tabular schema as feature, a DP tabular synthesizer, and a DP fine-tuned conditional generator, which we term ACTG (Attribute-Conditioned Text Generation). Second, we propose Anchored RL (ARL), a post-training method that improves the instruction-following ability of ACTG for conditional generation. ARL combines RL to boost control with an SFT anchor on best-of-$N$ data to prevent reward hacking. Together, these components form our end-to-end algorithm ACTG-ARL, which advances both the quality of DP synthetic text (+20% MAUVE over prior work) and the control of the conditional generator under strong privacy guarantees.</li>
</ul>

<h3>Title: Learning with Dual-level Noisy Correspondence for Multi-modal Entity Alignment</h3>
<ul>
<li><strong>Authors: </strong>Haobin Li, Yijie Lin, Peng Hu, Mouxing Yang, Xi Peng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18240">https://arxiv.org/abs/2510.18240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18240">https://arxiv.org/pdf/2510.18240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18240]] Learning with Dual-level Noisy Correspondence for Multi-modal Entity Alignment(https://arxiv.org/abs/2510.18240)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-modal entity alignment (MMEA) aims to identify equivalent entities across heterogeneous multi-modal knowledge graphs (MMKGs), where each entity is described by attributes from various modalities. Existing methods typically assume that both intra-entity and inter-graph correspondences are faultless, which is often violated in real-world MMKGs due to the reliance on expert annotations. In this paper, we reveal and study a highly practical yet under-explored problem in MMEA, termed Dual-level Noisy Correspondence (DNC). DNC refers to misalignments in both intra-entity (entity-attribute) and inter-graph (entity-entity and attribute-attribute) correspondences. To address the DNC problem, we propose a robust MMEA framework termed RULE. RULE first estimates the reliability of both intra-entity and inter-graph correspondences via a dedicated two-fold principle. Leveraging the estimated reliabilities, RULE mitigates the negative impact of intra-entity noise during attribute fusion and prevents overfitting to noisy inter-graph correspondences during inter-graph discrepancy elimination. Beyond the training-time designs, RULE further incorporates a correspondence reasoning module that uncovers the underlying attribute-attribute connection across graphs, guaranteeing more accurate equivalent entity identification. Extensive experiments on five benchmarks verify the effectiveness of our method against the DNC compared with seven state-of-the-art this http URL code is available at \href{this https URL}{XLearning-SCU/RULE}</li>
</ul>

<h3>Title: BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Ajinkya Khoche, Gergő László Nagy, Maciej Wozniak, Thomas Gustafsson, Patric Jensfelt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18244">https://arxiv.org/abs/2510.18244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18244">https://arxiv.org/pdf/2510.18244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18244]] BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining(https://arxiv.org/abs/2510.18244)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Zero-shot 3D object classification is crucial for real-world applications like autonomous driving, however it is often hindered by a significant domain gap between the synthetic data used for training and the sparse, noisy LiDAR scans encountered in the real-world. Current methods trained solely on synthetic data fail to generalize to outdoor scenes, while those trained only on real data lack the semantic diversity to recognize rare or unseen objects. We introduce BlendCLIP, a multimodal pretraining framework that bridges this synthetic-to-real gap by strategically combining the strengths of both domains. We first propose a pipeline to generate a large-scale dataset of object-level triplets -- consisting of a point cloud, image, and text description -- mined directly from real-world driving data and human annotated 3D boxes. Our core contribution is a curriculum-based data mixing strategy that first grounds the model in the semantically rich synthetic CAD data before progressively adapting it to the specific characteristics of real-world scans. Our experiments show that our approach is highly label-efficient: introducing as few as 1.5\% real-world samples per batch into training boosts zero-shot accuracy on the nuScenes benchmark by 27\%. Consequently, our final model achieves state-of-the-art performance on challenging outdoor datasets like nuScenes and TruckScenes, improving over the best prior method by 19.3\% on nuScenes, while maintaining strong generalization on diverse synthetic benchmarks. Our findings demonstrate that effective domain adaptation, not full-scale real-world annotation, is the key to unlocking robust open-vocabulary 3D perception. Our code and dataset will be released upon acceptance on this https URL.</li>
</ul>

<h3>Title: Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs</h3>
<ul>
<li><strong>Authors: </strong>Song Bian, Tao Yu, Shivaram Venkataraman, Youngsuk Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18245">https://arxiv.org/abs/2510.18245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18245">https://arxiv.org/pdf/2510.18245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18245]] Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs(https://arxiv.org/abs/2510.18245)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scaling the number of parameters and the size of training data has proven to be an effective strategy for improving large language model (LLM) performance. Yet, as these models grow increasingly powerful and widely deployed, the cost of inference has become a pressing concern. Despite its importance, the trade-off between model accuracy and inference efficiency remains underexplored. In this work, we examine how key architectural factors, hidden size, the allocation of parameters between MLP and attention (mlp-to-attention ratio), and grouped-query attention (GQA), influence both inference cost and accuracy. We introduce a conditional scaling law that augments the Chinchilla framework with architectural information, along with a search framework for identifying architectures that are simultaneously inference-efficient and accurate. To validate our approach, we train more than 200 models spanning 80M to 3B parameters and 8B to 100B training tokens, and fit the proposed conditional scaling law. Our results show that the conditional scaling law reliably predicts optimal architectural choices and that the resulting models outperform existing open-source baselines. Under the same training budget, optimized architectures achieve up to 2.1% higher accuracy and 42% greater inference throughput compared to LLaMA-3.2.</li>
</ul>

<h3>Title: OpenInsGaussian: Open-vocabulary Instance Gaussian Segmentation with Context-aware Cross-view Fusion</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Huang, Runnan Chen, Dongting Hu, Fengming Huang, Mingming Gong, Tongliang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18253">https://arxiv.org/abs/2510.18253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18253">https://arxiv.org/pdf/2510.18253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18253]] OpenInsGaussian: Open-vocabulary Instance Gaussian Segmentation with Context-aware Cross-view Fusion(https://arxiv.org/abs/2510.18253)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Understanding 3D scenes is pivotal for autonomous driving, robotics, and augmented reality. Recent semantic Gaussian Splatting approaches leverage large-scale 2D vision models to project 2D semantic features onto 3D scenes. However, they suffer from two major limitations: (1) insufficient contextual cues for individual masks during preprocessing and (2) inconsistencies and missing details when fusing multi-view features from these 2D models. In this paper, we introduce \textbf{OpenInsGaussian}, an \textbf{Open}-vocabulary \textbf{Ins}tance \textbf{Gaussian} segmentation framework with Context-aware Cross-view Fusion. Our method consists of two modules: Context-Aware Feature Extraction, which augments each mask with rich semantic context, and Attention-Driven Feature Aggregation, which selectively fuses multi-view features to mitigate alignment errors and incompleteness. Through extensive experiments on benchmark datasets, OpenInsGaussian achieves state-of-the-art results in open-vocabulary 3D Gaussian segmentation, outperforming existing baselines by a large margin. These findings underscore the robustness and generality of our proposed approach, marking a significant step forward in 3D scene understanding and its practical deployment across diverse real-world scenarios.</li>
</ul>

<h3>Title: Hyperbolic Space Learning Method Leveraging Temporal Motion Priors for Human Mesh Recovery</h3>
<ul>
<li><strong>Authors: </strong>Xiang Zhang, Suping Wu, Weibin Qiu, Zhaocheng Jin, Sheng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18256">https://arxiv.org/abs/2510.18256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18256">https://arxiv.org/pdf/2510.18256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18256]] Hyperbolic Space Learning Method Leveraging Temporal Motion Priors for Human Mesh Recovery(https://arxiv.org/abs/2510.18256)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>3D human meshes show a natural hierarchical structure (like torso-limbs-fingers). But existing video-based 3D human mesh recovery methods usually learn mesh features in Euclidean space. It's hard to catch this hierarchical structure accurately. So wrong human meshes are reconstructed. To solve this problem, we propose a hyperbolic space learning method leveraging temporal motion prior for recovering 3D human meshes from videos. First, we design a temporal motion prior extraction module. This module extracts the temporal motion features from the input 3D pose sequences and image feature sequences respectively. Then it combines them into the temporal motion prior. In this way, it can strengthen the ability to express features in the temporal motion dimension. Since data representation in non-Euclidean space has been proved to effectively capture hierarchical relationships in real-world datasets (especially in hyperbolic space), we further design a hyperbolic space optimization learning strategy. This strategy uses the temporal motion prior information to assist learning, and uses 3D pose and pose motion information respectively in the hyperbolic space to optimize and learn the mesh features. Then, we combine the optimized results to get an accurate and smooth human mesh. Besides, to make the optimization learning process of human meshes in hyperbolic space stable and effective, we propose a hyperbolic mesh optimization loss. Extensive experimental results on large publicly available datasets indicate superiority in comparison with most state-of-the-art.</li>
</ul>

<h3>Title: DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Tao Tao, Guanghui Zhu, Lang Guo, Hongyi Chen, Chunfeng Yuan, Yihua Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18257">https://arxiv.org/abs/2510.18257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18257">https://arxiv.org/pdf/2510.18257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18257]] DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization(https://arxiv.org/abs/2510.18257)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompt Optimization has emerged as a crucial approach due to its capabilities in steering Large Language Models to solve various tasks. However, current works mainly rely on the random rewriting ability of LLMs, and the optimization process generally focus on specific influencing factors, which makes it easy to fall into local optimum. Besides, the performance of the optimized prompt is often unstable, which limits its transferability in different tasks. To address the above challenges, we propose $\textbf{DelvePO}$ ($\textbf{D}$irection-Guid$\textbf{e}$d Se$\textbf{l}$f-E$\textbf{v}$olving Framework for Fl$\textbf{e}$xible $\textbf{P}$rompt $\textbf{O}$ptimization), a task-agnostic framework to optimize prompts in self-evolve manner. In our framework, we decouple prompts into different components that can be used to explore the impact that different factors may have on various tasks. On this basis, we introduce working memory, through which LLMs can alleviate the deficiencies caused by their own uncertainties and further obtain key insights to guide the generation of new prompts. Extensive experiments conducted on different tasks covering various domains for both open- and closed-source LLMs, including DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct and GPT-4o-mini. Experimental results show that DelvePO consistently outperforms previous SOTA methods under identical experimental settings, demonstrating its effectiveness and transferability across different tasks.</li>
</ul>

<h3>Title: From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziwei Huang, Ying Shu, Hao Fang, Quanyu Long, Wenya Wang, Qiushi Guo, Tiezheng Ge, Leilei Gan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18263">https://arxiv.org/abs/2510.18263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18263">https://arxiv.org/pdf/2510.18263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18263]] From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation(https://arxiv.org/abs/2510.18263)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Subject-driven image generation models face a fundamental trade-off between identity preservation (fidelity) and prompt adherence (editability). While online reinforcement learning (RL), specifically GPRO, offers a promising solution, we find that a naive application of GRPO leads to competitive degradation, as the simple linear aggregation of rewards with static weights causes conflicting gradient signals and a misalignment with the temporal dynamics of the diffusion process. To overcome these limitations, we propose Customized-GRPO, a novel framework featuring two key innovations: (i) Synergy-Aware Reward Shaping (SARS), a non-linear mechanism that explicitly penalizes conflicted reward signals and amplifies synergistic ones, providing a sharper and more decisive gradient. (ii) Time-Aware Dynamic Weighting (TDW), which aligns the optimization pressure with the model's temporal dynamics by prioritizing prompt-following in the early, identity preservation in the later. Extensive experiments demonstrate that our method significantly outperforms naive GRPO baselines, successfully mitigating competitive degradation. Our model achieves a superior balance, generating images that both preserve key identity features and accurately adhere to complex textual prompts.</li>
</ul>

<h3>Title: TreeFedDG: Alleviating Global Drift in Federated Domain Generalization for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Song, Chenxi Li, Haokang Ding, Zhining Liao, Zhifang Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18268">https://arxiv.org/abs/2510.18268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18268">https://arxiv.org/pdf/2510.18268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18268]] TreeFedDG: Alleviating Global Drift in Federated Domain Generalization for Medical Image Segmentation(https://arxiv.org/abs/2510.18268)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, federate, segmentation</a></li>
<li><strong>Abstract: </strong>In medical image segmentation tasks, Domain Generalization (DG) under the Federated Learning (FL) framework is crucial for addressing challenges related to privacy protection and data heterogeneity. However, traditional federated learning methods fail to account for the imbalance in information aggregation across clients in cross-domain scenarios, leading to the Global Drift (GD) problem and a consequent decline in model generalization performance. This motivates us to delve deeper and define a new critical issue: global drift in federated domain generalization for medical imaging (FedDG-GD). In this paper, we propose a novel tree topology framework called TreeFedDG. First, starting from the distributed characteristics of medical images, we design a hierarchical parameter aggregation method based on a tree-structured topology to suppress deviations in the global model direction. Second, we introduce a parameter difference-based style mixing method (FedStyle), which enforces mixing among clients with maximum parameter differences to enhance robustness against drift. Third, we develop a a progressive personalized fusion strategy during model distribution, ensuring a balance between knowledge transfer and personalized features. Finally, during the inference phase, we use feature similarity to guide the retrieval of the most relevant model chain from the tree structure for ensemble decision-making, thereby fully leveraging the advantages of hierarchical knowledge. We conducted extensive experiments on two publicly available datasets. The results demonstrate that our method outperforms other state-of-the-art domain generalization approaches in these challenging tasks and achieves better balance in cross-domain performance.</li>
</ul>

<h3>Title: Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yanhong Li, Zixuan Lan, Jiawei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18279">https://arxiv.org/abs/2510.18279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18279">https://arxiv.org/pdf/2510.18279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18279]] Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs(https://arxiv.org/abs/2510.18279)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) and their multimodal variants can now process visual inputs, including images of text. This raises an intriguing question: can we compress textual inputs by feeding them as images to reduce token usage while preserving performance? In this paper, we show that visual text representations are a practical and surprisingly effective form of input compression for decoder LLMs. We exploit the idea of rendering long text inputs as a single image and provide it directly to the model. This leads to dramatically reduced number of decoder tokens required, offering a new form of input compression. Through experiments on two distinct benchmarks RULER (long-context retrieval) and CNN/DailyMail (document summarization) we demonstrate that this text-as-image method yields substantial token savings (often nearly half) without degrading task performance.</li>
</ul>

<h3>Title: Efficient Few-shot Identity Preserving Attribute Editing for 3D-aware Deep Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Vishal Vinod</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18287">https://arxiv.org/abs/2510.18287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18287">https://arxiv.org/pdf/2510.18287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18287]] Efficient Few-shot Identity Preserving Attribute Editing for 3D-aware Deep Generative Models(https://arxiv.org/abs/2510.18287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Identity preserving editing of faces is a generative task that enables modifying the illumination, adding/removing eyeglasses, face aging, editing hairstyles, modifying expression etc., while preserving the identity of the face. Recent progress in 2D generative models have enabled photorealistic editing of faces using simple techniques leveraging the compositionality in GANs. However, identity preserving editing for 3D faces with a given set of attributes is a challenging task as the generative model must reason about view consistency from multiple poses and render a realistic 3D face. Further, 3D portrait editing requires large-scale attribute labelled datasets and presents a trade-off between editability in low-resolution and inflexibility to editing in high resolution. In this work, we aim to alleviate some of the constraints in editing 3D faces by identifying latent space directions that correspond to photorealistic edits. To address this, we present a method that builds on recent advancements in 3D-aware deep generative models and 2D portrait editing techniques to perform efficient few-shot identity preserving attribute editing for 3D-aware generative models. We aim to show from experimental results that using just ten or fewer labelled images of an attribute is sufficient to estimate edit directions in the latent space that correspond to 3D-aware attribute editing. In this work, we leverage an existing face dataset with masks to obtain the synthetic images for few attribute examples required for estimating the edit directions. Further, to demonstrate the linearity of edits, we investigate one-shot stylization by performing sequential editing and use the (2D) Attribute Style Manipulation (ASM) technique to investigate a continuous style manifold for 3D consistent identity preserving face aging. Code and results are available at: this https URL</li>
</ul>

<h3>Title: BrailleLLM: Braille Instruction Tuning with Large Language Models for Braille Domain Tasks</h3>
<ul>
<li><strong>Authors: </strong>Tianyuan Huang, Zepeng Zhu, Hangdi Xing, Zirui Shao, Zhi Yu, Chaoxiong Yang, Jiaxian He, Xiaozhong Liu, Jiajun Bu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18288">https://arxiv.org/abs/2510.18288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18288">https://arxiv.org/pdf/2510.18288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18288]] BrailleLLM: Braille Instruction Tuning with Large Language Models for Braille Domain Tasks(https://arxiv.org/abs/2510.18288)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Braille plays a vital role in education and information accessibility for visually impaired individuals. However, Braille information processing faces challenges such as data scarcity and ambiguities in mixed-text contexts. We construct English and Chinese Braille Mixed Datasets (EBMD/CBMD) with mathematical formulas to support diverse Braille domain research, and propose a syntax tree-based augmentation method tailored for Braille data. To address the underperformance of traditional fine-tuning methods in Braille-related tasks, we investigate Braille Knowledge-Based Fine-Tuning (BKFT), which reduces the learning difficulty of Braille contextual features. BrailleLLM employs BKFT via instruction tuning to achieve unified Braille translation, formula-to-Braille conversion, and mixed-text translation. Experiments demonstrate that BKFT achieves significant performance improvements over conventional fine-tuning in Braille translation scenarios. Our open-sourced datasets and methodologies establish a foundation for low-resource multilingual Braille research.</li>
</ul>

<h3>Title: Food4All: A Multi-Agent Framework for Real-time Free Food Discovery with Integrated Nutritional Metadata</h3>
<ul>
<li><strong>Authors: </strong>Zhengqing Yuan, Yiyang Li, Weixiang Sun, Zheyuan Zhang, Kaiwen Shi, Keerthiram Murugesan, Yanfang Ye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18289">https://arxiv.org/abs/2510.18289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18289">https://arxiv.org/pdf/2510.18289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18289]] Food4All: A Multi-Agent Framework for Real-time Free Food Discovery with Integrated Nutritional Metadata(https://arxiv.org/abs/2510.18289)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Food insecurity remains a persistent public health emergency in the United States, tightly interwoven with chronic disease, mental illness, and opioid misuse. Yet despite the existence of thousands of food banks and pantries, access remains fragmented: 1) current retrieval systems depend on static directories or generic search engines, which provide incomplete and geographically irrelevant results; 2) LLM-based chatbots offer only vague nutritional suggestions and fail to adapt to real-world constraints such as time, mobility, and transportation; and 3) existing food recommendation systems optimize for culinary diversity but overlook survival-critical needs of food-insecure populations, including immediate proximity, verified availability, and contextual barriers. These limitations risk leaving the most vulnerable individuals, those experiencing homelessness, addiction, or digital illiteracy, unable to access urgently needed resources. To address this, we introduce Food4All, the first multi-agent framework explicitly designed for real-time, context-aware free food retrieval. Food4All unifies three innovations: 1) heterogeneous data aggregation across official databases, community platforms, and social media to provide a continuously updated pool of food resources; 2) a lightweight reinforcement learning algorithm trained on curated cases to optimize for both geographic accessibility and nutritional correctness; and 3) an online feedback loop that dynamically adapts retrieval policies to evolving user needs. By bridging information acquisition, semantic analysis, and decision support, Food4All delivers nutritionally annotated and guidance at the point of need. This framework establishes an urgent step toward scalable, equitable, and intelligent systems that directly support populations facing food insecurity and its compounding health risks.</li>
</ul>

<h3>Title: GeoDiff: Geometry-Guided Diffusion for Metric Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Tuan Pham, Thanh-Tung Le, Xiaohui Xie, Stephan Mandt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18291">https://arxiv.org/abs/2510.18291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18291">https://arxiv.org/pdf/2510.18291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18291]] GeoDiff: Geometry-Guided Diffusion for Metric Depth Estimation(https://arxiv.org/abs/2510.18291)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce a novel framework for metric depth estimation that enhances pretrained diffusion-based monocular depth estimation (DB-MDE) models with stereo vision guidance. While existing DB-MDE methods excel at predicting relative depth, estimating absolute metric depth remains challenging due to scale ambiguities in single-image scenarios. To address this, we reframe depth estimation as an inverse problem, leveraging pretrained latent diffusion models (LDMs) conditioned on RGB images, combined with stereo-based geometric constraints, to learn scale and shift for accurate depth recovery. Our training-free solution seamlessly integrates into existing DB-MDE frameworks and generalizes across indoor, outdoor, and complex environments. Extensive experiments demonstrate that our approach matches or surpasses state-of-the-art methods, particularly in challenging scenarios involving translucent and specular surfaces, all without requiring retraining.</li>
</ul>

<h3>Title: From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Lei Li, Xiao Zhou, Yingying Zhang, Xian Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18297">https://arxiv.org/abs/2510.18297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18297">https://arxiv.org/pdf/2510.18297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18297]] From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering(https://arxiv.org/abs/2510.18297)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Medical question answering (QA) requires extensive access to domain-specific knowledge. A promising direction is to enhance large language models (LLMs) with external knowledge retrieved from medical corpora or parametric knowledge stored in model parameters. Existing approaches typically fall into two categories: Retrieval-Augmented Generation (RAG), which grounds model reasoning on externally retrieved evidence, and Generation-Augmented Generation (GAG), which depends solely on the models internal knowledge to generate contextual documents. However, RAG often suffers from noisy or incomplete retrieval, while GAG is vulnerable to hallucinated or inaccurate information due to unconstrained generation. Both issues can mislead reasoning and undermine answer reliability. To address these challenges, we propose MedRGAG, a unified retrieval-generation augmented framework that seamlessly integrates external and parametric knowledge for medical QA. MedRGAG comprises two key modules: Knowledge-Guided Context Completion (KGCC), which directs the generator to produce background documents that complement the missing knowledge revealed by retrieval; and Knowledge-Aware Document Selection (KADS), which adaptively selects an optimal combination of retrieved and generated documents to form concise yet comprehensive evidence for answer generation. Extensive experiments on five medical QA benchmarks demonstrate that MedRGAG achieves a 12.5% improvement over MedRAG and a 4.5% gain over MedGENIE, highlighting the effectiveness of unifying retrieval and generation for knowledge-intensive reasoning. Our code and data are publicly available at this https URL</li>
</ul>

<h3>Title: Physics-Informed Parametric Bandits for Beam Alignment in mmWave Communications</h3>
<ul>
<li><strong>Authors: </strong>Hao Qin, Thang Duong, Ming Li, Chicheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18299">https://arxiv.org/abs/2510.18299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18299">https://arxiv.org/pdf/2510.18299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18299]] Physics-Informed Parametric Bandits for Beam Alignment in mmWave Communications(https://arxiv.org/abs/2510.18299)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In millimeter wave (mmWave) communications, beam alignment and tracking are crucial to combat the significant path loss. As scanning the entire directional space is inefficient, designing an efficient and robust method to identify the optimal beam directions is essential. Since traditional bandit algorithms require a long time horizon to converge under large beam spaces, many existing works propose efficient bandit algorithms for beam alignment by relying on unimodality or multimodality assumptions on the reward function's structure. However, such assumptions often do not hold (or cannot be strictly satisfied) in practice, which causes such algorithms to converge to choosing suboptimal beams. In this work, we propose two physics-informed bandit algorithms \textit{pretc} and \textit{prgreedy} that exploit the sparse multipath property of mmWave channels - a generic but realistic assumption - which is connected to the Phase Retrieval Bandit problem. Our algorithms treat the parameters of each path as black boxes and maintain optimal estimates of them based on sampled historical rewards. \textit{pretc} starts with a random exploration phase and then commits to the optimal beam under the estimated reward function. \textit{prgreedy} performs such estimation in an online manner and chooses the best beam under current estimates. Our algorithms can also be easily adapted to beam tracking in the mobile setting. Through experiments using both the synthetic DeepMIMO dataset and the real-world DeepSense6G dataset, we demonstrate that both algorithms outperform existing approaches in a wide range of scenarios across diverse channel environments, showing their generalizability and robustness.</li>
</ul>

<h3>Title: Proactive Reasoning-with-Retrieval Framework for Medical Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lehan Wang, Yi Qin, Honglong Yang, Xiaomeng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18303">https://arxiv.org/abs/2510.18303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18303">https://arxiv.org/pdf/2510.18303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18303]] Proactive Reasoning-with-Retrieval Framework for Medical Multimodal Large Language Models(https://arxiv.org/abs/2510.18303)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Incentivizing the reasoning ability of Multimodal Large Language Models (MLLMs) is essential for medical applications to transparently analyze medical scans and provide reliable diagnosis. However, existing medical MLLMs rely solely on internal knowledge during reasoning, leading to hallucinated reasoning and factual inaccuracies when encountering cases beyond their training scope. Although recent Agentic Retrieval-Augmented Generation (RAG) methods elicit the medical model's proactive retrieval ability during reasoning, they are confined to unimodal LLMs, neglecting the crucial visual information during reasoning and retrieval. Consequently, we propose the first Multimodal Medical Reasoning-with-Retrieval framework, Med-RwR, which actively retrieves external knowledge by querying observed symptoms or domain-specific medical concepts during reasoning. Specifically, we design a two-stage reinforcement learning strategy with tailored rewards that stimulate the model to leverage both visual diagnostic findings and textual clinical information for effective retrieval. Building on this foundation, we further propose a Confidence-Driven Image Re-retrieval (CDIR) method for test-time scaling when low prediction confidence is detected. Evaluation on various public medical benchmarks demonstrates Med-RwR's significant improvements over baseline models, proving the effectiveness of enhancing reasoning capabilities with external knowledge integration. Furthermore, Med-RwR demonstrates remarkable generalizability to unfamiliar domains, evidenced by 8.8% performance gain on our proposed EchoCardiography Benchmark (ECBench), despite the scarcity of echocardiography data in the training corpus. Our data, model, and codes will be made publicly available at this https URL.</li>
</ul>

<h3>Title: The Impact of Image Resolution on Biomedical Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Liangyu Chen, James Burgess, Jeffrey J Nirschl, Orr Zohar, Serena Yeung-Levy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18304">https://arxiv.org/abs/2510.18304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18304">https://arxiv.org/pdf/2510.18304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18304]] The Impact of Image Resolution on Biomedical Multimodal Large Language Models(https://arxiv.org/abs/2510.18304)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Imaging technologies are fundamental to biomedical research and modern medicine, requiring analysis of high-resolution images across various modalities. While multimodal large language models (MLLMs) show promise for biomedical image analysis, most are designed for low-resolution images from general-purpose datasets, risking critical information loss. We investigate how image resolution affects MLLM performance in biomedical applications and demonstrate that: (1) native-resolution training and inference significantly improve performance across multiple tasks, (2) misalignment between training and inference resolutions severely degrades performance, and (3) mixed-resolution training effectively mitigates misalignment and balances computational constraints with performance requirements. Based on these findings, we recommend prioritizing native-resolution inference and mixed-resolution datasets to optimize biomedical MLLMs for transformative impact in scientific research and clinical applications.</li>
</ul>

<h3>Title: Towards Identifiability of Hierarchical Temporal Causal Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Zijian Li, Minghao Fu, Junxian Huang, Yifan Shen, Ruichu Cai, Yuewen Sun, Guangyi Chen, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18310">https://arxiv.org/abs/2510.18310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18310">https://arxiv.org/pdf/2510.18310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18310]] Towards Identifiability of Hierarchical Temporal Causal Representation Learning(https://arxiv.org/abs/2510.18310)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modeling hierarchical latent dynamics behind time series data is critical for capturing temporal dependencies across multiple levels of abstraction in real-world tasks. However, existing temporal causal representation learning methods fail to capture such dynamics, as they fail to recover the joint distribution of hierarchical latent variables from \textit{single-timestep observed variables}. Interestingly, we find that the joint distribution of hierarchical latent variables can be uniquely determined using three conditionally independent observations. Building on this insight, we propose a Causally Hierarchical Latent Dynamic (CHiLD) identification framework. Our approach first employs temporal contextual observed variables to identify the joint distribution of multi-layer latent variables. Sequentially, we exploit the natural sparsity of the hierarchical structure among latent variables to identify latent variables within each layer. Guided by the theoretical results, we develop a time series generative model grounded in variational inference. This model incorporates a contextual encoder to reconstruct multi-layer latent variables and normalize flow-based hierarchical prior networks to impose the independent noise condition of hierarchical latent dynamics. Empirical evaluations on both synthetic and real-world datasets validate our theoretical claims and demonstrate the effectiveness of CHiLD in modeling hierarchical latent dynamics.</li>
</ul>

<h3>Title: Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task</h3>
<ul>
<li><strong>Authors: </strong>Brady Bhalla, Honglu Fan, Nancy Chen, Tony Yue YU</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18315">https://arxiv.org/abs/2510.18315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18315">https://arxiv.org/pdf/2510.18315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18315]] Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task(https://arxiv.org/abs/2510.18315)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>We investigate how embedding dimension affects the emergence of an internal "world model" in a transformer trained with reinforcement learning to perform bubble-sort-style adjacent swaps. Models achieve high accuracy even with very small embedding dimensions, but larger dimensions yield more faithful, consistent, and robust internal representations. In particular, higher embedding dimensions strengthen the formation of structured internal representation and lead to better interpretability. After hundreds of experiments, we observe two consistent mechanisms: (1) the last row of the attention weight matrix monotonically encodes the global ordering of tokens; and (2) the selected transposition aligns with the largest adjacent difference of these encoded values. Our results provide quantitative evidence that transformers build structured internal world models and that model size improves representation quality in addition to end performance. We release our metrics and analyses, which can be used to probe similar algorithmic tasks.</li>
</ul>

<h3>Title: Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding</h3>
<ul>
<li><strong>Authors: </strong>Jinlin Li, Yuran Wang, Yifei Yuan, Xiao Zhou, Yingying Zhang, Xixian Yong, Yefeng Zheng, Xian Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18321">https://arxiv.org/abs/2510.18321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18321">https://arxiv.org/pdf/2510.18321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18321]] Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding(https://arxiv.org/abs/2510.18321)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) have recently achieved impressive results in multimodal tasks such as image captioning and visual question answering. However, they remain prone to object hallucination -- generating descriptions of nonexistent or misidentified objects. Prior work has partially mitigated this via auxiliary training objectives or external modules, but challenges remain in terms of scalability, adaptability, and model independence. To address these limitations, we propose Adaptive Token Ensemble Decoding (ATED), a training-free, token-level ensemble framework that mitigates hallucination by aggregating predictions from multiple LVLMs during inference. ATED dynamically computes uncertainty-based weights for each model, reflecting their reliability at each decoding step. It also integrates diverse decoding paths to improve contextual grounding and semantic consistency. Experiments on standard hallucination detection benchmarks demonstrate that ATED significantly outperforms state-of-the-art methods, reducing hallucination without compromising fluency or relevance. Our findings highlight the benefits of adaptive ensembling and point to a promising direction for improving LVLM robustness in high-stakes applications. The code is available at this https URL.</li>
</ul>

<h3>Title: Uncertainty Estimation by Flexible Evidential Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Taeseong Yoon, Heeyoung Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18322">https://arxiv.org/abs/2510.18322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18322">https://arxiv.org/pdf/2510.18322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18322]] Uncertainty Estimation by Flexible Evidential Deep Learning(https://arxiv.org/abs/2510.18322)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Uncertainty quantification (UQ) is crucial for deploying machine learning models in high-stakes applications, where overconfident predictions can lead to serious consequences. An effective UQ method must balance computational efficiency with the ability to generalize across diverse scenarios. Evidential deep learning (EDL) achieves efficiency by modeling uncertainty through the prediction of a Dirichlet distribution over class probabilities. However, the restrictive assumption of Dirichlet-distributed class probabilities limits EDL's robustness, particularly in complex or unforeseen situations. To address this, we propose \textit{flexible evidential deep learning} ($\mathcal{F}$-EDL), which extends EDL by predicting a flexible Dirichlet distribution -- a generalization of the Dirichlet distribution -- over class probabilities. This approach provides a more expressive and adaptive representation of uncertainty, significantly enhancing UQ generalization and reliability under challenging scenarios. We theoretically establish several advantages of $\mathcal{F}$-EDL and empirically demonstrate its state-of-the-art UQ performance across diverse evaluation settings, including classical, long-tailed, and noisy in-distribution scenarios.</li>
</ul>

<h3>Title: CryptoGuard: Lightweight Hybrid Detection and Response to Host-based Cryptojackers in Linux Cloud Environments</h3>
<ul>
<li><strong>Authors: </strong>Gyeonghoon Park, Jaehan Kim, Jinu Choi, Jinwoo Kim</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18324">https://arxiv.org/abs/2510.18324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18324">https://arxiv.org/pdf/2510.18324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18324]] CryptoGuard: Lightweight Hybrid Detection and Response to Host-based Cryptojackers in Linux Cloud Environments(https://arxiv.org/abs/2510.18324)</code><input type="text"></li>
<li><strong>Keywords: </strong>steal</a></li>
<li><strong>Abstract: </strong>Host-based cryptomining malware, commonly known as cryptojackers, have gained notoriety for their stealth and the significant financial losses they cause in Linux-based cloud environments. Existing solutions often struggle with scalability due to high monitoring overhead, low detection accuracy against obfuscated behavior, and lack of integrated remediation. We present CryptoGuard, a lightweight hybrid solution that combines detection and remediation strategies to counter cryptojackers. To ensure scalability, CryptoGuard uses sketch- and sliding window-based syscall monitoring to collect behavior patterns with minimal overhead. It decomposes the classification task into a two-phase process, leveraging deep learning models to identify suspicious activity with high precision. To counter evasion techniques such as entry point poisoning and PID manipulation, CryptoGuard integrates targeted remediation mechanisms based on eBPF, a modern Linux kernel feature deployable on any compatible host. Evaluated on 123 real-world cryptojacker samples, it achieves average F1-scores of 96.12% and 92.26% across the two phases, and outperforms state-of-the-art baselines in terms of true and false positive rates, while incurring only 0.06% CPU overhead per host.</li>
</ul>

<h3>Title: Enhancing Few-Shot Classification of Benchmark and Disaster Imagery with ATTBHFA-Net</h3>
<ul>
<li><strong>Authors: </strong>Gao Yu Lee, Tanmoy Dam, Md Meftahul Ferdaus, Daniel Puiu Poenar, Vu Duong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18326">https://arxiv.org/abs/2510.18326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18326">https://arxiv.org/pdf/2510.18326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18326]] Enhancing Few-Shot Classification of Benchmark and Disaster Imagery with ATTBHFA-Net(https://arxiv.org/abs/2510.18326)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The increasing frequency of natural and human-induced disasters necessitates advanced visual recognition techniques capable of analyzing critical photographic data. With progress in artificial intelligence and resilient computational systems, rapid and accurate disaster classification has become crucial for efficient rescue operations. However, visual recognition in disaster contexts faces significant challenges due to limited and diverse data from the difficulties in collecting and curating comprehensive, high-quality disaster imagery. Few-Shot Learning (FSL) provides a promising approach to data scarcity, yet current FSL research mainly relies on generic benchmark datasets lacking remote-sensing disaster imagery, limiting its practical effectiveness. Moreover, disaster images exhibit high intra-class variation and inter-class similarity, hindering the performance of conventional metric-based FSL methods. To address these issues, this paper introduces the Attention-based Bhattacharyya-Hellinger Feature Aggregation Network (ATTBHFA-Net), which linearly combines the Bhattacharyya coefficient and Hellinger distances to compare and aggregate feature probability distributions for robust prototype formation. The Bhattacharyya coefficient serves as a contrastive margin that enhances inter-class separability, while the Hellinger distance regularizes same-class alignment. This framework parallels contrastive learning but operates over probability distributions rather than embedded feature points. Furthermore, a Bhattacharyya-Hellinger distance-based contrastive loss is proposed as a distributional counterpart to cosine similarity loss, used jointly with categorical cross-entropy to significantly improve FSL performance. Experiments on four FSL benchmarks and two disaster image datasets demonstrate the superior effectiveness and generalization of ATTBHFA-Net compared to existing approaches.</li>
</ul>

<h3>Title: Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Zhong Li, Qi Huang, Yuxuan Zhu, Lincen Yang, Mohammad Mohammadi Amiri, Niki van Stein, Matthijs van Leeuwen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18328">https://arxiv.org/abs/2510.18328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18328">https://arxiv.org/pdf/2510.18328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18328]] Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching(https://arxiv.org/abs/2510.18328)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability, diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce Time-Conditioned Contraction Matching (TCCM), a novel method for semi-supervised anomaly detection in tabular data. TCCM is inspired by flow matching, a recent generative modeling framework that learns velocity fields between probability distributions and has shown strong performance compared to diffusion models and generative adversarial networks. Instead of directly applying flow matching as originally formulated, TCCM builds on its core idea -- learning velocity fields between distributions -- but simplifies the framework by predicting a time-conditioned contraction vector toward a fixed target (the origin) at each sampled time step. This design offers three key advantages: (1) a lightweight and scalable training objective that removes the need for solving ordinary differential equations during training and inference; (2) an efficient scoring strategy called one time-step deviation, which quantifies deviation from expected contraction behavior in a single forward pass, addressing the inference bottleneck of existing continuous-time models such as DTE (a diffusion-based model with leading anomaly detection accuracy but heavy inference cost); and (3) explainability and provable robustness, as the learned velocity field operates directly in input space, making the anomaly score inherently feature-wise attributable; moreover, the score function is Lipschitz-continuous with respect to the input, providing theoretical guarantees under small perturbations. Extensive experiments on the ADBench benchmark show that TCCM strikes a favorable balance between detection accuracy and inference cost, outperforming state-of-the-art methods -- especially on high-dimensional and large-scale datasets. The source code is available at our GitHub repository.</li>
</ul>

<h3>Title: Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption</h3>
<ul>
<li><strong>Authors: </strong>Yepeng Liu, Xuandong Zhao, Dawn Song, Gregory W. Wornell, Yuheng Bu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18333">https://arxiv.org/abs/2510.18333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18333">https://arxiv.org/pdf/2510.18333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18333]] Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption(https://arxiv.org/abs/2510.18333)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Despite progress in watermarking algorithms for large language models (LLMs), real-world deployment remains limited. We argue that this gap stems from misaligned incentives among LLM providers, platforms, and end users, which manifest as four key barriers: competitive risk, detection-tool governance, robustness concerns and attribution issues. We revisit three classes of watermarking through this lens. \emph{Model watermarking} naturally aligns with LLM provider interests, yet faces new challenges in open-source ecosystems. \emph{LLM text watermarking} offers modest provider benefit when framed solely as an anti-misuse tool, but can gain traction in narrowly scoped settings such as dataset de-contamination or user-controlled provenance. \emph{In-context watermarking} (ICW) is tailored for trusted parties, such as conference organizers or educators, who embed hidden watermarking instructions into documents. If a dishonest reviewer or student submits this text to an LLM, the output carries a detectable watermark indicating misuse. This setup aligns incentives: users experience no quality loss, trusted parties gain a detection tool, and LLM providers remain neutral by simply following watermark instructions. We advocate for a broader exploration of incentive-aligned methods, with ICW as an example, in domains where trusted parties need reliable tools to detect misuse. More broadly, we distill design principles for incentive-aligned, domain-specific watermarking and outline future research directions. Our position is that the practical adoption of LLM watermarking requires aligning stakeholder incentives in targeted application domains and fostering active community engagement.</li>
</ul>

<h3>Title: ECG-LLM-- training and evaluation of domain-specific large language models for electrocardiography</h3>
<ul>
<li><strong>Authors: </strong>Lara Ahrens, Wilhelm Haverkamp, Nils Strodthoff</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18339">https://arxiv.org/abs/2510.18339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18339">https://arxiv.org/pdf/2510.18339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18339]] ECG-LLM-- training and evaluation of domain-specific large language models for electrocardiography(https://arxiv.org/abs/2510.18339)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Domain-adapted open-weight large language models (LLMs) offer promising healthcare applications, from queryable knowledge bases to multimodal assistants, with the crucial advantage of local deployment for privacy preservation. However, optimal adaptation strategies, evaluation methodologies, and performance relative to general-purpose LLMs remain poorly characterized. We investigated these questions in electrocardiography, an important area of cardiovascular medicine, by finetuning open-weight models on domain-specific literature and implementing a multi-layered evaluation framework comparing finetuned models, retrieval-augmented generation (RAG), and Claude Sonnet 3.7 as a representative general-purpose model. Finetuned Llama 3.1 70B achieved superior performance on multiple-choice evaluations and automatic text metrics, ranking second to Claude 3.7 in LLM-as-a-judge assessments. Human expert evaluation favored Claude 3.7 and RAG approaches for complex queries. Finetuned models significantly outperformed their base counterparts across nearly all evaluation modes. Our findings reveal substantial performance heterogeneity across evaluation methodologies, underscoring assessment complexity. Nevertheless, domain-specific adaptation through finetuning and RAG achieves competitive performance with proprietary models, supporting the viability of privacy-preserving, locally deployable clinical solutions.</li>
</ul>

<h3>Title: Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs</h3>
<ul>
<li><strong>Authors: </strong>Jongmin Lee, Ernest K. Ryu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18340">https://arxiv.org/abs/2510.18340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18340">https://arxiv.org/pdf/2510.18340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18340]] Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs(https://arxiv.org/abs/2510.18340)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The classical policy gradient method is the theoretical and conceptual foundation of modern policy-based reinforcement learning (RL) algorithms. Most rigorous analyses of such methods, particularly those establishing convergence guarantees, assume a discount factor $\gamma < 1$. In contrast, however, a recent line of work on policy-based RL for large language models uses the undiscounted total-reward setting with $\gamma = 1$, rendering much of the existing theory inapplicable. In this paper, we provide analyses of the policy gradient method for undiscounted expected total-reward infinite-horizon MDPs based on two key insights: (i) the classification of the MDP states into recurrent and transient states is invariant over the set of policies that assign strictly positive probability to every action (as is typical in deep RL models employing a softmax output layer) and (ii) the classical state visitation measure (which may be ill-defined when $\gamma = 1$) can be replaced with a new object that we call the transient visitation measure.</li>
</ul>

<h3>Title: ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation</h3>
<ul>
<li><strong>Authors: </strong>Kaiyuan Tan, Yingying Shen, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18341">https://arxiv.org/abs/2510.18341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18341">https://arxiv.org/pdf/2510.18341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18341]] ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation(https://arxiv.org/abs/2510.18341)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Realistic view extrapolation is critical for closed-loop simulation in autonomous driving, yet it remains a significant challenge for current Novel View Synthesis (NVS) methods, which often produce distorted and inconsistent images beyond the original trajectory. This report presents our winning solution which ctook first place in the RealADSim Workshop NVS track at ICCV 2025. To address the core challenges of street view extrapolation, we introduce a comprehensive four-stage pipeline. First, we employ a data-driven initialization strategy to generate a robust pseudo-LiDAR point cloud, avoiding local minima. Second, we inject strong geometric priors by modeling the road surface with a novel dimension-reduced SDF termed 2D-SDF. Third, we leverage a generative prior to create pseudo ground truth for extrapolated viewpoints, providing auxilary supervision. Finally, a data-driven adaptation network removes time-specific artifacts. On the RealADSim-NVS benchmark, our method achieves a final score of 0.441, ranking first among all participants.</li>
</ul>

<h3>Title: Combining Distantly Supervised Models with In Context Learning for Monolingual and Cross-Lingual Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Vipul Rathore, Malik Hammad Faisal, Parag Singla, Mausam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18344">https://arxiv.org/abs/2510.18344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18344">https://arxiv.org/pdf/2510.18344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18344]] Combining Distantly Supervised Models with In Context Learning for Monolingual and Cross-Lingual Relation Extraction(https://arxiv.org/abs/2510.18344)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Distantly Supervised Relation Extraction (DSRE) remains a long-standing challenge in NLP, where models must learn from noisy bag-level annotations while making sentence-level predictions. While existing state-of-the-art (SoTA) DSRE models rely on task-specific training, their integration with in-context learning (ICL) using large language models (LLMs) remains underexplored. A key challenge is that the LLM may not learn relation semantics correctly, due to noisy annotation. In response, we propose HYDRE -- HYbrid Distantly Supervised Relation Extraction framework. It first uses a trained DSRE model to identify the top-k candidate relations for a given test sentence, then uses a novel dynamic exemplar retrieval strategy that extracts reliable, sentence-level exemplars from training data, which are then provided in LLM prompt for outputting the final relation(s). We further extend HYDRE to cross-lingual settings for RE in low-resource languages. Using available English DSRE training data, we evaluate all methods on English as well as a newly curated benchmark covering four diverse low-resource Indic languages -- Oriya, Santali, Manipuri, and Tulu. HYDRE achieves up to 20 F1 point gains in English and, on average, 17 F1 points on Indic languages over prior SoTA DSRE models. Detailed ablations exhibit HYDRE's efficacy compared to other prompting strategies.</li>
</ul>

<h3>Title: GPTFace: Generative Pre-training of Facial-Linguistic Transformer by Span Masking and Weakly Correlated Text-image Data</h3>
<ul>
<li><strong>Authors: </strong>Yudong Li, Hao Li, Xianxu Hou, Linlin Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18345">https://arxiv.org/abs/2510.18345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18345">https://arxiv.org/pdf/2510.18345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18345]] GPTFace: Generative Pre-training of Facial-Linguistic Transformer by Span Masking and Weakly Correlated Text-image Data(https://arxiv.org/abs/2510.18345)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Compared to the prosperity of pre-training models in natural image understanding, the research on large-scale pre-training models for facial knowledge learning is still limited. Current approaches mainly rely on manually assembled and annotated face datasets for training, but labeling such datasets is labor-intensive and the trained models have limited scalability beyond the training data. To address these limitations, we present a generative pre-training model for facial knowledge learning that leverages large-scale web-built data for training. We use texts and images containing human faces crawled from the internet and conduct pre-training on self-supervised tasks, including masked image/language modeling (MILM) and image-text matching (ITM). During the generation stage, we further utilize the image-text matching loss to pull the generation distribution towards the control signal for controllable image/text generation. Experimental results demonstrate that our model achieves comparable performance to state-of-the-art pre-training models for various facial downstream tasks, such as attribution classification and expression recognition. Furthermore, our approach is also applicable to a wide range of face editing tasks, including face attribute editing, expression manipulation, mask removal, and photo inpainting.</li>
</ul>

<h3>Title: Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback</h3>
<ul>
<li><strong>Authors: </strong>Yi-Lun Wu, Bo-Kai Ruan, Chiang Tseng, Hong-Han Shuai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18353">https://arxiv.org/abs/2510.18353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18353">https://arxiv.org/pdf/2510.18353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18353]] Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback(https://arxiv.org/abs/2510.18353)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Direct preference optimization (DPO) methods have shown strong potential in aligning text-to-image diffusion models with human preferences by training on paired comparisons. These methods improve training stability by avoiding the REINFORCE algorithm but still struggle with challenges such as accurately estimating image probabilities due to the non-linear nature of the sigmoid function and the limited diversity of offline datasets. In this paper, we introduce Diffusion Denoising Ranking Optimization (Diffusion-DRO), a new preference learning framework grounded in inverse reinforcement learning. Diffusion-DRO removes the dependency on a reward model by casting preference learning as a ranking problem, thereby simplifying the training objective into a denoising formulation and overcoming the non-linear estimation issues found in prior methods. Moreover, Diffusion-DRO uniquely integrates offline expert demonstrations with online policy-generated negative samples, enabling it to effectively capture human preferences while addressing the limitations of offline data. Comprehensive experiments show that Diffusion-DRO delivers improved generation quality across a range of challenging and unseen prompts, outperforming state-of-the-art baselines in both both quantitative metrics and user studies. Our source code and pre-trained models are available at this https URL.</li>
</ul>

<h3>Title: KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory Call Center for Bengali Farmers</h3>
<ul>
<li><strong>Authors: </strong>Mohd Ruhul Ameen, Akif Islam, Farjana Aktar, M. Saifuzzaman Rafat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18355">https://arxiv.org/abs/2510.18355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18355">https://arxiv.org/pdf/2510.18355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18355]] KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory Call Center for Bengali Farmers(https://arxiv.org/abs/2510.18355)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>In Bangladesh, many farmers continue to face challenges in accessing timely, expert-level agricultural guidance. This paper presents KrishokBondhu, a voice-enabled, call-centre-integrated advisory platform built on a Retrieval-Augmented Generation (RAG) framework, designed specifically for Bengali-speaking farmers. The system aggregates authoritative agricultural handbooks, extension manuals, and NGO publications; applies Optical Character Recognition (OCR) and document-parsing pipelines to digitize and structure the content; and indexes this corpus in a vector database for efficient semantic retrieval. Through a simple phone-based interface, farmers can call the system to receive real-time, context-aware advice: speech-to-text converts the Bengali query, the RAG module retrieves relevant content, a large language model (Gemma 3-4B) generates a context-grounded response, and text-to-speech delivers the answer in natural spoken Bengali. In a pilot evaluation, KrishokBondhu produced high-quality responses for 72.7% of diverse agricultural queries covering crop management, disease control, and cultivation practices. Compared to the KisanQRS benchmark, the system achieved a composite score of 4.53 (vs. 3.13) on a 5-point scale, a 44.7% improvement, with especially large gains in contextual richness (+367%) and completeness (+100.4%), while maintaining comparable relevance and technical specificity. Semantic similarity analysis further revealed a strong correlation between retrieved context and answer quality, emphasizing the importance of grounding generative responses in curated documentation. KrishokBondhu demonstrates the feasibility of integrating call-centre accessibility, multilingual voice interaction, and modern RAG techniques to deliver expert-level agricultural guidance to remote Bangladeshi farmers, paving the way toward a fully AI-driven agricultural advisory ecosystem.</li>
</ul>

<h3>Title: Learning Human-Object Interaction as Groups</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Hong, Jianan Wei, Wenguan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18357">https://arxiv.org/abs/2510.18357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18357">https://arxiv.org/pdf/2510.18357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18357]] Learning Human-Object Interaction as Groups(https://arxiv.org/abs/2510.18357)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Human-Object Interaction Detection (HOI-DET) aims to localize human-object pairs and identify their interactive relationships. To aggregate contextual cues, existing methods typically propagate information across all detected entities via self-attention mechanisms, or establish message passing between humans and objects with bipartite graphs. However, they primarily focus on pairwise relationships, overlooking that interactions in real-world scenarios often emerge from collective behaviors (multiple humans and objects engaging in joint activities). In light of this, we revisit relation modeling from a group view and propose GroupHOI, a framework that propagates contextual information in terms of geometric proximity and semantic similarity. To exploit the geometric proximity, humans and objects are grouped into distinct clusters using a learnable proximity estimator based on spatial features derived from bounding boxes. In each group, a soft correspondence is computed via self-attention to aggregate and dispatch contextual cues. To incorporate the semantic similarity, we enhance the vanilla transformer-based interaction decoder with local contextual cues from HO-pair features. Extensive experiments on HICO-DET and V-COCO benchmarks demonstrate the superiority of GroupHOI over the state-of-the-art methods. It also exhibits leading performance on the more challenging Nonverbal Interaction Detection (NVI-DET) task, which involves varied forms of higher-order interactions within groups.</li>
</ul>

<h3>Title: Ensembling Pruned Attention Heads For Uncertainty-Aware Efficient Transformers</h3>
<ul>
<li><strong>Authors: </strong>Firas Gabetni, Giuseppe Curci, Andrea Pilzer, Subhankar Roy, Elisa Ricci, Gianni Franchi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18358">https://arxiv.org/abs/2510.18358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18358">https://arxiv.org/pdf/2510.18358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18358]] Ensembling Pruned Attention Heads For Uncertainty-Aware Efficient Transformers(https://arxiv.org/abs/2510.18358)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Uncertainty quantification (UQ) is essential for deploying deep neural networks in safety-critical settings. Although methods like Deep Ensembles achieve strong UQ performance, their high computational and memory costs hinder scalability to large models. We introduce Hydra Ensembles, an efficient transformer-based ensemble that prunes attention heads to create diverse members and merges them via a new multi-head attention with grouped fully-connected layers. This yields a compact model with inference speed close to a single network, matching or surpassing Deep Ensembles in UQ performance without retraining from scratch. We also provide an in-depth analysis of pruning, showing that naive approaches can harm calibration, whereas Hydra Ensembles preserves robust uncertainty. Experiments on image and text classification tasks, with various architectures, show consistent gains over Deep Ensembles. Remarkably, in zero-shot classification on ImageNet-1k, our approach surpasses state of the art methods, even without requiring additional training.</li>
</ul>

<h3>Title: Learning to Flow from Generative Pretext Tasks for Neural Architecture Encoding</h3>
<ul>
<li><strong>Authors: </strong>Sunwoo Kim, Hyunjin Hwang, Kijung Shin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18360">https://arxiv.org/abs/2510.18360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18360">https://arxiv.org/pdf/2510.18360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18360]] Learning to Flow from Generative Pretext Tasks for Neural Architecture Encoding(https://arxiv.org/abs/2510.18360)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The performance of a deep learning model on a specific task and dataset depends heavily on its neural architecture, motivating considerable efforts to rapidly and accurately identify architectures suited to the target task and dataset. To achieve this, researchers use machine learning models-typically neural architecture encoders-to predict the performance of a neural architecture. Many state-of-the-art encoders aim to capture information flow within a neural architecture, which reflects how information moves through the forward pass and backpropagation, via a specialized model structure. However, due to their complicated structures, these flow-based encoders are significantly slower to process neural architectures compared to simpler encoders, presenting a notable practical challenge. To address this, we propose FGP, a novel pre-training method for neural architecture encoding that trains an encoder to capture the information flow without requiring specialized model structures. FGP trains an encoder to reconstruct a flow surrogate, our proposed representation of the neural architecture's information flow. Our experiments show that FGP boosts encoder performance by up to 106% in Precision-1%, compared to the same encoder trained solely with supervised learning.</li>
</ul>

<h3>Title: FeatureFool: Zero-Query Fooling of Video Models via Feature Map</h3>
<ul>
<li><strong>Authors: </strong>Duoxun Tang, Xi Xiao, Guangwu Hu, Kangkang Sun, Xiao Yang, Dongyang Chen, Qing Li, Yongjie Yin, Jiyao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18362">https://arxiv.org/abs/2510.18362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18362">https://arxiv.org/pdf/2510.18362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18362]] FeatureFool: Zero-Query Fooling of Video Models via Feature Map(https://arxiv.org/abs/2510.18362)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal</a></li>
<li><strong>Abstract: </strong>The vulnerability of deep neural networks (DNNs) has been preliminarily verified. Existing black-box adversarial attacks usually require multi-round interaction with the model and consume numerous queries, which is impractical in the real-world and hard to scale to recently emerged Video-LLMs. Moreover, no attack in the video domain directly leverages feature maps to shift the clean-video feature space. We therefore propose FeatureFool, a stealthy, video-domain, zero-query black-box attack that utilizes information extracted from a DNN to alter the feature space of clean videos. Unlike query-based methods that rely on iterative interaction, FeatureFool performs a zero-query attack by directly exploiting DNN-extracted information. This efficient approach is unprecedented in the video domain. Experiments show that FeatureFool achieves an attack success rate above 70\% against traditional video classifiers without any queries. Benefiting from the transferability of the feature map, it can also craft harmful content and bypass Video-LLM recognition. Additionally, adversarial videos generated by FeatureFool exhibit high quality in terms of SSIM, PSNR, and Temporal-Inconsistency, making the attack barely perceptible. This paper may contain violent or explicit content.</li>
</ul>

<h3>Title: KoSimpleQA: A Korean Factuality Benchmark with an Analysis of Reasoning LLMs</h3>
<ul>
<li><strong>Authors: </strong>Donghyeon Ko, Yeguk Jin, Kyubyung Chae, Byungwook Lee, Chansong Jo, Sookyo In, Jaehong Lee, Taesup Kim, Donghyun Kwak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18368">https://arxiv.org/abs/2510.18368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18368">https://arxiv.org/pdf/2510.18368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18368]] KoSimpleQA: A Korean Factuality Benchmark with an Analysis of Reasoning LLMs(https://arxiv.org/abs/2510.18368)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present $\textbf{Korean SimpleQA (KoSimpleQA)}$, a benchmark for evaluating factuality in large language models (LLMs) with a focus on Korean cultural knowledge. KoSimpleQA is designed to be challenging yet easy to grade, consisting of 1,000 short, fact-seeking questions with unambiguous answers. We conduct a comprehensive evaluation across a diverse set of open-source LLMs of varying sizes that support Korean, and find that even the strongest model generates correct answer only 33.7% of the time, underscoring the challenging nature of KoSimpleQA. Notably, performance rankings on KoSimpleQA differ substantially from those on the English SimpleQA, highlighting the unique value of our dataset. Furthermore, our analysis of reasoning LLMs shows that engaging reasoning capabilities in the factual QA task can both help models better elicit their latent knowledge and improve their ability to abstain when uncertain. KoSimpleQA can be found at this https URL.</li>
</ul>

<h3>Title: Towards Fair ASR For Second Language Speakers Using Fairness Prompted Finetuning</h3>
<ul>
<li><strong>Authors: </strong>Monorama Swain, Bubai Maji, Jagabandhu Mishra, Markus Schedl, Anders Søgaard, Jesper Rindom Jensen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18374">https://arxiv.org/abs/2510.18374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18374">https://arxiv.org/pdf/2510.18374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18374]] Towards Fair ASR For Second Language Speakers Using Fairness Prompted Finetuning(https://arxiv.org/abs/2510.18374)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>In this work, we address the challenge of building fair English ASR systems for second-language speakers. Our analysis of widely used ASR models, Whisper and Seamless-M4T, reveals large fluctuations in word error rate (WER) across 26 accent groups, indicating significant fairness gaps. To mitigate this, we propose fairness-prompted finetuning with lightweight adapters, incorporating Spectral Decoupling (SD), Group Distributionally Robust Optimization (Group-DRO), and Invariant Risk Minimization (IRM). Our proposed fusion of traditional empirical risk minimization (ERM) with cross-entropy and fairness-driven objectives (SD, Group DRO, and IRM) enhances fairness across accent groups while maintaining overall recognition accuracy. In terms of macro-averaged word error rate, our approach achieves a relative improvement of 58.7% and 58.5% over the large pretrained Whisper and SeamlessM4T, and 9.7% and 7.8% over them, finetuning with standard empirical risk minimization with cross-entropy loss.</li>
</ul>

<h3>Title: S2AP: Score-space Sharpness Minimization for Adversarial Pruning</h3>
<ul>
<li><strong>Authors: </strong>Giorgio Piras, Qi Zhao, Fabio Brau, Maura Pintor, Christian Wressnegger, Battista Biggio</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18381">https://arxiv.org/abs/2510.18381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18381">https://arxiv.org/pdf/2510.18381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18381]] S2AP: Score-space Sharpness Minimization for Adversarial Pruning(https://arxiv.org/abs/2510.18381)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Adversarial pruning methods have emerged as a powerful tool for compressing neural networks while preserving robustness against adversarial attacks. These methods typically follow a three-step pipeline: (i) pretrain a robust model, (ii) select a binary mask for weight pruning, and (iii) finetune the pruned model. To select the binary mask, these methods minimize a robust loss by assigning an importance score to each weight, and then keep the weights with the highest scores. However, this score-space optimization can lead to sharp local minima in the robust loss landscape and, in turn, to an unstable mask selection, reducing the robustness of adversarial pruning methods. To overcome this issue, we propose a novel plug-in method for adversarial pruning, termed Score-space Sharpness-aware Adversarial Pruning (S2AP). Through our method, we introduce the concept of score-space sharpness minimization, which operates during the mask search by perturbing importance scores and minimizing the corresponding robust loss. Extensive experiments across various datasets, models, and sparsity levels demonstrate that S2AP effectively minimizes sharpness in score space, stabilizing the mask selection, and ultimately improving the robustness of adversarial pruning methods.</li>
</ul>

<h3>Title: MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models</h3>
<ul>
<li><strong>Authors: </strong>ChangSu Choi, Hoyun Song, Dongyeon Kim, WooHyeon Jung, Minkyung Cho, Sunjin Park, NohHyeob Bae, Seona Yu, KyungTae Lim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18383">https://arxiv.org/abs/2510.18383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18383">https://arxiv.org/pdf/2510.18383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18383]] MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models(https://arxiv.org/abs/2510.18383)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Distilling the tool-using capabilities of large language models (LLMs) into smaller, more efficient small language models (SLMs) is a key challenge for their practical application. The predominant approach, supervised fine-tuning (SFT), suffers from poor generalization as it trains models to imitate a static set of teacher trajectories rather than learn a robust methodology. While reinforcement learning (RL) offers an alternative, the standard RL using sparse rewards fails to effectively guide SLMs, causing them to struggle with inefficient exploration and adopt suboptimal strategies. To address these distinct challenges, we propose MENTOR, a framework that synergistically combines RL with teacher-guided distillation. Instead of simple imitation, MENTOR employs an RL-based process to learn a more generalizable policy through exploration. In addition, to solve the problem of reward sparsity, it uses a teacher's reference trajectory to construct a dense, composite teacher-guided reward that provides fine-grained guidance. Extensive experiments demonstrate that MENTOR significantly improves the cross-domain generalization and strategic competence of SLMs compared to both SFT and standard sparse-reward RL baselines.</li>
</ul>

<h3>Title: Entropy-Enhanced Conformal Features from Ricci Flow for Robust Alzheimer's Disease Classification</h3>
<ul>
<li><strong>Authors: </strong>F.Ahmadi, B.Bidabad, H.Nasiri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18396">https://arxiv.org/abs/2510.18396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18396">https://arxiv.org/pdf/2510.18396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18396]] Entropy-Enhanced Conformal Features from Ricci Flow for Robust Alzheimer's Disease Classification(https://arxiv.org/abs/2510.18396)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Background and Objective: In brain imaging, geometric surface models are essential for analyzing the 3D shapes of anatomical structures. Alzheimer's disease (AD) is associated with significant cortical atrophy, making such shape analysis a valuable diagnostic tool. The objective of this study is to introduce and validate a novel local surface representation method for the automated and accurate diagnosis of AD. Methods: The study utilizes T1-weighted MRI scans from 160 participants (80 AD patients and 80 healthy controls) from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Cortical surface models were reconstructed from the MRI data using Freesurfer. Key geometric attributes were computed from the 3D meshes. Area distortion and conformal factor were derived using Ricci flow for conformal parameterization, while Gaussian curvature was calculated directly from the mesh geometry. Shannon entropy was applied to these three features to create compact and informative feature vectors. The feature vectors were used to train and evaluate a suite of classifiers (e.g. XGBoost, MLP, Logistic Regression, etc.). Results: Statistical significance of performance differences between classifiers was evaluated using paired Welch's t-test. The method proved highly effective in distinguishing AD patients from healthy controls. The Multi-Layer Perceptron (MLP) and Logistic Regression classifiers outperformed all others, achieving an accuracy and F$_1$ Score of 98.62%. Conclusions: This study confirms that the entropy of conformally-derived geometric features provides a powerful and robust metric for cortical morphometry. The high classification accuracy underscores the method's potential to enhance the study and diagnosis of Alzheimer's disease, offering a straightforward yet powerful tool for clinical research applications.</li>
</ul>

<h3>Title: Bayesian Fully-Connected Tensor Network for Hyperspectral-Multispectral Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Linsong Shan, Zecan Yang, Laurence T. Yang, Changlong Li, Honglu Zhao, Xin Nie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18400">https://arxiv.org/abs/2510.18400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18400">https://arxiv.org/pdf/2510.18400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18400]] Bayesian Fully-Connected Tensor Network for Hyperspectral-Multispectral Image Fusion(https://arxiv.org/abs/2510.18400)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Tensor decomposition is a powerful tool for data analysis and has been extensively employed in the field of hyperspectral-multispectral image fusion (HMF). Existing tensor decomposition-based fusion methods typically rely on disruptive data vectorization/reshaping or impose rigid constraints on the arrangement of factor tensors, hindering the preservation of spatial-spectral structures and the modeling of cross-dimensional correlations. Although recent advances utilizing the Fully-Connected Tensor Network (FCTN) decomposition have partially alleviated these limitations, the process of reorganizing data into higher-order tensors still disrupts the intrinsic spatial-spectral structure. Furthermore, these methods necessitate extensive manual parameter tuning and exhibit limited robustness against noise and spatial degradation. To alleviate these issues, we propose the Bayesian FCTN (BFCTN) method. Within this probabilistic framework, a hierarchical sparse prior that characterizing the sparsity of physical elements, establishes connections between the factor tensors. This framework explicitly models the intrinsic physical coupling among spatial structures, spectral signatures, and local scene homogeneity. For model learning, we develop a parameter estimation method based on Variational Bayesian inference (VB) and the Expectation-Maximization (EM) algorithm, which significantly reduces the need for manual parameter tuning. Extensive experiments demonstrate that BFCTN not only achieves state-of-the-art fusion accuracy and strong robustness but also exhibits practical applicability in complex real-world scenarios.</li>
</ul>

<h3>Title: Automated Wicket-Taking Delivery Segmentation and Weakness Detection in Cricket Videos Using OCR-Guided YOLOv8 and Trajectory Modeling</h3>
<ul>
<li><strong>Authors: </strong>Mst Jannatun Ferdous, Masum Billah, Joy Karmoker, Mohd Ruhul Ameen, Akif Islam, Md. Omar Faruqe</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18405">https://arxiv.org/abs/2510.18405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18405">https://arxiv.org/pdf/2510.18405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18405]] Automated Wicket-Taking Delivery Segmentation and Weakness Detection in Cricket Videos Using OCR-Guided YOLOv8 and Trajectory Modeling(https://arxiv.org/abs/2510.18405)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents an automated system for cricket video analysis that leverages deep learning techniques to extract wicket-taking deliveries, detect cricket balls, and model ball trajectories. The system employs the YOLOv8 architecture for pitch and ball detection, combined with optical character recognition (OCR) for scorecard extraction to identify wicket-taking moments. Through comprehensive image preprocessing, including grayscale transformation, power transformation, and morphological operations, the system achieves robust text extraction from video frames. The pitch detection model achieved 99.5% mean Average Precision at 50% IoU (mAP50) with a precision of 0.999, while the ball detection model using transfer learning attained 99.18% mAP50 with 0.968 precision and 0.978 recall. The system enables trajectory modeling on detected pitches, providing data-driven insights for identifying batting weaknesses. Experimental results on multiple cricket match videos demonstrate the effectiveness of this approach for automated cricket analytics, offering significant potential for coaching and strategic decision-making.</li>
</ul>

<h3>Title: Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Miao Zhang, Junpeng Li, ChangChun HUa, Yana Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18406">https://arxiv.org/abs/2510.18406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18406">https://arxiv.org/pdf/2510.18406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18406]] Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees(https://arxiv.org/abs/2510.18406)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Weakly supervised learning often operates with coarse aggregate signals rather than instance labels. We study a setting where each training example is an $n$-tuple containing exactly m positives, while only the count m per tuple is observed. This NTMP (N-tuple with M positives) supervision arises in, e.g., image classification with region proposals and multi-instance measurements. We show that tuple counts admit a trainable unbiased risk estimator (URE) by linking the tuple-generation process to latent instance marginals. Starting from fixed (n,m), we derive a closed-form URE and extend it to variable tuple sizes, variable counts, and their combination. Identification holds whenever the effective mixing rate is separated from the class prior. We establish generalization bounds via Rademacher complexity and prove statistical consistency with standard rates under mild regularity assumptions. To improve finite-sample stability, we introduce simple ReLU corrections to the URE that preserve asymptotic correctness. Across benchmarks converted to NTMP tasks, the approach consistently outperforms representative weak-supervision baselines and yields favorable precision-recall and F1 trade-offs. It remains robust under class-prior imbalance and across diverse tuple configurations, demonstrating that count-only supervision can be exploited effectively through a theoretically grounded and practically stable objective.</li>
</ul>

<h3>Title: Provable Generalization Bounds for Deep Neural Networks with Adaptive Regularization</h3>
<ul>
<li><strong>Authors: </strong>Adeel Safder</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18410">https://arxiv.org/abs/2510.18410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18410">https://arxiv.org/pdf/2510.18410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18410]] Provable Generalization Bounds for Deep Neural Networks with Adaptive Regularization(https://arxiv.org/abs/2510.18410)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) achieve remarkable performance but often suffer from overfitting due to their high capacity. We introduce Momentum-Adaptive Gradient Dropout (MAGDrop), a novel regularization method that dynamically adjusts dropout rates on activations based on current gradients and accumulated momentum, enhancing stability in non-convex optimization landscapes. To theoretically justify MAGDrop's effectiveness, we derive a tightened PAC-Bayes generalization bound that accounts for its adaptive nature, achieving up to 20% sharper bounds compared to standard approaches by leveraging momentum-driven perturbation control. Empirically, the activation-based MAGDrop outperforms baseline regularization techniques, including standard dropout and adaptive gradient regularization, by 1-2% in test accuracy on MNIST (99.52%) and CIFAR-10 (90.63%), with generalization gaps of 0.48% and 7.14%, respectively. Our work bridges theoretical insights and practical advancements, offering a robust framework for enhancing DNN generalization suitable for high-stakes applications.</li>
</ul>

<h3>Title: Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Yan, Guo-Qing Jiang, Yuchen Zhang, Xiaoxing Ma, Ran Zhu, Chun Cao, Jingwei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18413">https://arxiv.org/abs/2510.18413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18413">https://arxiv.org/pdf/2510.18413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18413]] Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference(https://arxiv.org/abs/2510.18413)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) now support context windows of hundreds of thousands to millions of tokens, enabling applications such as long-document summarization, large-scale code synthesis, multi-document question answering and persistent multi-turn dialogue. However, such extended contexts exacerbate the quadratic cost of self-attention, leading to severe latency in autoregressive decoding. Existing sparse attention methods alleviate these costs but rely on heuristic patterns that struggle to recall critical key-value (KV) pairs for each query, resulting in accuracy degradation. We introduce Adamas, a lightweight yet highly accurate sparse attention mechanism designed for long-context inference. Adamas applies the Hadamard transform, bucketization and 2-bit compression to produce compact representations, and leverages Manhattan-distance estimation for efficient top-k selections. Experiments show that Adamas matches the accuracy of full attention with only a 64-token budget, achieves near-lossless performance at 128, and supports up to 8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering up to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences. Remarkably, Adamas attains comparable or even lower perplexity than full attention, underscoring its effectiveness in maintaining accuracy under aggressive sparsity.</li>
</ul>

<h3>Title: ScaleNet: Scaling up Pretrained Neural Networks with Incremental Parameters</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Hao, Jianyuan Guo, Li Shen, Kai Han, Yehui Tang, Han Hu, Yunhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18431">https://arxiv.org/abs/2510.18431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18431">https://arxiv.org/pdf/2510.18431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18431]] ScaleNet: Scaling up Pretrained Neural Networks with Incremental Parameters(https://arxiv.org/abs/2510.18431)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advancements in vision transformers (ViTs) have demonstrated that larger models often achieve superior performance. However, training these models remains computationally intensive and costly. To address this challenge, we introduce ScaleNet, an efficient approach for scaling ViT models. Unlike conventional training from scratch, ScaleNet facilitates rapid model expansion with negligible increases in parameters, building on existing pretrained models. This offers a cost-effective solution for scaling up ViTs. Specifically, ScaleNet achieves model expansion by inserting additional layers into pretrained ViTs, utilizing layer-wise weight sharing to maintain parameters efficiency. Each added layer shares its parameter tensor with a corresponding layer from the pretrained model. To mitigate potential performance degradation due to shared weights, ScaleNet introduces a small set of adjustment parameters for each layer. These adjustment parameters are implemented through parallel adapter modules, ensuring that each instance of the shared parameter tensor remains distinct and optimized for its specific function. Experiments on the ImageNet-1K dataset demonstrate that ScaleNet enables efficient expansion of ViT models. With a 2$\times$ depth-scaled DeiT-Base model, ScaleNet achieves a 7.42% accuracy improvement over training from scratch while requiring only one-third of the training epochs, highlighting its efficiency in scaling ViTs. Beyond image classification, our method shows significant potential for application in downstream vision areas, as evidenced by the validation in object detection task.</li>
</ul>

<h3>Title: ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization</h3>
<ul>
<li><strong>Authors: </strong>Yuanhe Guo, Linxi Xie, Zhuoran Chen, Kangrui Yu, Ryan Po, Guandao Yang, Gordon Wetztein, Hongyi Wen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18433">https://arxiv.org/abs/2510.18433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18433">https://arxiv.org/pdf/2510.18433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18433]] ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization(https://arxiv.org/abs/2510.18433)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce ImageGem, a dataset for studying generative models that understand fine-grained individual preferences. We posit that a key challenge hindering the development of such a generative model is the lack of in-the-wild and fine-grained user preference annotations. Our dataset features real-world interaction data from 57K users, who collectively have built 242K customized LoRAs, written 3M text prompts, and created 5M generated images. With user preference annotations from our dataset, we were able to train better preference alignment models. In addition, leveraging individual user preference, we investigated the performance of retrieval models and a vision-language model on personalized image retrieval and generative model recommendation. Finally, we propose an end-to-end framework for editing customized diffusion models in a latent weight space to align with individual user preferences. Our results demonstrate that the ImageGem dataset enables, for the first time, a new paradigm for generative model personalization.</li>
</ul>

<h3>Title: Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Ji Du, Xin Wang, Fangwei Hao, Mingyang Yu, Chunyuan Chen, Jiesheng Wu, Bin Wang, Jing Xu, Ping Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18437">https://arxiv.org/abs/2510.18437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18437">https://arxiv.org/pdf/2510.18437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18437]] Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection(https://arxiv.org/abs/2510.18437)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>At the core of Camouflaged Object Detection (COD) lies segmenting objects from their highly similar surroundings. Previous efforts navigate this challenge primarily through image-level modeling or annotation-based optimization. Despite advancing considerably, this commonplace practice hardly taps valuable dataset-level contextual information or relies on laborious annotations. In this paper, we propose RISE, a RetrIeval SElf-augmented paradigm that exploits the entire training dataset to generate pseudo-labels for single images, which could be used to train COD models. RISE begins by constructing prototype libraries for environments and camouflaged objects using training images (without ground truth), followed by K-Nearest Neighbor (KNN) retrieval to generate pseudo-masks for each image based on these libraries. It is important to recognize that using only training images without annotations exerts a pronounced challenge in crafting high-quality prototype libraries. In this light, we introduce a Clustering-then-Retrieval (CR) strategy, where coarse masks are first generated through clustering, facilitating subsequent histogram-based image filtering and cross-category retrieval to produce high-confidence prototypes. In the KNN retrieval stage, to alleviate the effect of artifacts in feature maps, we propose Multi-View KNN Retrieval (MVKR), which integrates retrieval results from diverse views to produce more robust and precise pseudo-masks. Extensive experiments demonstrate that RISE outperforms state-of-the-art unsupervised and prompt-based methods. Code is available at this https URL.</li>
</ul>

<h3>Title: DeepTx: Real-Time Transaction Risk Analysis via Multi-Modal Features and LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Liu, Xinlei Li, Yi Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18438">https://arxiv.org/abs/2510.18438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18438">https://arxiv.org/pdf/2510.18438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18438]] DeepTx: Real-Time Transaction Risk Analysis via Multi-Modal Features and LLM Reasoning(https://arxiv.org/abs/2510.18438)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Phishing attacks in Web3 ecosystems are increasingly sophisticated, exploiting deceptive contract logic, malicious frontend scripts, and token approval patterns. We present DeepTx, a real-time transaction analysis system that detects such threats before user confirmation. DeepTx simulates pending transactions, extracts behavior, context, and UI features, and uses multiple large language models (LLMs) to reason about transaction intent. A consensus mechanism with self-reflection ensures robust and explainable decisions. Evaluated on our phishing dataset, DeepTx achieves high precision and recall (demo video: this https URL).</li>
</ul>

<h3>Title: Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation</h3>
<ul>
<li><strong>Authors: </strong>Yasser Hamidullah, Koel Dutta Chowdury, Yusser Al-Ghussin, Shakib Yazdani, Cennet Oguz, Josef van Genabith, Cristina España-Bonet</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18439">https://arxiv.org/abs/2510.18439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18439">https://arxiv.org/pdf/2510.18439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18439]] Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation(https://arxiv.org/abs/2510.18439)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hallucination, where models generate fluent text unsupported by visual evidence, remains a major flaw in vision-language models and is particularly critical in sign language translation (SLT). In SLT, meaning depends on precise grounding in video, and gloss-free models are especially vulnerable because they map continuous signer movements directly into natural language without intermediate gloss supervision that serves as alignment. We argue that hallucinations arise when models rely on language priors rather than visual input. To capture this, we propose a token-level reliability measure that quantifies how much the decoder uses visual information. Our method combines feature-based sensitivity, which measures internal changes when video is masked, with counterfactual signals, which capture probability differences between clean and altered video inputs. These signals are aggregated into a sentence-level reliability score, providing a compact and interpretable measure of visual grounding. We evaluate the proposed measure on two SLT benchmarks (PHOENIX-2014T and CSL-Daily) with both gloss-based and gloss-free models. Our results show that reliability predicts hallucination rates, generalizes across datasets and architectures, and decreases under visual degradations. Beyond these quantitative trends, we also find that reliability distinguishes grounded tokens from guessed ones, allowing risk estimation without references; when combined with text-based signals (confidence, perplexity, or entropy), it further improves hallucination risk estimation. Qualitative analysis highlights why gloss-free models are more susceptible to hallucinations. Taken together, our findings establish reliability as a practical and reusable tool for diagnosing hallucinations in SLT, and lay the groundwork for more robust hallucination detection in multimodal generation.</li>
</ul>

<h3>Title: LAND: Lung and Nodule Diffusion for 3D Chest CT Synthesis with Anatomical Guidance</h3>
<ul>
<li><strong>Authors: </strong>Anna Oliveras, Roger Marí, Rafael Redondo, Oriol Guardià, Ana Tost, Bhalaji Nagarajan, Carolina Migliorelli, Vicent Ribas, Petia Radeva</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18446">https://arxiv.org/abs/2510.18446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18446">https://arxiv.org/pdf/2510.18446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18446]] LAND: Lung and Nodule Diffusion for 3D Chest CT Synthesis with Anatomical Guidance(https://arxiv.org/abs/2510.18446)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work introduces a new latent diffusion model to generate high-quality 3D chest CT scans conditioned on 3D anatomical masks. The method synthesizes volumetric images of size 256x256x256 at 1 mm isotropic resolution using a single mid-range GPU, significantly lowering the computational cost compared to existing approaches. The conditioning masks delineate lung and nodule regions, enabling precise control over the output anatomical features. Experimental results demonstrate that conditioning solely on nodule masks leads to anatomically incorrect outputs, highlighting the importance of incorporating global lung structure for accurate conditional synthesis. The proposed approach supports the generation of diverse CT volumes with and without lung nodules of varying attributes, providing a valuable tool for training AI models or healthcare professionals.</li>
</ul>

<h3>Title: Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Atharvan Dogra, Soumya Suvra Ghosal, Ameet Deshpande, Ashwin Kalyan, Dinesh Manocha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18454">https://arxiv.org/abs/2510.18454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18454">https://arxiv.org/pdf/2510.18454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18454]] Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor in Language Models(https://arxiv.org/abs/2510.18454)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly used for creative writing and engagement content, raising safety concerns about the outputs. Therefore, casting humor generation as a testbed, this work evaluates how funniness optimization in modern LLM pipelines couples with harmful content by jointly measuring humor, stereotypicality, and toxicity. This is further supplemented by analyzing incongruity signals through information-theoretic metrics. Across six models, we observe that harmful outputs receive higher humor scores which further increase under role-based prompting, indicating a bias amplification loop between generators and evaluators. Information-theoretic analyses show harmful cues widen predictive uncertainty and surprisingly, can even make harmful punchlines more expected for some models, suggesting structural embedding in learned humor distributions. External validation on an additional satire-generation task with human perceived funniness judgments shows that LLM satire increases stereotypicality and typically toxicity, including for closed models. Quantitatively, stereotypical/toxic jokes gain $10-21\%$ in mean humor score, stereotypical jokes appear $11\%$ to $28\%$ more often among the jokes marked funny by LLM-based metric and up to $10\%$ more often in generations perceived as funny by humans.</li>
</ul>

<h3>Title: Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tianci Bi, Xiaoyi Zhang, Yan Lu, Nanning Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18457">https://arxiv.org/abs/2510.18457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18457">https://arxiv.org/pdf/2510.18457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18457]] Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models(https://arxiv.org/abs/2510.18457)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>The performance of Latent Diffusion Models (LDMs) is critically dependent on the quality of their visual tokenizer. While recent works have explored incorporating Vision Foundation Models (VFMs) via distillation, we identify a fundamental flaw in this approach: it inevitably weakens the robustness of alignment with the original VFM, causing the aligned latents to deviate semantically under distribution shifts. In this paper, we bypass distillation by proposing a more direct approach: Vision Foundation Model Variational Autoencoder (VFM-VAE). To resolve the inherent tension between the VFM's semantic focus and the need for pixel-level fidelity, we redesign the VFM-VAE decoder with Multi-Scale Latent Fusion and Progressive Resolution Reconstruction blocks, enabling high-quality reconstruction from spatially coarse VFM features. Furthermore, we provide a comprehensive analysis of representation dynamics during diffusion training, introducing the proposed SE-CKNNA metric as a more precise tool for this diagnosis. This analysis allows us to develop a joint tokenizer-diffusion alignment strategy that dramatically accelerates convergence. Our innovations in tokenizer design and training strategy lead to superior performance and efficiency: our system reaches a gFID (w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers). With continued training to 640 epochs, it further attains a gFID (w/o CFG) of 1.62, establishing direct VFM integration as a superior paradigm for LDMs.</li>
</ul>

<h3>Title: DePass: Unified Feature Attributing by Simple Decomposed Forward Pass</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Hong, Che Jiang, Kai Tian, Biqing Qi, Youbang Sun, Ning Ding, Bowen Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18462">https://arxiv.org/abs/2510.18462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18462">https://arxiv.org/pdf/2510.18462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18462]] DePass: Unified Feature Attributing by Simple Decomposed Forward Pass(https://arxiv.org/abs/2510.18462)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Attributing the behavior of Transformer models to internal computations is a central challenge in mechanistic interpretability. We introduce DePass, a unified framework for feature attribution based on a single decomposed forward pass. DePass decomposes hidden states into customized additive components, then propagates them with attention scores and MLP's activations fixed. It achieves faithful, fine-grained attribution without requiring auxiliary training. We validate DePass across token-level, model component-level, and subspace-level attribution tasks, demonstrating its effectiveness and fidelity. Our experiments highlight its potential to attribute information flow between arbitrary components of a Transformer model. We hope DePass serves as a foundational tool for broader applications in interpretability.</li>
</ul>

<h3>Title: PP3D: An In-Browser Vision-Based Defense Against Web Behavior Manipulation Attacks</h3>
<ul>
<li><strong>Authors: </strong>Spencer King, Irfan Ozen, Karthika Subramani, Saranyan Senthivel, Phani Vadrevu, Roberto Perdisci</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18465">https://arxiv.org/abs/2510.18465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18465">https://arxiv.org/pdf/2510.18465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18465]] PP3D: An In-Browser Vision-Based Defense Against Web Behavior Manipulation Attacks(https://arxiv.org/abs/2510.18465)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>Web-based behavior-manipulation attacks (BMAs) - such as scareware, fake software downloads, tech support scams, etc. - are a class of social engineering (SE) attacks that exploit human decision-making vulnerabilities. These attacks remain under-studied compared to other attacks such as information harvesting attacks (e.g., phishing) or malware infections. Prior technical work has primarily focused on measuring BMAs, offering little in the way of generic defenses. To address this gap, we introduce Pixel Patrol 3D (PP3D), the first end-to-end browser framework for discovering, detecting, and defending against behavior-manipulating SE attacks in real time. PP3D consists of a visual detection model implemented within a browser extension, which deploys the model client-side to protect users across desktop and mobile devices while preserving privacy. Our evaluation shows that PP3D can achieve above 99% detection rate at 1% false positives, while maintaining good latency and overhead performance across devices. Even when faced with new BMA samples collected months after training the detection model, our defense system can still achieve above 97% detection rate at 1% false positives. These results demonstrate that our framework offers a practical, effective, and generalizable defense against a broad and evolving class of web behavior-manipulation attacks.</li>
</ul>

<h3>Title: CEFR-Annotated WordNet: LLM-Based Proficiency-Guided Semantic Database for Language Learning</h3>
<ul>
<li><strong>Authors: </strong>Masato Kikuchi, Masatsugu Ono, Toshioki Soga, Tetsu Tanabe, Tadachika Ozono</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18466">https://arxiv.org/abs/2510.18466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18466">https://arxiv.org/pdf/2510.18466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18466]] CEFR-Annotated WordNet: LLM-Based Proficiency-Guided Semantic Database for Language Learning(https://arxiv.org/abs/2510.18466)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although WordNet is a valuable resource owing to its structured semantic networks and extensive vocabulary, its fine-grained sense distinctions can be challenging for second-language learners. To address this, we developed a WordNet annotated with the Common European Framework of Reference for Languages (CEFR), integrating its semantic networks with language-proficiency levels. We automated this process using a large language model to measure the semantic similarity between sense definitions in WordNet and entries in the English Vocabulary Profile Online. To validate our method, we constructed a large-scale corpus containing both sense and CEFR-level information from our annotated WordNet and used it to develop contextual lexical classifiers. Our experiments demonstrate that models fine-tuned on our corpus perform comparably to those trained on gold-standard annotations. Furthermore, by combining our corpus with the gold-standard data, we developed a practical classifier that achieves a Macro-F1 score of 0.81, indicating the high accuracy of our annotations. Our annotated WordNet, corpus, and classifiers are publicly available to help bridge the gap between natural language processing and language education, thereby facilitating more effective and efficient language learning.</li>
</ul>

<h3>Title: Simple and Efficient Heterogeneous Temporal Graph Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Yili Wang, Tairan Huang, Changlong He, Qiutong Li, Jianliang Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18467">https://arxiv.org/abs/2510.18467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18467">https://arxiv.org/pdf/2510.18467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18467]] Simple and Efficient Heterogeneous Temporal Graph Neural Network(https://arxiv.org/abs/2510.18467)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Heterogeneous temporal graphs (HTGs) are ubiquitous data structures in the real world. Recently, to enhance representation learning on HTGs, numerous attention-based neural networks have been proposed. Despite these successes, existing methods rely on a decoupled temporal and spatial learning paradigm, which weakens interactions of spatio-temporal information and leads to a high model complexity. To bridge this gap, we propose a novel learning paradigm for HTGs called Simple and Efficient Heterogeneous Temporal Graph N}eural Network (SE-HTGNN). Specifically, we innovatively integrate temporal modeling into spatial learning via a novel dynamic attention mechanism, which retains attention information from historical graph snapshots to guide subsequent attention computation, thereby improving the overall discriminative representations learning of HTGs. Additionally, to comprehensively and adaptively understand HTGs, we leverage large language models to prompt SE-HTGNN, enabling the model to capture the implicit properties of node types as prior knowledge. Extensive experiments demonstrate that SE-HTGNN achieves up to 10x speed-up over the state-of-the-art and latest baseline while maintaining the best forecasting accuracy.</li>
</ul>

<h3>Title: IMB: An Italian Medical Benchmark for Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Antonio Romano, Giuseppe Riccio, Mariano Barone, Marco Postiglione, Vincenzo Moscato</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18468">https://arxiv.org/abs/2510.18468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18468">https://arxiv.org/pdf/2510.18468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18468]] IMB: An Italian Medical Benchmark for Question Answering(https://arxiv.org/abs/2510.18468)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Online medical forums have long served as vital platforms where patients seek professional healthcare advice, generating vast amounts of valuable knowledge. However, the informal nature and linguistic complexity of forum interactions pose significant challenges for automated question answering systems, especially when dealing with non-English languages. We present two comprehensive Italian medical benchmarks: \textbf{IMB-QA}, containing 782,644 patient-doctor conversations from 77 medical categories, and \textbf{IMB-MCQA}, comprising 25,862 multiple-choice questions from medical specialty examinations. We demonstrate how Large Language Models (LLMs) can be leveraged to improve the clarity and consistency of medical forum data while retaining their original meaning and conversational style, and compare a variety of LLM architectures on both open and multiple-choice question answering tasks. Our experiments with Retrieval Augmented Generation (RAG) and domain-specific fine-tuning reveal that specialized adaptation strategies can outperform larger, general-purpose models in medical question answering tasks. These findings suggest that effective medical AI systems may benefit more from domain expertise and efficient information retrieval than from increased model scale. We release both datasets and evaluation frameworks in our GitHub repository to support further research on multilingual medical question answering: this https URL.</li>
</ul>

<h3>Title: Benchmarking Fairness-aware Graph Neural Networks in Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Yuya Sasaki</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18473">https://arxiv.org/abs/2510.18473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18473">https://arxiv.org/pdf/2510.18473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18473]] Benchmarking Fairness-aware Graph Neural Networks in Knowledge Graphs(https://arxiv.org/abs/2510.18473)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs) are powerful tools for learning from graph-structured data but often produce biased predictions with respect to sensitive attributes. Fairness-aware GNNs have been actively studied for mitigating biased predictions. However, no prior studies have evaluated fairness-aware GNNs on knowledge graphs, which are one of the most important graphs in many applications, such as recommender systems. Therefore, we introduce a benchmarking study on knowledge graphs. We generate new graphs from three knowledge graphs, YAGO, DBpedia, and Wikidata, that are significantly larger than the existing graph datasets used in fairness studies. We benchmark inprocessing and preprocessing methods in different GNN backbones and early stopping conditions. We find several key insights: (i) knowledge graphs show different trends from existing datasets; clearer trade-offs between prediction accuracy and fairness metrics than other graphs in fairness-aware GNNs, (ii) the performance is largely affected by not only fairness-aware GNN methods but also GNN backbones and early stopping conditions, and (iii) preprocessing methods often improve fairness metrics, while inprocessing methods improve prediction accuracy.</li>
</ul>

<h3>Title: DART: A Structured Dataset of Regulatory Drug Documents in Italian for Clinical NLP</h3>
<ul>
<li><strong>Authors: </strong>Mariano Barone, Antonio Laudante, Giuseppe Riccio, Antonio Romano, Marco Postiglione, Vincenzo Moscato</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18475">https://arxiv.org/abs/2510.18475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18475">https://arxiv.org/pdf/2510.18475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18475]] DART: A Structured Dataset of Regulatory Drug Documents in Italian for Clinical NLP(https://arxiv.org/abs/2510.18475)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>The extraction of pharmacological knowledge from regulatory documents has become a key focus in biomedical natural language processing, with applications ranging from adverse event monitoring to AI-assisted clinical decision support. However, research in this field has predominantly relied on English-language corpora such as DrugBank, leaving a significant gap in resources tailored to other healthcare systems. To address this limitation, we introduce DART (Drug Annotation from Regulatory Texts), the first structured corpus of Italian Summaries of Product Characteristics derived from the official repository of the Italian Medicines Agency (AIFA). The dataset was built through a reproducible pipeline encompassing web-scale document retrieval, semantic segmentation of regulatory sections, and clinical summarization using a few-shot-tuned large language model with low-temperature decoding. DART provides structured information on key pharmacological domains such as indications, adverse drug reactions, and drug-drug interactions. To validate its utility, we implemented an LLM-based drug interaction checker that leverages the dataset to infer clinically meaningful interactions. Experimental results show that instruction-tuned LLMs can accurately infer potential interactions and their clinical implications when grounded in the structured textual fields of DART. We publicly release our code on GitHub: this https URL.</li>
</ul>

<h3>Title: How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices</h3>
<ul>
<li><strong>Authors: </strong>Han Peng, Peiyu Liu, Zican Dong, Daixuan Cheng, Junyi Li, Yiru Tang, Shuo Wang, Wayne Xin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18480">https://arxiv.org/abs/2510.18480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18480">https://arxiv.org/pdf/2510.18480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18480]] How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices(https://arxiv.org/abs/2510.18480)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a roofline-based theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs.</li>
</ul>

<h3>Title: The Attribution Story of WhisperGate: An Academic Perspective</h3>
<ul>
<li><strong>Authors: </strong>Oleksandr Adamov, Anders Carlsson</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18484">https://arxiv.org/abs/2510.18484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18484">https://arxiv.org/pdf/2510.18484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18484]] The Attribution Story of WhisperGate: An Academic Perspective(https://arxiv.org/abs/2510.18484)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>This paper explores the challenges of cyberattack attribution, specifically APTs, applying the case study approach for the WhisperGate cyber operation of January 2022 executed by the Russian military intelligence service (GRU) and targeting Ukrainian government entities. The study provides a detailed review of the threat actor identifiers and taxonomies used by leading cybersecurity vendors, focusing on the evolving attribution from Microsoft, ESET, and CrowdStrike researchers. Once the attribution to Ember Bear (GRU Unit 29155) is established through technical and intelligence reports, we use both traditional machine learning classifiers and a large language model (ChatGPT) to analyze the indicators of compromise (IoCs), tactics, and techniques to statistically and semantically attribute the WhisperGate attack. Our findings reveal overlapping indicators with the Sandworm group (GRU Unit 74455) but also strong evidence pointing to Ember Bear, especially when the LLM is fine-tuned or contextually augmented with additional intelligence. Thus, showing how AI/GenAI with proper fine-tuning are capable of solving the attribution challenge.</li>
</ul>

<h3>Title: Learning to Navigate Under Imperfect Perception: Conformalised Segmentation for Safe Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Daniel Bethell, Simos Gerasimou, Radu Calinescu, Calum Imrie</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18485">https://arxiv.org/abs/2510.18485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18485">https://arxiv.org/pdf/2510.18485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18485]] Learning to Navigate Under Imperfect Perception: Conformalised Segmentation for Safe Reinforcement Learning(https://arxiv.org/abs/2510.18485)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Reliable navigation in safety-critical environments requires both accurate hazard perception and principled uncertainty handling to strengthen downstream safety handling. Despite the effectiveness of existing approaches, they assume perfect hazard detection capabilities, while uncertainty-aware perception approaches lack finite-sample guarantees. We present COPPOL, a conformal-driven perception-to-policy learning approach that integrates distribution-free, finite-sample safety guarantees into semantic segmentation, yielding calibrated hazard maps with rigorous bounds for missed detections. These maps induce risk-aware cost fields for downstream RL planning. Across two satellite-derived benchmarks, COPPOL increases hazard coverage (up to 6x) compared to comparative baselines, achieving near-complete detection of unsafe regions while reducing hazardous violations during navigation (up to approx 50%). More importantly, our approach remains robust to distributional shift, preserving both safety and efficiency.</li>
</ul>

<h3>Title: Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from Alternating-exposure Monocular Videos</h3>
<ul>
<li><strong>Authors: </strong>Jinfeng Liu, Lingtong Kong, Mi Zhou, Jinwen Chen, Dan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18489">https://arxiv.org/abs/2510.18489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18489">https://arxiv.org/pdf/2510.18489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18489]] Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from Alternating-exposure Monocular Videos(https://arxiv.org/abs/2510.18489)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes from unposed monocular low dynamic range (LDR) videos captured with alternating exposures. To tackle such a challenging problem, we present a unified framework with two-stage optimization approach based on Gaussian Splatting. The first stage learns a video HDR Gaussian representation in orthographic camera coordinate space, eliminating the need for camera poses and enabling robust initial HDR video reconstruction. The second stage transforms video Gaussians into world space and jointly refines the world Gaussians with camera poses. Furthermore, we propose a temporal luminance regularization strategy to enhance the temporal consistency of the HDR appearance. Since our task has not been studied before, we construct a new evaluation benchmark using publicly available datasets for HDR video reconstruction. Extensive experiments demonstrate that Mono4DGS-HDR significantly outperforms alternative solutions adapted from state-of-the-art methods in both rendering quality and speed.</li>
</ul>

<h3>Title: One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for Customizable Privacy-Preserving Phone Scam Detection</h3>
<ul>
<li><strong>Authors: </strong>Kangzhong Wang, Zitong Shen, Youqian Zhang, Michael MK Cheung, Xiapu Luo, Grace Ngai, Eugene Yujun Fu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18493">https://arxiv.org/abs/2510.18493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18493">https://arxiv.org/pdf/2510.18493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18493]] One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for Customizable Privacy-Preserving Phone Scam Detection(https://arxiv.org/abs/2510.18493)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, large language model</a></li>
<li><strong>Abstract: </strong>Phone scams remain a pervasive threat to both personal safety and financial security worldwide. Recent advances in large language models (LLMs) have demonstrated strong potential in detecting fraudulent behavior by analyzing transcribed phone conversations. However, these capabilities introduce notable privacy risks, as such conversations frequently contain sensitive personal information that may be exposed to third-party service providers during processing. In this work, we explore how to harness LLMs for phone scam detection while preserving user privacy. We propose MASK (Modular Adaptive Sanitization Kit), a trainable and extensible framework that enables dynamic privacy adjustment based on individual preferences. MASK provides a pluggable architecture that accommodates diverse sanitization methods - from traditional keyword-based techniques for high-privacy users to sophisticated neural approaches for those prioritizing accuracy. We also discuss potential modeling approaches and loss function designs for future development, enabling the creation of truly personalized, privacy-aware LLM-based detection systems that balance user trust and detection effectiveness, even beyond phone scam context.</li>
</ul>

<h3>Title: Alibaba International E-commerce Product Search Competition DILAB Team Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Hyewon Lee, Junghyun Oh, Minkyung Song, Soyoung Park, Seunghoon Han</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18499">https://arxiv.org/abs/2510.18499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18499">https://arxiv.org/pdf/2510.18499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18499]] Alibaba International E-commerce Product Search Competition DILAB Team Technical Report(https://arxiv.org/abs/2510.18499)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study presents the multilingual e-commerce search system developed by the DILAB team, which achieved 5th place on the final leaderboard with a competitive overall score of 0.8819, demonstrating stable and high-performing results across evaluation metrics. To address challenges in multilingual query-item understanding, we designed a multi-stage pipeline integrating data refinement, lightweight preprocessing, and adaptive modeling. The data refinement stage enhanced dataset consistency and category coverage, while language tagging and noise filtering improved input quality. In the modeling phase, multiple architectures and fine-tuning strategies were explored, and hyperparameters optimized using curated validation sets to balance performance across query-category (QC) and query-item (QI) tasks. The proposed framework exhibited robustness and adaptability across languages and domains, highlighting the effectiveness of systematic data curation and iterative evaluation for multilingual search systems. The source code is available at this https URL.</li>
</ul>

<h3>Title: Prompting the Priorities: A First Look at Evaluating LLMs for Vulnerability Triage and Prioritization</h3>
<ul>
<li><strong>Authors: </strong>Osama Al Haddad, Muhammad Ikram, Ejaz Ahmed, Young Lee</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18508">https://arxiv.org/abs/2510.18508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18508">https://arxiv.org/pdf/2510.18508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18508]] Prompting the Priorities: A First Look at Evaluating LLMs for Vulnerability Triage and Prioritization(https://arxiv.org/abs/2510.18508)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, fair, large language model</a></li>
<li><strong>Abstract: </strong>Security analysts face increasing pressure to triage large and complex vulnerability backlogs. Large Language Models (LLMs) offer a potential aid by automating parts of the interpretation process. We evaluate four models (ChatGPT, Claude, Gemini, and DeepSeek) across twelve prompting techniques to interpret semi-structured and unstructured vulnerability information. As a concrete use case, we test each model's ability to predict decision points in the Stakeholder-Specific Vulnerability Categorization (SSVC) framework: Exploitation, Automatable, Technical Impact, and Mission and Wellbeing. Using 384 real-world vulnerabilities from the VulZoo dataset, we issued more than 165,000 queries to assess performance under prompting styles including one-shot, few-shot, and chain-of-thought. We report F1 scores for each SSVC decision point and Cohen's kappa (weighted and unweighted) for the final SSVC decision outcomes. Gemini consistently ranked highest, leading on three of four decision points and yielding the most correct recommendations. Prompting with exemplars generally improved accuracy, although all models struggled on some decision points. Only DeepSeek achieved fair agreement under weighted metrics, and all models tended to over-predict risk. Overall, current LLMs do not replace expert judgment. However, specific LLM and prompt combinations show moderate effectiveness for targeted SSVC decisions. When applied with care, LLMs can support vulnerability prioritization workflows and help security teams respond more efficiently to emerging threats.</li>
</ul>

<h3>Title: Identity-Aware Large Language Models require Cultural Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Alistair Plum, Anne-Marie Lutgen, Christoph Purschke, Achim Rettinger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18510">https://arxiv.org/abs/2510.18510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18510">https://arxiv.org/pdf/2510.18510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18510]] Identity-Aware Large Language Models require Cultural Reasoning(https://arxiv.org/abs/2510.18510)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have become the latest trend in natural language processing, heavily featuring in the digital tools we use every day. However, their replies often reflect a narrow cultural viewpoint that overlooks the diversity of global users. This missing capability could be referred to as cultural reasoning, which we define here as the capacity of a model to recognise culture-specific knowledge values and social norms, and to adjust its output so that it aligns with the expectations of individual users. Because culture shapes interpretation, emotional resonance, and acceptable behaviour, cultural reasoning is essential for identity-aware AI. When this capacity is limited or absent, models can sustain stereotypes, ignore minority perspectives, erode trust, and perpetuate hate. Recent empirical studies strongly suggest that current models default to Western norms when judging moral dilemmas, interpreting idioms, or offering advice, and that fine-tuning on survey data only partly reduces this tendency. The present evaluation methods mainly report static accuracy scores and thus fail to capture adaptive reasoning in context. Although broader datasets can help, they cannot alone ensure genuine cultural competence. Therefore, we argue that cultural reasoning must be treated as a foundational capability alongside factual accuracy and linguistic coherence. By clarifying the concept and outlining initial directions for its assessment, a foundation is laid for future systems to be able to respond with greater sensitivity to the complex fabric of human culture.</li>
</ul>

<h3>Title: RayPose: Ray Bundling Diffusion for Template Views in Unseen 6D Object Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Junwen Huang, Shishir Reddy Vutukur, Peter KT Yu, Nassir Navab, Slobodan Ilic, Benjamin Busam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18521">https://arxiv.org/abs/2510.18521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18521">https://arxiv.org/pdf/2510.18521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18521]] RayPose: Ray Bundling Diffusion for Template Views in Unseen 6D Object Pose Estimation(https://arxiv.org/abs/2510.18521)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Typical template-based object pose pipelines estimate the pose by retrieving the closest matching template and aligning it with the observed image. However, failure to retrieve the correct template often leads to inaccurate pose predictions. To address this, we reformulate template-based object pose estimation as a ray alignment problem, where the viewing directions from multiple posed template images are learned to align with a non-posed query image. Inspired by recent progress in diffusion-based camera pose estimation, we embed this formulation into a diffusion transformer architecture that aligns a query image with a set of posed templates. We reparameterize object rotation using object-centered camera rays and model object translation by extending scale-invariant translation estimation to dense translation offsets. Our model leverages geometric priors from the templates to guide accurate query pose inference. A coarse-to-fine training strategy based on narrowed template sampling improves performance without modifying the network architecture. Extensive experiments across multiple benchmark datasets show competitive results of our method compared to state-of-the-art approaches in unseen object pose estimation.</li>
</ul>

<h3>Title: GBlobs: Local LiDAR Geometry for Improved Sensor Placement Generalization</h3>
<ul>
<li><strong>Authors: </strong>Dušan Malić, Christian Fruhwirth-Reisinger, Alexander Prutsch, Wei Lin, Samuel Schulter, Horst Possegger</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18539">https://arxiv.org/abs/2510.18539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18539">https://arxiv.org/pdf/2510.18539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18539]] GBlobs: Local LiDAR Geometry for Improved Sensor Placement Generalization(https://arxiv.org/abs/2510.18539)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This technical report outlines the top-ranking solution for RoboSense 2025: Track 3, achieving state-of-the-art performance on 3D object detection under various sensor placements. Our submission utilizes GBlobs, a local point cloud feature descriptor specifically designed to enhance model generalization across diverse LiDAR configurations. Current LiDAR-based 3D detectors often suffer from a \enquote{geometric shortcut} when trained on conventional global features (\ie, absolute Cartesian coordinates). This introduces a position bias that causes models to primarily rely on absolute object position rather than distinguishing shape and appearance characteristics. Although effective for in-domain data, this shortcut severely limits generalization when encountering different point distributions, such as those resulting from varying sensor placements. By using GBlobs as network input features, we effectively circumvent this geometric shortcut, compelling the network to learn robust, object-centric representations. This approach significantly enhances the model's ability to generalize, resulting in the exceptional performance demonstrated in this challenge.</li>
</ul>

<h3>Title: Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation</h3>
<ul>
<li><strong>Authors: </strong>Giovanni De Muri, Mark Vero, Robin Staab, Martin Vechev</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18541">https://arxiv.org/abs/2510.18541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18541">https://arxiv.org/pdf/2510.18541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18541]] Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation(https://arxiv.org/abs/2510.18541)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>LLMs are often used by downstream users as teacher models for knowledge distillation, compressing their capabilities into memory-efficient models. However, as these teacher models may stem from untrusted parties, distillation can raise unexpected security risks. In this paper, we investigate the security implications of knowledge distillation from backdoored teacher models. First, we show that prior backdoors mostly do not transfer onto student models. Our key insight is that this is because existing LLM backdooring methods choose trigger tokens that rarely occur in usual contexts. We argue that this underestimates the security risks of knowledge distillation and introduce a new backdooring technique, T-MTB, that enables the construction and study of transferable backdoors. T-MTB carefully constructs a composite backdoor trigger, made up of several specific tokens that often occur individually in anticipated distillation datasets. As such, the poisoned teacher remains stealthy, while during distillation the individual presence of these tokens provides enough signal for the backdoor to transfer onto the student. Using T-MTB, we demonstrate and extensively study the security risks of transferable backdoors across two attack scenarios, jailbreaking and content modulation, and across four model families of LLMs.</li>
</ul>

<h3>Title: Descriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving</h3>
<ul>
<li><strong>Authors: </strong>Sanjay Kumar, Tim Brophy, Reenu Mohandas, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella, Ciaran Eising</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18552">https://arxiv.org/abs/2510.18552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18552">https://arxiv.org/pdf/2510.18552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18552]] Descriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving(https://arxiv.org/abs/2510.18552)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust perception in automated driving requires reliable performance under adverse conditions, where sensors may be affected by partial failures or environmental occlusions. Although existing autonomous driving datasets inherently contain sensor noise and environmental variability, very few enable controlled, parameterised, and reproducible degradations across multiple sensing modalities. This gap limits the ability to systematically evaluate how perception and fusion architectures perform under well-defined adverse conditions. To address this limitation, we introduce the Occluded nuScenes Dataset, a novel extension of the widely used nuScenes benchmark. For the camera modality, we release both the full and mini versions with four types of occlusions, two adapted from public implementations and two newly designed. For radar and LiDAR, we provide parameterised occlusion scripts that implement three types of degradations each, enabling flexible and repeatable generation of occluded data. This resource supports consistent, reproducible evaluation of perception models under partial sensor failures and environmental interference. By releasing the first multi-sensor occlusion dataset with controlled and reproducible degradations, we aim to advance research on robust sensor fusion, resilience analysis, and safety-critical perception in automated driving.</li>
</ul>

<h3>Title: Building Trust in Clinical LLMs: Bias Analysis and Dataset Transparency</h3>
<ul>
<li><strong>Authors: </strong>Svetlana Maslenkova, Clement Christophe, Marco AF Pimentel, Tathagata Raha, Muhammad Umar Salman, Ahmed Al Mahrooqi, Avani Gupta, Shadab Khan, Ronnie Rajan, Praveenkumar Kanithi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18556">https://arxiv.org/abs/2510.18556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18556">https://arxiv.org/pdf/2510.18556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18556]] Building Trust in Clinical LLMs: Bias Analysis and Dataset Transparency(https://arxiv.org/abs/2510.18556)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large language models offer transformative potential for healthcare, yet their responsible and equitable development depends critically on a deeper understanding of how training data characteristics influence model behavior, including the potential for bias. Current practices in dataset curation and bias assessment often lack the necessary transparency, creating an urgent need for comprehensive evaluation frameworks to foster trust and guide improvements. In this study, we present an in-depth analysis of potential downstream biases in clinical language models, with a focus on differential opioid prescription tendencies across diverse demographic groups, such as ethnicity, gender, and age. As part of this investigation, we introduce HC4: Healthcare Comprehensive Commons Corpus, a novel and extensively curated pretraining dataset exceeding 89 billion tokens. Our evaluation leverages both established general benchmarks and a novel, healthcare-specific methodology, offering crucial insights to support fairness and safety in clinical AI applications.</li>
</ul>

<h3>Title: RAISE: A Unified Framework for Responsible AI Scoring and Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Loc Phuc Truong Nguyen, Hung Thanh Do</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18559">https://arxiv.org/abs/2510.18559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18559">https://arxiv.org/pdf/2510.18559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18559]] RAISE: A Unified Framework for Responsible AI Scoring and Evaluation(https://arxiv.org/abs/2510.18559)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, explainability, transformer</a></li>
<li><strong>Abstract: </strong>As AI systems enter high-stakes domains, evaluation must extend beyond predictive accuracy to include explainability, fairness, robustness, and sustainability. We introduce RAISE (Responsible AI Scoring and Evaluation), a unified framework that quantifies model performance across these four dimensions and aggregates them into a single, holistic Responsibility Score. We evaluated three deep learning models: a Multilayer Perceptron (MLP), a Tabular ResNet, and a Feature Tokenizer Transformer, on structured datasets from finance, healthcare, and socioeconomics. Our findings reveal critical trade-offs: the MLP demonstrated strong sustainability and robustness, the Transformer excelled in explainability and fairness at a very high environmental cost, and the Tabular ResNet offered a balanced profile. These results underscore that no single model dominates across all responsibility criteria, highlighting the necessity of multi-dimensional evaluation for responsible model selection. Our implementation is available at: this https URL.</li>
</ul>

<h3>Title: Large language models for folktale type automation based on motifs: Cinderella case study</h3>
<ul>
<li><strong>Authors: </strong>Tjaša Arčon, Marko Robnik-Šikonja, Polona Tratnik</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18561">https://arxiv.org/abs/2510.18561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18561">https://arxiv.org/pdf/2510.18561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18561]] Large language models for folktale type automation based on motifs: Cinderella case study(https://arxiv.org/abs/2510.18561)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Artificial intelligence approaches are being adapted to many research areas, including digital humanities. We built a methodology for large-scale analyses in folkloristics. Using machine learning and natural language processing, we automatically detected motifs in a large collection of Cinderella variants and analysed their similarities and differences with clustering and dimensionality reduction. The results show that large language models detect complex interactions in tales, enabling computational analysis of extensive text collections and facilitating cross-lingual comparisons.</li>
</ul>

<h3>Title: The Trust Paradox in LLM-Based Multi-Agent Systems: When Collaboration Becomes a Security Vulnerability</h3>
<ul>
<li><strong>Authors: </strong>Zijie Xu, Minfeng Qi, Shiqing Wu, Lefeng Zhang, Qiwen Wei, Han He, Ningran Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18563">https://arxiv.org/abs/2510.18563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18563">https://arxiv.org/pdf/2510.18563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18563]] The Trust Paradox in LLM-Based Multi-Agent Systems: When Collaboration Becomes a Security Vulnerability(https://arxiv.org/abs/2510.18563)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, large language model</a></li>
<li><strong>Abstract: </strong>Multi-agent systems powered by large language models are advancing rapidly, yet the tension between mutual trust and security remains underexplored. We introduce and empirically validate the Trust-Vulnerability Paradox (TVP): increasing inter-agent trust to enhance coordination simultaneously expands risks of over-exposure and over-authorization. To investigate this paradox, we construct a scenario-game dataset spanning 3 macro scenes and 19 sub-scenes, and run extensive closed-loop interactions with trust explicitly parameterized. Using Minimum Necessary Information (MNI) as the safety baseline, we propose two unified metrics: Over-Exposure Rate (OER) to detect boundary violations, and Authorization Drift (AD) to capture sensitivity to trust levels. Results across multiple model backends and orchestration frameworks reveal consistent trends: higher trust improves task success but also heightens exposure risks, with heterogeneous trust-to-risk mappings across systems. We further examine defenses such as Sensitive Information Repartitioning and Guardian-Agent enablement, both of which reduce OER and attenuate AD. Overall, this study formalizes TVP, establishes reproducible baselines with unified metrics, and demonstrates that trust must be modeled and scheduled as a first-class security variable in multi-agent system design.</li>
</ul>

<h3>Title: Privacy-Preserving Healthcare Data in IoT: A Synergistic Approach with Deep Learning and Blockchain</h3>
<ul>
<li><strong>Authors: </strong>Behnam Rezaei Bezanjani, Seyyed Hamid Ghafouri, Reza Gholamrezaei</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18568">https://arxiv.org/abs/2510.18568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18568">https://arxiv.org/pdf/2510.18568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18568]] Privacy-Preserving Healthcare Data in IoT: A Synergistic Approach with Deep Learning and Blockchain(https://arxiv.org/abs/2510.18568)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack</a></li>
<li><strong>Abstract: </strong>The integration of Internet of Things (IoT) devices in healthcare has revolutionized patient care by enabling real-time monitoring, personalized treatments, and efficient data management. However, this technological advancement introduces significant security risks, particularly concerning the confidentiality, integrity, and availability of sensitive medical data. Traditional security measures are often insufficient to address the unique challenges posed by IoT environments, such as heterogeneity, resource constraints, and the need for real-time processing. To tackle these challenges, we propose a comprehensive three-phase security framework designed to enhance the security and reliability of IoT-enabled healthcare systems. In the first phase, the framework assesses the reliability of IoT devices using a reputation-based trust estimation mechanism, which combines device behavior analytics with off-chain data storage to ensure scalability. The second phase integrates blockchain technology with a lightweight proof-of-work mechanism, ensuring data immutability, secure communication, and resistance to unauthorized access. The third phase employs a lightweight Long Short-Term Memory (LSTM) model for anomaly detection and classification, enabling real-time identification of cyber threats. Simulation results demonstrate that the proposed framework outperforms existing methods, achieving a 2% increase in precision, accuracy, and recall, a 5% higher attack detection rate, and a 3% reduction in false alarm rate. These improvements highlight the framework's ability to address critical security concerns while maintaining scalability and real-time performance.</li>
</ul>

<h3>Title: Forward to Hell? On the Potentials of Misusing Transparent DNS Forwarders in Reflective Amplification Attacks</h3>
<ul>
<li><strong>Authors: </strong>Maynard Koch, Florian Dolzmann, Thomas C. Schmidt, Matthias Wählisch</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18572">https://arxiv.org/abs/2510.18572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18572">https://arxiv.org/pdf/2510.18572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18572]] Forward to Hell? On the Potentials of Misusing Transparent DNS Forwarders in Reflective Amplification Attacks(https://arxiv.org/abs/2510.18572)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>The DNS infrastructure is infamous for facilitating reflective amplification attacks. Various countermeasures such as server shielding, access control, rate limiting, and protocol restrictions have been implemented. Still, the threat remains throughout the deployment of DNS servers. In this paper, we report on and evaluate the often unnoticed threat that derives from transparent DNS forwarders, a widely deployed, incompletely functional set of DNS components. Transparent DNS forwarders transfer DNS requests without rebuilding packets with correct source addresses. As such, transparent forwarders feed DNS requests into (mainly powerful and anycasted) open recursive resolvers, which thereby can be misused to participate unwillingly in distributed reflective amplification attacks. We show how transparent forwarders raise severe threats to the Internet infrastructure. They easily circumvent rate limiting and achieve an additional, scalable impact via the DNS anycast infrastructure. We empirically verify this scaling behavior up to a factor of 14. Transparent forwarders can also assist in bypassing firewall rules that protect recursive resolvers, making these shielded infrastructure entities part of the global DNS attack surface.</li>
</ul>

<h3>Title: CLASP: Cost-Optimized LLM-based Agentic System for Phishing Detection</h3>
<ul>
<li><strong>Authors: </strong>Fouad Trad, Ali Chehab</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18585">https://arxiv.org/abs/2510.18585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18585">https://arxiv.org/pdf/2510.18585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18585]] CLASP: Cost-Optimized LLM-based Agentic System for Phishing Detection(https://arxiv.org/abs/2510.18585)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Phishing websites remain a significant cybersecurity threat, necessitating accurate and cost-effective detection mechanisms. In this paper, we present CLASP, a novel system that effectively identifies phishing websites by leveraging multiple intelligent agents, built using large language models (LLMs), to analyze different aspects of a web resource. The system processes URLs or QR codes, employing specialized LLM-based agents that evaluate the URL structure, webpage screenshot, and HTML content to predict potential phishing threats. To optimize performance while minimizing operational costs, we experimented with multiple combination strategies for agent-based analysis, ultimately designing a strategic combination that ensures the per-website evaluation expense remains minimal without compromising detection accuracy. We tested various LLMs, including Gemini 1.5 Flash and GPT-4o mini, to build these agents and found that Gemini 1.5 Flash achieved the best performance with an F1 score of 83.01% on a newly curated dataset. Also, the system maintained an average processing time of 2.78 seconds per website and an API cost of around $3.18 per 1,000 websites. Moreover, CLASP surpasses leading previous solutions, achieving over 40% higher recall and a 20% improvement in F1 score for phishing detection on the collected dataset. To support further research, we have made our dataset publicly available, supporting the development of more advanced phishing detection systems.</li>
</ul>

<h3>Title: Robustness Verification of Graph Neural Networks Via Lightweight Satisfiability Testing</h3>
<ul>
<li><strong>Authors: </strong>Chia-Hsuan Lu, Tony Tan, Michael Benedikt</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18591">https://arxiv.org/abs/2510.18591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18591">https://arxiv.org/pdf/2510.18591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18591]] Robustness Verification of Graph Neural Networks Via Lightweight Satisfiability Testing(https://arxiv.org/abs/2510.18591)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs) are the predominant architecture for learning over graphs. As with any machine learning model, and important issue is the detection of adversarial attacks, where an adversary can change the output with a small perturbation of the input. Techniques for solving the adversarial robustness problem - determining whether such an attack exists - were originally developed for image classification, but there are variants for many other machine learning architectures. In the case of graph learning, the attack model usually considers changes to the graph structure in addition to or instead of the numerical features of the input, and the state of the art techniques in the area proceed via reduction to constraint solving, working on top of powerful solvers, e.g. for mixed integer programming. We show that it is possible to improve on the state of the art in structural robustness by replacing the use of powerful solvers by calls to efficient partial solvers, which run in polynomial time but may be incomplete. We evaluate our tool RobLight on a diverse set of GNN variants and datasets.</li>
</ul>

<h3>Title: Evaluating Large Language Models in detecting Secrets in Android Apps</h3>
<ul>
<li><strong>Authors: </strong>Marco Alecci, Jordan Samhi, Tegawendé F. Bissyandé, Jacques Klein</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18601">https://arxiv.org/abs/2510.18601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18601">https://arxiv.org/pdf/2510.18601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18601]] Evaluating Large Language Models in detecting Secrets in Android Apps(https://arxiv.org/abs/2510.18601)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Mobile apps often embed authentication secrets, such as API keys, tokens, and client IDs, to integrate with cloud services. However, developers often hardcode these credentials into Android apps, exposing them to extraction through reverse engineering. Once compromised, adversaries can exploit secrets to access sensitive data, manipulate resources, or abuse APIs, resulting in significant security and financial risks. Existing detection approaches, such as regex-based analysis, static analysis, and machine learning, are effective for identifying known patterns but are fundamentally limited: they require prior knowledge of credential structures, API signatures, or training data. In this paper, we propose SecretLoc, an LLM-based approach for detecting hardcoded secrets in Android apps. SecretLoc goes beyond pattern matching; it leverages contextual and structural cues to identify secrets without relying on predefined patterns or labeled training sets. Using a benchmark dataset from the literature, we demonstrate that SecretLoc detects secrets missed by regex-, static-, and ML-based methods, including previously unseen types of secrets. In total, we discovered 4828 secrets that were undetected by existing approaches, discovering more than 10 "new" types of secrets, such as OpenAI API keys, GitHub Access Tokens, RSA private keys, and JWT tokens, and more. We further extend our analysis to newly crawled apps from Google Play, where we uncovered and responsibly disclosed additional hardcoded secrets. Across a set of 5000 apps, we detected secrets in 2124 apps (42.5%), several of which were confirmed and remediated by developers after we contacted them. Our results reveal a dual-use risk: if analysts can uncover these secrets with LLMs, so can attackers. This underscores the urgent need for proactive secret management and stronger mitigation practices across the mobile ecosystem.</li>
</ul>

<h3>Title: Unrolled-SINDy: A Stable Explicit Method for Non linear PDE Discovery from Sparsely Sampled Data</h3>
<ul>
<li><strong>Authors: </strong>Fayad Ali Banna, Antoine Caradot, Eduardo Brandao, Jean-Philippe Colombier, Rémi Emonet, Marc Sebban</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18611">https://arxiv.org/abs/2510.18611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18611">https://arxiv.org/pdf/2510.18611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18611]] Unrolled-SINDy: A Stable Explicit Method for Non linear PDE Discovery from Sparsely Sampled Data(https://arxiv.org/abs/2510.18611)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Identifying from observation data the governing differential equations of a physical dynamics is a key challenge in machine learning. Although approaches based on SINDy have shown great promise in this area, they still fail to address a whole class of real world problems where the data is sparsely sampled in time. In this article, we introduce Unrolled-SINDy, a simple methodology that leverages an unrolling scheme to improve the stability of explicit methods for PDE discovery. By decorrelating the numerical time step size from the sampling rate of the available data, our approach enables the recovery of equation parameters that would not be the minimizers of the original SINDy optimization problem due to large local truncation errors. Our method can be exploited either through an iterative closed-form approach or by a gradient descent scheme. Experiments show the versatility of our method. On both traditional SINDy and state-of-the-art noise-robust iNeuralSINDy, with different numerical schemes (Euler, RK4), our proposed unrolling scheme allows to tackle problems not accessible to non-unrolled methods.</li>
</ul>

<h3>Title: DRsam: Detection of Fault-Based Microarchitectural Side-Channel Attacks in RISC-V Using Statistical Preprocessing and Association Rule Mining</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Hassan (1), Maria Mushtaq (2), Jaan Raik (1), Tara Ghasempouri (1) ((1) Tallinn University of Technology, (2) Telecom Paris (Institut Polytechnique de Paris))</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18612">https://arxiv.org/abs/2510.18612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18612">https://arxiv.org/pdf/2510.18612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18612]] DRsam: Detection of Fault-Based Microarchitectural Side-Channel Attacks in RISC-V Using Statistical Preprocessing and Association Rule Mining(https://arxiv.org/abs/2510.18612)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>RISC-V processors are becoming ubiquitous in critical applications, but their susceptibility to microarchitectural side-channel attacks is a serious concern. Detection of microarchitectural attacks in RISC-V is an emerging research topic that is relatively underexplored, compared to x86 and ARM. The first line of work to detect flush+fault-based microarchitectural attacks in RISC-V leverages Machine Learning (ML) models, yet it leaves several practical aspects that need further investigation. To address overlooked issues, we leveraged gem5 and propose a new detection method combining statistical preprocessing and association rule mining having reconfiguration capabilities to generalize the detection method for any microarchitectural attack. The performance comparison with state-of-the-art reveals that the proposed detection method achieves up to 5.15% increase in accuracy, 7% rise in precision, and 3.91% improvement in recall under the cryptographic, computational, and memory-intensive workloads alongside its flexibility to detect new variant of flush+fault attack. Moreover, as the attack detection relies on association rules, their human-interpretable nature provides deep insight to understand microarchitectural behavior during the execution of attack and benign applications.</li>
</ul>

<h3>Title: Qatsi: Stateless Secret Generation via Hierarchical Memory-Hard Key Derivation</h3>
<ul>
<li><strong>Authors: </strong>René Coignard, Anton Rygin</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18614">https://arxiv.org/abs/2510.18614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18614">https://arxiv.org/pdf/2510.18614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18614]] Qatsi: Stateless Secret Generation via Hierarchical Memory-Hard Key Derivation(https://arxiv.org/abs/2510.18614)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>We present Qatsi, a hierarchical key derivation scheme using Argon2id that generates reproducible cryptographic secrets without persistent storage. The system eliminates vault-based attack surfaces by deriving all secrets deterministically from a single high-entropy master secret and contextual layers. Outputs achieve 103-312 bits of entropy through memory-hard derivation (64-128 MiB, 16-32 iterations) and provably uniform rejection sampling over 7776-word mnemonics or 90-character passwords. We formalize the hierarchical construction, prove output uniformity, and quantify GPU attack costs: $2.4 \times 10^{16}$ years for 80-bit master secrets on single-GPU adversaries under Paranoid parameters (128 MiB memory). The implementation in Rust provides automatic memory zeroization, compile-time wordlist integrity verification, and comprehensive test coverage. Reference benchmarks on Apple M1 Pro (2021) demonstrate practical usability with 544 ms Standard mode and 2273 ms Paranoid mode single-layer derivations. Qatsi targets air-gapped systems and master credential generation where stateless reproducibility outweighs rotation flexibility.</li>
</ul>

<h3>Title: A Rectification-Based Approach for Distilling Boosted Trees into Decision Trees</h3>
<ul>
<li><strong>Authors: </strong>Gilles Audemard, Sylvie Coste-Marquis, Pierre Marquis, Mehdi Sabiri, Nicolas Szczepanski</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18615">https://arxiv.org/abs/2510.18615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18615">https://arxiv.org/pdf/2510.18615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18615]] A Rectification-Based Approach for Distilling Boosted Trees into Decision Trees(https://arxiv.org/abs/2510.18615)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We present a new approach for distilling boosted trees into decision trees, in the objective of generating an ML model offering an acceptable compromise in terms of predictive performance and interpretability. We explain how the correction approach called rectification can be used to implement such a distillation process. We show empirically that this approach provides interesting results, in comparison with an approach to distillation achieved by retraining the model.</li>
</ul>

<h3>Title: C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression</h3>
<ul>
<li><strong>Authors: </strong>Baptiste Bauvin, Loïc Baret, Ola Ahmad</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18636">https://arxiv.org/abs/2510.18636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18636">https://arxiv.org/pdf/2510.18636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18636]] C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression(https://arxiv.org/abs/2510.18636)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer</a></li>
<li><strong>Abstract: </strong>Neural network compression has gained increasing attention in recent years, particularly in computer vision applications, where the need for model reduction is crucial for overcoming deployment constraints. Pruning is a widely used technique that prompts sparsity in model structures, e.g. weights, neurons, and layers, reducing size and inference costs. Structured pruning is especially important as it allows for the removal of entire structures, which further accelerates inference time and reduces memory overhead. However, it can be computationally expensive, requiring iterative retraining and optimization. To overcome this problem, recent methods considered one-shot setting, which applies pruning directly at post-training. Unfortunately, they often lead to a considerable drop in performance. In this paper, we focus on this issue by proposing a novel one-shot pruning framework that relies on explainable deep learning. First, we introduce a causal-aware pruning approach that leverages cause-effect relations between model predictions and structures in a progressive pruning process. It allows us to efficiently reduce the size of the network, ensuring that the removed structures do not deter the performance of the model. Then, through experiments conducted on convolution neural network and vision transformer baselines, pre-trained on classification tasks, we demonstrate that our method consistently achieves substantial reductions in model size, with minimal impact on performance, and without the need for fine-tuning. Overall, our approach outperforms its counterparts, offering the best trade-off. Our code is available on GitHub.</li>
</ul>

<h3>Title: ε-Seg: Sparsely Supervised Semantic Segmentation of Microscopy Data</h3>
<ul>
<li><strong>Authors: </strong>Sheida Rahnamai Kordasiabi, Damian Dalle Nogare, Florian Jug</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18637">https://arxiv.org/abs/2510.18637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18637">https://arxiv.org/pdf/2510.18637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18637]] ε-Seg: Sparsely Supervised Semantic Segmentation of Microscopy Data(https://arxiv.org/abs/2510.18637)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation of electron microscopy (EM) images of biological samples remains a challenge in the life sciences. EM data captures details of biological structures, sometimes with such complexity that even human observers can find it overwhelming. We introduce {\epsilon}-Seg, a method based on hierarchical variational autoencoders (HVAEs), employing center-region masking, sparse label contrastive learning (CL), a Gaussian mixture model (GMM) prior, and clustering-free label prediction. Center-region masking and the inpainting loss encourage the model to learn robust and representative embeddings to distinguish the desired classes, even if training labels are sparse (0.05% of the total image data or less). For optimal performance, we employ CL and a GMM prior to shape the latent space of the HVAE such that encoded input patches tend to cluster wrt. the semantic classes we wish to distinguish. Finally, instead of clustering latent embeddings for semantic segmentation, we propose a MLP semantic segmentation head to directly predict class labels from latent embeddings. We show empirical results of {\epsilon}-Seg and baseline methods on 2 dense EM datasets of biological tissues and demonstrate the applicability of our method also on fluorescence microscopy data. Our results show that {\epsilon}-Seg is capable of achieving competitive sparsely-supervised segmentation results on complex biological image data, even if only limited amounts of training labels are available.</li>
</ul>

<h3>Title: Optimality and NP-Hardness of Transformers in Learning Markovian Dynamical Functions</h3>
<ul>
<li><strong>Authors: </strong>Yanna Ding, Songtao Lu, Yingdong Lu, Tomasz Nowicki, Jianxi Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18638">https://arxiv.org/abs/2510.18638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18638">https://arxiv.org/pdf/2510.18638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18638]] Optimality and NP-Hardness of Transformers in Learning Markovian Dynamical Functions(https://arxiv.org/abs/2510.18638)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer architectures can solve unseen tasks based on input-output pairs in a given prompt due to in-context learning (ICL). Existing theoretical studies on ICL have mainly focused on linear regression tasks, often with i.i.d. inputs. To understand how transformers express ICL when modeling dynamics-driven functions, we investigate Markovian function learning through a structured ICL setup, where we characterize the loss landscape to reveal underlying optimization behaviors. Specifically, we (1) provide the closed-form expression of the global minimizer (in an enlarged parameter space) for a single-layer linear self-attention (LSA) model; (2) prove that recovering transformer parameters that realize the optimal solution is NP-hard in general, revealing a fundamental limitation of one-layer LSA in representing structured dynamical functions; and (3) supply a novel interpretation of a multilayer LSA as performing preconditioned gradient descent to optimize multiple objectives beyond the square loss. These theoretical results are numerically validated using simplified transformers.</li>
</ul>

<h3>Title: Informed Learning for Estimating Drought Stress at Fine-Scale Resolution Enables Accurate Yield Prediction</h3>
<ul>
<li><strong>Authors: </strong>Miro Miranda, Marcela Charfuelan, Matias Valdenegro Toro, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18648">https://arxiv.org/abs/2510.18648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18648">https://arxiv.org/pdf/2510.18648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18648]] Informed Learning for Estimating Drought Stress at Fine-Scale Resolution Enables Accurate Yield Prediction(https://arxiv.org/abs/2510.18648)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, explainability, transformer</a></li>
<li><strong>Abstract: </strong>Water is essential for agricultural productivity. Assessing water shortages and reduced yield potential is a critical factor in decision-making for ensuring agricultural productivity and food security. Crop simulation models, which align with physical processes, offer intrinsic explainability but often perform poorly. Conversely, machine learning models for crop yield modeling are powerful and scalable, yet they commonly operate as black boxes and lack adherence to the physical principles of crop growth. This study bridges this gap by coupling the advantages of both worlds. We postulate that the crop yield is inherently defined by the water availability. Therefore, we formulate crop yield as a function of temporal water scarcity and predict both the crop drought stress and the sensitivity to water scarcity at fine-scale resolution. Sequentially modeling the crop yield response to water enables accurate yield prediction. To enforce physical consistency, a novel physics-informed loss function is proposed. We leverage multispectral satellite imagery, meteorological data, and fine-scale yield data. Further, to account for the uncertainty within the model, we build upon a deep ensemble approach. Our method surpasses state-of-the-art models like LSTM and Transformers in crop yield prediction with a coefficient of determination ($R^2$-score) of up to 0.82 while offering high explainability. This method offers decision support for industry, policymakers, and farmers in building a more resilient agriculture in times of changing climate conditions.</li>
</ul>

<h3>Title: Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued Matrix Compression</h3>
<ul>
<li><strong>Authors: </strong>Kyo Kuroki, Yasuyuki Okoshi, Thiem Van Chu, Kazushi Kawamura, Masato Motomura</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18650">https://arxiv.org/abs/2510.18650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18650">https://arxiv.org/pdf/2510.18650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18650]] Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued Matrix Compression(https://arxiv.org/abs/2510.18650)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free, transformer</a></li>
<li><strong>Abstract: </strong>This paper proposes a novel matrix quantization method, Binary Quadratic Quantization (BQQ). In contrast to conventional first-order quantization approaches, such as uniform quantization and binary coding quantization, that approximate real-valued matrices via linear combinations of binary bases, BQQ leverages the expressive power of binary quadratic expressions while maintaining an extremely compact data format. We validate our approach with two experiments: a matrix compression benchmark and post-training quantization (PTQ) on pretrained Vision Transformer-based models. Experimental results demonstrate that BQQ consistently achieves a superior trade-off between memory efficiency and reconstruction error than conventional methods for compressing diverse matrix data. It also delivers strong PTQ performance, even though we neither target state-of-the-art PTQ accuracy under tight memory constraints nor rely on PTQ-specific binary matrix optimization. For example, our proposed method outperforms the state-of-the-art PTQ method by up to 2.2\% and 59.1% on the ImageNet dataset under the calibration-based and data-free scenarios, respectively, with quantization equivalent to 2 bits. These findings highlight the surprising effectiveness of binary quadratic expressions for efficient matrix approximation and neural network compression.</li>
</ul>

<h3>Title: Prototyping an End-to-End Multi-Modal Tiny-CNN for Cardiovascular Sensor Patches</h3>
<ul>
<li><strong>Authors: </strong>Mustafa Fuad Rifet Ibrahim, Tunc Alkanat, Maurice Meijer, Felix Manthey, Alexander Schlaefer, Peer Stelldinger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18668">https://arxiv.org/abs/2510.18668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18668">https://arxiv.org/pdf/2510.18668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18668]] Prototyping an End-to-End Multi-Modal Tiny-CNN for Cardiovascular Sensor Patches(https://arxiv.org/abs/2510.18668)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The vast majority of cardiovascular diseases may be preventable if early signs and risk factors are detected. Cardiovascular monitoring with body-worn sensor devices like sensor patches allows for the detection of such signs while preserving the freedom and comfort of patients. However, the analysis of the sensor data must be robust, reliable, efficient, and highly accurate. Deep learning methods can automate data interpretation, reducing the workload of clinicians. In this work, we analyze the feasibility of applying deep learning models to the classification of synchronized electrocardiogram (ECG) and phonocardiogram (PCG) recordings on resource-constrained medical edge devices. We propose a convolutional neural network with early fusion of data to solve a binary classification problem. We train and validate our model on the synchronized ECG and PCG recordings from the Physionet Challenge 2016 dataset. Our approach reduces memory footprint and compute cost by three orders of magnitude compared to the state-of-the-art while maintaining competitive accuracy. We demonstrate the applicability of our proposed model on medical edge devices by analyzing energy consumption on a microcontroller and an experimental sensor device setup, confirming that on-device inference can be more energy-efficient than continuous data streaming.</li>
</ul>

<h3>Title: Beyond the Pipeline: Analyzing Key Factors in End-to-End Deep Learning for Historical Writer Identification</h3>
<ul>
<li><strong>Authors: </strong>Hanif Rasyidi, Moshiur Farazi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18671">https://arxiv.org/abs/2510.18671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18671">https://arxiv.org/pdf/2510.18671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18671]] Beyond the Pipeline: Analyzing Key Factors in End-to-End Deep Learning for Historical Writer Identification(https://arxiv.org/abs/2510.18671)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>This paper investigates various factors that influence the performance of end-to-end deep learning approaches for historical writer identification (HWI), a task that remains challenging due to the diversity of handwriting styles, document degradation, and the limited number of labelled samples per writer. These conditions often make accurate recognition difficult, even for human experts. Traditional HWI methods typically rely on handcrafted image processing and clustering techniques, which tend to perform well on small and carefully curated datasets. In contrast, end-to-end pipelines aim to automate the process by learning features directly from document images. However, our experiments show that many of these models struggle to generalise in more realistic, document-level settings, especially under zero-shot scenarios where writers in the test set are not present in the training data. We explore different combinations of pre-processing methods, backbone architectures, and post-processing strategies, including text segmentation, patch sampling, and feature aggregation. The results suggest that most configurations perform poorly due to weak capture of low-level visual features, inconsistent patch representations, and high sensitivity to content noise. Still, we identify one end-to-end setup that achieves results comparable to the top-performing system, despite using a simpler design. These findings point to key challenges in building robust end-to-end systems and offer insight into design choices that improve performance in historical document writer identification.</li>
</ul>

<h3>Title: Reasoning Language Model Inference Serving Unveiled: An Empirical Study</h3>
<ul>
<li><strong>Authors: </strong>Qi Li, Junpan Wu, Xiang Liu, Yuxin Wang, Zeyu Li, Zhenheng Tang, Yuhan Chen, Shaohuai Shi, Xiaowen Chu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18672">https://arxiv.org/abs/2510.18672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18672">https://arxiv.org/pdf/2510.18672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18672]] Reasoning Language Model Inference Serving Unveiled: An Empirical Study(https://arxiv.org/abs/2510.18672)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The reasoning large language model (RLLM) has been proven competitive in solving complex reasoning tasks such as mathematics, coding, compared to general LLM. However, the serving performance and behavior of RLLM remains unexplored, which may undermine the deployment and utilization of RLLM in real-world scenario. To close this gap, in this paper, we conduct a comprehensive study of RLLM service. We first perform a pilot study on comparing the serving performance between RLLM and traditional LLM and reveal that there are several distinct differences regarding serving behavior: (1) significant memory usage and fluctuations; (2) straggler requests; (3) adaptive running time; (4) domain preference. Then we further investigate whether existing inference optimization techniques are valid for RLLM. Our main takeaways are that model quantization methods and speculative decoding can improve service system efficiency with small compromise to RLLM accuracy, while prefix caching, KV cache quantization may even degrade accuracy or serving performance for small RLLM. Lastly, we conduct evaluation under real world workload modeled by Gamma distribution to verify our findings. Empirical results of real world workload evaluation across different dataset are aligned with our main findings regarding RLLM serving. We hope our work can provide the research community and industry with insights to advance RLLM inference serving.</li>
</ul>

<h3>Title: Exploring Membership Inference Vulnerabilities in Clinical Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alexander Nemecek, Zebin Yun, Zahra Rahmani, Yaniv Harel, Vipin Chaudhary, Mahmood Sharif, Erman Ayday</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18674">https://arxiv.org/abs/2510.18674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18674">https://arxiv.org/pdf/2510.18674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18674]] Exploring Membership Inference Vulnerabilities in Clinical Large Language Models(https://arxiv.org/abs/2510.18674)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become progressively more embedded in clinical decision-support, documentation, and patient-information systems, ensuring their privacy and trustworthiness has emerged as an imperative challenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic health record (EHR) data improves domain alignment but also raises the risk of exposing patient information through model behaviors. In this work-in-progress, we present an exploratory empirical study on membership inference vulnerabilities in clinical LLMs, focusing on whether adversaries can infer if specific patient records were used during model training. Using a state-of-the-art clinical question-answering model, Llemr, we evaluate both canonical loss-based attacks and a domain-motivated paraphrasing-based perturbation strategy that more realistically reflects clinical adversarial conditions. Our preliminary findings reveal limited but measurable membership leakage, suggesting that current clinical LLMs provide partial resistance yet remain susceptible to subtle privacy risks that could undermine trust in clinical AI adoption. These results motivate continued development of context-aware, domain-specific privacy evaluations and defenses such as differential privacy fine-tuning and paraphrase-aware training, to strengthen the security and trustworthiness of healthcare AI systems.</li>
</ul>

<h3>Title: MLMA: Towards Multilingual with Mamba Based Architectures</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Nabih Ali, Daniele Falavigna, Alessio Brutti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18684">https://arxiv.org/abs/2510.18684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18684">https://arxiv.org/pdf/2510.18684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18684]] MLMA: Towards Multilingual with Mamba Based Architectures(https://arxiv.org/abs/2510.18684)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Multilingual automatic speech recognition (ASR) remains a challenging task, especially when balancing performance across high- and low-resource languages. Recent advances in sequence modeling suggest that architectures beyond Transformers may offer better scalability and efficiency. In this work, we introduce MLMA (Multilingual Language Modeling with Mamba for ASR), a new approach that leverages the Mamba architecture--an efficient state-space model optimized for long-context sequence processing--for multilingual ASR. Using Mamba, MLMA implicitly incorporates language-aware conditioning and shared representations to support robust recognition across diverse languages. Experiments on standard multilingual benchmarks show that MLMA achieves competitive performance compared to Transformer-based architectures. These results highlight Mamba's potential as a strong backbone for scalable, efficient, and accurate multilingual speech recognition.</li>
</ul>

<h3>Title: MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Weinan Jia, Yuning Lu, Mengqi Huang, Hualiang Wang, Binyuan Huang, Nan Chen, Mu Liu, Jidong Jiang, Zhendong Mao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18692">https://arxiv.org/abs/2510.18692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18692">https://arxiv.org/pdf/2510.18692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18692]] MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation(https://arxiv.org/abs/2510.18692)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full attention with sequence length. Since attention is highly redundant, outputs are dominated by a small subset of query-key pairs. Existing sparse methods rely on blockwise coarse estimation, whose accuracy-efficiency trade-offs are constrained by block size. This paper introduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention that uses a lightweight, learnable token router to precisely match tokens without blockwise estimation. Through semantic-aware routing, MoGA enables effective long-range interactions. As a kernel-free method, MoGA integrates seamlessly with modern attention stacks, including FlashAttention and sequence parallelism. Building on MoGA, we develop an efficient long video generation model that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps, with a context length of approximately 580k. Comprehensive experiments on various video generation tasks validate the effectiveness of our approach.</li>
</ul>

<h3>Title: UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yibin Wang, Zhimin Li, Yuhang Zang, Jiazi Bu, Yujie Zhou, Yi Xin, Junjun He, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18701">https://arxiv.org/abs/2510.18701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18701">https://arxiv.org/pdf/2510.18701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18701]] UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation(https://arxiv.org/abs/2510.18701)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent progress in text-to-image (T2I) generation underscores the importance of reliable benchmarks in evaluating how accurately generated images reflect the semantics of their textual prompt. However, (1) existing benchmarks lack the diversity of prompt scenarios and multilingual support, both essential for real-world applicability; (2) they offer only coarse evaluations across primary dimensions, covering a narrow range of sub-dimensions, and fall short in fine-grained sub-dimension assessment. To address these limitations, we introduce UniGenBench++, a unified semantic assessment benchmark for T2I generation. Specifically, it comprises 600 prompts organized hierarchically to ensure both coverage and efficiency: (1) spans across diverse real-world scenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively probes T2I models' semantic consistency over 10 primary and 27 sub evaluation criteria, with each prompt assessing multiple testpoints. To rigorously assess model robustness to variations in language and prompt length, we provide both English and Chinese versions of each prompt in short and long forms. Leveraging the general world knowledge and fine-grained image understanding capabilities of a closed-source Multi-modal Large Language Model (MLLM), i.e., Gemini-2.5-Pro, an effective pipeline is developed for reliable benchmark construction and streamlined model assessment. Moreover, to further facilitate community use, we train a robust evaluation model that enables offline assessment of T2I model outputs. Through comprehensive benchmarking of both open- and closed-sourced T2I models, we systematically reveal their strengths and weaknesses across various aspects.</li>
</ul>

<h3>Title: Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents</h3>
<ul>
<li><strong>Authors: </strong>Yiqi Lin, Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18703">https://arxiv.org/abs/2510.18703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18703">https://arxiv.org/pdf/2510.18703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18703]] Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents(https://arxiv.org/abs/2510.18703)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Contrastive vision-language models such as CLIP have demonstrated strong performance across a wide range of multimodal tasks by learning from aligned image-text pairs. However, their ability to handle complex, real-world web documents remains limited, particularly in scenarios where text and images are interleaved, loosely aligned, or embedded in visual form. To address these challenges, we propose Vision-Centric Contrastive Learning (VC2L), a unified framework that models text, images, and their combinations using a single vision transformer. VC2L operates entirely in pixel space by rendering all inputs, whether textual, visual, or combined, as images, thus eliminating the need for OCR, text tokenization, or modality fusion strategy. To capture complex cross-modal relationships in multimodal web documents, VC2L employs a snippet-level contrastive learning objective that aligns consecutive multimodal segments, leveraging the inherent coherence of documents without requiring explicitly paired image-text data. To assess the effectiveness of this approach, we introduce three retrieval benchmarks, AnyCIR, SeqCIR, and CSR, designed to evaluate cross-modal retrieval, fine-grained sequential understanding, and generalization to unseen data, respectively. Empirical results show that VC2L achieves competitive or superior performance compared to CLIP-style models on both the proposed benchmarks and established datasets such as M-BEIR and MTEB. These findings underscore the potential of multimodal web data as a valuable training resource for contrastive learning and illustrate the scalability of a unified, vision-centric approach for multimodal representation learning. Code and models are available at: this https URL.</li>
</ul>

<h3>Title: A Renaissance of Explicit Motion Information Mining from Transformers for Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Peiqin Zhuang, Lei Bai, Yichao Wu, Ding Liang, Luping Zhou, Yali Wang, Wanli Ouyang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18705">https://arxiv.org/abs/2510.18705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18705">https://arxiv.org/pdf/2510.18705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18705]] A Renaissance of Explicit Motion Information Mining from Transformers for Action Recognition(https://arxiv.org/abs/2510.18705)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recently, action recognition has been dominated by transformer-based methods, thanks to their spatiotemporal contextual aggregation capacities. However, despite the significant progress achieved on scene-related datasets, they do not perform well on motion-sensitive datasets due to the lack of elaborate motion modeling designs. Meanwhile, we observe that the widely-used cost volume in traditional action recognition is highly similar to the affinity matrix defined in self-attention, but equipped with powerful motion modeling capacities. In light of this, we propose to integrate those effective motion modeling properties into the existing transformer in a unified and neat way, with the proposal of the Explicit Motion Information Mining module (EMIM). In EMIM, we propose to construct the desirable affinity matrix in a cost volume style, where the set of key candidate tokens is sampled from the query-based neighboring area in the next frame in a sliding-window manner. Then, the constructed affinity matrix is used to aggregate contextual information for appearance modeling and is converted into motion features for motion modeling as well. We validate the motion modeling capacities of our method on four widely-used datasets, and our method performs better than existing state-of-the-art approaches, especially on motion-sensitive datasets, i.e., Something-Something V1 & V2.</li>
</ul>

<h3>Title: OmniCast: A Masked Latent Diffusion Model for Weather Forecasting Across Time Scales</h3>
<ul>
<li><strong>Authors: </strong>Tung Nguyen, Tuan Pham, Troy Arcomano, Veerabhadra Kotamarthi, Ian Foster, Sandeep Madireddy, Aditya Grover</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18707">https://arxiv.org/abs/2510.18707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18707">https://arxiv.org/pdf/2510.18707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18707]] OmniCast: A Masked Latent Diffusion Model for Weather Forecasting Across Time Scales(https://arxiv.org/abs/2510.18707)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Accurate weather forecasting across time scales is critical for anticipating and mitigating the impacts of climate change. Recent data-driven methods based on deep learning have achieved significant success in the medium range, but struggle at longer subseasonal-to-seasonal (S2S) horizons due to error accumulation in their autoregressive approach. In this work, we propose OmniCast, a scalable and skillful probabilistic model that unifies weather forecasting across timescales. OmniCast consists of two components: a VAE model that encodes raw weather data into a continuous, lower-dimensional latent space, and a diffusion-based transformer model that generates a sequence of future latent tokens given the initial conditioning tokens. During training, we mask random future tokens and train the transformer to estimate their distribution given conditioning and visible tokens using a per-token diffusion head. During inference, the transformer generates the full sequence of future tokens by iteratively unmasking random subsets of tokens. This joint sampling across space and time mitigates compounding errors from autoregressive approaches. The low-dimensional latent space enables modeling long sequences of future latent states, allowing the transformer to learn weather dynamics beyond initial conditions. OmniCast performs competitively with leading probabilistic methods at the medium-range timescale while being 10x to 20x faster, and achieves state-of-the-art performance at the subseasonal-to-seasonal scale across accuracy, physics-based, and probabilistic metrics. Furthermore, we demonstrate that OmniCast can generate stable rollouts up to 100 years ahead. Code and model checkpoints are available at this https URL.</li>
</ul>

<h3>Title: Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options</h3>
<ul>
<li><strong>Authors: </strong>Joongkyu Lee, Seouh-won Yi, Min-hwan Oh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18713">https://arxiv.org/abs/2510.18713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18713">https://arxiv.org/pdf/2510.18713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18713]] Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options(https://arxiv.org/abs/2510.18713)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged-motivated by PbRL's recent empirical success, particularly in aligning large language models (LLMs)-most existing studies focus only on pairwise comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024, Thekumparampil et al., 2024) have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve-and can even deteriorate-as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett-Luce (PL) model for ranking feedback over action subsets and propose M-AUPO, an algorithm that selects multiple actions by maximizing the average uncertainty within the offered subset. We prove that M-AUPO achieves a suboptimality gap of $\tilde{\mathcal{O}}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}} \right)$, where $T$ is the total number of rounds, $d$ is the feature dimension, and $|S_t|$ is the size of the subset at round $t$. This result shows that larger subsets directly lead to improved performance and, notably, the bound avoids the exponential dependence on the unknown parameter's norm, which was a fundamental limitation in most previous works. Moreover, we establish a near-matching lower bound of $\Omega \left( \frac{d}{K \sqrt{T}} \right)$, where $K$ is the maximum subset size. To the best of our knowledge, this is the first theoretical result in PbRL with ranking feedback that explicitly shows improved sample efficiency as a function of the subset size.</li>
</ul>

<h3>Title: PLANA3R: Zero-shot Metric Planar 3D Reconstruction via Feed-Forward Planar Splatting</h3>
<ul>
<li><strong>Authors: </strong>Changkun Liu, Bin Tan, Zeran Ke, Shangzhan Zhang, Jiachen Liu, Ming Qian, Nan Xue, Yujun Shen, Tristan Braud</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18714">https://arxiv.org/abs/2510.18714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18714">https://arxiv.org/pdf/2510.18714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18714]] PLANA3R: Zero-shot Metric Planar 3D Reconstruction via Feed-Forward Planar Splatting(https://arxiv.org/abs/2510.18714)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>This paper addresses metric 3D reconstruction of indoor scenes by exploiting their inherent geometric regularities with compact representations. Using planar 3D primitives - a well-suited representation for man-made environments - we introduce PLANA3R, a pose-free framework for metric Planar 3D Reconstruction from unposed two-view images. Our approach employs Vision Transformers to extract a set of sparse planar primitives, estimate relative camera poses, and supervise geometry learning via planar splatting, where gradients are propagated through high-resolution rendered depth and normal maps of primitives. Unlike prior feedforward methods that require 3D plane annotations during training, PLANA3R learns planar 3D structures without explicit plane supervision, enabling scalable training on large-scale stereo datasets using only depth and normal annotations. We validate PLANA3R on multiple indoor-scene datasets with metric supervision and demonstrate strong generalization to out-of-domain indoor environments across diverse tasks under metric evaluation protocols, including 3D surface reconstruction, depth estimation, and relative pose estimation. Furthermore, by formulating with planar 3D representation, our method emerges with the ability for accurate plane segmentation. The project page is available at this https URL</li>
</ul>

<h3>Title: International Students and Scams: At Risk Abroad</h3>
<ul>
<li><strong>Authors: </strong>Katherine Zhang, Arjun Arunasalam, Pubali Datta, Z. Berkay Celik</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18715">https://arxiv.org/abs/2510.18715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18715">https://arxiv.org/pdf/2510.18715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18715]] International Students and Scams: At Risk Abroad(https://arxiv.org/abs/2510.18715)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>International students (IntlS) in the US refer to foreign students who acquire student visas to study in the US, primarily in higher education. As IntlS arrive in the US, they face several challenges, such as adjusting to a new country and culture, securing housing remotely, and arranging finances for tuition and personal expenses. These experiences, coupled with recent events such as visa revocations and the cessation of new visas, compound IntlS' risk of being targeted by and falling victim to online scams. While prior work has investigated IntlS' security and privacy, as well as general end users' reactions to online scams, research on how IntlS are uniquely impacted by scams remains largely absent. To address this gap, we conduct a two-phase user study comprising surveys (n=48) and semi-structured interviews (n=9). We investigate IntlS' exposure and interactions with scams, post-exposure actions such as reporting, and their perceptions of the usefulness of existing prevention resources and the barriers to following prevention advice. We find that IntlS are often targeted by scams (e.g., attackers impersonating government officials) and fear legal implications or deportation, which directly impacts their interactions with scams (e.g., they may prolong engagement with a scammer due to a sense of urgency). Interestingly, we also find that IntlS may lack awareness of - or access to - reliable resources that inform them about scams or guide them in reporting incidents to authorities. In fact, they may also face unique barriers in enacting scam prevention advice, such as avoiding reporting financial losses, since IntlS are required to demonstrate financial ability to stay in the US. The findings produced by our study help synthesize guidelines for stakeholders to better aid IntlS in reacting to scams.</li>
</ul>

<h3>Title: Bayesian Low-Rank Factorization for Robust Model Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18723">https://arxiv.org/abs/2510.18723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18723">https://arxiv.org/pdf/2510.18723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18723]] Bayesian Low-Rank Factorization for Robust Model Adaptation(https://arxiv.org/abs/2510.18723)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large speech foundation models achieve strong performance across many domains, but they often require adaptation to handle local needs such as code-switching, where speakers mix languages within the same utterance. Direct fine-tuning of these models risks overfitting to the target domain and overwriting the broad capabilities of the base model. To address this challenge, we explore Bayesian factorized adapters for speech foundation models, which place priors near zero to achieve sparser adaptation matrices and thereby retain general performance while adapting to specific domains. We apply our approach to the Whisper model and evaluate on different multilingual code-switching scenarios. Our results show only minimal adaptation loss while significantly reducing catastrophic forgetting of the base model. Compared to LoRA, our method achieves a backward gain of 54% with only a 4% drop on the new domain. These findings highlight the effectiveness of Bayesian adaptation for fine-tuning speech foundation models without sacrificing generalization.</li>
</ul>

<h3>Title: Adapting Language Balance in Code-Switching Speech</h3>
<ul>
<li><strong>Authors: </strong>Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18724">https://arxiv.org/abs/2510.18724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18724">https://arxiv.org/pdf/2510.18724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18724]] Adapting Language Balance in Code-Switching Speech(https://arxiv.org/abs/2510.18724)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite achieving impressive results on standard benchmarks, large foundational models still struggle against code-switching test cases. When data scarcity cannot be used as the usual justification for poor performance, the reason may lie in the infrequent occurrence of code-switched moments, where the embedding of the second language appears subtly. Instead of expecting the models to learn this infrequency on their own, it might be beneficial to provide the training process with labels. Evaluating model performance on code-switching data requires careful localization of code-switching points where recognition errors are most consequential, so that the analysis emphasizes mistakes occurring at those moments. Building on this observation, we leverage the difference between the embedded and the main language to highlight those code-switching points and thereby emphasize learning at those locations. This simple yet effective differentiable surrogate mitigates context bias during generation -- the central challenge in code-switching -- thereby improving the model's robustness. Our experiments with Arabic and Chinese-English showed that the models are able to predict the switching places more correctly, reflected by the reduced substitution error.</li>
</ul>

<h3>Title: SemiAdapt and SemiLoRA: Efficient Domain Adaptation for Transformer-based Low-Resource Language Translation with a Case Study on Irish</h3>
<ul>
<li><strong>Authors: </strong>Josh McGiff, Nikola S. Nikolov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18725">https://arxiv.org/abs/2510.18725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18725">https://arxiv.org/pdf/2510.18725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18725]] SemiAdapt and SemiLoRA: Efficient Domain Adaptation for Transformer-based Low-Resource Language Translation with a Case Study on Irish(https://arxiv.org/abs/2510.18725)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning is widely used to tailor large language models for specific tasks such as neural machine translation (NMT). However, leveraging transfer learning is computationally expensive when fine-tuning large multilingual models with billions of parameters, thus creating a barrier to entry for researchers working on low-resource domains such as Irish translation. Parameter-efficient fine-tuning (PEFT) bridges this gap by training on a fraction of the original model parameters, with the Low-Rank Adaptation (LoRA) approach introducing small, trainable adapter layers. We introduce SemiAdapt and SemiLoRA as semi-supervised inference-efficient approaches that strengthen domain adaptation and lead to improved overall performance in NMT. We demonstrate that SemiAdapt can outperform full-domain fine-tuning, while most notably, SemiLoRA can propel PEFT methods to match or even outperform full-model fine-tuning. We further evaluate domain-by-dataset fine-tuning and demonstrate that our embedding-based inference methods perform especially well on larger and noisier corpora. All Irish translation models developed in this work are released as open resources. These methods aim to make high-quality domain adaptation and fine-tuning more accessible to researchers working with low-resource languages.</li>
</ul>

<h3>Title: IF-VidCap: Can Video Caption Models Follow Instructions?</h3>
<ul>
<li><strong>Authors: </strong>Shihao Li, Yuanxing Zhang, Jiangtao Wu, Zhide Lei, Yiwen He, Runzhe Wen, Chenxi Liao, Chengkang Jiang, An Ping, Shuo Gao, Suhan Wang, Zhaozhou Bian, Zijun Zhou, Jingyi Xie, Jiayi Zhou, Jing Wang, Yifan Yao, Weihao Xie, Yingshui Tan, Yanghai Wang, Qianqian Xie, Zhaoxiang Zhang, Jiaheng Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18726">https://arxiv.org/abs/2510.18726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18726">https://arxiv.org/pdf/2510.18726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18726]] IF-VidCap: Can Video Caption Models Follow Instructions?(https://arxiv.org/abs/2510.18726)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although Multimodal Large Language Models (MLLMs) have demonstrated proficiency in video captioning, practical applications require captions that follow specific user instructions rather than generating exhaustive, unconstrained descriptions. Current benchmarks, however, primarily assess descriptive comprehensiveness while largely overlooking instruction-following capabilities. To address this gap, we introduce IF-VidCap, a new benchmark for evaluating controllable video captioning, which contains 1,400 high-quality samples. Distinct from existing video captioning or general instruction-following benchmarks, IF-VidCap incorporates a systematic framework that assesses captions on two dimensions: format correctness and content correctness. Our comprehensive evaluation of over 20 prominent models reveals a nuanced landscape: despite the continued dominance of proprietary models, the performance gap is closing, with top-tier open-source solutions now achieving near-parity. Furthermore, we find that models specialized for dense captioning underperform general-purpose MLLMs on complex instructions, indicating that future work should simultaneously advance both descriptive richness and instruction-following fidelity.</li>
</ul>

<h3>Title: HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sidhant Narula, Javad Rafiei Asl, Mohammad Ghasemigol, Eduardo Blanco, Daniel Takabi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18728">https://arxiv.org/abs/2510.18728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18728">https://arxiv.org/pdf/2510.18728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18728]] HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large Language Models(https://arxiv.org/abs/2510.18728)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) remain vulnerable to multi-turn jailbreak attacks. We introduce HarmNet, a modular framework comprising ThoughtNet, a hierarchical semantic network; a feedback-driven Simulator for iterative query refinement; and a Network Traverser for real-time adaptive attack execution. HarmNet systematically explores and refines the adversarial space to uncover stealthy, high-success attack paths. Experiments across closed-source and open-source LLMs show that HarmNet outperforms state-of-the-art methods, achieving higher attack success rates. For example, on Mistral-7B, HarmNet achieves a 99.4% attack success rate, 13.9% higher than the best baseline. Index terms: jailbreak attacks; large language models; adversarial framework; query refinement.</li>
</ul>

<h3>Title: Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation</h3>
<ul>
<li><strong>Authors: </strong>Ming Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18731">https://arxiv.org/abs/2510.18731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18731">https://arxiv.org/pdf/2510.18731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18731]] Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation(https://arxiv.org/abs/2510.18731)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models demonstrate strong capabilities in single-turn instruction following but suffer from Lost-in-Conversation (LiC), a degradation in performance as information is revealed progressively in multi-turn settings. Motivated by the current progress on Reinforcement Learning with Verifiable Rewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards (RLAAR), a framework that encourages models not only to generate correct answers, but also to judge the solvability of questions in the multi-turn conversation setting. Our approach employs a competence-gated curriculum that incrementally increases dialogue difficulty (in terms of instruction shards), stabilizing training while promoting reliability. Using multi-turn, on-policy rollouts and a mixed-reward system, RLAAR teaches models to balance problem-solving with informed abstention, reducing premature answering behaviors that cause LiC. Evaluated on LiC benchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to 75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together, these results provide a practical recipe for building multi-turn reliable and trustworthy LLMs.</li>
</ul>

<h3>Title: Topoformer: brain-like topographic organization in Transformer language models through spatial querying and reweighting</h3>
<ul>
<li><strong>Authors: </strong>Taha Binhuraib, Greta Tuckute, Nicholas Blauch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18745">https://arxiv.org/abs/2510.18745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18745">https://arxiv.org/pdf/2510.18745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18745]] Topoformer: brain-like topographic organization in Transformer language models through spatial querying and reweighting(https://arxiv.org/abs/2510.18745)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Spatial functional organization is a hallmark of biological brains: neurons are arranged topographically according to their response properties, at multiple scales. In contrast, representations within most machine learning models lack spatial biases, instead manifesting as disorganized vector spaces that are difficult to visualize and interpret. Here, we propose a novel form of self-attention that turns Transformers into "Topoformers" with topographic organization. We introduce spatial querying - where keys and queries are arranged on 2D grids, and local pools of queries are associated with a given key - and spatial reweighting, where we convert the standard fully connected layer of self-attention into a locally connected layer. We first demonstrate the feasibility of our approach by training a 1-layer Topoformer on a sentiment classification task. Training with spatial querying encourages topographic organization in the queries and keys, and spatial reweighting separately encourages topographic organization in the values and self-attention outputs. We then apply the Topoformer motifs at scale, training a BERT architecture with a masked language modeling objective. We find that the topographic variant performs on par with a non-topographic control model on NLP benchmarks, yet produces interpretable topographic organization as evaluated via eight linguistic test suites. Finally, analyzing an fMRI dataset of human brain responses to a large set of naturalistic sentences, we demonstrate alignment between low-dimensional topographic variability in the Topoformer model and human brain language network. Scaling up Topoformers further holds promise for greater interpretability in NLP research, and for more accurate models of the organization of linguistic information in the human brain.</li>
</ul>

<h3>Title: sNVMe-oF: Secure and Efficient Disaggregated Storage</h3>
<ul>
<li><strong>Authors: </strong>Marcin Chrapek, Meni Orenbach, Ahmad Atamli, Marcin Copik, Fritz Alder, Torsten Hoefler</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR, cs.DC, cs.NI, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18756">https://arxiv.org/abs/2510.18756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18756">https://arxiv.org/pdf/2510.18756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18756]] sNVMe-oF: Secure and Efficient Disaggregated Storage(https://arxiv.org/abs/2510.18756)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect</a></li>
<li><strong>Abstract: </strong>Disaggregated storage with NVMe-over-Fabrics (NVMe-oF) has emerged as the standard solution in modern data centers, achieving superior performance, resource utilization, and power efficiency. Simultaneously, confidential computing (CC) is becoming the de facto security paradigm, enforcing stronger isolation and protection for sensitive workloads. However, securing state-of-the-art storage with traditional CC methods struggles to scale and compromises performance or security. To address these issues, we introduce sNVMe-oF, a storage management system extending the NVMe-oF protocol and adhering to the CC threat model by providing confidentiality, integrity, and freshness guarantees. sNVMe-oF offers an appropriate control path and novel concepts such as counter-leasing. sNVMe-oF also optimizes data path performance by leveraging NVMe metadata, introducing a new disaggregated Hazel Merkle Tree (HMT), and avoiding redundant IPSec protections. We achieve this without modifying the NVMe-oF protocol. To prevent excessive resource usage while delivering line rate, sNVMe-oF also uses accelerators of CC-capable smart NICs. We prototype sNVMe-oF on an NVIDIA BlueField-3 and demonstrate how it can achieve as little as 2% performance degradation for synthetic patterns and AI training.</li>
</ul>

<h3>Title: Improving the Generation and Evaluation of Synthetic Data for Downstream Medical Causal Inference</h3>
<ul>
<li><strong>Authors: </strong>Harry Amad, Zhaozhi Qian, Dennis Frauen, Julianna Piskorz, Stefan Feuerriegel, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18768">https://arxiv.org/abs/2510.18768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18768">https://arxiv.org/pdf/2510.18768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18768]] Improving the Generation and Evaluation of Synthetic Data for Downstream Medical Causal Inference(https://arxiv.org/abs/2510.18768)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Causal inference is essential for developing and evaluating medical interventions, yet real-world medical datasets are often difficult to access due to regulatory barriers. This makes synthetic data a potentially valuable asset that enables these medical analyses, along with the development of new inference methods themselves. Generative models can produce synthetic data that closely approximate real data distributions, yet existing methods do not consider the unique challenges that downstream causal inference tasks, and specifically those focused on treatments, pose. We establish a set of desiderata that synthetic data containing treatments should satisfy to maximise downstream utility: preservation of (i) the covariate distribution, (ii) the treatment assignment mechanism, and (iii) the outcome generation mechanism. Based on these desiderata, we propose a set of evaluation metrics to assess such synthetic data. Finally, we present STEAM: a novel method for generating Synthetic data for Treatment Effect Analysis in Medicine that mimics the data-generating process of data containing treatments and optimises for our desiderata. We empirically demonstrate that STEAM achieves state-of-the-art performance across our metrics as compared to existing generative models, particularly as the complexity of the true data-generating process increases.</li>
</ul>

<h3>Title: UltraGen: High-Resolution Video Generation with Hierarchical Attention</h3>
<ul>
<li><strong>Authors: </strong>Teng Hu, Jiangning Zhang, Zihan Su, Ran Yi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18775">https://arxiv.org/abs/2510.18775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18775">https://arxiv.org/pdf/2510.18775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18775]] UltraGen: High-Resolution Video Generation with Hierarchical Attention(https://arxiv.org/abs/2510.18775)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in video generation have made it possible to produce visually compelling videos, with wide-ranging applications in content creation, entertainment, and virtual reality. However, most existing diffusion transformer based video generation models are limited to low-resolution outputs (<=720P) due to the quadratic computational complexity of the attention mechanism with respect to the output width and height. This computational bottleneck makes native high-resolution video generation (1080P/2K/4K) impractical for both training and inference. To address this challenge, we present UltraGen, a novel video generation framework that enables i) efficient and ii) end-to-end native high-resolution video synthesis. Specifically, UltraGen features a hierarchical dual-branch attention architecture based on global-local attention decomposition, which decouples full attention into a local attention branch for high-fidelity regional content and a global attention branch for overall semantic consistency. We further propose a spatially compressed global modeling strategy to efficiently learn global dependencies, and a hierarchical cross-window local attention mechanism to reduce computational costs while enhancing information flow across different local windows. Extensive experiments demonstrate that UltraGen can effectively scale pre-trained low-resolution video models to 1080P and even 4K resolution for the first time, outperforming existing state-of-the-art methods and super-resolution based two-stage pipelines in both qualitative and quantitative evaluations.</li>
</ul>

<h3>Title: KAT-Coder Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Zizheng Zhan, Ken Deng, Xiaojiang Zhang, Jinghui Wang, Huaixi Tang, Zhiyi Lai, Haoyang Huang, Wen Xiang, Kun Wu, Wenhao Zhuang, Minglei Zhang, Shaojie Wang, Shangpeng Yan, Kepeng Lei, Zongxian Feng, Huiming Wang, Zheng Lin, Mengtong Li, Mengfei Xie, Yinghan Cui, Xuxing Chen, Chao Wang, Weihao Li, Wenqiang Zhu, Jiarong Zhang, Jingxuan Xu, Songwei Yu, Yifan Yao, Xinping Lei, Han Li, Junqi Xiong, Zuchen Gao, Dailin Li, Haimo Li, Jiaheng Liu, Yuqun Zhang, Junyi Peng, Haotian Zhang, Bin Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18779">https://arxiv.org/abs/2510.18779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18779">https://arxiv.org/pdf/2510.18779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18779]] KAT-Coder Technical Report(https://arxiv.org/abs/2510.18779)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have enabled progress in agentic coding, where models autonomously reason, plan, and act within interactive software development workflows. However, bridging the gap between static text-based training and dynamic real-world agentic execution remains a core challenge. In this technical report, we present KAT-Coder, a large-scale agentic code model trained through a multi-stage curriculum encompassing Mid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning (RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances reasoning, planning, and reflection capabilities through a corpus of real software engineering data and synthetic agentic interactions. The SFT stage constructs a million-sample dataset balancing twenty programming languages, ten development contexts, and ten task archetypes. The RFT stage introduces a novel multi-ground-truth reward formulation for stable and sample-efficient policy optimization. Finally, the Reinforcement-to-Deployment phase adapts the model to production-grade IDE environments using Error-Masked SFT and Tree-Structured Trajectory Training. In summary, these stages enable KAT-Coder to achieve robust tool-use reliability, instruction alignment, and long-context reasoning, forming a deployable foundation for real-world intelligent coding agents. Our KAT series 32B model, KAT-Dev, has been open-sourced on this https URL.</li>
</ul>

<h3>Title: Rebellious Student: A Complementary Learning Framework for Background Feature Enhancement in Hyperspectral Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Wenping Jin, Yuyang Tang, Li Zhu, Fei Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18781">https://arxiv.org/abs/2510.18781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18781">https://arxiv.org/pdf/2510.18781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18781]] Rebellious Student: A Complementary Learning Framework for Background Feature Enhancement in Hyperspectral Anomaly Detection(https://arxiv.org/abs/2510.18781)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A recent class of hyperspectral anomaly detection methods that can be trained once on background datasets and then universally deployed -- without per-scene retraining or parameter tuning -- has demonstrated remarkable efficiency and robustness. Building upon this paradigm, we focus on the integration of spectral and spatial cues and introduce a novel "Rebellious Student" framework for complementary feature learning. Unlike conventional teacher-student paradigms driven by imitation, our method intentionally trains the spatial branch to diverge from the spectral teacher, thereby learning complementary spatial patterns that the teacher fails to capture. A two-stage learning strategy is adopted: (1) a spectral enhancement network is first trained via reverse distillation to obtain robust background spectral representations; and (2) a spatial network -- the rebellious student -- is subsequently optimized using decorrelation losses that enforce feature orthogonality while maintaining reconstruction fidelity to avoid irrelevant noise. Once trained, the framework enhances both spectral and spatial background features, enabling parameter-free and training-free anomaly detection when paired with conventional detectors. Extensive experiments on the HAD100 benchmark show substantial improvements over several established baselines with minimal computational overhead, confirming the effectiveness and generality of the proposed complementary learning paradigm. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: On Biologically Plausible Learning in Continuous Time</h3>
<ul>
<li><strong>Authors: </strong>Marc Gong Bacvanski, Liu Ziyin, Tomaso Poggio</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18808">https://arxiv.org/abs/2510.18808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18808">https://arxiv.org/pdf/2510.18808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18808]] On Biologically Plausible Learning in Continuous Time(https://arxiv.org/abs/2510.18808)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Biological learning unfolds continuously in time, yet most algorithmic models rely on discrete updates and separate inference and learning phases. We study a continuous-time neural model that unifies several biologically plausible learning algorithms and removes the need for phase separation. Rules including stochastic gradient descent (SGD), feedback alignment (FA), direct feedback alignment (DFA), and Kolen-Pollack (KP) emerge naturally as limiting cases of the dynamics. Simulations show that these continuous-time networks stably learn at biological timescales, even under temporal mismatches and integration noise. Through analysis and simulation, we show that learning depends on temporal overlap: a synapse updates correctly only when its input and the corresponding error signal coincide in time. When inputs are held constant, learning strength declines linearly as the delay between input and error approaches the stimulus duration, explaining observed robustness and failure across network depths. Critically, robust learning requires the synaptic plasticity timescale to exceed the stimulus duration by one to two orders of magnitude. For typical cortical stimuli (tens of milliseconds), this places the functional plasticity window in the few-second range, a testable prediction that identifies seconds-scale eligibility traces as necessary for error-driven learning in biological circuits.</li>
</ul>

<h3>Title: When LRP Diverges from Leave-One-Out in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Weiqiu You, Siqi Zeng, Yao-Hung Hubert Tsai, Makoto Yamada, Han Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18810">https://arxiv.org/abs/2510.18810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18810">https://arxiv.org/pdf/2510.18810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18810]] When LRP Diverges from Leave-One-Out in Transformers(https://arxiv.org/abs/2510.18810)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Leave-One-Out (LOO) provides an intuitive measure of feature importance but is computationally prohibitive. While Layer-Wise Relevance Propagation (LRP) offers a potentially efficient alternative, its axiomatic soundness in modern Transformers remains largely under-examined. In this work, we first show that the bilinear propagation rules used in recent advances of AttnLRP violate the implementation invariance axiom. We prove this analytically and confirm it empirically in linear attention layers. Second, we also revisit CP-LRP as a diagnostic baseline and find that bypassing relevance propagation through the softmax layer -- backpropagating relevance only through the value matrices -- significantly improves alignment with LOO, particularly in middle-to-late Transformer layers. Overall, our results suggest that (i) bilinear factorization sensitivity and (ii) softmax propagation error potentially jointly undermine LRP's ability to approximate LOO in Transformers.</li>
</ul>

<h3>Title: A Geometric Approach to Steerable Convolutions</h3>
<ul>
<li><strong>Authors: </strong>Soumyabrata Kundu, Risi Kondor</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18813">https://arxiv.org/abs/2510.18813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18813">https://arxiv.org/pdf/2510.18813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18813]] A Geometric Approach to Steerable Convolutions(https://arxiv.org/abs/2510.18813)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In contrast to the somewhat abstract, group theoretical approach adopted by many papers, our work provides a new and more intuitive derivation of steerable convolutional neural networks in $d$ dimensions. This derivation is based on geometric arguments and fundamental principles of pattern matching. We offer an intuitive explanation for the appearance of the Clebsch--Gordan decomposition and spherical harmonic basis functions. Furthermore, we suggest a novel way to construct steerable convolution layers using interpolation kernels that improve upon existing implementation, and offer greater robustness to noisy data.</li>
</ul>

<h3>Title: Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards</h3>
<ul>
<li><strong>Authors: </strong>Mengqi Li, Lei Zhao, Anthony Man-Cho So, Ruoyu Sun, Xiao Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18814">https://arxiv.org/abs/2510.18814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18814">https://arxiv.org/pdf/2510.18814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18814]] Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards(https://arxiv.org/abs/2510.18814)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a simple, self-help online supervised finetuning (OSFT) paradigm for LLM reasoning. In this paradigm, the model generates its own responses and is immediately finetuned on this self-generated data. OSFT is a highly efficient training strategy for LLM reasoning, as it is reward-free and uses just one rollout by default. Experiment results show that OSFT achieves downstream performance on challenging mathematical reasoning tasks comparable to strong reinforcement learning with verifiable rewards (RLVR) methods such as GRPO. Our ablation study further demonstrates the efficiency and robustness of OSFT. The major mechanism of OSFT lies in facilitating the model's own existing preference (latent knowledge) learned from pretraining, which leads to reasoning ability improvement. We believe that OSFT offers an efficient and promising alternative to more complex, reward-based training paradigms. Our code is available at this https URL.</li>
</ul>

<h3>Title: Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Shuxin Lin, Dhaval Patel, Christodoulos Constantinides</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18817">https://arxiv.org/abs/2510.18817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18817">https://arxiv.org/pdf/2510.18817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18817]] Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring(https://arxiv.org/abs/2510.18817)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Small Language Models (SLMs) are becoming increasingly popular in specialized fields, such as industrial applications, due to their efficiency, lower computational requirements, and ability to be fine-tuned for domain-specific tasks, enabling accurate and cost-effective solutions. However, performing complex reasoning using SLMs in specialized fields such as Industry 4.0 remains challenging. In this paper, we propose a knowledge distillation framework for industrial asset health, which transfers reasoning capabilities via Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) to smaller, more efficient models (SLMs). We discuss the advantages and the process of distilling LLMs using multi-choice question answering (MCQA) prompts to enhance reasoning and refine decision-making. We also perform in-context learning to verify the quality of the generated knowledge and benchmark the performance of fine-tuned SLMs with generated knowledge against widely used LLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform the base models by a significant margin, narrowing the gap to their LLM counterparts. Our code is open-sourced at: this https URL.</li>
</ul>

<h3>Title: An Explainable Hybrid AI Framework for Enhanced Tuberculosis and Symptom Detection</h3>
<ul>
<li><strong>Authors: </strong>Neel Patel, Alexander Wong, Ashkan Ebadi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18819">https://arxiv.org/abs/2510.18819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18819">https://arxiv.org/pdf/2510.18819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18819]] An Explainable Hybrid AI Framework for Enhanced Tuberculosis and Symptom Detection(https://arxiv.org/abs/2510.18819)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Tuberculosis remains a critical global health issue, particularly in resource-limited and remote areas. Early detection is vital for treatment, yet the lack of skilled radiologists underscores the need for artificial intelligence (AI)-driven screening tools. Developing reliable AI models is challenging due to the necessity for large, high-quality datasets, which are costly to obtain. To tackle this, we propose a teacher--student framework which enhances both disease and symptom detection on chest X-rays by integrating two supervised heads and a self-supervised head. Our model achieves an accuracy of 98.85% for distinguishing between COVID-19, tuberculosis, and normal cases, and a macro-F1 score of 90.09% for multilabel symptom detection, significantly outperforming baselines. The explainability assessments also show the model bases its predictions on relevant anatomical features, demonstrating promise for deployment in clinical screening and triage settings.</li>
</ul>

<h3>Title: SAM 2++: Tracking Anything at Any Granularity</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Zhang, Cheng Liang, Yichun Yang, Chenkai Zeng, Yutao Cui, Xinwen Zhang, Xin Zhou, Kai Ma, Gangshan Wu, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18822">https://arxiv.org/abs/2510.18822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18822">https://arxiv.org/pdf/2510.18822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18822]] SAM 2++: Tracking Anything at Any Granularity(https://arxiv.org/abs/2510.18822)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Video tracking aims at finding the specific target in subsequent frames given its initial state. Due to the varying granularity of target states across different tasks, most existing trackers are tailored to a single task and heavily rely on custom-designed modules within the individual task, which limits their generalization and leads to redundancy in both model design and parameters. To unify video tracking tasks, we present SAM 2++, a unified model towards tracking at any granularity, including masks, boxes, and points. First, to extend target granularity, we design task-specific prompts to encode various task inputs into general prompt embeddings, and a unified decoder to unify diverse task results into a unified form pre-output. Next, to satisfy memory matching, the core operation of tracking, we introduce a task-adaptive memory mechanism that unifies memory across different granularities. Finally, we introduce a customized data engine to support tracking training at any granularity, producing a large and diverse video tracking dataset with rich annotations at three granularities, termed Tracking-Any-Granularity, which represents a comprehensive resource for training and benchmarking on unified tracking. Comprehensive experiments on multiple benchmarks confirm that SAM 2++ sets a new state of the art across diverse tracking tasks at different granularities, establishing a unified and robust tracking framework.</li>
</ul>

<h3>Title: Unifying and Enhancing Graph Transformers via a Hierarchical Mask Framework</h3>
<ul>
<li><strong>Authors: </strong>Yujie Xing, Xiao Wang, Bin Wu, Hai Huang, Chuan Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18825">https://arxiv.org/abs/2510.18825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18825">https://arxiv.org/pdf/2510.18825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18825]] Unifying and Enhancing Graph Transformers via a Hierarchical Mask Framework(https://arxiv.org/abs/2510.18825)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Graph Transformers (GTs) have emerged as a powerful paradigm for graph representation learning due to their ability to model diverse node interactions. However, existing GTs often rely on intricate architectural designs tailored to specific interactions, limiting their flexibility. To address this, we propose a unified hierarchical mask framework that reveals an underlying equivalence between model architecture and attention mask construction. This framework enables a consistent modeling paradigm by capturing diverse interactions through carefully designed attention masks. Theoretical analysis under this framework demonstrates that the probability of correct classification positively correlates with the receptive field size and label consistency, leading to a fundamental design principle: an effective attention mask should ensure both a sufficiently large receptive field and a high level of label consistency. While no single existing mask satisfies this principle across all scenarios, our analysis reveals that hierarchical masks offer complementary strengths, motivating their effective integration. Then, we introduce M3Dphormer, a Mixture-of-Experts-based Graph Transformer with Multi-Level Masking and Dual Attention Computation. M3Dphormer incorporates three theoretically grounded hierarchical masks and employs a bi-level expert routing mechanism to adaptively integrate multi-level interaction information. To ensure scalability, we further introduce a dual attention computation scheme that dynamically switches between dense and sparse modes based on local mask sparsity. Extensive experiments across multiple benchmarks demonstrate that M3Dphormer achieves state-of-the-art performance, validating the effectiveness of our unified framework and model design.</li>
</ul>

<h3>Title: MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long Context Training</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18830">https://arxiv.org/abs/2510.18830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18830">https://arxiv.org/pdf/2510.18830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18830]] MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long Context Training(https://arxiv.org/abs/2510.18830)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The adoption of long context windows has become a standard feature in Large Language Models (LLMs), as extended contexts significantly enhance their capacity for complex reasoning and broaden their applicability across diverse scenarios. Dynamic sparse attention is a promising approach for reducing the computational cost of long-context. However, efficiently training LLMs with dynamic sparse attention on ultra-long contexts-especially in distributed settings-remains a significant challenge, due in large part to worker- and step-level imbalance. This paper introduces MTraining, a novel distributed methodology leveraging dynamic sparse attention to enable efficient training for LLMs with ultra-long contexts. Specifically, MTraining integrates three key components: a dynamic sparse training pattern, balanced sparse ring attention, and hierarchical sparse ring attention. These components are designed to synergistically address the computational imbalance and communication overheads inherent in dynamic sparse attention mechanisms during the training of models with extensive context lengths. We demonstrate the efficacy of MTraining by training Qwen2.5-3B, successfully expanding its context window from 32K to 512K tokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite of downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A Haystack, reveal that MTraining achieves up to a 6x higher training throughput while preserving model accuracy. Our code is available at this https URL.</li>
</ul>

<h3>Title: FedDEAP: Adaptive Dual-Prompt Tuning for Multi-Domain Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Yubin Zheng, Pak-Hei Yeung, Jing Xia, Tianjie Ju, Peng Tang, Weidong Qiu, Jagath C. Rajapakse</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18837">https://arxiv.org/abs/2510.18837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18837">https://arxiv.org/pdf/2510.18837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18837]] FedDEAP: Adaptive Dual-Prompt Tuning for Multi-Domain Federated Learning(https://arxiv.org/abs/2510.18837)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables multiple clients to collaboratively train machine learning models without exposing local data, balancing performance and privacy. However, domain shift and label heterogeneity across clients often hinder the generalization of the aggregated global model. Recently, large-scale vision-language models like CLIP have shown strong zero-shot classification capabilities, raising the question of how to effectively fine-tune CLIP across domains in a federated setting. In this work, we propose an adaptive federated prompt tuning framework, FedDEAP, to enhance CLIP's generalization in multi-domain scenarios. Our method includes the following three key components: (1) To mitigate the loss of domain-specific information caused by label-supervised tuning, we disentangle semantic and domain-specific features in images by using semantic and domain transformation networks with unbiased mappings; (2) To preserve domain-specific knowledge during global prompt aggregation, we introduce a dual-prompt design with a global semantic prompt and a local domain prompt to balance shared and personalized information; (3) To maximize the inclusion of semantic and domain information from images in the generated text features, we align textual and visual representations under the two learned transformations to preserve semantic and domain consistency. Theoretical analysis and extensive experiments on four datasets demonstrate the effectiveness of our method in enhancing the generalization of CLIP for federated image recognition across multiple domains.</li>
</ul>

<h3>Title: See the Text: From Tokenization to Visual Reading</h3>
<ul>
<li><strong>Authors: </strong>Ling Xing, Alex Jinpeng Wang, Rui Yan, Hongyu Qu, Zechao Li, Jinhui Tang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18840">https://arxiv.org/abs/2510.18840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18840">https://arxiv.org/pdf/2510.18840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18840]] See the Text: From Tokenization to Visual Reading(https://arxiv.org/abs/2510.18840)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>People see text. Humans read by recognizing words as visual objects, including their shapes, layouts, and patterns, before connecting them to meaning, which enables us to handle typos, distorted fonts, and various scripts effectively. Modern large language models (LLMs), however, rely on subword tokenization, fragmenting text into pieces from a fixed vocabulary. While effective for high-resource languages, this approach over-segments low-resource languages, yielding long, linguistically meaningless sequences and inflating computation. In this work, we challenge this entrenched paradigm and move toward a vision-centric alternative. Our method, SeeTok, renders text as images (visual-text) and leverages pretrained multimodal LLMs to interpret them, reusing strong OCR and text-vision alignment abilities learned from large-scale multimodal training. Across three different language tasks, SeeTok matches or surpasses subword tokenizers while requiring 4.43 times fewer tokens and reducing FLOPs by 70.5%, with additional gains in cross-lingual generalization, robustness to typographic noise, and linguistic hierarchy. SeeTok signals a shift from symbolic tokenization to human-like visual reading, and takes a step toward more natural and cognitively inspired language models.</li>
</ul>

<h3>Title: Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Zhu, Meiling Tao, Tiannan Wang, Dongyi Ding, Yuchen Eleanor Jiang, Wangchunshu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18849">https://arxiv.org/abs/2510.18849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18849">https://arxiv.org/pdf/2510.18849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18849]] Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning(https://arxiv.org/abs/2510.18849)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Faithfully personalizing large language models (LLMs) to align with individual user preferences is a critical but challenging task. While supervised fine-tuning (SFT) quickly reaches a performance plateau, standard reinforcement learning from human feedback (RLHF) also struggles with the nuances of personalization. Scalar-based reward models are prone to reward hacking which leads to verbose and superficially personalized responses. To address these limitations, we propose Critique-Post-Edit, a robust reinforcement learning framework that enables more faithful and controllable personalization. Our framework integrates two key components: (1) a Personalized Generative Reward Model (GRM) that provides multi-dimensional scores and textual critiques to resist reward hacking, and (2) a Critique-Post-Edit mechanism where the policy model revises its own outputs based on these critiques for more targeted and efficient learning. Under a rigorous length-controlled evaluation, our method substantially outperforms standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an average 11\% win-rate improvement, and personalized Qwen2.5-14B model surpasses the performance of GPT-4.1. These results demonstrate a practical path to faithful, efficient, and controllable personalization.</li>
</ul>

<h3>Title: DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Rongyuan Wu, Lingchen Sun, Zhengqiang Zhang, Shihao Wang, Tianhe Wu, Qiaosi Yi, Shuai Li, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18851">https://arxiv.org/abs/2510.18851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18851">https://arxiv.org/pdf/2510.18851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18851]] DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution(https://arxiv.org/abs/2510.18851)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Benefiting from pre-trained text-to-image (T2I) diffusion models, real-world image super-resolution (Real-ISR) methods can synthesize rich and realistic details. However, due to the inherent stochasticity of T2I models, different noise inputs often lead to outputs with varying perceptual quality. Although this randomness is sometimes seen as a limitation, it also introduces a wider perceptual quality range, which can be exploited to improve Real-ISR performance. To this end, we introduce Direct Perceptual Preference Optimization for Real-ISR (DP$^2$O-SR), a framework that aligns generative models with perceptual preferences without requiring costly human annotations. We construct a hybrid reward signal by combining full-reference and no-reference image quality assessment (IQA) models trained on large-scale human preference datasets. This reward encourages both structural fidelity and natural appearance. To better utilize perceptual diversity, we move beyond the standard best-vs-worst selection and construct multiple preference pairs from outputs of the same model. Our analysis reveals that the optimal selection ratio depends on model capacity: smaller models benefit from broader coverage, while larger models respond better to stronger contrast in supervision. Furthermore, we propose hierarchical preference optimization, which adaptively weights training pairs based on intra-group reward gaps and inter-group diversity, enabling more efficient and stable learning. Extensive experiments across both diffusion- and flow-based T2I backbones demonstrate that DP$^2$O-SR significantly improves perceptual quality and generalizes well to real-world benchmarks.</li>
</ul>

<h3>Title: LightMem: Lightweight and Efficient Memory-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18866">https://arxiv.org/abs/2510.18866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18866">https://arxiv.org/pdf/2510.18866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18866]] LightMem: Lightweight and Efficient Memory-Augmented Generation(https://arxiv.org/abs/2510.18866)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. Experiments on LongMemEval with GPT and Qwen backbones show that LightMem outperforms strong baselines in accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API calls by up to 159x, and runtime by over 12x. The code is available at this https URL.</li>
</ul>

<h3>Title: How Do LLMs Use Their Depth?</h3>
<ul>
<li><strong>Authors: </strong>Akshat Gupta, Jay Yeung, Gopala Anumanchipalli, Anna Ivanova</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18871">https://arxiv.org/abs/2510.18871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18871">https://arxiv.org/pdf/2510.18871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18871]] How Do LLMs Use Their Depth?(https://arxiv.org/abs/2510.18871)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Growing evidence suggests that large language models do not use their depth uniformly, yet we still lack a fine-grained understanding of their layer-wise prediction dynamics. In this paper, we trace the intermediate representations of several open-weight models during inference and reveal a structured and nuanced use of depth. Specifically, we propose a "Guess-then-Refine" framework that explains how LLMs internally structure their computations to make predictions. We first show that the top-ranked predictions in early LLM layers are composed primarily of high-frequency tokens, which act as statistical guesses proposed by the model early on due to the lack of appropriate contextual information. As contextual information develops deeper into the model, these initial guesses get refined into contextually appropriate tokens. Even high-frequency token predictions from early layers get refined >70% of the time, indicating that correct token prediction is not "one-and-done". We then go beyond frequency-based prediction to examine the dynamic usage of layer depth across three case studies. (i) Part-of-speech analysis shows that function words are, on average, the earliest to be predicted correctly. (ii) Fact recall task analysis shows that, in a multi-token answer, the first token requires more computational depth than the rest. (iii) Multiple-choice task analysis shows that the model identifies the format of the response within the first half of the layers, but finalizes its response only toward the end. Together, our results provide a detailed view of depth usage in LLMs, shedding light on the layer-by-layer computations that underlie successful predictions and providing insights for future works to improve computational efficiency in transformer-based models.</li>
</ul>

<h3>Title: Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting</h3>
<ul>
<li><strong>Authors: </strong>Howard Chen, Noam Razin, Karthik Narasimhan, Danqi Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18874">https://arxiv.org/abs/2510.18874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18874">https://arxiv.org/pdf/2510.18874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18874]] Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting(https://arxiv.org/abs/2510.18874)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Adapting language models (LMs) to new tasks via post-training carries the risk of degrading existing capabilities -- a phenomenon classically known as catastrophic forgetting. In this paper, toward identifying guidelines for mitigating this phenomenon, we systematically compare the forgetting patterns of two widely adopted post-training methods: supervised fine-tuning (SFT) and reinforcement learning (RL). Our experiments reveal a consistent trend across LM families (Llama, Qwen) and tasks (instruction following, general knowledge, and arithmetic reasoning): RL leads to less forgetting than SFT while achieving comparable or higher target task performance. To investigate the cause for this difference, we consider a simplified setting in which the LM is modeled as a mixture of two distributions, one corresponding to prior knowledge and the other to the target task. We identify that the mode-seeking nature of RL, which stems from its use of on-policy data, enables keeping prior knowledge intact when learning the target task. We then verify this insight by demonstrating that the use on-policy data underlies the robustness of RL to forgetting in practical settings, as opposed to other algorithmic choices such as the KL regularization or advantage estimation. Lastly, as a practical implication, our results highlight the potential of mitigating forgetting using approximately on-policy data, which can be substantially more efficient to obtain than fully on-policy data.</li>
</ul>

<h3>Title: Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Haochen Wang, Yuhao Wang, Tao Zhang, Yikang Zhou, Yanwei Li, Jiacong Wang, Ye Tian, Jiahao Meng, Zilong Huang, Guangcan Mai, Anran Wang, Yunhai Tong, Zhuochen Wang, Xiangtai Li, Zhaoxiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18876">https://arxiv.org/abs/2510.18876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18876">https://arxiv.org/pdf/2510.18876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18876]] Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs(https://arxiv.org/abs/2510.18876)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Multimodal Large Language Models (MLLMs) excel at holistic understanding, they struggle in capturing the dense world with complex scenes, requiring fine-grained analysis of intricate details and object inter-relationships. Region-level MLLMs have been a promising step. However, previous attempts are generally optimized to understand given regions in isolation, neglecting crucial global contexts. To address this, we introduce Grasp Any Region (GAR) for comprehen- sive region-level visual understanding. Empowered by an effective RoI-aligned feature replay technique, GAR supports (1) precise perception by leveraging necessary global contexts, and (2) modeling interactions between multiple prompts. Together, it then naturally achieves (3) advanced compositional reasoning to answer specific free-form questions about any region, shifting the paradigm from passive description to active dialogue. Moreover, we construct GAR-Bench, which not only provides a more accurate evaluation of single-region comprehension, but also, more importantly, measures interactions and complex reasoning across multiple regions. Extensive experiments have demonstrated that GAR-1B not only maintains the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5 on DLC-Bench, but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities, even surpassing InternVL3-78B on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong capabilities can be easily transferred to videos.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
