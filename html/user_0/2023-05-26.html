<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Foundational Models for Malware Embeddings Using Spatio-Temporal Parallel Convolutional Networks. (arXiv:2305.15488v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15488">http://arxiv.org/abs/2305.15488</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15488] Foundational Models for Malware Embeddings Using Spatio-Temporal Parallel Convolutional Networks](http://arxiv.org/abs/2305.15488) #security</code></li>
<li>Summary: <p>In today's interconnected digital landscape, the proliferation of malware
poses a significant threat to the security and stability of computer networks
and systems worldwide. As the complexity of malicious tactics, techniques, and
procedures (TTPs) continuously grows to evade detection, so does the need for
advanced methods capable of capturing and characterizing malware behavior. The
current state of the art in malware classification and detection uses task
specific objectives; however, this method fails to generalize to other
downstream tasks involving the same malware class. In this paper, the authors
introduce a novel method that combines convolutional neural networks, standard
graph embedding techniques, and a metric learning objective to extract
meaningful information from network flow data and create strong embeddings
characterizing malware behavior. These embeddings enable the development of
highly accurate, efficient, and generalizable machine learning models for tasks
such as malware strain classification, zero day threat detection, and closest
attack type attribution as demonstrated in this paper. A shift from task
specific objectives to strong embeddings will not only allow rapid iteration of
cyber-threat detection models, but also allow different modalities to be
introduced in the development of these models.
</p></li>
</ul>

<h3>Title: Security Impact Analysis of Degree of Field Extension in Lattice Attacks on Ring-LWE Problem. (arXiv:2305.15772v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15772">http://arxiv.org/abs/2305.15772</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15772] Security Impact Analysis of Degree of Field Extension in Lattice Attacks on Ring-LWE Problem](http://arxiv.org/abs/2305.15772) #security</code></li>
<li>Summary: <p>Modern information communications use cryptography to keep the contents of
communications confidential. RSA (Rivest-Shamir-Adleman) cryptography and
elliptic curve cryptography, which are public-key cryptosystems, are widely
used cryptographic schemes. However, it is known that these cryptographic
schemes can be deciphered in a very short time by Shor's algorithm when a
quantum computer is put into practical use. Therefore, several methods have
been proposed for quantum computer-resistant cryptosystems that cannot be
cracked even by a quantum computer. A simple implementation of LWE-based
lattice cryptography based on the LWE (Learning With Errors) problem requires a
key length of $O(n^2)$ to ensure the same level of security as existing
public-key cryptography schemes such as RSA and elliptic curve cryptography. In
this paper, we attacked the Ring-LWE (RLWE) scheme, which can be implemented
with a short key length, with a modified LLL (Lenstra-Lenstra-Lov\'asz) basis
reduction algorithm and investigated the trend in the degree of field extension
required to generate a secure and small key. Results showed that the
lattice-based cryptography may be strengthened by employing Cullen or Mersenne
prime numbers as the degree of field extension.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Differentially Private Synthetic Data via Foundation Model APIs 1: Images. (arXiv:2305.15560v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15560">http://arxiv.org/abs/2305.15560</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15560] Differentially Private Synthetic Data via Foundation Model APIs 1: Images](http://arxiv.org/abs/2305.15560) #privacy</code></li>
<li>Summary: <p>Generating differentially private (DP) synthetic data that closely resembles
the original private data without leaking sensitive user information is a
scalable way to mitigate privacy concerns in the current data-driven world. In
contrast to current practices that train customized models for this task, we
aim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundation
models as blackboxes and only utilize their inference APIs. Such API-based,
training-free approaches are easier to deploy as exemplified by the recent
surge in the number of API-based apps. These approaches can also leverage the
power of large foundation models which are accessible via their inference APIs
while the model weights are unreleased. However, this comes with greater
challenges due to strictly more restrictive model access and the additional
need to protect privacy from the API provider.
</p></li>
</ul>

<p>In this paper, we present a new framework called Private Evolution (PE) to
solve this problem and show its initial promise on synthetic images.
Surprisingly, PE can match or even outperform state-of-the-art (SOTA) methods
without any model training. For example, on CIFAR10 (with ImageNet as the
public data), we achieve FID<=7.9 with privacy cost epsilon=0.67, significantly
improving the previous SOTA from epsilon=32. We further demonstrate the promise
of applying PE on large foundation models such as Stable Diffusion to tackle
challenging private datasets with a small number of high-resolution images.
</p>

<h3>Title: Meta Adaptive Task Sampling for Few-Domain Generalization. (arXiv:2305.15644v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15644">http://arxiv.org/abs/2305.15644</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15644] Meta Adaptive Task Sampling for Few-Domain Generalization](http://arxiv.org/abs/2305.15644) #privacy</code></li>
<li>Summary: <p>To ensure the out-of-distribution (OOD) generalization performance,
traditional domain generalization (DG) methods resort to training on data from
multiple sources with different underlying distributions. And the success of
those DG methods largely depends on the fact that there are diverse training
distributions. However, it usually needs great efforts to obtain enough
heterogeneous data due to the high expenses, privacy issues or the scarcity of
data. Thus an interesting yet seldom investigated problem arises: how to
improve the OOD generalization performance when the perceived heterogeneity is
limited. In this paper, we instantiate a new framework called few-domain
generalization (FDG), which aims to learn a generalizable model from very few
domains of novel tasks with the knowledge acquired from previous learning
experiences on base tasks. Moreover, we propose a Meta Adaptive Task Sampling
(MATS) procedure to differentiate base tasks according to their semantic and
domain-shift similarity to the novel task. Empirically, we show that the newly
introduced FDG framework can substantially improve the OOD generalization
performance on the novel task and further combining MATS with episodic training
could outperform several state-of-the-art DG baselines on widely used
benchmarks like PACS and DomainNet.
</p></li>
</ul>

<h3>Title: Camera-Incremental Object Re-Identification with Identity Knowledge Evolution. (arXiv:2305.15909v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15909">http://arxiv.org/abs/2305.15909</a></li>
<li>Code URL: <a href="https://github.com/htyao89/camera-incremental-object-reid">https://github.com/htyao89/camera-incremental-object-reid</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15909] Camera-Incremental Object Re-Identification with Identity Knowledge Evolution](http://arxiv.org/abs/2305.15909) #privacy</code></li>
<li>Summary: <p>Object Re-identification (ReID) aims to retrieve the probe object from many
gallery images with the ReID model inferred based on a stationary camera-free
dataset by associating and collecting the identities across all camera views.
When deploying the ReID algorithm in real-world scenarios, the aspect of
storage, privacy constraints, and dynamic changes of cameras would degrade its
generalizability and applicability. Treating each camera's data independently,
we introduce a novel ReID task named Camera-Incremental Object
Re-identification (CIOR) by continually optimizing the ReID mode from the
incoming stream of the camera dataset. Since the identities under different
camera views might describe the same object, associating and distilling the
knowledge of common identities would boost the discrimination and benefit from
alleviating the catastrophic forgetting. In this paper, we propose a novel
Identity Knowledge Evolution (IKE) framework for CIOR, consisting of the
Identity Knowledge Association (IKA), Identity Knowledge Distillation (IKD),
and Identity Knowledge Update (IKU). IKA is proposed to discover the common
identities between the current identity and historical identities. IKD has
applied to distillate historical identity knowledge from common identities and
quickly adapt the historical model to the current camera view. After each
camera has been trained, IKU is applied to continually expand the identity
knowledge by combining the historical and current identity memories. The
evaluation of Market-CL and Veri-CL shows the Identity Knowledge Evolution
(IKE) effectiveness for CIOR.
code:https://github.com/htyao89/Camera-Incremental-Object-ReID
</p></li>
</ul>

<h3>Title: MERGE: Fast Private Text Generation. (arXiv:2305.15769v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15769">http://arxiv.org/abs/2305.15769</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15769] MERGE: Fast Private Text Generation](http://arxiv.org/abs/2305.15769) #privacy</code></li>
<li>Summary: <p>Recent years have seen increasing concerns about the private inference of NLP
services and Transformer models. However, existing two-party privacy-preserving
methods solely consider NLU scenarios, while the private inference of text
generation such as translation, dialogue, and code completion remains unsolved.
Besides, while migrated to NLG models, existing privacy-preserving methods
perform poorly in terms of inference speed, and suffer from the convergence
problem during the training stage. To address these issues, we propose MERGE, a
fast private text generation framework for Transformer-based language models.
Specifically, MERGE reuse the output hidden state as the word embedding to
bypass the embedding computation, and reorganize the linear operations in the
Transformer module to accelerate the forward procedure. Based on these two
optimizations, extensive experiments show that MERGE can achieve a 26.5x
speedup under the sequence length 512, and reduce 80\% communication bytes,
with an up to 10x speedup to existing state-of-art models.
</p></li>
</ul>

<h3>Title: Private Meeting Summarization Without Performance Loss. (arXiv:2305.15894v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15894">http://arxiv.org/abs/2305.15894</a></li>
<li>Code URL: <a href="https://github.com/coastalcph/private_meeting_summarization">https://github.com/coastalcph/private_meeting_summarization</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15894] Private Meeting Summarization Without Performance Loss](http://arxiv.org/abs/2305.15894) #privacy</code></li>
<li>Summary: <p>Meeting summarization has an enormous business potential, but in addition to
being a hard problem, roll-out is challenged by privacy concerns. We explore
the problem of meeting summarization under differential privacy constraints and
find, to our surprise, that while differential privacy leads to slightly lower
performance on in-sample data, differential privacy improves performance when
evaluated on unseen meeting types. Since meeting summarization systems will
encounter a great variety of meeting types in practical employment scenarios,
this observation makes safe meeting summarization seem much more feasible. We
perform extensive error analysis and identify potential risks in meeting
summarization under differential privacy, including a faithfulness analysis.
</p></li>
</ul>

<h3>Title: Post-processing Private Synthetic Data for Improving Utility on Selected Measures. (arXiv:2305.15538v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15538">http://arxiv.org/abs/2305.15538</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15538] Post-processing Private Synthetic Data for Improving Utility on Selected Measures](http://arxiv.org/abs/2305.15538) #privacy</code></li>
<li>Summary: <p>Existing private synthetic data generation algorithms are agnostic to
downstream tasks. However, end users may have specific requirements that the
synthetic data must satisfy. Failure to meet these requirements could
significantly reduce the utility of the data for downstream use. We introduce a
post-processing technique that improves the utility of the synthetic data with
respect to measures selected by the end user, while preserving strong privacy
guarantees and dataset quality. Our technique involves resampling from the
synthetic data to filter out samples that do not meet the selected utility
measures, using an efficient stochastic first-order algorithm to find optimal
resampling weights. Through comprehensive numerical experiments, we demonstrate
that our approach consistently improves the utility of synthetic data across
multiple benchmark datasets and state-of-the-art synthetic data generation
algorithms.
</p></li>
</ul>

<h3>Title: Privacy Protectability: An Information-theoretical Approach. (arXiv:2305.15697v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15697">http://arxiv.org/abs/2305.15697</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15697] Privacy Protectability: An Information-theoretical Approach](http://arxiv.org/abs/2305.15697) #privacy</code></li>
<li>Summary: <p>Recently, inference privacy has attracted increasing attention. The inference
privacy concern arises most notably in the widely deployed edge-cloud video
analytics systems, where the cloud needs the videos captured from the edge. The
video data can contain sensitive information and subject to attack when they
are transmitted to the cloud for inference. Many privacy protection schemes
have been proposed. Yet, the performance of a scheme needs to be determined by
experiments or inferred by analyzing the specific case. In this paper, we
propose a new metric, \textit{privacy protectability}, to characterize to what
degree a video stream can be protected given a certain video analytics task.
Such a metric has strong operational meaning. For example, low protectability
means that it may be necessary to set up an overall secure environment. We can
also evaluate a privacy protection scheme, e.g., assume it obfuscates the video
data, what level of protection this scheme has achieved after obfuscation. Our
definition of privacy protectability is rooted in information theory and we
develop efficient algorithms to estimate the metric. We use experiments on real
data to validate that our metric is consistent with empirical measurements on
how well a video stream can be protected for a video analytics task.
</p></li>
</ul>

<h3>Title: Learning across Data Owners with Joint Differential Privacy. (arXiv:2305.15723v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15723">http://arxiv.org/abs/2305.15723</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15723] Learning across Data Owners with Joint Differential Privacy](http://arxiv.org/abs/2305.15723) #privacy</code></li>
<li>Summary: <p>In this paper, we study the setting in which data owners train machine
learning models collaboratively under a privacy notion called joint
differential privacy [Kearns et al., 2018]. In this setting, the model trained
for each data owner $j$ uses $j$'s data without privacy consideration and other
owners' data with differential privacy guarantees. This setting was initiated
in [Jain et al., 2021] with a focus on linear regressions. In this paper, we
study this setting for stochastic convex optimization (SCO). We present an
algorithm that is a variant of DP-SGD [Song et al., 2013; Abadi et al., 2016]
and provides theoretical bounds on its population loss. We compare our
algorithm to several baselines and discuss for what parameter setups our
algorithm is more preferred. We also empirically study joint differential
privacy in the multi-class classification problem over two public datasets. Our
empirical findings are well-connected to the insights from our theoretical
results.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: ACAI: Extending Arm Confidential Computing Architecture Protection from CPUs to Accelerators. (arXiv:2305.15986v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15986">http://arxiv.org/abs/2305.15986</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15986] ACAI: Extending Arm Confidential Computing Architecture Protection from CPUs to Accelerators](http://arxiv.org/abs/2305.15986) #protect</code></li>
<li>Summary: <p>Trusted execution environments in several existing and upcoming CPUs
demonstrate the success of confidential computing, with the caveat that tenants
cannot use accelerators such as GPUs and FPGAs. If the accelerators have TEE
support, the user-code executing on the CPU in a confidential VM has to rely on
software-based encryption to facilitate communication between VMs and
accelerators. Even after hardware changes to enable TEEs on both sides and
software changes to adopt existing code to leverage these features, it results
in redundant data copies and hardware encryption at the bus-level and on the
accelerator thus degrading the performance and defeating the purpose of using
accelerators. In this paper, we reconsider the Arm Confidential Computing
Architecture (CCA) design-an upcoming TEE feature in Arm v9-to address this
gap. We observe that CCA offers the right abstraction and mechanisms to allow
confidential VM to use accelerators as a first class abstraction, while relying
on the hardware-based memory protection to preserve security. We build Acai, a
CCA-based solution, to demonstrate the feasibility of our approach without
changes to hardware or software on the CPU and the accelerator. Our
experimental results on GPU and FPGA show that Acai can achieve strong security
guarantees with low performance overheads.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: IDEA: Invariant Causal Defense for Graph Adversarial Robustness. (arXiv:2305.15792v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15792">http://arxiv.org/abs/2305.15792</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15792] IDEA: Invariant Causal Defense for Graph Adversarial Robustness](http://arxiv.org/abs/2305.15792) #defense</code></li>
<li>Summary: <p>Graph neural networks (GNNs) have achieved remarkable success in various
tasks, however, their vulnerability to adversarial attacks raises concerns for
the real-world applications. Existing defense methods can resist some attacks,
but suffer unbearable performance degradation under other unknown attacks. This
is due to their reliance on either limited observed adversarial examples to
optimize (adversarial training) or specific heuristics to alter graph or model
structures (graph purification or robust aggregation). In this paper, we
propose an Invariant causal DEfense method against adversarial Attacks (IDEA),
providing a new perspective to address this issue. The method aims to learn
causal features that possess strong predictability for labels and invariant
predictability across attacks, to achieve graph adversarial robustness. Through
modeling and analyzing the causal relationships in graph adversarial attacks,
we design two invariance objectives to learn the causal features. Extensive
experiments demonstrate that our IDEA significantly outperforms all the
baselines under both poisoning and evasion attacks on five benchmark datasets,
highlighting the strong and invariant predictability of IDEA. The
implementation of IDEA is available at
https://anonymous.4open.science/r/IDEA_repo-666B.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Fast Adversarial CNN-based Perturbation Attack on No-Reference Image- and Video-Quality Metrics. (arXiv:2305.15544v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15544">http://arxiv.org/abs/2305.15544</a></li>
<li>Code URL: <a href="https://github.com/katiashh/FACPA">https://github.com/katiashh/FACPA</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15544] Fast Adversarial CNN-based Perturbation Attack on No-Reference Image- and Video-Quality Metrics](http://arxiv.org/abs/2305.15544) #attack</code></li>
<li>Summary: <p>Modern neural-network-based no-reference image- and video-quality metrics
exhibit performance as high as full-reference metrics. These metrics are widely
used to improve visual quality in computer vision methods and compare video
processing methods. However, these metrics are not stable to traditional
adversarial attacks, which can cause incorrect results. Our goal is to
investigate the boundaries of no-reference metrics applicability, and in this
paper, we propose a fast adversarial perturbation attack on no-reference
quality metrics. The proposed attack (FACPA) can be exploited as a
preprocessing step in real-time video processing and compression algorithms.
This research can yield insights to further aid in designing of stable
neural-network-based no-reference quality metrics.
</p></li>
</ul>

<h3>Title: Mask Attack Detection Using Vascular-weighted Motion-robust rPPG Signals. (arXiv:2305.15940v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15940">http://arxiv.org/abs/2305.15940</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15940] Mask Attack Detection Using Vascular-weighted Motion-robust rPPG Signals](http://arxiv.org/abs/2305.15940) #attack</code></li>
<li>Summary: <p>Detecting 3D mask attacks to a face recognition system is challenging.
Although genuine faces and 3D face masks show significantly different remote
photoplethysmography (rPPG) signals, rPPG-based face anti-spoofing methods
often suffer from performance degradation due to unstable face alignment in the
video sequence and weak rPPG signals. To enhance the rPPG signal in a
motion-robust way, a landmark-anchored face stitching method is proposed to
align the faces robustly and precisely at the pixel-wise level by using both
SIFT keypoints and facial landmarks. To better encode the rPPG signal, a
weighted spatial-temporal representation is proposed, which emphasizes the face
regions with rich blood vessels. In addition, characteristics of rPPG signals
in different color spaces are jointly utilized. To improve the generalization
capability, a lightweight EfficientNet with a Gated Recurrent Unit (GRU) is
designed to extract both spatial and temporal features from the rPPG
spatial-temporal representation for classification. The proposed method is
compared with the state-of-the-art methods on five benchmark datasets under
both intra-dataset and cross-dataset evaluations. The proposed method shows a
significant and consistent improvement in performance over other
state-of-the-art rPPG-based methods for face spoofing detection.
</p></li>
</ul>

<h3>Title: How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks. (arXiv:2305.15587v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15587">http://arxiv.org/abs/2305.15587</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15587] How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks](http://arxiv.org/abs/2305.15587) #attack</code></li>
<li>Summary: <p>Natural Language Processing (NLP) models based on Machine Learning (ML) are
susceptible to adversarial attacks -- malicious algorithms that imperceptibly
modify input text to force models into making incorrect predictions. However,
evaluations of these attacks ignore the property of imperceptibility or study
it under limited settings. This entails that adversarial perturbations would
not pass any human quality gate and do not represent real threats to
human-checked NLP systems. To bypass this limitation and enable proper
assessment (and later, improvement) of NLP model robustness, we have surveyed
378 human participants about the perceptibility of text adversarial examples
produced by state-of-the-art methods. Our results underline that existing text
attacks are impractical in real-world scenarios where humans are involved. This
contrasts with previous smaller-scale human studies, which reported overly
optimistic conclusions regarding attack success. Through our work, we hope to
position human perceptibility as a first-class success criterion for text
attacks, and provide guidance for research to build effective attack algorithms
and, in turn, design appropriate defence mechanisms.
</p></li>
</ul>

<h3>Title: Healing Unsafe Dialogue Responses with Weak Supervision Signals. (arXiv:2305.15757v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15757">http://arxiv.org/abs/2305.15757</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15757] Healing Unsafe Dialogue Responses with Weak Supervision Signals](http://arxiv.org/abs/2305.15757) #attack</code></li>
<li>Summary: <p>Recent years have seen increasing concerns about the unsafe response
generation of large-scale dialogue systems, where agents will learn offensive
or biased behaviors from the real-world corpus. Some methods are proposed to
address the above issue by detecting and replacing unsafe training examples in
a pipeline style. Though effective, they suffer from a high annotation cost and
adapt poorly to unseen scenarios as well as adversarial attacks. Besides, the
neglect of providing safe responses (e.g. simply replacing with templates) will
cause the information-missing problem of dialogues. To address these issues, we
propose an unsupervised pseudo-label sampling method, TEMP, that can
automatically assign potential safe responses. Specifically, our TEMP method
groups responses into several clusters and samples multiple labels with an
adaptively sharpened sampling strategy, inspired by the observation that unsafe
samples in the clusters are usually few and distribute in the tail. Extensive
experiments in chitchat and task-oriented dialogues show that our TEMP
outperforms state-of-the-art models with weak supervision signals and obtains
comparable results under unsupervised learning settings.
</p></li>
</ul>

<h3>Title: Adaptive Data Analysis in a Balanced Adversarial Model. (arXiv:2305.15452v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15452">http://arxiv.org/abs/2305.15452</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15452] Adaptive Data Analysis in a Balanced Adversarial Model](http://arxiv.org/abs/2305.15452) #attack</code></li>
<li>Summary: <p>In adaptive data analysis, a mechanism gets $n$ i.i.d. samples from an
unknown distribution $D$, and is required to provide accurate estimations to a
sequence of adaptively chosen statistical queries with respect to $D$. Hardt
and Ullman (FOCS 2014) and Steinke and Ullman (COLT 2015) showed that in
general, it is computationally hard to answer more than $\Theta(n^2)$ adaptive
queries, assuming the existence of one-way functions.
</p></li>
</ul>

<p>However, these negative results strongly rely on an adversarial model that
significantly advantages the adversarial analyst over the mechanism, as the
analyst, who chooses the adaptive queries, also chooses the underlying
distribution $D$. This imbalance raises questions with respect to the
applicability of the obtained hardness results -- an analyst who has complete
knowledge of the underlying distribution $D$ would have little need, if at all,
to issue statistical queries to a mechanism which only holds a finite number of
samples from $D$.
</p>
<p>We consider more restricted adversaries, called \emph{balanced}, where each
such adversary consists of two separated algorithms: The \emph{sampler} who is
the entity that chooses the distribution and provides the samples to the
mechanism, and the \emph{analyst} who chooses the adaptive queries, but does
not have a prior knowledge of the underlying distribution. We improve the
quality of previous lower bounds by revisiting them using an efficient
\emph{balanced} adversary, under standard public-key cryptography assumptions.
We show that these stronger hardness assumptions are unavoidable in the sense
that any computationally bounded \emph{balanced} adversary that has the
structure of all known attacks, implies the existence of public-key
cryptography.
</p>

<h2>robust</h2>
<h3>Title: Characterizing Out-of-Distribution Error via Optimal Transport. (arXiv:2305.15640v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15640">http://arxiv.org/abs/2305.15640</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15640] Characterizing Out-of-Distribution Error via Optimal Transport](http://arxiv.org/abs/2305.15640) #robust</code></li>
<li>Summary: <p>Out-of-distribution (OOD) data poses serious challenges in deployed machine
learning models, so methods of predicting a model's performance on OOD data
without labels are important for machine learning safety. While a number of
methods have been proposed by prior work, they often underestimate the actual
error, sometimes by a large margin, which greatly impacts their applicability
to real tasks. In this work, we identify pseudo-label shift, or the difference
between the predicted and true OOD label distributions, as a key indicator to
this underestimation. Based on this observation, we introduce a novel method
for estimating model performance by leveraging optimal transport theory,
Confidence Optimal Transport (COT), and show that it provably provides more
robust error estimates in the presence of pseudo-label shift. Additionally, we
introduce an empirically-motivated variant of COT, Confidence Optimal Transport
with Thresholding (COTT), which applies thresholding to the individual
transport costs and further improves the accuracy of COT's error estimates. We
evaluate COT and COTT on a variety of standard benchmarks that induce various
types of distribution shift -- synthetic, novel subpopulation, and natural --
and show that our approaches significantly outperform existing state-of-the-art
methods with an up to 3x lower prediction error.
</p></li>
</ul>

<h3>Title: Cross-view Action Recognition Understanding From Exocentric to Egocentric Perspective. (arXiv:2305.15699v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15699">http://arxiv.org/abs/2305.15699</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15699] Cross-view Action Recognition Understanding From Exocentric to Egocentric Perspective](http://arxiv.org/abs/2305.15699) #robust</code></li>
<li>Summary: <p>Understanding action recognition in egocentric videos has emerged as a vital
research topic with numerous practical applications. With the limitation in the
scale of egocentric data collection, learning robust deep learning-based action
recognition models remains difficult. Transferring knowledge learned from the
large-scale exocentric data to the egocentric data is challenging due to the
difference in videos across views. Our work introduces a novel cross-view
learning approach to action recognition (CVAR) that effectively transfers
knowledge from the exocentric to the egocentric view. First, we introduce a
novel geometric-based constraint into the self-attention mechanism in
Transformer based on analyzing the camera positions between two views. Then, we
propose a new cross-view self-attention loss learned on unpaired cross-view
data to enforce the self-attention mechanism learning to transfer knowledge
across views. Finally, to further improve the performance of our cross-view
learning approach, we present the metrics to measure the correlations in videos
and attention maps effectively. Experimental results on standard egocentric
action recognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, and
EPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-art
performance.
</p></li>
</ul>

<h3>Title: PEARL: Preprocessing Enhanced Adversarial Robust Learning of Image Deraining for Semantic Segmentation. (arXiv:2305.15709v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15709">http://arxiv.org/abs/2305.15709</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15709] PEARL: Preprocessing Enhanced Adversarial Robust Learning of Image Deraining for Semantic Segmentation](http://arxiv.org/abs/2305.15709) #robust</code></li>
<li>Summary: <p>In light of the significant progress made in the development and application
of semantic segmentation tasks, there has been increasing attention towards
improving the robustness of segmentation models against natural degradation
factors (e.g., rain streaks) or artificially attack factors (e.g., adversarial
attack). Whereas, most existing methods are designed to address a single
degradation factor and are tailored to specific application scenarios. In this
work, we present the first attempt to improve the robustness of semantic
segmentation tasks by simultaneously handling different types of degradation
factors. Specifically, we introduce the Preprocessing Enhanced Adversarial
Robust Learning (PEARL) framework based on the analysis of our proposed Naive
Adversarial Training (NAT) framework. Our approach effectively handles both
rain streaks and adversarial perturbation by transferring the robustness of the
segmentation model to the image derain model. Furthermore, as opposed to the
commonly used Negative Adversarial Attack (NAA), we design the Auxiliary Mirror
Attack (AMA) to introduce positive information prior to the training of the
PEARL framework, which improves defense capability and segmentation
performance. Our extensive experiments and ablation studies based on different
derain methods and segmentation models have demonstrated the significant
performance improvement of PEARL with AMA in defense against various
adversarial attacks and rain streaks while maintaining high generalization
performance across different datasets.
</p></li>
</ul>

<h3>Title: CUEING: A pioneer work of encoding human gaze for autonomous driving. (arXiv:2305.15710v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15710">http://arxiv.org/abs/2305.15710</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15710] CUEING: A pioneer work of encoding human gaze for autonomous driving](http://arxiv.org/abs/2305.15710) #robust</code></li>
<li>Summary: <p>Recent analysis of incidents involving Autonomous Driving Systems (ADS) has
shown that the decision-making process of ADS can be significantly different
from that of human drivers. To improve the performance of ADS, it may be
helpful to incorporate the human decision-making process, particularly the
signals provided by the human gaze. There are many existing works to create
human gaze datasets and predict the human gaze using deep learning models.
However, current datasets of human gaze are noisy and include irrelevant
objects that can hinder model training. Additionally, existing CNN-based models
for predicting human gaze lack generalizability across different datasets and
driving conditions, and many models have a centre bias in their prediction such
that the gaze tends to be generated in the centre of the gaze map. To address
these gaps, we propose an adaptive method for cleansing existing human gaze
datasets and a robust convolutional self-attention gaze prediction model. Our
quantitative metrics show that our cleansing method improves models'
performance by up to 7.38% and generalizability by up to 8.24% compared to
those trained on the original datasets. Furthermore, our model demonstrates an
improvement of up to 12.13% in terms of generalizability compared to the
state-of-the-art (SOTA) models. Notably, it achieves these gains while
conserving up to 98.12% of computation resources.
</p></li>
</ul>

<h3>Title: POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference. (arXiv:2305.15727v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15727">http://arxiv.org/abs/2305.15727</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15727] POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference](http://arxiv.org/abs/2305.15727) #robust</code></li>
<li>Summary: <p>Despite the significant progress in six degrees-of-freedom (6DoF) object pose
estimation, existing methods have limited applicability in real-world scenarios
involving embodied agents and downstream 3D vision tasks. These limitations
mainly come from the necessity of 3D models, closed-category detection, and a
large number of densely annotated support views. To mitigate this issue, we
propose a general paradigm for object pose estimation, called Promptable Object
Pose Estimation (POPE). The proposed approach POPE enables zero-shot 6DoF
object pose estimation for any target object in any scene, while only a single
reference is adopted as the support view. To achieve this, POPE leverages the
power of the pre-trained large-scale 2D foundation model, employs a framework
with hierarchical feature representation and 3D geometry principles. Moreover,
it estimates the relative camera pose between object prompts and the target
object in new views, enabling both two-view and multi-view 6DoF pose estimation
tasks. Comprehensive experimental results demonstrate that POPE exhibits
unrivaled robust performance in zero-shot settings, by achieving a significant
reduction in the averaged Median Pose Error by 52.38% and 50.47% on the LINEMOD
and OnePose datasets, respectively. We also conduct more challenging testings
in causally captured images (see Figure 1), which further demonstrates the
robustness of POPE. Project page can be found with
https://paulpanwang.github.io/POPE/.
</p></li>
</ul>

<h3>Title: MPE4G: Multimodal Pretrained Encoder for Co-Speech Gesture Generation. (arXiv:2305.15740v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15740">http://arxiv.org/abs/2305.15740</a></li>
<li>Code URL: <a href="https://github.com/gt-kim/co-speech_gesture_generation">https://github.com/gt-kim/co-speech_gesture_generation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15740] MPE4G: Multimodal Pretrained Encoder for Co-Speech Gesture Generation](http://arxiv.org/abs/2305.15740) #robust</code></li>
<li>Summary: <p>When virtual agents interact with humans, gestures are crucial to delivering
their intentions with speech. Previous multimodal co-speech gesture generation
models required encoded features of all modalities to generate gestures. If
some input modalities are removed or contain noise, the model may not generate
the gestures properly. To acquire robust and generalized encodings, we propose
a novel framework with a multimodal pre-trained encoder for co-speech gesture
generation. In the proposed method, the multi-head-attention-based encoder is
trained with self-supervised learning to contain the information on each
modality. Moreover, we collect full-body gestures that consist of 3D joint
rotations to improve visualization and apply gestures to the extensible body
model. Through the series of experiments and human evaluation, the proposed
method renders realistic co-speech gestures not only when all input modalities
are given but also when the input modalities are missing or noisy.
</p></li>
</ul>

<h3>Title: MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation. (arXiv:2305.15904v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15904">http://arxiv.org/abs/2305.15904</a></li>
<li>Code URL: <a href="https://github.com/st-vincent1/mtcue">https://github.com/st-vincent1/mtcue</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15904] MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation](http://arxiv.org/abs/2305.15904) #robust</code></li>
<li>Summary: <p>Efficient utilisation of both intra- and extra-textual context remains one of
the critical gaps between machine and human translation. Existing research has
primarily focused on providing individual, well-defined types of context in
translation, such as the surrounding text or discrete external variables like
the speaker's gender. This work introduces MTCue, a novel neural machine
translation (NMT) framework that interprets all context (including discrete
variables) as text. MTCue learns an abstract representation of context,
enabling transferability across different data settings and leveraging similar
attributes in low-resource scenarios. With a focus on a dialogue domain with
access to document and metadata context, we extensively evaluate MTCue in four
language pairs in both translation directions. Our framework demonstrates
significant improvements in translation quality over a parameter-matched
non-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58).
Moreover, MTCue significantly outperforms a "tagging" baseline at translating
English text. Analysis reveals that the context encoder of MTCue learns a
representation space that organises context based on specific attributes, such
as formality, enabling effective zero-shot control. Pre-training on context
embeddings also improves MTCue's few-shot performance compared to the "tagging"
baseline. Finally, an ablation study conducted on model components and
contextual variables further supports the robustness of MTCue for context-based
NMT.
</p></li>
</ul>

<h3>Title: A Robust Classifier Under Missing-Not-At-Random Sample Selection Bias. (arXiv:2305.15641v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15641">http://arxiv.org/abs/2305.15641</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15641] A Robust Classifier Under Missing-Not-At-Random Sample Selection Bias](http://arxiv.org/abs/2305.15641) #robust</code></li>
<li>Summary: <p>The shift between the training and testing distributions is commonly due to
sample selection bias, a type of bias caused by non-random sampling of examples
to be included in the training set. Although there are many approaches proposed
to learn a classifier under sample selection bias, few address the case where a
subset of labels in the training set are missing-not-at-random (MNAR) as a
result of the selection process. In statistics, Greene's method formulates this
type of sample selection with logistic regression as the prediction model.
However, we find that simply integrating this method into a robust
classification framework is not effective for this bias setting. In this paper,
we propose BiasCorr, an algorithm that improves on Greene's method by modifying
the original training set in order for a classifier to learn under MNAR sample
selection bias. We provide theoretical guarantee for the improvement of
BiasCorr over Greene's method by analyzing its bias. Experimental results on
real-world datasets demonstrate that BiasCorr produces robust classifiers and
can be extended to outperform state-of-the-art classifiers that have been
proposed to train under sample selection bias.
</p></li>
</ul>

<h3>Title: The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning. (arXiv:2305.15703v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15703">http://arxiv.org/abs/2305.15703</a></li>
<li>Code URL: <a href="https://github.com/kevinzhou497/distcb">https://github.com/kevinzhou497/distcb</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15703] The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning](http://arxiv.org/abs/2305.15703) #robust</code></li>
<li>Summary: <p>While distributional reinforcement learning (RL) has demonstrated empirical
success, the question of when and why it is beneficial has remained unanswered.
In this work, we provide one explanation for the benefits of distributional RL
through the lens of small-loss bounds, which scale with the instance-dependent
optimal cost. If the optimal cost is small, our bounds are stronger than those
from non-distributional approaches. As warmup, we show that learning the cost
distribution leads to small-loss regret bounds in contextual bandits (CB), and
we find that distributional CB empirically outperforms the state-of-the-art on
three challenging tasks. For online RL, we propose a distributional
version-space algorithm that constructs confidence sets using maximum
likelihood estimation, and we prove that it achieves small-loss regret in the
tabular MDPs and enjoys small-loss PAC bounds in latent variable models.
Building on similar insights, we propose a distributional offline RL algorithm
based on the pessimism principle and prove that it enjoys small-loss PAC
bounds, which exhibit a novel robustness property. For both online and offline
RL, our results provide the first theoretical benefits of learning
distributions even when we only need the mean for making decisions.
</p></li>
</ul>

<h3>Title: Robust Ante-hoc Graph Explainer using Bilevel Optimization. (arXiv:2305.15745v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15745">http://arxiv.org/abs/2305.15745</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15745] Robust Ante-hoc Graph Explainer using Bilevel Optimization](http://arxiv.org/abs/2305.15745) #robust</code></li>
<li>Summary: <p>Explaining the decisions made by machine learning models for high-stakes
applications is critical for increasing transparency and guiding improvements
to these decisions. This is particularly true in the case of models for graphs,
where decisions often depend on complex patterns combining rich structural and
attribute data. While recent work has focused on designing so-called post-hoc
explainers, the question of what constitutes a good explanation remains open.
One intuitive property is that explanations should be sufficiently informative
to enable humans to approximately reproduce the predictions given the data.
However, we show that post-hoc explanations do not achieve this goal as their
explanations are highly dependent on fixed model parameters (e.g., learned GNN
weights). To address this challenge, this paper proposes RAGE (Robust Ante-hoc
Graph Explainer), a novel and flexible ante-hoc explainer designed to discover
explanations for a broad class of graph neural networks using bilevel
optimization. RAGE is able to efficiently identify explanations that contain
the full information needed for prediction while still enabling humans to rank
these explanations based on their influence. Our experiments, based on graph
classification and regression, show that RAGE explanations are more robust than
existing post-hoc and ante-hoc approaches and often achieve similar or better
accuracy than state-of-the-art models.
</p></li>
</ul>

<h3>Title: Unifying gradient regularization for Heterogeneous Graph Neural Networks. (arXiv:2305.15811v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15811">http://arxiv.org/abs/2305.15811</a></li>
<li>Code URL: <a href="https://github.com/anonymous2nips2023/grug">https://github.com/anonymous2nips2023/grug</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15811] Unifying gradient regularization for Heterogeneous Graph Neural Networks](http://arxiv.org/abs/2305.15811) #robust</code></li>
<li>Summary: <p>Heterogeneous Graph Neural Networks (HGNNs) are a class of powerful deep
learning methods widely used to learn representations of heterogeneous graphs.
Despite the fast development of HGNNs, they still face some challenges such as
over-smoothing, and non-robustness. Previous studies have shown that these
problems can be reduced by using gradient regularization methods. However, the
existing gradient regularization methods focus on either graph topology or node
features. There is no universal approach to integrate these features, which
severely affects the efficiency of regularization. In addition, the inclusion
of gradient regularization into HGNNs sometimes leads to some problems, such as
an unstable training process, increased complexity and insufficient coverage
regularized information. Furthermore, there is still short of a complete
theoretical analysis of the effects of gradient regularization on HGNNs. In
this paper, we propose a novel gradient regularization method called Grug,
which iteratively applies regularization to the gradients generated by both
propagated messages and the node features during the message-passing process.
Grug provides a unified framework integrating graph topology and node features,
based on which we conduct a detailed theoretical analysis of their
effectiveness. Specifically, the theoretical analyses elaborate the advantages
of Grug: 1) Decreasing sample variance during the training process (Stability);
2) Enhancing the generalization of the model (Universality); 3) Reducing the
complexity of the model (Simplicity); 4) Improving the integrity and diversity
of graph information utilization (Diversity). As a result, Grug has the
potential to surpass the theoretical upper bounds set by DropMessage (AAAI-23
Distinguished Papers). In addition, we evaluate Grug on five public real-world
datasets with two downstream tasks.
</p></li>
</ul>

<h3>Title: TabGSL: Graph Structure Learning for Tabular Data Prediction. (arXiv:2305.15843v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15843">http://arxiv.org/abs/2305.15843</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15843] TabGSL: Graph Structure Learning for Tabular Data Prediction](http://arxiv.org/abs/2305.15843) #robust</code></li>
<li>Summary: <p>This work presents a novel approach to tabular data prediction leveraging
graph structure learning and graph neural networks. Despite the prevalence of
tabular data in real-world applications, traditional deep learning methods
often overlook the potentially valuable associations between data instances.
Such associations can offer beneficial insights for classification tasks, as
instances may exhibit similar patterns of correlations among features and
target labels. This information can be exploited by graph neural networks,
necessitating robust graph structures. However, existing studies primarily
focus on improving graph structure from noisy data, largely neglecting the
possibility of deriving graph structures from tabular data. We present a novel
solution, Tabular Graph Structure Learning (TabGSL), to enhance tabular data
prediction by simultaneously learning instance correlation and feature
interaction within a unified framework. This is achieved through a proposed
graph contrastive learning module, along with transformer-based feature
extractor and graph neural network. Comprehensive experiments conducted on 30
benchmark tabular datasets demonstrate that TabGSL markedly outperforms both
tree-based models and recent deep learning-based tabular models. Visualizations
of the learned instance embeddings further substantiate the effectiveness of
TabGSL.
</p></li>
</ul>

<h3>Title: Quantifying the Intrinsic Usefulness of Attributional Explanations for Graph Neural Networks with Artificial Simulatability Studies. (arXiv:2305.15961v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15961">http://arxiv.org/abs/2305.15961</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15961] Quantifying the Intrinsic Usefulness of Attributional Explanations for Graph Neural Networks with Artificial Simulatability Studies](http://arxiv.org/abs/2305.15961) #robust</code></li>
<li>Summary: <p>Despite the increasing relevance of explainable AI, assessing the quality of
explanations remains a challenging issue. Due to the high costs associated with
human-subject experiments, various proxy metrics are often used to
approximately quantify explanation quality. Generally, one possible
interpretation of the quality of an explanation is its inherent value for
teaching a related concept to a student. In this work, we extend artificial
simulatability studies to the domain of graph neural networks. Instead of
costly human trials, we use explanation-supervisable graph neural networks to
perform simulatability studies to quantify the inherent usefulness of
attributional graph explanations. We perform an extensive ablation study to
investigate the conditions under which the proposed analyses are most
meaningful. We additionally validate our methods applicability on real-world
graph classification and regression datasets. We find that relevant
explanations can significantly boost the sample efficiency of graph neural
networks and analyze the robustness towards noise and bias in the explanations.
We believe that the notion of usefulness obtained from our proposed
simulatability analysis provides a dimension of explanation quality that is
largely orthogonal to the common practice of faithfulness and has great
potential to expand the toolbox of explanation quality assessments,
specifically for graph explanations.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Feature space reduction method for ultrahigh-dimensional, multiclass data: Random forest-based multiround screening (RFMS). (arXiv:2305.15793v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15793">http://arxiv.org/abs/2305.15793</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15793] Feature space reduction method for ultrahigh-dimensional, multiclass data: Random forest-based multiround screening (RFMS)](http://arxiv.org/abs/2305.15793) #biometric</code></li>
<li>Summary: <p>In recent years, numerous screening methods have been published for
ultrahigh-dimensional data that contain hundreds of thousands of features;
however, most of these features cannot handle data with thousands of classes.
Prediction models built to authenticate users based on multichannel biometric
data result in this type of problem. In this study, we present a novel method
known as random forest-based multiround screening (RFMS) that can be
effectively applied under such circumstances. The proposed algorithm divides
the feature space into small subsets and executes a series of partial model
builds. These partial models are used to implement tournament-based sorting and
the selection of features based on their importance. To benchmark RFMS, a
synthetic biometric feature space generator known as BiometricBlender is
employed. Based on the results, the RFMS is on par with industry-standard
feature screening methods while simultaneously possessing many advantages over
these methods.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Transcending Grids: Point Clouds and Surface Representations Powering Neurological Processing. (arXiv:2305.15426v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15426">http://arxiv.org/abs/2305.15426</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15426] Transcending Grids: Point Clouds and Surface Representations Powering Neurological Processing](http://arxiv.org/abs/2305.15426) #extraction</code></li>
<li>Summary: <p>In healthcare, accurately classifying medical images is vital, but
conventional methods often hinge on medical data with a consistent grid
structure, which may restrict their overall performance. Recent medical
research has been focused on tweaking the architectures to attain better
performance without giving due consideration to the representation of data. In
this paper, we present a novel approach for transforming grid based data into
its higher dimensional representations, leveraging unstructured point cloud
data structures. We first generate a sparse point cloud from an image by
integrating pixel color information as spatial coordinates. Next, we construct
a hypersurface composed of points based on the image dimensions, with each
smooth section within this hypersurface symbolizing a specific pixel location.
Polygonal face construction is achieved using an adjacency tensor. Finally, a
dense point cloud is generated by densely sampling the constructed
hypersurface, with a focus on regions of higher detail. The effectiveness of
our approach is demonstrated on a publicly accessible brain tumor dataset,
achieving significant improvements over existing classification techniques.
This methodology allows the extraction of intricate details from the original
image, opening up new possibilities for advanced image analysis and processing
tasks.
</p></li>
</ul>

<h3>Title: Deep Neural Networks in Video Human Action Recognition: A Review. (arXiv:2305.15692v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15692">http://arxiv.org/abs/2305.15692</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15692] Deep Neural Networks in Video Human Action Recognition: A Review](http://arxiv.org/abs/2305.15692) #extraction</code></li>
<li>Summary: <p>Currently, video behavior recognition is one of the most foundational tasks
of computer vision. The 2D neural networks of deep learning are built for
recognizing pixel-level information such as images with RGB, RGB-D, or optical
flow formats, with the current increasingly wide usage of surveillance video
and more tasks related to human action recognition. There are increasing tasks
requiring temporal information for frames dependency analysis. The researchers
have widely studied video-based recognition rather than
image-based(pixel-based) only to extract more informative elements from
geometry tasks. Our current related research addresses multiple novel proposed
research works and compares their advantages and disadvantages between the
derived deep learning frameworks rather than machine learning frameworks. The
comparison happened between existing frameworks and datasets, which are video
format data only. Due to the specific properties of human actions and the
increasingly wide usage of deep neural networks, we collected all research
works within the last three years between 2020 to 2022. In our article, the
performance of deep neural networks surpassed most of the techniques in the
feature learning and extraction tasks, especially video action recognition.
</p></li>
</ul>

<h3>Title: Learning Occupancy for Monocular 3D Object Detection. (arXiv:2305.15694v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15694">http://arxiv.org/abs/2305.15694</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15694] Learning Occupancy for Monocular 3D Object Detection](http://arxiv.org/abs/2305.15694) #extraction</code></li>
<li>Summary: <p>Monocular 3D detection is a challenging task due to the lack of accurate 3D
information. Existing approaches typically rely on geometry constraints and
dense depth estimates to facilitate the learning, but often fail to fully
exploit the benefits of three-dimensional feature extraction in frustum and 3D
space. In this paper, we propose \textbf{OccupancyM3D}, a method of learning
occupancy for monocular 3D detection. It directly learns occupancy in frustum
and 3D space, leading to more discriminative and informative 3D features and
representations. Specifically, by using synchronized raw sparse LiDAR point
clouds, we define the space status and generate voxel-based occupancy labels.
We formulate occupancy prediction as a simple classification problem and design
associated occupancy losses. Resulting occupancy estimates are employed to
enhance original frustum/3D features. As a result, experiments on KITTI and
Waymo open datasets demonstrate that the proposed method achieves a new state
of the art and surpasses other methods by a significant margin. Codes and
pre-trained models will be available at
\url{https://github.com/SPengLiang/OccupancyM3D}.
</p></li>
</ul>

<h3>Title: Exploring Automatically Perturbed Natural Language Explanations in Relation Extraction. (arXiv:2305.15520v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15520">http://arxiv.org/abs/2305.15520</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15520] Exploring Automatically Perturbed Natural Language Explanations in Relation Extraction](http://arxiv.org/abs/2305.15520) #extraction</code></li>
<li>Summary: <p>Previous research has demonstrated that natural language explanations provide
valuable inductive biases that guide models, thereby improving the
generalization ability and data efficiency. In this paper, we undertake a
systematic examination of the effectiveness of these explanations. Remarkably,
we find that corrupted explanations with diminished inductive biases can
achieve competitive or superior performance compared to the original
explanations. Our findings furnish novel insights into the characteristics of
natural language explanations in the following ways: (1) the impact of
explanations varies across different training styles and datasets, with
previously believed improvements primarily observed in frozen language models.
(2) While previous research has attributed the effect of explanations solely to
their inductive biases, our study shows that the effect persists even when the
explanations are completely corrupted. We propose that the main effect is due
to the provision of additional context space. (3) Utilizing the proposed
automatic perturbed context, we were able to attain comparable results to
annotated explanations, but with a significant increase in computational
efficiency, 20-30 times faster.
</p></li>
</ul>

<h3>Title: Automated Refugee Case Analysis: An NLP Pipeline for Supporting Legal Practitioners. (arXiv:2305.15533v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15533">http://arxiv.org/abs/2305.15533</a></li>
<li>Code URL: <a href="https://github.com/clairebarale/refugee_cases_ner">https://github.com/clairebarale/refugee_cases_ner</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15533] Automated Refugee Case Analysis: An NLP Pipeline for Supporting Legal Practitioners](http://arxiv.org/abs/2305.15533) #extraction</code></li>
<li>Summary: <p>In this paper, we introduce an end-to-end pipeline for retrieving,
processing, and extracting targeted information from legal cases. We
investigate an under-studied legal domain with a case study on refugee law in
Canada. Searching case law for past similar cases is a key part of legal work
for both lawyers and judges, the potential end-users of our prototype. While
traditional named-entity recognition labels such as dates provide meaningful
information in legal work, we propose to extend existing models and retrieve a
total of 19 useful categories of items from refugee cases. After creating a
novel data set of cases, we perform information extraction based on
state-of-the-art neural named-entity recognition (NER). We test different
architectures including two transformer models, using contextual and
non-contextual embeddings, and compare general purpose versus domain-specific
pre-training. The results demonstrate that models pre-trained on legal data
perform best despite their smaller size, suggesting that domain matching had a
larger effect than network architecture. We achieve a F1 score above 90% on
five of the targeted categories and over 80% on four further categories.
</p></li>
</ul>

<h3>Title: Jointprop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation. (arXiv:2305.15872v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15872">http://arxiv.org/abs/2305.15872</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15872] Jointprop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation](http://arxiv.org/abs/2305.15872) #extraction</code></li>
<li>Summary: <p>Semi-supervised learning has been an important approach to address challenges
in extracting entities and relations from limited data. However, current
semi-supervised works handle the two tasks (i.e., Named Entity Recognition and
Relation Extraction) separately and ignore the cross-correlation of entity and
relation instances as well as the existence of similar instances across
unlabeled data. To alleviate the issues, we propose Jointprop, a Heterogeneous
Graph-based Propagation framework for joint semi-supervised entity and relation
extraction, which captures the global structure information between individual
tasks and exploits interactions within unlabeled data. Specifically, we
construct a unified span-based heterogeneous graph from entity and relation
candidates and propagate class labels based on confidence scores. We then
employ a propagation learning scheme to leverage the affinities between
labelled and unlabeled samples. Experiments on benchmark datasets show that our
framework outperforms the state-of-the-art semi-supervised approaches on NER
and RE tasks. We show that the joint semi-supervised learning of the two tasks
benefits from their codependency and validates the importance of utilizing the
shared information between unlabeled data.
</p></li>
</ul>

<h3>Title: LFTK: Handcrafted Features in Computational Linguistics. (arXiv:2305.15878v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15878">http://arxiv.org/abs/2305.15878</a></li>
<li>Code URL: <a href="https://github.com/brucewlee/lftk">https://github.com/brucewlee/lftk</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15878] LFTK: Handcrafted Features in Computational Linguistics](http://arxiv.org/abs/2305.15878) #extraction</code></li>
<li>Summary: <p>Past research has identified a rich set of handcrafted linguistic features
that can potentially assist various tasks. However, their extensive number
makes it difficult to effectively select and utilize existing handcrafted
features. Coupled with the problem of inconsistent implementation across
research works, there has been no categorization scheme or generally-accepted
feature names. This creates unwanted confusion. Also, most existing handcrafted
feature extraction libraries are not open-source or not actively maintained. As
a result, a researcher often has to build such an extraction system from the
ground up.
</p></li>
</ul>

<p>We collect and categorize more than 220 popular handcrafted features grounded
on past literature. Then, we conduct a correlation analysis study on several
task-specific datasets and report the potential use cases of each feature.
Lastly, we devise a multilingual handcrafted linguistic feature extraction
system in a systematically expandable manner. We open-source our system for
public access to a rich set of pre-implemented handcrafted features. Our system
is coined LFTK and is the largest of its kind. Find it at
github.com/brucewlee/lftk.
</p>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Composite Saddle Point Optimization. (arXiv:2305.15643v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15643">http://arxiv.org/abs/2305.15643</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15643] Federated Composite Saddle Point Optimization](http://arxiv.org/abs/2305.15643) #federate</code></li>
<li>Summary: <p>Federated learning (FL) approaches for saddle point problems (SPP) have
recently gained in popularity due to the critical role they play in machine
learning (ML). Existing works mostly target smooth unconstrained objectives in
Euclidean space, whereas ML problems often involve constraints or non-smooth
regularization, which results in a need for composite optimization. Addressing
these issues, we propose Federated Dual Extrapolation (FeDualEx), an extra-step
primal-dual algorithm, which is the first of its kind that encompasses both
saddle point optimization and composite objectives under the FL paradigm. Both
the convergence analysis and the empirical evaluation demonstrate the
effectiveness of FeDualEx in these challenging settings. In addition, even for
the sequential version of FeDualEx, we provide rates for the stochastic
composite saddle point setting which, to our knowledge, are not found in prior
literature.
</p></li>
</ul>

<h3>Title: pFedSim: Similarity-Aware Model Aggregation Towards Personalized Federated Learning. (arXiv:2305.15706v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15706">http://arxiv.org/abs/2305.15706</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15706] pFedSim: Similarity-Aware Model Aggregation Towards Personalized Federated Learning](http://arxiv.org/abs/2305.15706) #federate</code></li>
<li>Summary: <p>The federated learning (FL) paradigm emerges to preserve data privacy during
model training by only exposing clients' model parameters rather than original
data. One of the biggest challenges in FL lies in the non-IID (not identical
and independently distributed) data (a.k.a., data heterogeneity) distributed on
clients. To address this challenge, various personalized FL (pFL) methods are
proposed such as similarity-based aggregation and model decoupling. The former
one aggregates models from clients of a similar data distribution. The later
one decouples a neural network (NN) model into a feature extractor and a
classifier. Personalization is captured by classifiers which are obtained by
local training. To advance pFL, we propose a novel pFedSim (pFL based on model
similarity) algorithm in this work by combining these two kinds of methods.
More specifically, we decouple a NN model into a personalized feature
extractor, obtained by aggregating models from similar clients, and a
classifier, which is obtained by local training and used to estimate client
similarity. Compared with the state-of-the-art baselines, the advantages of
pFedSim include: 1) significantly improved model accuracy; 2) low communication
and computation overhead; 3) a low risk of privacy leakage; 4) no requirement
for any external public information. To demonstrate the superiority of pFedSim,
extensive experiments are conducted on real datasets. The results validate the
superb performance of our algorithm which can significantly outperform
baselines under various heterogeneous data settings.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments. (arXiv:2305.15700v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15700">http://arxiv.org/abs/2305.15700</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15700] Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments](http://arxiv.org/abs/2305.15700) #fair</code></li>
<li>Summary: <p>Continual semantic segmentation aims to learn new classes while maintaining
the information from the previous classes. Although prior studies have shown
impressive progress in recent years, the fairness concern in the continual
semantic segmentation needs to be better addressed. Meanwhile, fairness is one
of the most vital factors in deploying the deep learning model, especially in
human-related or safety applications. In this paper, we present a novel
Fairness Continual Learning approach to the semantic segmentation problem. In
particular, under the fairness objective, a new fairness continual learning
framework is proposed based on class distributions. Then, a novel Prototypical
Contrastive Clustering loss is proposed to address the significant challenges
in continual learning, i.e., catastrophic forgetting and background shift. Our
proposed loss has also been proven as a novel, generalized learning paradigm of
knowledge distillation commonly used in continual learning. Moreover, the
proposed Conditional Structural Consistency loss further regularized the
structural constraint of the predicted segmentation. Our proposed approach has
achieved State-of-the-Art performance on three standard scene understanding
benchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC, and promoted the fairness
of the segmentation model.
</p></li>
</ul>

<h3>Title: Language Model Tokenizers Introduce Unfairness Between Languages. (arXiv:2305.15425v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15425">http://arxiv.org/abs/2305.15425</a></li>
<li>Code URL: <a href="https://github.com/AleksandarPetrov/tokenization-fairness">https://github.com/AleksandarPetrov/tokenization-fairness</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15425] Language Model Tokenizers Introduce Unfairness Between Languages](http://arxiv.org/abs/2305.15425) #fair</code></li>
<li>Summary: <p>Recent language models have shown impressive multilingual performance, even
when not explicitly trained for it. Despite this, concerns have been raised
about the quality of their outputs across different languages. In this paper,
we show how disparity in the treatment of different languages arises at the
tokenization stage, well before a model is even invoked. The same text
translated into different languages can have drastically different tokenization
lengths, with differences up to 15 times in some cases. These disparities
persist across the 17 tokenizers we evaluate, even if they are intentionally
trained for multilingual support. Character-level and byte-level models also
exhibit over 4 times the difference in the encoding length for some language
pairs. This induces unfair treatment for some language communities in regard to
the cost of accessing commercial language services, the processing time and
latency, as well as the amount of content that can be provided as context to
the models. Therefore, we make the case that we should train future language
models using multilingually fair tokenizers.
</p></li>
</ul>

<h3>Title: GFairHint: Improving Individual Fairness for Graph Neural Networks via Fairness Hint. (arXiv:2305.15622v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15622">http://arxiv.org/abs/2305.15622</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15622] GFairHint: Improving Individual Fairness for Graph Neural Networks via Fairness Hint](http://arxiv.org/abs/2305.15622) #fair</code></li>
<li>Summary: <p>Given the growing concerns about fairness in machine learning and the
impressive performance of Graph Neural Networks (GNNs) on graph data learning,
algorithmic fairness in GNNs has attracted significant attention. While many
existing studies improve fairness at the group level, only a few works promote
individual fairness, which renders similar outcomes for similar individuals. A
desirable framework that promotes individual fairness should (1) balance
between fairness and performance, (2) accommodate two commonly-used individual
similarity measures (externally annotated and computed from input features),
(3) generalize across various GNN models, and (4) be computationally efficient.
Unfortunately, none of the prior work achieves all the desirables. In this
work, we propose a novel method, GFairHint, which promotes individual fairness
in GNNs and achieves all aforementioned desirables. GFairHint learns fairness
representations through an auxiliary link prediction task, and then
concatenates the representations with the learned node embeddings in original
GNNs as a "fairness hint". Through extensive experimental investigations on
five real-world graph datasets under three prevalent GNN models covering both
individual similarity measures above, GFairHint achieves the best fairness
results in almost all combinations of datasets with various backbone models,
while generating comparable utility results, with much less computational cost
compared to the previous state-of-the-art (SoTA) method.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: On the Impact of Knowledge Distillation for Model Interpretability. (arXiv:2305.15734v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15734">http://arxiv.org/abs/2305.15734</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15734] On the Impact of Knowledge Distillation for Model Interpretability](http://arxiv.org/abs/2305.15734) #interpretability</code></li>
<li>Summary: <p>Several recent studies have elucidated why knowledge distillation (KD)
improves model performance. However, few have researched the other advantages
of KD in addition to its improving model performance. In this study, we have
attempted to show that KD enhances the interpretability as well as the accuracy
of models. We measured the number of concept detectors identified in network
dissection for a quantitative comparison of model interpretability. We
attributed the improvement in interpretability to the class-similarity
information transferred from the teacher to student models. First, we confirmed
the transfer of class-similarity information from the teacher to student model
via logit distillation. Then, we analyzed how class-similarity information
affects model interpretability in terms of its presence or absence and degree
of similarity information. We conducted various quantitative and qualitative
experiments and examined the results on different datasets, different KD
methods, and according to different measures of interpretability. Our research
showed that KD models by large models could be used more reliably in various
fields.
</p></li>
</ul>

<h3>Title: Concept-Centric Transformers: Concept Transformers with Object-Centric Concept Learning for Interpretability. (arXiv:2305.15775v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15775">http://arxiv.org/abs/2305.15775</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15775] Concept-Centric Transformers: Concept Transformers with Object-Centric Concept Learning for Interpretability](http://arxiv.org/abs/2305.15775) #interpretability</code></li>
<li>Summary: <p>Attention mechanisms have greatly improved the performance of deep-learning
models on visual, NLP, and multimodal tasks while also providing tools to aid
in the model's interpretability. In particular, attention scores over input
regions or concrete image features can be used to measure how much the attended
elements contribute to the model inference. The recently proposed Concept
Transformer (CT) generalizes the Transformer attention mechanism from such
low-level input features to more abstract, intermediate-level latent concepts
that better allow human analysts to more directly assess an explanation for the
reasoning of the model about any particular output classification. However, the
concept learning employed by CT implicitly assumes that across every image in a
class, each image patch makes the same contribution to concepts that
characterize membership in that class. Instead of using the CT's
image-patch-centric concepts, object-centric concepts could lead to better
classification performance as well as better explainability. Thus, we propose
Concept-Centric Transformers (CCT), a new family of concept transformers that
provides more robust explanations and performance by integrating a novel
concept-extraction module based on object-centric learning. We test our
proposed CCT against the CT and several other existing approaches on
classification problems for MNIST (odd/even), CIFAR100 (super-classes), and
CUB-200-2011 (bird species). Our experiments demonstrate that CCT not only
achieves significantly better classification accuracy than all selected
benchmark classifiers across all three of our test problems, but it generates
more consistent concept-based explanations of classification output when
compared to CT.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: TLNets: Transformation Learning Networks for long-range time-series prediction. (arXiv:2305.15770v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15770">http://arxiv.org/abs/2305.15770</a></li>
<li>Code URL: <a href="https://github.com/anonymity111222/tlnets">https://github.com/anonymity111222/tlnets</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15770] TLNets: Transformation Learning Networks for long-range time-series prediction](http://arxiv.org/abs/2305.15770) #explainability</code></li>
<li>Summary: <p>Time series prediction is a prevalent issue across various disciplines, such
as meteorology, traffic surveillance, investment, and energy production and
consumption. Many statistical and machine-learning strategies have been
developed to tackle this problem. However, these approaches either lack
explainability or exhibit less satisfactory performance when the prediction
horizon increases. To this end, we propose a novel plan for the designing of
networks' architecture based on transformations, possessing the potential to
achieve an enhanced receptive field in learning which brings benefits to fuse
features across scales. In this context, we introduce four different
transformation mechanisms as bases to construct the learning model including
Fourier Transform (FT), Singular Value Decomposition (SVD), matrix
multiplication and Conv block. Hence, we develop four learning models based on
the above building blocks, namely, FT-Matrix, FT-SVD, FT-Conv, and Conv-SVD.
Note that the FT and SVD blocks are capable of learning global information,
while the Conv blocks focus on learning local information. The matrix block is
sparsely designed to learn both global and local information simultaneously.
The above Transformation Learning Networks (TLNets) have been extensively
tested and compared with multiple baseline models based on several real-world
datasets and showed clear potential in long-range time-series forecasting.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Unsupervised Semantic Correspondence Using Stable Diffusion. (arXiv:2305.15581v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15581">http://arxiv.org/abs/2305.15581</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15581] Unsupervised Semantic Correspondence Using Stable Diffusion](http://arxiv.org/abs/2305.15581) #diffusion</code></li>
<li>Summary: <p>Text-to-image diffusion models are now capable of generating images that are
often indistinguishable from real images. To generate such images, these models
must understand the semantics of the objects they are asked to generate. In
this work we show that, without any training, one can leverage this semantic
knowledge within diffusion models to find semantic correspondences -- locations
in multiple images that have the same semantic meaning. Specifically, given an
image, we optimize the prompt embeddings of these models for maximum attention
on the regions of interest. These optimized embeddings capture semantic
information about the location, which can then be transferred to another image.
By doing so we obtain results on par with the strongly supervised state of the
art on the PF-Willow dataset and significantly outperform (20.9% relative for
the SPair-71k dataset) any existing weakly or unsupervised method on PF-Willow,
CUB-200 and SPair-71k datasets.
</p></li>
</ul>

<h3>Title: Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps. (arXiv:2305.15583v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15583">http://arxiv.org/abs/2305.15583</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15583] Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps](http://arxiv.org/abs/2305.15583) #diffusion</code></li>
<li>Summary: <p>Denoising Diffusion Probabilistic Models (DDPM) have shown remarkable
efficacy in the synthesis of high-quality images. However, their inference
process characteristically requires numerous, potentially hundreds, of
iterative steps, which could lead to the problem of exposure bias due to the
accumulation of prediction errors over iterations. Previous work has attempted
to mitigate this issue by perturbing inputs during training, which consequently
mandates the retraining of the DDPM. In this work, we conduct a systematic
study of exposure bias in diffusion models and, intriguingly, we find that the
exposure bias could be alleviated with a new sampling method, without
retraining the model. We empirically and theoretically show that, during
inference, for each backward time step $t$ and corresponding state $\hat{x}_t$,
there might exist another time step $t_s$ which exhibits superior coupling with
$\hat{x}_t$. Based on this finding, we introduce an inference method named
Time-Shift Sampler. Our framework can be seamlessly integrated with existing
sampling algorithms, such as DDIM or DDPM, inducing merely minimal additional
computations. Experimental results show that our proposed framework can
effectively enhance the quality of images generated by existing sampling
algorithms.
</p></li>
</ul>

<h3>Title: Zero-shot Generation of Training Data with Denoising Diffusion Probabilistic Model for Handwritten Chinese Character Recognition. (arXiv:2305.15660v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15660">http://arxiv.org/abs/2305.15660</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15660] Zero-shot Generation of Training Data with Denoising Diffusion Probabilistic Model for Handwritten Chinese Character Recognition](http://arxiv.org/abs/2305.15660) #diffusion</code></li>
<li>Summary: <p>There are more than 80,000 character categories in Chinese while most of them
are rarely used. To build a high performance handwritten Chinese character
recognition (HCCR) system supporting the full character set with a traditional
approach, many training samples need be collected for each character category,
which is both time-consuming and expensive. In this paper, we propose a novel
approach to transforming Chinese character glyph images generated from font
libraries to handwritten ones with a denoising diffusion probabilistic model
(DDPM). Training from handwritten samples of a small character set, the DDPM is
capable of mapping printed strokes to handwritten ones, which makes it possible
to generate photo-realistic and diverse style handwritten samples of unseen
character categories. Combining DDPM-synthesized samples of unseen categories
with real samples of other categories, we can build an HCCR system to support
the full character set. Experimental results on CASIA-HWDB dataset with 3,755
character categories show that the HCCR systems trained with synthetic samples
perform similarly with the one trained with real samples in terms of
recognition accuracy. The proposed method has the potential to address HCCR
with a larger vocabulary.
</p></li>
</ul>

<h3>Title: Knowledge Diffusion for Distillation. (arXiv:2305.15712v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15712">http://arxiv.org/abs/2305.15712</a></li>
<li>Code URL: <a href="https://github.com/hunto/diffkd">https://github.com/hunto/diffkd</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15712] Knowledge Diffusion for Distillation](http://arxiv.org/abs/2305.15712) #diffusion</code></li>
<li>Summary: <p>The representation gap between teacher and student is an emerging topic in
knowledge distillation (KD). To reduce the gap and improve the performance,
current methods often resort to complicated training schemes, loss functions,
and feature alignments, which are task-specific and feature-specific. In this
paper, we state that the essence of these methods is to discard the noisy
information and distill the valuable information in the feature, and propose a
novel KD method dubbed DiffKD, to explicitly denoise and match features using
diffusion models. Our approach is based on the observation that student
features typically contain more noises than teacher features due to the smaller
capacity of student model. To address this, we propose to denoise student
features using a diffusion model trained by teacher features. This allows us to
perform better distillation between the refined clean feature and teacher
feature. Additionally, we introduce a light-weight diffusion model with a
linear autoencoder to reduce the computation cost and an adpative noise
matching module to improve the denoising performance. Extensive experiments
demonstrate that DiffKD is effective across various types of features and
achieves state-of-the-art performance consistently on image classification,
object detection, and semantic segmentation tasks. Code will be available at
https://github.com/hunto/DiffKD.
</p></li>
</ul>

<h3>Title: Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models. (arXiv:2305.15779v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15779">http://arxiv.org/abs/2305.15779</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15779] Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models](http://arxiv.org/abs/2305.15779) #diffusion</code></li>
<li>Summary: <p>Text-to-image diffusion models can generate diverse, high-fidelity images
based on user-provided text prompts. Recent research has extended these models
to support text-guided image editing. While text guidance is an intuitive
editing interface for users, it often fails to ensure the precise concept
conveyed by users. To address this issue, we propose Custom-Edit, in which we
(i) customize a diffusion model with a few reference images and then (ii)
perform text-guided editing. Our key discovery is that customizing only
language-relevant parameters with augmented prompts improves reference
similarity significantly while maintaining source similarity. Moreover, we
provide our recipe for each customization and editing process. We compare
popular customization methods and validate our findings on two editing methods
using various datasets.
</p></li>
</ul>

<h3>Title: Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3). (arXiv:2305.15873v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15873">http://arxiv.org/abs/2305.15873</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15873] Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3)](http://arxiv.org/abs/2305.15873) #diffusion</code></li>
<li>Summary: <p>Addressing accuracy limitations and pose ambiguity in 6D object pose
estimation from single RGB images presents a significant challenge,
particularly due to object symmetries or occlusions. In response, we introduce
a novel score-based diffusion method applied to the $SE(3)$ group, marking the
first application of diffusion models to $SE(3)$ within the image domain,
specifically tailored for pose estimation tasks. Extensive evaluations
demonstrate the method's efficacy in handling pose ambiguity, mitigating
perspective-induced ambiguity, and showcasing the robustness of our surrogate
Stein score formulation on $SE(3)$. This formulation not only improves the
convergence of Langevin dynamics but also enhances computational efficiency.
Thus, we pioneer a promising strategy for 6D object pose estimation.
</p></li>
</ul>

<h3>Title: Anomaly Detection with Conditioned Denoising Diffusion Models. (arXiv:2305.15956v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15956">http://arxiv.org/abs/2305.15956</a></li>
<li>Code URL: <a href="https://github.com/arimousa/DDAD">https://github.com/arimousa/DDAD</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15956] Anomaly Detection with Conditioned Denoising Diffusion Models](http://arxiv.org/abs/2305.15956) #diffusion</code></li>
<li>Summary: <p>Reconstruction-based methods have struggled to achieve competitive
performance on anomaly detection. In this paper, we introduce Denoising
Diffusion Anomaly Detection (DDAD). We propose a novel denoising process for
image reconstruction conditioned on a target image. This results in a coherent
restoration that closely resembles the target image. Subsequently, our anomaly
detection framework leverages this conditioning where the target image is set
as the input image to guide the denoising process, leading to defectless
reconstruction while maintaining nominal patterns. We localise anomalies via a
pixel-wise and feature-wise comparison of the input and reconstructed image.
Finally, to enhance the effectiveness of feature comparison, we introduce a
domain adaptation method that utilises generated examples from our conditioned
denoising process to fine-tune the feature extractor. The veracity of the
approach is demonstrated on various datasets including MVTec and VisA
benchmarks, achieving state-of-the-art results of 99.5% and 99.3% image-level
AUROC respectively.
</p></li>
</ul>

<h3>Title: DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification. (arXiv:2305.15957v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15957">http://arxiv.org/abs/2305.15957</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15957] DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification](http://arxiv.org/abs/2305.15957) #diffusion</code></li>
<li>Summary: <p>Large pre-trained models have had a significant impact on computer vision by
enabling multi-modal learning, where the CLIP model has achieved impressive
results in image classification, object detection, and semantic segmentation.
However, the model's performance on 3D point cloud processing tasks is limited
due to the domain gap between depth maps from 3D projection and training images
of CLIP. This paper proposes DiffCLIP, a new pre-training framework that
incorporates stable diffusion with ControlNet to minimize the domain gap in the
visual branch. Additionally, a style-prompt generation module is introduced for
few-shot tasks in the textual branch. Extensive experiments on the ModelNet10,
ModelNet40, and ScanObjectNN datasets show that DiffCLIP has strong abilities
for 3D understanding. By using stable diffusion and style-prompt generation,
DiffCLIP achieves an accuracy of 43.2\% for zero-shot classification on OBJ_BG
of ScanObjectNN, which is state-of-the-art performance, and an accuracy of
80.6\% for zero-shot classification on ModelNet10, which is comparable to
state-of-the-art performance.
</p></li>
</ul>

<h3>Title: Non-Parametric Learning of Stochastic Differential Equations with Fast Rates of Convergence. (arXiv:2305.15557v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15557">http://arxiv.org/abs/2305.15557</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15557] Non-Parametric Learning of Stochastic Differential Equations with Fast Rates of Convergence](http://arxiv.org/abs/2305.15557) #diffusion</code></li>
<li>Summary: <p>We propose a novel non-parametric learning paradigm for the identification of
drift and diffusion coefficients of non-linear stochastic differential
equations, which relies upon discrete-time observations of the state. The key
idea essentially consists of fitting a RKHS-based approximation of the
corresponding Fokker-Planck equation to such observations, yielding theoretical
estimates of learning rates which, unlike previous works, become increasingly
tighter when the regularity of the unknown drift and diffusion coefficients
becomes higher. Our method being kernel-based, offline pre-processing may in
principle be profitably leveraged to enable efficient numerical implementation.
</p></li>
</ul>

<h3>Title: Manifold Diffusion Fields. (arXiv:2305.15586v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15586">http://arxiv.org/abs/2305.15586</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15586] Manifold Diffusion Fields](http://arxiv.org/abs/2305.15586) #diffusion</code></li>
<li>Summary: <p>We present Manifold Diffusion Fields (MDF), an approach to learn generative
models of continuous functions defined over Riemannian manifolds. Leveraging
insights from spectral geometry analysis, we define an intrinsic coordinate
system on the manifold via the eigen-functions of the Laplace-Beltrami
Operator. MDF represents functions using an explicit parametrization formed by
a set of multiple input-output pairs. Our approach allows to sample continuous
functions on manifolds and is invariant with respect to rigid and isometric
transformations of the manifold. Empirical results on several datasets and
manifolds show that MDF can capture distributions of such functions with better
diversity and fidelity than previous approaches.
</p></li>
</ul>

<h3>Title: Reversible and irreversible bracket-based dynamics for deep graph neural networks. (arXiv:2305.15616v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15616">http://arxiv.org/abs/2305.15616</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15616] Reversible and irreversible bracket-based dynamics for deep graph neural networks](http://arxiv.org/abs/2305.15616) #diffusion</code></li>
<li>Summary: <p>Recent works have shown that physics-inspired architectures allow the
training of deep graph neural networks (GNNs) without oversmoothing. The role
of these physics is unclear, however, with successful examples of both
reversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena
producing comparable results despite diametrically opposed mechanisms, and
further complications arising due to empirical departures from mathematical
theory. This work presents a series of novel GNN architectures based upon
structure-preserving bracket-based dynamical systems, which are provably
guaranteed to either conserve energy or generate positive dissipation with
increasing depth. It is shown that the theoretically principled framework
employed here allows for inherently explainable constructions, which
contextualize departures from theory in current architectures and better
elucidate the roles of reversibility and irreversibility in network
performance.
</p></li>
</ul>

<h3>Title: Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models. (arXiv:2305.15618v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15618">http://arxiv.org/abs/2305.15618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15618] Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models](http://arxiv.org/abs/2305.15618) #diffusion</code></li>
<li>Summary: <p>We introduce a two-stage probabilistic framework for statistical downscaling
between unpaired data. Statistical downscaling seeks a probabilistic map to
transform low-resolution data from a (possibly biased) coarse-grained numerical
scheme to high-resolution data that is consistent with a high-fidelity scheme.
Our framework tackles the problem by tandeming two transformations: a debiasing
step that is performed by an optimal transport map, and an upsampling step that
is achieved by a probabilistic diffusion model with \textit{a posteriori}
conditional sampling. This approach characterizes a conditional distribution
without the need for paired data, and faithfully recovers relevant physical
statistics from biased samples. We demonstrate the utility of the proposed
approach on one- and two-dimensional fluid flow problems, which are
representative of the core difficulties present in numerical simulations of
weather and climate. Our method produces realistic high-resolution outputs from
low-resolution inputs, by upsampling resolutions of $8\times$ and $16\times$.
Moreover, our procedure correctly matches the statistics of physical
quantities, even when the low-frequency content of the inputs and outputs do
not match, a crucial but difficult-to-satisfy assumption needed by current
state-of-the-art alternatives.
</p></li>
</ul>

<h3>Title: Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and Training with Non-Linear Diffusion. (arXiv:2305.15639v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15639">http://arxiv.org/abs/2305.15639</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15639] Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and Training with Non-Linear Diffusion](http://arxiv.org/abs/2305.15639) #diffusion</code></li>
<li>Summary: <p>This work presents a comprehensive theoretical analysis of graph p-Laplacian
based framelet network (pL-UFG) to establish a solid understanding of its
properties. We begin by conducting a convergence analysis of the p-Laplacian
based implicit layer integrated after the framelet convolution, providing
insights into the asymptotic behavior of pL-UFG. By exploring the generalized
Dirichlet energy of pL-UFG, we demonstrate that the Dirichlet energy remains
non-zero, ensuring the avoidance of over-smoothing issues in pL-UFG as it
approaches convergence. Furthermore, we elucidate the dynamic energy
perspective through which the implicit layer in pL-UFG synergizes with graph
framelets, enhancing the model's adaptability to both homophilic and
heterophilic data. Remarkably, we establish that the implicit layer can be
interpreted as a generalized non-linear diffusion process, enabling training
using diverse schemes. These multifaceted analyses lead to unified conclusions
that provide novel insights for understanding and implementing pL-UFG,
contributing to advancements in the field of graph-based deep learning.
</p></li>
</ul>

<h3>Title: On Architectural Compression of Text-to-Image Diffusion Models. (arXiv:2305.15798v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15798">http://arxiv.org/abs/2305.15798</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15798] On Architectural Compression of Text-to-Image Diffusion Models](http://arxiv.org/abs/2305.15798) #diffusion</code></li>
<li>Summary: <p>Exceptional text-to-image (T2I) generation results of Stable Diffusion models
(SDMs) come with substantial computational demands. To resolve this issue,
recent research on efficient SDMs has prioritized reducing the number of
sampling steps and utilizing network quantization. Orthogonal to these
directions, this study highlights the power of classical architectural
compression for general-purpose T2I synthesis by introducing block-removed
knowledge-distilled SDMs (BK-SDMs). We eliminate several residual and attention
blocks from the U-Net of SDMs, obtaining over a 30% reduction in the number of
parameters, MACs per sampling step, and latency. We conduct distillation-based
pretraining with only 0.22M LAION pairs (fewer than 0.1% of the full training
pairs) on a single A100 GPU. Despite being trained with limited resources, our
compact models can imitate the original SDM by benefiting from transferred
knowledge and achieve competitive results against larger multi-billion
parameter models on the zero-shot MS-COCO benchmark. Moreover, we demonstrate
the applicability of our lightweight pretrained models in personalized
generation with DreamBooth finetuning.
</p></li>
</ul>

<h3>Title: PDE+: Enhancing Generalization via PDE with Adaptive Distributional Diffusion. (arXiv:2305.15835v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15835">http://arxiv.org/abs/2305.15835</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15835] PDE+: Enhancing Generalization via PDE with Adaptive Distributional Diffusion](http://arxiv.org/abs/2305.15835) #diffusion</code></li>
<li>Summary: <p>The generalization of neural networks is a central challenge in machine
learning, especially concerning the performance under distributions that differ
from training ones. Current methods, mainly based on the data-driven paradigm
such as data augmentation, adversarial training, and noise injection, may
encounter limited generalization due to model non-smoothness. In this paper, we
propose to investigate generalization from a Partial Differential Equation
(PDE) perspective, aiming to enhance it directly through the underlying
function of neural networks, rather than focusing on adjusting input data.
Specifically, we first establish the connection between neural network
generalization and the smoothness of the solution to a specific PDE, namely
``transport equation''. Building upon this, we propose a general framework that
introduces adaptive distributional diffusion into transport equation to enhance
the smoothness of its solution, thereby improving generalization. In the
context of neural networks, we put this theoretical framework into practice as
PDE+ (\textbf{PDE} with \textbf{A}daptive \textbf{D}istributional
\textbf{D}iffusion) which diffuses each sample into a distribution covering
semantically similar inputs. This enables better coverage of potentially
unobserved distributions in training, thus improving generalization beyond
merely data-driven methods. The effectiveness of PDE+ is validated in extensive
settings, including clean samples and various corruptions, demonstrating its
superior performance compared to SOTA methods.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: T2TD: Text-3D Generation Model based on Prior Knowledge Guidance. (arXiv:2305.15753v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15753">http://arxiv.org/abs/2305.15753</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15753] T2TD: Text-3D Generation Model based on Prior Knowledge Guidance](http://arxiv.org/abs/2305.15753) #transformer</code></li>
<li>Summary: <p>In recent years, 3D models have been utilized in many applications, such as
auto-driver, 3D reconstruction, VR, and AR. However, the scarcity of 3D model
data does not meet its practical demands. Thus, generating high-quality 3D
models efficiently from textual descriptions is a promising but challenging way
to solve this problem. In this paper, inspired by the ability of human beings
to complement visual information details from ambiguous descriptions based on
their own experience, we propose a novel text-3D generation model (T2TD), which
introduces the related shapes or textual information as the prior knowledge to
improve the performance of the 3D generation model. In this process, we first
introduce the text-3D knowledge graph to save the relationship between 3D
models and textual semantic information, which can provide the related shapes
to guide the target 3D model generation. Second, we integrate an effective
causal inference model to select useful feature information from these related
shapes, which removes the unrelated shape information and only maintains
feature information that is strongly relevant to the textual description.
Meanwhile, to effectively integrate multi-modal prior knowledge into textual
information, we adopt a novel multi-layer transformer structure to
progressively fuse related shape and textual information, which can effectively
compensate for the lack of structural information in the text and enhance the
final performance of the 3D generation model. The final experimental results
demonstrate that our approach significantly improves 3D model generation
quality and outperforms the SOTA methods on the text2shape datasets.
</p></li>
</ul>

<h3>Title: Multi-scale Efficient Graph-Transformer for Whole Slide Image Classification. (arXiv:2305.15773v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15773">http://arxiv.org/abs/2305.15773</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15773] Multi-scale Efficient Graph-Transformer for Whole Slide Image Classification](http://arxiv.org/abs/2305.15773) #transformer</code></li>
<li>Summary: <p>The multi-scale information among the whole slide images (WSIs) is essential
for cancer diagnosis. Although the existing multi-scale vision Transformer has
shown its effectiveness for learning multi-scale image representation, it still
cannot work well on the gigapixel WSIs due to their extremely large image
sizes. To this end, we propose a novel Multi-scale Efficient Graph-Transformer
(MEGT) framework for WSI classification. The key idea of MEGT is to adopt two
independent Efficient Graph-based Transformer (EGT) branches to process the
low-resolution and high-resolution patch embeddings (i.e., tokens in a
Transformer) of WSIs, respectively, and then fuse these tokens via a
multi-scale feature fusion module (MFFM). Specifically, we design an EGT to
efficiently learn the local-global information of patch tokens, which
integrates the graph representation into Transformer to capture spatial-related
information of WSIs. Meanwhile, we propose a novel MFFM to alleviate the
semantic gap among different resolution patches during feature fusion, which
creates a non-patch token for each branch as an agent to exchange information
with another branch by cross-attention. In addition, to expedite network
training, a novel token pruning module is developed in EGT to reduce the
redundant tokens. Extensive experiments on TCGA-RCC and CAMELYON16 datasets
demonstrate the effectiveness of the proposed MEGT.
</p></li>
</ul>

<h3>Title: Text-to-Motion Retrieval: Towards Joint Understanding of Human Motion Data and Natural Language. (arXiv:2305.15842v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15842">http://arxiv.org/abs/2305.15842</a></li>
<li>Code URL: <a href="https://github.com/mesnico/text-to-motion-retrieval">https://github.com/mesnico/text-to-motion-retrieval</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15842] Text-to-Motion Retrieval: Towards Joint Understanding of Human Motion Data and Natural Language](http://arxiv.org/abs/2305.15842) #transformer</code></li>
<li>Summary: <p>Due to recent advances in pose-estimation methods, human motion can be
extracted from a common video in the form of 3D skeleton sequences. Despite
wonderful application opportunities, effective and efficient content-based
access to large volumes of such spatio-temporal skeleton data still remains a
challenging problem. In this paper, we propose a novel content-based
text-to-motion retrieval task, which aims at retrieving relevant motions based
on a specified natural-language textual description. To define baselines for
this uncharted task, we employ the BERT and CLIP language representations to
encode the text modality and successful spatio-temporal models to encode the
motion modality. We additionally introduce our transformer-based approach,
called Motion Transformer (MoT), which employs divided space-time attention to
effectively aggregate the different skeleton joints in space and time. Inspired
by the recent progress in text-to-image/video matching, we experiment with two
widely-adopted metric-learning loss functions. Finally, we set up a common
evaluation protocol by defining qualitative metrics for assessing the quality
of the retrieved motions, targeting the two recently-introduced KIT
Motion-Language and HumanML3D datasets. The code for reproducing our results is
available at https://github.com/mesnico/text-to-motion-retrieval.
</p></li>
</ul>

<h3>Title: MixFormerV2: Efficient Fully Transformer Tracking. (arXiv:2305.15896v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15896">http://arxiv.org/abs/2305.15896</a></li>
<li>Code URL: <a href="https://github.com/mcg-nju/mixformerv2">https://github.com/mcg-nju/mixformerv2</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15896] MixFormerV2: Efficient Fully Transformer Tracking](http://arxiv.org/abs/2305.15896) #transformer</code></li>
<li>Summary: <p>Transformer-based trackers have achieved strong accuracy on the standard
benchmarks. However, their efficiency remains an obstacle to practical
deployment on both GPU and CPU platforms. In this paper, to overcome this
issue, we propose a fully transformer tracking framework, coined as
\emph{MixFormerV2}, without any dense convolutional operation and complex score
prediction module. Our key design is to introduce four special prediction
tokens and concatenate them with the tokens from target template and search
areas. Then, we apply the unified transformer backbone on these mixed token
sequence. These prediction tokens are able to capture the complex correlation
between target template and search area via mixed attentions. Based on them, we
can easily predict the tracking box and estimate its confidence score through
simple MLP heads. To further improve the efficiency of MixFormerV2, we present
a new distillation-based model reduction paradigm, including dense-to-sparse
distillation and deep-to-shallow distillation. The former one aims to transfer
knowledge from the dense-head based MixViT to our fully transformer tracker,
while the latter one is used to prune some layers of the backbone. We
instantiate two types of MixForemrV2, where the MixFormerV2-B achieves an AUC
of 70.6\% on LaSOT and an AUC of 57.4\% on TNL2k with a high GPU speed of 165
FPS, and the MixFormerV2-S surpasses FEAR-L by 2.7\% AUC on LaSOT with a
real-time CPU speed.
</p></li>
</ul>

<h3>Title: Comparative Study of Pre-Trained BERT Models for Code-Mixed Hindi-English Data. (arXiv:2305.15722v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15722">http://arxiv.org/abs/2305.15722</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15722] Comparative Study of Pre-Trained BERT Models for Code-Mixed Hindi-English Data](http://arxiv.org/abs/2305.15722) #transformer</code></li>
<li>Summary: <p>The term "Code Mixed" refers to the use of more than one language in the same
text. This phenomenon is predominantly observed on social media platforms, with
an increasing amount of adaptation as time goes on. It is critical to detect
foreign elements in a language and process them correctly, as a considerable
number of individuals are using code-mixed languages that could not be
comprehended by understanding one of those languages. In this work, we focus on
low-resource Hindi-English code-mixed language and enhancing the performance of
different code-mixed natural language processing tasks such as sentiment
analysis, emotion recognition, and hate speech identification. We perform a
comparative analysis of different Transformer-based language Models pre-trained
using unsupervised approaches. We have included the code-mixed models like
HingBERT, HingRoBERTa, HingRoBERTa-Mixed, mBERT, and non-code-mixed models like
AlBERT, BERT, and RoBERTa for comparative analysis of code-mixed Hindi-English
downstream tasks. We report state-of-the-art results on respective datasets
using HingBERT-based models which are specifically pre-trained on real
code-mixed text. Our HingBERT-based models provide significant improvements
thus highlighting the poor performance of vanilla BERT models on code-mixed
text.
</p></li>
</ul>

<h3>Title: UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation. (arXiv:2305.15756v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15756">http://arxiv.org/abs/2305.15756</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15756] UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation](http://arxiv.org/abs/2305.15756) #transformer</code></li>
<li>Summary: <p>Prior study has shown that pretrained language models (PLM) can boost the
performance of text-based recommendation. In contrast to previous works that
either use PLM to encode user history as a whole input text, or impose an
additional aggregation network to fuse multi-turn history representations, we
propose a unified local- and global-attention Transformer encoder to better
model two-level contexts of user history. Moreover, conditioned on user history
encoded by Transformer encoders, our framework leverages Transformer decoders
to estimate the language perplexity of candidate text items, which can serve as
a straightforward yet significant contrastive signal for user-item text
matching. Based on this, our framework, UniTRec, unifies the contrastive
objectives of discriminative matching scores and candidate text perplexity to
jointly enhance text-based recommendation. Extensive evaluation shows that
UniTRec delivers SOTA performance on three text-based recommendation tasks.
Code is available at https://github.com/Veason-silverbullet/UniTRec.
</p></li>
</ul>

<h3>Title: Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. (arXiv:2305.15805v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15805">http://arxiv.org/abs/2305.15805</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15805] Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers](http://arxiv.org/abs/2305.15805) #transformer</code></li>
<li>Summary: <p>Autoregressive Transformers adopted in Large Language Models (LLMs) are hard
to scale to long sequences. Despite several works trying to reduce their
computational cost, most of LLMs still adopt attention layers between all pairs
of tokens in the sequence, thus incurring a quadratic cost. In this study, we
present a novel approach that dynamically prunes contextual information while
preserving the model's expressiveness, resulting in reduced memory and
computational requirements during inference. Our method employs a learnable
mechanism that determines which uninformative tokens can be dropped from the
context at any point across the generation process. By doing so, our approach
not only addresses performance concerns but also enhances interpretability,
providing valuable insight into the model's decision-making process. Our
technique can be applied to existing pre-trained models through a
straightforward fine-tuning process, and the pruning strength can be specified
by a sparsity parameter. Notably, our empirical findings demonstrate that we
can effectively prune up to 80\% of the context without significant performance
degradation on downstream tasks, offering a valuable tool for mitigating
inference costs. Our reference implementation achieves up to $2\times$ increase
in inference throughput and even greater memory savings.
</p></li>
</ul>

<h3>Title: Union Subgraph Neural Networks. (arXiv:2305.15747v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15747">http://arxiv.org/abs/2305.15747</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15747] Union Subgraph Neural Networks](http://arxiv.org/abs/2305.15747) #transformer</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) are widely used for graph representation
learning in many application domains. The expressiveness of vanilla GNNs is
upper-bounded by 1-dimensional Weisfeiler-Leman (1-WL) test as they operate on
rooted subtrees through iterative message passing. In this paper, we empower
GNNs by injecting neighbor-connectivity information extracted from a new type
of substructure. We first investigate different kinds of connectivities
existing in a local neighborhood and identify a substructure called union
subgraph, which is able to capture the complete picture of the 1-hop
neighborhood of an edge. We then design a shortest-path-based substructure
descriptor that possesses three nice properties and can effectively encode the
high-order connectivities in union subgraphs. By infusing the encoded neighbor
connectivities, we propose a novel model, namely Union Subgraph Neural Network
(UnionSNN), which is proven to be strictly more powerful than 1-WL in
distinguishing non-isomorphic graphs. Additionally, the local encoding from
union subgraphs can also be injected into arbitrary message-passing neural
networks (MPNNs) and Transformer-based models as a plugin. Extensive
experiments on 17 benchmarks of both graph-level and node-level tasks
demonstrate that UnionSNN outperforms state-of-the-art baseline models, with
competitive computational efficiency. The injection of our local encoding to
existing models is able to boost the performance by up to 11.09%.
</p></li>
</ul>

<h3>Title: End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes. (arXiv:2305.15930v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15930">http://arxiv.org/abs/2305.15930</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15930] End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes](http://arxiv.org/abs/2305.15930) #transformer</code></li>
<li>Summary: <p>Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency of
Bayesian optimisation by leveraging data from related tasks. While previous
methods successfully meta-learn either a surrogate model or an acquisition
function independently, joint training of both components remains an open
challenge. This paper proposes the first end-to-end differentiable meta-BO
framework that generalises neural processes to learn acquisition functions via
transformer architectures. We enable this end-to-end framework with
reinforcement learning (RL) to tackle the lack of labelled acquisition data.
Early on, we notice that training transformer-based neural processes from
scratch with RL is challenging due to insufficient supervision, especially when
rewards are sparse. We formalise this claim with a combinatorial analysis
showing that the widely used notion of regret as a reward signal exhibits a
logarithmic sparsity pattern in trajectory lengths. To tackle this problem, we
augment the RL objective with an auxiliary task that guides part of the
architecture to learn a valid probabilistic model as an inductive bias. We
demonstrate that our method achieves state-of-the-art regret results against
various baselines in experiments on standard hyperparameter optimisation tasks
and also outperforms others in the real-world problems of mixed-integer
programming tuning, antibody design, and logic synthesis for electronic design
automation.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Score-Based Multimodal Autoencoders. (arXiv:2305.15708v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15708">http://arxiv.org/abs/2305.15708</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15708] Score-Based Multimodal Autoencoders](http://arxiv.org/abs/2305.15708) #generative</code></li>
<li>Summary: <p>Multimodal Variational Autoencoders (VAEs) represent a promising group of
generative models that facilitate the construction of a tractable posterior
within the latent space, given multiple modalities. Daunhawer et al. (2022)
demonstrate that as the number of modalities increases, the generative quality
of each modality declines. In this study, we explore an alternative approach to
enhance the generative performance of multimodal VAEs by jointly modeling the
latent space of unimodal VAEs using score-based models (SBMs). The role of the
SBM is to enforce multimodal coherence by learning the correlation among the
latent variables. Consequently, our model combines the superior generative
quality of unimodal VAEs with coherent integration across different modalities.
</p></li>
</ul>

<h3>Title: Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback. (arXiv:2305.15808v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15808">http://arxiv.org/abs/2305.15808</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15808] Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback](http://arxiv.org/abs/2305.15808) #generative</code></li>
<li>Summary: <p>Generating and editing a 3D scene guided by natural language poses a
challenge, primarily due to the complexity of specifying the positional
relations and volumetric changes within the 3D space. Recent advancements in
Large Language Models (LLMs) have demonstrated impressive reasoning,
conversational, and zero-shot generation abilities across various domains.
Surprisingly, these models also show great potential in realizing and
interpreting the 3D space. In light of this, we propose a novel language-guided
interactive 3D generation system, dubbed LI3D, that integrates LLMs as a 3D
layout interpreter into the off-the-shelf layout-to-3D generative models,
allowing users to flexibly and interactively generate visual content.
Specifically, we design a versatile layout structure base on the bounding boxes
and semantics to prompt the LLMs to model the spatial generation and reasoning
from language. Our system also incorporates LLaVA, a large language and vision
assistant, to provide generative feedback from the visual aspect for improving
the visual quality of generated content. We validate the effectiveness of LI3D,
primarily in 3D generation and editing through multi-round interactions, which
can be flexibly extended to 2D generation and editing. Various experiments
demonstrate the potential benefits of incorporating LLMs in generative AI for
applications, e.g., metaverse. Moreover, we benchmark the layout reasoning
performance of LLMs with neural visual artist tasks, revealing their emergent
ability in the spatial layout domain.
</p></li>
</ul>

<h3>Title: Generative Adversarial Reduced Order Modelling. (arXiv:2305.15881v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15881">http://arxiv.org/abs/2305.15881</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15881] Generative Adversarial Reduced Order Modelling](http://arxiv.org/abs/2305.15881) #generative</code></li>
<li>Summary: <p>In this work, we present GAROM, a new approach for reduced order modelling
(ROM) based on generative adversarial networks (GANs). GANs have the potential
to learn data distribution and generate more realistic data. While widely
applied in many areas of deep learning, little research is done on their
application for ROM, i.e. approximating a high-fidelity model with a simpler
one. In this work, we combine the GAN and ROM framework, by introducing a
data-driven generative adversarial model able to learn solutions to parametric
differential equations. The latter is achieved by modelling the discriminator
network as an autoencoder, extracting relevant features of the input, and
applying a conditioning mechanism to the generator and discriminator networks
specifying the differential equation parameters. We show how to apply our
methodology for inference, provide experimental evidence of the model
generalisation, and perform a convergence study of the method.
</p></li>
</ul>

<h3>Title: Empirical Optimal Transport between Conditional Distributions. (arXiv:2305.15901v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15901">http://arxiv.org/abs/2305.15901</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15901] Empirical Optimal Transport between Conditional Distributions](http://arxiv.org/abs/2305.15901) #generative</code></li>
<li>Summary: <p>Given samples from two joint distributions, we consider the problem of
Optimal Transportation (OT) between the corresponding distributions conditioned
on a common variable. The objective of this work is to estimate the associated
transport cost (Wasserstein distance) as well as the transport plan between the
conditionals as a function of the conditioned value. Since matching conditional
distributions is at the core of supervised training of discriminative models
and (implicit) conditional-generative models, OT between conditionals has the
potential to be employed in diverse machine learning applications. However,
since the conditionals involved in OT are implicitly specified via the joint
samples, it is challenging to formulate this problem, especially when (i) the
variable conditioned on is continuous and (ii) the marginal of this variable in
the two distributions is different. We overcome these challenges by employing a
specific kernel MMD (Maximum Mean Discrepancy) based regularizer that ensures
the marginals of our conditional transport plan are close to the conditionals
specified via the given joint samples. Under mild conditions, we prove that our
estimator for this regularized transport cost is statistically consistent and
derive finite-sample bounds on the estimation error. Application-specific
details for parameterizing our conditional transport plan are also presented.
Furthermore, we empirically evaluate our methodology on benchmark datasets in
applications like classification, prompt learning for few-shot classification,
and conditional-generation in the context of predicting cell responses to
cancer treatment.
</p></li>
</ul>

<h3>Title: How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits. (arXiv:2305.15944v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15944">http://arxiv.org/abs/2305.15944</a></li>
<li>Code URL: <a href="https://github.com/anongekc/gekcs">https://github.com/anongekc/gekcs</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15944] How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits](http://arxiv.org/abs/2305.15944) #generative</code></li>
<li>Summary: <p>Some of the most successful knowledge graph embedding (KGE) models for link
prediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-based
models. Under this perspective they are not amenable for exact
maximum-likelihood estimation (MLE), sampling and struggle to integrate logical
constraints. This work re-interprets the score functions of these KGEs as
circuits -- constrained computational graphs allowing efficient
marginalisation. Then, we design two recipes to obtain efficient generative
circuit models by either restricting their activations to be non-negative or
squaring their outputs. Our interpretation comes with little or no loss of
performance for link prediction, while the circuits framework unlocks exact
learning by MLE, efficient sampling of new triples, and guarantee that logical
constraints are satisfied by design. Furthermore, our models scale more
gracefully than the original KGEs on graphs with millions of entities.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: PromptNER: Prompting For Named Entity Recognition. (arXiv:2305.15444v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15444">http://arxiv.org/abs/2305.15444</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15444] PromptNER: Prompting For Named Entity Recognition](http://arxiv.org/abs/2305.15444) #large language model</code></li>
<li>Summary: <p>In a surprising turn, Large Language Models (LLMs) together with a growing
arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches
providing few-shot solutions to myriad classic NLP problems. However, despite
promising early results, these LLM-based few-shot methods remain far from the
state of the art in Named Entity Recognition (NER), where prevailing methods
include learning representations via end-to-end structural understanding and
fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER,
a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to
any new NER task PromptNER requires a set of entity definitions in addition to
the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to
produce a list of potential entities along with corresponding explanations
justifying their compatibility with the provided entity type definitions.
Remarkably, PromptNER achieves state-of-the-art performance on few-shot NER,
achieving an 11% (absolute) improvement in F1 score on the ConLL dataset, and a
10% (absolute) improvement on the FewNERD dataset. PromptNER also moves the
state of the art on Cross Domain NER, outperforming all prior methods
(including those not limited to the few-shot setting), setting a new mark on
all 5 CrossNER target domains, with an average F1 gain of 9%, despite using
less than 2% of the available data.
</p></li>
</ul>

<h3>Title: Large Language Models for User Interest Journeys. (arXiv:2305.15498v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15498">http://arxiv.org/abs/2305.15498</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15498] Large Language Models for User Interest Journeys](http://arxiv.org/abs/2305.15498) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have shown impressive capabilities in natural
language understanding and generation. Their potential for deeper user
understanding and improved personalized user experience on recommendation
platforms is, however, largely untapped. This paper aims to address this gap.
Recommender systems today capture users' interests through encoding their
historical activities on the platforms. The generated user representations are
hard to examine or interpret. On the other hand, if we were to ask people about
interests they pursue in their life, they might talk about their hobbies, like
I just started learning the ukulele, or their relaxation routines, e.g., I like
to watch Saturday Night Live, or I want to plant a vertical garden. We argue,
and demonstrate through extensive experiments, that LLMs as foundation models
can reason through user activities, and describe their interests in nuanced and
interesting ways, similar to how a human would.
</p></li>
</ul>

<p>We define interest journeys as the persistent and overarching user interests,
in other words, the non-transient ones. These are the interests that we believe
will benefit most from the nuanced and personalized descriptions. We introduce
a framework in which we first perform personalized extraction of interest
journeys, and then summarize the extracted journeys via LLMs, using techniques
like few-shot prompting, prompt-tuning and fine-tuning. Together, our results
in prompting LLMs to name extracted user journeys in a large-scale industrial
platform demonstrate great potential of these models in providing deeper, more
interpretable, and controllable user understanding. We believe LLM powered user
understanding can be a stepping stone to entirely new user experiences on
recommendation platforms that are journey-aware, assistive, and enabling
frictionless conversation down the line.
</p>

<h3>Title: The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python. (arXiv:2305.15507v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15507">http://arxiv.org/abs/2305.15507</a></li>
<li>Code URL: <a href="https://github.com/avmb/inverse_scaling_prize_code_identifier_swap">https://github.com/avmb/inverse_scaling_prize_code_identifier_swap</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15507] The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python](http://arxiv.org/abs/2305.15507) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs) have successfully been applied to code
generation tasks, raising the question of how well these models understand
programming. Typical programming languages have invariances and equivariances
in their semantics that human programmers intuitively understand and exploit,
such as the (near) invariance to the renaming of identifiers. We show that LLMs
not only fail to properly generate correct Python code when default function
names are swapped, but some of them even become more confident in their
incorrect predictions as the model size increases, an instance of the recently
discovered phenomenon of Inverse Scaling, which runs contrary to the commonly
observed trend of increasing prediction quality with increasing model size. Our
findings indicate that, despite their astonishing typical-case performance,
LLMs still lack a deep, abstract understanding of the content they manipulate,
making them unsuitable for tasks that statistically deviate from their training
data, and that mere scaling is not enough to achieve such capability.
</p></li>
</ul>

<h3>Title: Large Language Models are Few-Shot Health Learners. (arXiv:2305.15525v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15525">http://arxiv.org/abs/2305.15525</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15525] Large Language Models are Few-Shot Health Learners](http://arxiv.org/abs/2305.15525) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) can capture rich representations of concepts
that are useful for real-world tasks. However, language alone is limited. While
existing LLMs excel at text-based inferences, health applications require that
models be grounded in numerical data (e.g., vital signs, laboratory values in
clinical domains; steps, movement in the wellness domain) that is not easily or
readily expressed as text in existing training corpus. We demonstrate that with
only few-shot tuning, a large language model is capable of grounding various
physiological and behavioral time-series data and making meaningful inferences
on numerous health tasks for both clinical and wellness contexts. Using data
from wearable and medical sensor recordings, we evaluate these capabilities on
the tasks of cardiac signal analysis, physical activity recognition, metabolic
calculation (e.g., calories burned), and estimation of stress reports and
mental health screeners.
</p></li>
</ul>

<h3>Title: Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation. (arXiv:2305.15541v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15541">http://arxiv.org/abs/2305.15541</a></li>
<li>Code URL: <a href="https://github.com/gblackout/logicllama">https://github.com/gblackout/logicllama</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15541] Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation](http://arxiv.org/abs/2305.15541) #large language model</code></li>
<li>Summary: <p>Translating natural language sentences to first-order logic (NL-FOL
translation) is a longstanding challenge in the NLP and formal logic
literature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for
NL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable of
directly translating natural language into FOL rules, which outperforms
GPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5,
and can achieve similar performance as GPT-4 with a fraction of the cost. This
correction ability was achieved by a novel supervised fine-tuning (SFT) +
reinforcement learning with human feedback (RLHF) framework, which initially
trains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought
reasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier
as the reward model.
</p></li>
</ul>

<p>To train LogicLLaMA, we present MALLS (large language $\textbf{M}$odel
gener$\textbf{A}$ted N$\textbf{L}$-FO$\textbf{L}$ pair$\textbf{S}$), a dataset
of 34K high-quality and diverse sentence-level NL-FOL pairs collected from
GPT-4. The dataset was created by implementing a pipeline that prompts GPT-4
for pairs, and dynamically adjusts the prompts to ensure the collection of
pairs with rich and diverse contexts at different levels of complexity, and
verifies the validity of the generated FOL rules. Codes, weights, and data are
available at $\href{https://github.com/gblackout/LogicLLaMA}{{\small
\text{https://github.com/gblackout/LogicLLaMA}}}$.
</p>

<h3>Title: Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models. (arXiv:2305.15594v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15594">http://arxiv.org/abs/2305.15594</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15594] Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models](http://arxiv.org/abs/2305.15594) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) are excellent in-context learners. However, the
sensitivity of data contained in prompts raises privacy concerns. Our work
first shows that these concerns are valid: we instantiate a simple but highly
effective membership inference attack against the data used to prompt LLMs. To
address this vulnerability, one could forego prompting and resort to
fine-tuning LLMs with known algorithms for private gradient descent. However,
this comes at the expense of the practicality and efficiency offered by
prompting. Therefore, we propose to privately learn to prompt. We first show
that soft prompts can be obtained privately through gradient descent on
downstream data. However, this is not the case for discrete prompts. Thus, we
orchestrate a noisy vote among an ensemble of LLMs presented with different
prompts, i.e., a flock of stochastic parrots. The vote privately transfers the
flock's knowledge into a single public prompt. We show that LLMs prompted with
our private algorithms closely match the non-private baselines. For example,
using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on the
sst2 dataset with ($\epsilon=0.147, \delta=10^{-6}$)-differential privacy vs.
95.2% for the non-private baseline. Through our experiments, we also show that
our prompt-based approach is easily deployed with existing commercial APIs.
</p></li>
</ul>

<h3>Title: RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting. (arXiv:2305.15685v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15685">http://arxiv.org/abs/2305.15685</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15685] RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting](http://arxiv.org/abs/2305.15685) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated impressive zero-shot
capabilities in long-form text generation tasks expressed through natural
language instructions. However, user expectations for long-form text rewriting
is high, and unintended rewrites (''hallucinations'') produced by the model can
negatively impact its overall performance. Existing evaluation benchmarks
primarily focus on limited rewriting styles and sentence-level rewriting rather
than long-form open-ended rewriting.We introduce OpenRewriteEval, a novel
benchmark that covers a wide variety of rewriting types expressed through
natural language instructions. It is specifically designed to facilitate the
evaluation of open-ended rewriting of long-form texts. In addition, we propose
a strong baseline model, RewriteLM, an instruction-tuned large language model
for long-form text rewriting. We develop new strategies that facilitate the
generation of diverse instructions and preference data with minimal human
intervention. We conduct empirical experiments and demonstrate that our model
outperforms the current state-of-the-art LLMs in text rewriting. Specifically,
it excels in preserving the essential content and meaning of the source text,
minimizing the generation of ''hallucinated'' content, while showcasing the
ability to generate rewrites with diverse wording and structures.
</p></li>
</ul>

<h3>Title: Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15852">http://arxiv.org/abs/2305.15852</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15852] Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation](http://arxiv.org/abs/2305.15852) #large language model</code></li>
<li>Summary: <p>Large language models (large LMs) are susceptible to producing text with
hallucinated content. Self-contradiction, where the LM generates two
contradictory sentences within the same context, is an important form of
hallucination. In this work, we present a comprehensive analysis on
self-contradiction for state-of-the-art, instruction-tuned LMs, including
evaluation, detection, and mitigation. To effectively trigger
self-contradictions, we design a framework that constrains LMs to generate
appropriate sentence pairs. Our evaluation on these sentence pairs reveals that
self-contradictions occur frequently across different LMs for both famous and
lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our
results indicate that ChatGPT and GPT-4 are able to accurately identify
self-contradictions, while Vicuna-13B struggles to do so. For example, with our
best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the
sentence pairs generated by itself. To automatically mitigate
self-contradictions, we develop an iterative algorithm that prompts the LMs to
remove the detected self-contradictions from the generated text. Our algorithm
successfully revises the text such that self-contradictions are significantly
reduced, while maintaining its fluency and informativeness. Importantly, our
entire pipeline of triggering, detecting, and mitigating self-contradictions is
applicable to black-box LMs and does not require any external grounded
knowledge.
</p></li>
</ul>

<h3>Title: Emergence of a phonological bias in ChatGPT. (arXiv:2305.15929v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15929">http://arxiv.org/abs/2305.15929</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15929] Emergence of a phonological bias in ChatGPT](http://arxiv.org/abs/2305.15929) #large language model</code></li>
<li>Summary: <p>Current large language models, such as OpenAI's ChatGPT, have captured the
public's attention because how remarkable they are in the use of language.
Here, I demonstrate that ChatGPT displays phonological biases that are a
hallmark of human language processing. More concretely, just like humans,
ChatGPT has a consonant bias. That is, the chatbot has a tendency to use
consonants over vowels to identify words. This is observed across languages
that differ in their relative distribution of consonants and vowels such as
English and Spanish. Despite the differences in how current artificial
intelligence language models are trained to process linguistic stimuli and how
human infants acquire language, such training seems to be enough for the
emergence of a phonological bias in ChatGPT
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: A Hybrid Semantic-Geometric Approach for Clutter-Resistant Floorplan Generation from Building Point Clouds. (arXiv:2305.15420v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15420">http://arxiv.org/abs/2305.15420</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15420] A Hybrid Semantic-Geometric Approach for Clutter-Resistant Floorplan Generation from Building Point Clouds](http://arxiv.org/abs/2305.15420) #segmentation</code></li>
<li>Summary: <p>Building Information Modeling (BIM) technology is a key component of modern
construction engineering and project management workflows. As-is BIM models
that represent the spatial reality of a project site can offer crucial
information to stakeholders for construction progress monitoring, error
checking, and building maintenance purposes. Geometric methods for
automatically converting raw scan data into BIM models (Scan-to-BIM) often fail
to make use of higher-level semantic information in the data. Whereas, semantic
segmentation methods only output labels at the point level without creating
object level models that is necessary for BIM. To address these issues, this
research proposes a hybrid semantic-geometric approach for clutter-resistant
floorplan generation from laser-scanned building point clouds. The input point
clouds are first pre-processed by normalizing the coordinate system and
removing outliers. Then, a semantic segmentation network based on PointNet++ is
used to label each point as ceiling, floor, wall, door, stair, and clutter. The
clutter points are removed whereas the wall, door, and stair points are used
for 2D floorplan generation. A region-growing segmentation algorithm paired
with geometric reasoning rules is applied to group the points together into
individual building elements. Finally, a 2-fold Random Sample Consensus
(RANSAC) algorithm is applied to parameterize the building elements into 2D
lines which are used to create the output floorplan. The proposed method is
evaluated using the metrics of precision, recall, Intersection-over-Union
(IOU), Betti error, and warping error.
</p></li>
</ul>

<h3>Title: Semantic Segmentation by Semantic Proportions. (arXiv:2305.15608v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15608">http://arxiv.org/abs/2305.15608</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15608] Semantic Segmentation by Semantic Proportions](http://arxiv.org/abs/2305.15608) #segmentation</code></li>
<li>Summary: <p>Semantic segmentation is a critical task in computer vision that aims to
identify and classify individual pixels in an image, with numerous applications
for example autonomous driving and medical image analysis. However, semantic
segmentation can be super challenging particularly due to the need for large
amounts of annotated data. Annotating images is a time-consuming and costly
process, often requiring expert knowledge and significant effort. In this
paper, we propose a novel approach for semantic segmentation by eliminating the
need of ground-truth segmentation maps. Instead, our approach requires only the
rough information of individual semantic class proportions, shortened as
semantic proportions. It greatly simplifies the data annotation process and
thus will significantly reduce the annotation time and cost, making it more
feasible for large-scale applications. Moreover, it opens up new possibilities
for semantic segmentation tasks where obtaining the full ground-truth
segmentation maps may not be feasible or practical. Extensive experimental
results demonstrate that our approach can achieve comparable and sometimes even
better performance against the benchmark method that relies on the ground-truth
segmentation maps. Utilising semantic proportions suggested in this work offers
a promising direction for future research in the field of semantic
segmentation.
</p></li>
</ul>

<h3>Title: All Points Matter: Entropy-Regularized Distribution Alignment for Weakly-supervised 3D Segmentation. (arXiv:2305.15832v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.15832">http://arxiv.org/abs/2305.15832</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.15832] All Points Matter: Entropy-Regularized Distribution Alignment for Weakly-supervised 3D Segmentation](http://arxiv.org/abs/2305.15832) #segmentation</code></li>
<li>Summary: <p>Pseudo-labels are widely employed in weakly supervised 3D segmentation tasks
where only sparse ground-truth labels are available for learning. Existing
methods often rely on empirical label selection strategies, such as confidence
thresholding, to generate beneficial pseudo-labels for model training. This
approach may, however, hinder the comprehensive exploitation of unlabeled data
points. We hypothesize that this selective usage arises from the noise in
pseudo-labels generated on unlabeled data. The noise in pseudo-labels may
result in significant discrepancies between pseudo-labels and model
predictions, thus confusing and affecting the model training greatly. To
address this issue, we propose a novel learning strategy to regularize the
generated pseudo-labels and effectively narrow the gaps between pseudo-labels
and model predictions. More specifically, our method introduces an Entropy
Regularization loss and a Distribution Alignment loss for weakly supervised
learning in 3D segmentation tasks, resulting in an ERDA learning strategy.
Interestingly, by using KL distance to formulate the distribution alignment
loss, it reduces to a deceptively simple cross-entropy-based loss which
optimizes both the pseudo-label generation network and the 3D segmentation
network simultaneously. Despite the simplicity, our method promisingly improves
the performance. We validate the effectiveness through extensive experiments on
various baselines and large-scale datasets. Results show that ERDA effectively
enables the effective usage of all unlabeled data points for learning and
achieves state-of-the-art performance under different settings. Remarkably, our
method can outperform fully-supervised baselines using only 1% of true
annotations. Code and model will be made publicly available.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
