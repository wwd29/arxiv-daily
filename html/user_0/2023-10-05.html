<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Secure and Effective Data Appraisal for Machine Learning. (arXiv:2310.02373v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02373">http://arxiv.org/abs/2310.02373</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02373]] Secure and Effective Data Appraisal for Machine Learning(http://arxiv.org/abs/2310.02373)</code></li>
<li>Summary: <p>Essential for an unfettered data market is the ability to discreetly select
and evaluate training data before finalizing a transaction between the data
owner and model owner. To safeguard the privacy of both data and model, this
process involves scrutinizing the target model through Multi-Party Computation
(MPC). While prior research has posited that the MPC-based evaluation of
Transformer models is excessively resource-intensive, this paper introduces an
innovative approach that renders data selection practical. The contributions of
this study encompass three pivotal elements: (1) a groundbreaking pipeline for
confidential data selection using MPC, (2) replicating intricate
high-dimensional operations with simplified low-dimensional MLPs trained on a
limited subset of pertinent data, and (3) implementing MPC in a concurrent,
multi-phase manner. The proposed method is assessed across an array of
Transformer models and NLP/CV benchmarks. In comparison to the direct MPC-based
evaluation of the target model, our approach substantially reduces the time
required, from thousands of hours to mere tens of hours, with only a nominal
0.20% dip in accuracy when training with the selected data.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: AGIR: Automating Cyber Threat Intelligence Reporting with Natural Language Generation. (arXiv:2310.02655v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02655">http://arxiv.org/abs/2310.02655</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02655]] AGIR: Automating Cyber Threat Intelligence Reporting with Natural Language Generation(http://arxiv.org/abs/2310.02655)</code></li>
<li>Summary: <p>Cyber Threat Intelligence (CTI) reporting is pivotal in contemporary risk
management strategies. As the volume of CTI reports continues to surge, the
demand for automated tools to streamline report generation becomes increasingly
apparent. While Natural Language Processing techniques have shown potential in
handling text data, they often struggle to address the complexity of diverse
data sources and their intricate interrelationships. Moreover, established
paradigms like STIX have emerged as de facto standards within the CTI
community, emphasizing the formal categorization of entities and relations to
facilitate consistent data sharing. In this paper, we introduce AGIR (Automatic
Generation of Intelligence Reports), a transformative Natural Language
Generation tool specifically designed to address the pressing challenges in the
realm of CTI reporting. AGIR's primary objective is to empower security
analysts by automating the labor-intensive task of generating comprehensive
intelligence reports from formal representations of entity graphs. AGIR
utilizes a two-stage pipeline by combining the advantages of template-based
approaches and the capabilities of Large Language Models such as ChatGPT. We
evaluate AGIR's report generation capabilities both quantitatively and
qualitatively. The generated reports accurately convey information expressed
through formal language, achieving a high recall value (0.99) without
introducing hallucination. Furthermore, we compare the fluency and utility of
the reports with state-of-the-art approaches, showing how AGIR achieves higher
scores in terms of Syntactic Log-Odds Ratio (SLOR) and through questionnaires.
By using our tool, we estimate that the report writing time is reduced by more
than 40%, therefore streamlining the CTI production of any organization and
contributing to the automation of several CTI tasks.
</p></li>
</ul>

<h3>Title: Identifying Vulnerability Patches by Comprehending Code Commits with Comprehensive Change Contexts. (arXiv:2310.02530v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02530">http://arxiv.org/abs/2310.02530</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02530]] Identifying Vulnerability Patches by Comprehending Code Commits with Comprehensive Change Contexts(http://arxiv.org/abs/2310.02530)</code></li>
<li>Summary: <p>To help application developers apply vulnerability patches timely, security
researchers maintain vulnerability databases such as National Vulnerability
Database (NVD). By directly monitoring NVD with the name of each used library,
application developers can be aware of vulnerabilities and their patches. Given
that the monitoring results of vulnerability patches are unreliable due to
patch incompleteness of NVD, existing approaches employ deep-learning (DL)
models to identify additional vulnerability patches by determining whether a
code commit fixes a vulnerability. However, these approaches suffer from low
accuracy due to not considering code commits' comprehensive contexts such as
control/data-flow contexts or method-invocation contexts. To improve accuracy,
we design CompVPD, the first approach to identify vulnerability patches by
fine-tuning a large language model (LLM) named StarCoder to comprehend code
commits with comprehensive contexts. Considering that including comprehensive
contexts needs to balance the context size and the training costs of LLM,
CompVPD includes our two novel algorithms to generate comprehensive contexts
within the given window size by removing irrelevant components (i.e., files,
methods, and statements) and adaptively expanding each context. We empirically
compare CompVPD with four state-of-the-art/practice (SOTA) approaches that
identify vulnerability patches. The results show that CompVPD improves the AUC
score by 11% and the F1 score by 30% when compared with the best scores of the
SOTA approaches. Additionally, CompVPD provides high value to security practice
by helping identify 20 vulnerability patches and 18 fixes of high-risk bugs
from 2,500 recent code commits of five highly popular open-source projects.
</p></li>
</ul>

<h3>Title: The Key to Deobfuscation is Pattern of Life, not Overcoming Encryption. (arXiv:2310.02536v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02536">http://arxiv.org/abs/2310.02536</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02536]] The Key to Deobfuscation is Pattern of Life, not Overcoming Encryption(http://arxiv.org/abs/2310.02536)</code></li>
<li>Summary: <p>Preserving privacy is an undeniable benefit to users online. However, this
benefit (unfortunately) also extends to those who conduct cyber attacks and
other types of malfeasance. In this work, we consider the scenario in which
Privacy Preserving Technologies (PPTs) have been used to obfuscate users who
are communicating online with ill intentions. We present a novel methodology
that is effective at deobfuscating such sources by synthesizing measurements
from key locations along protocol transaction paths. Our approach links online
personas with their origin IP addresses based on a Pattern of Life (PoL)
analysis, and is successful even when different PPTs are used. We show that,
when monitoring in the correct places on the Internet, DNS over HTTPS (DoH) and
DNS over TLS (DoT) can be deobfuscated with up to 100% accuracy, when they are
the only privacy-preserving technologies used. Our evaluation used multiple
simulated monitoring points and communications are sampled from an actual
multiyear-long social network message board to replay actual user behavior. Our
evaluation compared plain old DNS, DoH, DoT, and VPN in order to quantify their
relative privacy-preserving abilities and provide recommendations for where
ideal monitoring vantage points would be in the Internet to achieve the best
performance. To illustrate the utility of our methodology, we created a
proof-of-concept cybersecurity analyst dashboard (with backend processing
infrastructure) that uses a search engine interface to allow analysts to
deobfuscate sources based on observed screen names and by providing packet
captures from subsets of vantage points.
</p></li>
</ul>

<h3>Title: Practical, Private Assurance of the Value of Collaboration. (arXiv:2310.02563v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02563">http://arxiv.org/abs/2310.02563</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02563]] Practical, Private Assurance of the Value of Collaboration(http://arxiv.org/abs/2310.02563)</code></li>
<li>Summary: <p>Two parties wish to collaborate on their datasets. However, before they
reveal their datasets to each other, the parties want to have the guarantee
that the collaboration would be fruitful. We look at this problem from the
point of view of machine learning, where one party is promised an improvement
on its prediction model by incorporating data from the other party. The parties
would only wish to collaborate further if the updated model shows an
improvement in accuracy. Before this is ascertained, the two parties would not
want to disclose their models and datasets. In this work, we construct an
interactive protocol for this problem based on the fully homomorphic encryption
scheme over the Torus (TFHE) and label differential privacy, where the
underlying machine learning model is a neural network. Label differential
privacy is used to ensure that computations are not done entirely in the
encrypted domain, which is a significant bottleneck for neural network training
according to the current state-of-the-art FHE implementations. We prove the
security of our scheme in the universal composability framework assuming
honest-but-curious parties, but where one party may not have any expertise in
labelling its initial dataset. Experiments show that we can obtain the output,
i.e., the accuracy of the updated model, with time many orders of magnitude
faster than a protocol using entirely FHE operations.
</p></li>
</ul>

<h3>Title: RLTrace: Synthesizing High-Quality System Call Traces for OS Fuzz Testing. (arXiv:2310.02609v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02609">http://arxiv.org/abs/2310.02609</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02609]] RLTrace: Synthesizing High-Quality System Call Traces for OS Fuzz Testing(http://arxiv.org/abs/2310.02609)</code></li>
<li>Summary: <p>Securing operating system (OS) kernel is one central challenge in today's
cyber security landscape. The cutting-edge testing technique of OS kernel is
software fuzz testing. By mutating the program inputs with random variations
for iterations, fuzz testing aims to trigger program crashes and hangs caused
by potential bugs that can be abused by the inputs. To achieve high OS code
coverage, the de facto OS fuzzer typically composes system call traces as the
input seed to mutate and to interact with OS kernels. Hence, quality and
diversity of the employed system call traces become the prominent factor to
decide the effectiveness of OS fuzzing. However, these system call traces to
date are generated with hand-coded rules, or by analyzing system call logs of
OS utility programs. Our observation shows that such system call traces can
only subsume common usage scenarios of OS system calls, and likely omit hidden
bugs.
</p>
<p>In this research, we propose a deep reinforcement learning-based solution,
called RLTrace, to synthesize diverse and comprehensive system call traces as
the seed to fuzz OS kernels. During model training, the deep learning model
interacts with OS kernels and infers optimal system call traces w.r.t. our
learning goal -- maximizing kernel code coverage. Our evaluation shows that
RLTrace outperforms other seed generators by producing more comprehensive
system call traces, subsuming system call corner usage cases and subtle
dependencies. By feeding the de facto OS fuzzer, SYZKALLER, with system call
traces synthesized by RLTrace, we show that SYZKALLER can achieve higher code
coverage for testing Linux kernels. Furthermore, RLTrace found one
vulnerability in the Linux kernel (version 5.5-rc6), which is publicly unknown
to the best of our knowledge by the time of writing.
</p></li>
</ul>

<h3>Title: Dissecting Smart Contract Languages: A Survey. (arXiv:2310.02799v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02799">http://arxiv.org/abs/2310.02799</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02799]] Dissecting Smart Contract Languages: A Survey(http://arxiv.org/abs/2310.02799)</code></li>
<li>Summary: <p>Blockchain is a distributed ledger technology that gained popularity for
enabling the transformation of cryptocurrency among peers without mediation by
a centralized third-party authority. Smart contracts expand the applications of
blockchain technology and have played a role in its widespread adoption. Smart
contracts are immutable digital programs that are deployed on blockchains to
codify agreements between parties. Existing smart contract implementations have
faced challenges, including security vulnerabilities, leading to significant
losses and concerns. This has stimulated a wave of attempts to improve Smart
Contract Languages (SCLs) to overcome implementation challenges and ensure code
quality, producing many languages with diverse features. Scholars have made
some attempts to classify SCLs and clarify the process of selecting an SCL, but
to the best of our knowledge, no comprehensive survey of existing SCLs has been
published. Our work surpasses earlier efforts by evaluating a significantly
larger set of SCLs, in greater depth, to ease the process of SCL selection for
blockchain research and implementation. In this paper, we (1) propose a robust
framework for comparing existing SCLs, (2) analyze and discuss 36 SCLs,
addressing issues beyond those used to construct the comparison framework, and
(3) define new parameters for future research and development of SCLs. The
survey provides a guide for those who intend to select or use an SCL to
implement smart contracts, develop new SCLs, or add new extensions to the
existing SCLs.
</p></li>
</ul>

<h3>Title: Comparative Analysis of Imbalanced Malware Byteplot Image Classification using Transfer Learning. (arXiv:2310.02742v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02742">http://arxiv.org/abs/2310.02742</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02742]] Comparative Analysis of Imbalanced Malware Byteplot Image Classification using Transfer Learning(http://arxiv.org/abs/2310.02742)</code></li>
<li>Summary: <p>Cybersecurity is a major concern due to the increasing reliance on technology
and interconnected systems. Malware detectors help mitigate cyber-attacks by
comparing malware signatures. Machine learning can improve these detectors by
automating feature extraction, identifying patterns, and enhancing dynamic
analysis. In this paper, the performance of six multiclass classification
models is compared on the Malimg dataset, Blended dataset, and Malevis dataset
to gain insights into the effect of class imbalance on model performance and
convergence. It is observed that the more the class imbalance less the number
of epochs required for convergence and a high variance across the performance
of different models. Moreover, it is also observed that for malware detectors
ResNet50, EfficientNetB0, and DenseNet169 can handle imbalanced and balanced
data well. A maximum precision of 97% is obtained for the imbalanced dataset, a
maximum precision of 95% is obtained on the intermediate imbalance dataset, and
a maximum precision of 95% is obtained for the perfectly balanced dataset.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Reversing Deep Face Embeddings with Probable Privacy Protection. (arXiv:2310.03005v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03005">http://arxiv.org/abs/2310.03005</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03005]] Reversing Deep Face Embeddings with Probable Privacy Protection(http://arxiv.org/abs/2310.03005)</code></li>
<li>Summary: <p>Generally, privacy-enhancing face recognition systems are designed to offer
permanent protection of face embeddings. Recently, so-called soft-biometric
privacy-enhancement approaches have been introduced with the aim of canceling
soft-biometric attributes. These methods limit the amount of soft-biometric
information (gender or skin-colour) that can be inferred from face embeddings.
Previous work has underlined the need for research into rigorous evaluations
and standardised evaluation protocols when assessing privacy protection
capabilities. Motivated by this fact, this paper explores to what extent the
non-invertibility requirement can be met by methods that claim to provide
soft-biometric privacy protection. Additionally, a detailed vulnerability
assessment of state-of-the-art face embedding extractors is analysed in terms
of the transformation complexity used for privacy protection. In this context,
a well-known state-of-the-art face image reconstruction approach has been
evaluated on protected face embeddings to break soft biometric privacy
protection. Experimental results show that biometric privacy-enhanced face
embeddings can be reconstructed with an accuracy of up to approximately 98%,
depending on the complexity of the protection algorithm.
</p></li>
</ul>

<h3>Title: Large Language Models Can Be Good Privacy Protection Learners. (arXiv:2310.02469v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02469">http://arxiv.org/abs/2310.02469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02469]] Large Language Models Can Be Good Privacy Protection Learners(http://arxiv.org/abs/2310.02469)</code></li>
<li>Summary: <p>The proliferation of Large Language Models (LLMs) has driven considerable
interest in fine-tuning them with domain-specific data to create specialized
language models. Nevertheless, such domain-specific fine-tuning data often
contains sensitive personally identifiable information (PII). Direct
fine-tuning LLMs on this data without privacy protection poses a risk of
leakage. To address this challenge, we introduce Privacy Protection Language
Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects
domain-specific knowledge while safeguarding data privacy. Our work offers a
theoretical analysis for model design and delves into various techniques such
as corpus curation, penalty-based unlikelihood in training loss, and
instruction-based tuning, etc. Extensive experiments across diverse datasets
and scenarios demonstrate the effectiveness of our approaches. In particular,
instruction tuning with both positive and negative examples, stands out as a
promising method, effectively protecting private data while enhancing the
model's knowledge. Our work underscores the potential for Large Language Models
as robust privacy protection learners.
</p></li>
</ul>

<h3>Title: Exploring the Impact of Disrupted Peer-to-Peer Communications on Fully Decentralized Learning in Disaster Scenarios. (arXiv:2310.02986v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02986">http://arxiv.org/abs/2310.02986</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02986]] Exploring the Impact of Disrupted Peer-to-Peer Communications on Fully Decentralized Learning in Disaster Scenarios(http://arxiv.org/abs/2310.02986)</code></li>
<li>Summary: <p>Fully decentralized learning enables the distribution of learning resources
and decision-making capabilities across multiple user devices or nodes, and is
rapidly gaining popularity due to its privacy-preserving and decentralized
nature. Importantly, this crowdsourcing of the learning process allows the
system to continue functioning even if some nodes are affected or disconnected.
In a disaster scenario, communication infrastructure and centralized systems
may be disrupted or completely unavailable, hindering the possibility of
carrying out standard centralized learning tasks in these settings. Thus, fully
decentralized learning can help in this case. However, transitioning from
centralized to peer-to-peer communications introduces a dependency between the
learning process and the topology of the communication graph among nodes. In a
disaster scenario, even peer-to-peer communications are susceptible to abrupt
changes, such as devices running out of battery or getting disconnected from
others due to their position. In this study, we investigate the effects of
various disruptions to peer-to-peer communications on decentralized learning in
a disaster setting. We examine the resilience of a decentralized learning
process when a subset of devices drop from the process abruptly. To this end,
we analyze the difference between losing devices holding data, i.e., potential
knowledge, vs. devices contributing only to the graph connectivity, i.e., with
no data. Our findings on a Barabasi-Albert graph topology, where training data
is distributed across nodes in an IID fashion, indicate that the accuracy of
the learning process is more affected by a loss of connectivity than by a loss
of data. Nevertheless, the network remains relatively robust, and the learning
process can achieve a good level of accuracy.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Runtime Verification for Trustworthy Computing. (arXiv:2310.02341v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02341">http://arxiv.org/abs/2310.02341</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02341]] Runtime Verification for Trustworthy Computing(http://arxiv.org/abs/2310.02341)</code></li>
<li>Summary: <p>Autonomous and robotic systems are increasingly being trusted with sensitive
activities with potentially serious consequences if that trust is broken.
Runtime verification techniques present a natural source of inspiration for
monitoring and enforcing the desirable properties of the communication
protocols in place, providing a formal basis and ways to limit intrusiveness. A
recently proposed approach, RV-TEE, shows how runtime verification can enhance
the level of trust to the Rich Execution Environment (REE), consequently adding
a further layer of protection around the Trusted Execution Environment (TEE).
</p>
<p>By reflecting on the implication of deploying RV in the context of
trustworthy computing, we propose practical solutions to two threat models for
the RV-TEE monitoring process: one where the adversary has gained access to the
system without elevated privileges, and another where the adversary gains all
privileges to the host system but fails to steal secrets from the TEE.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Jailbreaker in Jail: Moving Target Defense for Large Language Models. (arXiv:2310.02417v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02417">http://arxiv.org/abs/2310.02417</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02417]] Jailbreaker in Jail: Moving Target Defense for Large Language Models(http://arxiv.org/abs/2310.02417)</code></li>
<li>Summary: <p>Large language models (LLMs), known for their capability in understanding and
following instructions, are vulnerable to adversarial attacks. Researchers have
found that current commercial LLMs either fail to be "harmless" by presenting
unethical answers, or fail to be "helpful" by refusing to offer meaningful
answers when faced with adversarial queries. To strike a balance between being
helpful and harmless, we design a moving target defense (MTD) enhanced LLM
system. The system aims to deliver non-toxic answers that align with outputs
from multiple model candidates, making them more robust against adversarial
attacks. We design a query and output analysis model to filter out unsafe or
non-responsive answers. %to achieve the two objectives of randomly selecting
outputs from different LLMs. We evaluate over 8 most recent chatbot models with
state-of-the-art adversarial queries. Our MTD-enhanced LLM system reduces the
attack success rate from 37.5\% to 0\%. Meanwhile, it decreases the response
refusal rate from 50\% to 0\%.
</p></li>
</ul>

<h3>Title: Splitting the Difference on Adversarial Training. (arXiv:2310.02480v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02480">http://arxiv.org/abs/2310.02480</a></li>
<li>Code URL: https://github.com/matanle51/splitting-the-difference-on-adversarial-training</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02480]] Splitting the Difference on Adversarial Training(http://arxiv.org/abs/2310.02480)</code></li>
<li>Summary: <p>The existence of adversarial examples points to a basic weakness of deep
neural networks. One of the most effective defenses against such examples,
adversarial training, entails training models with some degree of robustness,
usually at the expense of a degraded natural accuracy. Most adversarial
training methods aim to learn a model that finds, for each class, a common
decision boundary encompassing both the clean and perturbed examples. In this
work, we take a fundamentally different approach by treating the perturbed
examples of each class as a separate class to be learned, effectively splitting
each class into two classes: "clean" and "adversarial." This split doubles the
number of classes to be learned, but at the same time considerably simplifies
the decision boundaries. We provide a theoretical plausibility argument that
sheds some light on the conditions under which our approach can be expected to
be beneficial. Likewise, we empirically demonstrate that our method learns
robust models while attaining optimal or near-optimal natural accuracy, e.g.,
on CIFAR-10 we obtain near-optimal natural accuracy of $95.01\%$ alongside
significant robustness across multiple tasks. The ability to achieve such
near-optimal natural accuracy, while maintaining a significant level of
robustness, makes our method applicable to real-world applications where
natural accuracy is at a premium. As a whole, our main contribution is a
general method that confers a significant level of robustness upon classifiers
with only minor or negligible degradation of their natural accuracy.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy Efficiency of Inference Efficient Vision Transformers. (arXiv:2310.02544v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02544">http://arxiv.org/abs/2310.02544</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02544]] SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy Efficiency of Inference Efficient Vision Transformers(http://arxiv.org/abs/2310.02544)</code></li>
<li>Summary: <p>Recently, there has been a lot of progress in reducing the computation of
deep models at inference time. These methods can reduce both the computational
needs and power usage of deep models. Some of these approaches adaptively scale
the compute based on the input instance. We show that such models can be
vulnerable to a universal adversarial patch attack, where the attacker
optimizes for a patch that when pasted on any image, can increase the compute
and power consumption of the model. We run experiments with three different
efficient vision transformer methods showing that in some cases, the attacker
can increase the computation to the maximum possible level by simply pasting a
patch that occupies only 8\% of the image area. We also show that a standard
adversarial training defense method can reduce some of the attack's success. We
believe adaptive efficient methods will be necessary for the future to lower
the power usage of deep models, so we hope our paper encourages the community
to study the robustness of these methods and develop better defense methods for
the proposed attack.
</p></li>
</ul>

<h3>Title: Low-Resource Languages Jailbreak GPT-4. (arXiv:2310.02446v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02446">http://arxiv.org/abs/2310.02446</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02446]] Low-Resource Languages Jailbreak GPT-4(http://arxiv.org/abs/2310.02446)</code></li>
<li>Summary: <p>AI safety training and red-teaming of large language models (LLMs) are
measures to mitigate the generation of unsafe content. Our work exposes the
inherent cross-lingual vulnerability of these safety mechanisms, resulting from
the linguistic inequality of safety training data, by successfully
circumventing GPT-4's safeguard through translating unsafe English inputs into
low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe
translated inputs and provides actionable items that can get the users towards
their harmful goals 79% of the time, which is on par with or even surpassing
state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have
significantly lower attack success rate, which suggests that the cross-lingual
vulnerability mainly applies to low-resource languages. Previously, limited
training on low-resource languages primarily affects speakers of those
languages, causing technological disparities. However, our work highlights a
crucial shift: this deficiency now poses a risk to all LLMs users. Publicly
available translation APIs enable anyone to exploit LLMs' safety
vulnerabilities. Therefore, our work calls for a more holistic red-teaming
efforts to develop robust multilingual safeguards with wide language coverage.
</p></li>
</ul>

<h3>Title: Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models. (arXiv:2310.02949v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02949">http://arxiv.org/abs/2310.02949</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02949]] Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models(http://arxiv.org/abs/2310.02949)</code></li>
<li>Summary: <p>Warning: This paper contains examples of harmful language, and reader
discretion is recommended. The increasing open release of powerful large
language models (LLMs) has facilitated the development of downstream
applications by reducing the essential cost of data annotation and computation.
To ensure AI safety, extensive safety-alignment measures have been conducted to
armor these models against malicious use (primarily hard prompt attack).
However, beneath the seemingly resilient facade of the armor, there might lurk
a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these
safely aligned LLMs can be easily subverted to generate harmful content.
Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of
data can elicit safely-aligned models to adapt to harmful tasks without
sacrificing model helpfulness. Remarkably, the subverted models retain their
capability to respond appropriately to regular inquiries. Experiments across 8
models released by 5 different organizations (LLaMa-2, Falcon, InternLM,
BaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack.
Besides, the single-turn English-only attack successfully transfers to
multi-turn dialogue and other languages. This study serves as a clarion call
for a collective effort to overhaul and fortify the safety of open-source LLMs
against malicious attackers.
</p></li>
</ul>

<h3>Title: No Forking Way: Detecting Cloning Attacks on Intel SGX Applications. (arXiv:2310.03002v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03002">http://arxiv.org/abs/2310.03002</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03002]] No Forking Way: Detecting Cloning Attacks on Intel SGX Applications(http://arxiv.org/abs/2310.03002)</code></li>
<li>Summary: <p>Forking attacks against TEEs like Intel SGX can be carried out either by
rolling back the application to a previous state, or by cloning the application
and by partitioning its inputs across the cloned instances. Current solutions
to forking attacks require Trusted Third Parties (TTP) that are hard to find in
real-world deployments. In the absence of a TTP, many TEE applications rely on
monotonic counters to mitigate forking attacks based on rollbacks; however,
they have no protection mechanism against forking attack based on cloning. In
this paper, we analyze 72 SGX applications and show that approximately 20% of
those are vulnerable to forking attacks based on cloning - including those that
rely on monotonic counters. To address this problem, we present CloneBuster,
the first practical clone-detection mechanism for Intel SGX that does not rely
on a TTP and, as such, can be used directly to protect existing applications.
CloneBuster allows enclaves to (self-) detect whether another enclave with the
same binary is running on the same platform. To do so, CloneBuster relies on a
cache-based covert channel for enclaves to signal their presence to (and detect
the presence of) clones on the same machine. We show that CloneBuster is robust
despite a malicious OS, only incurs a marginal impact on the application
performance, and adds approximately 800 LoC to the TCB. When used in
conjunction with monotonic counters, CloneBuster allows applications to benefit
from a comprehensive protection against forking attacks.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Bag of Tricks for Fully Test-Time Adaptation. (arXiv:2310.02416v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02416">http://arxiv.org/abs/2310.02416</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02416]] Bag of Tricks for Fully Test-Time Adaptation(http://arxiv.org/abs/2310.02416)</code></li>
<li>Summary: <p>Fully Test-Time Adaptation (TTA), which aims at adapting models to data
drifts, has recently attracted wide interest. Numerous tricks and techniques
have been proposed to ensure robust learning on arbitrary streams of unlabeled
data. However, assessing the true impact of each individual technique and
obtaining a fair comparison still constitutes a significant challenge. To help
consolidate the community's knowledge, we present a categorization of selected
orthogonal TTA techniques, including small batch normalization, stream
rebalancing, reliable sample selection, and network confidence calibration. We
meticulously dissect the effect of each approach on different scenarios of
interest. Through our analysis, we shed light on trade-offs induced by those
techniques between accuracy, the computational power required, and model
complexity. We also uncover the synergy that arises when combining techniques
and are able to establish new state-of-the-art results.
</p></li>
</ul>

<h3>Title: SCB-Dataset3: A Benchmark for Detecting Student Classroom Behavior. (arXiv:2310.02522v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02522">http://arxiv.org/abs/2310.02522</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02522]] SCB-Dataset3: A Benchmark for Detecting Student Classroom Behavior(http://arxiv.org/abs/2310.02522)</code></li>
<li>Summary: <p>The use of deep learning methods to automatically detect students' classroom
behavior is a promising approach for analyzing their class performance and
improving teaching effectiveness. However, the lack of publicly available
datasets on student behavior poses a challenge for researchers in this field.
To address this issue, we propose the Student Classroom Behavior dataset
(SCB-dataset3), which represents real-life scenarios. Our dataset comprises
5686 images with 45578 labels, focusing on six behaviors: hand-raising,
reading, writing, using a phone, bowing the head, and leaning over the table.
We evaluated the dataset using the YOLOv5, YOLOv7, and YOLOv8 algorithms,
achieving a mean average precision (map) of up to 80.3$\%$. We believe that our
dataset can serve as a robust foundation for future research in student
behavior detection and contribute to advancements in this field. Our
SCB-dataset3 is available for download at:
https://github.com/Whiffe/SCB-dataset
</p></li>
</ul>

<h3>Title: AdaMerging: Adaptive Model Merging for Multi-Task Learning. (arXiv:2310.02575v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02575">http://arxiv.org/abs/2310.02575</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02575]] AdaMerging: Adaptive Model Merging for Multi-Task Learning(http://arxiv.org/abs/2310.02575)</code></li>
<li>Summary: <p>Multi-task learning (MTL) aims to empower a model to tackle multiple tasks
simultaneously. A recent development known as task arithmetic has revealed that
several models, each fine-tuned for distinct tasks, can be directly merged into
a single model to execute MTL without necessitating a retraining process using
the initial training data. Nevertheless, this direct addition of models often
leads to a significant deterioration in the overall performance of the merged
model. This decline occurs due to potential conflicts and intricate
correlations among the multiple tasks. Consequently, the challenge emerges of
how to merge pre-trained models more effectively without using their original
training data. This paper introduces an innovative technique called Adaptive
Model Merging (AdaMerging). This approach aims to autonomously learn the
coefficients for model merging, either in a task-wise or layer-wise manner,
without relying on the original training data. Specifically, our AdaMerging
method operates as an automatic, unsupervised task arithmetic scheme. It
leverages entropy minimization on unlabeled test samples from the multi-task
setup as a surrogate objective function to iteratively refine the merging
coefficients of the multiple models. Our experimental findings across eight
tasks demonstrate the efficacy of the AdaMerging scheme we put forth. Compared
to the current state-of-the-art task arithmetic merging scheme, AdaMerging
showcases a remarkable 11\% improvement in performance. Notably, AdaMerging
also exhibits superior generalization capabilities when applied to unseen
downstream tasks. Furthermore, it displays a significantly enhanced robustness
to data distribution shifts that may occur during the testing phase.
</p></li>
</ul>

<h3>Title: CoBEV: Elevating Roadside 3D Object Detection with Depth and Height Complementarity. (arXiv:2310.02815v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02815">http://arxiv.org/abs/2310.02815</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02815]] CoBEV: Elevating Roadside 3D Object Detection with Depth and Height Complementarity(http://arxiv.org/abs/2310.02815)</code></li>
<li>Summary: <p>Roadside camera-driven 3D object detection is a crucial task in intelligent
transportation systems, which extends the perception range beyond the
limitations of vision-centric vehicles and enhances road safety. While previous
studies have limitations in using only depth or height information, we find
both depth and height matter and they are in fact complementary. The depth
feature encompasses precise geometric cues, whereas the height feature is
primarily focused on distinguishing between various categories of height
intervals, essentially providing semantic context. This insight motivates the
development of Complementary-BEV (CoBEV), a novel end-to-end monocular 3D
object detection framework that integrates depth and height to construct robust
BEV representations. In essence, CoBEV estimates each pixel's depth and height
distribution and lifts the camera features into 3D space for lateral fusion
using the newly proposed two-stage complementary feature selection (CFS)
module. A BEV feature distillation framework is also seamlessly integrated to
further enhance the detection accuracy from the prior knowledge of the
fusion-modal CoBEV teacher. We conduct extensive experiments on the public 3D
detection benchmarks of roadside camera-based DAIR-V2X-I and Rope3D, as well as
the private Supremind-Road dataset, demonstrating that CoBEV not only achieves
the accuracy of the new state-of-the-art, but also significantly advances the
robustness of previous methods in challenging long-distance scenarios and noisy
camera disturbance, and enhances generalization by a large margin in
heterologous settings with drastic changes in scene and camera parameters. For
the first time, the vehicle AP score of a camera model reaches 80% on
DAIR-V2X-I in terms of easy mode. The source code will be made publicly
available at https://github.com/MasterHow/CoBEV.
</p></li>
</ul>

<h3>Title: Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness. (arXiv:2310.02410v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02410">http://arxiv.org/abs/2310.02410</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02410]] Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness(http://arxiv.org/abs/2310.02410)</code></li>
<li>Summary: <p>Large Mixture of Experts (MoE) models could achieve state-of-the-art quality
on various language tasks, including machine translation task, thanks to the
efficient model scaling capability with expert parallelism. However, it has
brought a fundamental issue of larger memory consumption and increased memory
bandwidth bottleneck at deployment time. In this paper, we propose Mixture of
Quantized Experts (MoQE) which is a simple weight-only quantization method
applying ultra low-bit down to 2-bit quantizations only to expert weights for
mitigating the increased memory and latency issues of MoE models. We show that
low-bit quantization together with the MoE architecture delivers a reliable
model performance while reducing the memory size significantly even without any
additional training in most cases. In particular, expert layers in MoE models
are much more robust to the quantization than conventional feedforward networks
(FFN) layers. In our comprehensive analysis, we show that MoE models with 2-bit
expert weights can deliver better model performance than the dense model
trained on the same dataset. As a result of low-bit quantization, we show the
model size can be reduced by 79.6% of the original half precision floating
point (fp16) MoE model. Combined with an optimized GPU runtime implementation,
it also achieves 1.24X speed-up on A100 GPUs.
</p></li>
</ul>

<h3>Title: Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes. (arXiv:2310.02451v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02451">http://arxiv.org/abs/2310.02451</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02451]] Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes(http://arxiv.org/abs/2310.02451)</code></li>
<li>Summary: <p>Natural Language Processing (NLP) methods have been broadly applied to
clinical tasks. Machine learning and deep learning approaches have been used to
improve the performance of clinical NLP. However, these approaches require
sufficiently large datasets for training, and trained models have been shown to
transfer poorly across sites. These issues have led to the promotion of data
collection and integration across different institutions for accurate and
portable models. However, this can introduce a form of bias called confounding
by provenance. When source-specific data distributions differ at deployment,
this may harm model performance. To address this issue, we evaluate the utility
of backdoor adjustment for text classification in a multi-site dataset of
clinical notes annotated for mentions of substance abuse. Using an evaluation
framework devised to measure robustness to distributional shifts, we assess the
utility of backdoor adjustment. Our results indicate that backdoor adjustment
can effectively mitigate for confounding shift.
</p></li>
</ul>

<h3>Title: JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning. (arXiv:2310.02953v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02953">http://arxiv.org/abs/2310.02953</a></li>
<li>Code URL: https://github.com/gao-xiao-bai/jsontuning</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02953]] JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning(http://arxiv.org/abs/2310.02953)</code></li>
<li>Summary: <p>Instruction tuning has emerged as a crucial process for harnessing the
capabilities of large language models (LLMs) by providing explicit task
instructions, leading to improved performance in various tasks. However,
prevalent text-to-text instruction tuning (TextTuning) methods suffer from
limitations in generalization, robustness, and controllability due to the
ambiguity and lack of explicit structure in tasks. In this paper, we propose
JsonTuning, a novel structure-to-structure approach for instruction tuning. By
leveraging the versatility and structured nature of JSON to represent tasks,
JsonTuning enhances generalization by helping the model understand essential
task elements and their relations, improves robustness by minimizing ambiguity,
and increases controllability by providing explicit control over the output. We
conduct a comprehensive comparative study with diverse language models and
evaluation benchmarks. Experimental results show that JsonTuning outperforms
TextTuning in various applications, showcasing improved performance,
adaptability, robustness, and controllability. By overcoming the limitations of
TextTuning, JsonTuning demonstrates significant potential for more effective
and reliable LLMs capable of handling diverse scenarios.
</p></li>
</ul>

<h3>Title: ARRQP: Anomaly Resilient Real-time QoS Prediction Framework with Graph Convolution. (arXiv:2310.02269v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02269">http://arxiv.org/abs/2310.02269</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02269]] ARRQP: Anomaly Resilient Real-time QoS Prediction Framework with Graph Convolution(http://arxiv.org/abs/2310.02269)</code></li>
<li>Summary: <p>In the realm of modern service-oriented architecture, ensuring Quality of
Service (QoS) is of paramount importance. The ability to predict QoS values in
advance empowers users to make informed decisions. However, achieving accurate
QoS predictions in the presence of various issues and anomalies, including
outliers, data sparsity, grey-sheep instances, and cold-start scenarios,
remains a challenge. Current state-of-the-art methods often fall short when
addressing these issues simultaneously, resulting in performance degradation.
In this paper, we introduce a real-time QoS prediction framework (called ARRQP)
with a specific emphasis on improving resilience to anomalies in the data.
ARRQP utilizes the power of graph convolution techniques to capture intricate
relationships and dependencies among users and services, even when the data is
limited or sparse. ARRQP integrates both contextual information and
collaborative insights, enabling a comprehensive understanding of user-service
interactions. By utilizing robust loss functions, ARRQP effectively reduces the
impact of outliers during the model training. Additionally, we introduce a
sparsity-resilient grey-sheep detection method, which is subsequently treated
separately for QoS prediction. Furthermore, we address the cold-start problem
by emphasizing contextual features over collaborative features. Experimental
results on the benchmark WS-DREAM dataset demonstrate the framework's
effectiveness in achieving accurate and timely QoS predictions.
</p></li>
</ul>

<h3>Title: Reducing Intraspecies and Interspecies Covariate Shift in Traumatic Brain Injury EEG of Humans and Mice Using Transfer Euclidean Alignment. (arXiv:2310.02398v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02398">http://arxiv.org/abs/2310.02398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02398]] Reducing Intraspecies and Interspecies Covariate Shift in Traumatic Brain Injury EEG of Humans and Mice Using Transfer Euclidean Alignment(http://arxiv.org/abs/2310.02398)</code></li>
<li>Summary: <p>While analytics of sleep electroencephalography (EEG) holds certain
advantages over other methods in clinical applications, high variability across
subjects poses a significant challenge when it comes to deploying machine
learning models for classification tasks in the real world. In such instances,
machine learning models that exhibit exceptional performance on a specific
dataset may not necessarily demonstrate similar proficiency when applied to a
distinct dataset for the same task. The scarcity of high-quality biomedical
data further compounds this challenge, making it difficult to evaluate the
model's generality comprehensively. In this paper, we introduce Transfer
Euclidean Alignment - a transfer learning technique to tackle the problem of
the dearth of human biomedical data for training deep learning models. We
tested the robustness of this transfer learning technique on various rule-based
classical machine learning models as well as the EEGNet-based deep learning
model by evaluating on different datasets, including human and mouse data in a
binary classification task of detecting individuals with versus without
traumatic brain injury (TBI). By demonstrating notable improvements with an
average increase of 14.42% for intraspecies datasets and 5.53% for interspecies
datasets, our findings underscore the importance of the use of transfer
learning to improve the performance of machine learning and deep learning
models when using diverse datasets for training.
</p></li>
</ul>

<h3>Title: Feather: An Elegant Solution to Effective DNN Sparsification. (arXiv:2310.02448v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02448">http://arxiv.org/abs/2310.02448</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02448]] Feather: An Elegant Solution to Effective DNN Sparsification(http://arxiv.org/abs/2310.02448)</code></li>
<li>Summary: <p>Neural Network pruning is an increasingly popular way for producing compact
and efficient models, suitable for resource-limited environments, while
preserving high performance. While the pruning can be performed using a
multi-cycle training and fine-tuning process, the recent trend is to encompass
the sparsification process during the standard course of training. To this end,
we introduce Feather, an efficient sparse training module utilizing the
powerful Straight-Through Estimator as its core, coupled with a new
thresholding operator and a gradient scaling technique, enabling robust,
out-of-the-box sparsification performance. Feather's effectiveness and
adaptability is demonstrated using various architectures on the CIFAR dataset,
while on ImageNet it achieves state-of-the-art Top-1 validation accuracy using
the ResNet-50 architecture, surpassing existing methods, including more complex
and computationally heavy ones, by a considerable margin. Code is publicly
available at https://github.com/athglentis/feather .
</p></li>
</ul>

<h3>Title: A Recipe for Improved Certifiable Robustness: Capacity and Data. (arXiv:2310.02513v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02513">http://arxiv.org/abs/2310.02513</a></li>
<li>Code URL: https://github.com/hukkai/liresnet</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02513]] A Recipe for Improved Certifiable Robustness: Capacity and Data(http://arxiv.org/abs/2310.02513)</code></li>
<li>Summary: <p>A key challenge, supported both theoretically and empirically, is that
robustness demands greater network capacity and more data than standard
training. However, effectively adding capacity under stringent Lipschitz
constraints has proven more difficult than it may seem, evident by the fact
that state-of-the-art approach tend more towards \emph{underfitting} than
overfitting. Moreover, we posit that a lack of careful exploration of the
design space for Lipshitz-based approaches has left potential performance gains
on the table. In this work, we provide a more comprehensive evaluation to
better uncover the potential of Lipschitz-based certification methods. Using a
combination of novel techniques, design optimizations, and synthesis of prior
work, we are able to significantly improve the state-of-the-art \emph{verified
robust accuracy} (VRA) for deterministic certification on a variety of
benchmark datasets, and over a range of perturbation sizes. Of particular note,
we discover that the addition of large "Cholesky-orthogonalized residual dense"
layers to the end of existing state-of-the-art Lipschitz-controlled ResNet
architectures is especially effective for increasing network capacity and
performance. Combined with filtered generative data augmentation, our final
results further the state of the art deterministic VRA by up to 8.5 percentage
points. Code is available at \url{https://github.com/hukkai/liresnet}.
</p></li>
</ul>

<h3>Title: Robust Ocean Subgrid-Scale Parameterizations Using Fourier Neural Operators. (arXiv:2310.02691v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02691">http://arxiv.org/abs/2310.02691</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02691]] Robust Ocean Subgrid-Scale Parameterizations Using Fourier Neural Operators(http://arxiv.org/abs/2310.02691)</code></li>
<li>Summary: <p>In climate simulations, small-scale processes shape ocean dynamics but remain
computationally expensive to resolve directly. For this reason, their
contributions are commonly approximated using empirical parameterizations,
which lead to significant errors in long-term projections. In this work, we
develop parameterizations based on Fourier Neural Operators, showcasing their
accuracy and generalizability in comparison to other approaches. Finally, we
discuss the potential and limitations of neural networks operating in the
frequency domain, paving the way for future investigation.
</p></li>
</ul>

<h3>Title: Online Clustering of Bandits with Misspecified User Models. (arXiv:2310.02717v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02717">http://arxiv.org/abs/2310.02717</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02717]] Online Clustering of Bandits with Misspecified User Models(http://arxiv.org/abs/2310.02717)</code></li>
<li>Summary: <p>The contextual linear bandit is an important online learning problem where
given arm features, a learning agent selects an arm at each round to maximize
the cumulative rewards in the long run. A line of works, called the clustering
of bandits (CB), utilize the collaborative effect over user preferences and
have shown significant improvements over classic linear bandit algorithms.
However, existing CB algorithms require well-specified linear user models and
can fail when this critical assumption does not hold. Whether robust CB
algorithms can be designed for more practical scenarios with misspecified user
models remains an open problem. In this paper, we are the first to present the
important problem of clustering of bandits with misspecified user models
(CBMUM), where the expected rewards in user models can be perturbed away from
perfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB
(representing the learned clustering structure with dynamic graph and sets,
respectively), that can accommodate the inaccurate user preference estimations
and erroneous clustering caused by model misspecifications. We prove regret
upper bounds of $O(\epsilon_*T\sqrt{md\log T} + d\sqrt{mT}\log T)$ for our
algorithms under milder assumptions than previous CB works (notably, we move
past a restrictive technical assumption on the distribution of the arms), which
match the lower bound asymptotically in $T$ up to logarithmic factors, and also
match the state-of-the-art results in several degenerate cases. The techniques
in proving the regret caused by misclustering users are quite general and may
be of independent interest. Experiments on both synthetic and real-world data
show our outperformance over previous algorithms.
</p></li>
</ul>

<h3>Title: Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms. (arXiv:2310.02812v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02812">http://arxiv.org/abs/2310.02812</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02812]] Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms(http://arxiv.org/abs/2310.02812)</code></li>
<li>Summary: <p>Manufacturing is gathering extensive amounts of diverse data, thanks to the
growing number of sensors and rapid advances in sensing technologies. Among the
various data types available in SMS settings, time-series data plays a pivotal
role. Hence, TSC emerges is crucial in this domain. The objective of this study
is to fill this gap by providing a rigorous experimental evaluation of the SoTA
ML and DL algorithms for TSC tasks in manufacturing and industrial settings. We
first explored and compiled a comprehensive list of more than 92 SoTA
algorithms from both TSC and manufacturing literature. Following, we selected
the 36 most representative algorithms from this list. To evaluate their
performance across various manufacturing classification tasks, we curated a set
of 22 manufacturing datasets, representative of different characteristics that
cover diverse manufacturing problems. Subsequently, we implemented and
evaluated the algorithms on the manufacturing benchmark datasets, and analyzed
the results for each dataset. Based on the results, ResNet, DrCIF,
InceptionTime, and ARSENAL are the top-performing algorithms, boasting an
average accuracy of over 96.6% across all 22 manufacturing TSC datasets. These
findings underscore the robustness, efficiency, scalability, and effectiveness
of convolutional kernels in capturing temporal features in time-series data, as
three out of the top four performing algorithms leverage these kernels for
feature extraction. Additionally, LSTM, BiLSTM, and TS-LSTM algorithms deserve
recognition for their effectiveness in capturing features within time-series
data using RNN-based structures.
</p></li>
</ul>

<h3>Title: Stable and Interpretable Deep Learning for Tabular Data: Introducing InterpreTabNet with the Novel InterpreStability Metric. (arXiv:2310.02870v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02870">http://arxiv.org/abs/2310.02870</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02870]] Stable and Interpretable Deep Learning for Tabular Data: Introducing InterpreTabNet with the Novel InterpreStability Metric(http://arxiv.org/abs/2310.02870)</code></li>
<li>Summary: <p>As Artificial Intelligence (AI) integrates deeper into diverse sectors, the
quest for powerful models has intensified. While significant strides have been
made in boosting model capabilities and their applicability across domains, a
glaring challenge persists: many of these state-of-the-art models remain as
black boxes. This opacity not only complicates the explanation of model
decisions to end-users but also obstructs insights into intermediate processes
for model designers. To address these challenges, we introduce InterpreTabNet,
a model designed to enhance both classification accuracy and interpretability
by leveraging the TabNet architecture with an improved attentive module. This
design ensures robust gradient propagation and computational stability.
Additionally, we present a novel evaluation metric, InterpreStability, which
quantifies the stability of a model's interpretability. The proposed model and
metric mark a significant stride forward in explainable models' research,
setting a standard for transparency and interpretability in AI model design and
application across diverse sectors. InterpreTabNet surpasses other leading
solutions in tabular data analysis across varied application scenarios, paving
the way for further research into creating deep-learning models that are both
highly accurate and inherently explainable. The introduction of the
InterpreStability metric ensures that the interpretability of future models can
be measured and compared in a consistent and rigorous manner. Collectively,
these contributions have the potential to promote the design principles and
development of next-generation interpretable AI models, widening the adoption
of interpretable AI solutions in critical decision-making environments.
</p></li>
</ul>

<h3>Title: CoLiDE: Concomitant Linear DAG Estimation. (arXiv:2310.02895v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02895">http://arxiv.org/abs/2310.02895</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02895]] CoLiDE: Concomitant Linear DAG Estimation(http://arxiv.org/abs/2310.02895)</code></li>
<li>Summary: <p>We deal with the combinatorial problem of learning directed acyclic graph
(DAG) structure from observational data adhering to a linear structural
equation model (SEM). Leveraging advances in differentiable, nonconvex
characterizations of acyclicity, recent efforts have advocated a continuous
constrained optimization paradigm to efficiently explore the space of DAGs.
Most existing methods employ lasso-type score functions to guide this search,
which (i) require expensive penalty parameter retuning when the
$\textit{unknown}$ SEM noise variances change across problem instances; and
(ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we
propose a new convex score function for sparsity-aware learning of linear DAGs,
which incorporates concomitant estimation of scale and thus effectively
decouples the sparsity parameter from the exogenous noise levels.
Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE
($\textbf{Co}$ncomitant $\textbf{Li}$near $\textbf{D}$AG
$\textbf{E}$stimation), a regression-based criterion amenable to efficient
gradient computation and closed-form estimation of noise variances in
heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods
without incurring added complexity, especially when the DAGs are larger and the
noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced
stability manifested via reduced standard deviations in several domain-specific
metrics, underscoring the robustness of our novel linear DAG estimator.
</p></li>
</ul>

<h3>Title: Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits. (arXiv:2310.02975v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02975">http://arxiv.org/abs/2310.02975</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02975]] Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits(http://arxiv.org/abs/2310.02975)</code></li>
<li>Summary: <p>Heavy-tailed distributions naturally arise in many settings, from finance to
telecommunications. While regret minimization under sub-Gaussian or bounded
support rewards has been widely studied, learning on heavy-tailed distributions
only gained popularity over the last decade. In the stochastic heavy-tailed
bandit problem, an agent learns under the assumption that the distributions
have finite moments of maximum order $1+\epsilon$ which are uniformly bounded
by a constant $u$, for some $\epsilon \in (0,1]$. To the best of our knowledge,
literature only provides algorithms requiring these two quantities as an input.
In this paper, we study the stochastic adaptive heavy-tailed bandit, a
variation of the standard setting where both $\epsilon$ and $u$ are unknown to
the agent. We show that adaptivity comes at a cost, introducing two lower
bounds on the regret of any adaptive algorithm, implying a higher regret w.r.t.
the standard setting. Finally, we introduce a specific distributional
assumption and provide Adaptive Robust UCB, a regret minimization strategy
matching the known lower bound for the heavy-tailed MAB problem.
</p></li>
</ul>

<h3>Title: Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions. (arXiv:2310.02987v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02987">http://arxiv.org/abs/2310.02987</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02987]] Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions(http://arxiv.org/abs/2310.02987)</code></li>
<li>Summary: <p>Machine learning approaches relying on such criteria as adversarial
robustness or multi-agent settings have raised the need for solving
game-theoretic equilibrium problems. Of particular relevance to these
applications are methods targeting finite-sum structure, which generically
arises in empirical variants of learning problems in these contexts. Further,
methods with computable approximation errors are highly desirable, as they
provide verifiable exit criteria. Motivated by these applications, we study
finite-sum monotone inclusion problems, which model broad classes of
equilibrium problems. Our main contributions are variants of the classical
Halpern iteration that employ variance reduction to obtain improved complexity
guarantees in which $n$ component operators in the finite sum are ``on
average'' either cocoercive or Lipschitz continuous and monotone, with
parameter $L$. The resulting oracle complexity of our methods, which provide
guarantees for the last iterate and for a (computable) operator norm residual,
is $\widetilde{\mathcal{O}}( n + \sqrt{n}L\varepsilon^{-1})$, which improves
upon existing methods by a factor up to $\sqrt{n}$. This constitutes the first
variance reduction-type result for general finite-sum monotone inclusions and
for more specific problems such as convex-concave optimization when operator
norm residual is the optimality measure. We further argue that, up to
poly-logarithmic factors, this complexity is unimprovable in the monotone
Lipschitz setting; i.e., the provided result is near-optimal.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Optimizing Key-Selection for Face-based One-Time Biometrics via Morphing. (arXiv:2310.02997v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02997">http://arxiv.org/abs/2310.02997</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02997]] Optimizing Key-Selection for Face-based One-Time Biometrics via Morphing(http://arxiv.org/abs/2310.02997)</code></li>
<li>Summary: <p>Nowadays, facial recognition systems are still vulnerable to adversarial
attacks. These attacks vary from simple perturbations of the input image to
modifying the parameters of the recognition model to impersonate an authorised
subject. So-called privacy-enhancing facial recognition systems have been
mostly developed to provide protection of stored biometric reference data, i.e.
templates. In the literature, privacy-enhancing facial recognition approaches
have focused solely on conventional security threats at the template level,
ignoring the growing concern related to adversarial attacks. Up to now, few
works have provided mechanisms to protect face recognition against adversarial
attacks while maintaining high security at the template level. In this paper,
we propose different key selection strategies to improve the security of a
competitive cancelable scheme operating at the signal level. Experimental
results show that certain strategies based on signal-level key selection can
lead to complete blocking of the adversarial attack based on an iterative
optimization for the most secure threshold, while for the most practical
threshold, the attack success chance can be decreased to approximately 5.0%.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: A Prototype-Based Neural Network for Image Anomaly Detection and Localization. (arXiv:2310.02576v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02576">http://arxiv.org/abs/2310.02576</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02576]] A Prototype-Based Neural Network for Image Anomaly Detection and Localization(http://arxiv.org/abs/2310.02576)</code></li>
<li>Summary: <p>Image anomaly detection and localization perform not only image-level anomaly
classification but also locate pixel-level anomaly regions. Recently, it has
received much research attention due to its wide application in various fields.
This paper proposes ProtoAD, a prototype-based neural network for image anomaly
detection and localization. First, the patch features of normal images are
extracted by a deep network pre-trained on nature images. Then, the prototypes
of the normal patch features are learned by non-parametric clustering. Finally,
we construct an image anomaly localization network (ProtoAD) by appending the
feature extraction network with $L2$ feature normalization, a $1\times1$
convolutional layer, a channel max-pooling, and a subtraction operation. We use
the prototypes as the kernels of the $1\times1$ convolutional layer; therefore,
our neural network does not need a training phase and can conduct anomaly
detection and localization in an end-to-end manner. Extensive experiments on
two challenging industrial anomaly detection datasets, MVTec AD and BTAD,
demonstrate that ProtoAD achieves competitive performance compared to the
state-of-the-art methods with a higher inference speed. The source code is
available at: https://github.com/98chao/ProtoAD.
</p></li>
</ul>

<h3>Title: MedPrompt: Cross-Modal Prompting for Multi-Task Medical Image Translation. (arXiv:2310.02663v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02663">http://arxiv.org/abs/2310.02663</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02663]] MedPrompt: Cross-Modal Prompting for Multi-Task Medical Image Translation(http://arxiv.org/abs/2310.02663)</code></li>
<li>Summary: <p>Cross-modal medical image translation is an essential task for synthesizing
missing modality data for clinical diagnosis. However, current learning-based
techniques have limitations in capturing cross-modal and global features,
restricting their suitability to specific pairs of modalities. This lack of
versatility undermines their practical usefulness, particularly considering
that the missing modality may vary for different cases. In this study, we
present MedPrompt, a multi-task framework that efficiently translates different
modalities. Specifically, we propose the Self-adaptive Prompt Block, which
dynamically guides the translation network towards distinct modalities. Within
this framework, we introduce the Prompt Extraction Block and the Prompt Fusion
Block to efficiently encode the cross-modal prompt. To enhance the extraction
of global features across diverse modalities, we incorporate the Transformer
model. Extensive experimental results involving five datasets and four pairs of
modalities demonstrate that our proposed model achieves state-of-the-art visual
quality and exhibits excellent generalization capability.
</p></li>
</ul>

<h3>Title: DOMINO: A Dual-System for Multi-step Visual Language Reasoning. (arXiv:2310.02804v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02804">http://arxiv.org/abs/2310.02804</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02804]] DOMINO: A Dual-System for Multi-step Visual Language Reasoning(http://arxiv.org/abs/2310.02804)</code></li>
<li>Summary: <p>Visual language reasoning requires a system to extract text or numbers from
information-dense images like charts or plots and perform logical or arithmetic
reasoning to arrive at an answer. To tackle this task, existing work relies on
either (1) an end-to-end vision-language model trained on a large amount of
data, or (2) a two-stage pipeline where a captioning model converts the image
into text that is further read by another large language model to deduce the
answer. However, the former approach forces the model to answer a complex
question with one single step, and the latter approach is prone to inaccurate
or distracting information in the converted text that can confuse the language
model. In this work, we propose a dual-system for multi-step multimodal
reasoning, which consists of a "System-1" step for visual information
extraction and a "System-2" step for deliberate reasoning. Given an input,
System-2 breaks down the question into atomic sub-steps, each guiding System-1
to extract the information required for reasoning from the image. Experiments
on chart and plot datasets show that our method with a pre-trained System-2
module performs competitively compared to prior work on in- and
out-of-distribution data. By fine-tuning the System-2 module (LLaMA-2 70B) on
only a small amount of data on multi-step reasoning, the accuracy of our method
is further improved and surpasses the best fully-supervised end-to-end approach
by 5.7% and a pipeline approach with FlanPaLM (540B) by 7.5% on a challenging
dataset with human-authored questions.
</p></li>
</ul>

<h3>Title: Towards Domain-Specific Features Disentanglement for Domain Generalization. (arXiv:2310.03007v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03007">http://arxiv.org/abs/2310.03007</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03007]] Towards Domain-Specific Features Disentanglement for Domain Generalization(http://arxiv.org/abs/2310.03007)</code></li>
<li>Summary: <p>Distributional shift between domains poses great challenges to modern machine
learning algorithms. The domain generalization (DG) signifies a popular line
targeting this issue, where these methods intend to uncover universal patterns
across disparate distributions. Noted, the crucial challenge behind DG is the
existence of irrelevant domain features, and most prior works overlook this
information. Motivated by this, we propose a novel contrastive-based
disentanglement method CDDG, to effectively utilize the disentangled features
to exploit the over-looked domain-specific features, and thus facilitating the
extraction of the desired cross-domain category features for DG tasks.
Specifically, CDDG learns to decouple inherent mutually exclusive features by
leveraging them in the latent space, thus making the learning discriminative.
Extensive experiments conducted on various benchmark datasets demonstrate the
superiority of our method compared to other state-of-the-art approaches.
Furthermore, visualization evaluations confirm the potential of our method in
achieving effective feature disentanglement.
</p></li>
</ul>

<h3>Title: ProtoNER: Few shot Incremental Learning for Named Entity Recognition using Prototypical Networks. (arXiv:2310.02372v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02372">http://arxiv.org/abs/2310.02372</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02372]] ProtoNER: Few shot Incremental Learning for Named Entity Recognition using Prototypical Networks(http://arxiv.org/abs/2310.02372)</code></li>
<li>Summary: <p>Key value pair (KVP) extraction or Named Entity Recognition(NER) from
visually rich documents has been an active area of research in document
understanding and data extraction domain. Several transformer based models such
as LayoutLMv2, LayoutLMv3, and LiLT have emerged achieving state of the art
results. However, addition of even a single new class to the existing model
requires (a) re-annotation of entire training dataset to include this new class
and (b) retraining the model again. Both of these issues really slow down the
deployment of updated model. \\ We present \textbf{ProtoNER}: Prototypical
Network based end-to-end KVP extraction model that allows addition of new
classes to an existing model while requiring minimal number of newly annotated
training samples. The key contributions of our model are: (1) No dependency on
dataset used for initial training of the model, which alleviates the need to
retain original training dataset for longer duration as well as data
re-annotation which is very time consuming task, (2) No intermediate synthetic
data generation which tends to add noise and results in model's performance
degradation, and (3) Hybrid loss function which allows model to retain
knowledge about older classes as well as learn about newly added classes.\\
Experimental results show that ProtoNER finetuned with just 30 samples is able
to achieve similar results for the newly added classes as that of regular model
finetuned with 2600 samples.
</p></li>
</ul>

<h3>Title: Multimodal Question Answering for Unified Information Extraction. (arXiv:2310.03017v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03017">http://arxiv.org/abs/2310.03017</a></li>
<li>Code URL: https://github.com/osu-nlp-group/mqa</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03017]] Multimodal Question Answering for Unified Information Extraction(http://arxiv.org/abs/2310.03017)</code></li>
<li>Summary: <p>Multimodal information extraction (MIE) aims to extract structured
information from unstructured multimedia content. Due to the diversity of tasks
and settings, most current MIE models are task-specific and data-intensive,
which limits their generalization to real-world scenarios with diverse task
requirements and limited labeled data. To address these issues, we propose a
novel multimodal question answering (MQA) framework to unify three MIE tasks by
reformulating them into a unified span extraction and multi-choice QA pipeline.
Extensive experiments on six datasets show that: 1) Our MQA framework
consistently and significantly improves the performances of various
off-the-shelf large multimodal models (LMM) on MIE tasks, compared to vanilla
prompting. 2) In the zero-shot setting, MQA outperforms previous
state-of-the-art baselines by a large margin. In addition, the effectiveness of
our framework can successfully transfer to the few-shot setting, enhancing LMMs
on a scale of 10B parameters to be competitive or outperform much larger
language models such as ChatGPT and GPT-4. Our MQA framework can serve as a
general principle of utilizing LMMs to better solve MIE and potentially other
downstream multimodal tasks.
</p></li>
</ul>

<h3>Title: OneAdapt: Fast Adaptation for Deep Learning Applications via Backpropagation. (arXiv:2310.02422v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02422">http://arxiv.org/abs/2310.02422</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02422]] OneAdapt: Fast Adaptation for Deep Learning Applications via Backpropagation(http://arxiv.org/abs/2310.02422)</code></li>
<li>Summary: <p>Deep learning inference on streaming media data, such as object detection in
video or LiDAR feeds and text extraction from audio waves, is now ubiquitous.
To achieve high inference accuracy, these applications typically require
significant network bandwidth to gather high-fidelity data and extensive GPU
resources to run deep neural networks (DNNs). While the high demand for network
bandwidth and GPU resources could be substantially reduced by optimally
adapting the configuration knobs, such as video resolution and frame rate,
current adaptation techniques fail to meet three requirements simultaneously:
adapt configurations (i) with minimum extra GPU or bandwidth overhead; (ii) to
reach near-optimal decisions based on how the data affects the final DNN's
accuracy, and (iii) do so for a range of configuration knobs. This paper
presents OneAdapt, which meets these requirements by leveraging a
gradient-ascent strategy to adapt configuration knobs. The key idea is to
embrace DNNs' differentiability to quickly estimate the accuracy's gradient to
each configuration knob, called AccGrad. Specifically, OneAdapt estimates
AccGrad by multiplying two gradients: InputGrad (i.e. how each configuration
knob affects the input to the DNN) and DNNGrad (i.e. how the DNN input affects
the DNN inference output). We evaluate OneAdapt across five types of
configurations, four analytic tasks, and five types of input data. Compared to
state-of-the-art adaptation schemes, OneAdapt cuts bandwidth usage and GPU
usage by 15-59% while maintaining comparable accuracy or improves accuracy by
1-5% while using equal or fewer resources.
</p></li>
</ul>

<h3>Title: Credit card score prediction using machine learning models: A new dataset. (arXiv:2310.02956v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02956">http://arxiv.org/abs/2310.02956</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02956]] Credit card score prediction using machine learning models: A new dataset(http://arxiv.org/abs/2310.02956)</code></li>
<li>Summary: <p>The use of credit cards has recently increased, creating an essential need
for credit card assessment methods to minimize potential risks. This study
investigates the utilization of machine learning (ML) models for credit card
default prediction system. The main goal here is to investigate the
best-performing ML model for new proposed credit card scoring dataset. This new
dataset includes credit card transaction histories and customer profiles, is
proposed and tested using a variety of machine learning algorithms, including
logistic regression, decision trees, random forests, multi layer perceptron
(MLP) neural network, XGBoost, and LightGBM. To prepare the data for machine
learning models, we perform data pre-proccessing, feature extraction, feature
selection, and data balancing techniques. Experimental results demonstrate that
MLP outperforms logistic regression, decision trees, random forests, LightGBM,
and XGBoost in terms of predictive performance in true positive rate, achieving
an impressive area under the curve (AUC) of 86.7% and an accuracy rate of
91.6%, with a recall rate exceeding 80%. These results indicate the superiority
of MLP in predicting the default customers and assessing the potential risks.
Furthermore, they help banks and other financial institutions in predicting
loan defaults at an earlier stage.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedL2P: Federated Learning to Personalize. (arXiv:2310.02420v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02420">http://arxiv.org/abs/2310.02420</a></li>
<li>Code URL: https://github.com/royson/fedl2p</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02420]] FedL2P: Federated Learning to Personalize(http://arxiv.org/abs/2310.02420)</code></li>
<li>Summary: <p>Federated learning (FL) research has made progress in developing algorithms
for distributed learning of global models, as well as algorithms for local
personalization of those common models to the specifics of each client's local
data distribution. However, different FL problems may require different
personalization strategies, and it may not even be possible to define an
effective one-size-fits-all personalization strategy for all clients: depending
on how similar each client's optimal predictor is to that of the global model,
different personalization strategies may be preferred. In this paper, we
consider the federated meta-learning problem of learning personalization
strategies. Specifically, we consider meta-nets that induce the batch-norm and
learning rate parameters for each client given local data statistics. By
learning these meta-nets through FL, we allow the whole FL network to
collaborate in learning a customized personalization strategy for each client.
Empirical results show that this framework improves on a range of standard
hand-crafted personalization baselines in both label and feature shift
situations.
</p></li>
</ul>

<h3>Title: Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation. (arXiv:2310.02842v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02842">http://arxiv.org/abs/2310.02842</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02842]] Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation(http://arxiv.org/abs/2310.02842)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have the ability to solve a variety of tasks,
such as text summarization and mathematical questions, just out of the box, but
they are often trained with a single task in mind. Due to high computational
costs, the current trend is to use prompt instruction tuning to better adjust
monolithic, pretrained LLMs for new -- but often individual -- downstream
tasks. Thus, how one would expand prompt tuning to handle -- concomitantly --
heterogeneous tasks and data distributions is a widely open question. To
address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs,
associated with smart gating functionality: the latter -- whose design is one
of the contributions of this paper -- can identify relevant skills embedded in
different groups of prompts and dynamically assign combined experts (i.e.,
collection of prompts), based on the target task. Additionally, MoPs are
empirically agnostic to any model compression technique applied -- for
efficiency reasons -- as well as instruction data source and task composition.
In practice, MoPs can simultaneously mitigate prompt training "interference" in
multi-task, multi-source scenarios (e.g., task and data heterogeneity across
sources), as well as possible implications from model approximations. As a
highlight, MoPs manage to decrease final perplexity from $\sim20\%$ up to
$\sim70\%$, as compared to baselines, in the federated scenario, and from $\sim
3\%$ up to $\sim30\%$ in the centralized scenario.
</p></li>
</ul>

<h3>Title: Federated Conditional Stochastic Optimization. (arXiv:2310.02524v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02524">http://arxiv.org/abs/2310.02524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02524]] Federated Conditional Stochastic Optimization(http://arxiv.org/abs/2310.02524)</code></li>
<li>Summary: <p>Conditional stochastic optimization has found applications in a wide range of
machine learning tasks, such as invariant learning, AUPRC maximization, and
meta-learning. As the demand for training models with large-scale distributed
data grows in these applications, there is an increasing need for
communication-efficient distributed optimization algorithms, such as federated
learning algorithms. This paper considers the nonconvex conditional stochastic
optimization in federated learning and proposes the first federated conditional
stochastic optimization algorithm (FCSG) with a conditional stochastic gradient
estimator and a momentum-based algorithm (FCSG-M). To match the lower bound
complexity in the single-machine setting, we design an accelerated algorithm
(Acc-FCSG-M) via the variance reduction to achieve the best sample and
communication complexity. Compared with the existing optimization analysis for
MAML in FL, federated conditional stochastic optimization considers the sample
of tasks. Extensive experimental results on various tasks validate the
efficiency of these algorithms.
</p></li>
</ul>

<h3>Title: Heterogeneous Federated Learning Using Knowledge Codistillation. (arXiv:2310.02549v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02549">http://arxiv.org/abs/2310.02549</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02549]] Heterogeneous Federated Learning Using Knowledge Codistillation(http://arxiv.org/abs/2310.02549)</code></li>
<li>Summary: <p>Federated Averaging, and many federated learning algorithm variants which
build upon it, have a limitation: all clients must share the same model
architecture. This results in unused modeling capacity on many clients, which
limits model performance. To address this issue, we propose a method that
involves training a small model on the entire pool and a larger model on a
subset of clients with higher capacity. The models exchange information
bidirectionally via knowledge distillation, utilizing an unlabeled dataset on a
server without sharing parameters. We present two variants of our method, which
improve upon federated averaging on image classification and language modeling
tasks. We show this technique can be useful even if only out-of-domain or
limited in-domain distillation data is available. Additionally, the
bi-directional knowledge distillation allows for domain transfer between the
models when different pool populations introduce domain shift.
</p></li>
</ul>

<h3>Title: Hire When You Need to: Gradual Participant Recruitment for Auction-based Federated Learning. (arXiv:2310.02651v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02651">http://arxiv.org/abs/2310.02651</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02651]] Hire When You Need to: Gradual Participant Recruitment for Auction-based Federated Learning(http://arxiv.org/abs/2310.02651)</code></li>
<li>Summary: <p>The success of federated Learning (FL) depends on the quantity and quality of
the data owners (DOs) as well as their motivation to join FL model training.
Reputation-based FL participant selection methods have been proposed. However,
they still face the challenges of the cold start problem and potential
selection bias towards highly reputable DOs. Such a bias can result in lower
reputation DOs being prematurely excluded from future FL training rounds,
thereby reducing the diversity of training data and the generalizability of the
resulting models. To address these challenges, we propose the Gradual
Participant Selection scheme for Auction-based Federated Learning (GPS-AFL).
Unlike existing AFL incentive mechanisms which generally assume that all DOs
required for an FL task must be selected in one go, GPS-AFL gradually selects
the required DOs over multiple rounds of training as more information is
revealed through repeated interactions. It is designed to strike a balance
between cost saving and performance enhancement, while mitigating the drawbacks
of selection bias in reputation-based FL. Extensive experiments based on
real-world datasets demonstrate the significant advantages of GPS-AFL, which
reduces costs by 33.65% and improved total utility by 2.91%, on average
compared to the best-performing state-of-the-art approach.
</p></li>
</ul>

<h3>Title: Exploring Federated Optimization by Reducing Variance of Adaptive Unbiased Client Sampling. (arXiv:2310.02698v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02698">http://arxiv.org/abs/2310.02698</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02698]] Exploring Federated Optimization by Reducing Variance of Adaptive Unbiased Client Sampling(http://arxiv.org/abs/2310.02698)</code></li>
<li>Summary: <p>Federated Learning (FL) systems usually sample a fraction of clients to
conduct a training process. Notably, the variance of global estimates for
updating the global model built on information from sampled clients is highly
related to federated optimization quality. This paper explores a line of "free"
adaptive client sampling techniques in federated optimization, where the server
builds promising sampling probability and reliable global estimates without
requiring additional local communication and computation. We capture a minor
variant in the sampling procedure and improve the global estimation
accordingly. Based on that, we propose a novel sampler called K-Vib, which
solves an online convex optimization respecting client sampling in federated
optimization. It achieves improved a linear speed up on regret bound
$\tilde{\mathcal{O}}\big(N^{\frac{1}{3}}T^{\frac{2}{3}}/K^{\frac{4}{3}}\big)$
with communication budget $K$. As a result, it significantly improves the
performance of federated optimization. Theoretical improvements and intensive
experiments on classic federated tasks demonstrate our findings.
</p></li>
</ul>

<h3>Title: Tackling Hybrid Heterogeneity on Federated Optimization via Gradient Diversity Maximization. (arXiv:2310.02702v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02702">http://arxiv.org/abs/2310.02702</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02702]] Tackling Hybrid Heterogeneity on Federated Optimization via Gradient Diversity Maximization(http://arxiv.org/abs/2310.02702)</code></li>
<li>Summary: <p>Federated learning refers to a distributed machine learning paradigm in which
data samples are decentralized and distributed among multiple clients. These
samples may exhibit statistical heterogeneity, which refers to data
distributions are not independent and identical across clients. Additionally,
system heterogeneity, or variations in the computational power of the clients,
introduces biases into federated learning. The combined effects of statistical
and system heterogeneity can significantly reduce the efficiency of federated
optimization. However, the impact of hybrid heterogeneity is not rigorously
discussed. This paper explores how hybrid heterogeneity affects federated
optimization by investigating server-side optimization. The theoretical results
indicate that adaptively maximizing gradient diversity in server update
direction can help mitigate the potential negative consequences of hybrid
heterogeneity. To this end, we introduce a novel server-side gradient-based
optimizer \textsc{FedAWARE} with theoretical guarantees provided. Intensive
experiments in heterogeneous federated settings demonstrate that our proposed
optimizer can significantly enhance the performance of federated learning
across varying degrees of hybrid heterogeneity.
</p></li>
</ul>

<h3>Title: Recent Methodological Advances in Federated Learning for Healthcare. (arXiv:2310.02874v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02874">http://arxiv.org/abs/2310.02874</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02874]] Recent Methodological Advances in Federated Learning for Healthcare(http://arxiv.org/abs/2310.02874)</code></li>
<li>Summary: <p>For healthcare datasets, it is often not possible to combine data samples
from multiple sites due to ethical, privacy or logistical concerns. Federated
learning allows for the utilisation of powerful machine learning algorithms
without requiring the pooling of data. Healthcare data has many simultaneous
challenges which require new methodologies to address, such as highly-siloed
data, class imbalance, missing data, distribution shifts and non-standardised
variables. Federated learning adds significant methodological complexity to
conventional centralised machine learning, requiring distributed optimisation,
communication between nodes, aggregation of models and redistribution of
models. In this systematic review, we consider all papers on Scopus that were
published between January 2015 and February 2023 and which describe new
federated learning methodologies for addressing challenges with healthcare
data. We performed a detailed review of the 89 papers which fulfilled these
criteria. Significant systemic issues were identified throughout the literature
which compromise the methodologies in many of the papers reviewed. We give
detailed recommendations to help improve the quality of the methodology
development for federated learning in healthcare.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Eye Fairness: A Large-Scale 3D Imaging Dataset for Equitable Eye Diseases Screening and Fair Identity Scaling. (arXiv:2310.02492v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02492">http://arxiv.org/abs/2310.02492</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02492]] Eye Fairness: A Large-Scale 3D Imaging Dataset for Equitable Eye Diseases Screening and Fair Identity Scaling(http://arxiv.org/abs/2310.02492)</code></li>
<li>Summary: <p>Fairness or equity in machine learning is profoundly important for societal
well-being, but limited public datasets hinder its progress, especially in the
area of medicine. It is undeniable that fairness in medicine is one of the most
important areas for fairness learning's applications. Currently, no large-scale
public medical datasets with 3D imaging data for fairness learning are
available, while 3D imaging data in modern clinics are standard tests for
disease diagnosis. In addition, existing medical fairness datasets are actually
repurposed datasets, and therefore they typically have limited demographic
identity attributes with at most three identity attributes of age, gender, and
race for fairness modeling. To address this gap, we introduce our Eye Fairness
dataset with 30,000 subjects (Harvard-EF) covering three major eye diseases
including age-related macular degeneration, diabetic retinopathy, and glaucoma
affecting 380 million patients globally. Our Harvard-EF dataset includes both
2D fundus photos and 3D optical coherence tomography scans with six demographic
identity attributes including age, gender, race, ethnicity, preferred language,
and marital status. We also propose a fair identity scaling (FIS) approach
combining group and individual scaling together to improve model fairness. Our
FIS approach is compared with various state-of-the-art fairness learning
methods with superior performance in the racial, gender, and ethnicity fairness
tasks with 2D and 3D imaging data, which demonstrate the utilities of our
Harvard-EF dataset for fairness learning. To facilitate fairness comparisons
between different models, we propose performance-scaled disparity measures,
which can be used to compare model fairness accounting for overall performance
levels. The dataset and code are publicly accessible via
\url{https://ophai.hms.harvard.edu/datasets/harvard-ef30k}.
</p></li>
</ul>

<h3>Title: Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. (arXiv:2310.02980v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02980">http://arxiv.org/abs/2310.02980</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02980]] Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors(http://arxiv.org/abs/2310.02980)</code></li>
<li>Summary: <p>Modeling long-range dependencies across sequences is a longstanding goal in
machine learning and has led to architectures, such as state space models, that
dramatically outperform Transformers on long sequences. However, these
impressive empirical gains have been by and large demonstrated on benchmarks
(e.g. Long Range Arena), where models are randomly initialized and trained to
predict a target label from an input sequence. In this work, we show that
random initialization leads to gross overestimation of the differences between
architectures and that pretraining with standard denoising objectives, using
$\textit{only the downstream task data}$, leads to dramatic gains across
multiple architectures and to very small gaps between Transformers and state
space models (SSMs). In stark contrast to prior works, we find vanilla
Transformers to match the performance of S4 on Long Range Arena when properly
pretrained, and we improve the best reported results of SSMs on the PathX-256
task by 20 absolute points. Subsequently, we analyze the utility of
previously-proposed structured parameterizations for SSMs and show they become
mostly redundant in the presence of data-driven initialization obtained through
pretraining. Our work shows that, when evaluating different architectures on
supervised tasks, incorporation of data-driven priors via pretraining is
essential for reliable performance estimation, and can be done efficiently.
</p></li>
</ul>

<h3>Title: Fair Feature Selection: A Comparison of Multi-Objective Genetic Algorithms. (arXiv:2310.02752v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02752">http://arxiv.org/abs/2310.02752</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02752]] Fair Feature Selection: A Comparison of Multi-Objective Genetic Algorithms(http://arxiv.org/abs/2310.02752)</code></li>
<li>Summary: <p>Machine learning classifiers are widely used to make decisions with a major
impact on people's lives (e.g. accepting or denying a loan, hiring decisions,
etc). In such applications,the learned classifiers need to be both accurate and
fair with respect to different groups of people, with different values of
variables such as sex and race. This paper focuses on fair feature selection
for classification, i.e. methods that select a feature subset aimed at
maximising both the accuracy and the fairness of the predictions made by a
classifier. More specifically, we compare two recently proposed Genetic
Algorithms (GAs) for fair feature selection that are based on two different
multi-objective optimisation approaches: (a) a Pareto dominance-based GA; and
(b) a lexicographic optimisation-based GA, where maximising accuracy has higher
priority than maximising fairness. Both GAs use the same measures of accuracy
and fairness, allowing for a controlled comparison. As far as we know, this is
the first comparison between the Pareto and lexicographic approaches for fair
classification. The results show that, overall, the lexicographic GA
outperformed the Pareto GA with respect to accuracy without degradation of the
fairness of the learned classifiers. This is an important result because at
present nearly all GAs for fair classification are based on the Pareto
approach, so these results suggest a promising new direction for research in
this area.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: A Grammatical Compositional Model for Video Action Detection. (arXiv:2310.02887v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02887">http://arxiv.org/abs/2310.02887</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02887]] A Grammatical Compositional Model for Video Action Detection(http://arxiv.org/abs/2310.02887)</code></li>
<li>Summary: <p>Analysis of human actions in videos demands understanding complex human
dynamics, as well as the interaction between actors and context. However, these
interaction relationships usually exhibit large intra-class variations from
diverse human poses or object manipulations, and fine-grained inter-class
differences between similar actions. Thus the performance of existing methods
is severely limited. Motivated by the observation that interactive actions can
be decomposed into actor dynamics and participating objects or humans, we
propose to investigate the composite property of them. In this paper, we
present a novel Grammatical Compositional Model (GCM) for action detection
based on typical And-Or graphs. Our model exploits the intrinsic structures and
latent relationships of actions in a hierarchical manner to harness both the
compositionality of grammar models and the capability of expressing rich
features of DNNs. The proposed model can be readily embodied into a neural
network module for efficient optimization in an end-to-end manner. Extensive
experiments are conducted on the AVA dataset and the Something-Else task to
demonstrate the superiority of our model, meanwhile the interpretability is
enhanced through an inference parsing procedure.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h3>Title: FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models. (arXiv:2310.02401v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02401">http://arxiv.org/abs/2310.02401</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02401]] FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models(http://arxiv.org/abs/2310.02401)</code></li>
<li>Summary: <p>Text-to-image generative models based on latent diffusion models (LDM) have
demonstrated their outstanding ability in generating high-quality and
high-resolution images according to language prompt. Based on these powerful
latent diffusion models, various fine-tuning methods have been proposed to
achieve the personalization of text-to-image diffusion models such as artistic
style adaptation and human face transfer. However, the unauthorized usage of
data for model personalization has emerged as a prevalent concern in relation
to copyright violations. For example, a malicious user may use the fine-tuning
technique to generate images which mimic the style of a painter without his/her
permission. In light of this concern, we have proposed FT-Shield, a
watermarking approach specifically designed for the fine-tuning of
text-to-image diffusion models to aid in detecting instances of infringement.
We develop a novel algorithm for the generation of the watermark to ensure that
the watermark on the training images can be quickly and accurately transferred
to the generated images of text-to-image diffusion models. A watermark will be
detected on an image by a binary watermark detector if the image is generated
by a model that has been fine-tuned using the protected watermarked images.
Comprehensive experiments were conducted to validate the effectiveness of
FT-Shield.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. (arXiv:2310.02279v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02279">http://arxiv.org/abs/2310.02279</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02279]] Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion(http://arxiv.org/abs/2310.02279)</code></li>
<li>Summary: <p>Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion
model sampling at the cost of sample quality but lack a natural way to
trade-off quality for speed. To address this limitation, we propose Consistency
Trajectory Model (CTM), a generalization encompassing CM and score-based models
as special cases. CTM trains a single neural network that can -- in a single
forward pass -- output scores (i.e., gradients of log-density) and enables
unrestricted traversal between any initial and final time along the Probability
Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables
the efficient combination of adversarial training and denoising score matching
loss to enhance performance and achieves new state-of-the-art FIDs for
single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at
64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes,
both deterministic and stochastic, involving long jumps along the ODE solution
trajectories. It consistently improves sample quality as computational budgets
increase, avoiding the degradation seen in CM. Furthermore, CTM's access to the
score accommodates all diffusion model inference techniques, including exact
likelihood computation.
</p></li>
</ul>

<h3>Title: EditVal: Benchmarking Diffusion Based Text-Guided Image Editing Methods. (arXiv:2310.02426v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02426">http://arxiv.org/abs/2310.02426</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02426]] EditVal: Benchmarking Diffusion Based Text-Guided Image Editing Methods(http://arxiv.org/abs/2310.02426)</code></li>
<li>Summary: <p>A plethora of text-guided image editing methods have recently been developed
by leveraging the impressive capabilities of large-scale diffusion-based
generative models such as Imagen and Stable Diffusion. A standardized
evaluation protocol, however, does not exist to compare methods across
different types of fine-grained edits. To address this gap, we introduce
EditVal, a standardized benchmark for quantitatively evaluating text-guided
image editing methods. EditVal consists of a curated dataset of images, a set
of editable attributes for each image drawn from 13 possible edit types, and an
automated evaluation pipeline that uses pre-trained vision-language models to
assess the fidelity of generated images for each edit type. We use EditVal to
benchmark 8 cutting-edge diffusion-based editing methods including SINE, Imagic
and Instruct-Pix2Pix. We complement this with a large-scale human study where
we show that EditVall's automated evaluation pipeline is strongly correlated
with human-preferences for the edit types we considered. From both the human
study and automated evaluation, we find that: (i) Instruct-Pix2Pix, Null-Text
and SINE are the top-performing methods averaged across different edit types,
however {\it only} Instruct-Pix2Pix and Null-Text are able to preserve original
image properties; (ii) Most of the editing methods fail at edits involving
spatial operations (e.g., changing the position of an object). (iii) There is
no `winner' method which ranks the best individually across a range of
different edit types. We hope that our benchmark can pave the way to developing
more reliable text-guided image editing tools in the future. We will publicly
release EditVal, and all associated code and human-study templates to support
these research directions in https://deep-ml-research.github.io/editval/.
</p></li>
</ul>

<h3>Title: Generalization in diffusion models arises from geometry-adaptive harmonic representation. (arXiv:2310.02557v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02557">http://arxiv.org/abs/2310.02557</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02557]] Generalization in diffusion models arises from geometry-adaptive harmonic representation(http://arxiv.org/abs/2310.02557)</code></li>
<li>Summary: <p>High-quality samples generated with score-based reverse diffusion algorithms
provide evidence that deep neural networks (DNN) trained for denoising can
learn high-dimensional densities, despite the curse of dimensionality. However,
recent reports of memorization of the training set raise the question of
whether these networks are learning the "true" continuous density of the data.
Here, we show that two denoising DNNs trained on non-overlapping subsets of a
dataset learn nearly the same score function, and thus the same density, with a
surprisingly small number of training images. This strong generalization
demonstrates an alignment of powerful inductive biases in the DNN architecture
and/or training algorithm with properties of the data distribution. We analyze
these, demonstrating that the denoiser performs a shrinkage operation in a
basis adapted to the underlying image. Examination of these bases reveals
oscillating harmonic structures along contours and in homogeneous image
regions. We show that trained denoisers are inductively biased towards these
geometry-adaptive harmonic representations by demonstrating that they arise
even when the network is trained on image classes such as low-dimensional
manifolds, for which the harmonic basis is suboptimal. Additionally, we show
that the denoising performance of the networks is near-optimal when trained on
regular image classes for which the optimal basis is known to be
geometry-adaptive and harmonic.
</p></li>
</ul>

<h3>Title: SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D. (arXiv:2310.02596v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02596">http://arxiv.org/abs/2310.02596</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02596]] SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D(http://arxiv.org/abs/2310.02596)</code></li>
<li>Summary: <p>It is inherently ambiguous to lift 2D results from pre-trained diffusion
models to a 3D world for text-to-3D generation. 2D diffusion models solely
learn view-agnostic priors and thus lack 3D knowledge during the lifting,
leading to the multi-view inconsistency problem. We find that this problem
primarily stems from geometric inconsistency, and avoiding misplaced geometric
structures substantially mitigates the problem in the final outputs. Therefore,
we improve the consistency by aligning the 2D geometric priors in diffusion
models with well-defined 3D shapes during the lifting, addressing the vast
majority of the problem. This is achieved by fine-tuning the 2D diffusion model
to be viewpoint-aware and to produce view-specific coordinate maps of
canonically oriented 3D objects. In our process, only coarse 3D information is
used for aligning. This "coarse" alignment not only resolves the multi-view
inconsistency in geometries but also retains the ability in 2D diffusion models
to generate detailed and diversified high-quality objects unseen in the 3D
datasets. Furthermore, our aligned geometric priors (AGP) are generic and can
be seamlessly integrated into various state-of-the-art pipelines, obtaining
high generalizability in terms of unseen shapes and visual appearance while
greatly alleviating the multi-view inconsistency problem. Our method represents
a new state-of-the-art performance with an 85+% consistency rate by human
evaluation, while many previous methods are around 30%. Our project page is
https://sweetdreamer3d.github.io/
</p></li>
</ul>

<h3>Title: MagicDrive: Street View Generation with Diverse 3D Geometry Control. (arXiv:2310.02601v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02601">http://arxiv.org/abs/2310.02601</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02601]] MagicDrive: Street View Generation with Diverse 3D Geometry Control(http://arxiv.org/abs/2310.02601)</code></li>
<li>Summary: <p>Recent advancements in diffusion models have significantly enhanced the data
synthesis with 2D control. Yet, precise 3D control in street view generation,
crucial for 3D perception tasks, remains elusive. Specifically, utilizing
Bird's-Eye View (BEV) as the primary condition often leads to challenges in
geometry control (e.g., height), affecting the representation of object shapes,
occlusion patterns, and road surface elevations, all of which are essential to
perception data synthesis, especially for 3D object detection tasks. In this
paper, we introduce MagicDrive, a novel street view generation framework
offering diverse 3D geometry controls, including camera poses, road maps, and
3D bounding boxes, together with textual descriptions, achieved through
tailored encoding strategies. Besides, our design incorporates a cross-view
attention module, ensuring consistency across multiple camera views. With
MagicDrive, we achieve high-fidelity street-view synthesis that captures
nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV
segmentation and 3D object detection.
</p></li>
</ul>

<h3>Title: On Memorization in Diffusion Models. (arXiv:2310.02664v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02664">http://arxiv.org/abs/2310.02664</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02664]] On Memorization in Diffusion Models(http://arxiv.org/abs/2310.02664)</code></li>
<li>Summary: <p>Due to their capacity to generate novel and high-quality samples, diffusion
models have attracted significant research interest in recent years. Notably,
the typical training objective of diffusion models, i.e., denoising score
matching, has a closed-form optimal solution that can only generate training
data replicating samples. This indicates that a memorization behavior is
theoretically expected, which contradicts the common generalization ability of
state-of-the-art diffusion models, and thus calls for a deeper understanding.
Looking into this, we first observe that memorization behaviors tend to occur
on smaller-sized datasets, which motivates our definition of effective model
memorization (EMM), a metric measuring the maximum size of training data at
which a learned diffusion model approximates its theoretical optimum. Then, we
quantify the impact of the influential factors on these memorization behaviors
in terms of EMM, focusing primarily on data distribution, model configuration,
and training procedure. Besides comprehensive empirical results identifying the
influential factors, we surprisingly find that conditioning training data on
uninformative random labels can significantly trigger the memorization in
diffusion models. Our study holds practical significance for diffusion model
users and offers clues to theoretical research in deep generative models. Code
is available at https://github.com/sail-sg/DiffMemorize.
</p></li>
</ul>

<h3>Title: ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF. (arXiv:2310.02712v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02712">http://arxiv.org/abs/2310.02712</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02712]] ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF(http://arxiv.org/abs/2310.02712)</code></li>
<li>Summary: <p>Recently, there has been a significant advancement in text-to-image diffusion
models, leading to groundbreaking performance in 2D image generation. These
advancements have been extended to 3D models, enabling the generation of novel
3D objects from textual descriptions. This has evolved into NeRF editing
methods, which allow the manipulation of existing 3D objects through textual
conditioning. However, existing NeRF editing techniques have faced limitations
in their performance due to slow training speeds and the use of loss functions
that do not adequately consider editing. To address this, here we present a
novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding
real-world scenes into the latent space of the latent diffusion model (LDM)
through a unique refinement layer. This approach enables us to obtain a NeRF
backbone that is not only faster but also more amenable to editing compared to
traditional image space NeRF editing. Furthermore, we propose an improved loss
function tailored for editing by migrating the delta denoising score (DDS)
distillation loss, originally used in 2D image editing to the three-dimensional
domain. This novel loss function surpasses the well-known score distillation
sampling (SDS) loss in terms of suitability for editing purposes. Our
experimental results demonstrate that ED-NeRF achieves faster editing speed
while producing improved output quality compared to state-of-the-art 3D editing
models.
</p></li>
</ul>

<h3>Title: Magicremover: Tuning-free Text-guided Image inpainting with Diffusion Models. (arXiv:2310.02848v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02848">http://arxiv.org/abs/2310.02848</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02848]] Magicremover: Tuning-free Text-guided Image inpainting with Diffusion Models(http://arxiv.org/abs/2310.02848)</code></li>
<li>Summary: <p>Image inpainting aims to fill in the missing pixels with visually coherent
and semantically plausible content. Despite the great progress brought from
deep generative models, this task still suffers from i. the difficulties in
large-scale realistic data collection and costly model training; and ii. the
intrinsic limitations in the traditionally user-defined binary masks on objects
with unclear boundaries or transparent texture. In this paper, we propose
MagicRemover, a tuning-free method that leverages the powerful diffusion models
for text-guided image inpainting. We introduce an attention guidance strategy
to constrain the sampling process of diffusion models, enabling the erasing of
instructed areas and the restoration of occluded content. We further propose a
classifier optimization algorithm to facilitate the denoising stability within
less sampling steps. Extensive comparisons are conducted among our MagicRemover
and state-of-the-art methods including quantitative evaluation and user study,
demonstrating the significant improvement of MagicRemover on high-quality image
inpainting. We will release our code at https://github.com/exisas/Magicremover.
</p></li>
</ul>

<h3>Title: Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with Visual and Textual Prompts. (arXiv:2310.02906v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02906">http://arxiv.org/abs/2310.02906</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02906]] Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with Visual and Textual Prompts(http://arxiv.org/abs/2310.02906)</code></li>
<li>Summary: <p>Image synthesis approaches, e.g., generative adversarial networks, have been
popular as a form of data augmentation in medical image analysis tasks. It is
primarily beneficial to overcome the shortage of publicly accessible data and
associated quality annotations. However, the current techniques often lack
control over the detailed contents in generated images, e.g., the type of
disease patterns, the location of lesions, and attributes of the diagnosis. In
this work, we adapt the latest advance in the generative model, i.e., the
diffusion model, with the added control flow using lesion-specific visual and
textual prompts for generating dermatoscopic images. We further demonstrate the
advantage of our diffusion model-based framework over the classical generation
models in both the image quality and boosting the segmentation performance on
skin lesions. It can achieve a 9% increase in the SSIM image quality measure
and an over 5% increase in Dice coefficients over the prior arts.
</p></li>
</ul>

<h3>Title: T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation. (arXiv:2310.02977v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02977">http://arxiv.org/abs/2310.02977</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02977]] T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation(http://arxiv.org/abs/2310.02977)</code></li>
<li>Summary: <p>Recent methods in text-to-3D leverage powerful pretrained diffusion models to
optimize NeRF. Notably, these methods are able to produce high-quality 3D
scenes without training on 3D data. Due to the open-ended nature of the task,
most studies evaluate their results with subjective case studies and user
experiments, thereby presenting a challenge in quantitatively addressing the
question: How has current progress in Text-to-3D gone so far? In this paper, we
introduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing
diverse text prompts of three increasing complexity levels that are specially
designed for 3D generation. To assess both the subjective quality and the text
alignment, we propose two automatic metrics based on multi-view images produced
by the 3D contents. The quality metric combines multi-view text-image scores
and regional convolution to detect quality and view inconsistency. The
alignment metric uses multi-view captioning and Large Language Model (LLM)
evaluation to measure text-3D consistency. Both metrics closely correlate with
different dimensions of human judgments, providing a paradigm for efficiently
evaluating text-to-3D models. The benchmarking results, shown in Fig. 1, reveal
performance differences among six prevalent text-to-3D methods. Our analysis
further highlights the common struggles for current methods on generating
surroundings and multi-object scenes, as well as the bottleneck of leveraging
2D guidance for 3D generation. Our project page is available at:
https://t3bench.com.
</p></li>
</ul>

<h3>Title: Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples. (arXiv:2310.02988v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02988">http://arxiv.org/abs/2310.02988</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02988]] Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples(http://arxiv.org/abs/2310.02988)</code></li>
<li>Summary: <p>While vision-language models (VLMs) have achieved remarkable performance
improvements recently, there is growing evidence that these models also posses
harmful biases with respect to social attributes such as gender and race. Prior
studies have primarily focused on probing such bias attributes individually
while ignoring biases associated with intersections between social attributes.
This could be due to the difficulty of collecting an exhaustive set of
image-text pairs for various combinations of social attributes from existing
datasets. To address this challenge, we employ text-to-image diffusion models
to produce counterfactual examples for probing intserctional social biases at
scale. Our approach utilizes Stable Diffusion with cross attention control to
produce sets of counterfactual image-text pairs that are highly similar in
their depiction of a subject (e.g., a given occupation) while differing only in
their depiction of intersectional social attributes (e.g., race &amp; gender). We
conduct extensive experiments using our generated dataset which reveal the
intersectional social biases present in state-of-the-art VLMs.
</p></li>
</ul>

<h3>Title: Efficient-3DiM: Learning a Generalizable Single-image Novel-view Synthesizer in One Day. (arXiv:2310.03015v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03015">http://arxiv.org/abs/2310.03015</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03015]] Efficient-3DiM: Learning a Generalizable Single-image Novel-view Synthesizer in One Day(http://arxiv.org/abs/2310.03015)</code></li>
<li>Summary: <p>The task of novel view synthesis aims to generate unseen perspectives of an
object or scene from a limited set of input images. Nevertheless, synthesizing
novel views from a single image still remains a significant challenge in the
realm of computer vision. Previous approaches tackle this problem by adopting
mesh prediction, multi-plain image construction, or more advanced techniques
such as neural radiance fields. Recently, a pre-trained diffusion model that is
specifically designed for 2D image synthesis has demonstrated its capability in
producing photorealistic novel views, if sufficiently optimized on a 3D
finetuning task. Although the fidelity and generalizability are greatly
improved, training such a powerful diffusion model requires a vast volume of
training data and model parameters, resulting in a notoriously long time and
high computational costs. To tackle this issue, we propose Efficient-3DiM, a
simple but effective framework to learn a single-image novel-view synthesizer.
Motivated by our in-depth analysis of the inference process of diffusion
models, we propose several pragmatic strategies to reduce the training overhead
to a manageable scale, including a crafted timestep sampling strategy, a
superior 3D feature extractor, and an enhanced training scheme. When combined,
our framework is able to reduce the total training time from 10 days to less
than 1 day, significantly accelerating the training process under the same
computational platform (one instance with 8 Nvidia A100 GPUs). Comprehensive
experiments are conducted to demonstrate the efficiency and generalizability of
our proposed method.
</p></li>
</ul>

<h3>Title: Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models. (arXiv:2310.03020v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03020">http://arxiv.org/abs/2310.03020</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03020]] Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models(http://arxiv.org/abs/2310.03020)</code></li>
<li>Summary: <p>Zero-shot novel view synthesis (NVS) from a single image is an essential
problem in 3D object understanding. While recent approaches that leverage
pre-trained generative models can synthesize high-quality novel views from
in-the-wild inputs, they still struggle to maintain 3D consistency across
different views. In this paper, we present Consistent-1-to-3, which is a
generative framework that significantly mitigate this issue. Specifically, we
decompose the NVS task into two stages: (i) transforming observed regions to a
novel view, and (ii) hallucinating unseen regions. We design a scene
representation transformer and view-conditioned diffusion model for performing
these two stages respectively. Inside the models, to enforce 3D consistency, we
propose to employ epipolor-guided attention to incorporate geometry
constraints, and multi-view attention to better aggregate multi-view
information. Finally, we design a hierarchy generation paradigm to generate
long sequences of consistent views, allowing a full 360 observation of the
provided object image. Qualitative and quantitative evaluation over multiple
datasets demonstrate the effectiveness of the proposed mechanisms against
state-of-the-art approaches. Our project page is at
https://jianglongye.com/consistent123/
</p></li>
</ul>

<h3>Title: Stochastic force inference via density estimation. (arXiv:2310.02366v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02366">http://arxiv.org/abs/2310.02366</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02366]] Stochastic force inference via density estimation(http://arxiv.org/abs/2310.02366)</code></li>
<li>Summary: <p>Inferring dynamical models from low-resolution temporal data continues to be
a significant challenge in biophysics, especially within transcriptomics, where
separating molecular programs from noise remains an important open problem. We
explore a common scenario in which we have access to an adequate amount of
cross-sectional samples at a few time-points, and assume that our samples are
generated from a latent diffusion process. We propose an approach that relies
on the probability flow associated with an underlying diffusion process to
infer an autonomous, nonlinear force field interpolating between the
distributions. Given a prior on the noise model, we employ score-matching to
differentiate the force field from the intrinsic noise. Using relevant
biophysical examples, we demonstrate that our approach can extract
non-conservative forces from non-stationary data, that it learns equilibrium
dynamics when applied to steady-state data, and that it can do so with both
additive and multiplicative noise models.
</p></li>
</ul>

<h3>Title: SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02391">http://arxiv.org/abs/2310.02391</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02391]] SE(3)-Stochastic Flow Matching for Protein Backbone Generation(http://arxiv.org/abs/2310.02391)</code></li>
<li>Summary: <p>The computational design of novel protein structures has the potential to
impact numerous scientific disciplines greatly. Toward this goal, we introduce
$\text{FoldFlow}$ a series of novel generative models of increasing modeling
power based on the flow-matching paradigm over $3\text{D}$ rigid motions --
i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein
backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free
approach to learning deterministic continuous-time dynamics and matching
invariant target distributions on $\text{SE(3)}$. We next accelerate training
by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$,
leading to the construction of both more simple and stable flows. Finally, we
design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free
training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our
family of $\text{FoldFlow}$ generative models offer several key advantages over
previous approaches to the generative modeling of proteins: they are more
stable and faster to train than diffusion-based approaches, and our models
enjoy the ability to map any invariant source distribution to any invariant
target distribution over $\text{SE(3)}$. Empirically, we validate our FoldFlow
models on protein backbone generation of up to $300$ amino acids leading to
high-quality designable, diverse, and novel samples.
</p></li>
</ul>

<h3>Title: Learning to Reach Goals via Diffusion. (arXiv:2310.02505v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02505">http://arxiv.org/abs/2310.02505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02505]] Learning to Reach Goals via Diffusion(http://arxiv.org/abs/2310.02505)</code></li>
<li>Summary: <p>Diffusion models are a powerful class of generative models capable of mapping
random noise in high-dimensional spaces to a target manifold through iterative
denoising. In this work, we present a novel perspective on goal-conditioned
reinforcement learning by framing it within the context of diffusion modeling.
Analogous to the diffusion process, where Gaussian noise is used to create
random trajectories that walk away from the data manifold, we construct
trajectories that move away from potential goal states. We then learn a
goal-conditioned policy analogous to the score function. This approach, which
we call Merlin, can reach predefined or novel goals from an arbitrary initial
state without learning a separate value function. We consider three choices for
the noise model to replace Gaussian noise in diffusion - reverse play from the
buffer, reverse dynamics model, and a novel non-parametric approach. We
theoretically justify our approach and validate it on offline goal-reaching
tasks. Empirical results are competitive with state-of-the-art methods, which
suggests this perspective on diffusion for RL is a simple, scalable, and
effective direction for sequential decision-making.
</p></li>
</ul>

<h3>Title: Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical Coarse-graining SO(3)-Equivariant Autoencoders. (arXiv:2310.02508v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02508">http://arxiv.org/abs/2310.02508</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02508]] Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical Coarse-graining SO(3)-Equivariant Autoencoders(http://arxiv.org/abs/2310.02508)</code></li>
<li>Summary: <p>Three-dimensional native states of natural proteins display recurring and
hierarchical patterns. Yet, traditional graph-based modeling of protein
structures is often limited to operate within a single fine-grained resolution,
and lacks hourglass neural architectures to learn those high-level building
blocks. We narrow this gap by introducing Ophiuchus, an SO(3)-equivariant
coarse-graining model that efficiently operates on all heavy atoms of standard
protein residues, while respecting their relevant symmetries. Our model departs
from current approaches that employ graph modeling, instead focusing on local
convolutional coarsening to model sequence-motif interactions in log-linear
length complexity. We train Ophiuchus on contiguous fragments of PDB monomers,
investigating its reconstruction capabilities across different compression
rates. We examine the learned latent space and demonstrate its prompt usage in
conformational interpolation, comparing interpolated trajectories to structure
snapshots from the PDBFlex dataset. Finally, we leverage denoising diffusion
probabilistic models (DDPM) to efficiently sample readily-decodable latent
embeddings of diverse miniproteins. Our experiments demonstrate Ophiuchus to be
a scalable basis for efficient protein modeling and generation.
</p></li>
</ul>

<h3>Title: MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data Augmentation. (arXiv:2310.02520v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02520">http://arxiv.org/abs/2310.02520</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02520]] MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data Augmentation(http://arxiv.org/abs/2310.02520)</code></li>
<li>Summary: <p>Health risk prediction is one of the fundamental tasks under predictive
modeling in the medical domain, which aims to forecast the potential health
risks that patients may face in the future using their historical Electronic
Health Records (EHR). Researchers have developed several risk prediction models
to handle the unique challenges of EHR data, such as its sequential nature,
high dimensionality, and inherent noise. These models have yielded impressive
results. Nonetheless, a key issue undermining their effectiveness is data
insufficiency. A variety of data generation and augmentation methods have been
introduced to mitigate this issue by expanding the size of the training data
set through the learning of underlying data distributions. However, the
performance of these methods is often limited due to their task-unrelated
design. To address these shortcomings, this paper introduces a novel,
end-to-end diffusion-based risk prediction model, named MedDiffusion. It
enhances risk prediction performance by creating synthetic patient data during
training to enlarge sample space. Furthermore, MedDiffusion discerns hidden
relationships between patient visits using a step-wise attention mechanism,
enabling the model to automatically retain the most vital information for
generating high-quality data. Experimental evaluation on four real-world
medical datasets demonstrates that MedDiffusion outperforms 14 cutting-edge
baselines in terms of PR-AUC, F1, and Cohen's Kappa. We also conduct ablation
studies and benchmark our model against GAN-based alternatives to further
validate the rationality and adaptability of our model design. Additionally, we
analyze generated data to offer fresh insights into the model's
interpretability.
</p></li>
</ul>

<h3>Title: Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization. (arXiv:2310.02679v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02679">http://arxiv.org/abs/2310.02679</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02679]] Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization(http://arxiv.org/abs/2310.02679)</code></li>
<li>Summary: <p>We tackle the problem of sampling from intractable high-dimensional density
functions, a fundamental task that often appears in machine learning and
statistics. We extend recent sampling-based approaches that leverage controlled
stochastic processes to model approximate samples from these target densities.
The main drawback of these approaches is that the training objective requires
full trajectories to compute, resulting in sluggish credit assignment issues
due to use of entire trajectories and a learning signal present only at the
terminal time. In this work, we present Diffusion Generative Flow Samplers
(DGFS), a sampling-based framework where the learning process can be tractably
broken down into short partial trajectory segments, via parameterizing an
additional "flow function". Our method takes inspiration from the theory
developed for generative flow networks (GFlowNets), allowing us to make use of
intermediate learning signals and benefit from off-policy exploration
capabilities. Through a variety of challenging experiments, we demonstrate that
DGFS results in more accurate estimates of the normalization constant than
closely-related prior methods.
</p></li>
</ul>

<h3>Title: Fast, Expressive SE$(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space. (arXiv:2310.02970v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02970">http://arxiv.org/abs/2310.02970</a></li>
<li>Code URL: https://github.com/ebekkers/ponita</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02970]] Fast, Expressive SE$(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space(http://arxiv.org/abs/2310.02970)</code></li>
<li>Summary: <p>Based on the theory of homogeneous spaces we derive \textit{geometrically
optimal edge attributes} to be used within the flexible message passing
framework. We formalize the notion of weight sharing in convolutional networks
as the sharing of message functions over point-pairs that should be treated
equally. We define equivalence classes of point-pairs that are identical up to
a transformation in the group and derive attributes that uniquely identify
these classes. Weight sharing is then obtained by conditioning message
functions on these attributes. As an application of the theory, we develop an
efficient equivariant group convolutional network for processing 3D point
clouds. The theory of homogeneous spaces tells us how to do group convolutions
with feature maps over the homogeneous space of positions $\mathbb{R}^3$,
position and orientations $\mathbb{R}^3 {\times} S^2$, and the group SE$(3)$
itself. Among these, $\mathbb{R}^3 {\times} S^2$ is an optimal choice due to
the ability to represent directional information, which $\mathbb{R}^3$ methods
cannot, and it significantly enhances computational efficiency compared to
indexing features on the full SE$(3)$ group. We empirically support this claim
by reaching state-of-the-art results -- in accuracy and speed -- on three
different benchmarks: interatomic potential energy prediction, trajectory
forecasting in N-body systems, and generating molecules via equivariant
diffusion models.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: ViT-ReciproCAM: Gradient and Attention-Free Visual Explanations for Vision Transformer. (arXiv:2310.02588v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02588">http://arxiv.org/abs/2310.02588</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02588]] ViT-ReciproCAM: Gradient and Attention-Free Visual Explanations for Vision Transformer(http://arxiv.org/abs/2310.02588)</code></li>
<li>Summary: <p>This paper presents a novel approach to address the challenges of
understanding the prediction process and debugging prediction errors in Vision
Transformers (ViT), which have demonstrated superior performance in various
computer vision tasks such as image classification and object detection. While
several visual explainability techniques, such as CAM, Grad-CAM, Score-CAM, and
Recipro-CAM, have been extensively researched for Convolutional Neural Networks
(CNNs), limited research has been conducted on ViT. Current state-of-the-art
solutions for ViT rely on class agnostic Attention-Rollout and Relevance
techniques. In this work, we propose a new gradient-free visual explanation
method for ViT, called ViT-ReciproCAM, which does not require attention matrix
and gradient information. ViT-ReciproCAM utilizes token masking and generated
new layer outputs from the target layer's input to exploit the correlation
between activated tokens and network predictions for target classes. Our
proposed method outperforms the state-of-the-art Relevance method in the
Average Drop-Coherence-Complexity (ADCC) metric by $4.58\%$ to $5.80\%$ and
generates more localized saliency maps. Our experiments demonstrate the
effectiveness of ViT-ReciproCAM and showcase its potential for understanding
and debugging ViT models. Our proposed method provides an efficient and
easy-to-implement alternative for generating visual explanations, without
requiring attention and gradient information, which can be beneficial for
various applications in the field of computer vision.
</p></li>
</ul>

<h3>Title: P2CADNet: An End-to-End Reconstruction Network for Parametric 3D CAD Model from Point Clouds. (arXiv:2310.02638v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02638">http://arxiv.org/abs/2310.02638</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02638]] P2CADNet: An End-to-End Reconstruction Network for Parametric 3D CAD Model from Point Clouds(http://arxiv.org/abs/2310.02638)</code></li>
<li>Summary: <p>Computer Aided Design (CAD), especially the feature-based parametric CAD,
plays an important role in modern industry and society. However, the
reconstruction of featured CAD model is more challenging than the
reconstruction of other CAD models. To this end, this paper proposes an
end-to-end network to reconstruct featured CAD model from point cloud
(P2CADNet). Initially, the proposed P2CADNet architecture combines a point
cloud feature extractor, a CAD sequence reconstructor and a parameter
optimizer. Subsequently, in order to reconstruct the featured CAD model in an
autoregressive way, the CAD sequence reconstructor applies two transformer
decoders, one with target mask and the other without mask. Finally, for
predicting parameters more precisely, we design a parameter optimizer with
cross-attention mechanism to further refine the CAD feature parameters. We
evaluate P2CADNet on the public dataset, and the experimental results show that
P2CADNet has excellent reconstruction quality and accuracy. To our best
knowledge, P2CADNet is the first end-to-end network to reconstruct featured CAD
model from point cloud, and can be regarded as baseline for future works.
Therefore, we open the source code at https://github.com/Blice0415/P2CADNet.
</p></li>
</ul>

<h3>Title: Deformation-Invariant Neural Network and Its Applications in Distorted Image Restoration and Analysis. (arXiv:2310.02641v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02641">http://arxiv.org/abs/2310.02641</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02641]] Deformation-Invariant Neural Network and Its Applications in Distorted Image Restoration and Analysis(http://arxiv.org/abs/2310.02641)</code></li>
<li>Summary: <p>Images degraded by geometric distortions pose a significant challenge to
imaging and computer vision tasks such as object recognition. Deep
learning-based imaging models usually fail to give accurate performance for
geometrically distorted images. In this paper, we propose the
deformation-invariant neural network (DINN), a framework to address the problem
of imaging tasks for geometrically distorted images. The DINN outputs
consistent latent features for images that are geometrically distorted but
represent the same underlying object or scene. The idea of DINN is to
incorporate a simple component, called the quasiconformal transformer network
(QCTN), into other existing deep networks for imaging tasks. The QCTN is a deep
neural network that outputs a quasiconformal map, which can be used to
transform a geometrically distorted image into an improved version that is
closer to the distribution of natural or good images. It first outputs a
Beltrami coefficient, which measures the quasiconformality of the output
deformation map. By controlling the Beltrami coefficient, the local geometric
distortion under the quasiconformal mapping can be controlled. The QCTN is
lightweight and simple, which can be readily integrated into other existing
deep neural networks to enhance their performance. Leveraging our framework, we
have developed an image classification network that achieves accurate
classification of distorted images. Our proposed framework has been applied to
restore geometrically distorted images by atmospheric turbulence and water
turbulence. DINN outperforms existing GAN-based restoration methods under these
scenarios, demonstrating the effectiveness of the proposed framework.
Additionally, we apply our proposed framework to the 1-1 verification of human
face images under atmospheric turbulence and achieve satisfactory performance,
further demonstrating the efficacy of our approach.
</p></li>
</ul>

<h3>Title: GET: Group Event Transformer for Event-Based Vision. (arXiv:2310.02642v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02642">http://arxiv.org/abs/2310.02642</a></li>
<li>Code URL: https://github.com/peterande/get-group-event-transformer</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02642]] GET: Group Event Transformer for Event-Based Vision(http://arxiv.org/abs/2310.02642)</code></li>
<li>Summary: <p>Event cameras are a type of novel neuromorphic sen-sor that has been gaining
increasing attention. Existing event-based backbones mainly rely on image-based
designs to extract spatial information within the image transformed from
events, overlooking important event properties like time and polarity. To
address this issue, we propose a novel Group-based vision Transformer backbone
for Event-based vision, called Group Event Transformer (GET), which de-couples
temporal-polarity information from spatial infor-mation throughout the feature
extraction process. Specifi-cally, we first propose a new event representation
for GET, named Group Token, which groups asynchronous events based on their
timestamps and polarities. Then, GET ap-plies the Event Dual Self-Attention
block, and Group Token Aggregation module to facilitate effective feature
commu-nication and integration in both the spatial and temporal-polarity
domains. After that, GET can be integrated with different downstream tasks by
connecting it with vari-ous heads. We evaluate our method on four event-based
classification datasets (Cifar10-DVS, N-MNIST, N-CARS, and DVS128Gesture) and
two event-based object detection datasets (1Mpx and Gen1), and the results
demonstrate that GET outperforms other state-of-the-art methods. The code is
available at https://github.com/Peterande/GET-Group-Event-Transformer.
</p></li>
</ul>

<h3>Title: Land-cover change detection using paired OpenStreetMap data and optical high-resolution imagery via object-guided Transformer. (arXiv:2310.02674v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02674">http://arxiv.org/abs/2310.02674</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02674]] Land-cover change detection using paired OpenStreetMap data and optical high-resolution imagery via object-guided Transformer(http://arxiv.org/abs/2310.02674)</code></li>
<li>Summary: <p>Optical high-resolution imagery and OpenStreetMap (OSM) data are two
important data sources for land-cover change detection. Previous studies in
these two data sources focus on utilizing the information in OSM data to aid
the change detection on multi-temporal optical high-resolution images. This
paper pioneers the direct detection of land-cover changes utilizing paired OSM
data and optical imagery, thereby broadening the horizons of change detection
tasks to encompass more dynamic earth observations. To this end, we propose an
object-guided Transformer (ObjFormer) architecture by naturally combining the
prevalent object-based image analysis (OBIA) technique with the advanced vision
Transformer architecture. The introduction of OBIA can significantly reduce the
computational overhead and memory burden in the self-attention module.
Specifically, the proposed ObjFormer has a hierarchical pseudo-siamese encoder
consisting of object-guided self-attention modules that extract representative
features of different levels from OSM data and optical images; a decoder
consisting of object-guided cross-attention modules can progressively recover
the land-cover changes from the extracted heterogeneous features. In addition
to the basic supervised binary change detection task, this paper raises a new
semi-supervised semantic change detection task that does not require any
manually annotated land-cover labels of optical images to train semantic change
detectors. Two lightweight semantic decoders are added to ObjFormer to
accomplish this task efficiently. A converse cross-entropy loss is designed to
fully utilize the negative samples, thereby contributing to the great
performance improvement in this task. The first large-scale benchmark dataset
containing 1,287 map-image pairs (1024$\times$ 1024 pixels for each sample)
covering 40 regions on six continents ...(see the manuscript for the full
abstract)
</p></li>
</ul>

<h3>Title: MUNCH: Modelling Unique 'N Controllable Heads. (arXiv:2310.02753v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02753">http://arxiv.org/abs/2310.02753</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02753]] MUNCH: Modelling Unique 'N Controllable Heads(http://arxiv.org/abs/2310.02753)</code></li>
<li>Summary: <p>The automated generation of 3D human heads has been an intriguing and
challenging task for computer vision researchers. Prevailing methods synthesize
realistic avatars but with limited control over the diversity and quality of
rendered outputs and suffer from limited correlation between shape and texture
of the character. We propose a method that offers quality, diversity, control,
and realism along with explainable network design, all desirable features to
game-design artists in the domain. First, our proposed Geometry Generator
identifies disentangled latent directions and generate novel and diverse
samples. A Render Map Generator then learns to synthesize multiply high-fidelty
physically-based render maps including Albedo, Glossiness, Specular, and
Normals. For artists preferring fine-grained control over the output, we
introduce a novel Color Transformer Model that allows semantic color control
over generated maps. We also introduce quantifiable metrics called Uniqueness
and Novelty and a combined metric to test the overall performance of our model.
Demo for both shapes and textures can be found:
https://munch-seven.vercel.app/. We will release our model along with the
synthetic dataset.
</p></li>
</ul>

<h3>Title: Delving into CLIP latent space for Video Anomaly Recognition. (arXiv:2310.02835v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02835">http://arxiv.org/abs/2310.02835</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02835]] Delving into CLIP latent space for Video Anomaly Recognition(http://arxiv.org/abs/2310.02835)</code></li>
<li>Summary: <p>We tackle the complex problem of detecting and recognising anomalies in
surveillance videos at the frame level, utilising only video-level supervision.
We introduce the novel method AnomalyCLIP, the first to combine Large Language
and Vision (LLV) models, such as CLIP, with multiple instance learning for
joint video anomaly detection and classification. Our approach specifically
involves manipulating the latent CLIP feature space to identify the normal
event subspace, which in turn allows us to effectively learn text-driven
directions for abnormal events. When anomalous frames are projected onto these
directions, they exhibit a large feature magnitude if they belong to a
particular class. We also introduce a computationally efficient Transformer
architecture to model short- and long-term temporal dependencies between
frames, ultimately producing the final anomaly score and class prediction
probabilities. We compare AnomalyCLIP against state-of-the-art methods
considering three major anomaly detection benchmarks, i.e. ShanghaiTech,
UCF-Crime, and XD-Violence, and empirically show that it outperforms baselines
in recognising video anomalies.
</p></li>
</ul>

<h3>Title: Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models. (arXiv:2310.02409v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02409">http://arxiv.org/abs/2310.02409</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02409]] Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models(http://arxiv.org/abs/2310.02409)</code></li>
<li>Summary: <p>Standard Transformer-based language models (LMs) scale poorly to long
contexts. We propose a solution based on dynamic contextual compression, which
extends the Nugget approach of Qin &amp; Van Durme (2023) from BERT-like frameworks
to decoder-only LMs. Our method models history as compressed "nuggets" which
are trained to allow for reconstruction, and it can be initialized with
off-the-shelf models such as LLaMA. We demonstrate through experiments in
language modeling, question answering, and summarization that Nugget2D retains
capabilities in these tasks, while drastically reducing the overhead during
decoding in terms of time and space. For example, in the experiments of
autoencoding, Nugget2D can shrink context at a 20x compression ratio with a
BLEU score of 98% for reconstruction, achieving nearly lossless encoding.
</p></li>
</ul>

<h3>Title: ResidualTransformer: Residual Low-rank Learning with Weight-sharing for Transformer Layers. (arXiv:2310.02489v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02489">http://arxiv.org/abs/2310.02489</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02489]] ResidualTransformer: Residual Low-rank Learning with Weight-sharing for Transformer Layers(http://arxiv.org/abs/2310.02489)</code></li>
<li>Summary: <p>Memory constraint of always-on devices is one of the major concerns when
deploying speech processing models on these devices. While larger models
trained with sufficiently large amount of data generally perform better, making
them fit in the device memory is a demanding challenge. In this paper, we aim
to reduce model size by reparameterizing model weights across Transformer
encoder layers and assuming a special weight composition and structure. More
specifically, inspired by ResNet and the more recent LoRA work, we propose an
approach named ResidualTransformer, where each weight matrix in a Transformer
layer comprises 1) a shared full-rank component with its adjacent layers, and
2) a unique low-rank component to itself. The low-rank matrices only account
for a small amount of model size increase. In addition, we add diagonal weight
matrices to improve modeling capacity of the low-rank matrices. Experiments of
our 10k-hour speech recognition and speech translation tasks show that the
Transformer encoder size can be reduced by ~3X with very slight performance
degradation.
</p></li>
</ul>

<h3>Title: Low Resource Summarization using Pre-trained Language Models. (arXiv:2310.02790v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02790">http://arxiv.org/abs/2310.02790</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02790]] Low Resource Summarization using Pre-trained Language Models(http://arxiv.org/abs/2310.02790)</code></li>
<li>Summary: <p>With the advent of Deep Learning based Artificial Neural Networks models,
Natural Language Processing (NLP) has witnessed significant improvements in
textual data processing in terms of its efficiency and accuracy. However, the
research is mostly restricted to high-resource languages such as English and
low-resource languages still suffer from a lack of available resources in terms
of training datasets as well as models with even baseline evaluation results.
Considering the limited availability of resources for low-resource languages,
we propose a methodology for adapting self-attentive transformer-based
architecture models (mBERT, mT5) for low-resource summarization, supplemented
by the construction of a new baseline dataset (76.5k article, summary pairs) in
a low-resource language Urdu. Choosing news (a publicly available source) as
the application domain has the potential to make the proposed methodology
useful for reproducing in other languages with limited resources. Our adapted
summarization model \textit{urT5} with up to 44.78\% reduction in size as
compared to \textit{mT5} can capture contextual information of low resource
language effectively with evaluation score (up to 46.35 ROUGE-1, 77 BERTScore)
at par with state-of-the-art models in high resource language English
\textit{(PEGASUS: 47.21, BART: 45.14 on XSUM Dataset)}. The proposed method
provided a baseline approach towards extractive as well as abstractive
summarization with competitive evaluation results in a limited resource setup.
</p></li>
</ul>

<h3>Title: Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers. (arXiv:2310.02905v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02905">http://arxiv.org/abs/2310.02905</a></li>
<li>Code URL: https://github.com/xqlin98/INSTINCT</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02905]] Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers(http://arxiv.org/abs/2310.02905)</code></li>
<li>Summary: <p>Large language models (LLMs) have shown remarkable instruction-following
capabilities and achieved impressive performances in various applications.
However, the performances of LLMs depend heavily on the instructions given to
them, which are typically manually tuned with substantial human efforts. Recent
work has used the query-efficient Bayesian optimization (BO) algorithm to
automatically optimize the instructions given to black-box LLMs. However, BO
usually falls short when optimizing highly sophisticated (e.g.,
high-dimensional) objective functions, such as the functions mapping an
instruction to the performance of an LLM. This is mainly due to the limited
expressive power of the Gaussian process (GP) model which is used by BO as a
surrogate to model the objective function. Meanwhile, it has been repeatedly
shown that neural networks (NNs), especially pre-trained transformers, possess
strong expressive power and can model highly complex functions. So, we adopt a
neural bandit algorithm which replaces the GP in BO by an NN surrogate to
optimize instructions for black-box LLMs. More importantly, the neural bandit
algorithm allows us to naturally couple the NN surrogate with the hidden
representation learned by a pre-trained transformer (i.e., an open-source LLM),
which significantly boosts its performance. These motivate us to propose our
INSTruction optimization usIng Neural bandits Coupled with Transformers}
(INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use
extensive experiments to show that our INSTINCT consistently outperforms the
existing methods in different tasks, such as in various instruction induction
tasks and the task of improving the zero-shot chain-of-thought instruction.
</p></li>
</ul>

<h3>Title: Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions. (arXiv:2310.03016v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03016">http://arxiv.org/abs/2310.03016</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03016]] Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions(http://arxiv.org/abs/2310.03016)</code></li>
<li>Summary: <p>In order to understand the in-context learning phenomenon, recent works have
adopted a stylized experimental framework and demonstrated that Transformers
can learn gradient-based learning algorithms for various classes of real-valued
functions. However, the limitations of Transformers in implementing learning
algorithms, and their ability to learn other forms of algorithms are not well
understood. Additionally, the degree to which these capabilities are confined
to attention-based models is unclear. Furthermore, it remains to be seen
whether the insights derived from these stylized settings can be extrapolated
to pretrained Large Language Models (LLMs). In this work, we take a step
towards answering these questions by demonstrating the following: (a) On a
test-bed with a variety of Boolean function classes, we find that Transformers
can nearly match the optimal learning algorithm for 'simpler' tasks, while
their performance deteriorates on more 'complex' tasks. Additionally, we find
that certain attention-free models perform (almost) identically to Transformers
on a range of tasks. (b) When provided a teaching sequence, i.e. a set of
examples that uniquely identifies a function in a class, we show that
Transformers learn more sample-efficiently. Interestingly, our results show
that Transformers can learn to implement two distinct algorithms to solve a
single task, and can adaptively select the more sample-efficient algorithm
depending on the sequence of in-context examples. (c) Lastly, we show that
extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines
on prediction tasks that are guaranteed to not be in their training set.
</p></li>
</ul>

<h3>Title: PCGPT: Procedural Content Generation via Transformers. (arXiv:2310.02405v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02405">http://arxiv.org/abs/2310.02405</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02405]] PCGPT: Procedural Content Generation via Transformers(http://arxiv.org/abs/2310.02405)</code></li>
<li>Summary: <p>The paper presents the PCGPT framework, an innovative approach to procedural
content generation (PCG) using offline reinforcement learning and transformer
networks. PCGPT utilizes an autoregressive model based on transformers to
generate game levels iteratively, addressing the challenges of traditional PCG
methods such as repetitive, predictable, or inconsistent content. The framework
models trajectories of actions, states, and rewards, leveraging the
transformer's self-attention mechanism to capture temporal dependencies and
causal relationships. The approach is evaluated in the Sokoban puzzle game,
where the model predicts items that are needed with their corresponding
locations. Experimental results on the game Sokoban demonstrate that PCGPT
generates more complex and diverse game content. Interestingly, it achieves
these results in significantly fewer steps compared to existing methods,
showcasing its potential for enhancing game design and online content
generation. Our model represents a new PCG paradigm which outperforms previous
methods.
</p></li>
</ul>

<h3>Title: EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations. (arXiv:2310.02428v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02428">http://arxiv.org/abs/2310.02428</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02428]] EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations(http://arxiv.org/abs/2310.02428)</code></li>
<li>Summary: <p>Equivariant graph neural networks force fields (EGraFFs) have shown great
promise in modelling complex interactions in atomic systems by exploiting the
graphs' inherent symmetries. Recent works have led to a surge in the
development of novel architectures that incorporate equivariance-based
inductive biases alongside architectural innovations like graph transformers
and message passing to model atomic interactions. However, thorough evaluations
of these deploying EGraFFs for the downstream task of real-world atomistic
simulations, is lacking. To this end, here we perform a systematic benchmarking
of 6 EGraFF algorithms (NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet),
with the aim of understanding their capabilities and limitations for realistic
atomistic simulations. In addition to our thorough evaluation and analysis on
eight existing datasets based on the benchmarking literature, we release two
new benchmark datasets, propose four new metrics, and three new challenging
tasks. The new datasets and tasks evaluate the performance of EGraFF to
out-of-distribution data, in terms of different crystal structures,
temperatures, and new molecules. Interestingly, evaluation of the EGraFF models
based on dynamic simulations reveals that having a lower error on energy or
force does not guarantee stable or reliable simulation or faithful replication
of the atomic structures. Moreover, we find that no model clearly outperforms
other models on all datasets and tasks. Importantly, we show that the
performance of all the models on out-of-distribution datasets is unreliable,
pointing to the need for the development of a foundation model for force fields
that can be used in real-world simulations. In summary, this work establishes a
rigorous framework for evaluating machine learning force fields in the context
of atomic simulations and points to open research challenges within this
domain.
</p></li>
</ul>

<h3>Title: On the Stability of Expressive Positional Encodings for Graph Neural Networks. (arXiv:2310.02579v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02579">http://arxiv.org/abs/2310.02579</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02579]] On the Stability of Expressive Positional Encodings for Graph Neural Networks(http://arxiv.org/abs/2310.02579)</code></li>
<li>Summary: <p>Designing effective positional encodings for graphs is key to building
powerful graph transformers and enhancing message-passing graph neural
networks. Although widespread, using Laplacian eigenvectors as positional
encodings faces two fundamental challenges: (1) \emph{Non-uniqueness}: there
are many different eigendecompositions of the same Laplacian, and (2)
\emph{Instability}: small perturbations to the Laplacian could result in
completely different eigenspaces, leading to unpredictable changes in
positional encoding.
</p>
<p>Despite many attempts to address non-uniqueness, most methods overlook
stability, leading to poor generalization on unseen graph structures. We
identify the cause of instability to be a "hard partition" of eigenspaces.
Hence, we introduce Stable and Expressive Positional Encodings (SPE), an
architecture for processing eigenvectors that uses eigenvalues to "softly
partition" eigenspaces. SPE is the first architecture that is (1) provably
stable, and (2) universally expressive for basis invariant functions whilst
respecting all symmetries of eigenvectors. Besides guaranteed stability, we
prove that SPE is at least as expressive as existing methods, and highly
capable of counting graph structures. Finally, we evaluate the effectiveness of
our method on molecular property prediction, and out-of-distribution
generalization tasks, finding improved generalization compared to existing
positional encoding methods.
</p></li>
</ul>

<h3>Title: A Study of Quantisation-aware Training on Time Series Transformer Models for Resource-constrained FPGAs. (arXiv:2310.02654v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02654">http://arxiv.org/abs/2310.02654</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02654]] A Study of Quantisation-aware Training on Time Series Transformer Models for Resource-constrained FPGAs(http://arxiv.org/abs/2310.02654)</code></li>
<li>Summary: <p>This study explores the quantisation-aware training (QAT) on time series
Transformer models. We propose a novel adaptive quantisation scheme that
dynamically selects between symmetric and asymmetric schemes during the QAT
phase. Our approach demonstrates that matching the quantisation scheme to the
real data distribution can reduce computational overhead while maintaining
acceptable precision. Moreover, our approach is robust when applied to
real-world data and mixed-precision quantisation, where most objects are
quantised to 4 bits. Our findings inform model quantisation and deployment
decisions while providing a foundation for advancing quantisation techniques.
</p></li>
</ul>

<h3>Title: scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in Brain. (arXiv:2310.02713v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02713">http://arxiv.org/abs/2310.02713</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02713]] scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in Brain(http://arxiv.org/abs/2310.02713)</code></li>
<li>Summary: <p>Single-cell RNA sequencing (scRNA-seq) has made significant strides in
unraveling the intricate cellular diversity within complex tissues. This is
particularly critical in the brain, presenting a greater diversity of cell
types than other tissue types, to gain a deeper understanding of brain function
within various cellular contexts. However, analyzing scRNA-seq data remains a
challenge due to inherent measurement noise stemming from dropout events and
the limited utilization of extensive gene expression information. In this work,
we introduce scHyena, a foundation model designed to address these challenges
and enhance the accuracy of scRNA-seq analysis in the brain. Specifically,
inspired by the recent Hyena operator, we design a novel Transformer
architecture called singe-cell Hyena (scHyena) that is equipped with a linear
adaptor layer, the positional encoding via gene-embedding, and a
{bidirectional} Hyena operator. This enables us to process full-length
scRNA-seq data without losing any information from the raw data. In particular,
our model learns generalizable features of cells and genes through pre-training
scHyena using the full length of scRNA-seq data. We demonstrate the superior
performance of scHyena compared to other benchmark methods in downstream tasks,
including cell type classification and scRNA-seq imputation.
</p></li>
</ul>

<h3>Title: SALSA: Semantically-Aware Latent Space Autoencoder. (arXiv:2310.02744v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02744">http://arxiv.org/abs/2310.02744</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02744]] SALSA: Semantically-Aware Latent Space Autoencoder(http://arxiv.org/abs/2310.02744)</code></li>
<li>Summary: <p>In deep learning for drug discovery, chemical data are often represented as
simplified molecular-input line-entry system (SMILES) sequences which allow for
straightforward implementation of natural language processing methodologies,
one being the sequence-to-sequence autoencoder. However, we observe that
training an autoencoder solely on SMILES is insufficient to learn molecular
representations that are semantically meaningful, where semantics are defined
by the structural (graph-to-graph) similarities between molecules. We
demonstrate by example that autoencoders may map structurally similar molecules
to distant codes, resulting in an incoherent latent space that does not respect
the structural similarities between molecules. To address this shortcoming we
propose Semantically-Aware Latent Space Autoencoder (SALSA), a
transformer-autoencoder modified with a contrastive task, tailored specifically
to learn graph-to-graph similarity between molecules. Formally, the contrastive
objective is to map structurally similar molecules (separated by a single graph
edit) to nearby codes in the latent space. To accomplish this, we generate a
novel dataset comprised of sets of structurally similar molecules and opt for a
supervised contrastive loss that is able to incorporate full sets of positive
samples. We compare SALSA to its ablated counterparts, and show empirically
that the composed training objective (reconstruction and contrastive task)
leads to a higher quality latent space that is more 1) structurally-aware, 2)
semantically continuous, and 3) property-aware.
</p></li>
</ul>

<h3>Title: Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness. (arXiv:2310.02832v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02832">http://arxiv.org/abs/2310.02832</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02832]] Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness(http://arxiv.org/abs/2310.02832)</code></li>
<li>Summary: <p>Effective OOD detection is crucial for reliable machine learning models, yet
most current methods are limited in practical use due to requirements like
access to training data or intervention in training. We present a novel method
for detecting OOD data in deep neural networks based on transformation
smoothness between intermediate layers of a network (BLOOD), which is
applicable to pre-trained models without access to training data. BLOOD
utilizes the tendency of between-layer representation transformations of
in-distribution (ID) data to be smoother than the corresponding transformations
of OOD data, a property that we also demonstrate empirically for Transformer
networks. We evaluate BLOOD on several text classification tasks with
Transformer networks and demonstrate that it outperforms methods with
comparable resource requirements. Our analysis also suggests that when learning
simpler tasks, OOD data transformations maintain their original sharpness,
whereas sharpness increases with more complex tasks.
</p></li>
</ul>

<h3>Title: Searching for High-Value Molecules Using Reinforcement Learning and Transformers. (arXiv:2310.02902v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02902">http://arxiv.org/abs/2310.02902</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02902]] Searching for High-Value Molecules Using Reinforcement Learning and Transformers(http://arxiv.org/abs/2310.02902)</code></li>
<li>Summary: <p>Reinforcement learning (RL) over text representations can be effective for
finding high-value policies that can search over graphs. However, RL requires
careful structuring of the search space and algorithm design to be effective in
this challenge. Through extensive experiments, we explore how different design
choices for text grammar and algorithmic choices for training can affect an RL
policy's ability to generate molecules with desired properties. We arrive at a
new RL-based molecular design algorithm (ChemRLformer) and perform a thorough
analysis using 25 molecule design tasks, including computationally complex
protein docking simulations. From this analysis, we discover unique insights in
this problem space and show that ChemRLformer achieves state-of-the-art
performance while being more straightforward than prior work by demystifying
which design choices are actually helpful for text-based molecule design.
</p></li>
</ul>

<h3>Title: Multiple Physics Pretraining for Physical Surrogate Models. (arXiv:2310.02994v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02994">http://arxiv.org/abs/2310.02994</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02994]] Multiple Physics Pretraining for Physical Surrogate Models(http://arxiv.org/abs/2310.02994)</code></li>
<li>Summary: <p>We introduce multiple physics pretraining (MPP), an autoregressive
task-agnostic pretraining approach for physical surrogate modeling. MPP
involves training large surrogate models to predict the dynamics of multiple
heterogeneous physical systems simultaneously by learning features that are
broadly useful across diverse physical tasks. In order to learn effectively in
this setting, we introduce a shared embedding and normalization strategy that
projects the fields of multiple systems into a single shared embedding space.
We validate the efficacy of our approach on both pretraining and downstream
tasks over a broad fluid mechanics-oriented benchmark. We show that a single
MPP-pretrained transformer is able to match or outperform task-specific
baselines on all pretraining sub-tasks without the need for finetuning. For
downstream tasks, we demonstrate that finetuning MPP-trained models results in
more accurate predictions across multiple time-steps on new physics compared to
training from scratch or finetuning pretrained video foundation models. We
open-source our code and model weights trained at multiple scales for
reproducibility and community experimentation.
</p></li>
</ul>

<h3>Title: Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making. (arXiv:2310.03022v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03022">http://arxiv.org/abs/2310.03022</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03022]] Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making(http://arxiv.org/abs/2310.03022)</code></li>
<li>Summary: <p>The recent success of Transformer in natural language processing has sparked
its use in various domains. In offline reinforcement learning (RL), Decision
Transformer (DT) is emerging as a promising model based on Transformer.
However, we discovered that the attention module of DT is not appropriate to
capture the inherent local dependence pattern in trajectories of RL modeled as
a Markov decision process. To overcome the limitations of DT, we propose a
novel action sequence predictor, named Decision ConvFormer (DC), based on the
architecture of MetaFormer, which is a general structure to process multiple
entities in parallel and understand the interrelationship among the multiple
entities. DC employs local convolution filtering as the token mixer and can
effectively capture the inherent local associations of the RL dataset. In
extensive experiments, DC achieved state-of-the-art performance across various
standard RL benchmarks while requiring fewer resources. Furthermore, we show
that DC better understands the underlying meaning in data and exhibits enhanced
generalization capability.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Analyzing and Improving OT-based Adversarial Networks. (arXiv:2310.02611v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02611">http://arxiv.org/abs/2310.02611</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02611]] Analyzing and Improving OT-based Adversarial Networks(http://arxiv.org/abs/2310.02611)</code></li>
<li>Summary: <p>Optimal Transport (OT) problem aims to find a transport plan that bridges two
distributions while minimizing a given cost function. OT theory has been widely
utilized in generative modeling. In the beginning, OT distance has been used as
a measure for assessing the distance between data and generated distributions.
Recently, OT transport map between data and prior distributions has been
utilized as a generative model. These OT-based generative models share a
similar adversarial training objective. In this paper, we begin by unifying
these OT-based adversarial methods within a single framework. Then, we
elucidate the role of each component in training dynamics through a
comprehensive analysis of this unified framework. Moreover, we suggest a simple
but novel method that improves the previously best-performing OT-based model.
Intuitively, our approach conducts a gradual refinement of the generated
distribution, progressively aligning it with the data distribution. Our
approach achieves a FID score of 2.51 on CIFAR-10, outperforming unified
OT-based adversarial approaches.
</p></li>
</ul>

<h3>Title: GETAvatar: Generative Textured Meshes for Animatable Human Avatars. (arXiv:2310.02714v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02714">http://arxiv.org/abs/2310.02714</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02714]] GETAvatar: Generative Textured Meshes for Animatable Human Avatars(http://arxiv.org/abs/2310.02714)</code></li>
<li>Summary: <p>We study the problem of 3D-aware full-body human generation, aiming at
creating animatable human avatars with high-quality textures and geometries.
Generally, two challenges remain in this field: i) existing methods struggle to
generate geometries with rich realistic details such as the wrinkles of
garments; ii) they typically utilize volumetric radiance fields and neural
renderers in the synthesis process, making high-resolution rendering
non-trivial. To overcome these problems, we propose GETAvatar, a Generative
model that directly generates Explicit Textured 3D meshes for animatable human
Avatar, with photo-realistic appearance and fine geometric details.
Specifically, we first design an articulated 3D human representation with
explicit surface modeling, and enrich the generated humans with realistic
surface details by learning from the 2D normal maps of 3D scan data. Second,
with the explicit mesh representation, we can use a rasterization-based
renderer to perform surface rendering, allowing us to achieve high-resolution
image generation efficiently. Extensive experiments demonstrate that GETAvatar
achieves state-of-the-art performance on 3D-aware human generation both in
appearance and geometry quality. Notably, GETAvatar can generate images at
512x512 resolution with 17FPS and 1024x1024 resolution with 14FPS, improving
upon previous methods by 2x. Our code and models will be available.
</p></li>
</ul>

<h3>Title: Delta-AI: Local objectives for amortized inference in sparse graphical models. (arXiv:2310.02423v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02423">http://arxiv.org/abs/2310.02423</a></li>
<li>Code URL: https://github.com/gfnorg/delta-ai</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02423]] Delta-AI: Local objectives for amortized inference in sparse graphical models(http://arxiv.org/abs/2310.02423)</code></li>
<li>Summary: <p>We present a new algorithm for amortized inference in sparse probabilistic
graphical models (PGMs), which we call $\Delta$-amortized inference
($\Delta$-AI). Our approach is based on the observation that when the sampling
of variables in a PGM is seen as a sequence of actions taken by an agent,
sparsity of the PGM enables local credit assignment in the agent's policy
learning objective. This yields a local constraint that can be turned into a
local loss in the style of generative flow networks (GFlowNets) that enables
off-policy training but avoids the need to instantiate all the random variables
for each parameter update, thus speeding up training considerably. The
$\Delta$-AI objective matches the conditional distribution of a variable given
its Markov blanket in a tractable learned sampler, which has the structure of a
Bayesian network, with the same conditional distribution under the target PGM.
As such, the trained sampler recovers marginals and conditional distributions
of interest and enables inference of partial subsets of variables. We
illustrate $\Delta$-AI's effectiveness for sampling from synthetic PGMs and
training latent variable models with sparse factor structure.
</p></li>
</ul>

<h3>Title: GenCO: Generating Diverse Solutions to Design Problems with Combinatorial Nature. (arXiv:2310.02442v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02442">http://arxiv.org/abs/2310.02442</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02442]] GenCO: Generating Diverse Solutions to Design Problems with Combinatorial Nature(http://arxiv.org/abs/2310.02442)</code></li>
<li>Summary: <p>Generating diverse objects (e.g., images) using generative models (such as
GAN or VAE) has achieved impressive results in the recent years, to help solve
many design problems that are traditionally done by humans. Going beyond image
generation, we aim to find solutions to more general design problems, in which
both the diversity of the design and conformity of constraints are important.
Such a setting has applications in computer graphics, animation, industrial
design, material science, etc, in which we may want the output of the generator
to follow discrete/combinatorial constraints and penalize any deviation, which
is non-trivial with existing generative models and optimization solvers. To
address this, we propose GenCO, a novel framework that conducts end-to-end
training of deep generative models integrated with embedded combinatorial
solvers, aiming to uncover high-quality solutions aligned with nonlinear
objectives. While structurally akin to conventional generative models, GenCO
diverges in its role - it focuses on generating instances of combinatorial
optimization problems rather than final objects (e.g., images). This shift
allows finer control over the generated outputs, enabling assessments of their
feasibility and introducing an additional combinatorial loss component. We
demonstrate the effectiveness of our approach on a variety of generative tasks
characterized by combinatorial intricacies, including game level generation and
map creation for path planning, consistently demonstrating its capability to
yield diverse, high-quality solutions that reliably adhere to user-specified
combinatorial properties.
</p></li>
</ul>

<h3>Title: Dual-stage Flows-based Generative Modeling for Traceable Urban Planning. (arXiv:2310.02453v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02453">http://arxiv.org/abs/2310.02453</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02453]] Dual-stage Flows-based Generative Modeling for Traceable Urban Planning(http://arxiv.org/abs/2310.02453)</code></li>
<li>Summary: <p>Urban planning, which aims to design feasible land-use configurations for
target areas, has become increasingly essential due to the high-speed
urbanization process in the modern era. However, the traditional urban planning
conducted by human designers can be a complex and onerous task. Thanks to the
advancement of deep learning algorithms, researchers have started to develop
automated planning techniques. While these models have exhibited promising
results, they still grapple with a couple of unresolved limitations: 1)
Ignoring the relationship between urban functional zones and configurations and
failing to capture the relationship among different functional zones. 2) Less
interpretable and stable generation process. To overcome these limitations, we
propose a novel generative framework based on normalizing flows, namely
Dual-stage Urban Flows (DSUF) framework. Specifically, the first stage is to
utilize zone-level urban planning flows to generate urban functional zones
based on given surrounding contexts and human guidance. Then we employ an
Information Fusion Module to capture the relationship among functional zones
and fuse the information of different aspects. The second stage is to use
configuration-level urban planning flows to obtain land-use configurations
derived from fused information. We design several experiments to indicate that
our framework can outperform compared to other generative models for the urban
planning task.
</p></li>
</ul>

<h3>Title: Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs. (arXiv:2310.02619v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02619">http://arxiv.org/abs/2310.02619</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02619]] Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs(http://arxiv.org/abs/2310.02619)</code></li>
<li>Summary: <p>Generating realistic time series data is important for many engineering and
scientific applications. Existing work tackles this problem using generative
adversarial networks (GANs). However, GANs are often unstable during training,
and they can suffer from mode collapse. While variational autoencoders (VAEs)
are known to be more robust to these issues, they are (surprisingly) less often
considered for time series generation. In this work, we introduce Koopman VAE
(KVAE), a new generative framework that is based on a novel design for the
model prior, and that can be optimized for either regular and irregular
training data. Inspired by Koopman theory, we represent the latent conditional
prior dynamics using a linear map. Our approach enhances generative modeling
with two desired features: (i) incorporating domain knowledge can be achieved
by leverageing spectral tools that prescribe constraints on the eigenvalues of
the linear map; and (ii) studying the qualitative behavior and stablity of the
system can be performed using tools from dynamical systems theory. Our results
show that KVAE outperforms state-of-the-art GAN and VAE methods across several
challenging synthetic and real-world time series generation benchmarks. Whether
trained on regular or irregular data, KVAE generates time series that improve
both discriminative and predictive metrics. We also present visual evidence
suggesting that KVAE learns probability density functions that better
approximate empirical ground truth distributions.
</p></li>
</ul>

<h3>Title: Local Search GFlowNets. (arXiv:2310.02710v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02710">http://arxiv.org/abs/2310.02710</a></li>
<li>Code URL: https://github.com/dbsxodud-11/ls_gfn</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02710]] Local Search GFlowNets(http://arxiv.org/abs/2310.02710)</code></li>
<li>Summary: <p>Generative Flow Networks (GFlowNets) are amortized sampling methods that
learn a distribution over discrete objects proportional to their rewards.
GFlowNets exhibit a remarkable ability to generate diverse samples, yet
occasionally struggle to consistently produce samples with high rewards due to
over-exploration on wide sample space. This paper proposes to train GFlowNets
with local search which focuses on exploiting high rewarded sample space to
resolve this issue. Our main idea is to explore the local neighborhood via
destruction and reconstruction guided by backward and forward policies,
respectively. This allows biasing the samples toward high-reward solutions,
which is not possible for a typical GFlowNet solution generation scheme which
uses the forward policy to generate the solution from scratch. Extensive
experiments demonstrate a remarkable performance improvement in several
biochemical tasks. Source code is available:
\url{https://github.com/dbsxodud-11/ls_gfn}.
</p></li>
</ul>

<h3>Title: Expected flow networks in stochastic environments and two-player zero-sum games. (arXiv:2310.02779v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02779">http://arxiv.org/abs/2310.02779</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02779]] Expected flow networks in stochastic environments and two-player zero-sum games(http://arxiv.org/abs/2310.02779)</code></li>
<li>Summary: <p>Generative flow networks (GFlowNets) are sequential sampling models trained
to match a given distribution. GFlowNets have been successfully applied to
various structured object generation tasks, sampling a diverse set of
high-reward objects quickly. We propose expected flow networks (EFlowNets),
which extend GFlowNets to stochastic environments. We show that EFlowNets
outperform other GFlowNet formulations in stochastic tasks such as protein
design. We then extend the concept of EFlowNets to adversarial environments,
proposing adversarial flow networks (AFlowNets) for two-player zero-sum games.
We show that AFlowNets learn to find above 80% of optimal moves in Connect-4
via self-play and outperform AlphaZero in tournaments.
</p></li>
</ul>

<h3>Title: A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability. (arXiv:2310.02807v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02807">http://arxiv.org/abs/2310.02807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02807]] A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability(http://arxiv.org/abs/2310.02807)</code></li>
<li>Summary: <p>In the past few years, there has been an explosive surge in the use of
machine learning (ML) techniques to address combinatorial optimization (CO)
problems, especially mixed-integer linear programs (MILPs). Despite the
achievements, the limited availability of real-world instances often leads to
sub-optimal decisions and biased solver assessments, which motivates a suite of
synthetic MILP instance generation techniques. However, existing methods either
rely heavily on expert-designed formulations or struggle to capture the rich
features of real-world instances. To tackle this problem, we propose G2MILP,
which to the best of our knowledge is the first deep generative framework for
MILP instances. Specifically, G2MILP represents MILP instances as bipartite
graphs, and applies a masked variational autoencoder to iteratively corrupt and
replace parts of the original graphs to generate new ones. The appealing
feature of G2MILP is that it can learn to generate novel and realistic MILP
instances without prior expert-designed formulations, while preserving the
structures and computational hardness of real-world datasets, simultaneously.
Thus the generated instances can facilitate downstream tasks for enhancing MILP
solvers under limited data availability. We design a suite of benchmarks to
evaluate the quality of the generated MILP instances. Experiments demonstrate
that our method can produce instances that closely resemble real-world datasets
in terms of both structures and computational hardness.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: NOLA: Networks as Linear Combination of Low Rank Random Basis. (arXiv:2310.02556v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02556">http://arxiv.org/abs/2310.02556</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02556]] NOLA: Networks as Linear Combination of Low Rank Random Basis(http://arxiv.org/abs/2310.02556)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have recently gained popularity due to their
impressive few-shot performance across various downstream tasks. However,
fine-tuning all parameters and storing a unique model for each downstream task
or domain becomes impractical because of the massive size of checkpoints (e.g.,
350GB in GPT-3). Current literature, such as LoRA, showcases the potential of
low-rank modifications to the original weights of an LLM, enabling efficient
adaptation and storage for task-specific models. These methods can reduce the
number of parameters needed to fine-tune an LLM by several orders of magnitude.
Yet, these methods face two primary limitations: 1) the parameter reduction is
lower-bounded by the rank one decomposition, and 2) the extent of reduction is
heavily influenced by both the model architecture and the chosen rank. For
instance, in larger models, even a rank one decomposition might exceed the
number of parameters truly needed for adaptation. In this paper, we introduce
NOLA, which overcomes the rank one lower bound present in LoRA. It achieves
this by re-parameterizing the low-rank matrices in LoRA using linear
combinations of randomly generated matrices (basis) and optimizing the linear
mixture coefficients only. This approach allows us to decouple the number of
trainable parameters from both the choice of rank and the network architecture.
We present adaptation results using GPT-2 and ViT in natural language and
computer vision tasks. NOLA performs as well as, or better than models with
equivalent parameter counts. Furthermore, we demonstrate that we can halve the
parameters in larger models compared to LoRA with rank one, without sacrificing
performance.
</p></li>
</ul>

<h3>Title: Improving Automatic VQA Evaluation Using Large Language Models. (arXiv:2310.02567v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02567">http://arxiv.org/abs/2310.02567</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02567]] Improving Automatic VQA Evaluation Using Large Language Models(http://arxiv.org/abs/2310.02567)</code></li>
<li>Summary: <p>8 years after the visual question answering (VQA) task was proposed, accuracy
remains the primary metric for automatic evaluation. VQA Accuracy has been
effective so far in the IID evaluation setting. However, our community is
undergoing a shift towards open-ended generative models and OOD evaluation. In
this new paradigm, the existing VQA Accuracy metric is overly stringent and
underestimates the performance of VQA systems. Thus, there is a need to develop
more robust automatic VQA metrics that serve as a proxy for human judgment. In
this work, we propose to leverage the in-context learning capabilities of
instruction-tuned large language models (LLMs) to build a better VQA metric. We
formulate VQA evaluation as an answer-rating task where the LLM is instructed
to score the accuracy of a candidate answer given a set of reference answers.
We demonstrate the proposed metric better correlates with human judgment
compared to existing metrics across several VQA models and benchmarks. We hope
wide adoption of our metric will contribute to better estimating the research
progress on the VQA task.
</p></li>
</ul>

<h3>Title: Kosmos-G: Generating Images in Context with Multimodal Large Language Models. (arXiv:2310.02992v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02992">http://arxiv.org/abs/2310.02992</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02992]] Kosmos-G: Generating Images in Context with Multimodal Large Language Models(http://arxiv.org/abs/2310.02992)</code></li>
<li>Summary: <p>Recent advancements in text-to-image (T2I) and vision-language-to-image
(VL2I) generation have made significant strides. However, the generation from
generalized vision-language inputs, especially involving multiple images,
remains under-explored. This paper presents Kosmos-G, a model that leverages
the advanced perception capabilities of Multimodal Large Language Models
(MLLMs) to tackle the aforementioned challenge. Our approach aligns the output
space of MLLM with CLIP using the textual modality as an anchor and performs
compositional instruction tuning on curated data. Kosmos-G demonstrates a
unique capability of zero-shot multi-entity subject-driven generation. Notably,
the score distillation instruction tuning requires no modifications to the
image decoder. This allows for a seamless substitution of CLIP and effortless
integration with a myriad of U-Net techniques ranging from fine-grained
controls to personalized image decoder variants. We posit Kosmos-G as an
initial attempt towards the goal of "image as a foreign language in image
generation."
</p></li>
</ul>

<h3>Title: Conversational Health Agents: A Personalized LLM-Powered Agent Framework. (arXiv:2310.02374v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02374">http://arxiv.org/abs/2310.02374</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02374]] Conversational Health Agents: A Personalized LLM-Powered Agent Framework(http://arxiv.org/abs/2310.02374)</code></li>
<li>Summary: <p>Conversational Health Agents (CHAs) are interactive systems designed to
enhance personal healthcare services by engaging in empathetic conversations
and processing multimodal data. While current CHAs, especially those utilizing
Large Language Models (LLMs), primarily focus on conversation, they often lack
comprehensive agent capabilities. This includes the ability to access personal
user health data from wearables, 24/7 data collection sources, and electronic
health records, as well as integrating the latest published health insights and
connecting with established multimodal data analysis tools. We are developing a
framework to empower CHAs by equipping them with critical thinking, knowledge
acquisition, and problem-solving abilities. Our CHA platform, powered by LLMs,
seamlessly integrates healthcare tools, enables multilingual and multimodal
conversations, and interfaces with a variety of user data analysis tools. We
illustrate its proficiency in handling complex healthcare tasks, such as stress
level estimation, showcasing the agent's cognitive and operational
capabilities.
</p></li>
</ul>

<h3>Title: Can a student Large Language Model perform as well as it's teacher?. (arXiv:2310.02421v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02421">http://arxiv.org/abs/2310.02421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02421]] Can a student Large Language Model perform as well as it's teacher?(http://arxiv.org/abs/2310.02421)</code></li>
<li>Summary: <p>The burgeoning complexity of contemporary deep learning models, while
achieving unparalleled accuracy, has inadvertently introduced deployment
challenges in resource-constrained environments. Knowledge distillation, a
technique aiming to transfer knowledge from a high-capacity "teacher" model to
a streamlined "student" model, emerges as a promising solution to this dilemma.
This paper provides a comprehensive overview of the knowledge distillation
paradigm, emphasizing its foundational principles such as the utility of soft
labels and the significance of temperature scaling. Through meticulous
examination, we elucidate the critical determinants of successful distillation,
including the architecture of the student model, the caliber of the teacher,
and the delicate balance of hyperparameters. While acknowledging its profound
advantages, we also delve into the complexities and challenges inherent in the
process. Our exploration underscores knowledge distillation's potential as a
pivotal technique in optimizing the trade-off between model performance and
deployment efficiency.
</p></li>
</ul>

<h3>Title: Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of Large Language Models with Misconceptions. (arXiv:2310.02439v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02439">http://arxiv.org/abs/2310.02439</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02439]] Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of Large Language Models with Misconceptions(http://arxiv.org/abs/2310.02439)</code></li>
<li>Summary: <p>We propose novel evaluations for mathematical reasoning capabilities of Large
Language Models (LLMs) based on mathematical misconceptions. Our primary
approach is to simulate LLMs as a novice learner and an expert tutor, aiming to
identify the incorrect answer to math question resulted from a specific
misconception and to recognize the misconception(s) behind an incorrect answer,
respectively. Contrary to traditional LLMs-based mathematical evaluations that
focus on answering math questions correctly, our approach takes inspirations
from principles in educational learning sciences. We explicitly ask LLMs to
mimic a novice learner by answering questions in a specific incorrect manner
based on incomplete knowledge; and to mimic an expert tutor by identifying
misconception(s) corresponding to an incorrect answer to a question. Using
simple grade-school math problems, our experiments reveal that, while LLMs can
easily answer these questions correctly, they struggle to identify 1) the
incorrect answer corresponding to specific incomplete knowledge
(misconceptions); 2) the misconceptions that explain particular incorrect
answers. Our study indicates new opportunities for enhancing LLMs' math
reasoning capabilities, especially on developing robust student simulation and
expert tutoring models in the educational applications such as intelligent
tutoring systems.
</p></li>
</ul>

<h3>Title: The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising "Alignment" in Large Language Models. (arXiv:2310.02457v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02457">http://arxiv.org/abs/2310.02457</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02457]] The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising "Alignment" in Large Language Models(http://arxiv.org/abs/2310.02457)</code></li>
<li>Summary: <p>In this paper, we address the concept of "alignment" in large language models
(LLMs) through the lens of post-structuralist socio-political theory,
specifically examining its parallels to empty signifiers. To establish a shared
vocabulary around how abstract concepts of alignment are operationalised in
empirical datasets, we propose a framework that demarcates: 1) which dimensions
of model behaviour are considered important, then 2) how meanings and
definitions are ascribed to these dimensions, and by whom. We situate existing
empirical literature and provide guidance on deciding which paradigm to follow.
Through this framework, we aim to foster a culture of transparency and critical
evaluation, aiding the community in navigating the complexities of aligning
LLMs with human populations.
</p></li>
</ul>

<h3>Title: CITING: Large Language Models Create Curriculum for Instruction Tuning. (arXiv:2310.02527v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02527">http://arxiv.org/abs/2310.02527</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02527]] CITING: Large Language Models Create Curriculum for Instruction Tuning(http://arxiv.org/abs/2310.02527)</code></li>
<li>Summary: <p>The recent advancement of large language models (LLMs) has been achieved
through a combo of instruction tuning and human alignment. However, building
manually crafted instruction datasets and performing human alignment become the
bottleneck for scaling the development of LLMs. In this paper, we exploit the
idea of leveraging AI models in lieu of humans as the teacher to train student
LLMs. Our method is inspired by how human students refine their writing skills
by following the rubrics and learning from the revisions offered by their
tutors. Specifically, we employ a teacher LLM to create a curriculum for
instruction tuning of the student LLM, namely Curriculum Instruction TunING
(CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics
for evaluating the answers corresponding to various types of questions, and (2)
the student LLM learns to follow the rubrics and perform self-correction from
the revision made by the teacher. We further iteratively carry out it to embody
the procedure of CITING. We compare CITING to a series of state-of-the-art
baselines on four datasets. Our method demonstrates strong improvement in terms
of articulate, in-depth, and comprehensive by GPT-4 evaluation. Specifically,
it achieves an average winning rate of 79.4% over SFT, 73.4% over RLHF, 78.1%
over RRHF, and 76.3% over RAFT, respectively.
</p></li>
</ul>

<h3>Title: Comparative Study and Framework for Automated Summariser Evaluation: LangChain and Hybrid Algorithms. (arXiv:2310.02759v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02759">http://arxiv.org/abs/2310.02759</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02759]] Comparative Study and Framework for Automated Summariser Evaluation: LangChain and Hybrid Algorithms(http://arxiv.org/abs/2310.02759)</code></li>
<li>Summary: <p>Automated Essay Score (AES) is proven to be one of the cutting-edge
technologies. Scoring techniques are used for various purposes. Reliable scores
are calculated based on influential variables. Such variables can be computed
by different methods based on the domain. The research is concentrated on the
user's understanding of a given topic. The analysis is based on a scoring index
by using Large Language Models. The user can then compare and contrast the
understanding of a topic that they recently learned. The results are then
contributed towards learning analytics and progression is made for enhancing
the learning ability. In this research, the focus is on summarizing a PDF
document and gauging a user's understanding of its content. The process
involves utilizing a Langchain tool to summarize the PDF and extract the
essential information. By employing this technique, the research aims to
determine how well the user comprehends the summarized content.
</p></li>
</ul>

<h3>Title: A UMLS-Augmented Framework for Improving Factuality in Large Language Models within Healthcare. (arXiv:2310.02778v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02778">http://arxiv.org/abs/2310.02778</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02778]] A UMLS-Augmented Framework for Improving Factuality in Large Language Models within Healthcare(http://arxiv.org/abs/2310.02778)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated powerful text generation
capabilities, bringing unprecedented innovation to the healthcare field. While
LLMs hold immense promise for applications in healthcare, applying them to real
clinical scenarios presents significant challenges, as these models may
generate content that deviates from established medical facts and even exhibit
potential biases. In our research, we develop an augmented LLM framework based
on the Unified Medical Language System (UMLS), aiming to better serve the
healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our
benchmark models, and conduct automatic evaluations using the ROUGE Score and
BERTScore on 104 questions from the LiveQA test set. Additionally, we establish
criteria for physician-evaluation based on four dimensions: Factuality,
Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician
evaluation with 20 questions on the LiveQA test set. Multiple resident
physicians conducted blind reviews to evaluate the generated content, and the
results indicate that this framework effectively enhances the factuality,
completeness, and relevance of generated content. Our research demonstrates the
effectiveness of using UMLS-augmented LLMs and highlights the potential
application value of LLMs in in medical question-answering.
</p></li>
</ul>

<h3>Title: Assessing Large Language Models on Climate Information. (arXiv:2310.02932v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02932">http://arxiv.org/abs/2310.02932</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02932]] Assessing Large Language Models on Climate Information(http://arxiv.org/abs/2310.02932)</code></li>
<li>Summary: <p>Understanding how climate change affects us and learning about available
solutions are key steps toward empowering individuals and communities to
mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity,
it is necessary to assess their capability in this domain. In this study, we
present a comprehensive evaluation framework, grounded in science communication
principles, to analyze LLM responses to climate change topics. Our framework
emphasizes both the presentational and epistemological adequacy of answers,
offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our
framework discerns up to 30 distinct issues in model outputs. The task is a
real-world example of a growing number of challenging problems where AI can
complement and lift human performance. We introduce a novel and practical
protocol for scalable oversight that uses AI Assistance and relies on raters
with relevant educational backgrounds. We evaluate several recent LLMs and
conduct a comprehensive analysis of the results, shedding light on both the
potential and the limitations of LLMs in the realm of climate communication.
</p></li>
</ul>

<h3>Title: From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference. (arXiv:2310.03003v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03003">http://arxiv.org/abs/2310.03003</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03003]] From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference(http://arxiv.org/abs/2310.03003)</code></li>
<li>Summary: <p>Large language models (LLMs) have exploded in popularity due to their new
generative capabilities that go far beyond prior state-of-the-art. These
technologies are increasingly being leveraged in various domains such as law,
finance, and medicine. However, these models carry significant computational
challenges, especially the compute and energy costs required for inference.
Inference energy costs already receive less attention than the energy costs of
training LLMs -- despite how often these large models are called on to conduct
inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see
increasing usage and deployment in various domains, a better understanding of
their resource utilization is crucial for cost-savings, scaling performance,
efficient hardware usage, and optimal inference strategies.
</p>
<p>In this paper, we describe experiments conducted to study the computational
and energy utilization of inference with LLMs. We benchmark and conduct a
preliminary analysis of the inference performance and inference energy costs of
different sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta
AI on two generations of popular GPUs (NVIDIA V100 \&amp; A100) and two datasets
(Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in
research and practice. We present the results of multi-node, multi-GPU
inference using model sharding across up to 32 GPUs. To our knowledge, our work
is the one of the first to study LLM inference performance from the perspective
of computational and energy resources at this scale.
</p></li>
</ul>

<h3>Title: Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity. (arXiv:2310.02277v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02277">http://arxiv.org/abs/2310.02277</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02277]] Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity(http://arxiv.org/abs/2310.02277)</code></li>
<li>Summary: <p>The traditional notion of "Junk DNA" has long been linked to non-coding
segments within the human genome, constituting roughly 98% of its composition.
However, recent research has unveiled the critical roles some of these
seemingly non-functional DNA sequences play in cellular processes.
Intriguingly, the weights within deep neural networks exhibit a remarkable
similarity to the redundancy observed in human genes. It was believed that
weights in gigantic models contained excessive redundancy, and could be removed
without compromising performance. This paper challenges this conventional
wisdom by presenting a compelling counter-argument. We employ sparsity as a
tool to isolate and quantify the nuanced significance of low-magnitude weights
in pre-trained large language models (LLMs). Our study demonstrates a strong
correlation between these weight magnitudes and the knowledge they encapsulate,
from a downstream task-centric angle. we raise the "Junk DNA Hypothesis" backed
by our in-depth investigation: while small-magnitude weights may appear
"useless" for simple tasks and suitable for pruning, they actually encode
crucial knowledge necessary for solving more difficult downstream tasks.
Removing these seemingly insignificant weights can lead to irreversible
knowledge forgetting and performance damage in difficult tasks. These findings
offer fresh insights into how LLMs encode knowledge in a task-sensitive manner,
pave future research direction in model pruning, and open avenues for
task-aware conditional computation during inference.
</p></li>
</ul>

<h3>Title: Reward Model Ensembles Help Mitigate Overoptimization. (arXiv:2310.02743v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02743">http://arxiv.org/abs/2310.02743</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02743]] Reward Model Ensembles Help Mitigate Overoptimization(http://arxiv.org/abs/2310.02743)</code></li>
<li>Summary: <p>Reinforcement learning from human feedback (RLHF) is a standard approach for
fine-tuning large language models to follow instructions. As part of this
process, learned reward models are used to approximately model human
preferences. However, as imperfect representations of the "true" reward, these
learned reward models are susceptible to \textit{overoptimization}. Gao et al.
(2023) studied this phenomenon in a synthetic human feedback setup with a
significantly larger "gold" reward model acting as the true reward (instead of
humans) and showed that overoptimization remains a persistent problem
regardless of the size of the proxy reward model and training data used. Using
a similar setup, we conduct a systematic study to evaluate the efficacy of
using ensemble-based conservative optimization objectives, specifically
worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for
mitigating reward model overoptimization when using two optimization methods:
(a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We
additionally extend the setup of Gao et al. (2023) to include 25% label noise
to better mirror real-world conditions. Both with and without label noise, we
find that conservative optimization practically eliminates overoptimization and
improves performance by up to 70% for BoN sampling. For PPO, ensemble-based
conservative optimization always reduces overoptimization and outperforms
single reward model optimization. Moreover, combining it with a small KL
penalty successfully prevents overoptimization at no performance cost. Overall,
our results demonstrate that ensemble-based conservative optimization can
effectively counter overoptimization.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: CLIP Is Also a Good Teacher: A New Learning Framework for Inductive Zero-shot Semantic Segmentation. (arXiv:2310.02296v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02296">http://arxiv.org/abs/2310.02296</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02296]] CLIP Is Also a Good Teacher: A New Learning Framework for Inductive Zero-shot Semantic Segmentation(http://arxiv.org/abs/2310.02296)</code></li>
<li>Summary: <p>Existing Generalized Zero-shot Semantic Segmentation (GZLSS) methods apply
either finetuning the CLIP paradigm or formulating it as a mask classification
task, benefiting from the Vision-Language Models (VLMs). However, the
fine-tuning methods are restricted with fixed backbone models which are not
flexible for segmentation, and mask classification methods heavily rely on
additional explicit mask proposers. Meanwhile, prevalent methods utilize only
seen categories which is a great waste, i.e., neglecting the area exists but
not annotated. To this end, we propose CLIPTeacher, a new learning framework
that can be applied to various per-pixel classification segmentation models
without introducing any explicit mask proposer or changing the structure of
CLIP, and utilize both seen and ignoring areas. Specifically, CLIPTeacher
consists of two key modules: Global Learning Module (GLM) and Pixel Learning
Module (PLM). Specifically, GLM aligns the dense features from an image encoder
with the CLS token, i.e., the only token trained in CLIP, which is a simple but
effective way to probe global information from the CLIP models. In contrast,
PLM only leverages dense tokens from CLIP to produce high-level pseudo
annotations for ignoring areas without introducing any extra mask proposer.
Meanwhile, PLM can fully take advantage of the whole image based on the pseudo
annotations. Experimental results on three benchmark datasets: PASCAL VOC 2012,
COCO-Stuff 164k, and PASCAL Context show large performance gains, i.e., 2.2%,
1.3%, and 8.8%
</p></li>
</ul>

<h3>Title: Unsupervised Speech Recognition with N-Skipgram and Positional Unigram Matching. (arXiv:2310.02382v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02382">http://arxiv.org/abs/2310.02382</a></li>
<li>Code URL: https://github.com/lwang114/graphunsupasr</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02382]] Unsupervised Speech Recognition with N-Skipgram and Positional Unigram Matching(http://arxiv.org/abs/2310.02382)</code></li>
<li>Summary: <p>Training unsupervised speech recognition systems presents challenges due to
GAN-associated instability, misalignment between speech and text, and
significant memory demands. To tackle these challenges, we introduce a novel
ASR system, ESPUM. This system harnesses the power of lower-order N-skipgrams
(up to N=3) combined with positional unigram statistics gathered from a small
batch of samples. Evaluated on the TIMIT benchmark, our model showcases
competitive performance in ASR and phoneme segmentation tasks. Access our
publicly available code at https://github.com/lwang114/GraphUnsupASR.
</p></li>
</ul>

<h3>Title: End-to-End Training of a Neural HMM with Label and Transition Probabilities. (arXiv:2310.02724v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02724">http://arxiv.org/abs/2310.02724</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02724]] End-to-End Training of a Neural HMM with Label and Transition Probabilities(http://arxiv.org/abs/2310.02724)</code></li>
<li>Summary: <p>We investigate a novel modeling approach for end-to-end neural network
training using hidden Markov models (HMM) where the transition probabilities
between hidden states are modeled and learned explicitly. Most contemporary
sequence-to-sequence models allow for from-scratch training by summing over all
possible label segmentations in a given topology. In our approach there are
explicit, learnable probabilities for transitions between segments as opposed
to a blank label that implicitly encodes duration statistics. We implement a
GPU-based forward-backward algorithm that enables the simultaneous training of
label and transition probabilities. We investigate recognition results and
additionally Viterbi alignments of our models. We find that while the
transition model training does not improve recognition performance, it has a
positive impact on the alignment quality. The generated alignments are shown to
be viable targets in state-of-the-art Viterbi trainings.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
