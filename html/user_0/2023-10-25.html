<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: SecV: Secure Code Partitioning via Multi-Language Secure Values. (arXiv:2310.15582v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15582">http://arxiv.org/abs/2310.15582</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15582]] SecV: Secure Code Partitioning via Multi-Language Secure Values(http://arxiv.org/abs/2310.15582)</code></li>
<li>Summary: <p>Trusted execution environments like Intel SGX provide \emph{enclaves}, which
offer strong security guarantees for applications. Running entire applications
inside enclaves is possible, but this approach leads to a large trusted
computing base (TCB). As such, various tools have been developed to partition
programs written in languages such as C or Java into \emph{trusted} and
\emph{untrusted} parts, which are run in and out of enclaves respectively.
However, those tools depend on language-specific taint-analysis and
partitioning techniques. They cannot be reused for other languages and there is
thus a need for tools that transcend this language barrier.
</p>
<p>We address this challenge by proposing a multi-language technique to specify
sensitive code or data, as well as a multi-language tool to analyse and
partition the resulting programs for trusted execution environments like Intel
SGX. We leverage GraalVM's Truffle framework, which provides a
language-agnostic abstract syntax tree (AST) representation for programs, to
provide special AST nodes called \emph{secure nodes} that encapsulate sensitive
program information. Secure nodes can easily be embedded into the ASTs of a
wide range of languages via Truffle's \emph{polyglot API}. Our technique
includes a multi-language dynamic taint tracking tool to analyse and partition
applications based on our generic secure nodes. Our extensive evaluation with
micro- and macro-benchmarks shows that we can use our technique for two
languages (Javascript and \python), and that partitioned programs can obtain up
to $14.5\%$ performance improvement as compared to unpartitioned versions.
</p></li>
</ul>

<h3>Title: Exploring the Risks and Challenges of National Electronic Identity (NeID) System. (arXiv:2310.15813v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15813">http://arxiv.org/abs/2310.15813</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15813]] Exploring the Risks and Challenges of National Electronic Identity (NeID) System(http://arxiv.org/abs/2310.15813)</code></li>
<li>Summary: <p>Many countries have embraced national electronic identification (NeID)
systems, recognising their potential to foster a fair, transparent, and
well-governed society by ensuring the secure verification of citizens'
identities. The inclusive nature of NeID empowers people to exercise their
rights while holding them accountable for fulfilling their obligations.
Nevertheless, the development and implementation of these complex
identity-verification systems have raised concerns regarding security, privacy,
and exclusion. In this study, we discuss the different categories of NeID risk
and explore the successful deployment of these systems, while examining how the
specific risks and other challenges posed by this technology are addressed.
Based on the review of the different NeID systems and the efforts made to
mitigate the unique risks and challenges presented within each deployment, we
highlighted the best practices for mitigating risk, including implementing
strong security measures, conducting regular risk assessments, and involving
stakeholders in the design and implementation of the system.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: A Survey on Detection of LLMs-Generated Content. (arXiv:2310.15654v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15654">http://arxiv.org/abs/2310.15654</a></li>
<li>Code URL: https://github.com/xianjun-yang/awesome_papers_on_llms_detection</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15654]] A Survey on Detection of LLMs-Generated Content(http://arxiv.org/abs/2310.15654)</code></li>
<li>Summary: <p>The burgeoning capabilities of advanced large language models (LLMs) such as
ChatGPT have led to an increase in synthetic content generation with
implications across a variety of sectors, including media, cybersecurity,
public discourse, and education. As such, the ability to detect LLMs-generated
content has become of paramount importance. We aim to provide a detailed
overview of existing detection strategies and benchmarks, scrutinizing their
differences and identifying key challenges and prospects in the field,
advocating for more adaptable and robust models to enhance detection accuracy.
We also posit the necessity for a multi-faceted approach to defend against
various attacks to counter the rapidly advancing capabilities of LLMs. To the
best of our knowledge, this work is the first comprehensive survey on the
detection in the era of LLMs. We hope it will provide a broad understanding of
the current landscape of LLMs-generated content detection, offering a guiding
reference for researchers and practitioners striving to uphold the integrity of
digital information in an era increasingly dominated by synthetic content. The
relevant papers are summarized and will be consistently updated at
https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git.
</p></li>
</ul>

<h3>Title: Non-Fungible Token Security. (arXiv:2310.15518v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15518">http://arxiv.org/abs/2310.15518</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15518]] Non-Fungible Token Security(http://arxiv.org/abs/2310.15518)</code></li>
<li>Summary: <p>Non-fungible tokens (NFTs) are unique digital assets stored on the blockchain
and is used to certify ownership and authenticity of the digital asset. NFTs
were first created in 2014 while their popularity peaked between 2021 and 2022.
In this paper, the authors dive into the world of Non-Fungible Tokens (NFTs),
their history, the Future of NFTs, as well as the security concerns.
</p></li>
</ul>

<h3>Title: An Impact and Risk Assessment Framework for National Electronic Identity (eID) Systems. (arXiv:2310.15784v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15784">http://arxiv.org/abs/2310.15784</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15784]] An Impact and Risk Assessment Framework for National Electronic Identity (eID) Systems(http://arxiv.org/abs/2310.15784)</code></li>
<li>Summary: <p>Electronic identification (eID) systems allow citizens to assert and
authenticate their identities for various purposes, such as accessing
government services or conducting financial transactions. These systems improve
user access to rights, services, and the formal economy. As eID systems become
an essential facet of national development, any failure, compromise, or misuse
can be costly and damaging to the government, users, and society. Therefore, an
effective risk assessment is vital for identifying emerging risks to the system
and assessing their impact. However, developing a comprehensive risk assessment
for these systems must extend far beyond focusing on technical security and
privacy impacts and must be conducted with a contextual understanding of
stakeholders and the communities these systems serve. In this study, we posit
that current risk assessments do not address risk factors for all key
stakeholders and explore how potential compromise could impact them each in
turn. In the examination of the broader impact of risks and the potentially
significant consequences for stakeholders, we propose a framework that
considers a wide range of factors, including the social, economic, and
political contexts in which these systems were implemented. This provides a
holistic platform for a better assessment of risk to the eID system.
</p></li>
</ul>

<h3>Title: Empowering Distributed Solutions in Renewable Energy Systems and Grid Optimization. (arXiv:2310.15468v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15468">http://arxiv.org/abs/2310.15468</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15468]] Empowering Distributed Solutions in Renewable Energy Systems and Grid Optimization(http://arxiv.org/abs/2310.15468)</code></li>
<li>Summary: <p>This study delves into the shift from centralized to decentralized approaches
in the electricity industry, with a particular focus on how machine learning
(ML) advancements play a crucial role in empowering renewable energy sources
and improving grid management. ML models have become increasingly important in
predicting renewable energy generation and consumption, utilizing various
techniques like artificial neural networks, support vector machines, and
decision trees. Furthermore, data preprocessing methods, such as data
splitting, normalization, decomposition, and discretization, are employed to
enhance prediction accuracy.
</p>
<p>The incorporation of big data and ML into smart grids offers several
advantages, including heightened energy efficiency, more effective responses to
demand, and better integration of renewable energy sources. Nevertheless,
challenges like handling large data volumes, ensuring cybersecurity, and
obtaining specialized expertise must be addressed. The research investigates
various ML applications within the realms of solar energy, wind energy, and
electric distribution and storage, illustrating their potential to optimize
energy systems. To sum up, this research demonstrates the evolving landscape of
the electricity sector as it shifts from centralized to decentralized solutions
through the application of ML innovations and distributed decision-making,
ultimately shaping a more efficient and sustainable energy future.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Facial Data Minimization: Shallow Model as Your Privacy Filter. (arXiv:2310.15590v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15590">http://arxiv.org/abs/2310.15590</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15590]] Facial Data Minimization: Shallow Model as Your Privacy Filter(http://arxiv.org/abs/2310.15590)</code></li>
<li>Summary: <p>Face recognition service has been used in many fields and brings much
convenience to people. However, once the user's facial data is transmitted to a
service provider, the user will lose control of his/her private data. In recent
years, there exist various security and privacy issues due to the leakage of
facial data. Although many privacy-preserving methods have been proposed, they
usually fail when they are not accessible to adversaries' strategies or
auxiliary data. Hence, in this paper, by fully considering two cases of
uploading facial images and facial features, which are very typical in face
recognition service systems, we proposed a data privacy minimization
transformation (PMT) method. This method can process the original facial data
based on the shallow model of authorized services to obtain the obfuscated
data. The obfuscated data can not only maintain satisfactory performance on
authorized models and restrict the performance on other unauthorized models but
also prevent original privacy data from leaking by AI methods and human visual
theft. Additionally, since a service provider may execute preprocessing
operations on the received data, we also propose an enhanced perturbation
method to improve the robustness of PMT. Besides, to authorize one facial image
to multiple service models simultaneously, a multiple restriction mechanism is
proposed to improve the scalability of PMT. Finally, we conduct extensive
experiments and evaluate the effectiveness of the proposed PMT in defending
against face reconstruction, data abuse, and face attribute estimation attacks.
These experimental results demonstrate that PMT performs well in preventing
facial data abuse and privacy leakage while maintaining face recognition
accuracy.
</p></li>
</ul>

<h3>Title: 3D Masked Autoencoders for Enhanced Privacy in MRI Scans. (arXiv:2310.15778v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15778">http://arxiv.org/abs/2310.15778</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15778]] 3D Masked Autoencoders for Enhanced Privacy in MRI Scans(http://arxiv.org/abs/2310.15778)</code></li>
<li>Summary: <p>MRI scans provide valuable medical information, however they also contain
sensitive and personally identifiable information (PII) that needs to be
protected. Whereas MRI metadata is easily sanitized, MRI image data is a
privacy risk because it contains information to render highly-realistic 3D
visualizations of a patient's head, enabling malicious actors to possibly
identify the subject by cross-referencing a database. Data anonymization and
de-identification is concerned with ensuring the privacy and confidentiality of
individuals' personal information. Traditional MRI de-identification methods
remove privacy-sensitive parts (e.g. eyes, nose etc.) from a given scan. This
comes at the expense of introducing a domain shift that can throw off
downstream analyses. Recently, a GAN-based approach was proposed to de-identify
a patient's scan by remodeling it (e.g. changing the face) rather than by
removing parts. In this work, we propose CP-MAE, a model that de-identifies the
face using masked autoencoders and that outperforms all previous approaches in
terms of downstream task performance as well as de-identification. With our
method we are able to synthesize scans of resolution up to $256^3$ (previously
128 cubic) which constitutes an eight-fold increase in the number of voxels.
Using our construction we were able to design a system that exhibits a highly
robust training stage, making it easy to fit the network on novel data.
</p></li>
</ul>

<h3>Title: On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms. (arXiv:2310.15848v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15848">http://arxiv.org/abs/2310.15848</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15848]] On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms(http://arxiv.org/abs/2310.15848)</code></li>
<li>Summary: <p>Artificial Intelligence (AI) has made its way into various scientific fields,
providing astonishing improvements over existing algorithms for a wide variety
of tasks. In recent years, there have been severe concerns over the
trustworthiness of AI technologies. The scientific community has focused on the
development of trustworthy AI algorithms. However, machine and deep learning
algorithms, popular in the AI community today, depend heavily on the data used
during their development. These learning algorithms identify patterns in the
data, learning the behavioral objective. Any flaws in the data have the
potential to translate directly into algorithms. In this study, we discuss the
importance of Responsible Machine Learning Datasets and propose a framework to
evaluate the datasets through a responsible rubric. While existing work focuses
on the post-hoc evaluation of algorithms for their trustworthiness, we provide
a framework that considers the data component separately to understand its role
in the algorithm. We discuss responsible datasets through the lens of fairness,
privacy, and regulatory compliance and provide recommendations for constructing
future datasets. After surveying over 100 datasets, we use 60 datasets for
analysis and demonstrate that none of these datasets is immune to issues of
fairness, privacy preservation, and regulatory compliance. We provide
modifications to the ``datasheets for datasets" with important additions for
improved dataset documentation. With governments around the world regularizing
data protection laws, the method for the creation of datasets in the scientific
community requires revision. We believe this study is timely and relevant in
today's era of AI.
</p></li>
</ul>

<h3>Title: The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks. (arXiv:2310.15469v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15469">http://arxiv.org/abs/2310.15469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15469]] The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks(http://arxiv.org/abs/2310.15469)</code></li>
<li>Summary: <p>The era post-2018 marked the advent of Large Language Models (LLMs), with
innovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess.
As the industry galloped toward augmenting model parameters and capitalizing on
vast swaths of human language data, security and privacy challenges also
emerged. Foremost among these is the potential inadvertent accrual of Personal
Identifiable Information (PII) during web-based data acquisition, posing risks
of unintended PII disclosure. While strategies like RLHF during training and
Catastrophic Forgetting have been marshaled to control the risk of privacy
infringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning
interface for GPT-3.5, have reignited concerns. One may ask: can the
fine-tuning of LLMs precipitate the leakage of personal information embedded
within training datasets? This paper reports the first endeavor to seek the
answer to the question, particularly our discovery of a new LLM exploitation
avenue, called the Janus attack. In the attack, one can construct a PII
association task, whereby an LLM is fine-tuned using a minuscule PII dataset,
to potentially reinstate and reveal concealed PIIs. Our findings indicate that,
with a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition from
being impermeable to PII extraction to a state where they divulge a substantial
proportion of concealed PII. This research, through its deep dive into the
Janus attack vector, underscores the imperative of navigating the intricate
interplay between LLM utility and privacy preservation.
</p></li>
</ul>

<h3>Title: COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15694">http://arxiv.org/abs/2310.15694</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15694]] COPF: Continual Learning Human Preference through Optimal Policy Fitting(http://arxiv.org/abs/2310.15694)</code></li>
<li>Summary: <p>The technique of Reinforcement Learning from Human Feedback (RLHF) is a
commonly employed method to improve pre-trained Language Models (LM), enhancing
their ability to conform to human preferences. Nevertheless, the current
RLHF-based LMs necessitate full retraining each time novel queries or feedback
are introduced, which becomes a challenging task because human preferences can
vary between different domains or tasks. Retraining LMs poses practical
difficulties in many real-world situations due to the significant time and
computational resources required, along with concerns related to data privacy.
To address this limitation, we propose a new method called Continual Optimal
Policy Fitting (COPF), in which we estimate a series of optimal policies using
the Monte Carlo method, and then continually fit the policy sequence with the
function regularization. COPF involves a single learning phase and doesn't
necessitate complex reinforcement learning. Importantly, it shares the
capability with RLHF to learn from unlabeled data, making it flexible for
continual preference learning. Our experimental results show that COPF
outperforms strong Continuous learning (CL) baselines when it comes to
consistently aligning with human preferences on different tasks and domains.
</p></li>
</ul>

<h3>Title: Privacy Amplification for Matrix Mechanisms. (arXiv:2310.15526v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15526">http://arxiv.org/abs/2310.15526</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15526]] Privacy Amplification for Matrix Mechanisms(http://arxiv.org/abs/2310.15526)</code></li>
<li>Summary: <p>Privacy amplification exploits randomness in data selection to provide
tighter differential privacy (DP) guarantees. This analysis is key to DP-SGD's
success in machine learning, but, is not readily applicable to the newer
state-of-the-art algorithms. This is because these algorithms, known as
DP-FTRL, use the matrix mechanism to add correlated noise instead of
independent noise as in DP-SGD.
</p>
<p>In this paper, we propose "MMCC", the first algorithm to analyze privacy
amplification via sampling for any generic matrix mechanism. MMCC is nearly
tight in that it approaches a lower bound as $\epsilon\to0$. To analyze
correlated outputs in MMCC, we prove that they can be analyzed as if they were
independent, by conditioning them on prior outputs. Our "conditional
composition theorem" has broad utility: we use it to show that the noise added
to binary-tree-DP-FTRL can asymptotically match the noise added to DP-SGD with
amplification. Our amplification algorithm also has practical empirical
utility: we show it leads to significant improvement in the privacy-utility
trade-offs for DP-FTRL algorithms on standard benchmarks.
</p></li>
</ul>

<h3>Title: On the Inherent Privacy Properties of Discrete Denoising Diffusion Models. (arXiv:2310.15524v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15524">http://arxiv.org/abs/2310.15524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15524]] On the Inherent Privacy Properties of Discrete Denoising Diffusion Models(http://arxiv.org/abs/2310.15524)</code></li>
<li>Summary: <p>Privacy concerns have led to a surge in the creation of synthetic datasets,
with diffusion models emerging as a promising avenue. Although prior studies
have performed empirical evaluations on these models, there has been a gap in
providing a mathematical characterization of their privacy-preserving
capabilities. To address this, we present the pioneering theoretical
exploration of the privacy preservation inherent in discrete diffusion models
(DDMs) for discrete dataset generation. Focusing on per-instance differential
privacy (pDP), our framework elucidates the potential privacy leakage for each
data point in a given training dataset, offering insights into data
preprocessing to reduce privacy risks of the synthetic dataset generation via
DDMs. Our bounds also show that training with $s$-sized data points leads to a
surge in privacy leakage from $(\epsilon,
\mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP to $(\epsilon,
\mathcal{O}(\frac{1}{s\epsilon}))$-pDP during the transition from the pure
noise to the synthetic clean data phase, and a faster decay in diffusion
coefficients amplifies the privacy guarantee. Finally, we empirically verify
our theoretical findings on both synthetic and real-world datasets.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Creating a silver standard for patent simplification. (arXiv:2310.15689v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15689">http://arxiv.org/abs/2310.15689</a></li>
<li>Code URL: https://github.com/slvcsl/patentsilverstandard</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15689]] Creating a silver standard for patent simplification(http://arxiv.org/abs/2310.15689)</code></li>
<li>Summary: <p>Patents are legal documents that aim at protecting inventions on the one hand
and at making technical knowledge circulate on the other. Their complex style
-- a mix of legal, technical, and extremely vague language -- makes their
content hard to access for humans and machines and poses substantial challenges
to the information retrieval community. This paper proposes an approach to
automatically simplify patent text through rephrasing. Since no in-domain
parallel simplification data exist, we propose a method to automatically
generate a large-scale silver standard for patent sentences. To obtain
candidates, we use a general-domain paraphrasing system; however, the process
is error-prone and difficult to control. Thus, we pair it with proper filters
and construct a cleaner corpus that can successfully be used to train a
simplification system. Human evaluation of the synthetic silver corpus shows
that it is considered grammatical, adequate, and contains simple sentences.
</p></li>
</ul>

<h3>Title: Private Learning with Public Features. (arXiv:2310.15454v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15454">http://arxiv.org/abs/2310.15454</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15454]] Private Learning with Public Features(http://arxiv.org/abs/2310.15454)</code></li>
<li>Summary: <p>We study a class of private learning problems in which the data is a join of
private and public features. This is often the case in private personalization
tasks such as recommendation or ad prediction, in which features related to
individuals are sensitive, while features related to items (the movies or songs
to be recommended, or the ads to be shown to users) are publicly available and
do not require protection. A natural question is whether private algorithms can
achieve higher utility in the presence of public features. We give a positive
answer for multi-encoder models where one of the encoders operates on public
features. We develop new algorithms that take advantage of this separation by
only protecting certain sufficient statistics (instead of adding noise to the
gradient). This method has a guaranteed utility improvement for linear
regression, and importantly, achieves the state of the art on two standard
private recommendation benchmarks, demonstrating the importance of methods that
adapt to the private-public feature separation.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Fast Propagation is Better: Accelerating Single-Step Adversarial Training via Sampling Subnetworks. (arXiv:2310.15444v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15444">http://arxiv.org/abs/2310.15444</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15444]] Fast Propagation is Better: Accelerating Single-Step Adversarial Training via Sampling Subnetworks(http://arxiv.org/abs/2310.15444)</code></li>
<li>Summary: <p>Adversarial training has shown promise in building robust models against
adversarial examples. A major drawback of adversarial training is the
computational overhead introduced by the generation of adversarial examples. To
overcome this limitation, adversarial training based on single-step attacks has
been explored. Previous work improves the single-step adversarial training from
different perspectives, e.g., sample initialization, loss regularization, and
training strategy. Almost all of them treat the underlying model as a black
box. In this work, we propose to exploit the interior building blocks of the
model to improve efficiency. Specifically, we propose to dynamically sample
lightweight subnetworks as a surrogate model during training. By doing this,
both the forward and backward passes can be accelerated for efficient
adversarial training. Besides, we provide theoretical analysis to show the
model robustness can be improved by the single-step adversarial training with
sampled subnetworks. Furthermore, we propose a novel sampling strategy where
the sampling varies from layer to layer and from iteration to iteration.
Compared with previous methods, our method not only reduces the training cost
but also achieves better model robustness. Evaluations on a series of popular
datasets demonstrate the effectiveness of the proposed FB-Better. Our code has
been released at https://github.com/jiaxiaojunQAQ/FP-Better.
</p></li>
</ul>

<h3>Title: Self-Guard: Empower the LLM to Safeguard Itself. (arXiv:2310.15851v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15851">http://arxiv.org/abs/2310.15851</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15851]] Self-Guard: Empower the LLM to Safeguard Itself(http://arxiv.org/abs/2310.15851)</code></li>
<li>Summary: <p>The jailbreak attack can bypass the safety measures of a Large Language Model
(LLM), generating harmful content. This misuse of LLM has led to negative
societal consequences. Currently, there are two main approaches to address
jailbreak attacks: safety training and safeguards. Safety training focuses on
further training LLM to enhance its safety. On the other hand, safeguards
involve implementing external models or filters to prevent harmful outputs.
However, safety training has constraints in its ability to adapt to new attack
types and often leads to a drop in model performance. Safeguards have proven to
be of limited help. To tackle these issues, we propose a novel approach called
Self-Guard, which combines the strengths of both safety methods. Self-Guard
includes two stages. In the first stage, we enhance the model's ability to
assess harmful content, and in the second stage, we instruct the model to
consistently perform harmful content detection on its own responses. The
experiment has demonstrated that Self-Guard is robust against jailbreak
attacks. In the bad case analysis, we find that LLM occasionally provides
harmless responses to harmful queries. Additionally, we evaluated the general
capabilities of the LLM before and after safety training, providing evidence
that Self-Guard does not result in the LLM's performance degradation. In
sensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM
but also can even mitigate this issue.
</p></li>
</ul>

<h3>Title: Deceptive Fairness Attacks on Graphs via Meta Learning. (arXiv:2310.15653v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15653">http://arxiv.org/abs/2310.15653</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15653]] Deceptive Fairness Attacks on Graphs via Meta Learning(http://arxiv.org/abs/2310.15653)</code></li>
<li>Summary: <p>We study deceptive fairness attacks on graphs to answer the following
question: How can we achieve poisoning attacks on a graph learning model to
exacerbate the bias deceptively? We answer this question via a bi-level
optimization problem and propose a meta learning-based framework named FATE.
FATE is broadly applicable with respect to various fairness definitions and
graph learning models, as well as arbitrary choices of manipulation operations.
We further instantiate FATE to attack statistical parity and individual
fairness on graph neural networks. We conduct extensive experimental
evaluations on real-world datasets in the task of semi-supervised node
classification. The experimental results demonstrate that FATE could amplify
the bias of graph neural networks with or without fairness consideration while
maintaining the utility on the downstream task. We hope this paper provides
insights into the adversarial robustness of fair graph learning and can shed
light on designing robust and fair graph learning in future studies.
</p></li>
</ul>

<h3>Title: Momentum Gradient-based Untargeted Attack on Hypergraph Neural Networks. (arXiv:2310.15656v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15656">http://arxiv.org/abs/2310.15656</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15656]] Momentum Gradient-based Untargeted Attack on Hypergraph Neural Networks(http://arxiv.org/abs/2310.15656)</code></li>
<li>Summary: <p>Hypergraph Neural Networks (HGNNs) have been successfully applied in various
hypergraph-related tasks due to their excellent higher-order representation
capabilities. Recent works have shown that deep learning models are vulnerable
to adversarial attacks. Most studies on graph adversarial attacks have focused
on Graph Neural Networks (GNNs), and the study of adversarial attacks on HGNNs
remains largely unexplored. In this paper, we try to reduce this gap. We design
a new HGNNs attack model for the untargeted attack, namely MGHGA, which focuses
on modifying node features. We consider the process of HGNNs training and use a
surrogate model to implement the attack before hypergraph modeling.
Specifically, MGHGA consists of two parts: feature selection and feature
modification. We use a momentum gradient mechanism to choose the attack node
features in the feature selection module. In the feature modification module,
we use two feature generation approaches (direct modification and sign
gradient) to enable MGHGA to be employed on discrete and continuous datasets.
We conduct extensive experiments on five benchmark datasets to validate the
attack performance of MGHGA in the node and the visual object classification
tasks. The results show that MGHGA improves performance by an average of 2%
compared to the than the baselines.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Remote Heart Rate Monitoring in Smart Environments from Videos with Self-supervised Pre-training. (arXiv:2310.15388v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15388">http://arxiv.org/abs/2310.15388</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15388]] Remote Heart Rate Monitoring in Smart Environments from Videos with Self-supervised Pre-training(http://arxiv.org/abs/2310.15388)</code></li>
<li>Summary: <p>Recent advances in deep learning have made it increasingly feasible to
estimate heart rate remotely in smart environments by analyzing videos.
However, a notable limitation of deep learning methods is their heavy reliance
on extensive sets of labeled data for effective training. To address this
issue, self-supervised learning has emerged as a promising avenue. Building on
this, we introduce a solution that utilizes self-supervised contrastive
learning for the estimation of remote photoplethysmography (PPG) and heart rate
monitoring, thereby reducing the dependence on labeled data and enhancing
performance. We propose the use of 3 spatial and 3 temporal augmentations for
training an encoder through a contrastive framework, followed by utilizing the
late-intermediate embeddings of the encoder for remote PPG and heart rate
estimation. Our experiments on two publicly available datasets showcase the
improvement of our proposed approach over several related works as well as
supervised learning baselines, as our results approach the state-of-the-art. We
also perform thorough experiments to showcase the effects of using different
design choices such as the video representation learning method, the
augmentations used in the pre-training stage, and others. We also demonstrate
the robustness of our proposed method over the supervised learning approaches
on reduced amounts of labeled data.
</p></li>
</ul>

<h3>Title: Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework. (arXiv:2310.15646v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15646">http://arxiv.org/abs/2310.15646</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15646]] Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework(http://arxiv.org/abs/2310.15646)</code></li>
<li>Summary: <p>Unsupervised domain adaptation object detection(UDAOD) research on Detection
Transformer(DETR) mainly focuses on feature alignment and existing methods can
be divided into two kinds, each of which has its unresolved issues. One-stage
feature alignment methods can easily lead to performance fluctuation and
training stagnation. Two-stage feature alignment method based on mean teacher
comprises a pretraining stage followed by a self-training stage, each facing
problems in obtaining reliable pretrained model and achieving consistent
performance gains. Methods mentioned above have not yet explore how to utilize
the third related domain such as target-like domain to assist adaptation. To
address these issues, we propose a two-stage framework named MTM, i.e. Mean
Teacher-DETR with Masked Feature Alignment. In the pretraining stage, we
utilize labeled target-like images produced by image style transfer to avoid
performance fluctuation. In the self-training stage, we leverage unlabeled
target images by pseudo labels based on mean teacher and propose a module
called Object Queries Knowledge Transfer(OQKT) to ensure consistent performance
gains of the student model. Most importantly, we propose masked feature
alignment methods including Masked Domain Query-based Feature Alignment(MDQFA)
and Masked Token-wise Feature Alignment(MTWFA) to alleviate domain shift in a
more robust way, which not only prevent training stagnation and lead to a
robust pretrained model in the pretraining stage, but also enhance the model's
target performance in the self-training stage. Experiments on three challenging
scenarios and a theoretical analysis verify the effectiveness of MTM.
</p></li>
</ul>

<h3>Title: Breaking of brightness consistency in optical flow with a lightweight CNN network. (arXiv:2310.15655v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15655">http://arxiv.org/abs/2310.15655</a></li>
<li>Code URL: https://github.com/linyicheng1/LET-NET</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15655]] Breaking of brightness consistency in optical flow with a lightweight CNN network(http://arxiv.org/abs/2310.15655)</code></li>
<li>Summary: <p>Sparse optical flow is widely used in various computer vision tasks, however
assuming brightness consistency limits its performance in High Dynamic Range
(HDR) environments. In this work, a lightweight network is used to extract
illumination robust convolutional features and corners with strong invariance.
Modifying the typical brightness consistency of the optical flow method to the
convolutional feature consistency yields the light-robust hybrid optical flow
method. The proposed network runs at 190 FPS on a commercial CPU because it
uses only four convolutional layers to extract feature maps and score maps
simultaneously. Since the shallow network is difficult to train directly, a
deep network is designed to compute the reliability map that helps it. An
end-to-end unsupervised training mode is used for both networks. To validate
the proposed method, we compare corner repeatability and matching performance
with origin optical flow under dynamic illumination. In addition, a more
accurate visual inertial system is constructed by replacing the optical flow
method in VINS-Mono. In a public HDR dataset, it reduces translation errors by
93\%. The code is publicly available at https://github.com/linyicheng1/LET-NET.
</p></li>
</ul>

<h3>Title: Recent Advances in Multi-modal 3D Scene Understanding: A Comprehensive Survey and Evaluation. (arXiv:2310.15676v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15676">http://arxiv.org/abs/2310.15676</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15676]] Recent Advances in Multi-modal 3D Scene Understanding: A Comprehensive Survey and Evaluation(http://arxiv.org/abs/2310.15676)</code></li>
<li>Summary: <p>Multi-modal 3D scene understanding has gained considerable attention due to
its wide applications in many areas, such as autonomous driving and
human-computer interaction. Compared to conventional single-modal 3D
understanding, introducing an additional modality not only elevates the
richness and precision of scene interpretation but also ensures a more robust
and resilient understanding. This becomes especially crucial in varied and
challenging environments where solely relying on 3D data might be inadequate.
While there has been a surge in the development of multi-modal 3D methods over
past three years, especially those integrating multi-camera images (3D+2D) and
textual descriptions (3D+language), a comprehensive and in-depth review is
notably absent. In this article, we present a systematic survey of recent
progress to bridge this gap. We begin by briefly introducing a background that
formally defines various 3D multi-modal tasks and summarizes their inherent
challenges. After that, we present a novel taxonomy that delivers a thorough
categorization of existing methods according to modalities and tasks, exploring
their respective strengths and limitations. Furthermore, comparative results of
recent approaches on several benchmark datasets, together with insightful
analysis, are offered. Finally, we discuss the unresolved issues and provide
several potential avenues for future research.
</p></li>
</ul>

<h3>Title: Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards. (arXiv:2310.15259v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15259">http://arxiv.org/abs/2310.15259</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15259]] Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards(http://arxiv.org/abs/2310.15259)</code></li>
<li>Summary: <p>Community Question-Answering (CQA) portals serve as a valuable tool for
helping users within an organization. However, making them accessible to
non-English-speaking users continues to be a challenge. Translating questions
can broaden the community's reach, benefiting individuals with similar
inquiries in various languages. Translating questions using Neural Machine
Translation (NMT) poses more challenges, especially in noisy environments,
where the grammatical correctness of the questions is not monitored. These
questions may be phrased as statements by non-native speakers, with incorrect
subject-verb order and sometimes even missing question marks. Creating a
synthetic parallel corpus from such data is also difficult due to its noisy
nature. To address this issue, we propose a training methodology that
fine-tunes the NMT system only using source-side data. Our approach balances
adequacy and fluency by utilizing a loss function that combines BERTScore and
Masked Language Model (MLM) Score. Our method surpasses the conventional
Maximum Likelihood Estimation (MLE) based fine-tuning approach, which relies on
synthetic target data, by achieving a 1.9 BLEU score improvement. Our model
exhibits robustness while we add noise to our baseline, and still achieve 1.1
BLEU improvement and large improvements on TER and BLEURT metrics. Our proposed
methodology is model-agnostic and is only necessary during the training phase.
We make the codes and datasets publicly available at
\url{https://www.iitp.ac.in/~ai-nlp-ml/resources.html#DomainAdapt} for
facilitating further research.
</p></li>
</ul>

<h3>Title: Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey. (arXiv:2310.15264v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15264">http://arxiv.org/abs/2310.15264</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15264]] Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey(http://arxiv.org/abs/2310.15264)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have revolutionized the domain of natural
language processing (NLP) with remarkable capabilities of generating human-like
text responses. However, despite these advancements, several works in the
existing literature have raised serious concerns about the potential misuse of
LLMs such as spreading misinformation, generating fake news, plagiarism in
academia, and contaminating the web. To address these concerns, a consensus
among the research community is to develop algorithmic solutions to detect
AI-generated text. The basic idea is that whenever we can tell if the given
text is either written by a human or an AI, we can utilize this information to
address the above-mentioned concerns. To that end, a plethora of detection
frameworks have been proposed, highlighting the possibilities of AI-generated
text detection. But in parallel to the development of detection frameworks,
researchers have also concentrated on designing strategies to elude detection,
i.e., focusing on the impossibilities of AI-generated text detection. This is a
crucial step in order to make sure the detection frameworks are robust enough
and it is not too easy to fool a detector. Despite the huge interest and the
flurry of research in this domain, the community currently lacks a
comprehensive analysis of recent developments. In this survey, we aim to
provide a concise categorization and overview of current work encompassing both
the prospects and the limitations of AI-generated text detection. To enrich the
collective knowledge, we engage in an exhaustive discussion on critical and
challenging open questions related to ongoing research on AI-generated text
detection.
</p></li>
</ul>

<h3>Title: TaskDiff: A Similarity Metric for Task-Oriented Conversations. (arXiv:2310.15298v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15298">http://arxiv.org/abs/2310.15298</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15298]] TaskDiff: A Similarity Metric for Task-Oriented Conversations(http://arxiv.org/abs/2310.15298)</code></li>
<li>Summary: <p>The popularity of conversational digital assistants has resulted in the
availability of large amounts of conversational data which can be utilized for
improved user experience and personalized response generation. Building these
assistants using popular large language models like ChatGPT also require
additional emphasis on prompt engineering and evaluation methods. Textual
similarity metrics are a key ingredient for such analysis and evaluations.
While many similarity metrics have been proposed in the literature, they have
not proven effective for task-oriented conversations as they do not take
advantage of unique conversational features. To address this gap, we present
TaskDiff, a novel conversational similarity metric that utilizes different
dialogue components (utterances, intents, and slots) and their distributions to
compute similarity. Extensive experimental evaluation of TaskDiff on a
benchmark dataset demonstrates its superior performance and improved robustness
over other related approaches.
</p></li>
</ul>

<h3>Title: EpiK-Eval: Evaluation for Language Models as Epistemic Models. (arXiv:2310.15372v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15372">http://arxiv.org/abs/2310.15372</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15372]] EpiK-Eval: Evaluation for Language Models as Epistemic Models(http://arxiv.org/abs/2310.15372)</code></li>
<li>Summary: <p>In the age of artificial intelligence, the role of large language models
(LLMs) is becoming increasingly central. Despite their growing prevalence,
their capacity to consolidate knowledge from different training documents - a
crucial ability in numerous applications - remains unexplored. This paper
presents the first study examining the capability of LLMs to effectively
combine such information within their parameter space. We introduce EpiK-Eval,
a novel question-answering benchmark tailored to evaluate LLMs' proficiency in
formulating a coherent and consistent knowledge representation from segmented
narratives. Evaluations across various LLMs reveal significant weaknesses in
this domain. We contend that these shortcomings stem from the intrinsic nature
of prevailing training objectives. Consequently, we advocate for refining the
approach towards knowledge consolidation, as it harbors the potential to
dramatically improve their overall effectiveness and performance. The findings
from this study offer insights for developing more robust and reliable LLMs.
Our code and benchmark are available at
https://github.com/chandar-lab/EpiK-Eval
</p></li>
</ul>

<h3>Title: NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA. (arXiv:2310.15484v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15484">http://arxiv.org/abs/2310.15484</a></li>
<li>Code URL: https://github.com/mlvlab/nutrea</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15484]] NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA(http://arxiv.org/abs/2310.15484)</code></li>
<li>Summary: <p>Multi-hop Knowledge Graph Question Answering (KGQA) is a task that involves
retrieving nodes from a knowledge graph (KG) to answer natural language
questions. Recent GNN-based approaches formulate this task as a KG path
searching problem, where messages are sequentially propagated from the seed
node towards the answer nodes. However, these messages are past-oriented, and
they do not consider the full KG context. To make matters worse, KG nodes often
represent proper noun entities and are sometimes encrypted, being uninformative
in selecting between paths. To address these problems, we propose Neural Tree
Search (NuTrea), a tree search-based GNN model that incorporates the broader KG
context. Our model adopts a message-passing scheme that probes the unreached
subtree regions to boost the past-oriented embeddings. In addition, we
introduce the Relation Frequency-Inverse Entity Frequency (RF-IEF) node
embedding that considers the global KG context to better characterize ambiguous
KG nodes. The general effectiveness of our approach is demonstrated through
experiments on three major multi-hop KGQA benchmark datasets, and our extensive
analyses further validate its expressiveness and robustness. Overall, NuTrea
provides a powerful means to query the KG with complex natural language
questions. Code is available at https://github.com/mlvlab/NuTrea.
</p></li>
</ul>

<h3>Title: Light up that Droid! On the Effectiveness of Static Analysis Features against App Obfuscation for Android Malware Detection. (arXiv:2310.15645v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15645">http://arxiv.org/abs/2310.15645</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15645]] Light up that Droid! On the Effectiveness of Static Analysis Features against App Obfuscation for Android Malware Detection(http://arxiv.org/abs/2310.15645)</code></li>
<li>Summary: <p>Malware authors have seen obfuscation as the mean to bypass malware detectors
based on static analysis features. For Android, several studies have confirmed
that many anti-malware products are easily evaded with simple program
transformations. As opposed to these works, ML detection proposals for Android
leveraging static analysis features have also been proposed as
obfuscation-resilient. Therefore, it needs to be determined to what extent the
use of a specific obfuscation strategy or tool poses a risk for the validity of
ML malware detectors for Android based on static analysis features. To shed
some light in this regard, in this article we assess the impact of specific
obfuscation techniques on common features extracted using static analysis and
determine whether the changes are significant enough to undermine the
effectiveness of ML malware detectors that rely on these features. The
experimental results suggest that obfuscation techniques affect all static
analysis features to varying degrees across different tools. However, certain
features retain their validity for ML malware detection even in the presence of
obfuscation. Based on these findings, we propose a ML malware detector for
Android that is robust against obfuscation and outperforms current
state-of-the-art detectors.
</p></li>
</ul>

<h3>Title: Can strong structural encoding reduce the importance of Message Passing?. (arXiv:2310.15197v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15197">http://arxiv.org/abs/2310.15197</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15197]] Can strong structural encoding reduce the importance of Message Passing?(http://arxiv.org/abs/2310.15197)</code></li>
<li>Summary: <p>The most prevalent class of neural networks operating on graphs are message
passing neural networks (MPNNs), in which the representation of a node is
updated iteratively by aggregating information in the 1-hop neighborhood. Since
this paradigm for computing node embeddings may prevent the model from learning
coarse topological structures, the initial features are often augmented with
structural information of the graph, typically in the form of Laplacian
eigenvectors or Random Walk transition probabilities. In this work, we explore
the contribution of message passing when strong structural encodings are
provided. We introduce a novel way of modeling the interaction between feature
and structural information based on their tensor product rather than the
standard concatenation. The choice of interaction is compared in common
scenarios and in settings where the capacity of the message-passing layer is
severely reduced and ultimately the message-passing phase is removed
altogether. Our results indicate that using tensor-based encodings is always at
least on par with the concatenation-based encoding and that it makes the model
much more robust when the message passing layers are removed, on some tasks
incurring almost no drop in performance. This suggests that the importance of
message passing is limited when the model can construct strong structural
encodings.
</p></li>
</ul>

<h3>Title: ADMM Training Algorithms for Residual Networks: Convergence, Complexity and Parallel Training. (arXiv:2310.15334v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15334">http://arxiv.org/abs/2310.15334</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15334]] ADMM Training Algorithms for Residual Networks: Convergence, Complexity and Parallel Training(http://arxiv.org/abs/2310.15334)</code></li>
<li>Summary: <p>We design a series of serial and parallel proximal point (gradient) ADMMs for
the fully connected residual networks (FCResNets) training problem by
introducing auxiliary variables. Convergence of the proximal point version is
proven based on a Kurdyka-Lojasiewicz (KL) property analysis framework, and we
can ensure a locally R-linear or sublinear convergence rate depending on the
different ranges of the Kurdyka-Lojasiewicz (KL) exponent, in which a necessary
auxiliary function is constructed to realize our goal. Moreover, the advantages
of the parallel implementation in terms of lower time complexity and less
(per-node) memory consumption are analyzed theoretically. To the best of our
knowledge, this is the first work analyzing the convergence, convergence rate,
time complexity and (per-node) runtime memory requirement of the ADMM applied
in the FCResNets training problem theoretically. Experiments are reported to
show the high speed, better performance, robustness and potential in the deep
network training tasks. Finally, we present the advantage and potential of our
parallel training in large-scale problems.
</p></li>
</ul>

<h3>Title: Robust Learning via Conditional Prevalence Adjustment. (arXiv:2310.15766v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15766">http://arxiv.org/abs/2310.15766</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15766]] Robust Learning via Conditional Prevalence Adjustment(http://arxiv.org/abs/2310.15766)</code></li>
<li>Summary: <p>Healthcare data often come from multiple sites in which the correlations
between confounding variables can vary widely. If deep learning models exploit
these unstable correlations, they might fail catastrophically in unseen sites.
Although many methods have been proposed to tackle unstable correlations, each
has its limitations. For example, adversarial training forces models to
completely ignore unstable correlations, but doing so may lead to poor
predictive performance. Other methods (e.g. Invariant risk minimization [4])
try to learn domain-invariant representations that rely only on stable
associations by assuming a causal data-generating process (input X causes class
label Y ). Thus, they may be ineffective for anti-causal tasks (Y causes X),
which are common in computer vision. We propose a method called CoPA
(Conditional Prevalence-Adjustment) for anti-causal tasks. CoPA assumes that
(1) generation mechanism is stable, i.e. label Y and confounding variable(s) Z
generate X, and (2) the unstable conditional prevalence in each site E fully
accounts for the unstable correlations between X and Y . Our crucial
observation is that confounding variables are routinely recorded in healthcare
settings and the prevalence can be readily estimated, for example, from a set
of (Y, Z) samples (no need for corresponding samples of X). CoPA can work even
if there is a single training site, a scenario which is often overlooked by
existing methods. Our experiments on synthetic and real data show CoPA beating
competitive baselines.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Probing Representations for Document-level Event Extraction. (arXiv:2310.15316v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15316">http://arxiv.org/abs/2310.15316</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15316]] Probing Representations for Document-level Event Extraction(http://arxiv.org/abs/2310.15316)</code></li>
<li>Summary: <p>The probing classifiers framework has been employed for interpreting deep
neural network models for a variety of natural language processing (NLP)
applications. Studies, however, have largely focused on sentencelevel NLP
tasks. This work is the first to apply the probing paradigm to representations
learned for document-level information extraction (IE). We designed eight
embedding probes to analyze surface, semantic, and event-understanding
capabilities relevant to document-level event extraction. We apply them to the
representations acquired by learning models from three different LLM-based
document-level IE approaches on a standard dataset. We found that trained
encoders from these models yield embeddings that can modestly improve argument
detections and labeling but only slightly enhance event-level tasks, albeit
trade-offs in information helpful for coherence and event-type prediction. We
further found that encoder models struggle with document length and
cross-sentence discourse.
</p></li>
</ul>

<h3>Title: Continual Event Extraction with Semantic Confusion Rectification. (arXiv:2310.15470v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15470">http://arxiv.org/abs/2310.15470</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15470]] Continual Event Extraction with Semantic Confusion Rectification(http://arxiv.org/abs/2310.15470)</code></li>
<li>Summary: <p>We study continual event extraction, which aims to extract incessantly
emerging event information while avoiding forgetting. We observe that the
semantic confusion on event types stems from the annotations of the same text
being updated over time. The imbalance between event types even aggravates this
issue. This paper proposes a novel continual event extraction model with
semantic confusion rectification. We mark pseudo labels for each sentence to
alleviate semantic confusion. We transfer pivotal knowledge between current and
previous models to enhance the understanding of event types. Moreover, we
encourage the model to focus on the semantics of long-tailed event types by
leveraging other associated types. Experimental results show that our model
outperforms state-of-the-art baselines and is proficient in imbalanced
datasets.
</p></li>
</ul>

<h3>Title: MuLMS: A Multi-Layer Annotated Text Corpus for Information Extraction in the Materials Science Domain. (arXiv:2310.15569v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15569">http://arxiv.org/abs/2310.15569</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15569]] MuLMS: A Multi-Layer Annotated Text Corpus for Information Extraction in the Materials Science Domain(http://arxiv.org/abs/2310.15569)</code></li>
<li>Summary: <p>Keeping track of all relevant recent publications and experimental results
for a research area is a challenging task. Prior work has demonstrated the
efficacy of information extraction models in various scientific areas.
Recently, several datasets have been released for the yet understudied
materials science domain. However, these datasets focus on sub-problems such as
parsing synthesis procedures or on sub-domains, e.g., solid oxide fuel cells.
In this resource paper, we present MuLMS, a new dataset of 50 open-access
articles, spanning seven sub-domains of materials science. The corpus has been
annotated by domain experts with several layers ranging from named entities
over relations to frame structures. We present competitive neural models for
all tasks and demonstrate that multi-task training with existing related
resources leads to benefits.
</p></li>
</ul>

<h3>Title: Natural Language Processing for Drug Discovery Knowledge Graphs: promises and pitfalls. (arXiv:2310.15572v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15572">http://arxiv.org/abs/2310.15572</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15572]] Natural Language Processing for Drug Discovery Knowledge Graphs: promises and pitfalls(http://arxiv.org/abs/2310.15572)</code></li>
<li>Summary: <p>Building and analysing knowledge graphs (KGs) to aid drug discovery is a
topical area of research. A salient feature of KGs is their ability to combine
many heterogeneous data sources in a format that facilitates discovering
connections. The utility of KGs has been exemplified in areas such as drug
repurposing, with insights made through manual exploration and modelling of the
data. In this article, we discuss promises and pitfalls of using natural
language processing (NLP) to mine unstructured text typically from scientific
literature as a data source for KGs. This draws on our experience of initially
parsing structured data sources such as ChEMBL as the basis for data within a
KG, and then enriching or expanding upon them using NLP. The fundamental
promise of NLP for KGs is the automated extraction of data from millions of
documents a task practically impossible to do via human curation alone.
However, there are many potential pitfalls in NLP-KG pipelines such as
incorrect named entity recognition and ontology linking all of which could
ultimately lead to erroneous inferences and conclusions.
</p></li>
</ul>

<h3>Title: CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction. (arXiv:2310.15577v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15577">http://arxiv.org/abs/2310.15577</a></li>
<li>Code URL: https://github.com/nitkannen/contraste</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15577]] CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction(http://arxiv.org/abs/2310.15577)</code></li>
<li>Summary: <p>Existing works on Aspect Sentiment Triplet Extraction (ASTE) explicitly focus
on developing more efficient fine-tuning techniques for the task. Instead, our
motivation is to come up with a generic approach that can improve the
downstream performances of multiple ABSA tasks simultaneously. Towards this, we
present CONTRASTE, a novel pre-training strategy using CONTRastive learning to
enhance the ASTE performance. While we primarily focus on ASTE, we also
demonstrate the advantage of our proposed technique on other ABSA tasks such as
ACOS, TASD, and AESC. Given a sentence and its associated (aspect, opinion,
sentiment) triplets, first, we design aspect-based prompts with corresponding
sentiments masked. We then (pre)train an encoder-decoder model by applying
contrastive learning on the decoder-generated aspect-aware sentiment
representations of the masked terms. For fine-tuning the model weights thus
obtained, we then propose a novel multi-task approach where the base
encoder-decoder model is combined with two complementary modules, a
tagging-based Opinion Term Detector, and a regression-based Triplet Count
Estimator. Exhaustive experiments on four benchmark datasets and a detailed
ablation study establish the importance of each of our proposed components as
we achieve new state-of-the-art ASTE results.
</p></li>
</ul>

<h3>Title: Towards Automated Recipe Genre Classification using Semi-Supervised Learning. (arXiv:2310.15693v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15693">http://arxiv.org/abs/2310.15693</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15693]] Towards Automated Recipe Genre Classification using Semi-Supervised Learning(http://arxiv.org/abs/2310.15693)</code></li>
<li>Summary: <p>Sharing cooking recipes is a great way to exchange culinary ideas and provide
instructions for food preparation. However, categorizing raw recipes found
online into appropriate food genres can be challenging due to a lack of
adequate labeled data. In this study, we present a dataset named the
``Assorted, Archetypal, and Annotated Two Million Extended (3A2M+) Cooking
Recipe Dataset" that contains two million culinary recipes labeled in
respective categories with extended named entities extracted from recipe
descriptions. This collection of data includes various features such as title,
NER, directions, and extended NER, as well as nine different labels
representing genres including bakery, drinks, non-veg, vegetables, fast food,
cereals, meals, sides, and fusions. The proposed pipeline named 3A2M+ extends
the size of the Named Entity Recognition (NER) list to address missing named
entities like heat, time or process from the recipe directions using two NER
extraction tools. 3A2M+ dataset provides a comprehensive solution to the
various challenging recipe-related tasks, including classification, named
entity recognition, and recipe generation. Furthermore, we have demonstrated
traditional machine learning, deep learning and pre-trained language models to
classify the recipes into their corresponding genre and achieved an overall
accuracy of 98.6\%. Our investigation indicates that the title feature played a
more significant role in classifying the genre.
</p></li>
</ul>

<h3>Title: RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction. (arXiv:2310.15743v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15743">http://arxiv.org/abs/2310.15743</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15743]] RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction(http://arxiv.org/abs/2310.15743)</code></li>
<li>Summary: <p>How to identify semantic relations among entities in a document when only a
few labeled documents are available? Few-shot document-level relation
extraction (FSDLRE) is crucial for addressing the pervasive data scarcity
problem in real-world scenarios. Metric-based meta-learning is an effective
framework widely adopted for FSDLRE, which constructs class prototypes for
classification. However, existing works often struggle to obtain class
prototypes with accurate relational semantics: 1) To build prototype for a
target relation type, they aggregate the representations of all entity pairs
holding that relation, while these entity pairs may also hold other relations,
thus disturbing the prototype. 2) They use a set of generic NOTA
(none-of-the-above) prototypes across all tasks, neglecting that the NOTA
semantics differs in tasks with different target relation types. In this paper,
we propose a relation-aware prototype learning method for FSDLRE to strengthen
the relational semantics of prototype representations. By judiciously
leveraging the relation descriptions and realistic NOTA instances as guidance,
our method effectively refines the relation prototypes and generates
task-specific NOTA prototypes. Extensive experiments demonstrate that our
method outperforms state-of-the-art approaches by average 2.61% $F_1$ across
various settings of two FSDLRE benchmarks.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer's Disease. (arXiv:2310.15301v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15301">http://arxiv.org/abs/2310.15301</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15301]] ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer's Disease(http://arxiv.org/abs/2310.15301)</code></li>
<li>Summary: <p>Alzheimer's Disease (AD) and related dementia are a growing global health
challenge due to the aging population. In this paper, we present ADMarker, the
first end-to-end system that integrates multi-modal sensors and new federated
learning algorithms for detecting multidimensional AD digital biomarkers in
natural living environments. ADMarker features a novel three-stage multi-modal
federated learning architecture that can accurately detect digital biomarkers
in a privacy-preserving manner. Our approach collectively addresses several
major real-world challenges, such as limited data labels, data heterogeneity,
and limited computing resources. We built a compact multi-modality hardware
system and deployed it in a four-week clinical trial involving 91 elderly
participants. The results indicate that ADMarker can accurately detect a
comprehensive set of digital biomarkers with up to 93.8% accuracy and identify
early AD with an average of 88.9% accuracy. ADMarker offers a new platform that
can allow AD clinicians to characterize and track the complex correlation
between multidimensional interpretable digital biomarkers, demographic factors
of patients, and AD diagnosis in a longitudinal manner.
</p></li>
</ul>

<h3>Title: Serverless Federated Learning with flwr-serverless. (arXiv:2310.15329v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15329">http://arxiv.org/abs/2310.15329</a></li>
<li>Code URL: https://github.com/kungfuai/flwr_serverless</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15329]] Serverless Federated Learning with flwr-serverless(http://arxiv.org/abs/2310.15329)</code></li>
<li>Summary: <p>Federated learning is becoming increasingly relevant and popular as we
witness a surge in data collection and storage of personally identifiable
information. Alongside these developments there have been many proposals from
governments around the world to provide more protections for individuals' data
and a heightened interest in data privacy measures. As deep learning continues
to become more relevant in new and existing domains, it is vital to develop
strategies like federated learning that can effectively train data from
different sources, such as edge devices, without compromising security and
privacy. Recently, the Flower (\texttt{Flwr}) Python package was introduced to
provide a scalable, flexible, and easy-to-use framework for implementing
federated learning. However, to date, Flower is only able to run synchronous
federated learning which can be costly and time-consuming to run because the
process is bottlenecked by client-side training jobs that are slow or fragile.
Here, we introduce \texttt{flwr-serverless}, a wrapper around the Flower
package that extends its functionality to allow for both synchronous and
asynchronous federated learning with minimal modification to Flower's design
paradigm. Furthermore, our approach to federated learning allows the process to
run without a central server, which increases the domains of application and
accessibility of its use. This paper presents the design details and usage of
this approach through a series of experiments that were conducted using public
datasets. Overall, we believe that our approach decreases the time and cost to
run federated training and provides an easier way to implement and experiment
with federated learning systems.
</p></li>
</ul>

<h3>Title: Accelerating Split Federated Learning over Wireless Communication Networks. (arXiv:2310.15584v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15584">http://arxiv.org/abs/2310.15584</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15584]] Accelerating Split Federated Learning over Wireless Communication Networks(http://arxiv.org/abs/2310.15584)</code></li>
<li>Summary: <p>The development of artificial intelligence (AI) provides opportunities for
the promotion of deep neural network (DNN)-based applications. However, the
large amount of parameters and computational complexity of DNN makes it
difficult to deploy it on edge devices which are resource-constrained. An
efficient method to address this challenge is model partition/splitting, in
which DNN is divided into two parts which are deployed on device and server
respectively for co-training or co-inference. In this paper, we consider a
split federated learning (SFL) framework that combines the parallel model
training mechanism of federated learning (FL) and the model splitting structure
of split learning (SL). We consider a practical scenario of heterogeneous
devices with individual split points of DNN. We formulate a joint problem of
split point selection and bandwidth allocation to minimize the system latency.
By using alternating optimization, we decompose the problem into two
sub-problems and solve them optimally. Experiment results demonstrate the
superiority of our work in latency reduction and accuracy improvement.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: "One-size-fits-all"? Observations and Expectations of NLG Systems Across Identity-Related Language Features. (arXiv:2310.15398v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15398">http://arxiv.org/abs/2310.15398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15398]] "One-size-fits-all"? Observations and Expectations of NLG Systems Across Identity-Related Language Features(http://arxiv.org/abs/2310.15398)</code></li>
<li>Summary: <p>Fairness-related assumptions about what constitutes appropriate NLG system
behaviors range from invariance, where systems are expected to respond
identically to social groups, to adaptation, where responses should instead
vary across them. We design and conduct five case studies, in which we perturb
different types of identity-related language features (names, roles, locations,
dialect, and style) in NLG system inputs to illuminate tensions around
invariance and adaptation. We outline people's expectations of system
behaviors, and surface potential caveats of these two contrasting yet
commonly-held assumptions. We find that motivations for adaptation include
social norms, cultural differences, feature-specific information, and
accommodation; motivations for invariance include perspectives that favor
prescriptivism, view adaptation as unnecessary or too difficult for NLG systems
to do appropriately, and are wary of false assumptions. Our findings highlight
open challenges around defining what constitutes fair NLG system behavior.
</p></li>
</ul>

<h3>Title: MUSER: A Multi-View Similar Case Retrieval Dataset. (arXiv:2310.15602v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15602">http://arxiv.org/abs/2310.15602</a></li>
<li>Code URL: https://github.com/thulawtech/muser</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15602]] MUSER: A Multi-View Similar Case Retrieval Dataset(http://arxiv.org/abs/2310.15602)</code></li>
<li>Summary: <p>Similar case retrieval (SCR) is a representative legal AI application that
plays a pivotal role in promoting judicial fairness. However, existing SCR
datasets only focus on the fact description section when judging the similarity
between cases, ignoring other valuable sections (e.g., the court's opinion)
that can provide insightful reasoning process behind. Furthermore, the case
similarities are typically measured solely by the textual semantics of the fact
descriptions, which may fail to capture the full complexity of legal cases from
the perspective of legal knowledge. In this work, we present MUSER, a similar
case retrieval dataset based on multi-view similarity measurement and
comprehensive legal element with sentence-level legal element annotations.
Specifically, we select three perspectives (legal fact, dispute focus, and law
statutory) and build a comprehensive and structured label schema of legal
elements for each of them, to enable accurate and knowledgeable evaluation of
case similarities. The constructed dataset originates from Chinese civil cases
and contains 100 query cases and 4,024 candidate cases. We implement several
text classification algorithms for legal element prediction and various
retrieval methods for retrieving similar cases on MUSER. The experimental
results indicate that incorporating legal elements can benefit the performance
of SCR models, but further efforts are still required to address the remaining
challenges posed by MUSER. The source code and dataset are released at
https://github.com/THUlawtech/MUSER.
</p></li>
</ul>

<h3>Title: Learning Fair Representations with High-Confidence Guarantees. (arXiv:2310.15358v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15358">http://arxiv.org/abs/2310.15358</a></li>
<li>Code URL: https://github.com/jamesluoyh/frg</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15358]] Learning Fair Representations with High-Confidence Guarantees(http://arxiv.org/abs/2310.15358)</code></li>
<li>Summary: <p>Representation learning is increasingly employed to generate representations
that are predictive across multiple downstream tasks. The development of
representation learning algorithms that provide strong fairness guarantees is
thus important because it can prevent unfairness towards disadvantaged groups
for all downstream prediction tasks. To prevent unfairness towards
disadvantaged groups in all downstream tasks, it is crucial to provide
representation learning algorithms that provide fairness guarantees. In this
paper, we formally define the problem of learning representations that are fair
with high confidence. We then introduce the Fair Representation learning with
high-confidence Guarantees (FRG) framework, which provides high-confidence
guarantees for limiting unfairness across all downstream models and tasks, with
user-defined upper bounds. After proving that FRG ensures fairness for all
downstream models and tasks with high probability, we present empirical
evaluations that demonstrate FRG's effectiveness at upper bounding unfairness
for multiple downstream models and tasks.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Interpretable Medical Image Classification using Prototype Learning and Privileged Information. (arXiv:2310.15741v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15741">http://arxiv.org/abs/2310.15741</a></li>
<li>Code URL: https://github.com/xrad-ulm/proto-caps</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15741]] Interpretable Medical Image Classification using Prototype Learning and Privileged Information(http://arxiv.org/abs/2310.15741)</code></li>
<li>Summary: <p>Interpretability is often an essential requirement in medical imaging.
Advanced deep learning methods are required to address this need for
explainability and high performance. In this work, we investigate whether
additional information available during the training process can be used to
create an understandable and powerful model. We propose an innovative solution
called Proto-Caps that leverages the benefits of capsule networks, prototype
learning and the use of privileged information. Evaluating the proposed
solution on the LIDC-IDRI dataset shows that it combines increased
interpretability with above state-of-the-art prediction performance. Compared
to the explainable baseline model, our method achieves more than 6 % higher
accuracy in predicting both malignancy (93.0 %) and mean characteristic
features of lung nodules. Simultaneously, the model provides case-based
reasoning with prototype representations that allow visual validation of
radiologist-defined attributes.
</p></li>
</ul>

<h3>Title: Estimating Trustworthy and Safe Optimal Treatment Regimes. (arXiv:2310.15333v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15333">http://arxiv.org/abs/2310.15333</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15333]] Estimating Trustworthy and Safe Optimal Treatment Regimes(http://arxiv.org/abs/2310.15333)</code></li>
<li>Summary: <p>Recent statistical and reinforcement learning methods have significantly
advanced patient care strategies. However, these approaches face substantial
challenges in high-stakes contexts, including missing data, inherent
stochasticity, and the critical requirements for interpretability and patient
safety. Our work operationalizes a safe and interpretable framework to identify
optimal treatment regimes. This approach involves matching patients with
similar medical and pharmacological characteristics, allowing us to construct
an optimal policy via interpolation. We perform a comprehensive simulation
study to demonstrate the framework's ability to identify optimal policies even
in complex settings. Ultimately, we operationalize our approach to study
regimes for treating seizures in critically ill patients. Our findings strongly
support personalized treatment strategies based on a patient's medical history
and pharmacological features. Notably, we identify that reducing medication
doses for patients with mild and brief seizure episodes while adopting
aggressive treatment for patients in intensive care unit experiencing intense
seizures leads to more favorable outcomes.
</p></li>
</ul>

<h3>Title: Interpretable Survival Analysis for Heart Failure Risk Prediction. (arXiv:2310.15472v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15472">http://arxiv.org/abs/2310.15472</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15472]] Interpretable Survival Analysis for Heart Failure Risk Prediction(http://arxiv.org/abs/2310.15472)</code></li>
<li>Summary: <p>Survival analysis, or time-to-event analysis, is an important and widespread
problem in healthcare research. Medical research has traditionally relied on
Cox models for survival analysis, due to their simplicity and interpretability.
Cox models assume a log-linear hazard function as well as proportional hazards
over time, and can perform poorly when these assumptions fail. Newer survival
models based on machine learning avoid these assumptions and offer improved
accuracy, yet sometimes at the expense of model interpretability, which is
vital for clinical use. We propose a novel survival analysis pipeline that is
both interpretable and competitive with state-of-the-art survival models.
Specifically, we use an improved version of survival stacking to transform a
survival analysis problem to a classification problem, ControlBurn to perform
feature selection, and Explainable Boosting Machines to generate interpretable
predictions. To evaluate our pipeline, we predict risk of heart failure using a
large-scale EHR database. Our pipeline achieves state-of-the-art performance
and provides interesting and novel insights about risk factors for heart
failure.
</p></li>
</ul>

<h3>Title: Interactive Generalized Additive Model and Its Applications in Electric Load Forecasting. (arXiv:2310.15662v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15662">http://arxiv.org/abs/2310.15662</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15662]] Interactive Generalized Additive Model and Its Applications in Electric Load Forecasting(http://arxiv.org/abs/2310.15662)</code></li>
<li>Summary: <p>Electric load forecasting is an indispensable component of electric power
system planning and management. Inaccurate load forecasting may lead to the
threat of outages or a waste of energy. Accurate electric load forecasting is
challenging when there is limited data or even no data, such as load
forecasting in holiday, or under extreme weather conditions. As high-stakes
decision-making usually follows after load forecasting, model interpretability
is crucial for the adoption of forecasting models. In this paper, we propose an
interactive GAM which is not only interpretable but also can incorporate
specific domain knowledge in electric power industry for improved performance.
This boosting-based GAM leverages piecewise linear functions and can be learned
through our efficient algorithm. In both public benchmark and electricity
datasets, our interactive GAM outperforms current state-of-the-art methods and
demonstrates good generalization ability in the cases of extreme weather
events. We launched a user-friendly web-based tool based on interactive GAM and
already incorporated it into our eForecaster product, a unified AI platform for
electricity forecasting.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Multimodal Representations for Teacher-Guided Compositional Visual Reasoning. (arXiv:2310.15585v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15585">http://arxiv.org/abs/2310.15585</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15585]] Multimodal Representations for Teacher-Guided Compositional Visual Reasoning(http://arxiv.org/abs/2310.15585)</code></li>
<li>Summary: <p>Neural Module Networks (NMN) are a compelling method for visual question
answering, enabling the translation of a question into a program consisting of
a series of reasoning sub-tasks that are sequentially executed on the image to
produce an answer. NMNs provide enhanced explainability compared to integrated
models, allowing for a better understanding of the underlying reasoning
process. To improve the effectiveness of NMNs we propose to exploit features
obtained by a large-scale cross-modal encoder. Also, the current training
approach of NMNs relies on the propagation of module outputs to subsequent
modules, leading to the accumulation of prediction errors and the generation of
false answers. To mitigate this, we introduce an NMN learning strategy
involving scheduled teacher guidance. Initially, the model is fully guided by
the ground-truth intermediate outputs, but gradually transitions to an
autonomous behavior as training progresses. This reduces error accumulation,
thus improving training efficiency and final performance.We demonstrate that by
incorporating cross-modal features and employing more effective training
techniques for NMN, we achieve a favorable balance between performance and
transparency in the reasoning process.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM. (arXiv:2310.15296v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15296">http://arxiv.org/abs/2310.15296</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15296]] DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM(http://arxiv.org/abs/2310.15296)</code></li>
<li>Summary: <p>In the burgeoning field of natural language processing, Neural Topic Models
(NTMs) and Large Language Models (LLMs) have emerged as areas of significant
research interest. Despite this, NTMs primarily utilize contextual embeddings
from LLMs, which are not optimal for clustering or capable for topic
generation. Our study addresses this gap by introducing a novel framework named
Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME).
DeTiME leverages ncoder-Decoder-based LLMs to produce highly clusterable
embeddings that could generate topics that exhibit both superior clusterability
and enhanced semantic coherence compared to existing methods. Additionally, by
exploiting the power of diffusion, our framework also provides the capability
to generate content relevant to the identified topics. This dual functionality
allows users to efficiently produce highly clustered topics and related content
simultaneously. DeTiME's potential extends to generating clustered embeddings
as well. Notably, our proposed framework proves to be efficient to train and
exhibits high adaptability, demonstrating its potential for a wide array of
applications.
</p></li>
</ul>

<h3>Title: ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts. (arXiv:2310.15587v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15587">http://arxiv.org/abs/2310.15587</a></li>
<li>Code URL: https://github.com/dili-lab/scandl</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15587]] ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts(http://arxiv.org/abs/2310.15587)</code></li>
<li>Summary: <p>Eye movements in reading play a crucial role in psycholinguistic research
studying the cognitive mechanisms underlying human language processing. More
recently, the tight coupling between eye movements and cognition has also been
leveraged for language-related machine learning tasks such as the
interpretability, enhancement, and pre-training of language models, as well as
the inference of reader- and text-specific properties. However, scarcity of eye
movement data and its unavailability at application time poses a major
challenge for this line of research. Initially, this problem was tackled by
resorting to cognitive models for synthesizing eye movement data. However, for
the sole purpose of generating human-like scanpaths, purely data-driven
machine-learning-based methods have proven to be more suitable. Following
recent advances in adapting diffusion processes to discrete data, we propose
ScanDL, a novel discrete sequence-to-sequence diffusion model that generates
synthetic scanpaths on texts. By leveraging pre-trained word representations
and jointly embedding both the stimulus text and the fixation sequence, our
model captures multi-modal interactions between the two inputs. We evaluate
ScanDL within- and across-dataset and demonstrate that it significantly
outperforms state-of-the-art scanpath generation methods. Finally, we provide
an extensive psycholinguistic analysis that underlines the model's ability to
exhibit human-like reading behavior. Our implementation is made available at
https://github.com/DiLi-Lab/ScanDL.
</p></li>
</ul>

<h3>Title: A Diffusion Weighted Graph Framework for New Intent Discovery. (arXiv:2310.15836v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15836">http://arxiv.org/abs/2310.15836</a></li>
<li>Code URL: https://github.com/yibai-shi/dwgf</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15836]] A Diffusion Weighted Graph Framework for New Intent Discovery(http://arxiv.org/abs/2310.15836)</code></li>
<li>Summary: <p>New Intent Discovery (NID) aims to recognize both new and known intents from
unlabeled data with the aid of limited labeled data containing only known
intents. Without considering structure relationships between samples, previous
methods generate noisy supervisory signals which cannot strike a balance
between quantity and quality, hindering the formation of new intent clusters
and effective transfer of the pre-training knowledge. To mitigate this
limitation, we propose a novel Diffusion Weighted Graph Framework (DWGF) to
capture both semantic similarities and structure relationships inherent in
data, enabling more sufficient and reliable supervisory signals. Specifically,
for each sample, we diffuse neighborhood relationships along semantic paths
guided by the nearest neighbors for multiple hops to characterize its local
structure discriminately. Then, we sample its positive keys and weigh them
based on semantic similarities and local structures for contrastive learning.
During inference, we further propose Graph Smoothing Filter (GSF) to explicitly
utilize the structure relationships to filter high-frequency noise embodied in
semantically ambiguous samples on the cluster boundary. Extensive experiments
show that our method outperforms state-of-the-art models on all evaluation
metrics across multiple benchmark datasets. Code and data are available at
https://github.com/yibai-shi/DWGF.
</p></li>
</ul>

<h3>Title: Fast and Reliable Generation of EHR Time Series via Diffusion Models. (arXiv:2310.15290v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15290">http://arxiv.org/abs/2310.15290</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15290]] Fast and Reliable Generation of EHR Time Series via Diffusion Models(http://arxiv.org/abs/2310.15290)</code></li>
<li>Summary: <p>Electronic Health Records (EHRs) are rich sources of patient-level data,
including laboratory tests, medications, and diagnoses, offering valuable
resources for medical data analysis. However, concerns about privacy often
restrict access to EHRs, hindering downstream analysis. Researchers have
explored various methods for generating privacy-preserving EHR data. In this
study, we introduce a new method for generating diverse and realistic synthetic
EHR time series data using Denoising Diffusion Probabilistic Models (DDPM). We
conducted experiments on six datasets, comparing our proposed method with seven
existing methods. Our results demonstrate that our approach significantly
outperforms all existing methods in terms of data utility while requiring less
training effort. Our approach also enhances downstream medical data analysis by
providing diverse and realistic synthetic EHR data.
</p></li>
</ul>

<h3>Title: Improving Diffusion Models for ECG Imputation with an Augmented Template Prior. (arXiv:2310.15742v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15742">http://arxiv.org/abs/2310.15742</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15742]] Improving Diffusion Models for ECG Imputation with an Augmented Template Prior(http://arxiv.org/abs/2310.15742)</code></li>
<li>Summary: <p>Pulsative signals such as the electrocardiogram (ECG) are extensively
collected as part of routine clinical care. However, noisy and poor-quality
recordings, leading to missing values, are a major issue for signals collected
using mobile health systems, decreasing the signal quality and affecting the
automated downstream tasks. Recent studies have explored imputation of missing
values for ECG with probabilistic time-series models. Nevertheless, in
comparison with the deterministic models, their performance is still limited,
as the variations across subjects and heart-beat relationships are not
explicitly considered in the training objective. In this work, to improve the
ECG imputation and forecasting accuracy with probabilistic models, we present
an template-guided denoising diffusion probabilistic model, PulseDiff, which is
conditioned an informative prior for a range of health conditions.
Specifically, 1) we first extract a subject-level pulsative template from the
observation as an informative prior of missing values, which captures the
personal characteristics; 2) we then add beat-level stochastic shift terms on
the template for prior augmentation, which considers the beat-level variance of
positioning and amplitude; 3) we finally design a confidence score to consider
the health condition of subject, which ensures our prior is provided in a safe
way. Experiments with the PTBXL dataset reveal PulseDiff improves the
performance of two strong DDPMs baseline models, CSDI and SSSD$^{S4}$,
verifying our method guides the generation of DDPMs while managing the
uncertainty. When combining with SSSD$^{S4}$, our PulseDiff method outperforms
the leading deterministic model for short-interval missing data and is
comparable for long-interval data loss.
</p></li>
</ul>

<h3>Title: Good Better Best: Self-Motivated Imitation Learning for noisy Demonstrations. (arXiv:2310.15815v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15815">http://arxiv.org/abs/2310.15815</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15815]] Good Better Best: Self-Motivated Imitation Learning for noisy Demonstrations(http://arxiv.org/abs/2310.15815)</code></li>
<li>Summary: <p>Imitation Learning (IL) aims to discover a policy by minimizing the
discrepancy between the agent's behavior and expert demonstrations. However, IL
is susceptible to limitations imposed by noisy demonstrations from non-expert
behaviors, presenting a significant challenge due to the lack of supplementary
information to assess their expertise. In this paper, we introduce
Self-Motivated Imitation LEarning (SMILE), a method capable of progressively
filtering out demonstrations collected by policies deemed inferior to the
current policy, eliminating the need for additional information. We utilize the
forward and reverse processes of Diffusion Models to emulate the shift in
demonstration expertise from low to high and vice versa, thereby extracting the
noise information that diffuses expertise. Then, the noise information is
leveraged to predict the diffusion steps between the current policy and
demonstrators, which we theoretically demonstrate its equivalence to their
expertise gap. We further explain in detail how the predicted diffusion steps
are applied to filter out noisy demonstrations in a self-motivated manner and
provide its theoretical grounds. Through empirical evaluations on MuJoCo tasks,
we demonstrate that our method is proficient in learning the expert policy
amidst noisy demonstrations, and effectively filters out demonstrations with
expertise inferior to the current policy.
</p></li>
</ul>

<h3>Title: Discriminator Guidance for Autoregressive Diffusion Models. (arXiv:2310.15817v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15817">http://arxiv.org/abs/2310.15817</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15817]] Discriminator Guidance for Autoregressive Diffusion Models(http://arxiv.org/abs/2310.15817)</code></li>
<li>Summary: <p>We introduce discriminator guidance in the setting of Autoregressive
Diffusion Models. The use of a discriminator to guide a diffusion process has
previously been used for continuous diffusion models, and in this work we
derive ways of using a discriminator together with a pretrained generative
model in the discrete case. First, we show that using an optimal discriminator
will correct the pretrained model and enable exact sampling from the underlying
data distribution. Second, to account for the realistic scenario of using a
sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which
iteratively takes the predictions from the discrimiator into account during the
generation process. We test these approaches on the task of generating
molecular graphs and show how the discriminator improves the generative
performance over using only the pretrained model.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h3>Title: Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data. (arXiv:2310.15890v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15890">http://arxiv.org/abs/2310.15890</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15890]] Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data(http://arxiv.org/abs/2310.15890)</code></li>
<li>Summary: <p>The current state-of-the-art decentralized learning algorithms mostly assume
the data distribution to be Independent and Identically Distributed (IID).
However, in practical scenarios, the distributed datasets can have
significantly heterogeneous data distributions across the agents. In this work,
we present a novel approach for decentralized learning on heterogeneous data,
where data-free knowledge distillation through contrastive loss on
cross-features is utilized to improve performance. Cross-features for a pair of
neighboring agents are the features (i.e., last hidden layer activations)
obtained from the data of an agent with respect to the model parameters of the
other agent. We demonstrate the effectiveness of the proposed technique through
an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10,
CIFAR-100, Fashion MNIST, and ImageNet), model architectures, and network
topologies. Our experiments show that the proposed method achieves superior
performance (0.2-4% improvement in test accuracy) compared to other existing
techniques for decentralized learning on heterogeneous data.
</p></li>
</ul>

<h2>transformer</h2>
<h3>Title: Query-adaptive DETR for Crowded Pedestrian Detection. (arXiv:2310.15725v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15725">http://arxiv.org/abs/2310.15725</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15725]] Query-adaptive DETR for Crowded Pedestrian Detection(http://arxiv.org/abs/2310.15725)</code></li>
<li>Summary: <p>DEtection TRansformer (DETR) and its variants (DETRs) have been successfully
applied to crowded pedestrian detection, which achieved promising performance.
However, we find that, in different degrees of crowded scenes, the number of
DETRs' queries must be adjusted manually, otherwise, the performance would
degrade to varying degrees. In this paper, we first analyze the two current
query generation methods and summarize four guidelines for designing the
adaptive query generation method. Then, we propose Rank-based Adaptive Query
Generation (RAQG) to alleviate the problem. Specifically, we design a rank
prediction head that can predict the rank of the lowest confidence positive
training sample produced by the encoder. Based on the predicted rank, we design
an adaptive selection method that can adaptively select coarse detection
results produced by the encoder to generate queries. Moreover, to train the
rank prediction head better, we propose Soft Gradient L1 Loss. The gradient of
Soft Gradient L1 Loss is continuous, which can describe the relationship
between the loss value and the updated value of model parameters granularly.
Our method is simple and effective, which can be plugged into any DETRs to make
it query-adaptive in theory. The experimental results on Crowdhuman dataset and
Citypersons dataset show that our method can adaptively generate queries for
DETRs and achieve competitive results. Especially, our method achieves
state-of-the-art 39.4% MR on Crowdhuman dataset.
</p></li>
</ul>

<h3>Title: GradSim: Gradient-Based Language Grouping for Effective Multilingual Training. (arXiv:2310.15269v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15269">http://arxiv.org/abs/2310.15269</a></li>
<li>Code URL: https://github.com/boschresearch/gradsim</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15269]] GradSim: Gradient-Based Language Grouping for Effective Multilingual Training(http://arxiv.org/abs/2310.15269)</code></li>
<li>Summary: <p>Most languages of the world pose low-resource challenges to natural language
processing models. With multilingual training, knowledge can be shared among
languages. However, not all languages positively influence each other and it is
an open research question how to select the most suitable set of languages for
multilingual training and avoid negative interference among languages whose
characteristics or data distributions are not compatible. In this paper, we
propose GradSim, a language grouping method based on gradient similarity. Our
experiments on three diverse multilingual benchmark datasets show that it leads
to the largest performance gains compared to other similarity measures and it
is better correlated with cross-lingual model performance. As a result, we set
the new state of the art on AfriSenti, a benchmark dataset for sentiment
analysis on low-resource African languages. In our extensive analysis, we
further reveal that besides linguistic features, the topics of the datasets
play an important role for language grouping and that lower layers of
transformer models encode language-specific features while higher layers
capture task-specific information.
</p></li>
</ul>

<h3>Title: Hallucination Detection for Grounded Instruction Generation. (arXiv:2310.15319v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15319">http://arxiv.org/abs/2310.15319</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15319]] Hallucination Detection for Grounded Instruction Generation(http://arxiv.org/abs/2310.15319)</code></li>
<li>Summary: <p>We investigate the problem of generating instructions to guide humans to
navigate in simulated residential environments. A major issue with current
models is hallucination: they generate references to actions or objects that
are inconsistent with what a human follower would perform or encounter along
the described path. We develop a model that detects these hallucinated
references by adopting a model pre-trained on a large corpus of image-text
pairs, and fine-tuning it with a contrastive loss that separates correct
instructions from instructions containing synthesized hallucinations. Our final
model outperforms several baselines, including using word probability estimated
by the instruction-generation model, and supervised models based on LSTM and
Transformer.
</p></li>
</ul>

<h3>Title: TRAMS: Training-free Memory Selection for Long-range Language Modeling. (arXiv:2310.15494v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15494">http://arxiv.org/abs/2310.15494</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15494]] TRAMS: Training-free Memory Selection for Long-range Language Modeling(http://arxiv.org/abs/2310.15494)</code></li>
<li>Summary: <p>The Transformer architecture is crucial for numerous AI models, but it still
faces challenges in long-range language modeling. Though several specific
transformer architectures have been designed to tackle issues of long-range
dependencies, existing methods like Transformer-XL are plagued by a high
percentage of ineffective memories. In this study, we present a plug-and-play
strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens
participating in attention calculation based on one simple metric. This
strategy allows us to keep tokens that are likely to have a high attention
score with the current queries and ignore the other ones. We have tested our
approach on the word-level benchmark (WikiText-103) and the character-level
benchmark (enwik8), and the results indicate an improvement without having
additional training or adding additional parameters.
</p></li>
</ul>

<h3>Title: Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks. (arXiv:2310.15552v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15552">http://arxiv.org/abs/2310.15552</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15552]] Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks(http://arxiv.org/abs/2310.15552)</code></li>
<li>Summary: <p>Recent research suggests that the feed-forward module within Transformers can
be viewed as a collection of key-value memories, where the keys learn to
capture specific patterns from the input based on the training examples. The
values then combine the output from the 'memories' of the keys to generate
predictions about the next token. This leads to an incremental process of
prediction that gradually converges towards the final token choice near the
output layers. This interesting perspective raises questions about how
multilingual models might leverage this mechanism. Specifically, for
autoregressive models trained on two or more languages, do all neurons (across
layers) respond equally to all languages? No! Our hypothesis centers around the
notion that during pretraining, certain model parameters learn strong
language-specific features, while others learn more language-agnostic (shared
across languages) features. To validate this, we conduct experiments utilizing
parallel corpora of two languages that the model was initially pretrained on.
Our findings reveal that the layers closest to the network's input or output
tend to exhibit more language-specific behaviour compared to the layers in the
middle.
</p></li>
</ul>

<h3>Title: How Much Context Does My Attention-Based ASR System Need?. (arXiv:2310.15672v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15672">http://arxiv.org/abs/2310.15672</a></li>
<li>Code URL: https://github.com/robflynnyh/long-context-asr</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15672]] How Much Context Does My Attention-Based ASR System Need?(http://arxiv.org/abs/2310.15672)</code></li>
<li>Summary: <p>For the task of speech recognition, the use of more than 30 seconds of
acoustic context during training is uncommon, and under-investigated in
literature. In this work, we examine the effect of scaling the sequence length
used to train/evaluate (dense-attention based) acoustic and language models on
speech recognition performance. For these experiments a dataset of roughly
100,000 pseudo-labelled Spotify podcasts is used, with context lengths of 5
seconds to 1 hour being explored. Zero-shot evaluations on long-format datasets
Earnings-22 and Tedlium demonstrate a benefit from training with around 80
seconds of acoustic context, showing up to a 14.9% relative improvement from a
limited context baseline. Furthermore, we perform a system combination with
long-context transformer language models via beam search for a fully
long-context ASR system, with results that are competitive with the current
state-of-the-art.
</p></li>
</ul>

<h3>Title: Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models. (arXiv:2310.15852v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15852">http://arxiv.org/abs/2310.15852</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15852]] Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models(http://arxiv.org/abs/2310.15852)</code></li>
<li>Summary: <p>Numerous studies have demonstrated the ability of neural language models to
learn various linguistic properties without direct supervision. This work takes
an initial step towards exploring the less researched topic of how neural
models discover linguistic properties of words, such as gender, as well as the
rules governing their usage. We propose to use an artificial corpus generated
by a PCFG based on French to precisely control the gender distribution in the
training data and determine under which conditions a model correctly captures
gender information or, on the contrary, appears gender-biased.
</p></li>
</ul>

<h3>Title: General Identifiability and Achievability for Causal Representation Learning. (arXiv:2310.15450v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15450">http://arxiv.org/abs/2310.15450</a></li>
<li>Code URL: https://github.com/bvarici/score-general-id-crl</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15450]] General Identifiability and Achievability for Causal Representation Learning(http://arxiv.org/abs/2310.15450)</code></li>
<li>Summary: <p>This paper focuses on causal representation learning (CRL) under a general
nonparametric causal latent model and a general transformation model that maps
the latent data to the observational data. It establishes
\textbf{identifiability} and \textbf{achievability} results using two hard
\textbf{uncoupled} interventions per node in the latent causal graph. Notably,
one does not know which pair of intervention environments have the same node
intervened (hence, uncoupled environments). For identifiability, the paper
establishes that perfect recovery of the latent causal model and variables is
guaranteed under uncoupled interventions. For achievability, an algorithm is
designed that uses observational and interventional data and recovers the
latent causal model and variables with provable guarantees for the algorithm.
This algorithm leverages score variations across different environments to
estimate the inverse of the transformer and, subsequently, the latent
variables. The analysis, additionally, recovers the existing identifiability
result for two hard \textbf{coupled} interventions, that is when metadata about
the pair of environments that have the same node intervened is known. It is
noteworthy that the existing results on non-parametric identifiability require
assumptions on interventions and additional faithfulness assumptions. This
paper shows that when observational data is available, additional faithfulness
assumptions are unnecessary.
</p></li>
</ul>

<h3>Title: Detecting Intentional AIS Shutdown in Open Sea Maritime Surveillance Using Self-Supervised Deep Learning. (arXiv:2310.15586v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15586">http://arxiv.org/abs/2310.15586</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15586]] Detecting Intentional AIS Shutdown in Open Sea Maritime Surveillance Using Self-Supervised Deep Learning(http://arxiv.org/abs/2310.15586)</code></li>
<li>Summary: <p>In maritime traffic surveillance, detecting illegal activities, such as
illegal fishing or transshipment of illicit products is a crucial task of the
coastal administration. In the open sea, one has to rely on Automatic
Identification System (AIS) message transmitted by on-board transponders, which
are captured by surveillance satellites. However, insincere vessels often
intentionally shut down their AIS transponders to hide illegal activities. In
the open sea, it is very challenging to differentiate intentional AIS shutdowns
from missing reception due to protocol limitations, bad weather conditions or
restricting satellite positions. This paper presents a novel approach for the
detection of abnormal AIS missing reception based on self-supervised deep
learning techniques and transformer models. Using historical data, the trained
model predicts if a message should be received in the upcoming minute or not.
Afterwards, the model reports on detected anomalies by comparing the prediction
with what actually happens. Our method can process AIS messages in real-time,
in particular, more than 500 Millions AIS messages per month, corresponding to
the trajectories of more than 60 000 ships. The method is evaluated on 1-year
of real-world data coming from four Norwegian surveillance satellites. Using
related research results, we validated our method by rediscovering already
detected intentional AIS shutdowns.
</p></li>
</ul>

<h3>Title: Recurrent Linear Transformers. (arXiv:2310.15719v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15719">http://arxiv.org/abs/2310.15719</a></li>
<li>Code URL: https://github.com/subho406/Recurrent-Linear-Transformers</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15719]] Recurrent Linear Transformers(http://arxiv.org/abs/2310.15719)</code></li>
<li>Summary: <p>The self-attention mechanism in the transformer architecture is capable of
capturing long-range dependencies and it is the main reason behind its
effectiveness in processing sequential data. Nevertheless, despite their
success, transformers have two significant drawbacks that still limit their
broader applicability: (1) In order to remember past information, the
self-attention mechanism requires access to the whole history to be provided as
context. (2) The inference cost in transformers is expensive. In this paper we
introduce recurrent alternatives to the transformer self-attention mechanism
that offer a context-independent inference cost, leverage long-range
dependencies effectively, and perform well in practice. We evaluate our
approaches in reinforcement learning problems where the aforementioned
computational limitations make the application of transformers nearly
infeasible. We quantify the impact of the different components of our
architecture in a diagnostic environment and assess performance gains in 2D and
3D pixel-based partially-observable environments. When compared to a
state-of-the-art architecture, GTrXL, inference in our approach is at least 40%
cheaper while reducing memory use in more than 50%. Our approach either
performs similarly or better than GTrXL, improving more than 37% upon GTrXL
performance on harder tasks.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Videoprompter: an ensemble of foundational models for zero-shot video understanding. (arXiv:2310.15324v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15324">http://arxiv.org/abs/2310.15324</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15324]] Videoprompter: an ensemble of foundational models for zero-shot video understanding(http://arxiv.org/abs/2310.15324)</code></li>
<li>Summary: <p>Vision-language models (VLMs) classify the query video by calculating a
similarity score between the visual features and text-based class label
representations. Recently, large language models (LLMs) have been used to
enrich the text-based class labels by enhancing the descriptiveness of the
class names. However, these improvements are restricted to the text-based
classifier only, and the query visual features are not considered. In this
paper, we propose a framework which combines pre-trained discriminative VLMs
with pre-trained generative video-to-text and text-to-text models. We introduce
two key modifications to the standard zero-shot setting. First, we propose
language-guided visual feature enhancement and employ a video-to-text model to
convert the query video to its descriptive form. The resulting descriptions
contain vital visual cues of the query video, such as what objects are present
and their spatio-temporal interactions. These descriptive cues provide
additional semantic knowledge to VLMs to enhance their zeroshot performance.
Second, we propose video-specific prompts to LLMs to generate more meaningful
descriptions to enrich class label representations. Specifically, we introduce
prompt techniques to create a Tree Hierarchy of Categories for class names,
offering a higher-level action context for additional visual cues, We
demonstrate the effectiveness of our approach in video understanding across
three different zero-shot settings: 1) video action recognition, 2)
video-to-text and textto-video retrieval, and 3) time-sensitive video tasks.
Consistent improvements across multiple benchmarks and with various VLMs
demonstrate the effectiveness of our proposed framework. Our code will be made
publicly available.
</p></li>
</ul>

<h3>Title: Nighttime Thermal Infrared Image Colorization with Feedback-based Object Appearance Learning. (arXiv:2310.15688v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15688">http://arxiv.org/abs/2310.15688</a></li>
<li>Code URL: https://github.com/fuyaluo/foalgan</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15688]] Nighttime Thermal Infrared Image Colorization with Feedback-based Object Appearance Learning(http://arxiv.org/abs/2310.15688)</code></li>
<li>Summary: <p>Stable imaging in adverse environments (e.g., total darkness) makes thermal
infrared (TIR) cameras a prevalent option for night scene perception. However,
the low contrast and lack of chromaticity of TIR images are detrimental to
human interpretation and subsequent deployment of RGB-based vision algorithms.
Therefore, it makes sense to colorize the nighttime TIR images by translating
them into the corresponding daytime color images (NTIR2DC). Despite the
impressive progress made in the NTIR2DC task, how to improve the translation
performance of small object classes is under-explored. To address this problem,
we propose a generative adversarial network incorporating feedback-based object
appearance learning (FoalGAN). Specifically, an occlusion-aware mixup module
and corresponding appearance consistency loss are proposed to reduce the
context dependence of object translation. As a representative example of small
objects in nighttime street scenes, we illustrate how to enhance the realism of
traffic light by designing a traffic light appearance loss. To further improve
the appearance learning of small objects, we devise a dual feedback learning
strategy to selectively adjust the learning frequency of different samples. In
addition, we provide pixel-level annotation for a subset of the Brno dataset,
which can facilitate the research of NTIR image understanding under multiple
weather conditions. Extensive experiments illustrate that the proposed FoalGAN
is not only effective for appearance learning of small objects, but also
outperforms other image translation methods in terms of semantic preservation
and edge consistency for the NTIR2DC task.
</p></li>
</ul>

<h3>Title: Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation. (arXiv:2310.15515v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15515">http://arxiv.org/abs/2310.15515</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15515]] Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation(http://arxiv.org/abs/2310.15515)</code></li>
<li>Summary: <p>Recent ubiquity and disruptive impacts of large language models (LLMs) have
raised concerns about their potential to be misused (.i.e, generating
large-scale harmful and misleading content). To combat this emerging risk of
LLMs, we propose a novel "Fighting Fire with Fire" (F3) strategy that harnesses
modern LLMs' generative and emergent reasoning capabilities to counter
human-written and LLM-generated disinformation. First, we leverage
GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content
through paraphrase-based and perturbation-based prefix-style prompts,
respectively. Second, we apply zero-shot in-context semantic reasoning
techniques with cloze-style prompts to discern genuine from deceptive posts and
news articles. In our extensive experiments, we observe GPT-3.5-turbo's
zero-shot superiority for both in-distribution and out-of-distribution
datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike
the decline observed in previous customized and fine-tuned disinformation
detectors. Our codebase and dataset are available at
https://github.com/mickeymst/F3.
</p></li>
</ul>

<h3>Title: DALE: Generative Data Augmentation for Low-Resource Legal NLP. (arXiv:2310.15799v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15799">http://arxiv.org/abs/2310.15799</a></li>
<li>Code URL: https://github.com/sreyan88/dale</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15799]] DALE: Generative Data Augmentation for Low-Resource Legal NLP(http://arxiv.org/abs/2310.15799)</code></li>
<li>Summary: <p>We present DALE, a novel and effective generative Data Augmentation framework
for low-resource LEgal NLP. DALE addresses the challenges existing frameworks
pose in generating effective data augmentations of legal documents - legal
language, with its specialized vocabulary and complex semantics, morphology,
and syntax, does not benefit from data augmentations that merely rephrase the
source sentence. To address this, DALE, built on an Encoder-Decoder Language
Model, is pre-trained on a novel unsupervised text denoising objective based on
selective masking - our masking strategy exploits the domain-specific language
characteristics of templatized legal documents to mask collocated spans of
text. Denoising these spans helps DALE acquire knowledge about legal concepts,
principles, and language usage. Consequently, it develops the ability to
generate coherent and diverse augmentations with novel contexts. Finally, DALE
performs conditional generation to generate synthetic augmentations for
low-resource Legal NLP tasks. We demonstrate the effectiveness of DALE on 13
datasets spanning 6 tasks and 4 low-resource settings. DALE outperforms all our
baselines, including LLMs, qualitatively and quantitatively, with improvements
of 1%-50%.
</p></li>
</ul>

<h3>Title: Generative Language Models Exhibit Social Identity Biases. (arXiv:2310.15819v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15819">http://arxiv.org/abs/2310.15819</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15819]] Generative Language Models Exhibit Social Identity Biases(http://arxiv.org/abs/2310.15819)</code></li>
<li>Summary: <p>The surge in popularity of large language models has given rise to concerns
about biases that these models could learn from humans. In this study, we
investigate whether ingroup solidarity and outgroup hostility, fundamental
social biases known from social science, are present in 51 large language
models. We find that almost all foundational language models and some
instruction fine-tuned models exhibit clear ingroup-positive and
outgroup-negative biases when prompted to complete sentences (e.g., "We
are..."). A comparison of LLM-generated sentences with human-written sentences
on the internet reveals that these models exhibit similar level, if not
greater, levels of bias than human text. To investigate where these biases stem
from, we experimentally varied the amount of ingroup-positive or
outgroup-negative sentences the model was exposed to during fine-tuning in the
context of the United States Democrat-Republican divide. Doing so resulted in
the models exhibiting a marked increase in ingroup solidarity and an even
greater increase in outgroup hostility. Furthermore, removing either
ingroup-positive or outgroup-negative sentences (or both) from the fine-tuning
data leads to a significant reduction in both ingroup solidarity and outgroup
hostility, suggesting that biases can be reduced by removing biased training
data. Our findings suggest that modern language models exhibit fundamental
social identity biases and that such biases can be mitigated by curating
training data. Our results have practical implications for creating less biased
large-language models and further underscore the need for more research into
user interactions with LLMs to prevent potential bias reinforcement in humans.
</p></li>
</ul>

<h3>Title: Generative and Contrastive Paradigms Are Complementary for Graph Self-Supervised Learning. (arXiv:2310.15523v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15523">http://arxiv.org/abs/2310.15523</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15523]] Generative and Contrastive Paradigms Are Complementary for Graph Self-Supervised Learning(http://arxiv.org/abs/2310.15523)</code></li>
<li>Summary: <p>For graph self-supervised learning (GSSL), masked autoencoder (MAE) follows
the generative paradigm and learns to reconstruct masked graph edges or node
features. Contrastive Learning (CL) maximizes the similarity between augmented
views of the same graph and is widely used for GSSL. However, MAE and CL are
considered separately in existing works for GSSL. We observe that the MAE and
CL paradigms are complementary and propose the graph contrastive masked
autoencoder (GCMAE) framework to unify them. Specifically, by focusing on local
edges or node features, MAE cannot capture global information of the graph and
is sensitive to particular edges and features. On the contrary, CL excels in
extracting global information because it considers the relation between graphs.
As such, we equip GCMAE with an MAE branch and a CL branch, and the two
branches share a common encoder, which allows the MAE branch to exploit the
global information extracted by the CL branch. To force GCMAE to capture global
graph structures, we train it to reconstruct the entire adjacency matrix
instead of only the masked edges as in existing works. Moreover, a
discrimination loss is proposed for feature reconstruction, which improves the
disparity between node embeddings rather than reducing the reconstruction error
to tackle the feature smoothing problem of MAE. We evaluate GCMAE on four
popular graph tasks (i.e., node classification, node clustering, link
prediction, and graph classification) and compare with 14 state-of-the-art
baselines. The results show that GCMAE consistently provides good accuracy
across these tasks, and the maximum accuracy improvement is up to 3.2% compared
with the best-performing baseline.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Inject Semantic Concepts into Image Tagging for Open-Set Recognition. (arXiv:2310.15200v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15200">http://arxiv.org/abs/2310.15200</a></li>
<li>Code URL: https://github.com/xinyu1205/recognize-anything</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15200]] Inject Semantic Concepts into Image Tagging for Open-Set Recognition(http://arxiv.org/abs/2310.15200)</code></li>
<li>Summary: <p>In this paper, we introduce the Recognize Anything Plus Model~(RAM++), a
fundamental image recognition model with strong open-set recognition
capabilities, by injecting semantic concepts into image tagging training
framework. Previous approaches are either image tagging models constrained by
limited semantics, or vision-language models with shallow interaction for
suboptimal performance in multi-tag recognition. In contrast, RAM++ integrates
image-text alignment and image-tagging within a unified fine-grained
interaction framework based on image-tags-text triplets. This design enables
RAM++ not only excel in identifying predefined categories, but also
significantly augment the recognition ability in open-set categories. Moreover,
RAM++ employs large language models~(LLMs) to generate diverse visual tag
descriptions, pioneering the integration of LLM's knowledge into image tagging
training. This approach empowers RAM++ to integrate visual description concepts
for open-set recognition during inference. Evaluations on comprehensive image
recognition benchmarks demonstrate RAM++ exceeds existing state-of-the-art
(SOTA) fundamental image recognition models on most aspects. Specifically, for
predefined common-used tag categories, RAM++ showcases 10.2 mAP and 15.4 mAP
enhancements over CLIP on OpenImages and ImageNet. For open-set categories
beyond predefined, RAM++ records improvements of 5 mAP and 6.4 mAP over CLIP
and RAM respectively on OpenImages. For diverse human-object interaction
phrases, RAM++ achieves 7.8 mAP and 4.7 mAP improvements on the HICO benchmark.
Code, datasets and pre-trained models are available at
\url{https://github.com/xinyu1205/recognize-anything}.
</p></li>
</ul>

<h3>Title: Large Language Models are Temporal and Causal Reasoners for Video Question Answering. (arXiv:2310.15747v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15747">http://arxiv.org/abs/2310.15747</a></li>
<li>Code URL: https://github.com/mlvlab/Flipped-VQA</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15747]] Large Language Models are Temporal and Causal Reasoners for Video Question Answering(http://arxiv.org/abs/2310.15747)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have shown remarkable performances on a wide
range of natural language understanding and generation tasks. We observe that
the LLMs provide effective priors in exploiting $\textit{linguistic shortcuts}$
for temporal and causal reasoning in Video Question Answering (VideoQA).
However, such priors often cause suboptimal results on VideoQA by leading the
model to over-rely on questions, $\textit{i.e.}$, $\textit{linguistic bias}$,
while ignoring visual content. This is also known as `ungrounded guesses' or
`hallucinations'. To address this problem while leveraging LLMs' prior on
VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to
predict all the combinations of $\langle$V, Q, A$\rangle$ triplet by flipping
the source pair and the target label to understand their complex relationships,
$\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs,
respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to
LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five
challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general
framework that is applicable to various LLMs (OPT and GPT-J) and consistently
improves their performances. We empirically demonstrate that Flipped-VQA not
only enhances the exploitation of linguistic shortcuts but also mitigates the
linguistic bias, which causes incorrect answers over-relying on the question.
Code is available at https://github.com/mlvlab/Flipped-VQA.
</p></li>
</ul>

<h3>Title: DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning. (arXiv:2310.15205v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15205">http://arxiv.org/abs/2310.15205</a></li>
<li>Code URL: https://github.com/fudandisc/disc-finllm</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15205]] DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning(http://arxiv.org/abs/2310.15205)</code></li>
<li>Summary: <p>We propose Multiple Experts Fine-tuning Framework to build a financial large
language model (LLM), DISC-FinLLM. Our methodology improves general LLMs by
endowing them with multi-turn question answering abilities, domain text
processing capabilities, mathematical computation skills, and
retrieval-enhanced generation capabilities. We build a financial
instruction-tuning dataset named DISC-FIN-SFT, including instruction samples of
four categories (consulting, NLP tasks, computing and retrieval-augmented
generation). Evaluations conducted on multiple benchmarks demonstrate that our
model performs better than baseline models in various financial scenarios.
Further resources can be found at https://github.com/FudanDISC/DISC-FinLLM.
</p></li>
</ul>

<h3>Title: Function Vectors in Large Language Models. (arXiv:2310.15213v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15213">http://arxiv.org/abs/2310.15213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15213]] Function Vectors in Large Language Models(http://arxiv.org/abs/2310.15213)</code></li>
<li>Summary: <p>We report the presence of a simple neural mechanism that represents an
input-output function as a vector within autoregressive transformer language
models (LMs). Using causal mediation analysis on a diverse range of
in-context-learning (ICL) tasks, we find that a small number attention heads
transport a compact representation of the demonstrated task, which we call a
function vector (FV). FVs are robust to changes in context, i.e., they trigger
execution of the task on inputs such as zero-shot and natural text settings
that do not resemble the ICL contexts from which they are collected. We test
FVs across a range of tasks, models, and layers and find strong causal effects
across settings in middle layers. We investigate the internal structure of FVs
and find while that they often contain information that encodes the output
space of the function, this information alone is not sufficient to reconstruct
an FV. Finally, we test semantic vector composition in FVs, and find that to
some extent they can be summed to create vectors that trigger new complex
tasks. Taken together, our findings suggest that LLMs contain internal
abstractions of general-purpose functions that can be invoked in a variety of
contexts.
</p></li>
</ul>

<h3>Title: Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses. (arXiv:2310.15317v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15317">http://arxiv.org/abs/2310.15317</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15317]] Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses(http://arxiv.org/abs/2310.15317)</code></li>
<li>Summary: <p>In this paper, we explore the application of large language models (LLMs) for
generating code-tracing questions in introductory programming courses. We
designed targeted prompts for GPT4, guiding it to generate code-tracing
questions based on code snippets and descriptions. We established a set of
human evaluation metrics to assess the quality of questions produced by the
model compared to those created by human experts. Our analysis provides
insights into the capabilities and potential of LLMs in generating diverse
code-tracing questions. Additionally, we present a unique dataset of human and
LLM-generated tracing questions, serving as a valuable resource for both the
education and NLP research communities. This work contributes to the ongoing
dialogue on the potential uses of LLMs in educational settings.
</p></li>
</ul>

<h3>Title: Specialist or Generalist? Instruction Tuning for Specific NLP Tasks. (arXiv:2310.15326v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15326">http://arxiv.org/abs/2310.15326</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15326]] Specialist or Generalist? Instruction Tuning for Specific NLP Tasks(http://arxiv.org/abs/2310.15326)</code></li>
<li>Summary: <p>The potential of large language models (LLMs) to simultaneously perform a
wide range of natural language processing (NLP) tasks has been the subject of
extensive research. Although instruction tuning has proven to be a
data-efficient method for transforming LLMs into such generalist models, their
performance still lags behind specialist models trained exclusively for
specific tasks. In this paper, we investigate whether incorporating
broad-coverage generalist instruction tuning can contribute to building a
specialist model. We hypothesize that its efficacy depends on task specificity
and skill requirements. Our experiments assess four target tasks with distinct
coverage levels, revealing that integrating generalist instruction tuning
consistently enhances model performance when the task coverage is broad. The
effect is particularly pronounced when the amount of task-specific training
data is limited. Further investigation into three target tasks focusing on
different capabilities demonstrates that generalist instruction tuning improves
understanding and reasoning abilities. However, for tasks requiring factual
knowledge, generalist data containing hallucinatory information may negatively
affect the model's performance. Overall, our work provides a systematic guide
for developing specialist models with general instruction tuning. Our code and
other related resources can be found at
https://github.com/DavidFanzz/Generalist_or_Specialist.
</p></li>
</ul>

<h3>Title: Irreducible Curriculum for Language Model Pretraining. (arXiv:2310.15389v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15389">http://arxiv.org/abs/2310.15389</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15389]] Irreducible Curriculum for Language Model Pretraining(http://arxiv.org/abs/2310.15389)</code></li>
<li>Summary: <p>Automatic data selection and curriculum design for training large language
models is challenging, with only a few existing methods showing improvements
over standard training. Furthermore, current schemes focus on domain-level
selection, overlooking the more fine-grained contributions of each individual
training point. It is difficult to apply traditional datapoint selection
methods on large language models: most online batch selection methods perform
two-times forward or backward passes, which introduces considerable extra costs
with large-scale models. To mitigate these obstacles, we propose irreducible
curriculum as a curriculum learning algorithm for language model pretraining,
which prioritizes samples with higher learnability. Specifically, to avoid
prohibitive extra computation overhead, we simulate the sample loss along the
main model's training trajectory using a small-scale proxy model. Our
experiments on the RedPajama-1B dataset demonstrate a consistent improvement on
validation perplexity across all 7 domains compared to random uniform baseline
and the anti-curriculum strategy. Our method also reduces the sharpness of the
network and illustrates a better 5-shot accuracy on MMLU benchmarks.
</p></li>
</ul>

<h3>Title: DoGE: Domain Reweighting with Generalization Estimation. (arXiv:2310.15393v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15393">http://arxiv.org/abs/2310.15393</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15393]] DoGE: Domain Reweighting with Generalization Estimation(http://arxiv.org/abs/2310.15393)</code></li>
<li>Summary: <p>The coverage and composition of the pretraining data corpus significantly
impacts the generalization ability of large language models. Conventionally,
the pretraining corpus is composed of various source domains (e.g. CommonCrawl,
Wikipedia, Github etc.) according to certain sampling probabilities (domain
weights). However, current methods lack a principled way to optimize domain
weights for ultimate goal for generalization. We propose DOmain reweighting
with Generalization Estimation (DoGE), where we reweigh the sampling
probability from each domain based on its contribution to the final
generalization objective assessed by a gradient-based generalization estimation
function. First, we train a small-scale proxy model with a min-max optimization
to obtain the reweighted domain weights. At each step, the domain weights are
updated to maximize the overall generalization gain by mirror descent. Finally
we use the obtained domain weights to train a larger scale full-size language
model. On SlimPajama-6B dataset, with universal generalization objective, DoGE
achieves better average perplexity and zero-shot reasoning accuracy. On
out-of-domain generalization tasks, DoGE reduces perplexity on the target
domain by a large margin. We further apply a parameter-selection scheme which
improves the efficiency of generalization estimation.
</p></li>
</ul>

<h3>Title: GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions. (arXiv:2310.15405v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15405">http://arxiv.org/abs/2310.15405</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15405]] GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions(http://arxiv.org/abs/2310.15405)</code></li>
<li>Summary: <p>There is growing interest in systems that generate captions for scientific
figures. However, assessing these systems output poses a significant challenge.
Human evaluation requires academic expertise and is costly, while automatic
evaluation depends on often low-quality author-written captions. This paper
investigates using large language models (LLMs) as a cost-effective,
reference-free method for evaluating figure captions. We first constructed
SCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600
scientific figure captions, both original and machine-made, for 600 arXiv
figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption
based on its potential to aid reader understanding, given relevant context such
as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot
evaluator, outperformed all other models and even surpassed assessments made by
Computer Science and Informatics undergraduates, achieving a Kendall
correlation score of 0.401 with Ph.D. students rankings
</p></li>
</ul>

<h3>Title: FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions. (arXiv:2310.15421v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15421">http://arxiv.org/abs/2310.15421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15421]] FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions(http://arxiv.org/abs/2310.15421)</code></li>
<li>Summary: <p>Theory of mind (ToM) evaluations currently focus on testing models using
passive narratives that inherently lack interactivity. We introduce FANToM, a
new benchmark designed to stress-test ToM within information-asymmetric
conversational contexts via question answering. Our benchmark draws upon
important theoretical requisites from psychology and necessary empirical
considerations when evaluating large language models (LLMs). In particular, we
formulate multiple types of questions that demand the same underlying reasoning
to identify illusory or false sense of ToM capabilities in LLMs. We show that
FANToM is challenging for state-of-the-art LLMs, which perform significantly
worse than humans even with chain-of-thought reasoning or fine-tuning.
</p></li>
</ul>

<h3>Title: CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model. (arXiv:2310.15477v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15477">http://arxiv.org/abs/2310.15477</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15477]] CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model(http://arxiv.org/abs/2310.15477)</code></li>
<li>Summary: <p>Instruction tuning has recently been recognized as an effective way of
aligning Large Language Models (LLMs) to enhance their generalization ability
across various tasks. However, when tuning publicly accessible, centralized
LLMs with private instruction data, privacy concerns are inevitable. While
direct transfer of parameterized modules between models is a plausible approach
to address this, its implications and effectiveness need further exploration.
This paper focuses on Offsite-Tuning (OFT), a representative technique that
transfers transformer blocks between centralized LLMs and downstream emulators.
Given the limited understanding of the underlying mechanism of OFT, we perform
an empirical analysis on LLMs from the perspectives of representation and
functional similarity. Interestingly, our findings reveal a unique modular
structure within the layers of LLMs that appears to emerge as the model size
expands. Simultaneously, we note subtle but potentially significant changes in
representation and intermediate predictions across the layers. Inspired by
these observations, we propose CRaSh, involving Clustering, Removing, and
Sharing, a training-free strategy to derive improved emulators from LLMs. CRaSh
significantly boosts performance of OFT with billions of parameters.
Furthermore, we investigate the optimal solutions yielded by fine-tuning with
and without full model through the lens of loss landscape. Our findings
demonstrate a linear connectivity among these optima falling over the same
basin, thereby highlighting the effectiveness of CRaSh and OFT. The source code
is publicly available at https://github.com/TsinghuaC3I/CRaSh.
</p></li>
</ul>

<h3>Title: KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval. (arXiv:2310.15511v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15511">http://arxiv.org/abs/2310.15511</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15511]] KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval(http://arxiv.org/abs/2310.15511)</code></li>
<li>Summary: <p>We study the ability of state-of-the art models to answer constraint
satisfaction queries for information retrieval (e.g., 'a list of ice cream
shops in San Diego'). In the past, such queries were considered to be tasks
that could only be solved via web-search or knowledge bases. More recently,
large language models (LLMs) have demonstrated initial emergent abilities in
this task. However, many current retrieval benchmarks are either saturated or
do not measure constraint satisfaction. Motivated by rising concerns around
factual incorrectness and hallucinations of LLMs, we present KITAB, a new
dataset for measuring constraint satisfaction abilities of language models.
KITAB consists of book-related data across more than 600 authors and 13,000
queries, and also offers an associated dynamic data collection and constraint
verification approach for acquiring similar test data for other authors. Our
extended experiments on GPT4 and GPT3.5 characterize and decouple common
failure modes across dimensions such as information popularity, constraint
types, and context availability. Results show that in the absence of context,
models exhibit severe limitations as measured by irrelevant information,
factual errors, and incompleteness, many of which exacerbate as information
popularity decreases. While context availability mitigates irrelevant
information, it is not helpful for satisfying constraints, identifying
fundamental barriers to constraint satisfaction. We open source our
contributions to foster further research on improving constraint satisfaction
abilities of future models.
</p></li>
</ul>

<h3>Title: SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation. (arXiv:2310.15539v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15539">http://arxiv.org/abs/2310.15539</a></li>
<li>Code URL: https://github.com/sade-adrien/stelocoder</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15539]] SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation(http://arxiv.org/abs/2310.15539)</code></li>
<li>Summary: <p>With the recent focus on Large Language Models (LLMs), both StarCoder (Li et
al., 2023) and Code Llama (Rozi\`ere et al., 2023) have demonstrated remarkable
performance in code generation. However, there is still a need for improvement
in code translation functionality with efficient training techniques. In
response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM
designed specifically for multi-programming language-to-Python code
translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or
PHP-to-Python code translation without specifying the input programming
language. We modified StarCoder model architecture by incorporating a
Mixture-of-Experts (MoE) technique featuring five experts and a gating network
for multi-task handling. Experts are obtained by StarCoder fine-tuning.
Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each
expert size as only 0.06% of number of StarCoder's parameters. At the same
time, to enhance training efficiency in terms of time, we adopt curriculum
learning strategy and use self-instruct data for efficient fine-tuning. As a
result, each expert takes only 6 hours to train on one single 80Gb A100 HBM.
With experiments on XLCoST datasets, SteloCoder achieves an average of 73.76
CodeBLEU score in multi-programming language-to-Python translation, surpassing
the top performance from the leaderboard by at least 3.5. This accomplishment
is attributed to only 45M extra parameters with StarCoder as the backbone and
32 hours of valid training on one 80GB A100 HBM. The source code is release
here: https://github.com/sade-adrien/SteloCoder.
</p></li>
</ul>

<h3>Title: TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction. (arXiv:2310.15556v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15556">http://arxiv.org/abs/2310.15556</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15556]] TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction(http://arxiv.org/abs/2310.15556)</code></li>
<li>Summary: <p>Since ChatGPT released its API for public use, the number of applications
built on top of commercial large language models (LLMs) increase exponentially.
One popular usage of such models is leveraging its in-context learning ability
and generating responses given user queries leveraging knowledge obtained by
retrieval augmentation. One problem of deploying commercial retrieval-augmented
LLMs is the cost due to the additionally retrieved context that largely
increases the input token size of the LLMs. To mitigate this, we propose a
token compression scheme that includes two methods: summarization compression
and semantic compression. The first method applies a T5-based model that is
fine-tuned by datasets generated using self-instruct containing samples with
varying lengths and reduce token size by doing summarization. The second method
further compresses the token size by removing words with lower impact on the
semantic. In order to adequately evaluate the effectiveness of the proposed
methods, we propose and utilize a dataset called Food-Recommendation DB (FRDB)
focusing on food recommendation for women around pregnancy period or infants.
Our summarization compression can reduce 65% of the retrieval token size with
further 0.3% improvement on the accuracy; semantic compression provides a more
flexible way to trade-off the token size with performance, for which we can
reduce the token size by 20% with only 1.6% of accuracy drop.
</p></li>
</ul>

<h3>Title: POE: Process of Elimination for Multiple Choice Reasoning. (arXiv:2310.15575v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15575">http://arxiv.org/abs/2310.15575</a></li>
<li>Code URL: https://github.com/kasmasvan/poe</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15575]] POE: Process of Elimination for Multiple Choice Reasoning(http://arxiv.org/abs/2310.15575)</code></li>
<li>Summary: <p>Language models (LMs) are capable of conducting in-context learning for
multiple choice reasoning tasks, but the options in these tasks are treated
equally. As humans often first eliminate wrong options before picking the final
correct answer, we argue a similar two-step strategy can make LMs better at
these tasks. To this end, we present the Process of Elimination (POE), a
two-step scoring method. In the first step, POE scores each option, and
eliminates seemingly wrong options. In the second step, POE masks these wrong
options, and makes the final prediction from the remaining options. Zero-shot
experiments on 8 reasoning tasks illustrate the effectiveness of POE, and a
following analysis finds our method to be especially performant on logical
reasoning tasks. We further analyze the effect of masks, and show that POE
applies to few-shot settings and large language models (LLMs) like ChatGPT.
</p></li>
</ul>

<h3>Title: Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression. (arXiv:2310.15594v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15594">http://arxiv.org/abs/2310.15594</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15594]] Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression(http://arxiv.org/abs/2310.15594)</code></li>
<li>Summary: <p>Large-scale pre-trained language models (LLMs) have demonstrated exceptional
performance in various natural language processing (NLP) tasks. However, the
massive size of these models poses huge challenges for their deployment in
real-world applications. While numerous model compression techniques have been
proposed, most of them are not well-suited for achieving extreme model
compression when there is a significant gap in model scale. In this paper, we
introduce a novel compression paradigm called Retrieval-based Knowledge
Transfer (RetriKT), which effectively transfers the knowledge of LLMs to
extremely small-scale models (e.g., 1%). In particular, our approach extracts
knowledge from LLMs to construct a knowledge store, from which the small-scale
model can retrieve relevant information and leverage it for effective
inference. To improve the quality of the model, soft prompt tuning and Proximal
Policy Optimization (PPO) reinforcement learning techniques are employed.
Extensive experiments are conducted on low-resource tasks from SuperGLUE and
GLUE benchmarks. The results demonstrate that the proposed approach
significantly enhances the performance of small-scale models by leveraging the
knowledge from LLMs.
</p></li>
</ul>

<h3>Title: CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation. (arXiv:2310.15638v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15638">http://arxiv.org/abs/2310.15638</a></li>
<li>Code URL: https://github.com/salt-nlp/coannotating</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15638]] CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation(http://arxiv.org/abs/2310.15638)</code></li>
<li>Summary: <p>Annotated data plays a critical role in Natural Language Processing (NLP) in
training models and evaluating their performance. Given recent developments in
Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot
capability on many text-annotation tasks, comparable with or even exceeding
human annotators. Such LLMs can serve as alternatives for manual annotation,
due to lower costs and higher scalability. However, limited work has leveraged
LLMs as complementary annotators, nor explored how annotation work is best
allocated among humans and LLMs to achieve both quality and cost objectives. We
propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of
unstructured texts at scale. Under this framework, we utilize uncertainty to
estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to
be an effective means to allocate work from results on different datasets, with
up to 21% performance improvement over random baseline. For code
implementation, see https://github.com/SALT-NLP/CoAnnotating.
</p></li>
</ul>

<h3>Title: Prevalence and prevention of large language model use in crowd work. (arXiv:2310.15683v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15683">http://arxiv.org/abs/2310.15683</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15683]] Prevalence and prevention of large language model use in crowd work(http://arxiv.org/abs/2310.15683)</code></li>
<li>Summary: <p>We show that the use of large language models (LLMs) is prevalent among crowd
workers, and that targeted mitigation strategies can significantly reduce, but
not eliminate, LLM use. On a text summarization task where workers were not
directed in any way regarding their LLM use, the estimated prevalence of LLM
use was around 30%, but was reduced by about half by asking workers to not use
LLMs and by raising the cost of using them, e.g., by disabling copy-pasting.
Secondary analyses give further insight into LLM use and its prevention: LLM
use yields high-quality but homogeneous responses, which may harm research
concerned with human (rather than model) behavior and degrade future models
trained with crowdsourced data. At the same time, preventing LLM use may be at
odds with obtaining high-quality responses; e.g., when requesting workers not
to use LLMs, summaries contained fewer keywords carrying essential information.
Our estimates will likely change as LLMs increase in popularity or
capabilities, and as norms around their usage change. Yet, understanding the
co-evolution of LLM-based tools and users is key to maintaining the validity of
research done using crowdsourcing, and we provide a critical baseline before
widespread adoption ensues.
</p></li>
</ul>

<h3>Title: Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation. (arXiv:2310.15746v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15746">http://arxiv.org/abs/2310.15746</a></li>
<li>Code URL: https://github.com/thunlp-mt/tran</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15746]] Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation(http://arxiv.org/abs/2310.15746)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have showcased impressive performance. However,
due to their inability to capture relationships among samples, these frozen
LLMs inevitably keep repeating similar mistakes. In this work, we propose our
Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving
their performance by learning from previous mistakes. Considering data arrives
sequentially, LLMs gradually accumulate rules from incorrect cases, forming a
rule collection. These rules are then utilized by the LLMs to avoid making
similar mistakes when processing subsequent inputs. Moreover, the rules remain
independent of the primary prompts, seamlessly complementing prompt design
strategies. Experimentally, we show that TRAN improves over recent baselines by
a large margin.
</p></li>
</ul>

<h3>Title: BLESS: Benchmarking Large Language Models on Sentence Simplification. (arXiv:2310.15773v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15773">http://arxiv.org/abs/2310.15773</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15773]] BLESS: Benchmarking Large Language Models on Sentence Simplification(http://arxiv.org/abs/2310.15773)</code></li>
<li>Summary: <p>We present BLESS, a comprehensive performance benchmark of the most recent
state-of-the-art large language models (LLMs) on the task of text
simplification (TS). We examine how well off-the-shelf LLMs can solve this
challenging task, assessing a total of 44 models, differing in size,
architecture, pre-training methods, and accessibility, on three test sets from
different domains (Wikipedia, news, and medical) under a few-shot setting. Our
analysis considers a suite of automatic metrics as well as a large-scale
quantitative investigation into the types of common edit operations performed
by the different models. Furthermore, we perform a manual qualitative analysis
on a subset of model outputs to better gauge the quality of the generated
simplifications. Our evaluation indicates that the best LLMs, despite not being
trained on TS, perform comparably with state-of-the-art TS baselines.
Additionally, we find that certain LLMs demonstrate a greater range and
diversity of edit operations. Our performance benchmark will be available as a
resource for the development of future TS methods and evaluation metrics.
</p></li>
</ul>

<h3>Title: MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications. (arXiv:2310.15777v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15777">http://arxiv.org/abs/2310.15777</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15777]] MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications(http://arxiv.org/abs/2310.15777)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language tasks, marking significant strides towards general
artificial intelligence. While general artificial intelligence is leveraged by
developing increasingly large-scale models, there could be another branch to
develop lightweight custom models that better serve certain domains, taking
into account the high cost of training and deploying LLMs and the scarcity of
resources. In this paper, we present MindLLM, a novel series of bilingual
lightweight large language models, trained from scratch, alleviating such
burdens by offering models with 1.3 billion and 3 billion parameters. A
thorough account of experiences accrued during large model development is
given, covering every step of the process, including data construction, model
architecture, evaluation, and applications. Such insights are hopefully
valuable for fellow academics and developers. MindLLM consistently matches or
surpasses the performance of other open-source larger models on some public
benchmarks. We also introduce an innovative instruction tuning framework
tailored for smaller models to enhance their capabilities efficiently.
Moreover, we explore the application of MindLLM in specific vertical domains
such as law and finance, underscoring the agility and adaptability of our
lightweight models.
</p></li>
</ul>

<h3>Title: Improving generalization in large language models by learning prefix subspaces. (arXiv:2310.15793v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15793">http://arxiv.org/abs/2310.15793</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15793]] Improving generalization in large language models by learning prefix subspaces(http://arxiv.org/abs/2310.15793)</code></li>
<li>Summary: <p>This article focuses on large language models (LLMs) fine-tuning in the
scarce data regime (also known as the "few-shot" learning setting). We propose
a method to increase the generalization capabilities of LLMs based on neural
network subspaces. This optimization method, recently introduced in computer
vision, aims to improve model generalization by identifying wider local optima
through the joint optimization of an entire simplex of models in parameter
space. Its adaptation to massive, pretrained transformers, however, poses some
challenges. First, their considerable number of parameters makes it difficult
to train several models jointly, and second, their deterministic parameter
initialization schemes make them unfit for the subspace method as originally
proposed. We show in this paper that "Parameter Efficient Fine-Tuning" (PEFT)
methods, however, are perfectly compatible with this original approach, and
propose to learn entire simplex of continuous prefixes. We test our method on a
variant of the GLUE benchmark adapted to the few-shot learning setting, and
show that both our contributions jointly lead to a gain in average performances
compared to sota methods. The implementation can be found at the following
link: https://github.com/Liloulou/prefix_subspace
</p></li>
</ul>

<h3>Title: BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT. (arXiv:2310.15896v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15896">http://arxiv.org/abs/2310.15896</a></li>
<li>Code URL: https://github.com/scutcyr/bianque</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15896]] BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT(http://arxiv.org/abs/2310.15896)</code></li>
<li>Summary: <p>Large language models (LLMs) have performed well in providing general and
extensive health suggestions in single-turn conversations, exemplified by
systems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the
limited information provided by users during single turn results in inadequate
personalization and targeting of the generated suggestions, which requires
users to independently select the useful part. It is mainly caused by the
missing ability to engage in multi-turn questioning. In real-world medical
consultations, doctors usually employ a series of iterative inquiries to
comprehend the patient's condition thoroughly, enabling them to provide
effective and personalized suggestions subsequently, which can be defined as
chain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose
BianQue, a ChatGLM-based LLM finetuned with the self-constructed health
conversation dataset BianQueCorpus that is consist of multiple turns of
questioning and health suggestions polished by ChatGPT. Experimental results
demonstrate that the proposed BianQue can simultaneously balance the
capabilities of both questioning and health suggestions, which will help
promote the research and application of LLMs in the field of proactive health.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding. (arXiv:2310.15308v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15308">http://arxiv.org/abs/2310.15308</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15308]] SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding(http://arxiv.org/abs/2310.15308)</code></li>
<li>Summary: <p>The landscape of publicly available vision foundation models (VFMs), such as
CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed
with distinct capabilities stemming from their pre-training objectives. For
instance, CLIP excels in semantic understanding, while SAM specializes in
spatial understanding for segmentation. In this work, we introduce a simple
recipe to efficiently merge VFMs into a unified model that assimilates their
expertise. Our proposed method integrates multi-task learning, continual
learning techniques, and teacher-student distillation. This strategy entails
significantly less computational cost compared to traditional multi-task
training from scratch. Additionally, it only demands a small fraction of the
pre-training datasets that were initially used to train individual models. By
applying our method to SAM and CLIP, we derive SAM-CLIP: a unified model that
amalgamates the strengths of SAM and CLIP into a single backbone, making it apt
for edge device applications. We show that SAM-CLIP learns richer visual
representations, equipped with both localization and semantic features,
suitable for a broad range of vision tasks. SAM-CLIP obtains improved
performance on several head probing tasks when compared with SAM and CLIP. We
further show that SAM-CLIP not only retains the foundational strengths of its
precursor models but also introduces synergistic functionalities, most notably
in zero-shot semantic segmentation, where SAM-CLIP establishes new
state-of-the-art results on 5 benchmarks. It outperforms previous models that
are specifically designed for this task by a large margin, including +6.8% and
+5.9% mean IoU improvement on Pascal-VOC and COCO-Stuff datasets, respectively.
</p></li>
</ul>

<h3>Title: GNeSF: Generalizable Neural Semantic Fields. (arXiv:2310.15712v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15712">http://arxiv.org/abs/2310.15712</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15712]] GNeSF: Generalizable Neural Semantic Fields(http://arxiv.org/abs/2310.15712)</code></li>
<li>Summary: <p>3D scene segmentation based on neural implicit representation has emerged
recently with the advantage of training only on 2D supervision. However,
existing approaches still requires expensive per-scene optimization that
prohibits generalization to novel scenes during inference. To circumvent this
problem, we introduce a generalizable 3D segmentation framework based on
implicit representation. Specifically, our framework takes in multi-view image
features and semantic maps as the inputs instead of only spatial information to
avoid overfitting to scene-specific geometric and semantic information. We
propose a novel soft voting mechanism to aggregate the 2D semantic information
from different views for each 3D point. In addition to the image features, view
difference information is also encoded in our framework to predict the voting
scores. Intuitively, this allows the semantic information from nearby views to
contribute more compared to distant ones. Furthermore, a visibility module is
also designed to detect and filter out detrimental information from occluded
views. Due to the generalizability of our proposed method, we can synthesize
semantic maps or conduct 3D semantic segmentation for novel scenes with solely
2D semantic supervision. Experimental results show that our approach achieves
comparable performance with scene-specific approaches. More importantly, our
approach can even outperform existing strong supervision-based approaches with
only 2D annotations. Our source code is available at:
https://github.com/HLinChen/GNeSF.
</p></li>
</ul>

<h3>Title: Automatic Aorta Segmentation with Heavily Augmented, High-Resolution 3-D ResUNet: Contribution to the SEG.A Challenge. (arXiv:2310.15827v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15827">http://arxiv.org/abs/2310.15827</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15827]] Automatic Aorta Segmentation with Heavily Augmented, High-Resolution 3-D ResUNet: Contribution to the SEG(http://arxiv.org/abs/2310.15827)</code></li>
<li>Summary: <p>Automatic aorta segmentation from 3-D medical volumes is an important yet
difficult task. Several factors make the problem challenging, e.g. the
possibility of aortic dissection or the difficulty with segmenting and
annotating the small branches. This work presents a contribution by the MedGIFT
team to the SEG.A challenge organized during the MICCAI 2023 conference. We
propose a fully automated algorithm based on deep encoder-decoder architecture.
The main assumption behind our work is that data preprocessing and augmentation
are much more important than the deep architecture, especially in low data
regimes. Therefore, the solution is based on a variant of traditional
convolutional U-Net. The proposed solution achieved a Dice score above 0.9 for
all testing cases with the highest stability among all participants. The method
scored 1st, 4th, and 3rd in terms of the clinical evaluation, quantitative
results, and volumetric meshing quality, respectively. We freely release the
source code, pretrained model, and provide access to the algorithm on the
Grand-Challenge platform.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
