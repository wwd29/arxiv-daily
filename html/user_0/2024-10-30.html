<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-30</h1>
<h3>Title: TV-3DG: Mastering Text-to-3D Customized Generation with Visual Prompt</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Yang, Donglin Di, Baorui Ma, Xun Yang, Yongjia Ma, Wenzhang Sun, Wei Chen, Jianxun Cui, Zhou Xue, Meng Wang, Yebin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21299">https://arxiv.org/abs/2410.21299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21299">https://arxiv.org/pdf/2410.21299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21299]] TV-3DG: Mastering Text-to-3D Customized Generation with Visual Prompt(https://arxiv.org/abs/2410.21299)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In recent years, advancements in generative models have significantly expanded the capabilities of text-to-3D generation. Many approaches rely on Score Distillation Sampling (SDS) technology. However, SDS struggles to accommodate multi-condition inputs, such as text and visual prompts, in customized generation tasks. To explore the core reasons, we decompose SDS into a difference term and a classifier-free guidance term. Our analysis identifies the core issue as arising from the difference term and the random noise addition during the optimization process, both contributing to deviations from the target mode during distillation. To address this, we propose a novel algorithm, Classifier Score Matching (CSM), which removes the difference term in SDS and uses a deterministic noise addition process to reduce noise during optimization, effectively overcoming the low-quality limitations of SDS in our customized generation framework. Based on CSM, we integrate visual prompt information with an attention fusion mechanism and sampling guidance techniques, forming the Visual Prompt CSM (VPCSM) algorithm. Furthermore, we introduce a Semantic-Geometry Calibration (SGC) module to enhance quality through improved textual information integration. We present our approach as TV-3DG, with extensive experiments demonstrating its capability to achieve stable, high-quality, customized 3D generation. Project page: \url{this https URL}</li>
</ul>

<h3>Title: Domain-Adaptive Pre-training of Self-Supervised Foundation Models for Medical Image Classification in Gastrointestinal Endoscopy</h3>
<ul>
<li><strong>Authors: </strong>Marcel Roth, Micha V. Nowak</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21302">https://arxiv.org/abs/2410.21302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21302">https://arxiv.org/pdf/2410.21302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21302]] Domain-Adaptive Pre-training of Self-Supervised Foundation Models for Medical Image Classification in Gastrointestinal Endoscopy(https://arxiv.org/abs/2410.21302)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Video capsule endoscopy has transformed gastrointestinal endoscopy (GIE) diagnostics by offering a non-invasive method for capturing detailed images of the gastrointestinal tract, enabling early disease detection. However, its potential is limited by the sheer volume of images generated during the imaging procedure, which can take anywhere from 6-8 hours and often produce up to 1 million images, necessitating automated analysis. Additionally, the variability of these images, combined with the need for expert annotations and the scarcity of large, high-quality labeled datasets, constrains the effectiveness of current medical image analysis models. To address this, we introduce a novel large gastrointestinal endoscopy dataset, called EndoExtend24, created by merging and re-stratifying the train/test splits of ten existing public and private datasets, ensuring no overlap of patient data across splits. EndoExtend24 includes over 226,000 labeled images, as well as dynamic class mappings, which allow unified training across datasets with differing labeling granularity, supporting up to 123 distinct pathological findings. Further, we propose to leverage domain adaptive pre-training of foundation models in computer vision trained with self-supervision on generic image data, to adapt them to the task of GIE medical diagnosis. Specifically, the EVA-02 model, which is based on the vision transformer architecture and was trained on ImageNet-22k with masked image modeling (using EVA-CLIP as a MIM teacher), is pre-trained on the novel EndoExtend24 dataset to achieve domain adaptation, and finally trained on the Capsule Endoscopy 2024 Challenge dataset. Experimental results show promising results on the challenge validation set, with an AUC Macro score of 0.993 and a balanced accuracy of 89.3%.</li>
</ul>

<h3>Title: VideoSAM: A Large Vision Foundation Model for High-Speed Video Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chika Maduabuchi, Ericmoore Jossou, Matteo Bucci</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21304">https://arxiv.org/abs/2410.21304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21304">https://arxiv.org/pdf/2410.21304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21304]] VideoSAM: A Large Vision Foundation Model for High-Speed Video Segmentation(https://arxiv.org/abs/2410.21304)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>High-speed video (HSV) segmentation is essential for analyzing dynamic physical processes in scientific and industrial applications, such as boiling heat transfer. Existing models like U-Net struggle with generalization and accurately segmenting complex bubble formations. We present VideoSAM, a specialized adaptation of the Segment Anything Model (SAM), fine-tuned on a diverse HSV dataset for phase detection. Through diverse experiments, VideoSAM demonstrates superior performance across four fluid environments -- Water, FC-72, Nitrogen, and Argon -- significantly outperforming U-Net in complex segmentation tasks. In addition to introducing VideoSAM, we contribute an open-source HSV segmentation dataset designed for phase detection, enabling future research in this domain. Our findings underscore VideoSAM's potential to set new standards in robust and accurate HSV segmentation. The code and dataset used in this study are available online at this https URL .</li>
</ul>

<h3>Title: Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Farid Ariai, Gianluca Demartini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21306">https://arxiv.org/abs/2410.21306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21306">https://arxiv.org/pdf/2410.21306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21306]] Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges(https://arxiv.org/abs/2410.21306)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>Natural Language Processing is revolutionizing the way legal professionals and laypersons operate in the legal field. The considerable potential for Natural Language Processing in the legal sector, especially in developing computational tools for various legal processes, has captured the interest of researchers for years. This survey follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses framework, reviewing 148 studies, with a final selection of 127 after manual filtering. It explores foundational concepts related to Natural Language Processing in the legal domain, illustrating the unique aspects and challenges of processing legal texts, such as extensive document length, complex language, and limited open legal datasets. We provide an overview of Natural Language Processing tasks specific to legal text, such as Legal Document Summarization, legal Named Entity Recognition, Legal Question Answering, Legal Text Classification, and Legal Judgment Prediction. In the section on legal Language Models, we analyze both developed Language Models and approaches for adapting general Language Models to the legal domain. Additionally, we identify 15 Open Research Challenges, including bias in Artificial Intelligence applications, the need for more robust and interpretable models, and improving explainability to handle the complexities of legal language and reasoning.</li>
</ul>

<h3>Title: A Robust Anchor-based Method for Multi-Camera Pedestrian Localization</h3>
<ul>
<li><strong>Authors: </strong>Wanyu Zhang, Jiaqi Zhang, Dongdong Ge, Yu Lin, Huiwen Yang, Huikang Liu, Yinyu Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21308">https://arxiv.org/abs/2410.21308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21308">https://arxiv.org/pdf/2410.21308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21308]] A Robust Anchor-based Method for Multi-Camera Pedestrian Localization(https://arxiv.org/abs/2410.21308)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper addresses the problem of vision-based pedestrian localization, which estimates a pedestrian's location using images and camera parameters. In practice, however, calibrated camera parameters often deviate from the ground truth, leading to inaccuracies in localization. To address this issue, we propose an anchor-based method that leverages fixed-position anchors to reduce the impact of camera parameter errors. We provide a theoretical analysis that demonstrates the robustness of our approach. Experiments conducted on simulated, real-world, and public datasets show that our method significantly improves localization accuracy and remains resilient to noise in camera parameters, compared to methods without anchors.</li>
</ul>

<h3>Title: $\texttt{PatentAgent}$: Intelligent Agent for Automated Pharmaceutical Patent Analysis</h3>
<ul>
<li><strong>Authors: </strong>Xin Wang, Yifan Zhang, Xiaojing Zhang, Longhui Yu, Xinna Lin, Jindong Jiang, Bin Ma, Kaicheng Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21312">https://arxiv.org/abs/2410.21312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21312">https://arxiv.org/pdf/2410.21312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21312]] $\texttt{PatentAgent}$: Intelligent Agent for Automated Pharmaceutical Patent Analysis(https://arxiv.org/abs/2410.21312)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pharmaceutical patents play a vital role in biochemical industries, especially in drug discovery, providing researchers with unique early access to data, experimental results, and research insights. With the advancement of machine learning, patent analysis has evolved from manual labor to tasks assisted by automatic tools. However, there still lacks an unified agent that assists every aspect of patent analysis, from patent reading to core chemical identification. Leveraging the capabilities of Large Language Models (LLMs) to understand requests and follow instructions, we introduce the $\textbf{first}$ intelligent agent in this domain, $\texttt{PatentAgent}$, poised to advance and potentially revolutionize the landscape of pharmaceutical research. $\texttt{PatentAgent}$ comprises three key end-to-end modules -- $\textit{PA-QA}$, $\textit{PA-Img2Mol}$, and $\textit{PA-CoreId}$ -- that respectively perform (1) patent question-answering, (2) image-to-molecular-structure conversion, and (3) core chemical structure identification, addressing the essential needs of scientists and practitioners in pharmaceutical patent analysis. Each module of $\texttt{PatentAgent}$ demonstrates significant effectiveness with the updated algorithm and the synergistic design of $\texttt{PatentAgent}$ framework. $\textit{PA-Img2Mol}$ outperforms existing methods across CLEF, JPO, UOB, and USPTO patent benchmarks with an accuracy gain between 2.46% and 8.37% while $\textit{PA-CoreId}$ realizes accuracy improvement ranging from 7.15% to 7.62% on PatentNetML benchmark. Our code and dataset will be publicly available.</li>
</ul>

<h3>Title: Towards Robust Out-of-Distribution Generalization: Data Augmentation and Neural Architecture Search Approaches</h3>
<ul>
<li><strong>Authors: </strong>Haoyue Bai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21313">https://arxiv.org/abs/2410.21313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21313">https://arxiv.org/pdf/2410.21313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21313]] Towards Robust Out-of-Distribution Generalization: Data Augmentation and Neural Architecture Search Approaches(https://arxiv.org/abs/2410.21313)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning has been demonstrated with tremendous success in recent years. Despite so, its performance in practice often degenerates drastically when encountering out-of-distribution (OoD) data, i.e. training and test data are sampled from different distributions. In this thesis, we study ways toward robust OoD generalization for deep learning, i.e., its performance is not susceptible to distribution shift in the test data. We first propose a novel and effective approach to disentangle the spurious correlation between features that are not essential for recognition. It employs decomposed feature representation by orthogonalizing the two gradients of losses for category and context branches. Furthermore, we perform gradient-based augmentation on context-related features (e.g., styles, backgrounds, or scenes of target objects) to improve the robustness of learned representations. Results show that our approach generalizes well for different distribution shifts. We then study the problem of strengthening neural architecture search in OoD scenarios. We propose to optimize the architecture parameters that minimize the validation loss on synthetic OoD data, under the condition that corresponding network parameters minimize the training loss. Moreover, to obtain a proper validation set, we learn a conditional generator by maximizing their losses computed by different neural architectures. Results show that our approach effectively discovers robust architectures that perform well for OoD generalization.</li>
</ul>

<h3>Title: Decoding Diffusion: A Scalable Framework for Unsupervised Analysis of Latent Space Biases and Representations Using Natural Language Prompts</h3>
<ul>
<li><strong>Authors: </strong>E. Zhixuan Zeng, Yuhao Chen, Alexander Wong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21314">https://arxiv.org/abs/2410.21314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21314">https://arxiv.org/pdf/2410.21314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21314]] Decoding Diffusion: A Scalable Framework for Unsupervised Analysis of Latent Space Biases and Representations Using Natural Language Prompts(https://arxiv.org/abs/2410.21314)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in image generation have made diffusion models powerful tools for creating high-quality images. However, their iterative denoising process makes understanding and interpreting their semantic latent spaces more challenging than other generative models, such as GANs. Recent methods have attempted to address this issue by identifying semantically meaningful directions within the latent space. However, they often need manual interpretation or are limited in the number of vectors that can be trained, restricting their scope and utility. This paper proposes a novel framework for unsupervised exploration of diffusion latent spaces. We directly leverage natural language prompts and image captions to map latent directions. This method allows for the automatic understanding of hidden features and supports a broader range of analysis without the need to train specific vectors. Our method provides a more scalable and interpretable understanding of the semantic knowledge encoded within diffusion models, facilitating comprehensive analysis of latent biases and the nuanced representations these models learn. Experimental results show that our framework can uncover hidden patterns and associations in various domains, offering new insights into the interpretability of diffusion model latent spaces.</li>
</ul>

<h3>Title: GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization</h3>
<ul>
<li><strong>Authors: </strong>Margarita Bugue√±o, Hazem Abou Hamdan, Gerard de Melo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21315">https://arxiv.org/abs/2410.21315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21315">https://arxiv.org/pdf/2410.21315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21315]] GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization(https://arxiv.org/abs/2410.21315)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Heterogeneous graph neural networks have recently gained attention for long document summarization, modeling the extraction as a node classification task. Although effective, these models often require external tools or additional machine learning models to define graph components, producing highly complex and less intuitive structures. We present GraphLSS, a heterogeneous graph construction for long document extractive summarization, incorporating Lexical, Structural, and Semantic features. It defines two levels of information (words and sentences) and four types of edges (sentence semantic similarity, sentence occurrence order, word in sentence, and word semantic similarity) without any need for auxiliary learning models. Experiments on two benchmark datasets show that GraphLSS is competitive with top-performing graph-based methods, outperforming recent non-graph models. We release our code on GitHub.</li>
</ul>

<h3>Title: Deep Optimizer States: Towards Scalable Training of Transformer Models Using Interleaved Offloading</h3>
<ul>
<li><strong>Authors: </strong>Avinash Maurya, Jie Ye, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC, cs.ET, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21316">https://arxiv.org/abs/2410.21316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21316">https://arxiv.org/pdf/2410.21316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21316]] Deep Optimizer States: Towards Scalable Training of Transformer Models Using Interleaved Offloading(https://arxiv.org/abs/2410.21316)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformers and large language models~(LLMs) have seen rapid adoption in all domains. Their sizes have exploded to hundreds of billions of parameters and keep increasing. Under these circumstances, the training of transformers is very expensive and often hits a ``memory wall'', i.e., even when using 3D parallelism (pipeline, tensor, data) and aggregating the memory of many GPUs, it is still not enough to hold the necessary data structures (model parameters, optimizer state, gradients, activations) in GPU memory. To compensate, state-of-the-art approaches offload the optimizer state, at least partially, to the host memory and perform hybrid CPU-GPU computations. However, the management of the combined host-GPU memory is often suboptimal and results in poor overlapping between data movements and computations. This leads to missed opportunities to simultaneously leverage the interconnect bandwidth and computational capabilities of CPUs and GPUs. In this paper, we leverage a key observation that the interleaving of the forward, backward and update phases generate fluctuations in the GPU memory utilization, which can be exploited to dynamically move a part of the optimizer state between the host and the GPU memory at each iteration. To this end, we design and implement \proj, a novel technique to split the LLM into subgroups, whose update phase is scheduled on either the CPU or the GPU based on our proposed performance model that addresses the trade-off between data movement cost, acceleration on the GPUs vs the CPUs, and competition for shared resources. We integrate our approach with DeepSpeed and demonstrate 2.5$\times$ faster iterations over state-of-the-art approaches using extensive experiments.</li>
</ul>

<h3>Title: Mathematical Derivation Graphs: A Task for Summarizing Equation Dependencies in STEM Manuscripts</h3>
<ul>
<li><strong>Authors: </strong>Vishesh Prasad, Brian Kim, Nickvash Kani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21324">https://arxiv.org/abs/2410.21324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21324">https://arxiv.org/pdf/2410.21324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21324]] Mathematical Derivation Graphs: A Task for Summarizing Equation Dependencies in STEM Manuscripts(https://arxiv.org/abs/2410.21324)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in natural language processing (NLP), particularly with the emergence of large language models (LLMs), have significantly enhanced the field of textual analysis. However, while these developments have yielded substantial progress in analyzing textual data, applying analysis to mathematical equations and their relationships within texts has produced mixed results. In this paper, we take the initial steps toward understanding the dependency relationships between mathematical expressions in STEM articles. Our dataset, sourced from a random sampling of the arXiv corpus, contains an analysis of 107 published STEM manuscripts whose inter-equation dependency relationships have been hand-labeled, resulting in a new object we refer to as a derivation graph that summarizes the mathematical content of the manuscript. We exhaustively evaluate analytical and NLP-based models to assess their capability to identify and extract the derivation relationships for each article and compare the results with the ground truth. Our comprehensive testing finds that both analytical and NLP models (including LLMs) achieve $\sim$40-50% F1 scores for extracting derivation graphs from articles, revealing that the recent advances in NLP have not made significant inroads in comprehending mathematical texts compared to simpler analytic models. While current approaches offer a solid foundation for extracting mathematical information, further research is necessary to improve accuracy and depth in this area.</li>
</ul>

<h3>Title: Deconfounding Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Wentao Gao, Feiyu Yang, Mengze Hong, Xiaojing Du, Zechen Hu, Xiongren Chen, Ziqi Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21328">https://arxiv.org/abs/2410.21328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21328">https://arxiv.org/pdf/2410.21328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21328]] Deconfounding Time Series Forecasting(https://arxiv.org/abs/2410.21328)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Time series forecasting is a critical task in various domains, where accurate predictions can drive informed decision-making. Traditional forecasting methods often rely on current observations of variables to predict future outcomes, typically overlooking the influence of latent confounders, unobserved variables that simultaneously affect both the predictors and the target outcomes. This oversight can introduce bias and degrade the performance of predictive models. In this study, we address this challenge by proposing an enhanced forecasting approach that incorporates representations of latent confounders derived from historical data. By integrating these confounders into the predictive process, our method aims to improve the accuracy and robustness of time series forecasts. The proposed approach is demonstrated through its application to climate science data, showing significant improvements over traditional methods that do not account for confounders.</li>
</ul>

<h3>Title: LLM Robustness Against Misinformation in Biomedical Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Alexander Bondarenko, Adrian Viehweger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21330">https://arxiv.org/abs/2410.21330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21330">https://arxiv.org/pdf/2410.21330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21330]] LLM Robustness Against Misinformation in Biomedical Question Answering(https://arxiv.org/abs/2410.21330)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The retrieval-augmented generation (RAG) approach is used to reduce the confabulation of large language models (LLMs) for question answering by retrieving and providing additional context coming from external knowledge sources (e.g., by adding the context to the prompt). However, injecting incorrect information can mislead the LLM to generate an incorrect answer. In this paper, we evaluate the effectiveness and robustness of four LLMs against misinformation - Gemma 2, GPT-4o-mini, Llama~3.1, and Mixtral - in answering biomedical questions. We assess the answer accuracy on yes-no and free-form questions in three scenarios: vanilla LLM answers (no context is provided), "perfect" augmented generation (correct context is provided), and prompt-injection attacks (incorrect context is provided). Our results show that Llama 3.1 (70B parameters) achieves the highest accuracy in both vanilla (0.651) and "perfect" RAG (0.802) scenarios. However, the accuracy gap between the models almost disappears with "perfect" RAG, suggesting its potential to mitigate the LLM's size-related effectiveness differences. We further evaluate the ability of the LLMs to generate malicious context on one hand and the LLM's robustness against prompt-injection attacks on the other hand, using metrics such as attack success rate (ASR), accuracy under attack, and accuracy drop. As adversaries, we use the same four LLMs (Gemma 2, GPT-4o-mini, Llama 3.1, and Mixtral) to generate incorrect context that is injected in the target model's prompt. Interestingly, Llama is shown to be the most effective adversary, causing accuracy drops of up to 0.48 for vanilla answers and 0.63 for "perfect" RAG across target models. Our analysis reveals that robustness rankings vary depending on the evaluation measure, highlighting the complexity of assessing LLM resilience to adversarial attacks.</li>
</ul>

<h3>Title: Beyond Interpretability: The Gains of Feature Monosemanticity on Model Robustness</h3>
<ul>
<li><strong>Authors: </strong>Qi Zhang, Yifei Wang, Jingyi Cui, Xiang Pan, Qi Lei, Stefanie Jegelka, Yisen Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21331">https://arxiv.org/abs/2410.21331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21331">https://arxiv.org/pdf/2410.21331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21331]] Beyond Interpretability: The Gains of Feature Monosemanticity on Model Robustness(https://arxiv.org/abs/2410.21331)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Deep learning models often suffer from a lack of interpretability due to polysemanticity, where individual neurons are activated by multiple unrelated semantics, resulting in unclear attributions of model behavior. Recent advances in monosemanticity, where neurons correspond to consistent and distinct semantics, have significantly improved interpretability but are commonly believed to compromise accuracy. In this work, we challenge the prevailing belief of the accuracy-interpretability tradeoff, showing that monosemantic features not only enhance interpretability but also bring concrete gains in model performance. Across multiple robust learning scenarios-including input and label noise, few-shot learning, and out-of-domain generalization-our results show that models leveraging monosemantic features significantly outperform those relying on polysemantic features. Furthermore, we provide empirical and theoretical understandings on the robustness gains of feature monosemanticity. Our preliminary analysis suggests that monosemanticity, by promoting better separation of feature representations, leads to more robust decision boundaries. This diverse evidence highlights the generality of monosemanticity in improving model robustness. As a first step in this new direction, we embark on exploring the learning benefits of monosemanticity beyond interpretability, supporting the long-standing hypothesis of linking interpretability and robustness. Code is available at \url{this https URL}.</li>
</ul>

<h3>Title: Building, Reusing, and Generalizing Abstract Representations from Concrete Sequences</h3>
<ul>
<li><strong>Authors: </strong>Shuchen Wu, Mirko Thalmann, Peter Dayan, Zeynep Akata, Eric Schulz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21332">https://arxiv.org/abs/2410.21332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21332">https://arxiv.org/pdf/2410.21332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21332]] Building, Reusing, and Generalizing Abstract Representations from Concrete Sequences(https://arxiv.org/abs/2410.21332)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Humans excel at learning abstract patterns across different sequences, filtering out irrelevant details, and transferring these generalized concepts to new sequences. In contrast, many sequence learning models lack the ability to abstract, which leads to memory inefficiency and poor transfer. We introduce a non-parametric hierarchical variable learning model (HVM) that learns chunks from sequences and abstracts contextually similar chunks as variables. HVM efficiently organizes memory while uncovering abstractions, leading to compact sequence representations. When learning on language datasets such as babyLM, HVM learns a more efficient dictionary than standard compression algorithms such as Lempel-Ziv. In a sequence recall task requiring the acquisition and transfer of variables embedded in sequences, we demonstrate HVM's sequence likelihood correlates with human recall times. In contrast, large language models (LLMs) struggle to transfer abstract variables as effectively as humans. From HVM's adjustable layer of abstraction, we demonstrate that the model realizes a precise trade-off between compression and generalization. Our work offers a cognitive model that captures the learning and transfer of abstract representations in human cognition and differentiates itself from the behavior of large language models.</li>
</ul>

<h3>Title: E(3)-invaraint diffusion model for pocket-aware peptide generation</h3>
<ul>
<li><strong>Authors: </strong>Po-Yu Liang, Jun Bai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21335">https://arxiv.org/abs/2410.21335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21335">https://arxiv.org/pdf/2410.21335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21335]] E(3)-invaraint diffusion model for pocket-aware peptide generation(https://arxiv.org/abs/2410.21335)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Biologists frequently desire protein inhibitors for a variety of reasons, including use as research tools for understanding biological processes and application to societal problems in agriculture, healthcare, etc. Immunotherapy, for instance, relies on immune checkpoint inhibitors to block checkpoint proteins, preventing their binding with partner proteins and boosting immune cell function against abnormal cells. Inhibitor discovery has long been a tedious process, which in recent years has been accelerated by computational approaches. Advances in artificial intelligence now provide an opportunity to make inhibitor discovery smarter than ever before. While extensive research has been conducted on computer-aided inhibitor discovery, it has mainly focused on either sequence-to-structure mapping, reverse mapping, or bio-activity prediction, making it unrealistic for biologists to utilize such tools. Instead, our work proposes a new method of computer-assisted inhibitor discovery: de novo pocket-aware peptide structure and sequence generation network. Our approach consists of two sequential diffusion models for end-to-end structure generation and sequence prediction. By leveraging angle and dihedral relationships between backbone atoms, we ensure an E(3)-invariant representation of peptide structures. Our results demonstrate that our method achieves comparable performance to state-of-the-art models, highlighting its potential in pocket-aware peptide design. This work offers a new approach for precise drug discovery using receptor-specific peptide generation.</li>
</ul>

<h3>Title: Fine-tuned Large Language Models (LLMs): Improved Prompt Injection Attacks Detection</h3>
<ul>
<li><strong>Authors: </strong>Md Abdur Rahman, Fan Wu, Alfredo Cuzzocrea, Sheikh Iqbal Ahamed</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21337">https://arxiv.org/abs/2410.21337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21337">https://arxiv.org/pdf/2410.21337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21337]] Fine-tuned Large Language Models (LLMs): Improved Prompt Injection Attacks Detection(https://arxiv.org/abs/2410.21337)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are becoming a popular tool as they have significantly advanced in their capability to tackle a wide range of language-based tasks. However, LLMs applications are highly vulnerable to prompt injection attacks, which poses a critical problem. These attacks target LLMs applications through using carefully designed input prompts to divert the model from adhering to original instruction, thereby it could execute unintended actions. These manipulations pose serious security threats which potentially results in data leaks, biased outputs, or harmful responses. This project explores the security vulnerabilities in relation to prompt injection attacks. To detect whether a prompt is vulnerable or not, we follows two approaches: 1) a pre-trained LLM, and 2) a fine-tuned LLM. Then, we conduct a thorough analysis and comparison of the classification performance. Firstly, we use pre-trained XLM-RoBERTa model to detect prompt injections using test dataset without any fine-tuning and evaluate it by zero-shot classification. Then, this proposed work will apply supervised fine-tuning to this pre-trained LLM using a task-specific labeled dataset from deepset in huggingface, and this fine-tuned model achieves impressive results with 99.13\% accuracy, 100\% precision, 98.33\% recall and 99.15\% F1-score thorough rigorous experimentation and evaluation. We observe that our approach is highly efficient in detecting prompt injection attacks.</li>
</ul>

<h3>Title: FinTeamExperts: Role Specialized MOEs For Financial Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yue Yu, Prayag Tiwari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21338">https://arxiv.org/abs/2410.21338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21338">https://arxiv.org/pdf/2410.21338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21338]] FinTeamExperts: Role Specialized MOEs For Financial Analysis(https://arxiv.org/abs/2410.21338)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), such as ChatGPT, Phi3 and Llama-3, are leading a significant leap in AI, as they can generalize knowledge from their training to new tasks without fine-tuning. However, their application in the financial domain remains relatively limited. The financial field is inherently complex, requiring a deep understanding across various perspectives, from macro, micro economic trend to quantitative analysis. Motivated by this complexity, a mixture of expert LLMs tailored to specific financial domains could offer a more comprehensive understanding for intricate financial tasks. In this paper, we present the FinTeamExperts, a role-specialized LLM framework structured as a Mixture of Experts (MOEs) for financial analysis. The framework simulates a collaborative team setting by training each model to specialize in distinct roles: Macro Analysts, Micro analysts, and Quantitative Analysts. This role-specific specialization enhances the model's ability to integrate their domain-specific expertise. We achieve this by training three 8-billion parameter models on different corpus, each dedicated to excelling in specific finance-related roles. We then instruct-tune FinTeamExperts on downstream tasks to align with practical financial tasks. The experimental results show that FinTeamExperts outperform all models of the same size and larger on three out of four datasets. On the fourth dataset, which presents a more complex task, FinTeamExperts still surpass all models of the same size. This highlights the success of our role-based specialization approach and the continued training approach for FinTeamExperts.</li>
</ul>

<h3>Title: Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments</h3>
<ul>
<li><strong>Authors: </strong>Yuzhe Yang, Yipeng Du, Ahmad Farhan, Claudio Angione, Yue Zhao, Harry Yang, Fielding Johnston, James Buban, Patrick Colangelo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21340">https://arxiv.org/abs/2410.21340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21340">https://arxiv.org/pdf/2410.21340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21340]] Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments(https://arxiv.org/abs/2410.21340)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>The deployment of large-scale models, such as large language models (LLMs) and sophisticated image generation systems, incurs substantial costs due to their computational demands. To mitigate these costs and address challenges related to scalability and data security, there is a growing shift towards decentralized systems for deploying such models. In these decentralized environments, efficient inference acceleration becomes crucial to manage computational resources effectively and enhance system responsiveness. In this work, we address the challenge of selecting optimal acceleration methods in decentralized systems by introducing a meta-learning-based framework. This framework automates the selection process by learning from historical performance data of various acceleration techniques across different tasks. Unlike traditional methods that rely on random selection or expert intuition, our approach systematically identifies the best acceleration strategies based on the specific characteristics of each task. We demonstrate that our meta-learning framework not only streamlines the decision-making process but also consistently outperforms conventional methods in terms of efficiency and performance. Our results highlight the potential of meta-learning to revolutionize inference acceleration in decentralized AI systems, offering a path towards more democratic and economically feasible artificial intelligence solutions.</li>
</ul>

<h3>Title: Towards Trustworthy Machine Learning in Production: An Overview of the Robustness in MLOps Approach</h3>
<ul>
<li><strong>Authors: </strong>Firas Bayram, Bestoun S. Ahmed</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21346">https://arxiv.org/abs/2410.21346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21346">https://arxiv.org/pdf/2410.21346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21346]] Towards Trustworthy Machine Learning in Production: An Overview of the Robustness in MLOps Approach(https://arxiv.org/abs/2410.21346)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI), and especially its sub-field of Machine Learning (ML), are impacting the daily lives of everyone with their ubiquitous applications. In recent years, AI researchers and practitioners have introduced principles and guidelines to build systems that make reliable and trustworthy decisions. From a practical perspective, conventional ML systems process historical data to extract the features that are consequently used to train ML models that perform the desired task. However, in practice, a fundamental challenge arises when the system needs to be operationalized and deployed to evolve and operate in real-life environments continuously. To address this challenge, Machine Learning Operations (MLOps) have emerged as a potential recipe for standardizing ML solutions in deployment. Although MLOps demonstrated great success in streamlining ML processes, thoroughly defining the specifications of robust MLOps approaches remains of great interest to researchers and practitioners. In this paper, we provide a comprehensive overview of the trustworthiness property of MLOps systems. Specifically, we highlight technical practices to achieve robust MLOps systems. In addition, we survey the existing research approaches that address the robustness aspects of ML systems in production. We also review the tools and software available to build MLOps systems and summarize their support to handle the robustness aspects. Finally, we present the open challenges and propose possible future directions and opportunities within this emerging field. The aim of this paper is to provide researchers and practitioners working on practical AI applications with a comprehensive view to adopt robust ML solutions in production environments.</li>
</ul>

<h3>Title: Large Language Model Benchmarks in Medical Tasks</h3>
<ul>
<li><strong>Authors: </strong>Lawrence K.Q. Yan, Ming Li, Yichao Zhang, Caitlyn Heqi Yin, Cheng Fei, Benji Peng, Ziqian Bi, Pohsun Feng, Keyu Chen, Junyu Liu, Qian Niu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21348">https://arxiv.org/abs/2410.21348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21348">https://arxiv.org/pdf/2410.21348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21348]] Large Language Model Benchmarks in Medical Tasks(https://arxiv.org/abs/2410.21348)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the increasing application of large language models (LLMs) in the medical domain, evaluating these models' performance using benchmark datasets has become crucial. This paper presents a comprehensive survey of various benchmark datasets employed in medical LLM tasks. These datasets span multiple modalities including text, image, and multimodal benchmarks, focusing on different aspects of medical knowledge such as electronic health records (EHRs), doctor-patient dialogues, medical question-answering, and medical image captioning. The survey categorizes the datasets by modality, discussing their significance, data structure, and impact on the development of LLMs for clinical tasks such as diagnosis, report generation, and predictive decision support. Key benchmarks include MIMIC-III, MIMIC-IV, BioASQ, PubMedQA, and CheXpert, which have facilitated advancements in tasks like medical report generation, clinical summarization, and synthetic data generation. The paper summarizes the challenges and opportunities in leveraging these benchmarks for advancing multimodal medical intelligence, emphasizing the need for datasets with a greater degree of language diversity, structured omics data, and innovative approaches to synthesis. This work also provides a foundation for future research in the application of LLMs in medicine, contributing to the evolving field of medical artificial intelligence.</li>
</ul>

<h3>Title: FALCON: Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization system</h3>
<ul>
<li><strong>Authors: </strong>Zeyuan Li, Yangfan He, Lewei He, Jianhui Wang, Tianyu Shi, Bin Lei, Yuchen Li, Qiuwu Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21349">https://arxiv.org/abs/2410.21349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21349">https://arxiv.org/pdf/2410.21349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21349]] FALCON: Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization system(https://arxiv.org/abs/2410.21349)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, large language models (LLMs) have achieved significant progress in automated code generation. Despite their strong instruction-following capabilities, these models frequently struggled to align with user intent in coding scenarios. In particular, they were hampered by datasets that lacked diversity and failed to address specialized tasks or edge cases. Furthermore, challenges in supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) led to failures in generating precise, human-intent-aligned code. To tackle these challenges and improve the code generation performance for automated programming systems, we propose Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization (i.e., FALCON). FALCON is structured into two hierarchical levels. From the global level, long-term memory improves code quality by retaining and applying learned knowledge. At the local level, short-term memory allows for the incorporation of immediate feedback from compilers and AI systems. Additionally, we introduce meta-reinforcement learning with feedback rewards to solve the global-local bi-level optimization problem and enhance the model's adaptability across diverse code generation tasks. Extensive experiments demonstrate that our technique achieves state-of-the-art performance, leading other reinforcement learning methods by more than 4.5 percentage points on the MBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The open-sourced code is publicly available at this https URL.</li>
</ul>

<h3>Title: LinFormer: A Linear-based Lightweight Transformer Architecture For Time-Aware MIMO Channel Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yanliang Jin, Yifan Wu, Yuan Gao, Shunqing Zhang, Shugong Xu, Cheng-Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21351">https://arxiv.org/abs/2410.21351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21351">https://arxiv.org/pdf/2410.21351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21351]] LinFormer: A Linear-based Lightweight Transformer Architecture For Time-Aware MIMO Channel Prediction(https://arxiv.org/abs/2410.21351)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The emergence of 6th generation (6G) mobile networks brings new challenges in supporting high-mobility communications, particularly in addressing the issue of channel aging. While existing channel prediction methods offer improved accuracy at the expense of increased computational complexity, limiting their practical application in mobile networks. To address these challenges, we present LinFormer, an innovative channel prediction framework based on a scalable, all-linear, encoder-only Transformer model. Our approach, inspired by natural language processing (NLP) models such as BERT, adapts an encoder-only architecture specifically for channel prediction tasks. We propose replacing the computationally intensive attention mechanism commonly used in Transformers with a time-aware multi-layer perceptron (TMLP), significantly reducing computational demands. The inherent time awareness of TMLP module makes it particularly suitable for channel prediction tasks. We enhance LinFormer's training process by employing a weighted mean squared error loss (WMSELoss) function and data augmentation techniques, leveraging larger, readily available communication datasets. Our approach achieves a substantial reduction in computational complexity while maintaining high prediction accuracy, making it more suitable for deployment in cost-effective base stations (BS). Comprehensive experiments using both simulated and measured data demonstrate that LinFormer outperforms existing methods across various mobility scenarios, offering a promising solution for future wireless communication systems.</li>
</ul>

<h3>Title: LLMCBench: Benchmarking Large Language Model Compression for Efficient Deployment</h3>
<ul>
<li><strong>Authors: </strong>Ge Yang, Changyi He, Jinyang Guo, Jianyu Wu, Yifu Ding, Aishan Liu, Haotong Qin, Pengliang Ji, Xianglong Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21352">https://arxiv.org/abs/2410.21352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21352">https://arxiv.org/pdf/2410.21352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21352]] LLMCBench: Benchmarking Large Language Model Compression for Efficient Deployment(https://arxiv.org/abs/2410.21352)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although large language models (LLMs) have demonstrated their strong intelligence ability, the high demand for computation and storage hinders their practical application. To this end, many model compression techniques are proposed to increase the efficiency of LLMs. However, current researches only validate their methods on limited models, datasets, metrics, etc, and still lack a comprehensive evaluation under more general scenarios. So it is still a question of which model compression approach we should use under a specific case. To mitigate this gap, we present the Large Language Model Compression Benchmark (LLMCBench), a rigorously designed benchmark with an in-depth analysis for LLM compression algorithms. We first analyze the actual model production requirements and carefully design evaluation tracks and metrics. Then, we conduct extensive experiments and comparison using multiple mainstream LLM compression approaches. Finally, we perform an in-depth analysis based on the evaluation and provide useful insight for LLM compression design. We hope our LLMCBench can contribute insightful suggestions for LLM compression algorithm design and serve as a foundation for future research. Our code is available at this https URL.</li>
</ul>

<h3>Title: Causal Interventions on Causal Paths: Mapping GPT-2's Reasoning From Syntax to Semantics</h3>
<ul>
<li><strong>Authors: </strong>Isabelle Lee, Joshua Lum, Ziyi Liu, Dani Yogatama</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21353">https://arxiv.org/abs/2410.21353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21353">https://arxiv.org/pdf/2410.21353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21353]] Causal Interventions on Causal Paths: Mapping GPT-2's Reasoning From Syntax to Semantics(https://arxiv.org/abs/2410.21353)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>While interpretability research has shed light on some internal algorithms utilized by transformer-based LLMs, reasoning in natural language, with its deep contextuality and ambiguity, defies easy categorization. As a result, formulating clear and motivating questions for circuit analysis that rely on well-defined in-domain and out-of-domain examples required for causal interventions is challenging. Although significant work has investigated circuits for specific tasks, such as indirect object identification (IOI), deciphering natural language reasoning through circuits remains difficult due to its inherent complexity. In this work, we take initial steps to characterize causal reasoning in LLMs by analyzing clear-cut cause-and-effect sentences like "I opened an umbrella because it started raining," where causal interventions may be possible through carefully crafted scenarios using GPT-2 small. Our findings indicate that causal syntax is localized within the first 2-3 layers, while certain heads in later layers exhibit heightened sensitivity to nonsensical variations of causal sentences. This suggests that models may infer reasoning by (1) detecting syntactic cues and (2) isolating distinct heads in the final layers that focus on semantic relationships.</li>
</ul>

<h3>Title: Energy-Based Diffusion Language Models for Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Minkai Xu, Tomas Geffner, Karsten Kreis, Weili Nie, Yilun Xu, Jure Leskovec, Stefano Ermon, Arash Vahdat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21357">https://arxiv.org/abs/2410.21357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21357">https://arxiv.org/pdf/2410.21357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21357]] Energy-Based Diffusion Language Models for Text Generation(https://arxiv.org/abs/2410.21357)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Despite remarkable progress in autoregressive language models, alternative generative paradigms beyond left-to-right generation are still being actively explored. Discrete diffusion models, with the capacity for parallel generation, have recently emerged as a promising alternative. Unfortunately, these models still underperform the autoregressive counterparts, with the performance gap increasing when reducing the number of sampling steps. Our analysis reveals that this degradation is a consequence of an imperfect approximation used by diffusion models. In this work, we propose Energy-based Diffusion Language Model (EDLM), an energy-based model operating at the full sequence level for each diffusion step, introduced to improve the underlying approximation used by diffusion models. More specifically, we introduce an EBM in a residual form, and show that its parameters can be obtained by leveraging a pretrained autoregressive model or by finetuning a bidirectional transformer via noise contrastive estimation. We also propose an efficient generation algorithm via parallel important sampling. Comprehensive experiments on language modeling benchmarks show that our model can consistently outperform state-of-the-art diffusion models by a significant margin, and approaches autoregressive models' perplexity. We further show that, without any generation performance drop, our framework offers a 1.3$\times$ sampling speedup over existing diffusion models.</li>
</ul>

<h3>Title: Can Machines Think Like Humans? A Behavioral Evaluation of LLM-Agents in Dictator Games</h3>
<ul>
<li><strong>Authors: </strong>Ji Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21359">https://arxiv.org/abs/2410.21359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21359">https://arxiv.org/pdf/2410.21359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21359]] Can Machines Think Like Humans? A Behavioral Evaluation of LLM-Agents in Dictator Games(https://arxiv.org/abs/2410.21359)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Model (LLM)-based agents increasingly undertake real-world tasks and engage with human society, how well do we understand their behaviors? This study (1) investigates how LLM agents' prosocial behaviors -- a fundamental social norm -- can be induced by different personas and benchmarked against human behaviors; and (2) introduces a behavioral approach to evaluate the performance of LLM agents in complex decision-making scenarios. We explored how different personas and experimental framings affect these AI agents' altruistic behavior in dictator games and compared their behaviors within the same LLM family, across various families, and with human behaviors. Our findings reveal substantial variations and inconsistencies among LLMs and notable differences compared to human behaviors. Merely assigning a human-like identity to LLMs does not produce human-like behaviors. Despite being trained on extensive human-generated data, these AI agents cannot accurately predict human decisions. LLM agents are not able to capture the internal processes of human decision-making, and their alignment with human behavior is highly variable and dependent on specific model architectures and prompt formulations; even worse, such dependence does not follow a clear pattern.</li>
</ul>

<h3>Title: A Survey on Automatic Credibility Assessment of Textual Credibility Signals in the Era of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ivan Srba, Olesya Razuvayevskaya, Jo√£o A. Leite, Robert Moro, Ipek Baris Schlicht, Sara Tonelli, Francisco Moreno Garc√≠a, Santiago Barrio Lottmann, Denis Teyssou, Valentin Porcellini, Carolina Scarton, Kalina Bontcheva, Maria Bielikova</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21360">https://arxiv.org/abs/2410.21360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21360">https://arxiv.org/pdf/2410.21360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21360]] A Survey on Automatic Credibility Assessment of Textual Credibility Signals in the Era of Large Language Models(https://arxiv.org/abs/2410.21360)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>In the current era of social media and generative AI, an ability to automatically assess the credibility of online social media content is of tremendous importance. Credibility assessment is fundamentally based on aggregating credibility signals, which refer to small units of information, such as content factuality, bias, or a presence of persuasion techniques, into an overall credibility score. Credibility signals provide a more granular, more easily explainable and widely utilizable information in contrast to currently predominant fake news detection, which utilizes various (mostly latent) features. A growing body of research on automatic credibility assessment and detection of credibility signals can be characterized as highly fragmented and lacking mutual interconnections. This issue is even more prominent due to a lack of an up-to-date overview of research works on automatic credibility assessment. In this survey, we provide such systematic and comprehensive literature review of 175 research papers while focusing on textual credibility signals and Natural Language Processing (NLP), which undergoes a significant advancement due to Large Language Models (LLMs). While positioning the NLP research into the context of other multidisciplinary research works, we tackle with approaches for credibility assessment as well as with 9 categories of credibility signals (we provide a thorough analysis for 3 of them, namely: 1) factuality, subjectivity and bias, 2) persuasion techniques and logical fallacies, and 3) claims and veracity). Following the description of the existing methods, datasets and tools, we identify future challenges and opportunities, while paying a specific attention to recent rapid development of generative AI.</li>
</ul>

<h3>Title: Domain Adaptation with a Single Vision-Language Embedding</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick P√©rez, Raoul de Charette</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21361">https://arxiv.org/abs/2410.21361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21361">https://arxiv.org/pdf/2410.21361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21361]] Domain Adaptation with a Single Vision-Language Embedding(https://arxiv.org/abs/2410.21361)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Domain adaptation has been extensively investigated in computer vision but still requires access to target data at the training time, which might be difficult to obtain in some uncommon conditions. In this paper, we present a new framework for domain adaptation relying on a single Vision-Language (VL) latent embedding instead of full target data. First, leveraging a contrastive language-image pre-training model (CLIP), we propose prompt/photo-driven instance normalization (PIN). PIN is a feature augmentation method that mines multiple visual styles using a single target VL latent embedding, by optimizing affine transformations of low-level source features. The VL embedding can come from a language prompt describing the target domain, a partially optimized language prompt, or a single unlabeled target image. Second, we show that these mined styles (i.e., augmentations) can be used for zero-shot (i.e., target-free) and one-shot unsupervised domain adaptation. Experiments on semantic segmentation demonstrate the effectiveness of the proposed method, which outperforms relevant baselines in the zero-shot and one-shot settings.</li>
</ul>

<h3>Title: Unveiling the Role of Expert Guidance: A Comparative Analysis of User-centered Imitation Learning and Traditional Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Amr Gomaa, Bilal Mahdy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21403">https://arxiv.org/abs/2410.21403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21403">https://arxiv.org/pdf/2410.21403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21403]] Unveiling the Role of Expert Guidance: A Comparative Analysis of User-centered Imitation Learning and Traditional Reinforcement Learning(https://arxiv.org/abs/2410.21403)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Integration of human feedback plays a key role in improving the learning capabilities of intelligent systems. This comparative study delves into the performance, robustness, and limitations of imitation learning compared to traditional reinforcement learning methods within these systems. Recognizing the value of human-in-the-loop feedback, we investigate the influence of expert guidance and suboptimal demonstrations on the learning process. Through extensive experimentation and evaluations conducted in a pre-existing simulation environment using the Unity platform, we meticulously analyze the effectiveness and limitations of these learning approaches. The insights gained from this study contribute to the advancement of human-centered artificial intelligence by highlighting the benefits and challenges associated with the incorporation of human feedback into the learning process. Ultimately, this research promotes the development of models that can effectively address complex real-world problems.</li>
</ul>

<h3>Title: Exploring reinforcement learning for incident response in autonomous military vehicles</h3>
<ul>
<li><strong>Authors: </strong>Henrik Madsen, Gudmund Grov, Federico Mancini, Magnus Baksaas, √Övald √Öslaugson Sommervoll</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21407">https://arxiv.org/abs/2410.21407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21407">https://arxiv.org/pdf/2410.21407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21407]] Exploring reinforcement learning for incident response in autonomous military vehicles(https://arxiv.org/abs/2410.21407)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Unmanned vehicles able to conduct advanced operations without human intervention are being developed at a fast pace for many purposes. Not surprisingly, they are also expected to significantly change how military operations can be conducted. To leverage the potential of this new technology in a physically and logically contested environment, security risks are to be assessed and managed accordingly. Research on this topic points to autonomous cyber defence as one of the capabilities that may be needed to accelerate the adoption of these vehicles for military purposes. Here, we pursue this line of investigation by exploring reinforcement learning to train an agent that can autonomously respond to cyber attacks on unmanned vehicles in the context of a military operation. We first developed a simple simulation environment to quickly prototype and test some proof-of-concept agents for an initial evaluation. This agent was then applied to a more realistic simulation environment and finally deployed on an actual unmanned ground vehicle for even more realism. A key contribution of our work is demonstrating that reinforcement learning is a viable approach to train an agent that can be used for autonomous cyber defence on a real unmanned ground vehicle, even when trained in a simple simulation environment.</li>
</ul>

<h3>Title: SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization</h3>
<ul>
<li><strong>Authors: </strong>Wanhua Li, Zibin Meng, Jiawei Zhou, Donglai Wei, Chuang Gan, Hanspeter Pfister</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21411">https://arxiv.org/abs/2410.21411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21411">https://arxiv.org/pdf/2410.21411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21411]] SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization(https://arxiv.org/abs/2410.21411)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Social relation reasoning aims to identify relation categories such as friends, spouses, and colleagues from images. While current methods adopt the paradigm of training a dedicated network end-to-end using labeled image data, they are limited in terms of generalizability and interpretability. To address these issues, we first present a simple yet well-crafted framework named {\name}, which combines the perception capability of Vision Foundation Models (VFMs) and the reasoning capability of Large Language Models (LLMs) within a modular framework, providing a strong baseline for social relation recognition. Specifically, we instruct VFMs to translate image content into a textual social story, and then utilize LLMs for text-based reasoning. {\name} introduces systematic design principles to adapt VFMs and LLMs separately and bridge their gaps. Without additional model training, it achieves competitive zero-shot results on two databases while offering interpretable answers, as LLMs can generate language-based explanations for the decisions. The manual prompt design process for LLMs at the reasoning phase is tedious and an automated prompt optimization method is desired. As we essentially convert a visual classification task into a generative task of LLMs, automatic prompt optimization encounters a unique long prompt optimization issue. To address this issue, we further propose the Greedy Segment Prompt Optimization (GSPO), which performs a greedy search by utilizing gradient information at the segment level. Experimental results show that GSPO significantly improves performance, and our method also generalizes to different image styles. The code is available at this https URL.</li>
</ul>

<h3>Title: Sum-of-squares lower bounds for Non-Gaussian Component Analysis</h3>
<ul>
<li><strong>Authors: </strong>Ilias Diakonikolas, Sushrut Karmalkar, Shuo Pang, Aaron Potechin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CC, cs.DM, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21426">https://arxiv.org/abs/2410.21426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21426">https://arxiv.org/pdf/2410.21426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21426]] Sum-of-squares lower bounds for Non-Gaussian Component Analysis(https://arxiv.org/abs/2410.21426)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Non-Gaussian Component Analysis (NGCA) is the statistical task of finding a non-Gaussian direction in a high-dimensional dataset. Specifically, given i.i.d.\ samples from a distribution $P^A_{v}$ on $\mathbb{R}^n$ that behaves like a known distribution $A$ in a hidden direction $v$ and like a standard Gaussian in the orthogonal complement, the goal is to approximate the hidden direction. The standard formulation posits that the first $k-1$ moments of $A$ match those of the standard Gaussian and the $k$-th moment differs. Under mild assumptions, this problem has sample complexity $O(n)$. On the other hand, all known efficient algorithms require $\Omega(n^{k/2})$ samples. Prior work developed sharp Statistical Query and low-degree testing lower bounds suggesting an information-computation tradeoff for this problem. Here we study the complexity of NGCA in the Sum-of-Squares (SoS) framework. Our main contribution is the first super-constant degree SoS lower bound for NGCA. Specifically, we show that if the non-Gaussian distribution $A$ matches the first $(k-1)$ moments of $\mathcal{N}(0, 1)$ and satisfies other mild conditions, then with fewer than $n^{(1 - \varepsilon)k/2}$ many samples from the normal distribution, with high probability, degree $(\log n)^{{1\over 2}-o_n(1)}$ SoS fails to refute the existence of such a direction $v$. Our result significantly strengthens prior work by establishing a super-polynomial information-computation tradeoff against a broader family of algorithms. As corollaries, we obtain SoS lower bounds for several problems in robust statistics and the learning of mixture models. Our SoS lower bound proof introduces a novel technique, that we believe may be of broader interest, and a number of refinements over existing methods.</li>
</ul>

<h3>Title: TACO: Adversarial Camouflage Optimization on Trucks to Fool Object Detectors</h3>
<ul>
<li><strong>Authors: </strong>Adonisz Dimitriu, Tam√°s Michaletzky, Viktor Remeli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21443">https://arxiv.org/abs/2410.21443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21443">https://arxiv.org/pdf/2410.21443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21443]] TACO: Adversarial Camouflage Optimization on Trucks to Fool Object Detectors(https://arxiv.org/abs/2410.21443)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Adversarial attacks threaten the reliability of machine learning models in critical applications like autonomous vehicles and defense systems. As object detectors become more robust with models like YOLOv8, developing effective adversarial methodologies is increasingly challenging. We present Truck Adversarial Camouflage Optimization (TACO), a novel framework that generates adversarial camouflage patterns on 3D vehicle models to deceive state-of-the-art object detectors. Adopting Unreal Engine 5, TACO integrates differentiable rendering with a Photorealistic Rendering Network to optimize adversarial textures targeted at YOLOv8. To ensure the generated textures are both effective in deceiving detectors and visually plausible, we introduce the Convolutional Smooth Loss function, a generalized smooth loss function. Experimental evaluations demonstrate that TACO significantly degrades YOLOv8's detection performance, achieving an AP@0.5 of 0.0099 on unseen test data. Furthermore, these adversarial patterns exhibit strong transferability to other object detection models such as Faster R-CNN and earlier YOLO versions.</li>
</ul>

<h3>Title: A Temporal Linear Network for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Remi Genet, Hugo Inzirillo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21448">https://arxiv.org/abs/2410.21448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21448">https://arxiv.org/pdf/2410.21448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21448]] A Temporal Linear Network for Time Series Forecasting(https://arxiv.org/abs/2410.21448)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Recent research has challenged the necessity of complex deep learning architectures for time series forecasting, demonstrating that simple linear models can often outperform sophisticated approaches. Building upon this insight, we introduce a novel architecture the Temporal Linear Net (TLN), that extends the capabilities of linear models while maintaining interpretability and computational efficiency. TLN is designed to effectively capture both temporal and feature-wise dependencies in multivariate time series data. Our approach is a variant of TSMixer that maintains strict linearity throughout its architecture. TSMixer removes activation functions, introduces specialized kernel initializations, and incorporates dilated convolutions to handle various time scales, while preserving the linear nature of the model. Unlike transformer-based models that may lose temporal information due to their permutation-invariant nature, TLN explicitly preserves and leverages the temporal structure of the input data. A key innovation of TLN is its ability to compute an equivalent linear model, offering a level of interpretability not found in more complex architectures such as TSMixer. This feature allows for seamless conversion between the full TLN model and its linear equivalent, facilitating both training flexibility and inference optimization.</li>
</ul>

<h3>Title: Inverting Gradient Attacks Naturally Makes Data Poisons: An Availability Attack on Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Wassim Bouaziz, El-Mahdi El-Mhamdi, Nicolas Usunier</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21453">https://arxiv.org/abs/2410.21453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21453">https://arxiv.org/pdf/2410.21453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21453]] Inverting Gradient Attacks Naturally Makes Data Poisons: An Availability Attack on Neural Networks(https://arxiv.org/abs/2410.21453)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Gradient attacks and data poisoning tamper with the training of machine learning algorithms to maliciously alter them and have been proven to be equivalent in convex settings. The extent of harm these attacks can produce in non-convex settings is still to be determined. Gradient attacks can affect far less systems than data poisoning but have been argued to be more harmful since they can be arbitrary, whereas data poisoning reduces the attacker's power to only being able to inject data points to training sets, via e.g. legitimate participation in a collaborative dataset. This raises the question of whether the harm made by gradient attacks can be matched by data poisoning in non-convex settings. In this work, we provide a positive answer in a worst-case scenario and show how data poisoning can mimic a gradient attack to perform an availability attack on (non-convex) neural networks. Through gradient inversion, commonly used to reconstruct data points from actual gradients, we show how reconstructing data points out of malicious gradients can be sufficient to perform a range of attacks. This allows us to show, for the first time, an availability attack on neural networks through data poisoning, that degrades the model's performances to random-level through a minority (as low as 1%) of poisoned points.</li>
</ul>

<h3>Title: Constrained Transformer-Based Porous Media Generation to Spatial Distribution of Rock Properties</h3>
<ul>
<li><strong>Authors: </strong>Zihan Ren, Sanjay Srinivasan, Dustin Crandall</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21462">https://arxiv.org/abs/2410.21462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21462">https://arxiv.org/pdf/2410.21462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21462]] Constrained Transformer-Based Porous Media Generation to Spatial Distribution of Rock Properties(https://arxiv.org/abs/2410.21462)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Pore-scale modeling of rock images based on information in 3D micro-computed tomography data is crucial for studying complex subsurface processes such as CO2 and brine multiphase flow during Geologic Carbon Storage (GCS). While deep learning models can generate 3D rock microstructures that match static rock properties, they have two key limitations: they don't account for the spatial distribution of rock properties that can have an important influence on the flow and transport characteristics (such as permeability and relative permeability) of the rock and they generate structures below the representative elementary volume (REV) scale for those transport properties. Addressing these issues is crucial for building a consistent workflow between pore-scale analysis and field-scale modeling. To address these challenges, we propose a two-stage modeling framework that combines a Vector Quantized Variational Autoencoder (VQVAE) and a transformer model for spatial upscaling and arbitrary-size 3D porous media reconstruction in an autoregressive manner. The VQVAE first compresses and quantizes sub-volume training images into low-dimensional tokens, while we train a transformer to spatially assemble these tokens into larger images following specific spatial order. By employing a multi-token generation strategy, our approach preserves both sub-volume integrity and spatial relationships among these sub-image patches. We demonstrate the effectiveness of our multi-token transformer generation approach and validate it using real data from a test well, showcasing its potential to generate models for the porous media at the well scale using only a spatial porosity model. The interpolated representative porous media that reflect field-scale geological properties accurately model transport properties, including permeability and multiphase flow relative permeability of CO2 and brine.</li>
</ul>

<h3>Title: ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, Beidi Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21465">https://arxiv.org/abs/2410.21465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21465">https://arxiv.org/pdf/2410.21465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21465]] ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference(https://arxiv.org/abs/2410.21465)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\times$ larger batch sizes and boost throughput by up to 3.04$\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at this https URL.</li>
</ul>

<h3>Title: AdvI2I: Adversarial Image Attack on Image-to-Image Diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Yaopei Zeng, Yuanpu Cao, Bochuan Cao, Yurui Chang, Jinghui Chen, Lu Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21471">https://arxiv.org/abs/2410.21471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21471">https://arxiv.org/pdf/2410.21471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21471]] AdvI2I: Adversarial Image Attack on Image-to-Image Diffusion models(https://arxiv.org/abs/2410.21471)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have significantly enhanced the quality of image synthesis, yet they have also introduced serious safety concerns, particularly the generation of Not Safe for Work (NSFW) content. Previous research has demonstrated that adversarial prompts can be used to generate NSFW content. However, such adversarial text prompts are often easily detectable by text-based filters, limiting their efficacy. In this paper, we expose a previously overlooked vulnerability: adversarial image attacks targeting Image-to-Image (I2I) diffusion models. We propose AdvI2I, a novel framework that manipulates input images to induce diffusion models to generate NSFW content. By optimizing a generator to craft adversarial images, AdvI2I circumvents existing defense mechanisms, such as Safe Latent Diffusion (SLD), without altering the text prompts. Furthermore, we introduce AdvI2I-Adaptive, an enhanced version that adapts to potential countermeasures and minimizes the resemblance between adversarial images and NSFW concept embeddings, making the attack more resilient against defenses. Through extensive experiments, we demonstrate that both AdvI2I and AdvI2I-Adaptive can effectively bypass current safeguards, highlighting the urgent need for stronger security measures to address the misuse of I2I diffusion models.</li>
</ul>

<h3>Title: Estimating Causal Effects of Text Interventions Leveraging LLMs</h3>
<ul>
<li><strong>Authors: </strong>Siyi Guo, Myrl G. Marmarelis, Fred Morstatter, Kristina Lerman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21474">https://arxiv.org/abs/2410.21474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21474">https://arxiv.org/pdf/2410.21474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21474]] Estimating Causal Effects of Text Interventions Leveraging LLMs(https://arxiv.org/abs/2410.21474)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Quantifying the effect of textual interventions in social systems, such as reducing anger in social media posts to see its impact on engagement, poses significant challenges. Direct interventions on real-world systems are often infeasible, necessitating reliance on observational data. Traditional causal inference methods, typically designed for binary or discrete treatments, are inadequate for handling the complex, high-dimensional nature of textual data. This paper addresses these challenges by proposing a novel approach, CausalDANN, to estimate causal effects using text transformations facilitated by large language models (LLMs). Unlike existing methods, our approach accommodates arbitrary textual interventions and leverages text-level classifiers with domain adaptation ability to produce robust effect estimates against domain shifts, even when only the control group is observed. This flexibility in handling various text interventions is a key advancement in causal estimation for textual data, offering opportunities to better understand human behaviors and develop effective policies within social systems.</li>
</ul>

<h3>Title: TransformLLM: Adapting Large Language Models via LLM-Transformed Reading Comprehension Text</h3>
<ul>
<li><strong>Authors: </strong>Iftach Arbel, Yehonathan Refael, Ofir Lindenbaum</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21479">https://arxiv.org/abs/2410.21479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21479">https://arxiv.org/pdf/2410.21479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21479]] TransformLLM: Adapting Large Language Models via LLM-Transformed Reading Comprehension Text(https://arxiv.org/abs/2410.21479)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown promise in highly-specialized domains, however challenges are still present in aspects of accuracy and costs. These limitations restrict the usage of existing models in domain-specific tasks. While fine-tuning pre-trained models have shown promising results, this process can be computationally expensive and require massive datasets of the specialized application in hand. In this work, we bridge that gap. We have developed Phi-2-Legal and Mistral-Legal-7B, which are language models specifically designed for legal applications. These models are based on Phi-2 and Mistral-7B-v0.1, and have gone through continued pre-training with over 500 million tokens of legal texts. Our innovative approach significantly improves capabilities in legal tasks by using Large Language Models (LLMs) to convert raw training data into reading comprehension text. Our legal LLMs have demonstrated superior performance in legal benchmarks, even outperforming models trained on much larger datasets with more resources. This work emphasizes the effectiveness of continued pre-training on domain-specific texts, while using affordable LLMs for data conversion, which gives these models domain expertise while retaining general language understanding capabilities. While this work uses the legal domain as a test case, our method can be scaled and applied to any pre-training dataset, resulting in significant improvements across different tasks. These findings underscore the potential of domain-adaptive pre-training and reading comprehension for the development of highly effective domain-specific language models.</li>
</ul>

<h3>Title: AiSciVision: A Framework for Specializing Large Multimodal Models in Scientific Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Brendan Hogan, Anmol Kabra, Felipe Siqueira Pacheco, Laura Greenstreet, Joshua Fan, Aaron Ferber, Marta Ummus, Alecsander Brito, Olivia Graham, Lillian Aoki, Drew Harvell, Alex Flecker, Carla Gomes</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21480">https://arxiv.org/abs/2410.21480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21480">https://arxiv.org/pdf/2410.21480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21480]] AiSciVision: A Framework for Specializing Large Multimodal Models in Scientific Image Classification(https://arxiv.org/abs/2410.21480)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Trust and interpretability are crucial for the use of Artificial Intelligence (AI) in scientific research, but current models often operate as black boxes offering limited transparency and justifications for their outputs. We introduce AiSciVision, a framework that specializes Large Multimodal Models (LMMs) into interactive research partners and classification models for image classification tasks in niche scientific domains. Our framework uses two key components: (1) Visual Retrieval-Augmented Generation (VisRAG) and (2) domain-specific tools utilized in an agentic workflow. To classify a target image, AiSciVision first retrieves the most similar positive and negative labeled images as context for the LMM. Then the LMM agent actively selects and applies tools to manipulate and inspect the target image over multiple rounds, refining its analysis before making a final prediction. These VisRAG and tooling components are designed to mirror the processes of domain experts, as humans often compare new data to similar examples and use specialized tools to manipulate and inspect images before arriving at a conclusion. Each inference produces both a prediction and a natural language transcript detailing the reasoning and tool usage that led to the prediction. We evaluate AiSciVision on three real-world scientific image classification datasets: detecting the presence of aquaculture ponds, diseased eelgrass, and solar panels. Across these datasets, our method outperforms fully supervised models in low and full-labeled data settings. AiSciVision is actively deployed in real-world use, specifically for aquaculture research, through a dedicated web application that displays and allows the expert users to converse with the transcripts. This work represents a crucial step toward AI systems that are both interpretable and effective, advancing their use in scientific research and scientific discovery.</li>
</ul>

<h3>Title: A Systematic Review of Machine Learning in Sports Betting: Techniques, Challenges, and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Ren√© Manass√© Galekwa, Jean Marie Tshimula, Etienne Gael Tajeuna, Kyamakya Kyandoghere</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, cs.ET, cs.IR, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21484">https://arxiv.org/abs/2410.21484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21484">https://arxiv.org/pdf/2410.21484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21484]] A Systematic Review of Machine Learning in Sports Betting: Techniques, Challenges, and Future Directions(https://arxiv.org/abs/2410.21484)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The sports betting industry has experienced rapid growth, driven largely by technological advancements and the proliferation of online platforms. Machine learning (ML) has played a pivotal role in the transformation of this sector by enabling more accurate predictions, dynamic odds-setting, and enhanced risk management for both bookmakers and bettors. This systematic review explores various ML techniques, including support vector machines, random forests, and neural networks, as applied in different sports such as soccer, basketball, tennis, and cricket. These models utilize historical data, in-game statistics, and real-time information to optimize betting strategies and identify value bets, ultimately improving profitability. For bookmakers, ML facilitates dynamic odds adjustment and effective risk management, while bettors leverage data-driven insights to exploit market inefficiencies. This review also underscores the role of ML in fraud detection, where anomaly detection models are used to identify suspicious betting patterns. Despite these advancements, challenges such as data quality, real-time decision-making, and the inherent unpredictability of sports outcomes remain. Ethical concerns related to transparency and fairness are also of significant importance. Future research should focus on developing adaptive models that integrate multimodal data and manage risk in a manner akin to financial portfolios. This review provides a comprehensive examination of the current applications of ML in sports betting, and highlights both the potential and the limitations of these technologies.</li>
</ul>

<h3>Title: Can Large Language Models Act as Symbolic Reasoners?</h3>
<ul>
<li><strong>Authors: </strong>Rob Sullivan, Nelly Elsayed</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21490">https://arxiv.org/abs/2410.21490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21490">https://arxiv.org/pdf/2410.21490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21490]] Can Large Language Models Act as Symbolic Reasoners?(https://arxiv.org/abs/2410.21490)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>The performance of Large language models (LLMs) across a broad range of domains has been impressive but have been critiqued as not being able to reason about their process and conclusions derived. This is to explain the conclusions draw, and also for determining a plan or strategy for their approach. This paper explores the current research in investigating symbolic reasoning and LLMs, and whether an LLM can inherently provide some form of reasoning or whether supporting components are necessary, and, if there is evidence for a reasoning capability, is this evident in a specific domain or is this a general capability? In addition, this paper aims to identify the current research gaps and future trends of LLM explainability, presenting a review of the literature, identifying current research into this topic and suggests areas for future work.</li>
</ul>

<h3>Title: Trustworthiness of Stochastic Gradient Descent in Distributed Learning</h3>
<ul>
<li><strong>Authors: </strong>Hongyang Li, Caesar Wu, Mohammed Chadli, Said Mammar, Pascal Bouvry</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21491">https://arxiv.org/abs/2410.21491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21491">https://arxiv.org/pdf/2410.21491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21491]] Trustworthiness of Stochastic Gradient Descent in Distributed Learning(https://arxiv.org/abs/2410.21491)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Distributed learning (DL) leverages multiple nodes to accelerate training, enabling the efficient optimization of large-scale models. Stochastic Gradient Descent (SGD), a key optimization algorithm, plays a central role in this process. However, communication bottlenecks often limit scalability and efficiency, leading to the increasing adoption of compressed SGD techniques to alleviate these challenges. Despite addressing communication overheads, compressed SGD introduces trustworthiness concerns, as gradient exchanges among nodes are vulnerable to attacks like gradient inversion (GradInv) and membership inference attacks (MIA). The trustworthiness of compressed SGD remains underexplored, leaving important questions about its reliability unanswered. In this paper, we provide a trustworthiness evaluation of compressed versus uncompressed SGD. Specifically, we conduct empirical studies using GradInv attacks, revealing that compressed SGD demonstrates significantly higher resistance to privacy leakage compared to uncompressed SGD. Moreover, our findings suggest that MIA may not be a reliable metric for assessing privacy risks in machine learning.</li>
</ul>

<h3>Title: FATH: Authentication-based Test-time Defense against Indirect Prompt Injection Attacks</h3>
<ul>
<li><strong>Authors: </strong>Jiongxiao Wang, Fangzhou Wu, Wendi Li, Jinsheng Pan, Edward Suh, Z. Morley Mao, Muhao Chen, Chaowei Xiao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21492">https://arxiv.org/abs/2410.21492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21492">https://arxiv.org/pdf/2410.21492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21492]] FATH: Authentication-based Test-time Defense against Indirect Prompt Injection Attacks(https://arxiv.org/abs/2410.21492)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been widely deployed as the backbone with additional tools and text information for real-world applications. However, integrating external information into LLM-integrated applications raises significant security concerns. Among these, prompt injection attacks are particularly threatening, where malicious instructions injected in the external text information can exploit LLMs to generate answers as the attackers desire. While both training-time and test-time defense methods have been developed to mitigate such attacks, the unaffordable training costs associated with training-time methods and the limited effectiveness of existing test-time methods make them impractical. This paper introduces a novel test-time defense strategy, named Formatting AuThentication with Hash-based tags (FATH). Unlike existing approaches that prevent LLMs from answering additional instructions in external text, our method implements an authentication system, requiring LLMs to answer all received instructions with a security policy and selectively filter out responses to user instructions as the final output. To achieve this, we utilize hash-based authentication tags to label each response, facilitating accurate identification of responses according to the user's instructions and improving the robustness against adaptive attacks. Comprehensive experiments demonstrate that our defense method can effectively defend against indirect prompt injection attacks, achieving state-of-the-art performance under Llama3 and GPT3.5 models across various attack methods. Our code is released at: this https URL</li>
</ul>

<h3>Title: Towards Multi-dimensional Explanation Alignment for Medical Classification</h3>
<ul>
<li><strong>Authors: </strong>Lijie Hu, Songning Lai, Wenshuo Chen, Hongru Xiao, Hongbin Lin, Lu Yu, Jingfeng Zhang, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21494">https://arxiv.org/abs/2410.21494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21494">https://arxiv.org/pdf/2410.21494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21494]] Towards Multi-dimensional Explanation Alignment for Medical Classification(https://arxiv.org/abs/2410.21494)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The lack of interpretability in the field of medical image analysis has significant ethical and legal implications. Existing interpretable methods in this domain encounter several challenges, including dependency on specific models, difficulties in understanding and visualization, as well as issues related to efficiency. To address these limitations, we propose a novel framework called Med-MICN (Medical Multi-dimensional Interpretable Concept Network). Med-MICN provides interpretability alignment for various angles, including neural symbolic reasoning, concept semantics, and saliency maps, which are superior to current interpretable methods. Its advantages include high prediction accuracy, interpretability across multiple dimensions, and automation through an end-to-end concept labeling process that reduces the need for extensive human training effort when working with new datasets. To demonstrate the effectiveness and interpretability of Med-MICN, we apply it to four benchmark datasets and compare it with baselines. The results clearly demonstrate the superior performance and interpretability of our Med-MICN.</li>
</ul>

<h3>Title: RoBIn: A Transformer-Based Model For Risk Of Bias Inference With Machine Reading Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Abel Corr√™a Dias, Viviane Pereira Moreira, Jo√£o Luiz Dihl Comba</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21495">https://arxiv.org/abs/2410.21495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21495">https://arxiv.org/pdf/2410.21495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21495]] RoBIn: A Transformer-Based Model For Risk Of Bias Inference With Machine Reading Comprehension(https://arxiv.org/abs/2410.21495)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Objective: Scientific publications play a crucial role in uncovering insights, testing novel drugs, and shaping healthcare policies. Accessing the quality of publications requires evaluating their Risk of Bias (RoB), a process typically conducted by human reviewers. In this study, we introduce a new dataset for machine reading comprehension and RoB assessment and present RoBIn (Risk of Bias Inference), an innovative model crafted to automate such evaluation. The model employs a dual-task approach, extracting evidence from a given context and assessing the RoB based on the gathered evidence. Methods: We use data from the Cochrane Database of Systematic Reviews (CDSR) as ground truth to label open-access clinical trial publications from PubMed. This process enabled us to develop training and test datasets specifically for machine reading comprehension and RoB inference. Additionally, we created extractive (RoBInExt) and generative (RoBInGen) Transformer-based approaches to extract relevant evidence and classify the RoB effectively. Results: RoBIn is evaluated across various settings and benchmarked against state-of-the-art methods for RoB inference, including large language models in multiple scenarios. In most cases, the best-performing RoBIn variant surpasses traditional machine learning and LLM-based approaches, achieving an ROC AUC of 0.83. Conclusion: Based on the evidence extracted from clinical trial reports, RoBIn performs a binary classification to decide whether the trial is at a low RoB or a high/unclear RoB. We found that both RoBInGen and RoBInExt are robust and have the best results in many settings.</li>
</ul>

<h3>Title: SandboxAQ's submission to MRL 2024 Shared Task on Multi-lingual Multi-task Information Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Isidora Chara Tourni, Sayontan Ghosh, Brenda Miao, Constantijn van der Poel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21501">https://arxiv.org/abs/2410.21501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21501">https://arxiv.org/pdf/2410.21501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21501]] SandboxAQ's submission to MRL 2024 Shared Task on Multi-lingual Multi-task Information Retrieval(https://arxiv.org/abs/2410.21501)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper explores the problems of Question Answering (QA) and Named Entity Recognition (NER) in five diverse languages. We tested five Large Language Models with various prompting methods, including zero-shot, chain-of-thought reasoning, and translation techniques. Our results show that while some models consistently outperform others, their effectiveness varies significantly across tasks and languages. We saw that advanced prompting techniques generally improved QA performance but had mixed results for NER; and we observed that language difficulty patterns differed between tasks. Our findings highlight the need for task-specific approaches in multilingual NLP and suggest that current models may develop different linguistic competencies for different tasks.</li>
</ul>

<h3>Title: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups</h3>
<ul>
<li><strong>Authors: </strong>Davide Ghilardi, Federico Belotti, Marco Molinari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21508">https://arxiv.org/abs/2410.21508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21508">https://arxiv.org/pdf/2410.21508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21508]] Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups(https://arxiv.org/abs/2410.21508)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sparse AutoEnocders (SAEs) have recently been employed as an unsupervised approach for understanding the inner workings of Large Language Models (LLMs). They reconstruct the model's activations with a sparse linear combination of interpretable features. However, training SAEs is computationally intensive, especially as models grow in size and complexity. To address this challenge, we propose a novel training strategy that reduces the number of trained SAEs from one per layer to one for a given group of contiguous layers. Our experimental results on Pythia 160M highlight a speedup of up to 6x without compromising the reconstruction quality and performance on downstream tasks. Therefore, layer clustering presents an efficient approach to train SAEs in modern LLMs.</li>
</ul>

<h3>Title: LLM-Forest for Health Tabular Data Imputation</h3>
<ul>
<li><strong>Authors: </strong>Xinrui He, Yikun Ban, Jiaru Zou, Tianxin Wei, Curtiss B. Cook, Jingrui He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21520">https://arxiv.org/abs/2410.21520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21520">https://arxiv.org/pdf/2410.21520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21520]] LLM-Forest for Health Tabular Data Imputation(https://arxiv.org/abs/2410.21520)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Missing data imputation is a critical challenge in tabular datasets, especially in healthcare, where data completeness is vital for accurate analysis. Large language models (LLMs), trained on vast corpora, have shown strong potential in data generation, making them a promising tool for tabular data imputation. However, challenges persist in designing effective prompts for a finetuning-free process and in mitigating the risk of LLM hallucinations. To address these issues, we propose a novel framework, LLM-Forest, which introduces a "forest" of few-shot learning LLM "trees" with confidence-based weighted voting. This framework is established on a new concept of bipartite information graphs to identify high-quality relevant neighboring entries with both feature and value granularity. Extensive experiments on four real-world healthcare datasets demonstrate the effectiveness and efficiency of LLM-Forest.</li>
</ul>

<h3>Title: A Multi-Agent Reinforcement Learning Testbed for Cognitive Radio Applications</h3>
<ul>
<li><strong>Authors: </strong>Sriniketh Vangaru, Daniel Rosen, Dylan Green, Raphael Rodriguez, Maxwell Wiecek, Amos Johnson, Alyse M. Jones, William C. Headley</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21521">https://arxiv.org/abs/2410.21521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21521">https://arxiv.org/pdf/2410.21521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21521]] A Multi-Agent Reinforcement Learning Testbed for Cognitive Radio Applications(https://arxiv.org/abs/2410.21521)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Technological trends show that Radio Frequency Reinforcement Learning (RFRL) will play a prominent role in the wireless communication systems of the future. Applications of RFRL range from military communications jamming to enhancing WiFi networks. Before deploying algorithms for these purposes, they must be trained in a simulation environment to ensure adequate performance. For this reason, we previously created the RFRL Gym: a standardized, accessible tool for the development and testing of reinforcement learning (RL) algorithms in the wireless communications space. This environment leveraged the OpenAI Gym framework and featured customizable simulation scenarios within the RF spectrum. However, the RFRL Gym was limited to training a single RL agent per simulation; this is not ideal, as most real-world RF scenarios will contain multiple intelligent agents in cooperative, competitive, or mixed settings, which is a natural consequence of spectrum congestion. Therefore, through integration with Ray RLlib, multi-agent reinforcement learning (MARL) functionality for training and assessment has been added to the RFRL Gym, making it even more of a robust tool for RF spectrum simulation. This paper provides an overview of the updated RFRL Gym environment. In this work, the general framework of the tool is described relative to comparable existing resources, highlighting the significant additions and refactoring we have applied to the Gym. Afterward, results from testing various RF scenarios in the MARL environment and future additions are discussed.</li>
</ul>

<h3>Title: Diffusion-nested Auto-Regressive Synthesis of Heterogeneous Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Hengrui Zhang, Liancheng Fang, Qitian Wu, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21523">https://arxiv.org/abs/2410.21523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21523">https://arxiv.org/pdf/2410.21523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21523]] Diffusion-nested Auto-Regressive Synthesis of Heterogeneous Tabular Data(https://arxiv.org/abs/2410.21523)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Autoregressive models are predominant in natural language generation, while their application in tabular data remains underexplored. We posit that this can be attributed to two factors: 1) tabular data contains heterogeneous data type, while the autoregressive model is primarily designed to model discrete-valued data; 2) tabular data is column permutation-invariant, requiring a generation model to generate columns in arbitrary order. This paper proposes a Diffusion-nested Autoregressive model (TabDAR) to address these issues. To enable autoregressive methods for continuous columns, TabDAR employs a diffusion model to parameterize the conditional distribution of continuous features. To ensure arbitrary generation order, TabDAR resorts to masked transformers with bi-directional attention, which simulate various permutations of column order, hence enabling it to learn the conditional distribution of a target column given an arbitrary combination of other columns. These designs enable TabDAR to not only freely handle heterogeneous tabular data but also support convenient and flexible unconditional/conditional sampling. We conduct extensive experiments on ten datasets with distinct properties, and the proposed TabDAR outperforms previous state-of-the-art methods by 18% to 45% on eight metrics across three distinct aspects.</li>
</ul>

<h3>Title: Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Hsun-Yu Kuo, Yin-Hsiang Liao, Yu-Chieh Chao, Wei-Yun Ma, Pu-Jen Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21526">https://arxiv.org/abs/2410.21526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21526">https://arxiv.org/pdf/2410.21526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21526]] Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification(https://arxiv.org/abs/2410.21526)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Synthetic data augmentation via large language models (LLMs) allows researchers to leverage additional training data, thus enhancing the performance of downstream tasks, especially when real-world data is scarce. However, the generated data can deviate from the real-world data, and this misalignment can bring deficient outcomes while applying the trained model to applications. Therefore, we proposed efficient weighted-loss approaches to align synthetic data with real-world distribution by emphasizing high-quality and diversified data generated by LLMs with using merely a little real-world data. We empirically assessed the effectiveness of our method on multiple text classification tasks, and the results showed leveraging our approaches on a BERT-level model robustly outperformed standard cross-entropy and other data weighting approaches, providing potential solutions to effectively leveraging synthetic data from any suitable data generator for model training.</li>
</ul>

<h3>Title: L3Ms -- Lagrange Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guneet S. Dhillon, Xingjian Shi, Yee Whye Teh, Alex Smola</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21533">https://arxiv.org/abs/2410.21533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21533">https://arxiv.org/pdf/2410.21533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21533]] L3Ms -- Lagrange Large Language Models(https://arxiv.org/abs/2410.21533)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) and alignment of large language models (LLMs) are key steps in providing a good user experience. However, the concept of an appropriate alignment is inherently application-dependent, and current methods often rely on heuristic choices to drive the optimization. In this work, we formulate SFT and alignment as a constrained optimization problem, where the LLM is trained on a task while being required to meet application-specific requirements, without resorting to heuristics. To solve this, we propose Lagrange Large Language Models (L3Ms), which employ logarithmic barriers to enforce the constraints. This approach allows for the customization of L3Ms across diverse applications while avoiding heuristic-driven processes. We demonstrate experimentally the versatility and efficacy of L3Ms in achieving tailored alignments for various applications.</li>
</ul>

<h3>Title: Bayesian Regression for Predicting Subscription to Bank Term Deposits in Direct Marketing Campaigns</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Farhan Tanvir, Md Maruf Hossain, Md Asifuzzaman Jishan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21539">https://arxiv.org/abs/2410.21539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21539">https://arxiv.org/pdf/2410.21539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21539]] Bayesian Regression for Predicting Subscription to Bank Term Deposits in Direct Marketing Campaigns(https://arxiv.org/abs/2410.21539)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In the highly competitive environment of the banking industry, it is essential to precisely forecast the behavior of customers in order to maximize the effectiveness of marketing initiatives and improve financial consequences. The purpose of this research is to examine the efficacy of logit and probit models in predicting term deposit subscriptions using a Portuguese bank's direct marketing data. There are several demographic, economic, and behavioral characteristics in the dataset that affect the probability of subscribing. To increase model performance and provide an unbiased evaluation, the target variable was balanced, considering the inherent imbalance in the dataset. The two model's prediction abilities were evaluated using Bayesian techniques and Leave-One-Out Cross-Validation (LOO-CV). The logit model performed better than the probit model in handling this classification problem. The results highlight the relevance of model selection when dealing with complicated decision-making processes in the financial services industry and imbalanced datasets. Findings from this study shed light on how banks can optimize their decision-making processes, improve their client segmentation, and boost their marketing campaigns by utilizing machine learning models.</li>
</ul>

<h3>Title: Unveiling Context-Aware Criteria in Self-Assessing LLMs</h3>
<ul>
<li><strong>Authors: </strong>Taneesh Gupta, Shivam Shandilya, Xuchao Zhang, Supriyo Ghosh, Chetan Bansal, Huaxiu Yao, Saravan Rajmohan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21545">https://arxiv.org/abs/2410.21545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21545">https://arxiv.org/pdf/2410.21545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21545]] Unveiling Context-Aware Criteria in Self-Assessing LLMs(https://arxiv.org/abs/2410.21545)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>The use of large language models (LLMs) as evaluators has garnered significant attention due to their potential to rival human-level evaluations in long-form response assessments. However, current LLM evaluators rely heavily on static, human-defined criteria, limiting their ability to generalize across diverse generative tasks and incorporate context-specific knowledge. In this paper, we propose a novel Self-Assessing LLM framework that integrates Context-Aware Criteria (SALC) with dynamic knowledge tailored to each evaluation instance. This instance-level knowledge enhances the LLM evaluator's performance by providing relevant and context-aware insights that pinpoint the important criteria specific to the current instance. Additionally, the proposed framework adapts seamlessly to various tasks without relying on predefined human criteria, offering a more flexible evaluation approach. Empirical evaluations demonstrate that our approach significantly outperforms existing baseline evaluation frameworks, yielding improvements on average 4.8% across a wide variety of datasets. Furthermore, by leveraging knowledge distillation techniques, we fine-tuned smaller language models for criteria generation and evaluation, achieving comparable or superior performance to larger models with much lower cost. Our method also exhibits a improvement in LC Win-Rate in AlpacaEval2 leaderboard up to a 12% when employed for preference data generation in Direct Preference Optimization (DPO), underscoring its efficacy as a robust and scalable evaluation framework.</li>
</ul>

<h3>Title: Personalized Federated Learning with Mixture of Models for Adaptive Prediction and Model Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Pouya M. Ghari, Yanning Shen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21547">https://arxiv.org/abs/2410.21547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21547">https://arxiv.org/pdf/2410.21547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21547]] Personalized Federated Learning with Mixture of Models for Adaptive Prediction and Model Fine-Tuning(https://arxiv.org/abs/2410.21547)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning is renowned for its efficacy in distributed model training, ensuring that users, called clients, retain data privacy by not disclosing their data to the central server that orchestrates collaborations. Most previous work on federated learning assumes that clients possess static batches of training data. However, clients may also need to make real-time predictions on streaming data in non-stationary environments. In such dynamic environments, employing pre-trained models may be inefficient, as they struggle to adapt to the constantly evolving data streams. To address this challenge, clients can fine-tune models online, leveraging their observed data to enhance performance. Despite the potential benefits of client participation in federated online model fine-tuning, existing analyses have not conclusively demonstrated its superiority over local model fine-tuning. To bridge this gap, the present paper develops a novel personalized federated learning algorithm, wherein each client constructs a personalized model by combining a locally fine-tuned model with multiple federated models learned by the server over time. Theoretical analysis and experiments on real datasets corroborate the effectiveness of this approach for real-time predictions and federated model fine-tuning.</li>
</ul>

<h3>Title: MultiTok: Variable-Length Tokenization for Efficient LLMs Adapted from LZW Compression</h3>
<ul>
<li><strong>Authors: </strong>Noel Elias, Homa Esfahanizadeh, Kaan Kale, Sriram Vishwanath, Muriel Medard</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IT, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21548">https://arxiv.org/abs/2410.21548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21548">https://arxiv.org/pdf/2410.21548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21548]] MultiTok: Variable-Length Tokenization for Efficient LLMs Adapted from LZW Compression(https://arxiv.org/abs/2410.21548)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have drastically changed the prospects of AI by introducing technologies for more complex natural language processing. However, current methodologies to train such LLMs require extensive resources including but not limited to large amounts of data, expensive machinery, and lengthy training. To solve this problem, this paper proposes a new tokenization method inspired by universal Lempel-Ziv-Welch data compression that compresses repetitive phrases into multi-word tokens. With MultiTok as a new tokenizing tool, we show that language models are able to be trained notably more efficiently while offering a similar accuracy on more succinct and compressed training data. In fact, our results demonstrate that MultiTok achieves a comparable performance to the BERT standard as a tokenizer while also providing close to 2.5x faster training with more than 30% less training data.</li>
</ul>

<h3>Title: Exploring the Design Space of Diffusion Bridge Models via Stochasticity Control</h3>
<ul>
<li><strong>Authors: </strong>Shaorong Zhang, Yuanbin Cheng, Xianghao Kong, Greg Ver Steeg</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21553">https://arxiv.org/abs/2410.21553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21553">https://arxiv.org/pdf/2410.21553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21553]] Exploring the Design Space of Diffusion Bridge Models via Stochasticity Control(https://arxiv.org/abs/2410.21553)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion bridge models effectively facilitate image-to-image (I2I) translation by connecting two distributions. However, existing methods overlook the impact of noise in sampling SDEs, transition kernel, and the base distribution on sampling efficiency, image quality and diversity. To address this gap, we propose the Stochasticity-controlled Diffusion Bridge (SDB), a novel theoretical framework that extends the design space of diffusion bridges, and provides strategies to mitigate singularities during both training and sampling. By controlling stochasticity in the sampling SDEs, our sampler achieves speeds up to 5 times faster than the baseline, while also producing lower FID scores. After training, SDB sets new benchmarks in image quality and sampling efficiency via managing stochasticity within the transition kernel. Furthermore, introducing stochasticity into the base distribution significantly improves image diversity, as quantified by a newly introduced metric.</li>
</ul>

<h3>Title: Empirical curvelet based Fully Convolutional Network for supervised texture image segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yuan Huang, Fugen Zhou, Jerome Gilles</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21562">https://arxiv.org/abs/2410.21562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21562">https://arxiv.org/pdf/2410.21562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21562]] Empirical curvelet based Fully Convolutional Network for supervised texture image segmentation(https://arxiv.org/abs/2410.21562)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a new approach to perform supervised texture classification/segmentation. The proposed idea is to feed a Fully Convolutional Network with specific texture descriptors. These texture features are extracted from images by using an empirical curvelet transform. We propose a method to build a unique empirical curvelet filter bank adapted to a given dictionary of textures. We then show that the output of these filters can be used to build efficient texture descriptors utilized to finally feed deep learning networks. Our approach is finally evaluated on several datasets and compare the results to various state-of-the-art algorithms and show that the proposed method dramatically outperform all existing ones.</li>
</ul>

<h3>Title: Thank You, Stingray: Multilingual Large Language Models Can Not (Yet) Disambiguate Cross-Lingual Word Sense</h3>
<ul>
<li><strong>Authors: </strong>Samuel Cahyawijaya, Ruochen Zhang, Holy Lovenia, Jan Christian Blaise Cruz, Hiroki Nomoto, Alham Fikri Aji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21573">https://arxiv.org/abs/2410.21573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21573">https://arxiv.org/pdf/2410.21573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21573]] Thank You, Stingray: Multilingual Large Language Models Can Not (Yet) Disambiguate Cross-Lingual Word Sense(https://arxiv.org/abs/2410.21573)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Multilingual large language models (LLMs) have gained prominence, but concerns arise regarding their reliability beyond English. This study addresses the gap in cross-lingual semantic evaluation by introducing a novel benchmark for cross-lingual sense disambiguation, StingrayBench. In this paper, we demonstrate using false friends -- words that are orthographically similar but have completely different meanings in two languages -- as a possible approach to pinpoint the limitation of cross-lingual sense disambiguation in LLMs. We collect false friends in four language pairs, namely Indonesian-Malay, Indonesian-Tagalog, Chinese-Japanese, and English-German; and challenge LLMs to distinguish the use of them in context. In our analysis of various models, we observe they tend to be biased toward higher-resource languages. We also propose new metrics for quantifying the cross-lingual sense bias and comprehension based on our benchmark. Our work contributes to developing more diverse and inclusive language modeling, promoting fairer access for the wider multilingual community.</li>
</ul>

<h3>Title: ImageNet-RIB Benchmark: Large Pre-Training Datasets Don't Guarantee Robustness after Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Jaedong Hwang, Brian Cheung, Zhang-Wei Hong, Akhilan Boopathy, Pulkit Agrawal, Ila Fiete</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21582">https://arxiv.org/abs/2410.21582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21582">https://arxiv.org/pdf/2410.21582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21582]] ImageNet-RIB Benchmark: Large Pre-Training Datasets Don't Guarantee Robustness after Fine-Tuning(https://arxiv.org/abs/2410.21582)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Highly performant large-scale pre-trained models promise to also provide a valuable foundation for learning specialized tasks, by fine-tuning the model to the desired task. By starting from a good general-purpose model, the goal is to achieve both specialization in the target task and maintain robustness. To assess the robustness of models to out-of-distribution samples after fine-tuning on downstream datasets, we introduce a new robust fine-tuning benchmark, ImageNet-RIB (Robustness Inheritance Benchmark). The benchmark consists of a set of related but distinct specialized (downstream) tasks; pre-trained models are fine-tuned on one task in the set and their robustness is assessed on the rest, iterating across all tasks for fine-tuning and assessment. We find that the continual learning methods, EWC and LwF maintain robustness after fine-tuning though fine-tuning generally does reduce performance on generalization to related downstream tasks across models. Not surprisingly, models pre-trained on large and rich datasets exhibit higher initial robustness across datasets and suffer more pronounced degradation during fine-tuning. The distance between the pre-training and downstream datasets, measured by optimal transport, predicts this performance degradation on the pre-training dataset. However, counterintuitively, model robustness after fine-tuning on related downstream tasks is the worst when the pre-training dataset is the richest and the most diverse. This suggests that starting with the strongest foundation model is not necessarily the best approach for performance on specialist tasks. The benchmark thus offers key insights for developing more resilient fine-tuning strategies and building robust machine learning models. this https URL</li>
</ul>

<h3>Title: Hybrid-DAOs: Enhancing Governance, Scalability, and Compliance in Decentralized Systems</h3>
<ul>
<li><strong>Authors: </strong>Neil Shah</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21593">https://arxiv.org/abs/2410.21593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21593">https://arxiv.org/pdf/2410.21593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21593]] Hybrid-DAOs: Enhancing Governance, Scalability, and Compliance in Decentralized Systems(https://arxiv.org/abs/2410.21593)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, fair</a></li>
<li><strong>Abstract: </strong>Decentralized Autonomous Organizations (DAOs), based on block-chain systems such as Ethereum, are emerging governance protocols that enable decentralized community management without a central authority. For instance, UniswapDAO allows members to vote on policy changes for the Uniswap exchange. However, DAOs face challenges regarding scalability, governance, and compliance. Hybrid-DAOs, which combine the decentralized nature of DAOs with traditional legal frameworks, provide solutions to these issues. This research explores various aspects of DAOs, including their voting mechanisms, which, while ensuring fairness, are susceptible to Sybil attacks, where a user can create multiple accounts to exploit the system. Hybrid-DAOs offer robust solutions to these attacks, enabling more equitable voting methods. Moreover, decentralization can be understood through four properties: anonymity, transparency, accountability, and fairness, each with distinct implications for DAOs. Lastly, this work discusses legal challenges Hybrid-DAOs face and their promising applications across sectors such as nonprofit management, corporate governance, and startup funding. Overall, we argue that Hybrid-DAOs are the future of DAOs: the additional legal structure enhances the feasibility of many applications, and they offer innovative solutions to technical problems that plague DAOs.</li>
</ul>

<h3>Title: Deep Trees for (Un)structured Data: Tractability, Performance, and Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Dimitris Bertsimas, Lisa Everest, Jiayi Gu, Matthew Peroni, Vasiliki Stoumpou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21595">https://arxiv.org/abs/2410.21595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21595">https://arxiv.org/pdf/2410.21595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21595]] Deep Trees for (Un)structured Data: Tractability, Performance, and Interpretability(https://arxiv.org/abs/2410.21595)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Decision Trees have remained a popular machine learning method for tabular datasets, mainly due to their interpretability. However, they lack the expressiveness needed to handle highly nonlinear or unstructured datasets. Motivated by recent advances in tree-based machine learning (ML) techniques and first-order optimization methods, we introduce Generalized Soft Trees (GSTs), which extend soft decision trees (STs) and are capable of processing images directly. We demonstrate their advantages with respect to tractability, performance, and interpretability. We develop a tractable approach to growing GSTs, given by the DeepTree algorithm, which, in addition to new regularization terms, produces high-quality models with far fewer nodes and greater interpretability than traditional soft trees. We test the performance of our GSTs on benchmark tabular and image datasets, including MIMIC-IV, MNIST, Fashion MNIST, CIFAR-10 and Celeb-A. We show that our approach outperforms other popular tree methods (CART, Random Forests, XGBoost) in almost all of the datasets, with Convolutional Trees having a significant edge in the hardest CIFAR-10 and Fashion MNIST datasets. Finally, we explore the interpretability of our GSTs and find that even the most complex GSTs are considerably more interpretable than deep neural networks. Overall, our approach of Generalized Soft Trees provides a tractable method that is high-performing on (un)structured datasets and preserves interpretability more than traditional deep learning methods.</li>
</ul>

<h3>Title: Reducing the Scope of Language Models with Circuit Breakers</h3>
<ul>
<li><strong>Authors: </strong>David Yunis, Siyu Huo, Chulaka Gunasekara, Danish Contractor</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21597">https://arxiv.org/abs/2410.21597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21597">https://arxiv.org/pdf/2410.21597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21597]] Reducing the Scope of Language Models with Circuit Breakers(https://arxiv.org/abs/2410.21597)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Language models are now deployed in a wide variety of user-facing applications, often for specific purposes like answering questions about documentation or acting as coding assistants. As these models are intended for particular purposes, they should not be able to answer irrelevant queries like requests for poetry or questions about physics, or even worse, queries that can only be answered by humans like sensitive company policies. Instead we would like them to only answer queries corresponding to desired behavior and refuse all other requests, which we refer to as scoping. We find that, despite the use of system prompts, two representative language models can be poorly scoped and respond to queries they should not be addressing. We then conduct a comprehensive empirical evaluation of methods which could be used for scoping the behavior of language models. Among many other results, we show that a recently-proposed method for general alignment, Circuit Breakers (CB), can be adapted to scope language models to very specific tasks like sentiment analysis or summarization or even tasks with finer-grained scoping (e.g. summarizing only news articles). When compared to standard methods like fine-tuning or preference learning, CB is more robust both for out of distribution tasks, and to adversarial prompting techniques. We also show that layering SFT and CB together often results in the best of both worlds: improved performance only on relevant queries, while rejecting irrelevant ones.</li>
</ul>

<h3>Title: Accelerating Privacy-Preserving Medical Record Linkage: A Three-Party MPC Approach</h3>
<ul>
<li><strong>Authors: </strong>≈ûeyma Selcan Maƒüara, Noah Dietrich, Ali Burak √únal, Mete Akg√ºn</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21605">https://arxiv.org/abs/2410.21605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21605">https://arxiv.org/pdf/2410.21605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21605]] Accelerating Privacy-Preserving Medical Record Linkage: A Three-Party MPC Approach(https://arxiv.org/abs/2410.21605)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect</a></li>
<li><strong>Abstract: </strong>Motivation: Record linkage is a crucial concept for integrating data from multiple sources, particularly when datasets lack exact identifiers, and it has diverse applications in real-world data analysis. Privacy-Preserving Record Linkage (PPRL) ensures this integration occurs securely, protecting sensitive information from unauthorized access. This is especially important in sectors such as healthcare, where datasets include private identity information (IDAT) governed by strict privacy laws. However, maintaining both privacy and efficiency in large-scale record linkage poses significant challenges. Consequently, researchers must develop advanced methods to protect data privacy while optimizing processing performance. This paper presents a novel and efficient PPRL method based on a secure 3-party computation (MPC) framework. Our approach allows multiple parties to compute linkage results without exposing their private inputs and significantly improves the speed of linkage process compared to existing privacy-preserving solutions. Results: We demonstrated that our method preserves the linkage quality of the state-of-the-art PPRL method while achieving up to 14 times faster performance. For example, linking a record against a database of 10,000 records takes just 8.74 seconds in a realistic network with 700 Mbps bandwidth and 60 ms latency. Even on a slower internet connection with 100 Mbps bandwidth and 60 ms latency, the linkage completes in 28 seconds, highlighting the scalability and efficiency of our solution.</li>
</ul>

<h3>Title: CaloChallenge 2022: A Community Challenge for Fast Calorimeter Simulation</h3>
<ul>
<li><strong>Authors: </strong>Claudius Krause, Michele Faucci Giannelli, Gregor Kasieczka, Benjamin Nachman, Dalila Salamani, David Shih, Anna Zaborowska, Oz Amram, Kerstin Borras, Matthew R. Buckley, Erik Buhmann, Thorsten Buss, Renato Paulo Da Costa Cardoso, Anthony L. Caterini, Nadezda Chernyavskaya, Federico A.G. Corchia, Jesse C. Cresswell, Sascha Diefenbacher, Etienne Dreyer, Vijay Ekambaram, Engin Eren, Florian Ernst, Luigi Favaro, Matteo Franchini, Frank Gaede, Eilam Gross, Shih-Chieh Hsu, Kristina Jaruskova, Benno K√§ch, Jayant Kalagnanam, Raghav Kansal, Taewoo Kim, Dmitrii Kobylianskii, Anatolii Korol, William Korcari, Dirk Kr√ºcker, Katja Kr√ºger, Marco Letizia, Shu Li, Qibin Liu, Xiulong Liu, Gabriel Loaiza-Ganem, Thandikire Madula, Peter McKeown, Isabell-A. Melzer-Pellmann, Vinicius Mikuni, Nam Nguyen, Ayodele Ore, Sofia Palacios Schweitzer, Ian Pang, Kevin Pedro, Tilman Plehn, Witold Pokorski, Huilin Qu, Piyush Raikwar, John A. Raine, Humberto Reyes-Gonzalez, Lorenzo Rinaldi, Brendan Leigh Ross, Moritz A.W. Scham, Simon Schnake, Chase Shimmin, Eli Shlizerman, Nathalie Soybelman, Mudhakar Srivatsa, Kalliopi Tsolaki, Sofia Vallecorsa, Kyongmin Yeo, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, hep-ex, hep-ph, physics.ins-det</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21611">https://arxiv.org/abs/2410.21611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21611">https://arxiv.org/pdf/2410.21611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21611]] CaloChallenge 2022: A Community Challenge for Fast Calorimeter Simulation(https://arxiv.org/abs/2410.21611)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present the results of the "Fast Calorimeter Simulation Challenge 2022" - the CaloChallenge. We study state-of-the-art generative models on four calorimeter shower datasets of increasing dimensionality, ranging from a few hundred voxels to a few tens of thousand voxels. The 31 individual submissions span a wide range of current popular generative architectures, including Variational AutoEncoders (VAEs), Generative Adversarial Networks (GANs), Normalizing Flows, Diffusion models, and models based on Conditional Flow Matching. We compare all submissions in terms of quality of generated calorimeter showers, as well as shower generation time and model size. To assess the quality we use a broad range of different metrics including differences in 1-dimensional histograms of observables, KPD/FPD scores, AUCs of binary classifiers, and the log-posterior of a multiclass classifier. The results of the CaloChallenge provide the most complete and comprehensive survey of cutting-edge approaches to calorimeter fast simulation to date. In addition, our work provides a uniquely detailed perspective on the important problem of how to evaluate generative models. As such, the results presented here should be applicable for other domains that use generative AI and require fast and faithful generation of samples in a large phase space.</li>
</ul>

<h3>Title: NYC-Event-VPR: A Large-Scale High-Resolution Event-Based Visual Place Recognition Dataset in Dense Urban Environments</h3>
<ul>
<li><strong>Authors: </strong>Taiyi Pan, Junyang He, Chao Chen, Yiming Li, Chen Feng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21615">https://arxiv.org/abs/2410.21615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21615">https://arxiv.org/pdf/2410.21615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21615]] NYC-Event-VPR: A Large-Scale High-Resolution Event-Based Visual Place Recognition Dataset in Dense Urban Environments(https://arxiv.org/abs/2410.21615)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual place recognition (VPR) enables autonomous robots to identify previously visited locations, which contributes to tasks like simultaneous localization and mapping (SLAM). VPR faces challenges such as accurate image neighbor retrieval and appearance change in scenery. Event cameras, also known as dynamic vision sensors, are a new sensor modality for VPR and offer a promising solution to the challenges with their unique attributes: high temporal resolution (1MHz clock), ultra-low latency (in {\mu}s), and high dynamic range (>120dB). These attributes make event cameras less susceptible to motion blur and more robust in variable lighting conditions, making them suitable for addressing VPR challenges. However, the scarcity of event-based VPR datasets, partly due to the novelty and cost of event cameras, hampers their adoption. To fill this data gap, our paper introduces the NYC-Event-VPR dataset to the robotics and computer vision communities, featuring the Prophesee IMX636 HD event sensor (1280x720 resolution), combined with RGB camera and GPS module. It encompasses over 13 hours of geotagged event data, spanning 260 kilometers across New York City, covering diverse lighting and weather conditions, day/night scenarios, and multiple visits to various locations. Furthermore, our paper employs three frameworks to conduct generalization performance assessments, promoting innovation in event-based VPR and its integration into robotics applications.</li>
</ul>

<h3>Title: Graph Sparsification for Enhanced Conformal Prediction in Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Yuntian He, Pranav Maneriker, Anutam Srinivasan, Aditya T. Vadlamani, Srinivasan Parthasarathy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21618">https://arxiv.org/abs/2410.21618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21618">https://arxiv.org/pdf/2410.21618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21618]] Graph Sparsification for Enhanced Conformal Prediction in Graph Neural Networks(https://arxiv.org/abs/2410.21618)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Conformal Prediction is a robust framework that ensures reliable coverage across machine learning tasks. Although recent studies have applied conformal prediction to graph neural networks, they have largely emphasized post-hoc prediction set generation. Improving conformal prediction during the training stage remains unaddressed. In this work, we tackle this challenge from a denoising perspective by introducing SparGCP, which incorporates graph sparsification and a conformal prediction-specific objective into GNN training. SparGCP employs a parameterized graph sparsification module to filter out task-irrelevant edges, thereby improving conformal prediction efficiency. Extensive experiments on real-world graph datasets demonstrate that SparGCP outperforms existing methods, reducing prediction set sizes by an average of 32\% and scaling seamlessly to large networks on commodity GPUs.</li>
</ul>

<h3>Title: MCPDial: A Minecraft Persona-driven Dialogue Dataset</h3>
<ul>
<li><strong>Authors: </strong>Seyed Hossein Alavi, Sudha Rao, Ashutosh Adhikari, Gabriel A DesGarennes, Akanksha Malhotra, Chris Brockett, Mahmoud Adada, Raymond T. Ng, Vered Shwartz, Bill Dolan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21627">https://arxiv.org/abs/2410.21627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21627">https://arxiv.org/pdf/2410.21627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21627]] MCPDial: A Minecraft Persona-driven Dialogue Dataset(https://arxiv.org/abs/2410.21627)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose a novel approach that uses large language models (LLMs) to generate persona-driven conversations between Players and Non-Player Characters (NPC) in games. Showcasing the application of our methodology, we introduce the Minecraft Persona-driven Dialogue dataset (MCPDial). Starting with a small seed of expert-written conversations, we employ our method to generate hundreds of additional conversations. Each conversation in the dataset includes rich character descriptions of the player and NPC. The conversations are long, allowing for in-depth and extensive interactions between the player and NPC. MCPDial extends beyond basic conversations by incorporating canonical function calls (e.g. "Call find a resource on iron ore") between the utterances. Finally, we conduct a qualitative analysis of the dataset to assess its quality and characteristics.</li>
</ul>

<h3>Title: OFER: Occluded Face Expression Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Pratheba Selvaraju, Victoria Fernandez Abrevaya, Timo Bolkart, Rick Akkerman, Tianyu Ding, Faezeh Amjadi, Ilya Zharkov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21629">https://arxiv.org/abs/2410.21629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21629">https://arxiv.org/pdf/2410.21629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21629]] OFER: Occluded Face Expression Reconstruction(https://arxiv.org/abs/2410.21629)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reconstructing 3D face models from a single image is an inherently ill-posed problem, which becomes even more challenging in the presence of occlusions. In addition to fewer available observations, occlusions introduce an extra source of ambiguity, where multiple reconstructions can be equally valid. Despite the ubiquity of the problem, very few methods address its multi-hypothesis nature. In this paper we introduce OFER, a novel approach for single image 3D face reconstruction that can generate plausible, diverse, and expressive 3D faces, even under strong occlusions. Specifically, we train two diffusion models to generate the shape and expression coefficients of a face parametric model, conditioned on the input image. This approach captures the multi-modal nature of the problem, generating a distribution of solutions as output. Although this addresses the ambiguity problem, the challenge remains to pick the best matching shape to ensure consistency across diverse expressions. To achieve this, we propose a novel ranking mechanism that sorts the outputs of the shape diffusion network based on the predicted shape accuracy scores to select the best match. We evaluate our method using standard benchmarks and introduce CO-545, a new protocol and dataset designed to assess the accuracy of expressive faces under occlusion. Our results show improved performance over occlusion-based methods, with added ability to generate multiple expressions for a given image.</li>
</ul>

<h3>Title: Faster Local Solvers for Graph Diffusion Equations</h3>
<ul>
<li><strong>Authors: </strong>Jiahe Bai, Baojian Zhou, Deqing Yang, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21634">https://arxiv.org/abs/2410.21634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21634">https://arxiv.org/pdf/2410.21634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21634]] Faster Local Solvers for Graph Diffusion Equations(https://arxiv.org/abs/2410.21634)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Efficient computation of graph diffusion equations (GDEs), such as Personalized PageRank, Katz centrality, and the Heat kernel, is crucial for clustering, training neural networks, and many other graph-related problems. Standard iterative methods require accessing the whole graph per iteration, making them time-consuming for large-scale graphs. While existing local solvers approximate diffusion vectors through heuristic local updates, they often operate sequentially and are typically designed for specific diffusion types, limiting their applicability. Given that diffusion vectors are highly localizable, as measured by the participation ratio, this paper introduces a novel framework for approximately solving GDEs using a local diffusion process. This framework reveals the suboptimality of existing local solvers. Furthermore, our approach effectively localizes standard iterative solvers by designing simple and provably sublinear time algorithms. These new local solvers are highly parallelizable, making them well-suited for implementation on GPUs. We demonstrate the effectiveness of our framework in quickly obtaining approximate diffusion vectors, achieving up to a hundred-fold speed improvement, and its applicability to large-scale dynamic graphs. Our framework could also facilitate more efficient local message-passing mechanisms for GNNs.</li>
</ul>

<h3>Title: Are Paraphrases Generated by Large Language Models Invertible?</h3>
<ul>
<li><strong>Authors: </strong>Rafael Rivera Soto, Barry Chen, Nicholas Andrews</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21637">https://arxiv.org/abs/2410.21637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21637">https://arxiv.org/pdf/2410.21637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21637]] Are Paraphrases Generated by Large Language Models Invertible?(https://arxiv.org/abs/2410.21637)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models can produce highly fluent paraphrases while retaining much of the original meaning. While this capability has a variety of helpful applications, it may also be abused by bad actors, for example to plagiarize content or to conceal their identity. This motivates us to consider the problem of paraphrase inversion: given a paraphrased document, attempt to recover the original text. To explore the feasibility of this task, we fine-tune paraphrase inversion models, both with and without additional author-specific context to help guide the inversion process. We explore two approaches to author-specific inversion: one using in-context examples of the target author's writing, and another using learned style representations that capture distinctive features of the author's style. We show that, when starting from paraphrased machine-generated text, we can recover significant portions of the document using a learned inversion model. When starting from human-written text, the variety of source writing styles poses a greater challenge for invertability. However, even when the original tokens can't be recovered, we find the inverted text is stylistically similar to the original, which significantly improves the performance of plagiarism detectors and authorship identification systems that rely on stylistic markers.</li>
</ul>

<h3>Title: Adapting Diffusion Models for Improved Prompt Compliance and Controllable Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Deepak Sridhar, Abhishek Peri, Rohith Rachala, Nuno Vasconcelos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21638">https://arxiv.org/abs/2410.21638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21638">https://arxiv.org/pdf/2410.21638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21638]] Adapting Diffusion Models for Improved Prompt Compliance and Controllable Image Synthesis(https://arxiv.org/abs/2410.21638)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Recent advances in generative modeling with diffusion processes (DPs) enabled breakthroughs in image synthesis. Despite impressive image quality, these models have various prompt compliance problems, including low recall in generating multiple objects, difficulty in generating text in images, and meeting constraints like object locations and pose. For fine-grained editing and manipulation, they also require fine-grained semantic or instance maps that are tedious to produce manually. While prompt compliance can be enhanced by addition of loss functions at inference, this is time consuming and does not scale to complex scenes. To overcome these limitations, this work introduces a new family of \textit{Factor Graph Diffusion Models} (FG-DMs) that models the joint distribution of images and conditioning variables, such as semantic, sketch, depth or normal maps via a factor graph decomposition. This joint structure has several advantages, including support for efficient sampling based prompt compliance schemes, which produce images of high object recall, semi-automated fine-grained editing, text-based editing of conditions with noise inversion, explainability at intermediate levels, ability to produce labeled datasets for the training of downstream models such as segmentation or depth, training with missing data, and continual learning where new conditioning variables can be added with minimal or no modifications to the existing structure. We propose an implementation of FG-DMs by adapting a pre-trained Stable Diffusion (SD) model to implement all FG-DM factors, using only COCO dataset, and show that it is effective in generating images with 15\% higher recall than SD while retaining its generalization ability. We introduce an attention distillation loss that encourages consistency among the attention maps of all factors, improving the fidelity of the generated conditions and image.</li>
</ul>

<h3>Title: On filter design in deep convolutional neural network</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Hirani, Waleed Abdulla</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21644">https://arxiv.org/abs/2410.21644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21644">https://arxiv.org/pdf/2410.21644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21644]] On filter design in deep convolutional neural network(https://arxiv.org/abs/2410.21644)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>The deep convolutional neural network (DCNN) in computer vision has given promising results. It is widely applied in many areas, from medicine, agriculture, self-driving car, biometric system, and almost all computer vision-based applications. Filters or weights are the critical elements responsible for learning in DCNN. Backpropagation has been the primary learning algorithm for DCNN and provides promising results, but the size and numbers of the filters remain hyper-parameters. Various studies have been done in the last decade on semi-supervised, self-supervised, and unsupervised methods and their properties. The effects of filter initialization, size-shape selection, and the number of filters on learning and optimization have not been investigated in a separate publication to collate all the options. Such attributes are often treated as hyper-parameters and lack mathematical understanding. Computer vision algorithms have many limitations in real-life applications, and understanding the learning process is essential to have some significant improvement. To the best of our knowledge, no separate investigation has been published discussing the filters; this is our primary motivation. This study focuses on arguments for choosing specific physical parameters of filters, initialization, and learning technic over scattered methods. The promising unsupervised approaches have been evaluated. Additionally, the limitations, current challenges, and future scope have been discussed in this paper.</li>
</ul>

<h3>Title: Discriminative Pedestrian Features and Gated Channel Attention for Clothes-Changing Person Re-Identification</h3>
<ul>
<li><strong>Authors: </strong>Yongkang Ding, Rui Mao, Hanyue Zhu, Anqi Wang, Liyan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21663">https://arxiv.org/abs/2410.21663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21663">https://arxiv.org/pdf/2410.21663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21663]] Discriminative Pedestrian Features and Gated Channel Attention for Clothes-Changing Person Re-Identification(https://arxiv.org/abs/2410.21663)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In public safety and social life, the task of Clothes-Changing Person Re-Identification (CC-ReID) has become increasingly significant. However, this task faces considerable challenges due to appearance changes caused by clothing alterations. Addressing this issue, this paper proposes an innovative method for disentangled feature extraction, effectively extracting discriminative features from pedestrian images that are invariant to clothing. This method leverages pedestrian parsing techniques to identify and retain features closely associated with individual identity while disregarding the variable nature of clothing attributes. Furthermore, this study introduces a gated channel attention mechanism, which, by adjusting the network's focus, aids the model in more effectively learning and emphasizing features critical for pedestrian identity recognition. Extensive experiments conducted on two standard CC-ReID datasets validate the effectiveness of the proposed approach, with performance surpassing current leading solutions. The Top-1 accuracy under clothing change scenarios on the PRCC and VC-Clothes datasets reached 64.8% and 83.7%, respectively.</li>
</ul>

<h3>Title: Exploring Local Memorization in Diffusion Models via Bright Ending Attention</h3>
<ul>
<li><strong>Authors: </strong>Chen Chen, Daochang Liu, Mubarak Shah, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21665">https://arxiv.org/abs/2410.21665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21665">https://arxiv.org/pdf/2410.21665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21665]] Exploring Local Memorization in Diffusion Models via Bright Ending Attention(https://arxiv.org/abs/2410.21665)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we identify and leverage a novel `bright ending' (BE) anomaly in diffusion models prone to memorizing training images to address a new task: locating localized memorization regions within these models. BE refers to a distinct cross-attention pattern observed in text-to-image generations using diffusion models. Specifically, memorized image patches exhibit significantly greater attention to the end token during the final inference step compared to non-memorized patches. This attention map effectively highlights regions where the generated image replicates training data. Furthermore, driven by our observation that local memorization significantly underperforms in existing tasks of measuring, detecting, and mitigating memorization in diffusion models compared to global memorization, we propose a simple yet effective method to integrate BE and the results of the new localization task into these existing frameworks. This integration effectively improves their performances by narrowing the performance gap caused by local memorization. Our results not only demonstrate the successful execution of the new localization task but also establish new state-of-the-art performance across all existing tasks, underscoring the significance of the BE phenomenon.</li>
</ul>

<h3>Title: Investigating Memorization in Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Chen Chen, Enhuai Liu, Daochang Liu, Mubarak Shah, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21669">https://arxiv.org/abs/2410.21669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21669">https://arxiv.org/pdf/2410.21669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21669]] Investigating Memorization in Video Diffusion Models(https://arxiv.org/abs/2410.21669)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models, widely used for image and video generation, face a significant limitation: the risk of memorizing and reproducing training data during inference, potentially generating unauthorized copyrighted content. While prior research has focused on image diffusion models (IDMs), video diffusion models (VDMs) remain underexplored. To address this gap, we first formally define the two types of memorization in VDMs (content memorization and motion memorization) in a practical way that focuses on privacy preservation and applies to all generation types. We then introduce new metrics specifically designed to separately assess content and motion memorization in VDMs. Additionally, we curate a dataset of text prompts that are most prone to triggering memorization when used as conditioning in VDMs. By leveraging these prompts, we generate diverse videos from various open-source VDMs, successfully extracting numerous training videos from each tested model. Through the application of our proposed metrics, we systematically analyze memorization across various pretrained VDMs, including text-conditional and unconditional models, on a variety of datasets. Our comprehensive study reveals that memorization is widespread across all tested VDMs, indicating that VDMs can also memorize image training data in addition to video datasets. Finally, we propose efficient and effective detection strategies for both content and motion memorization, offering a foundational approach for improving privacy in VDMs.</li>
</ul>

<h3>Title: Sequential choice in ordered bundles</h3>
<ul>
<li><strong>Authors: </strong>Rajeev Kohli, Kriste Krstovski, Hengyu Kuang, Hengxu Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21670">https://arxiv.org/abs/2410.21670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21670">https://arxiv.org/pdf/2410.21670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21670]] Sequential choice in ordered bundles(https://arxiv.org/abs/2410.21670)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Experience goods such as sporting and artistic events, songs, videos, news stories, podcasts, and television series, are often packaged and consumed in bundles. Many such bundles are ordered in the sense that the individual items are consumed sequentially, one at a time. We examine if an individual's decision to consume the next item in an ordered bundle can be predicted based on his/her consumption pattern for the preceding items. We evaluate several predictive models, including two custom Transformers using decoder-only and encoder-decoder architectures, fine-tuned GPT-3, a custom LSTM model, a reinforcement learning model, two Markov models, and a zero-order model. Using data from Spotify, we find that the custom Transformer with a decoder-only architecture provides the most accurate predictions, both for individual choices and aggregate demand. This model captures a general form of state dependence. Analysis of Transformer attention weights suggests that the consumption of the next item in a bundle is based on approximately equal weighting of all preceding choices. Our results indicate that the Transformer can assist in queuing the next item that an individual is likely to consume from an ordered bundle, predicting the demand for individual items, and personalizing promotions to increase demand.</li>
</ul>

<h3>Title: BF-Meta: Secure Blockchain-enhanced Privacy-preserving Federated Learning for Metaverse</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Liu, Handi Chen, Edith C.H. Ngai</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21675">https://arxiv.org/abs/2410.21675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21675">https://arxiv.org/pdf/2410.21675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21675]] BF-Meta: Secure Blockchain-enhanced Privacy-preserving Federated Learning for Metaverse(https://arxiv.org/abs/2410.21675)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>The metaverse, emerging as a revolutionary platform for social and economic activities, provides various virtual services while posing security and privacy challenges. Wearable devices serve as bridges between the real world and the metaverse. To provide intelligent services without revealing users' privacy in the metaverse, leveraging federated learning (FL) to train models on local wearable devices is a promising solution. However, centralized model aggregation in traditional FL may suffer from external attacks, resulting in a single point of failure. Furthermore, the absence of incentive mechanisms may weaken users' participation during FL training, leading to degraded performance of the trained model and reduced quality of intelligent services. In this paper, we propose BF-Meta, a secure blockchain-empowered FL framework with decentralized model aggregation, to mitigate the negative influence of malicious users and provide secure virtual services in the metaverse. In addition, we design an incentive mechanism to give feedback to users based on their behaviors. Experiments conducted on five datasets demonstrate the effectiveness and applicability of BF-Meta.</li>
</ul>

<h3>Title: Impact of Code Transformation on Detection of Smart Contract Vulnerabilities</h3>
<ul>
<li><strong>Authors: </strong>Cuong Tran Manh, Hieu Dinh Vo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21685">https://arxiv.org/abs/2410.21685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21685">https://arxiv.org/pdf/2410.21685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21685]] Impact of Code Transformation on Detection of Smart Contract Vulnerabilities(https://arxiv.org/abs/2410.21685)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>While smart contracts are foundational elements of blockchain applications, their inherent susceptibility to security vulnerabilities poses a significant challenge. Existing training datasets employed for vulnerability detection tools may be limited, potentially compromising their efficacy. This paper presents a method for improving the quantity and quality of smart contract vulnerability datasets and evaluates current detection methods. The approach centers around semantic-preserving code transformation, a technique that modifies the source code structure without altering its semantic meaning. The transformed code snippets are inserted into all potential locations within benign smart contract code, creating new vulnerable contract versions. This method aims to generate a wider variety of vulnerable codes, including those that can bypass detection by current analysis tools. The paper experiments evaluate the method's effectiveness using tools like Slither, Mythril, and CrossFuzz, focusing on metrics like the number of generated vulnerable samples and the false negative rate in detecting these vulnerabilities. The improved results show that many newly created vulnerabilities can bypass tools and the false reporting rate goes up to 100% and increases dataset size minimum by 2.5X.</li>
</ul>

<h3>Title: CFSafety: Comprehensive Fine-grained Safety Assessment for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Liu, Chenhui Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21695">https://arxiv.org/abs/2410.21695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21695">https://arxiv.org/pdf/2410.21695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21695]] CFSafety: Comprehensive Fine-grained Safety Assessment for LLMs(https://arxiv.org/abs/2410.21695)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) rapidly evolve, they bring significant conveniences to our work and daily lives, but also introduce considerable safety risks. These models can generate texts with social biases or unethical content, and under specific adversarial instructions, may even incite illegal activities. Therefore, rigorous safety assessments of LLMs are crucial. In this work, we introduce a safety assessment benchmark, CFSafety, which integrates 5 classic safety scenarios and 5 types of instruction attacks, totaling 10 categories of safety questions, to form a test set with 25k prompts. This test set was used to evaluate the natural language generation (NLG) capabilities of LLMs, employing a combination of simple moral judgment and a 1-5 safety rating scale for scoring. Using this benchmark, we tested eight popular LLMs, including the GPT series. The results indicate that while GPT-4 demonstrated superior safety performance, the safety effectiveness of LLMs, including this model, still requires improvement. The data and code associated with this study are available on GitHub.</li>
</ul>

<h3>Title: On the Role of Depth and Looping for In-Context Learning with Task Diversity</h3>
<ul>
<li><strong>Authors: </strong>Khashayar Gatmiry, Nikunj Saunshi, Sashank J. Reddi, Stefanie Jegelka, Sanjiv Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21698">https://arxiv.org/abs/2410.21698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21698">https://arxiv.org/pdf/2410.21698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21698]] On the Role of Depth and Looping for In-Context Learning with Task Diversity(https://arxiv.org/abs/2410.21698)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>The intriguing in-context learning (ICL) abilities of deep Transformer models have lately garnered significant attention. By studying in-context linear regression on unimodal Gaussian data, recent empirical and theoretical works have argued that ICL emerges from Transformers' abilities to simulate learning algorithms like gradient descent. However, these works fail to capture the remarkable ability of Transformers to learn multiple tasks in context. To this end, we study in-context learning for linear regression with diverse tasks, characterized by data covariance matrices with condition numbers ranging from $[1, \kappa]$, and highlight the importance of depth in this setting. More specifically, (a) we show theoretical lower bounds of $\log(\kappa)$ (or $\sqrt{\kappa}$) linear attention layers in the unrestricted (or restricted) attention setting and, (b) we show that multilayer Transformers can indeed solve such tasks with a number of layers that matches the lower bounds. However, we show that this expressivity of multilayer Transformer comes at the price of robustness. In particular, multilayer Transformers are not robust to even distributional shifts as small as $O(e^{-L})$ in Wasserstein distance, where $L$ is the depth of the network. We then demonstrate that Looped Transformers -- a special class of multilayer Transformers with weight-sharing -- not only exhibit similar expressive power but are also provably robust under mild assumptions. Besides out-of-distribution generalization, we also show that Looped Transformers are the only models that exhibit a monotonic behavior of loss with respect to depth.</li>
</ul>

<h3>Title: Unsupervised Modality Adaptation with Text-to-Image Diffusion Models for Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ruihao Xia, Yu Liang, Peng-Tao Jiang, Hao Zhang, Bo Li, Yang Tang, Pan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21708">https://arxiv.org/abs/2410.21708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21708">https://arxiv.org/pdf/2410.21708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21708]] Unsupervised Modality Adaptation with Text-to-Image Diffusion Models for Semantic Segmentation(https://arxiv.org/abs/2410.21708)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Despite their success, unsupervised domain adaptation methods for semantic segmentation primarily focus on adaptation between image domains and do not utilize other abundant visual modalities like depth, infrared and event. This limitation hinders their performance and restricts their application in real-world multimodal scenarios. To address this issue, we propose Modality Adaptation with text-to-image Diffusion Models (MADM) for semantic segmentation task which utilizes text-to-image diffusion models pre-trained on extensive image-text pairs to enhance the model's cross-modality capabilities. Specifically, MADM comprises two key complementary components to tackle major challenges. First, due to the large modality gap, using one modal data to generate pseudo labels for another modality suffers from a significant drop in accuracy. To address this, MADM designs diffusion-based pseudo-label generation which adds latent noise to stabilize pseudo-labels and enhance label accuracy. Second, to overcome the limitations of latent low-resolution features in diffusion models, MADM introduces the label palette and latent regression which converts one-hot encoded labels into the RGB form by palette and regresses them in the latent space, thus ensuring the pre-trained decoder for up-sampling to obtain fine-grained features. Extensive experimental results demonstrate that MADM achieves state-of-the-art adaptation performance across various modality tasks, including images to depth, infrared, and event modalities. We open-source our code and models at this https URL.</li>
</ul>

<h3>Title: Fuzzing the PHP Interpreter via Dataflow Fusion</h3>
<ul>
<li><strong>Authors: </strong>Yuancheng Jiang, Chuqi Zhang, Bonan Ruan, Jiahao Liu, Manuel Rigger, Roland Yap, Zhenkai Liang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21713">https://arxiv.org/abs/2410.21713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21713">https://arxiv.org/pdf/2410.21713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21713]] Fuzzing the PHP Interpreter via Dataflow Fusion(https://arxiv.org/abs/2410.21713)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>PHP, a dominant scripting language in web development, powers a vast range of websites, from personal blogs to major platforms. While existing research primarily focuses on PHP application-level security issues like code injection, memory errors within the PHP interpreter have been largely overlooked. These memory errors, prevalent due to the PHP interpreter's extensive C codebase, pose significant risks to the confidentiality, integrity, and availability of PHP servers. This paper introduces FlowFusion, the first automatic fuzzing framework specifically designed to detect memory errors in the PHP interpreter. FlowFusion leverages dataflow as an efficient representation of test cases maintained by PHP developers, merging two or more test cases to produce fused test cases with more complex code semantics. Moreover, FlowFusion employs strategies such as test mutation, interface fuzzing, and environment crossover to further facilitate memory error detection. In our evaluation, FlowFusion identified 56 unknown memory errors in the PHP interpreter, with 38 fixed and 4 confirmed. We compared FlowFusion against the official test suite and a naive test concatenation approach, demonstrating that FlowFusion can detect new bugs that these methods miss, while also achieving greater code coverage. Furthermore, FlowFusion outperformed state-of-the-art fuzzers AFL++ and Polyglot, covering 24% more lines of code after 24 hours of fuzzing under identical execution environments. FlowFusion has been acknowledged by PHP developers, and we believe our approach offers a practical tool for enhancing the security of the PHP interpreter.</li>
</ul>

<h3>Title: A Bayesian Approach to Harnessing the Power of LLMs in Authorship Attribution</h3>
<ul>
<li><strong>Authors: </strong>Zhengmian Hu, Tong Zheng, Heng Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21716">https://arxiv.org/abs/2410.21716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21716">https://arxiv.org/pdf/2410.21716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21716]] A Bayesian Approach to Harnessing the Power of LLMs in Authorship Attribution(https://arxiv.org/abs/2410.21716)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Authorship attribution aims to identify the origin or author of a document. Traditional approaches have heavily relied on manual features and fail to capture long-range correlations, limiting their effectiveness. Recent advancements leverage text embeddings from pre-trained language models, which require significant fine-tuning on labeled data, posing challenges in data dependency and limited interpretability. Large Language Models (LLMs), with their deep reasoning capabilities and ability to maintain long-range textual associations, offer a promising alternative. This study explores the potential of pre-trained LLMs in one-shot authorship attribution, specifically utilizing Bayesian approaches and probability outputs of LLMs. Our methodology calculates the probability that a text entails previous writings of an author, reflecting a more nuanced understanding of authorship. By utilizing only pre-trained models such as Llama-3-70B, our results on the IMDb and blog datasets show an impressive 85\% accuracy in one-shot authorship classification across ten authors. Our findings set new baselines for one-shot authorship analysis using LLMs and expand the application scope of these models in forensic linguistics. This work also includes extensive ablation studies to validate our approach.</li>
</ul>

<h3>Title: Generating Realistic Tabular Data with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dang Nguyen, Sunil Gupta, Kien Do, Thin Nguyen, Svetha Venkatesh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21717">https://arxiv.org/abs/2410.21717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21717">https://arxiv.org/pdf/2410.21717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21717]] Generating Realistic Tabular Data with Large Language Models(https://arxiv.org/abs/2410.21717)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>While most generative models show achievements in image data generation, few are developed for tabular data generation. Recently, due to success of large language models (LLM) in diverse tasks, they have also been used for tabular data generation. However, these methods do not capture the correct correlation between the features and the target variable, hindering their applications in downstream predictive tasks. To address this problem, we propose a LLM-based method with three important improvements to correctly capture the ground-truth feature-class correlation in the real data. First, we propose a novel permutation strategy for the input data in the fine-tuning phase. Second, we propose a feature-conditional sampling approach to generate synthetic samples. Finally, we generate the labels by constructing prompts based on the generated samples to query our fine-tuned LLM. Our extensive experiments show that our method significantly outperforms 10 SOTA baselines on 20 datasets in downstream tasks. It also produces highly realistic synthetic samples in terms of quality and diversity. More importantly, classifiers trained with our synthetic data can even compete with classifiers trained with the original data on half of the benchmark datasets, which is a significant achievement in tabular data generation.</li>
</ul>

<h3>Title: DiffSTR: Controlled Diffusion Models for Scene Text Removal</h3>
<ul>
<li><strong>Authors: </strong>Sanhita Pathak, Vinay Kaushik, Brejesh Lall</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21721">https://arxiv.org/abs/2410.21721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21721">https://arxiv.org/pdf/2410.21721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21721]] DiffSTR: Controlled Diffusion Models for Scene Text Removal(https://arxiv.org/abs/2410.21721)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>To prevent unauthorized use of text in images, Scene Text Removal (STR) has become a crucial task. It focuses on automatically removing text and replacing it with a natural, text-less background while preserving significant details such as texture, color, and contrast. Despite its importance in privacy protection, STR faces several challenges, including boundary artifacts, inconsistent texture and color, and preserving correct shadows. Most STR approaches estimate a text region mask to train a model, solving for image translation or inpainting to generate a text-free image. Thus, the quality of the generated image depends on the accuracy of the inpainting mask and the generator's capability. In this work, we leverage the superior capabilities of diffusion models in generating high-quality, consistent images to address the STR problem. We introduce a ControlNet diffusion model, treating STR as an inpainting task. To enhance the model's robustness, we develop a mask pretraining pipeline to condition our diffusion model. This involves training a masked autoencoder (MAE) using a combination of box masks and coarse stroke masks, and fine-tuning it using masks derived from our novel segmentation-based mask refinement framework. This framework iteratively refines an initial mask and segments it using the SLIC and Hierarchical Feature Selection (HFS) algorithms to produce an accurate final text mask. This improves mask prediction and utilizes rich textural information in natural scene images to provide accurate inpainting masks. Experiments on the SCUT-EnsText and SCUT-Syn datasets demonstrate that our method significantly outperforms existing state-of-the-art techniques.</li>
</ul>

<h3>Title: Fine-tuning Large Language Models for DGA and DNS Exfiltration Detection</h3>
<ul>
<li><strong>Authors: </strong>Md Abu Sayed, Asif Rahman, Christopher Kiekintveld, Sebastian Garcia</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21723">https://arxiv.org/abs/2410.21723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21723">https://arxiv.org/pdf/2410.21723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21723]] Fine-tuning Large Language Models for DGA and DNS Exfiltration Detection(https://arxiv.org/abs/2410.21723)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Domain Generation Algorithms (DGAs) are malicious techniques used by malware to dynamically generate seemingly random domain names for communication with Command & Control (C&C) servers. Due to the fast and simple generation of DGA domains, detection methods must be highly efficient and precise to be effective. Large Language Models (LLMs) have demonstrated their proficiency in real-time detection tasks, making them ideal candidates for detecting DGAs. Our work validates the effectiveness of fine-tuned LLMs for detecting DGAs and DNS exfiltration attacks. We developed LLM models and conducted comprehensive evaluation using a diverse dataset comprising 59 distinct real-world DGA malware families and normal domain data. Our LLM model significantly outperformed traditional natural language processing techniques, especially in detecting unknown DGAs. We also evaluated its performance on DNS exfiltration datasets, demonstrating its effectiveness in enhancing cybersecurity measures. To the best of our knowledge, this is the first work that empirically applies LLMs for DGA and DNS exfiltration detection.</li>
</ul>

<h3>Title: Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kangyang Luo, Zichen Ding, Zhenmin Weng, Lingfeng Qiao, Meng Zhao, Xiang Li, Di Yin, Jinlong Shu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21728">https://arxiv.org/abs/2410.21728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21728">https://arxiv.org/pdf/2410.21728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21728]] Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models(https://arxiv.org/abs/2410.21728)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Chain of Thought (CoT) prompting approaches have significantly consolidated the reasoning capabilities of large language models (LLMs), they still face limitations that require extensive human effort or have performance needs to be improved. Existing endeavors have focused on bridging these gaps; however, these approaches either hinge on external data and cannot completely eliminate manual effort, or they fall short in effectively directing LLMs to generate high-quality exemplary prompts. To address the said pitfalls, we propose a novel prompt approach for automatic reasoning named \textbf{LBS3}, inspired by curriculum learning which better reflects human learning habits. Specifically, LBS3 initially steers LLMs to recall easy-to-hard proxy queries that are pertinent to the target query. Following this, it invokes a progressive strategy that utilizes exemplary prompts stemmed from easy-proxy queries to direct LLMs in solving hard-proxy queries, enabling the high-quality of the proxy solutions. Finally, our extensive experiments in various reasoning-intensive tasks with varying open- and closed-source LLMs show that LBS3 achieves strongly competitive performance compared to the SOTA baselines.</li>
</ul>

<h3>Title: Enhancing Financial Question Answering with a Multi-Agent Reflection Framework</h3>
<ul>
<li><strong>Authors: </strong>Sorouralsadat Fatemi, Yuheng Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21741">https://arxiv.org/abs/2410.21741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21741">https://arxiv.org/pdf/2410.21741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21741]] Enhancing Financial Question Answering with a Multi-Agent Reflection Framework(https://arxiv.org/abs/2410.21741)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) have shown impressive capabilities in numerous Natural Language Processing (NLP) tasks, they still struggle with financial question answering (QA), particularly when numerical reasoning is required. Recently, LLM-based multi-agent frameworks have demonstrated remarkable effectiveness in multi-step reasoning, which is crucial for financial QA tasks as it involves extracting relevant information from tables and text and then performing numerical reasoning on the extracted data to infer answers. In this study, we propose a multi-agent framework incorporating a critic agent that reflects on the reasoning steps and final answers for each question. Additionally, we enhance our system by adding multiple critic agents, each focusing on a specific aspect of the answer. Our results indicate that this framework significantly improves performance compared to single-agent reasoning, with an average performance increase of 15% for the LLaMA3-8B model and 5% for the LLaMA3-70B model. Furthermore, our framework performs on par with, and in some cases surpasses, larger single-agent LLMs such as LLaMA3.1-405B and GPT-4o-mini, though it falls slightly short compared to Claude-3.5 Sonnet. Overall, our framework presents an effective solution to enhance open-source LLMs for financial QA tasks, offering a cost-effective alternative to larger models like Claude-3.5 Sonnet.</li>
</ul>

<h3>Title: EI-Nexus: Towards Unmediated and Flexible Inter-Modality Local Feature Extraction and Matching for Event-Image Data</h3>
<ul>
<li><strong>Authors: </strong>Zhonghua Yi, Hao Shi, Qi Jiang, Kailun Yang, Ze Wang, Diyang Gu, Yufan Zhang, Kaiwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21743">https://arxiv.org/abs/2410.21743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21743">https://arxiv.org/pdf/2410.21743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21743]] EI-Nexus: Towards Unmediated and Flexible Inter-Modality Local Feature Extraction and Matching for Event-Image Data(https://arxiv.org/abs/2410.21743)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Event cameras, with high temporal resolution and high dynamic range, have limited research on the inter-modality local feature extraction and matching of event-image data. We propose EI-Nexus, an unmediated and flexible framework that integrates two modality-specific keypoint extractors and a feature matcher. To achieve keypoint extraction across viewpoint and modality changes, we bring Local Feature Distillation (LFD), which transfers the viewpoint consistency from a well-learned image extractor to the event extractor, ensuring robust feature correspondence. Furthermore, with the help of Context Aggregation (CA), a remarkable enhancement is observed in feature matching. We further establish the first two inter-modality feature matching benchmarks, MVSEC-RPE and EC-RPE, to assess relative pose estimation on event-image data. Our approach outperforms traditional methods that rely on explicit modal transformation, offering more unmediated and adaptable feature extraction and matching, achieving better keypoint similarity and state-of-the-art results on the MVSEC-RPE and EC-RPE benchmarks. The source code and benchmarks will be made publicly available at this https URL.</li>
</ul>

<h3>Title: A Dual Adaptive Assignment Approach for Robust Graph-Based Clustering</h3>
<ul>
<li><strong>Authors: </strong>Yang Xiang, Li Fan, Tulika Saha, Yushan Pan, Haiyang Zhang, Chengtao Ji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21745">https://arxiv.org/abs/2410.21745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21745">https://arxiv.org/pdf/2410.21745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21745]] A Dual Adaptive Assignment Approach for Robust Graph-Based Clustering(https://arxiv.org/abs/2410.21745)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graph clustering is an essential aspect of network analysis that involves grouping nodes into separate clusters. Recent developments in deep learning have resulted in advanced deep graph clustering techniques, which have proven effective in many applications. Nonetheless, these methods often encounter difficulties when dealing with the complexities of real-world graphs, particularly in the presence of noisy edges. Additionally, many denoising graph clustering strategies tend to suffer from lower performance compared to their non-denoised counterparts, training instability, and challenges in scaling to large datasets. To tackle these issues, we introduce a new framework called the Dual Adaptive Assignment Approach for Robust Graph-Based Clustering (RDSA). RDSA consists of three key components: (i) a node embedding module that effectively integrates the graph's topological features and node attributes; (ii) a structure-based soft assignment module that improves graph modularity by utilizing an affinity matrix for node assignments; and (iii) a node-based soft assignment module that identifies community landmarks and refines node assignments to enhance the model's robustness. We assess RDSA on various real-world datasets, demonstrating its superior performance relative to existing state-of-the-art methods. Our findings indicate that RDSA provides robust clustering across different graph types, excelling in clustering effectiveness and robustness, including adaptability to noise, stability, and scalability.</li>
</ul>

<h3>Title: MotionGPT-2: A General-Purpose Motion-Language Model for Motion Generation and Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yuan Wang, Di Huang, Yaqi Zhang, Wanli Ouyang, Jile Jiao, Xuetao Feng, Yan Zhou, Pengfei Wan, Shixiang Tang, Dan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21747">https://arxiv.org/abs/2410.21747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21747">https://arxiv.org/pdf/2410.21747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21747]] MotionGPT-2: A General-Purpose Motion-Language Model for Motion Generation and Understanding(https://arxiv.org/abs/2410.21747)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Generating lifelike human motions from descriptive texts has experienced remarkable research focus in the recent years, propelled by the emerging requirements of digital this http URL impressive advances, existing approaches are often constrained by limited control modalities, task specificity, and focus solely on body motion this http URL this paper, we present MotionGPT-2, a unified Large Motion-Language Model (LMLM) that addresses these limitations. MotionGPT-2 accommodates multiple motion-relevant tasks and supporting multimodal control conditions through pre-trained Large Language Models (LLMs). It quantizes multimodal inputs-such as text and single-frame poses-into discrete, LLM-interpretable tokens, seamlessly integrating them into the LLM's vocabulary. These tokens are then organized into unified prompts, guiding the LLM to generate motion outputs through a pretraining-then-finetuning paradigm. We also show that the proposed MotionGPT-2 is highly adaptable to the challenging 3D holistic motion generation task, enabled by the innovative motion discretization framework, Part-Aware VQVAE, which ensures fine-grained representations of body and hand movements. Extensive experiments and visualizations validate the effectiveness of our method, demonstrating the adaptability of MotionGPT-2 across motion generation, motion captioning, and generalized motion completion tasks.</li>
</ul>

<h3>Title: Learning and Unlearning of Fabricated Knowledge in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chen Sun, Nolan Andrew Miller, Andrey Zhmoginov, Max Vladymyrov, Mark Sandler</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21750">https://arxiv.org/abs/2410.21750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21750">https://arxiv.org/pdf/2410.21750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21750]] Learning and Unlearning of Fabricated Knowledge in Language Models(https://arxiv.org/abs/2410.21750)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>What happens when a new piece of knowledge is introduced into the training data and how long does it last while a large language model (LM) continues to train? We investigate this question by injecting facts into LMs from a new probing dataset, "Outlandish", which is designed to permit the testing of a spectrum of different fact types. When studying how robust these memories are, there appears to be a sweet spot in the spectrum of fact novelty between consistency with world knowledge and total randomness, where the injected memory is the most enduring. Specifically we show that facts that conflict with common knowledge are remembered for tens of thousands of training steps, while prompts not conflicting with common knowledge (mundane), as well as scrambled prompts (randomly jumbled) are both forgotten much more rapidly. Further, knowledge-conflicting facts can "prime'' how the language model hallucinates on logically unrelated prompts, showing their propensity for non-target generalization, while both mundane and randomly jumbled facts prime significantly less. Finally, we show that impacts of knowledge-conflicting facts in LMs, though they can be long lasting, can be largely erased by novel application of multi-step sparse updates, even while the training ability of the model is preserved. As such, this very simple procedure has direct implications for mitigating the effects of data poisoning in training.</li>
</ul>

<h3>Title: Memory-Efficient Point Cloud Registration via Overlapping Region Sampling</h3>
<ul>
<li><strong>Authors: </strong>Tomoyasu Shimada, Kazuhiko Murasaki, Shogo Sato, Toshihiko Nishimura, Taiga Yoshida, Ryuichi Tanida</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21753">https://arxiv.org/abs/2410.21753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21753">https://arxiv.org/pdf/2410.21753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21753]] Memory-Efficient Point Cloud Registration via Overlapping Region Sampling(https://arxiv.org/abs/2410.21753)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in deep learning have improved 3D point cloud registration but increased graphics processing unit (GPU) memory usage, often requiring preliminary sampling that reduces accuracy. We propose an overlapping region sampling method to reduce memory usage while maintaining accuracy. Our approach estimates the overlapping region and intensively samples from it, using a k-nearest-neighbor (kNN) based point compression mechanism with multi layer perceptron (MLP) and transformer architectures. Evaluations on 3DMatch and 3DLoMatch datasets show our method outperforms other sampling methods in registration recall, especially at lower GPU memory levels. For 3DMatch, we achieve 94% recall with 33% reduced memory usage, with greater advantages in 3DLoMatch. Our method enables efficient large-scale point cloud registration in resource-constrained environments, maintaining high accuracy while significantly reducing memory requirements.</li>
</ul>

<h3>Title: IntLoRA: Integral Low-rank Adaptation of Quantized Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hang Guo, Yawei Li, Tao Dai, Shu-Tao Xia, Luca Benini</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21759">https://arxiv.org/abs/2410.21759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21759">https://arxiv.org/pdf/2410.21759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21759]] IntLoRA: Integral Low-rank Adaptation of Quantized Diffusion Models(https://arxiv.org/abs/2410.21759)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Fine-tuning large-scale text-to-image diffusion models for various downstream tasks has yielded impressive results. However, the heavy computational burdens of tuning large models prevent personal customization. Recent advances have attempted to employ parameter-efficient fine-tuning (PEFT) techniques to adapt the floating-point (FP) or quantized pre-trained weights. Nonetheless, the adaptation parameters in existing works are still restricted to FP arithmetic, hindering hardware-friendly acceleration. In this work, we propose IntLoRA, to further push the efficiency limits by using integer type (INT) low-rank parameters to adapt the quantized diffusion models. By working in the integer arithmetic, our IntLoRA offers three key advantages: (i) for fine-tuning, the pre-trained weights are quantized, reducing memory usage; (ii) for storage, both pre-trained and low-rank weights are in INT which consumes less disk space; (iii) for inference, IntLoRA weights can be naturally merged into quantized pre-trained weights through efficient integer multiplication or bit-shifting, eliminating additional post-training quantization. Extensive experiments demonstrate that IntLoRA can achieve performance on par with or even superior to the vanilla LoRA, accompanied by significant efficiency improvements. Code is available at \url{this https URL}.</li>
</ul>

<h3>Title: Online Mirror Descent for Tchebycheff Scalarization in Multi-Objective Optimization</h3>
<ul>
<li><strong>Authors: </strong>Meitong Liu, Xiaoyuan Zhang, Chulin Xie, Kate Donahue, Han Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21764">https://arxiv.org/abs/2410.21764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21764">https://arxiv.org/pdf/2410.21764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21764]] Online Mirror Descent for Tchebycheff Scalarization in Multi-Objective Optimization(https://arxiv.org/abs/2410.21764)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, fair</a></li>
<li><strong>Abstract: </strong>The goal of multi-objective optimization (MOO) is to learn under multiple, potentially conflicting, objectives. One widely used technique to tackle MOO is through linear scalarization, where one fixed preference vector is used to combine the objectives into a single scalar value for optimization. However, recent work (Hu et al., 2024) has shown linear scalarization often fails to capture the non-convex regions of the Pareto Front, failing to recover the complete set of Pareto optimal solutions. In light of the above limitations, this paper focuses on Tchebycheff scalarization that optimizes for the worst-case objective. In particular, we propose an online mirror descent algorithm for Tchebycheff scalarization, which we call OMD-TCH. We show that OMD-TCH enjoys a convergence rate of $O(\sqrt{\log m/T})$ where $m$ is the number of objectives and $T$ is the number of iteration rounds. We also propose a novel adaptive online-to-batch conversion scheme that significantly improves the practical performance of OMD-TCH while maintaining the same convergence guarantees. We demonstrate the effectiveness of OMD-TCH and the adaptive conversion scheme on both synthetic problems and federated learning tasks under fairness constraints, showing state-of-the-art performance.</li>
</ul>

<h3>Title: Leveraging LLMs for Hypothetical Deduction in Logical Inference: A Neuro-Symbolic Approach</h3>
<ul>
<li><strong>Authors: </strong>Qingchuan Li, Jiatong Li, Tongxuan Liu, Yuting Zeng, Mingyue Cheng, Weizhe Huang, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21779">https://arxiv.org/abs/2410.21779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21779">https://arxiv.org/pdf/2410.21779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21779]] Leveraging LLMs for Hypothetical Deduction in Logical Inference: A Neuro-Symbolic Approach(https://arxiv.org/abs/2410.21779)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have exhibited remarkable potential across a wide array of reasoning tasks, including logical reasoning. Although massive efforts have been made to empower the logical reasoning ability of LLMs via external logical symbolic solvers, crucial challenges of the poor generalization ability to questions with different features and inevitable question information loss of symbolic solver-driven approaches remain unresolved. To mitigate these issues, we introduce LINA, a LLM-driven neuro-symbolic approach for faithful logical reasoning. By enabling an LLM to autonomously perform the transition from propositional logic extraction to sophisticated logical reasoning, LINA not only bolsters the resilience of the reasoning process but also eliminates the dependency on external solvers. Additionally, through its adoption of a hypothetical-deductive reasoning paradigm, LINA effectively circumvents the expansive search space challenge that plagues traditional forward reasoning methods. Empirical evaluations demonstrate that LINA substantially outperforms both established propositional logic frameworks and conventional prompting techniques across a spectrum of five logical reasoning tasks. Specifically, LINA achieves an improvement of 24.34% over LINC on the FOLIO dataset, while also surpassing prompting strategies like CoT and CoT-SC by up to 24.02%. Our code is available at this https URL.</li>
</ul>

<h3>Title: HairDiffusion: Vivid Multi-Colored Hair Editing via Latent Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yu Zeng, Yang Zhang, Jiachen Liu, Linlin Shen, Kaijun Deng, Weizhao He, Jinbao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21789">https://arxiv.org/abs/2410.21789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21789">https://arxiv.org/pdf/2410.21789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21789]] HairDiffusion: Vivid Multi-Colored Hair Editing via Latent Diffusion(https://arxiv.org/abs/2410.21789)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Hair editing is a critical image synthesis task that aims to edit hair color and hairstyle using text descriptions or reference images, while preserving irrelevant attributes (e.g., identity, background, cloth). Many existing methods are based on StyleGAN to address this task. However, due to the limited spatial distribution of StyleGAN, it struggles with multiple hair color editing and facial preservation. Considering the advancements in diffusion models, we utilize Latent Diffusion Models (LDMs) for hairstyle editing. Our approach introduces Multi-stage Hairstyle Blend (MHB), effectively separating control of hair color and hairstyle in diffusion latent space. Additionally, we train a warping module to align the hair color with the target region. To further enhance multi-color hairstyle editing, we fine-tuned a CLIP model using a multi-color hairstyle dataset. Our method not only tackles the complexity of multi-color hairstyles but also addresses the challenge of preserving original colors during diffusion editing. Extensive experiments showcase the superiority of our method in editing multi-color hairstyles while preserving facial attributes given textual descriptions and reference images.</li>
</ul>

<h3>Title: Enhancing Adversarial Attacks through Chain of Thought</h3>
<ul>
<li><strong>Authors: </strong>Jingbo Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21791">https://arxiv.org/abs/2410.21791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21791">https://arxiv.org/pdf/2410.21791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21791]] Enhancing Adversarial Attacks through Chain of Thought(https://arxiv.org/abs/2410.21791)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive performance across various domains but remain susceptible to safety concerns. Prior research indicates that gradient-based adversarial attacks are particularly effective against aligned LLMs and the chain of thought (CoT) prompting can elicit desired answers through step-by-step reasoning. This paper proposes enhancing the robustness of adversarial attacks on aligned LLMs by integrating CoT prompts with the greedy coordinate gradient (GCG) technique. Using CoT triggers instead of affirmative targets stimulates the reasoning abilities of backend LLMs, thereby improving the transferability and universality of adversarial attacks. We conducted an ablation study comparing our CoT-GCG approach with Amazon Web Services auto-cot. Results revealed our approach outperformed both the baseline GCG attack and CoT prompting. Additionally, we used Llama Guard to evaluate potentially harmful interactions, providing a more objective risk assessment of entire conversations compared to matching outputs to rejection phrases. The code of this paper is available at this https URL.</li>
</ul>

<h3>Title: Text-Guided Attention is All You Need for Zero-Shot Robustness in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lu Yu, Haiyang Zhang, Changsheng Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21802">https://arxiv.org/abs/2410.21802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21802">https://arxiv.org/pdf/2410.21802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21802]] Text-Guided Attention is All You Need for Zero-Shot Robustness in Vision-Language Models(https://arxiv.org/abs/2410.21802)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Due to the impressive zero-shot capabilities, pre-trained vision-language models (e.g. CLIP), have attracted widespread attention and adoption across various domains. Nonetheless, CLIP has been observed to be susceptible to adversarial examples. Through experimental analysis, we have observed a phenomenon wherein adversarial perturbations induce shifts in text-guided attention. Building upon this observation, we propose a simple yet effective strategy: __Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR)__. This framework incorporates two components: the Attention Refinement module and the Attention-based Model Constraint module. Our goal is to maintain the generalization of the CLIP model and enhance its adversarial robustness: The Attention Refinement module aligns the text-guided attention obtained from the target model via adversarial examples with the text-guided attention acquired from the original model via clean examples. This alignment enhances the model's robustness. Additionally, the Attention-based Model Constraint module acquires text-guided attention from both the target and original models using clean examples. Its objective is to maintain model performance on clean samples while enhancing overall robustness. The experiments validate that our method yields a 9.58\% enhancement in zero-shot robust accuracy over the current state-of-the-art techniques across 16 datasets. __Our code is available at__ this https URL.</li>
</ul>

<h3>Title: SimSiam Naming Game: A Unified Approach for Representation Learning and Emergent Communication</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Le Hoang, Tadahiro Taniguchi, Fang Tianwei, Akira Taniguchi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21803">https://arxiv.org/abs/2410.21803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21803">https://arxiv.org/pdf/2410.21803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21803]] SimSiam Naming Game: A Unified Approach for Representation Learning and Emergent Communication(https://arxiv.org/abs/2410.21803)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Emergent communication, driven by generative models, enables agents to develop a shared language for describing their individual views of the same objects through interactions. Meanwhile, self-supervised learning (SSL), particularly SimSiam, uses discriminative representation learning to make representations of augmented views of the same data point closer in the representation space. Building on the prior work of VI-SimSiam, which incorporates a generative and Bayesian perspective into the SimSiam framework via variational inference (VI) interpretation, we propose SimSiam+VAE, a unified approach for both representation learning and emergent communication. SimSiam+VAE integrates a variational autoencoder (VAE) into the predictor of the SimSiam network to enhance representation learning and capture uncertainty. Experimental results show that SimSiam+VAE outperforms both SimSiam and VI-SimSiam. We further extend this model into a communication framework called the SimSiam Naming Game (SSNG), which applies the generative and Bayesian approach based on VI to develop internal representations and emergent language, while utilizing the discriminative process of SimSiam to facilitate mutual understanding between agents. In experiments with established models, despite the dynamic alternation of agent roles during interactions, SSNG demonstrates comparable performance to the referential game and slightly outperforms the Metropolis-Hastings naming game.</li>
</ul>

<h3>Title: Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Li Shen, Anke Tang, Enneng Yang, Guibing Guo, Yong Luo, Lefei Zhang, Xiaochun Cao, Bo Du, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21804">https://arxiv.org/abs/2410.21804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21804">https://arxiv.org/pdf/2410.21804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21804]] Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging(https://arxiv.org/abs/2410.21804)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Multi-task learning (MTL) leverages a shared model to accomplish multiple tasks and facilitate knowledge transfer. Recent research on task arithmetic-based MTL demonstrates that merging the parameters of independently fine-tuned models can effectively achieve MTL. However, existing merging methods primarily seek a static optimal solution within the original model parameter space, which often results in performance degradation due to the inherent diversity among tasks and potential interferences. To address this challenge, in this paper, we propose a Weight-Ensembling Mixture of Experts (WEMoE) method for multi-task model merging. Specifically, we first identify critical (or sensitive) modules by analyzing parameter variations in core modules of Transformer-based models before and after finetuning. Then, our WEMoE statically merges non-critical modules while transforming critical modules into a mixture-of-experts (MoE) structure. During inference, expert modules in the MoE are dynamically merged based on input samples, enabling a more flexible and adaptive merging approach. Building on WEMoE, we further introduce an efficient-and-effective WEMoE (E-WEMoE) method, whose core mechanism involves eliminating non-essential elements in the critical modules of WEMoE and implementing shared routing across multiple MoE modules, thereby significantly reducing both the trainable parameters, the overall parameter count, and computational overhead of the merged model by WEMoE. Experimental results across various architectures and tasks demonstrate that both WEMoE and E-WEMoE outperform state-of-the-art (SOTA) model merging methods in terms of MTL performance, generalization, and robustness.</li>
</ul>

<h3>Title: SAM-Swin: SAM-Driven Dual-Swin Transformers with Adaptive Lesion Enhancement for Laryngo-Pharyngeal Tumor Detection</h3>
<ul>
<li><strong>Authors: </strong>Jia Wei, Yun Li, Xiaomao Fan, Wenjun Ma, Meiyu Qiu, Hongyu Chen, Wenbin Lei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21813">https://arxiv.org/abs/2410.21813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21813">https://arxiv.org/pdf/2410.21813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21813]] SAM-Swin: SAM-Driven Dual-Swin Transformers with Adaptive Lesion Enhancement for Laryngo-Pharyngeal Tumor Detection(https://arxiv.org/abs/2410.21813)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Laryngo-pharyngeal cancer (LPC) is a highly lethal malignancy in the head and neck region. Recent advancements in tumor detection, particularly through dual-branch network architectures, have significantly improved diagnostic accuracy by integrating global and local feature extraction. However, challenges remain in accurately localizing lesions and fully capitalizing on the complementary nature of features within these branches. To address these issues, we propose SAM-Swin, an innovative SAM-driven Dual-Swin Transformer for laryngo-pharyngeal tumor detection. This model leverages the robust segmentation capabilities of the Segment Anything Model 2 (SAM2) to achieve precise lesion segmentation. Meanwhile, we present a multi-scale lesion-aware enhancement module (MS-LAEM) designed to adaptively enhance the learning of nuanced complementary features across various scales, improving the quality of feature extraction and representation. Furthermore, we implement a multi-scale class-aware guidance (CAG) loss that delivers multi-scale targeted supervision, thereby enhancing the model's capacity to extract class-specific features. To validate our approach, we compiled three LPC datasets from the First Affiliated Hospital (FAHSYSU), the Sixth Affiliated Hospital (SAHSYSU) of Sun Yat-sen University, and Nanfang Hospital of Southern Medical University (NHSMU). The FAHSYSU dataset is utilized for internal training, while the SAHSYSU and NHSMU datasets serve for external evaluation. Extensive experiments demonstrate that SAM-Swin outperforms state-of-the-art methods, showcasing its potential for advancing LPC detection and improving patient outcomes. The source code of SAM-Swin is available at the URL of \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box Models</h3>
<ul>
<li><strong>Authors: </strong>Shaobo Wang, Hongxuan Tang, Mingyang Wang, Hongrui Zhang, Xuyang Liu, Weiya Li, Xuming Hu, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21815">https://arxiv.org/abs/2410.21815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21815">https://arxiv.org/pdf/2410.21815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21815]] Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box Models(https://arxiv.org/abs/2410.21815)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>The debate between self-interpretable models and post-hoc explanations for black-box models is central to Explainable AI (XAI). Self-interpretable models, such as concept-based networks, offer insights by connecting decisions to human-understandable concepts but often struggle with performance and scalability. Conversely, post-hoc methods like Shapley values, while theoretically robust, are computationally expensive and resource-intensive. To bridge the gap between these two lines of research, we propose a novel method that combines their strengths, providing theoretically guaranteed self-interpretability for black-box models without compromising prediction accuracy. Specifically, we introduce a parameter-efficient pipeline, *AutoGnothi*, which integrates a small side network into the black-box model, allowing it to generate Shapley value explanations without changing the original network parameters. This side-tuning approach significantly reduces memory, training, and inference costs, outperforming traditional parameter-efficient methods, where full fine-tuning serves as the optimal baseline. *AutoGnothi* enables the black-box model to predict and explain its predictions with minimal overhead. Extensive experiments show that *AutoGnothi* offers accurate explanations for both vision and language tasks, delivering superior computational efficiency with comparable interpretability.</li>
</ul>

<h3>Title: Self-Preference Bias in LLM-as-a-Judge</h3>
<ul>
<li><strong>Authors: </strong>Koki Wataoka, Tsubasa Takahashi, Ryokan Ri</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21819">https://arxiv.org/abs/2410.21819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21819">https://arxiv.org/pdf/2410.21819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21819]] Self-Preference Bias in LLM-as-a-Judge(https://arxiv.org/abs/2410.21819)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automated evaluation leveraging large language models (LLMs), commonly referred to as LLM evaluators or LLM-as-a-judge, has been widely used in measuring the performance of dialogue systems. However, the self-preference bias in LLMs has posed significant risks, including promoting specific styles or policies intrinsic to the LLMs. Despite the importance of this issue, there is a lack of established methods to measure the self-preference bias quantitatively, and its underlying causes are poorly understood. In this paper, we introduce a novel quantitative metric to measure the self-preference bias. Our experimental results demonstrate that GPT-4 exhibits a significant degree of self-preference bias. To explore the causes, we hypothesize that LLMs may favor outputs that are more familiar to them, as indicated by lower perplexity. We analyze the relationship between LLM evaluations and the perplexities of outputs. Our findings reveal that LLMs assign significantly higher evaluations to outputs with lower perplexity than human evaluators, regardless of whether the outputs were self-generated. This suggests that the essence of the bias lies in perplexity and that the self-preference bias exists because LLMs prefer texts more familiar to them.</li>
</ul>

<h3>Title: Volumetric Conditioning Module to Control Pretrained Diffusion Models for 3D Medical Images</h3>
<ul>
<li><strong>Authors: </strong>Suhyun Ahn, Wonjung Park, Jihoon Cho, Seunghyuck Park, Jinah Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21826">https://arxiv.org/abs/2410.21826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21826">https://arxiv.org/pdf/2410.21826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21826]] Volumetric Conditioning Module to Control Pretrained Diffusion Models for 3D Medical Images(https://arxiv.org/abs/2410.21826)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Spatial control methods using additional modules on pretrained diffusion models have gained attention for enabling conditional generation in natural images. These methods guide the generation process with new conditions while leveraging the capabilities of large models. They could be beneficial as training strategies in the context of 3D medical imaging, where training a diffusion model from scratch is challenging due to high computational costs and data scarcity. However, the potential application of spatial control methods with additional modules to 3D medical images has not yet been explored. In this paper, we present a tailored spatial control method for 3D medical images with a novel lightweight module, Volumetric Conditioning Module (VCM). Our VCM employs an asymmetric U-Net architecture to effectively encode complex information from various levels of 3D conditions, providing detailed guidance in image synthesis. To examine the applicability of spatial control methods and the effectiveness of VCM for 3D medical data, we conduct experiments under single- and multimodal conditions scenarios across a wide range of dataset sizes, from extremely small datasets with 10 samples to large datasets with 500 samples. The experimental results show that the VCM is effective for conditional generation and efficient in terms of requiring less training data and computational resources. We further investigate the potential applications for our spatial control method through axial super-resolution for medical images. Our code is available at \url{this https URL}</li>
</ul>

<h3>Title: Enhanced Survival Prediction in Head and Neck Cancer Using Convolutional Block Attention and Multimodal Data Fusion</h3>
<ul>
<li><strong>Authors: </strong>Aiman Farooq, Utkarsh Sharma, Deepak Mishra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21831">https://arxiv.org/abs/2410.21831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21831">https://arxiv.org/pdf/2410.21831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21831]] Enhanced Survival Prediction in Head and Neck Cancer Using Convolutional Block Attention and Multimodal Data Fusion(https://arxiv.org/abs/2410.21831)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Accurate survival prediction in head and neck cancer (HNC) is essential for guiding clinical decision-making and optimizing treatment strategies. Traditional models, such as Cox proportional hazards, have been widely used but are limited in their ability to handle complex multi-modal data. This paper proposes a deep learning-based approach leveraging CT and PET imaging modalities to predict survival outcomes in HNC patients. Our method integrates feature extraction with a Convolutional Block Attention Module (CBAM) and a multi-modal data fusion layer that combines imaging data to generate a compact feature representation. The final prediction is achieved through a fully parametric discrete-time survival model, allowing for flexible hazard functions that overcome the limitations of traditional survival models. We evaluated our approach using the HECKTOR and HEAD-NECK-RADIOMICS- HN1 datasets, demonstrating its superior performance compared to conconventional statistical and machine learning models. The results indicate that our deep learning model significantly improves survival prediction accuracy, offering a robust tool for personalized treatment planning in HNC</li>
</ul>

<h3>Title: Multi-aspect Depression Severity Assessment via Inductive Dialogue System</h3>
<ul>
<li><strong>Authors: </strong>Chaebin Lee, Seungyeon Seo, Heejin Do, Gary Geunbae Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21836">https://arxiv.org/abs/2410.21836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21836">https://arxiv.org/pdf/2410.21836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21836]] Multi-aspect Depression Severity Assessment via Inductive Dialogue System(https://arxiv.org/abs/2410.21836)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the advancement of chatbots and the growing demand for automatic depression detection, identifying depression in patient conversations has gained more attention. However, prior methods often assess depression in a binary way or only a single score without diverse feedback and lack focus on enhancing dialogue responses. In this paper, we present a novel task of multi-aspect depression severity assessment via an inductive dialogue system (MaDSA), evaluating a patient's depression level on multiple criteria by incorporating an assessment-aided response generation. Further, we propose a foundational system for MaDSA, which induces psychological dialogue responses with an auxiliary emotion classification task within a hierarchical severity assessment structure. We synthesize the conversational dataset annotated with eight aspects of depression severity alongside emotion labels, proven robust via human evaluations. Experimental results show potential for our preliminary work on MaDSA.</li>
</ul>

<h3>Title: Optimized Homomorphic Vector Permutation From New Decomposition Techniques</h3>
<ul>
<li><strong>Authors: </strong>Xirong Ma, Junling Fang, Steven Duong, Yali Jiang, Chunpeng Ge</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21840">https://arxiv.org/abs/2410.21840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21840">https://arxiv.org/pdf/2410.21840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21840]] Optimized Homomorphic Vector Permutation From New Decomposition Techniques(https://arxiv.org/abs/2410.21840)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Homomorphic permutations are fundamental to privacy-preserving computations based on word-wise homomorphic encryptions, which can be accelerated through permutation decomposition. This paper defines an ideal performance of any decomposition on permutations and designs algorithms to achieve this bound. We start by proposing an algorithm searching depth-1 ideal decomposition solutions for permutations. This allows us to ascertain the full-depth ideal decomposability of two types of permutations used in specific homomorphic matrix transposition (SIGSAC 18) and multiplication (CCSW 22), enabling these algorithms to achieve asymptotic improvement in speed and rotation key reduction. We further devise a new strategy for homomorphically computing arbitrary permutations, aiming to approximate the performance limits of ideal decomposition, as permutations with weak structures are unlikely to be ideally factorized. Our design deviates from the conventional scope of permutation decomposition and surpasses state-of-the-art techniques (EUROCRYPT 12, CRYPTO 14) with a speed-up of $\times 1.05 \sim \times 2.27$ under minimum requirement of rotation keys.</li>
</ul>

<h3>Title: Diffusion as Reasoning: Enhancing Object Goal Navigation with LLM-Biased Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yiming Ji, Yang Liu, Zhengpu Wang, Boyu Ma, Zongwu Xie, Hong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21842">https://arxiv.org/abs/2410.21842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21842">https://arxiv.org/pdf/2410.21842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21842]] Diffusion as Reasoning: Enhancing Object Goal Navigation with LLM-Biased Diffusion Model(https://arxiv.org/abs/2410.21842)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The Object Goal Navigation (ObjectNav) task requires the agent to navigate to a specified target in an unseen environment. Since the environment layout is unknown, the agent needs to perform semantic reasoning to infer the potential location of the target, based on its accumulated memory of the environment during the navigation process. Diffusion models have been shown to be able to learn the distribution relationships between features in RGB images, and thus generate new realistic this http URL this work, we propose a new approach to solving the ObjectNav task, by training a diffusion model to learn the statistical distribution patterns of objects in semantic maps, and using the map of the explored regions during navigation as the condition to generate the map of the unknown regions, thereby realizing the semantic reasoning of the target object, i.e., diffusion as reasoning (DAR). Meanwhile, we propose the global target bias and local LLM bias methods, where the former can constrain the diffusion model to generate the target object more effectively, and the latter utilizes the common sense knowledge extracted from the LLM to improve the generalization of the reasoning process. Based on the generated map in the unknown region, the agent sets the predicted location of the target as the goal and moves towards it. Experiments on Gibson and MP3D show the effectiveness of our method.</li>
</ul>

<h3>Title: Micro-Structures Graph-Based Point Cloud Registration for Balancing Efficiency and Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Rongling Zhang, Li Yan, Pengcheng Wei, Hong Xie, Pinzhuo Wang, Binbing Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21857">https://arxiv.org/abs/2410.21857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21857">https://arxiv.org/pdf/2410.21857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21857]] Micro-Structures Graph-Based Point Cloud Registration for Balancing Efficiency and Accuracy(https://arxiv.org/abs/2410.21857)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Point Cloud Registration (PCR) is a fundamental and significant issue in photogrammetry and remote sensing, aiming to seek the optimal rigid transformation between sets of points. Achieving efficient and precise PCR poses a considerable challenge. We propose a novel micro-structures graph-based global point cloud registration method. The overall method is comprised of two stages. 1) Coarse registration (CR): We develop a graph incorporating micro-structures, employing an efficient graph-based hierarchical strategy to remove outliers for obtaining the maximal consensus set. We propose a robust GNC-Welsch estimator for optimization derived from a robust estimator to the outlier process in the Lie algebra space, achieving fast and robust alignment. 2) Fine registration (FR): To refine local alignment further, we use the octree approach to adaptive search plane features in the micro-structures. By minimizing the distance from the point-to-plane, we can obtain a more precise local alignment, and the process will also be addressed effectively by being treated as a planar adjustment algorithm combined with Anderson accelerated optimization (PA-AA). After extensive experiments on real data, our proposed method performs well on the 3DMatch and ETH datasets compared to the most advanced methods, achieving higher accuracy metrics and reducing the time cost by at least one-third.</li>
</ul>

<h3>Title: Token-based identity management in the distributed cloud</h3>
<ul>
<li><strong>Authors: </strong>Ivana Kovacevic, Tamara Rankovic, Milan Stojkov, Milos Simic</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21865">https://arxiv.org/abs/2410.21865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21865">https://arxiv.org/pdf/2410.21865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21865]] Token-based identity management in the distributed cloud(https://arxiv.org/abs/2410.21865)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, robust</a></li>
<li><strong>Abstract: </strong>The immense shift to cloud computing has brought changes in security and privacy requirements, impacting critical Identity Management services. Currently, many IdM systems and solutions are accessible as cloud services, delivering identity services for applications in closed domains and the public cloud. This research paper centres on identity management in distributed environments, emphasising the importance of robust up to date authorisation mechanisms. The paper concentrates on implementing robust security paradigms to minimise communication overhead among services while preserving privacy and access control. The key contribution focuses on solving the problem of restricted access to resources in cases when the authentication token is still valid, but permissions are updated. The proposed solution incorporates an Identity and Access Management server as a component that authenticates all external requests. The IAM server key responsibilities include maintaining user data, assigning privileges within the system, and authorisation. Furthermore, it empowers users by offering an Application Programming Interface for managing users and their rights within the same organisation, providing finer granularity in authorisation. The IAM server has been integrated with a configuration dissemination tool designed as a distributed cloud infrastructure to evaluate the solution.</li>
</ul>

<h3>Title: Improving In-Context Learning with Small Language Model Ensembles</h3>
<ul>
<li><strong>Authors: </strong>M. Mehdi Mojarradi, Lingyi Yang, Robert McCraith, Adam Mahdi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21868">https://arxiv.org/abs/2410.21868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21868">https://arxiv.org/pdf/2410.21868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21868]] Improving In-Context Learning with Small Language Model Ensembles(https://arxiv.org/abs/2410.21868)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown impressive capabilities across various tasks, but their performance on domain-specific tasks remains limited. While methods like retrieval augmented generation and fine-tuning can help to address this, they require significant resources. In-context learning (ICL) is a cheap and efficient alternative but cannot match the accuracies of advanced methods. We present Ensemble SuperICL, a novel approach that enhances ICL by leveraging the expertise of multiple fine-tuned small language models (SLMs). Ensemble SuperICL achieves state of the art (SoTA) results on several natural language understanding benchmarks. Additionally, we test it on a medical-domain labelling task and showcase its practicality by using off-the-shelf SLMs fine-tuned on a general language task, achieving superior accuracy in large-scale data labelling compared to all baselines. Finally, we conduct an ablation study and sensitivity analyses to elucidate the underlying mechanism of Ensemble SuperICL. Our research contributes to the growing demand for efficient domain specialisation methods in LLMs, offering a cheap and effective method for practitioners.</li>
</ul>

<h3>Title: Authentication and identity management based on zero trust security model in micro-cloud environment</h3>
<ul>
<li><strong>Authors: </strong>Ivana Kovacevic, Milan Stojkov, Milos Simic</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21870">https://arxiv.org/abs/2410.21870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21870">https://arxiv.org/pdf/2410.21870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21870]] Authentication and identity management based on zero trust security model in micro-cloud environment(https://arxiv.org/abs/2410.21870)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>The abilities of traditional perimeter-based security architectures are rapidly decreasing as more enterprise assets are moved toward the cloud environment. From a security viewpoint, the Zero Trust framework can better track and block external attackers while limiting security breaches resulting from insider attacks in the cloud paradigm. Furthermore, Zero Trust can better accomplish access privileges for users and devices across cloud environments to enable the secure sharing of resources. Moreover, the concept of zero trust architecture in cloud computing requires the integration of complex practices on multiple layers of system architecture, as well as a combination of a variety of existing technologies. This paper focuses on authentication mechanisms, calculation of trust score, and generation of policies in order to establish required access control to resources. The main objective is to incorporate an unbiased trust score as a part of policy expressions while preserving the configurability and adaptiveness of parameters of interest. Finally, the proof-of-concept is demonstrated on a micro-cloud plat-form solution.</li>
</ul>

<h3>Title: SCGNet-Stacked Convolution with Gated Recurrent Unit Network for Cyber Network Intrusion Detection and Intrusion Type Classification</h3>
<ul>
<li><strong>Authors: </strong>Rajana Akter, Shahnure Rabib, Rahul Deb Mohalder, Laboni Paul, Ferdous Bin Ali</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21873">https://arxiv.org/abs/2410.21873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21873">https://arxiv.org/pdf/2410.21873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21873]] SCGNet-Stacked Convolution with Gated Recurrent Unit Network for Cyber Network Intrusion Detection and Intrusion Type Classification(https://arxiv.org/abs/2410.21873)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>Intrusion detection system (IDS) is a piece of hardware or software that looks for malicious activity or policy violations in a network. It looks for malicious activity or security flaws on a network or system. IDS protects hosts or networks by looking for indications of known attacks or deviations from normal behavior (Network-based intrusion detection system, or NIDS for short). Due to the rapidly increasing amount of network data, traditional intrusion detection systems (IDSs) are far from being able to quickly and efficiently identify complex and varied network attacks, especially those linked to low-frequency attacks. The SCGNet (Stacked Convolution with Gated Recurrent Unit Network) is a novel deep learning architecture that we propose in this study. It exhibits promising results on the NSL-KDD dataset in both task, network attack detection, and attack type classification with 99.76% and 98.92% accuracy, respectively. We have also introduced a general data preprocessing pipeline that is easily applicable to other similar datasets. We have also experimented with conventional machine-learning techniques to evaluate the performance of the data processing pipeline.</li>
</ul>

<h3>Title: Self-Relaxed Joint Training: Sample Selection for Severity Estimation with Ordinal Noisy Labels</h3>
<ul>
<li><strong>Authors: </strong>Shumpei Takezaki, Kiyohito Tanaka, Seiichi Uchida</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21885">https://arxiv.org/abs/2410.21885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21885">https://arxiv.org/pdf/2410.21885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21885]] Self-Relaxed Joint Training: Sample Selection for Severity Estimation with Ordinal Noisy Labels(https://arxiv.org/abs/2410.21885)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Severity level estimation is a crucial task in medical image diagnosis. However, accurately assigning severity class labels to individual images is very costly and challenging. Consequently, the attached labels tend to be noisy. In this paper, we propose a new framework for training with ``ordinal'' noisy labels. Since severity levels have an ordinal relationship, we can leverage this to train a classifier while mitigating the negative effects of noisy labels. Our framework uses two techniques: clean sample selection and dual-network architecture. A technical highlight of our approach is the use of soft labels derived from noisy hard labels. By appropriately using the soft and hard labels in the two techniques, we achieve more accurate sample selection and robust network training. The proposed method outperforms various state-of-the-art methods in experiments using two endoscopic ulcerative colitis (UC) datasets and a retinal Diabetic Retinopathy (DR) dataset. Our codes are available at this https URL.</li>
</ul>

<h3>Title: Evaluating K-Fold Cross Validation for Transformer Based Symbolic Regression Models</h3>
<ul>
<li><strong>Authors: </strong>Kaustubh Kislay, Shlok Singh, Soham Joshi, Rohan Dutta, Jay Shim George Flint, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21896">https://arxiv.org/abs/2410.21896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21896">https://arxiv.org/pdf/2410.21896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21896]] Evaluating K-Fold Cross Validation for Transformer Based Symbolic Regression Models(https://arxiv.org/abs/2410.21896)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Symbolic Regression remains an NP-Hard problem, with extensive research focusing on AI models for this task. Transformer models have shown promise in Symbolic Regression, but performance suffers with smaller datasets. We propose applying k-fold cross-validation to a transformer-based symbolic regression model trained on a significantly reduced dataset (15,000 data points, down from 500,000). This technique partitions the training data into multiple subsets (folds), iteratively training on some while validating on others. Our aim is to provide an estimate of model generalization and mitigate overfitting issues associated with smaller datasets. Results show that this process improves the model's output consistency and generalization by a relative improvement in validation loss of 53.31%. Potentially enabling more efficient and accessible symbolic regression in resource-constrained environments.</li>
</ul>

<h3>Title: Multi-step feature fusion for natural disaster damage assessment on satellite images</h3>
<ul>
<li><strong>Authors: </strong>Mateusz ≈ªarski, Jaros≈Çaw Adam Miszczak</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21901">https://arxiv.org/abs/2410.21901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21901">https://arxiv.org/pdf/2410.21901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21901]] Multi-step feature fusion for natural disaster damage assessment on satellite images(https://arxiv.org/abs/2410.21901)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Quick and accurate assessment of the damage state of buildings after natural disasters is crucial for undertaking properly targeted rescue and subsequent recovery operations, which can have a major impact on the safety of victims and the cost of disaster recovery. The quality of such a process can be significantly improved by harnessing the potential of machine learning methods in computer vision. This paper presents a novel damage assessment method using an original multi-step feature fusion network for the classification of the damage state of buildings based on pre- and post-disaster large-scale satellite images. We introduce a novel convolutional neural network (CNN) module that performs feature fusion at multiple network levels between pre- and post-disaster images in the horizontal and vertical directions of CNN network. An additional network element - Fuse Module - was proposed to adapt any CNN model to analyze image pairs in the issue of pair classification. We use, open, large-scale datasets (IDA-BD and xView2) to verify, that the proposed method is suitable to improve on existing state-of-the-art architectures. We report over a 3 percentage point increase in the accuracy of the Vision Transformer model.</li>
</ul>

<h3>Title: SceneGenAgent: Precise Industrial Scene Generation with Coding Agent</h3>
<ul>
<li><strong>Authors: </strong>Xiao Xia, Dan Zhang, Zibo Liao, Zhenyu Hou, Tianrui Sun, Jing Li, Ling Fu, Yuxiao Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21909">https://arxiv.org/abs/2410.21909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21909">https://arxiv.org/pdf/2410.21909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21909]] SceneGenAgent: Precise Industrial Scene Generation with Coding Agent(https://arxiv.org/abs/2410.21909)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The modeling of industrial scenes is essential for simulations in industrial manufacturing. While large language models (LLMs) have shown significant progress in generating general 3D scenes from textual descriptions, generating industrial scenes with LLMs poses a unique challenge due to their demand for precise measurements and positioning, requiring complex planning over spatial arrangement. To address this challenge, we introduce SceneGenAgent, an LLM-based agent for generating industrial scenes through C# code. SceneGenAgent ensures precise layout planning through a structured and calculable format, layout verification, and iterative refinement to meet the quantitative requirements of industrial scenarios. Experiment results demonstrate that LLMs powered by SceneGenAgent exceed their original performance, reaching up to 81.0% success rate in real-world industrial scene generation tasks and effectively meeting most scene generation requirements. To further enhance accessibility, we construct SceneInstruct, a dataset designed for fine-tuning open-source LLMs to integrate into SceneGenAgent. Experiments show that fine-tuning open-source LLMs on SceneInstruct yields significant performance improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our code and data are available at this https URL .</li>
</ul>

<h3>Title: LogSHIELD: A Graph-based Real-time Anomaly Detection Framework using Frequency Analysis</h3>
<ul>
<li><strong>Authors: </strong>Krishna Chandra Roy, Qian Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21936">https://arxiv.org/abs/2410.21936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21936">https://arxiv.org/pdf/2410.21936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21936]] LogSHIELD: A Graph-based Real-time Anomaly Detection Framework using Frequency Analysis(https://arxiv.org/abs/2410.21936)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Anomaly-based cyber threat detection using deep learning is on a constant growth in popularity for novel cyber-attack detection and forensics. A robust, efficient, and real-time threat detector in a large-scale operational enterprise network requires high accuracy, high fidelity, and a high throughput model to detect malicious activities. Traditional anomaly-based detection models, however, suffer from high computational overhead and low detection accuracy, making them unsuitable for real-time threat detection. In this work, we propose LogSHIELD, a highly effective graph-based anomaly detection model in host data. We present a real-time threat detection approach using frequency-domain analysis of provenance graphs. To demonstrate the significance of graph-based frequency analysis we proposed two approaches. Approach-I uses a Graph Neural Network (GNN) LogGNN and approach-II performs frequency domain analysis on graph node samples for graph embedding. Both approaches use a statistical clustering algorithm for anomaly detection. The proposed models are evaluated using a large host log dataset consisting of 774M benign logs and 375K malware logs. LogSHIELD explores the provenance graph to extract contextual and causal relationships among logs, exposing abnormal activities. It can detect stealthy and sophisticated attacks with over 98% average AUC and F1 scores. It significantly improves throughput, achieves an average detection latency of 0.13 seconds, and outperforms state-of-the-art models in detection time.</li>
</ul>

<h3>Title: Benchmarking OpenAI o1 in Cyber Security</h3>
<ul>
<li><strong>Authors: </strong>Dan Ristea, Vasilios Mavroudis, Chris Hicks</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21939">https://arxiv.org/abs/2410.21939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21939">https://arxiv.org/pdf/2410.21939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21939]] Benchmarking OpenAI o1 in Cyber Security(https://arxiv.org/abs/2410.21939)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>We evaluate OpenAI's o1-preview and o1-mini models, benchmarking their performance against the earlier GPT-4o model. Our evaluation focuses on their ability to detect vulnerabilities in real-world software by generating structured inputs that trigger known sanitizers. Using DARPA's AI Cyber Challenge (AIxCC) framework and the Nginx challenge project--a deliberately modified version of the widely-used Nginx web server--we create a well-defined yet complex environment for testing LLMs on automated vulnerability detection (AVD) tasks. Our results show that the o1-preview model significantly outperforms GPT-4o in both success rate and efficiency, especially in more complex scenarios.</li>
</ul>

<h3>Title: Human-Readable Programs as Actors of Reinforcement Learning Agents Using Critic-Moderated Evolution</h3>
<ul>
<li><strong>Authors: </strong>Senne Deproost, Denis Steckelmacher, Ann Now√©</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21940">https://arxiv.org/abs/2410.21940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21940">https://arxiv.org/pdf/2410.21940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21940]] Human-Readable Programs as Actors of Reinforcement Learning Agents Using Critic-Moderated Evolution(https://arxiv.org/abs/2410.21940)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>With Deep Reinforcement Learning (DRL) being increasingly considered for the control of real-world systems, the lack of transparency of the neural network at the core of RL becomes a concern. Programmatic Reinforcement Learning (PRL) is able to to create representations of this black-box in the form of source code, not only increasing the explainability of the controller but also allowing for user adaptations. However, these methods focus on distilling a black-box policy into a program and do so after learning using the Mean Squared Error between produced and wanted behaviour, discarding other elements of the RL algorithm. The distilled policy may therefore perform significantly worse than the black-box learned policy. In this paper, we propose to directly learn a program as the policy of an RL agent. We build on TD3 and use its critics as the basis of the objective function of a genetic algorithm that syntheses the program. Our approach builds the program during training, as opposed to after the fact. This steers the program to actual high rewards, instead of a simple Mean Squared Error. Also, our approach leverages the TD3 critics to achieve high sample-efficiency, as opposed to pure genetic methods that rely on Monte-Carlo evaluations. Our experiments demonstrate the validity, explainability and sample-efficiency of our approach in a simple gridworld environment.</li>
</ul>

<h3>Title: Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications</h3>
<ul>
<li><strong>Authors: </strong>Monica Riedler, Stefan Langer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21943">https://arxiv.org/abs/2410.21943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21943">https://arxiv.org/pdf/2410.21943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21943]] Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications(https://arxiv.org/abs/2410.21943)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive capabilities in answering questions, but they lack domain-specific knowledge and are prone to hallucinations. Retrieval Augmented Generation (RAG) is one approach to address these challenges, while multimodal models are emerging as promising AI assistants for processing both text and images. In this paper we describe a series of experiments aimed at determining how to best integrate multimodal models into RAG systems for the industrial domain. The purpose of the experiments is to determine whether including images alongside text from documents within the industrial domain increases RAG performance and to find the optimal configuration for such a multimodal RAG system. Our experiments include two approaches for image processing and retrieval, as well as two LLMs (GPT4-Vision and LLaVA) for answer synthesis. These image processing strategies involve the use of multimodal embeddings and the generation of textual summaries from images. We evaluate our experiments with an LLM-as-a-Judge approach. Our results reveal that multimodal RAG can outperform single-modality RAG settings, although image retrieval poses a greater challenge than text retrieval. Additionally, leveraging textual summaries from images presents a more promising approach compared to the use of multimodal embeddings, providing more opportunities for future advancements.</li>
</ul>

<h3>Title: On the Robustness of Adversarial Training Against Uncertainty Attacks</h3>
<ul>
<li><strong>Authors: </strong>Emanuele Ledda, Giovanni Scodeller, Daniele Angioni, Giorgio Piras, Antonio Emanuele Cin√†, Giorgio Fumera, Battista Biggio, Fabio Roli</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21952">https://arxiv.org/abs/2410.21952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21952">https://arxiv.org/pdf/2410.21952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21952]] On the Robustness of Adversarial Training Against Uncertainty Attacks(https://arxiv.org/abs/2410.21952)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>In learning problems, the noise inherent to the task at hand hinders the possibility to infer without a certain degree of uncertainty. Quantifying this uncertainty, regardless of its wide use, assumes high relevance for security-sensitive applications. Within these scenarios, it becomes fundamental to guarantee good (i.e., trustworthy) uncertainty measures, which downstream modules can securely employ to drive the final decision-making process. However, an attacker may be interested in forcing the system to produce either (i) highly uncertain outputs jeopardizing the system's availability or (ii) low uncertainty estimates, making the system accept uncertain samples that would instead require a careful inspection (e.g., human intervention). Therefore, it becomes fundamental to understand how to obtain robust uncertainty estimates against these kinds of attacks. In this work, we reveal both empirically and theoretically that defending against adversarial examples, i.e., carefully perturbed samples that cause misclassification, additionally guarantees a more secure, trustworthy uncertainty estimate under common attack scenarios without the need for an ad-hoc defense strategy. To support our claims, we evaluate multiple adversarial-robust models from the publicly available benchmark RobustBench on the CIFAR-10 and ImageNet datasets.</li>
</ul>

<h3>Title: Spatio-temporal Transformers for Action Unit Classification with Event Cameras</h3>
<ul>
<li><strong>Authors: </strong>Luca Cultrera, Federico Becattini, Lorenzo Berlincioni, Claudio Ferrari, Alberto Del Bimbo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21958">https://arxiv.org/abs/2410.21958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21958">https://arxiv.org/pdf/2410.21958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21958]] Spatio-temporal Transformers for Action Unit Classification with Event Cameras(https://arxiv.org/abs/2410.21958)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Face analysis has been studied from different angles to infer emotion, poses, shapes, and landmarks. Traditionally RGB cameras are used, yet for fine-grained tasks standard sensors might not be up to the task due to their latency, making it impossible to record and detect micro-movements that carry a highly informative signal, which is necessary for inferring the true emotions of a subject. Event cameras have been increasingly gaining interest as a possible solution to this and similar high-frame rate tasks. We propose a novel spatiotemporal Vision Transformer model that uses Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA) to enhance the accuracy of Action Unit classification from event streams. We also address the lack of labeled event data in the literature, which can be considered one of the main causes of an existing gap between the maturity of RGB and neuromorphic vision models. Gathering data is harder in the event domain since it cannot be crawled from the web and labeling frames should take into account event aggregation rates and the fact that static parts might not be visible in certain frames. To this end, we present FACEMORPHIC, a temporally synchronized multimodal face dataset composed of RGB videos and event streams. The dataset is annotated at a video level with facial Action Units and contains streams collected with various possible applications, ranging from 3D shape estimation to lip-reading. We then show how temporal synchronization can allow effective neuromorphic face analysis without the need to manually annotate videos: we instead leverage cross-modal supervision bridging the domain gap by representing face shapes in a 3D space. Our proposed model outperforms baseline methods by effectively capturing spatial and temporal information, crucial for recognizing subtle facial micro-expressions.</li>
</ul>

<h3>Title: FakeFormer: Efficient Vulnerability-Driven Transformers for Generalisable Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Dat Nguyen, Marcella Astrid, Enjie Ghorbel, Djamila Aouada</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21964">https://arxiv.org/abs/2410.21964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21964">https://arxiv.org/pdf/2410.21964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21964]] FakeFormer: Efficient Vulnerability-Driven Transformers for Generalisable Deepfake Detection(https://arxiv.org/abs/2410.21964)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Recently, Vision Transformers (ViTs) have achieved unprecedented effectiveness in the general domain of image classification. Nonetheless, these models remain underexplored in the field of deepfake detection, given their lower performance as compared to Convolution Neural Networks (CNNs) in that specific context. In this paper, we start by investigating why plain ViT architectures exhibit a suboptimal performance when dealing with the detection of facial forgeries. Our analysis reveals that, as compared to CNNs, ViT struggles to model localized forgery artifacts that typically characterize deepfakes. Based on this observation, we propose a deepfake detection framework called FakeFormer, which extends ViTs to enforce the extraction of subtle inconsistency-prone information. For that purpose, an explicit attention learning guided by artifact-vulnerable patches and tailored to ViTs is introduced. Extensive experiments are conducted on diverse well-known datasets, including FF++, Celeb-DF, WildDeepfake, DFD, DFDCP, and DFDC. The results show that FakeFormer outperforms the state-of-the-art in terms of generalization and computational cost, without the need for large-scale training datasets. The code is available at \url{this https URL}.</li>
</ul>

<h3>Title: SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types</h3>
<ul>
<li><strong>Authors: </strong>Yutao Mou, Shikun Zhang, Wei Ye</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21965">https://arxiv.org/abs/2410.21965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21965">https://arxiv.org/pdf/2410.21965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21965]] SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types(https://arxiv.org/abs/2410.21965)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Ensuring the safety of large language model (LLM) applications is essential for developing trustworthy artificial intelligence. Current LLM safety benchmarks have two limitations. First, they focus solely on either discriminative or generative evaluation paradigms while ignoring their interconnection. Second, they rely on standardized inputs, overlooking the effects of widespread prompting techniques, such as system prompts, few-shot demonstrations, and chain-of-thought prompting. To overcome these issues, we developed SG-Bench, a novel benchmark to assess the generalization of LLM safety across various tasks and prompt types. This benchmark integrates both generative and discriminative evaluation tasks and includes extended data to examine the impact of prompt engineering and jailbreak on LLM safety. Our assessment of 3 advanced proprietary LLMs and 10 open-source LLMs with the benchmark reveals that most LLMs perform worse on discriminative tasks than generative ones, and are highly susceptible to prompts, indicating poor generalization in safety alignment. We also explain these findings quantitatively and qualitatively to provide insights for future research.</li>
</ul>

<h3>Title: PrefPaint: Aligning Image Inpainting Diffusion Model with Human Preference</h3>
<ul>
<li><strong>Authors: </strong>Kendong Liu, Zhiyu Zhu, Chuanhao Li, Hui Liu, Huanqiang Zeng, Junhui Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21966">https://arxiv.org/abs/2410.21966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21966">https://arxiv.org/pdf/2410.21966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21966]] PrefPaint: Aligning Image Inpainting Diffusion Model with Human Preference(https://arxiv.org/abs/2410.21966)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we make the first attempt to align diffusion models for image inpainting with human aesthetic standards via a reinforcement learning framework, significantly improving the quality and visual appeal of inpainted images. Specifically, instead of directly measuring the divergence with paired images, we train a reward model with the dataset we construct, consisting of nearly 51,000 images annotated with human preferences. Then, we adopt a reinforcement learning process to fine-tune the distribution of a pre-trained diffusion model for image inpainting in the direction of higher reward. Moreover, we theoretically deduce the upper bound on the error of the reward model, which illustrates the potential confidence of reward estimation throughout the reinforcement alignment process, thereby facilitating accurate regularization. Extensive experiments on inpainting comparison and downstream tasks, such as image extension and 3D reconstruction, demonstrate the effectiveness of our approach, showing significant improvements in the alignment of inpainted images with human preference compared with state-of-the-art methods. This research not only advances the field of image inpainting but also provides a framework for incorporating human preference into the iterative refinement of generative models based on modeling reward accuracy, with broad implications for the design of visually driven AI applications. Our code and dataset are publicly available at this https URL.</li>
</ul>

<h3>Title: Automated Vulnerability Detection Using Deep Learning Technique</h3>
<ul>
<li><strong>Authors: </strong>Guan-Yan Yang, Yi-Heng Ko, Farn Wang, Kuo-Hui Yeh, Haw-Shiang Chang, Hsueh-Yi Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21968">https://arxiv.org/abs/2410.21968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21968">https://arxiv.org/pdf/2410.21968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21968]] Automated Vulnerability Detection Using Deep Learning Technique(https://arxiv.org/abs/2410.21968)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Our work explores the utilization of deep learning, specifically leveraging the CodeBERT model, to enhance code security testing for Python applications by detecting SQL injection vulnerabilities. Unlike traditional security testing methods that may be slow and error-prone, our approach transforms source code into vector representations and trains a Long Short-Term Memory (LSTM) model to identify vulnerable patterns. When compared with existing static application security testing (SAST) tools, our model displays superior performance, achieving higher precision, recall, and F1-score. The study demonstrates that deep learning techniques, particularly with CodeBERT's advanced contextual understanding, can significantly improve vulnerability detection, presenting a scalable methodology applicable to various programming languages and vulnerability types.</li>
</ul>

<h3>Title: BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhou, Tan Li Hui Faith, Yanyu Xu, Sicong Leng, Xinxing Xu, Yong Liu, Rick Siow Mong Goh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21969">https://arxiv.org/abs/2410.21969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21969">https://arxiv.org/pdf/2410.21969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21969]] BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays(https://arxiv.org/abs/2410.21969)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports. MedVLP can provide useful features to downstream tasks and facilitate adapting task-specific models to new setups using fewer examples. However, existing MedVLP methods often differ in terms of datasets, preprocessing, and finetuning implementations. This pose great challenges in evaluating how well a MedVLP method generalizes to various clinically-relevant tasks due to the lack of unified, standardized, and comprehensive benchmark. To fill this gap, we propose BenchX, a unified benchmark framework that enables head-to-head comparison and systematical analysis between MedVLP methods using public chest X-ray datasets. Specifically, BenchX is composed of three components: 1) Comprehensive datasets covering nine datasets and four medical tasks; 2) Benchmark suites to standardize data preprocessing, train-test splits, and parameter selection; 3) Unified finetuning protocols that accommodate heterogeneous MedVLP methods for consistent task adaptation in classification, segmentation, and report generation, respectively. Utilizing BenchX, we establish baselines for nine state-of-the-art MedVLP methods and found that the performance of some early MedVLP methods can be enhanced to surpass more recent ones, prompting a revisiting of the developments and conclusions from prior works in MedVLP. Our code are available at this https URL.</li>
</ul>

<h3>Title: Not All Languages are Equal: Insights into Multilingual Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Suhang Wu, Jialong Tang, Baosong Yang, Ante Wang, Kaidi Jia, Jiawei Yu, Junfeng Yao, Jinsong Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21970">https://arxiv.org/abs/2410.21970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21970">https://arxiv.org/pdf/2410.21970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21970]] Not All Languages are Equal: Insights into Multilingual Retrieval-Augmented Generation(https://arxiv.org/abs/2410.21970)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>RALMs (Retrieval-Augmented Language Models) broaden their knowledge scope by incorporating external textual resources. However, the multilingual nature of global knowledge necessitates RALMs to handle diverse languages, a topic that has received limited research focus. In this work, we propose \textit{Futurepedia}, a carefully crafted benchmark containing parallel texts across eight representative languages. We evaluate six multilingual RALMs using our benchmark to explore the challenges of multilingual RALMs. Experimental results reveal linguistic inequalities: 1) high-resource languages stand out in Monolingual Knowledge Extraction; 2) Indo-European languages lead RALMs to provide answers directly from documents, alleviating the challenge of expressing answers across languages; 3) English benefits from RALMs' selection bias and speaks louder in multilingual knowledge selection. Based on these findings, we offer advice for improving multilingual Retrieval Augmented Generation. For monolingual knowledge extraction, careful attention must be paid to cascading errors from translating low-resource languages into high-resource ones. In cross-lingual knowledge transfer, encouraging RALMs to provide answers within documents in different languages can improve transfer performance. For multilingual knowledge selection, incorporating more non-English documents and repositioning English documents can help mitigate RALMs' selection bias. Through comprehensive experiments, we underscore the complexities inherent in multilingual RALMs and offer valuable insights for future research.</li>
</ul>

<h3>Title: VaultFS: Write-once Software Support at the File System Level Against Ransomware Attacks</h3>
<ul>
<li><strong>Authors: </strong>Pasquale Caporaso, Giuseppe Bianchi, Francesco Quaglia</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21979">https://arxiv.org/abs/2410.21979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21979">https://arxiv.org/pdf/2410.21979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21979]] VaultFS: Write-once Software Support at the File System Level Against Ransomware Attacks(https://arxiv.org/abs/2410.21979)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>The demand for data protection measures against unauthorized changes or deletions is steadily increasing. These measures are essential for maintaining the integrity and accessibility of data, effectively guarding against threats like ransomware attacks that focus on encrypting large volumes of stored data, as well as insider threats that involve tampering with or erasing system and access logs. Such protection measures have become crucial in today's landscape, and hardware-based solutions like Write-Once Read-Many (WORM) storage devices, have been put forth as viable options, which however impose hardware-level investments, and the impossibility to reuse the blocks of the storage devices after they have been written. In this article we propose VaultFS, a Linux-suited file system oriented to the maintenance of cold-data, namely data that are written using a common file system interface, are kept accessible, but are not modifiable, even by threads running with (effective)root-id. Essentially, these files are supported via the write-once semantic, and cannot be subject to the rewriting (or deletion) of their content up to the end of their (potentially infinite) protection life time. Hence they cannot be subject to ransomware attacks even under privilege escalation. This takes place with no need for any underlying WORM device -- since ValutFS is a pure software solution working with common read/write devices (e.g., hard disks and SSD). Also, VaultFS offers the possibility to protect the storage against Denial-of-Service (DOS) attacks, possibly caused by un-trusted applications that simply write on the file system to make its device blocks busy with non-removable content.</li>
</ul>

<h3>Title: A Survey on RGB, 3D, and Multimodal Approaches for Unsupervised Industrial Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Lin, Yang Chang, Xuan Tong, Jiawen Yu, Antonio Liotta, Guofan Huang, Wei Song, Deyu Zeng, Zongze Wu, Yan Wang, Wenqiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21982">https://arxiv.org/abs/2410.21982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21982">https://arxiv.org/pdf/2410.21982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21982]] A Survey on RGB, 3D, and Multimodal Approaches for Unsupervised Industrial Anomaly Detection(https://arxiv.org/abs/2410.21982)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the advancement of industrial informatization, Unsupervised Industrial Anomaly Detection (UIAD) technology effectively overcomes the scarcity of abnormal samples and significantly enhances the automation and reliability of smart manufacturing. While RGB, 3D, and multimodal anomaly detection have demonstrated comprehensive and robust capabilities within the industrial informatization sector, existing reviews on industrial anomaly detection have not sufficiently classified and discussed methods in 3D and multimodal settings. We focus on 3D UIAD and multimodal UIAD, providing a comprehensive summary of unsupervised industrial anomaly detection in three modal settings. Firstly, we compare our surveys with recent works, introducing commonly used datasets, evaluation metrics, and the definitions of anomaly detection problems. Secondly, we summarize five research paradigms in RGB, 3D and multimodal UIAD and three emerging industrial manufacturing optimization directions in RGB UIAD, and review three multimodal feature fusion strategies in multimodal settings. Finally, we outline the primary challenges currently faced by UIAD in three modal settings, and offer insights into future development directions, aiming to provide researchers with a thorough reference and offer new perspectives for the advancement of industrial informatization. Corresponding resources are available at this https URL.</li>
</ul>

<h3>Title: ReDAN: An Empirical Study on Remote DoS Attacks against NAT Networks</h3>
<ul>
<li><strong>Authors: </strong>Xuewei Feng, Yuxiang Yang, Qi Li, xingxiang Zhan, Kun Sun, Ziqiang Wang, Ao Wang, Ganqiu Du, Ke Xu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21984">https://arxiv.org/abs/2410.21984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21984">https://arxiv.org/pdf/2410.21984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21984]] ReDAN: An Empirical Study on Remote DoS Attacks against NAT Networks(https://arxiv.org/abs/2410.21984)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>In this paper, we conduct an empirical study on remote DoS attacks targeting NAT networks. We show that Internet attackers operating outside local NAT networks can remotely identify a NAT device and subsequently terminate TCP connections initiated from the identified NAT device to external servers. Our attack involves two steps. First, we identify NAT devices on the Internet by exploiting inadequacies in the PMTUD mechanism within NAT specifications. This deficiency creates a fundamental side channel that allows Internet attackers to distinguish if a public IPv4 address serves a NAT device or a separate IP host, aiding in the identification of target NAT devices. Second, we launch a remote DoS attack to terminate TCP connections on the identified NAT devices. While recent NAT implementations may include protective measures, such as packet legitimacy validation to prevent malicious manipulations on NAT mappings, we discover that these safeguards are not widely adopted in real world. Consequently, attackers can send crafted packets to deceive NAT devices into erroneously removing innocent TCP connection mappings, thereby disrupting the NATed clients to access remote TCP servers. Our experimental results reveal widespread security vulnerabilities in existing NAT devices. After testing 8 types of router firmware and 30 commercial NAT devices from 14 vendors, we identify vulnerabilities in 6 firmware types and 29 NAT devices. Moreover, our measurements reveal a stark reality: 166 out of 180 (over 92%) tested real-world NAT networks, comprising 90 4G LTE/5G networks, 60 public Wi-Fi networks, and 30 cloud VPS networks, are susceptible to exploitation. We responsibly disclosed the vulnerabilities to affected vendors and received a significant number of acknowledgments. Finally, we propose our countermeasures against the identified DoS attack.</li>
</ul>

<h3>Title: From 5G to 6G: A Survey on Security, Privacy, and Standardization Pathways</h3>
<ul>
<li><strong>Authors: </strong>Mengmeng Yang, Youyang Qu, Thilina Ranbaduge, Chandra Thapa, Nazatul Sultan, Ming Ding, Hajime Suzuki, Wei Ni, Sharif Abuadbba, David Smith, Paul Tyler, Josef Pieprzyk, Thierry Rakotoarivelo, Xinlong Guan, Sirine M'rabet</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21986">https://arxiv.org/abs/2410.21986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21986">https://arxiv.org/pdf/2410.21986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21986]] From 5G to 6G: A Survey on Security, Privacy, and Standardization Pathways(https://arxiv.org/abs/2410.21986)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, robust</a></li>
<li><strong>Abstract: </strong>The vision for 6G aims to enhance network capabilities with faster data rates, near-zero latency, and higher capacity, supporting more connected devices and seamless experiences within an intelligent digital ecosystem where artificial intelligence (AI) plays a crucial role in network management and data analysis. This advancement seeks to enable immersive mixed-reality experiences, holographic communications, and smart city infrastructures. However, the expansion of 6G raises critical security and privacy concerns, such as unauthorized access and data breaches. This is due to the increased integration of IoT devices, edge computing, and AI-driven analytics. This paper provides a comprehensive overview of 6G protocols, focusing on security and privacy, identifying risks, and presenting mitigation strategies. The survey examines current risk assessment frameworks and advocates for tailored 6G solutions. We further discuss industry visions, government projects, and standardization efforts to balance technological innovation with robust security and privacy measures.</li>
</ul>

<h3>Title: From Explicit Rules to Implicit Reasoning in an Interpretable Violence Monitoring System</h3>
<ul>
<li><strong>Authors: </strong>Wen-Dong Jiang, Chih-Yung Chang, Hsiang-Chuan Chang, Diptendu Sinha Roy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21991">https://arxiv.org/abs/2410.21991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21991">https://arxiv.org/pdf/2410.21991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21991]] From Explicit Rules to Implicit Reasoning in an Interpretable Violence Monitoring System(https://arxiv.org/abs/2410.21991)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Recently, research based on pre-trained models has demonstrated outstanding performance in violence surveillance tasks. However, these black-box systems face challenges regarding explainability during training and inference processes. An important question is how to incorporate explicit knowledge into these implicit models, thereby designing expert-driven and interpretable violence surveillance systems. This paper proposes a new paradigm for weakly supervised violence monitoring (WSVM) called Rule base Violence monitoring (RuleVM). The proposed RuleVM uses a dual-branch structure for different designs for images and text. One of the branches is called the implicit branch, which uses only visual features for coarse-grained binary classification. In this branch, image feature extraction is divided into two channels: one responsible for extracting scene frames and the other focusing on extracting actions. The other branch is called the explicit branch, which utilizes language-image alignment to perform fine-grained classification. For the language channel design in the explicit branch, the proposed RuleCLIP uses the state-of-the-art YOLO-World model to detect objects and actions in video frames, and association rules are identified through data mining methods as descriptions of the video. Leveraging the dual?branch architecture, RuleVM achieves interpretable coarse?grained and fine-grained violence surveillance. Extensive experiments were conducted on two commonly used benchmarks, and the results show that RuleCLIP achieved the best performance in both coarse-grained and fine-grained detection, significantly outperforming existing state-of-the-art methods. Moreover, interpretability experiments uncovered some interesting rules, such as the observation that as the number of people increases, the risk level of violent behavior also rises.</li>
</ul>

<h3>Title: A Machine Learning-Based Secure Face Verification Scheme and Its Applications to Digital Surveillance</h3>
<ul>
<li><strong>Authors: </strong>Huan-Chih Wang, Ja-Ling Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.21993">https://arxiv.org/abs/2410.21993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.21993">https://arxiv.org/pdf/2410.21993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.21993]] A Machine Learning-Based Secure Face Verification Scheme and Its Applications to Digital Surveillance(https://arxiv.org/abs/2410.21993)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect</a></li>
<li><strong>Abstract: </strong>Face verification is a well-known image analysis application and is widely used to recognize individuals in contemporary society. However, most real-world recognition systems ignore the importance of protecting the identity-sensitive facial images that are used for verification. To address this problem, we investigate how to implement a secure face verification system that protects the facial images from being imitated. In our work, we use the DeepID2 convolutional neural network to extract the features of a facial image and an EM algorithm to solve the facial verification problem. To maintain the privacy of facial images, we apply homomorphic encryption schemes to encrypt the facial data and compute the EM algorithm in the ciphertext domain. We develop three face verification systems for surveillance (or entrance) control of a local community based on three levels of privacy concerns. The associated timing performances are presented to demonstrate their feasibility for practical implementation.</li>
</ul>

<h3>Title: Feature distribution Adaptation Network for Speech Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Shaokai Li, Yixuan Ji, Peng Song, Haoqin Sun, Wenming Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22023">https://arxiv.org/abs/2410.22023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22023">https://arxiv.org/pdf/2410.22023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22023]] Feature distribution Adaptation Network for Speech Emotion Recognition(https://arxiv.org/abs/2410.22023)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a novel deep inductive transfer learning framework, named feature distribution adaptation network, to tackle the challenging multi-modal speech emotion recognition problem. Our method aims to use deep transfer learning strategies to align visual and audio feature distributions to obtain consistent representation of emotion, thereby improving the performance of speech emotion recognition. In our model, the pre-trained ResNet-34 is utilized for feature extraction for facial expression images and acoustic Mel spectrograms, respectively. Then, the cross-attention mechanism is introduced to model the intrinsic similarity relationships of multi-modal features. Finally, the multi-modal feature distribution adaptation is performed efficiently with feed-forward network, which is extended using the local maximum mean discrepancy loss. Experiments are carried out on two benchmark datasets, and the results demonstrate that our model can achieve excellent performance compared with existing this http URL code is available at this https URL.</li>
</ul>

<h3>Title: Benchmarking Human and Automated Prompting in the Segment Anything Model</h3>
<ul>
<li><strong>Authors: </strong>Jorge Quesada, Zoe Fowler, Mohammad Alotaibi, Mohit Prabhushankar, Ghassan AlRegib</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22048">https://arxiv.org/abs/2410.22048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22048">https://arxiv.org/pdf/2410.22048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22048]] Benchmarking Human and Automated Prompting in the Segment Anything Model(https://arxiv.org/abs/2410.22048)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>The remarkable capabilities of the Segment Anything Model (SAM) for tackling image segmentation tasks in an intuitive and interactive manner has sparked interest in the design of effective visual prompts. Such interest has led to the creation of automated point prompt selection strategies, typically motivated from a feature extraction perspective. However, there is still very little understanding of how appropriate these automated visual prompting strategies are, particularly when compared to humans, across diverse image domains. Additionally, the performance benefits of including such automated visual prompting strategies within the finetuning process of SAM also remains unexplored, as does the effect of interpretable factors like distance between the prompt points on segmentation performance. To bridge these gaps, we leverage a recently released visual prompting dataset, PointPrompt, and introduce a number of benchmarking tasks that provide an array of opportunities to improve the understanding of the way human prompts differ from automated ones and what underlying factors make for effective visual prompts. We demonstrate that the resulting segmentation scores obtained by humans are approximately 29% higher than those given by automated strategies and identify potential features that are indicative of prompting performance with $R^2$ scores over 0.5. Additionally, we demonstrate that performance when using automated methods can be improved by up to 68% via a finetuning approach. Overall, our experiments not only showcase the existing gap between human prompts and automated methods, but also highlight potential avenues through which this gap can be leveraged to improve effective visual prompt design. Further details along with the dataset links and codes are available at this https URL</li>
</ul>

<h3>Title: Distinguishing Ignorance from Error in LLM Hallucinations</h3>
<ul>
<li><strong>Authors: </strong>Adi Simhi, Jonathan Herzig, Idan Szpektor, Yonatan Belinkov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22071">https://arxiv.org/abs/2410.22071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22071">https://arxiv.org/pdf/2410.22071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22071]] Distinguishing Ignorance from Error in LLM Hallucinations(https://arxiv.org/abs/2410.22071)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are susceptible to hallucinations-outputs that are ungrounded, factually incorrect, or inconsistent with prior generations. We focus on close-book Question Answering (CBQA), where previous work has not fully addressed the distinction between two possible kinds of hallucinations, namely, whether the model (1) does not hold the correct answer in its parameters or (2) answers incorrectly despite having the required knowledge. We argue that distinguishing these cases is crucial for detecting and mitigating hallucinations. Specifically, case (2) may be mitigated by intervening in the model's internal computation, as the knowledge resides within the model's parameters. In contrast, in case (1) there is no parametric knowledge to leverage for mitigation, so it should be addressed by resorting to an external knowledge source or abstaining. To help distinguish between the two cases, we introduce Wrong Answer despite having Correct Knowledge (WACK), an approach for constructing model-specific datasets for the second hallucination type. Our probing experiments indicate that the two kinds of hallucinations are represented differently in the model's inner states. Next, we show that datasets constructed using WACK exhibit variations across models, demonstrating that even when models share knowledge of certain facts, they still vary in the specific examples that lead to hallucinations. Finally, we show that training a probe on our WACK datasets leads to better hallucination detection of case (2) hallucinations than using the common generic one-size-fits-all datasets. The code is available at this https URL .</li>
</ul>

<h3>Title: HRPVT: High-Resolution Pyramid Vision Transformer for medium and small-scale human pose estimation</h3>
<ul>
<li><strong>Authors: </strong>Zhoujie Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22079">https://arxiv.org/abs/2410.22079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22079">https://arxiv.org/pdf/2410.22079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22079]] HRPVT: High-Resolution Pyramid Vision Transformer for medium and small-scale human pose estimation(https://arxiv.org/abs/2410.22079)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Human pose estimation on medium and small scales has long been a significant challenge in this field. Most existing methods focus on restoring high-resolution feature maps by stacking multiple costly deconvolutional layers or by continuously aggregating semantic information from low-resolution feature maps while maintaining high-resolution ones, which can lead to information redundancy. Additionally, due to quantization errors, heatmap-based methods have certain disadvantages in accurately locating keypoints of medium and small-scale human figures. In this paper, we propose HRPVT, which utilizes PVT v2 as the backbone to model long-range dependencies. Building on this, we introduce the High-Resolution Pyramid Module (HRPM), designed to generate higher quality high-resolution representations by incorporating the intrinsic inductive biases of Convolutional Neural Networks (CNNs) into the high-resolution feature maps. The integration of HRPM enhances the performance of pure transformer-based models for human pose estimation at medium and small scales. Furthermore, we replace the heatmap-based method with SimCC approach, which eliminates the need for costly upsampling layers, thereby allowing us to allocate more computational resources to HRPM. To accommodate models with varying parameter scales, we have developed two insertion strategies of HRPM, each designed to enhancing the model's ability to perceive medium and small-scale human poses from two distinct perspectives.</li>
</ul>

<h3>Title: Choosy Babies Need One Coach: Inducing Mode-Seeking Behavior in BabyLlama with Reverse KL Divergence</h3>
<ul>
<li><strong>Authors: </strong>Shaozhen Shi, Yevgen Matusevych, Malvina Nissim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22081">https://arxiv.org/abs/2410.22081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22081">https://arxiv.org/pdf/2410.22081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22081]] Choosy Babies Need One Coach: Inducing Mode-Seeking Behavior in BabyLlama with Reverse KL Divergence(https://arxiv.org/abs/2410.22081)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study presents our submission to the Strict-Small Track of the 2nd BabyLM Challenge. We use a teacher-student distillation setup with the BabyLLaMa model (Timiryasov and Tastet, 2023) as a backbone. To make the student's learning process more focused, we replace the objective function with a reverse Kullback-Leibler divergence, known to cause mode-seeking (rather than mode-averaging) behaviour in computational learners. We further experiment with having a single teacher (instead of an ensemble of two teachers) and implement additional optimization strategies to improve the distillation process. Our experiments show that under reverse KL divergence, a single-teacher model often outperforms or matches multiple-teacher models across most tasks. Additionally, incorporating advanced optimization techniques further enhances model performance, demonstrating the effectiveness and robustness of our proposed approach. These findings support our idea that "choosy babies need one coach".</li>
</ul>

<h3>Title: Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate</h3>
<ul>
<li><strong>Authors: </strong>Zhiqi Bu, Xiaomeng Jin, Bhanukiran Vinzamuri, Anil Ramakrishna, Kai-Wei Chang, Volkan Cevher, Mingyi Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22086">https://arxiv.org/abs/2410.22086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22086">https://arxiv.org/pdf/2410.22086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22086]] Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate(https://arxiv.org/abs/2410.22086)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler. We provide a theoretical analysis and empirically demonstrate the superior performance of NGDiff among state-of-the-art unlearning methods on the TOFU and MUSE datasets while exhibiting stable training.</li>
</ul>

<h3>Title: TractShapeNet: Efficient Multi-Shape Learning with 3D Tractography Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Yui Lo, Yuqian Chen, Dongnan Liu, Jon Haitz Legarreta, Leo Zekelman, Fan Zhang, Jarrett Rushmore, Yogesh Rathi, Nikos Makris, Alexandra J. Golby, Weidong Cai, Lauren J. O'Donnell</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22099">https://arxiv.org/abs/2410.22099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22099">https://arxiv.org/pdf/2410.22099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22099]] TractShapeNet: Efficient Multi-Shape Learning with 3D Tractography Point Clouds(https://arxiv.org/abs/2410.22099)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Brain imaging studies have demonstrated that diffusion MRI tractography geometric shape descriptors can inform the study of the brain's white matter pathways and their relationship to brain function. In this work, we investigate the possibility of utilizing a deep learning model to compute shape measures of the brain's white matter connections. We introduce a novel framework, TractShapeNet, that leverages a point cloud representation of tractography to compute five shape measures: length, span, volume, total surface area, and irregularity. We assess the performance of the method on a large dataset including 1065 healthy young adults. Experiments for shape measure computation demonstrate that our proposed TractShapeNet outperforms other point cloud-based neural network models in both the Pearson correlation coefficient and normalized error metrics. We compare the inference runtime results with the conventional shape computation tool DSI-Studio. Our results demonstrate that a deep learning approach enables faster and more efficient shape measure computation. We also conduct experiments on two downstream language cognition prediction tasks, showing that shape measures from TractShapeNet perform similarly to those computed by DSI-Studio. Our code will be available at: this https URL.</li>
</ul>

<h3>Title: Hyperspectral Imaging-Based Perception in Autonomous Driving Scenarios: Benchmarking Baseline Semantic Segmentation Models</h3>
<ul>
<li><strong>Authors: </strong>Imad Ali Shah, Jiarong Li, Martin Glavin, Edward Jones, Enda Ward, Brian Deegan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22101">https://arxiv.org/abs/2410.22101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22101">https://arxiv.org/pdf/2410.22101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22101]] Hyperspectral Imaging-Based Perception in Autonomous Driving Scenarios: Benchmarking Baseline Semantic Segmentation Models(https://arxiv.org/abs/2410.22101)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Hyperspectral Imaging (HSI) is known for its advantages over traditional RGB imaging in remote sensing, agriculture, and medicine. Recently, it has gained attention for enhancing Advanced Driving Assistance Systems (ADAS) perception. Several HSI datasets such as HyKo, HSI-Drive, HSI-Road, and Hyperspectral City have been made available. However, a comprehensive evaluation of semantic segmentation models (SSM) using these datasets is lacking. To address this gap, we evaluated the available annotated HSI datasets on four deep learning-based baseline SSMs: DeepLab v3+, HRNet, PSPNet, and U-Net, along with its two variants: Coordinate Attention (UNet-CA) and Convolutional Block-Attention Module (UNet-CBAM). The original model architectures were adapted to handle the varying spatial and spectral dimensions of the datasets. These baseline SSMs were trained using a class-weighted loss function for individual HSI datasets and evaluated using mean-based metrics such as intersection over union (IoU), recall, precision, F1 score, specificity, and accuracy. Our results indicate that UNet-CBAM, which extracts channel-wise features, outperforms other SSMs and shows potential to leverage spectral information for enhanced semantic segmentation. This study establishes a baseline SSM benchmark on available annotated datasets for future evaluation of HSI-based ADAS perception. However, limitations of current HSI datasets, such as limited dataset size, high class imbalance, and lack of fine-grained annotations, remain significant constraints for developing robust SSMs for ADAS applications.</li>
</ul>

<h3>Title: Joint Extraction and Classification of Danish Competences for Job Matching</h3>
<ul>
<li><strong>Authors: </strong>Qiuchi Li, Christina Lioma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22103">https://arxiv.org/abs/2410.22103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22103">https://arxiv.org/pdf/2410.22103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22103]] Joint Extraction and Classification of Danish Competences for Job Matching(https://arxiv.org/abs/2410.22103)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The matching of competences, such as skills, occupations or knowledges, is a key desiderata for candidates to be fit for jobs. Automatic extraction of competences from CVs and Jobs can greatly promote recruiters' productivity in locating relevant candidates for job vacancies. This work presents the first model that jointly extracts and classifies competence from Danish job postings. Different from existing works on skill extraction and skill classification, our model is trained on a large volume of annotated Danish corpora and is capable of extracting a wide range of Danish competences, including skills, occupations and knowledges of different categories. More importantly, as a single BERT-like architecture for joint extraction and classification, our model is lightweight and efficient at inference. On a real-scenario job matching dataset, our model beats the state-of-the-art models in the overall performance of Danish competence extraction and classification, and saves over 50% time at inference.</li>
</ul>

<h3>Title: Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench</h3>
<ul>
<li><strong>Authors: </strong>Zheyuan Liu, Guangyao Dou, Mengzhao Jia, Zhaoxuan Tan, Qingkai Zeng, Yongle Yuan, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22108">https://arxiv.org/abs/2410.22108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22108">https://arxiv.org/pdf/2410.22108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22108]] Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench(https://arxiv.org/abs/2410.22108)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative models such as Large Language Models (LLM) and Multimodal Large Language models (MLLMs) trained on massive web corpora can memorize and disclose individuals' confidential and private data, raising legal and ethical concerns. While many previous works have addressed this issue in LLM via machine unlearning, it remains largely unexplored for MLLMs. To tackle this challenge, we introduce Multimodal Large Language Model Unlearning Benchmark (MLLMU-Bench), a novel benchmark aimed at advancing the understanding of multimodal machine unlearning. MLLMU-Bench consists of 500 fictitious profiles and 153 profiles for public celebrities, each profile feature over 14 customized question-answer pairs, evaluated from both multimodal (image+text) and unimodal (text) perspectives. The benchmark is divided into four sets to assess unlearning algorithms in terms of efficacy, generalizability, and model utility. Finally, we provide baseline results using existing generative model unlearning algorithms. Surprisingly, our experiments show that unimodal unlearning algorithms excel in generation and cloze tasks, while multimodal unlearning approaches perform better in classification tasks with multimodal inputs.</li>
</ul>

<h3>Title: Data Generation for Hardware-Friendly Post-Training Quantization</h3>
<ul>
<li><strong>Authors: </strong>Lior Dikstein, Ariel Lapid, Arnon Netzer, Hai Victor Habi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22110">https://arxiv.org/abs/2410.22110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22110">https://arxiv.org/pdf/2410.22110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22110]] Data Generation for Hardware-Friendly Post-Training Quantization(https://arxiv.org/abs/2410.22110)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Zero-shot quantization (ZSQ) using synthetic data is a key approach for post-training quantization (PTQ) under privacy and security constraints. However, existing data generation methods often struggle to effectively generate data suitable for hardware-friendly quantization, where all model layers are quantized. We analyze existing data generation methods based on batch normalization (BN) matching and identify several gaps between synthetic and real data: 1) Current generation algorithms do not optimize the entire synthetic dataset simultaneously; 2) Data augmentations applied during training are often overlooked; and 3) A distribution shift occurs in the final model layers due to the absence of BN in those layers. These gaps negatively impact ZSQ performance, particularly in hardware-friendly quantization scenarios. In this work, we propose Data Generation for Hardware-friendly quantization (DGH), a novel method that addresses these gaps. DGH jointly optimizes all generated images, regardless of the image set size or GPU memory constraints. To address data augmentation mismatches, DGH includes a preprocessing stage that mimics the augmentation process and enhances image quality by incorporating natural image priors. Finally, we propose a new distribution-stretching loss that aligns the support of the feature map distribution between real and synthetic data. This loss is applied to the model's output and can be adapted to various tasks. DGH demonstrates significant improvements in quantization performance across multiple tasks, achieving up to a 30% increase in accuracy for hardware-friendly ZSQ in both classification and object detection, often performing on par with real data.</li>
</ul>

<h3>Title: Policy Gradient for Robust Markov Decision Processes</h3>
<ul>
<li><strong>Authors: </strong>Qiuhao Wang, Shaohang Xu, Chin Pang Ho, Marek Petrick</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22114">https://arxiv.org/abs/2410.22114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22114">https://arxiv.org/pdf/2410.22114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22114]] Policy Gradient for Robust Markov Decision Processes(https://arxiv.org/abs/2410.22114)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We develop a generic policy gradient method with the global optimality guarantee for robust Markov Decision Processes (MDPs). While policy gradient methods are widely used for solving dynamic decision problems due to their scalable and efficient nature, adapting these methods to account for model ambiguity has been challenging, often making it impractical to learn robust policies. This paper introduces a novel policy gradient method, Double-Loop Robust Policy Mirror Descent (DRPMD), for solving robust MDPs. DRPMD employs a general mirror descent update rule for the policy optimization with adaptive tolerance per iteration, guaranteeing convergence to a globally optimal policy. We provide a comprehensive analysis of DRPMD, including new convergence results under both direct and softmax parameterizations, and provide novel insights into the inner problem solution through Transition Mirror Ascent (TMA). Additionally, we propose innovative parametric transition kernels for both discrete and continuous state-action spaces, broadening the applicability of our approach. Empirical results validate the robustness and global convergence of DRPMD across various challenging robust MDP settings.</li>
</ul>

<h3>Title: The Impact of Inference Acceleration Strategies on Bias of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Elisabeth Kirsten, Ivan Habernal, Vedant Nanda, Muhammad Bilal Zafar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22118">https://arxiv.org/abs/2410.22118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22118">https://arxiv.org/pdf/2410.22118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22118]] The Impact of Inference Acceleration Strategies on Bias of LLMs(https://arxiv.org/abs/2410.22118)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Last few years have seen unprecedented advances in capabilities of Large Language Models (LLMs). These advancements promise to deeply benefit a vast array of application domains. However, due to their immense size, performing inference with LLMs is both costly and slow. Consequently, a plethora of recent work has proposed strategies to enhance inference efficiency, e.g., quantization, pruning, and caching. These acceleration strategies reduce the inference cost and latency, often by several factors, while maintaining much of the predictive performance measured via common benchmarks. In this work, we explore another critical aspect of LLM performance: demographic bias in model generations due to inference acceleration optimizations. Using a wide range of metrics, we probe bias in model outputs from a number of angles. Analysis of outputs before and after inference acceleration shows significant change in bias. Worryingly, these bias effects are complex and unpredictable. A combination of an acceleration strategy and bias type may show little bias change in one model but may lead to a large effect in another. Our results highlight a need for in-depth and case-by-case evaluation of model bias after it has been modified to accelerate inference.</li>
</ul>

<h3>Title: Vision Paper: Designing Graph Neural Networks in Compliance with the European Artificial Intelligence Act</h3>
<ul>
<li><strong>Authors: </strong>Barbara Hoffmann, Jana Vatter, Ruben Mayer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22120">https://arxiv.org/abs/2410.22120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22120">https://arxiv.org/pdf/2410.22120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22120]] Vision Paper: Designing Graph Neural Networks in Compliance with the European Artificial Intelligence Act(https://arxiv.org/abs/2410.22120)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, fair, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>The European Union's Artificial Intelligence Act (AI Act) introduces comprehensive guidelines for the development and oversight of Artificial Intelligence (AI) and Machine Learning (ML) systems, with significant implications for Graph Neural Networks (GNNs). This paper addresses the unique challenges posed by the AI Act for GNNs, which operate on complex graph-structured data. The legislation's requirements for data management, data governance, robustness, human oversight, and privacy necessitate tailored strategies for GNNs. Our study explores the impact of these requirements on GNN training and proposes methods to ensure compliance. We provide an in-depth analysis of bias, robustness, explainability, and privacy in the context of GNNs, highlighting the need for fair sampling strategies and effective interpretability techniques. Our contributions fill the research gap by offering specific guidance for GNNs under the new legislative framework and identifying open questions and future research directions.</li>
</ul>

<h3>Title: Lightweight Frequency Masker for Cross-Domain Few-Shot Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jintao Tong, Yixiong Zou, Yuhua Li, Ruixuan Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22135">https://arxiv.org/abs/2410.22135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22135">https://arxiv.org/pdf/2410.22135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22135]] Lightweight Frequency Masker for Cross-Domain Few-Shot Semantic Segmentation(https://arxiv.org/abs/2410.22135)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Cross-domain few-shot segmentation (CD-FSS) is proposed to first pre-train the model on a large-scale source-domain dataset, and then transfer the model to data-scarce target-domain datasets for pixel-level segmentation. The significant domain gap between the source and target datasets leads to a sharp decline in the performance of existing few-shot segmentation (FSS) methods in cross-domain scenarios. In this work, we discover an intriguing phenomenon: simply filtering different frequency components for target domains can lead to a significant performance improvement, sometimes even as high as 14% mIoU. Then, we delve into this phenomenon for an interpretation, and find such improvements stem from the reduced inter-channel correlation in feature maps, which benefits CD-FSS with enhanced robustness against domain gaps and larger activated regions for segmentation. Based on this, we propose a lightweight frequency masker, which further reduces channel correlations by an amplitude-phase-masker (APM) module and an Adaptive Channel Phase Attention (ACPA) module. Notably, APM introduces only 0.01% additional parameters but improves the average performance by over 10%, and ACPA imports only 2.5% parameters but further improves the performance by over 1.5%, which significantly surpasses the state-of-the-art CD-FSS methods.</li>
</ul>

<h3>Title: AmpleGCG-Plus: A Strong Generative Model of Adversarial Suffixes to Jailbreak LLMs with Higher Success Rates in Fewer Attempts</h3>
<ul>
<li><strong>Authors: </strong>Vishal Kumar, Zeyi Liao, Jaylen Jones, Huan Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22143">https://arxiv.org/abs/2410.22143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22143">https://arxiv.org/pdf/2410.22143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22143]] AmpleGCG-Plus: A Strong Generative Model of Adversarial Suffixes to Jailbreak LLMs with Higher Success Rates in Fewer Attempts(https://arxiv.org/abs/2410.22143)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, generative, large language model</a></li>
<li><strong>Abstract: </strong>Although large language models (LLMs) are typically aligned, they remain vulnerable to jailbreaking through either carefully crafted prompts in natural language or, interestingly, gibberish adversarial suffixes. However, gibberish tokens have received relatively less attention despite their success in attacking aligned LLMs. Recent work, AmpleGCG~\citep{liao2024amplegcg}, demonstrates that a generative model can quickly produce numerous customizable gibberish adversarial suffixes for any harmful query, exposing a range of alignment gaps in out-of-distribution (OOD) language spaces. To bring more attention to this area, we introduce AmpleGCG-Plus, an enhanced version that achieves better performance in fewer attempts. Through a series of exploratory experiments, we identify several training strategies to improve the learning of gibberish suffixes. Our results, verified under a strict evaluation setting, show that it outperforms AmpleGCG on both open-weight and closed-source models, achieving increases in attack success rate (ASR) of up to 17\% in the white-box setting against Llama-2-7B-chat, and more than tripling ASR in the black-box setting against GPT-4. Notably, AmpleGCG-Plus jailbreaks the newer GPT-4o series of models at similar rates to GPT-4, and, uncovers vulnerabilities against the recently proposed circuit breakers defense. We publicly release AmpleGCG-Plus along with our collected training datasets.</li>
</ul>

<h3>Title: Capacity Control is an Effective Memorization Mitigation Mechanism in Text-Conditional Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Raman Dutt, Pedro Sanchez, Ondrej Bohdal, Sotirios A. Tsaftaris, Timothy Hospedales</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22149">https://arxiv.org/abs/2410.22149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22149">https://arxiv.org/pdf/2410.22149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22149]] Capacity Control is an Effective Memorization Mitigation Mechanism in Text-Conditional Diffusion Models(https://arxiv.org/abs/2410.22149)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this work, we present compelling evidence that controlling model capacity during fine-tuning can effectively mitigate memorization in diffusion models. Specifically, we demonstrate that adopting Parameter-Efficient Fine-Tuning (PEFT) within the pre-train fine-tune paradigm significantly reduces memorization compared to traditional full fine-tuning approaches. Our experiments utilize the MIMIC dataset, which comprises image-text pairs of chest X-rays and their corresponding reports. The results, evaluated through a range of memorization and generation quality metrics, indicate that PEFT not only diminishes memorization but also enhances downstream generation quality. Additionally, PEFT methods can be seamlessly combined with existing memorization mitigation techniques for further improvement. The code for our experiments is available at: this https URL</li>
</ul>

<h3>Title: Standardization Trends on Safety and Trustworthiness Technology for Advanced AI</h3>
<ul>
<li><strong>Authors: </strong>Jonghong Jeon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22151">https://arxiv.org/abs/2410.22151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22151">https://arxiv.org/pdf/2410.22151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22151]] Standardization Trends on Safety and Trustworthiness Technology for Advanced AI(https://arxiv.org/abs/2410.22151)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI) has rapidly evolved over the past decade and has advanced in areas such as language comprehension, image and video recognition, programming, and scientific reasoning. Recent AI technologies based on large language models and foundation models are approaching or surpassing artificial general intelligence. These systems demonstrate superior performance in complex problem solving, natural language processing, and multi-domain tasks, and can potentially transform fields such as science, industry, healthcare, and education. However, these advancements have raised concerns regarding the safety and trustworthiness of advanced AI, including risks related to uncontrollability, ethical conflicts, long-term socioeconomic impacts, and safety assurance. Efforts are being expended to develop internationally agreed-upon standards to ensure the safety and reliability of AI. This study analyzes international trends in safety and trustworthiness standardization for advanced AI, identifies key areas for standardization, proposes future directions and strategies, and draws policy implications. The goal is to support the safe and trustworthy development of advanced AI and enhance international competitiveness through effective standardization.</li>
</ul>

<h3>Title: Benchmarking LLM Guardrails in Handling Multilingual Toxicity</h3>
<ul>
<li><strong>Authors: </strong>Yahan Yang, Soham Dan, Dan Roth, Insup Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22153">https://arxiv.org/abs/2410.22153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22153">https://arxiv.org/pdf/2410.22153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22153]] Benchmarking LLM Guardrails in Handling Multilingual Toxicity(https://arxiv.org/abs/2410.22153)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>With the ubiquity of Large Language Models (LLMs), guardrails have become crucial to detect and defend against toxic content. However, with the increasing pervasiveness of LLMs in multilingual scenarios, their effectiveness in handling multilingual toxic inputs remains unclear. In this work, we introduce a comprehensive multilingual test suite, spanning seven datasets and over ten languages, to benchmark the performance of state-of-the-art guardrails. We also investigates the resilience of guardrails against recent jailbreaking techniques, and assess the impact of in-context safety policies and language resource availability on guardrails' performance. Our findings show that existing guardrails are still ineffective at handling multilingual toxicity and lack robustness against jailbreaking prompts. This work aims to identify the limitations of guardrails and to build a more reliable and trustworthy LLMs in multilingual scenarios.</li>
</ul>

<h3>Title: Very Attentive Tacotron: Robust and Unbounded Length Generalization in Autoregressive Transformer-Based Text-to-Speech</h3>
<ul>
<li><strong>Authors: </strong>Eric Battenberg, RJ Skerry-Ryan, Daisy Stanton, Soroosh Mariooryad, Matt Shannon, Julian Salazar, David Kao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22179">https://arxiv.org/abs/2410.22179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22179">https://arxiv.org/pdf/2410.22179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22179]] Very Attentive Tacotron: Robust and Unbounded Length Generalization in Autoregressive Transformer-Based Text-to-Speech(https://arxiv.org/abs/2410.22179)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) Transformer-based sequence models are known to have difficulty generalizing to sequences longer than those seen during training. When applied to text-to-speech (TTS), these models tend to drop or repeat words or produce erratic output, especially for longer utterances. In this paper, we introduce enhancements aimed at AR Transformer-based encoder-decoder TTS systems that address these robustness and length generalization issues. Our approach uses an alignment mechanism to provide cross-attention operations with relative location information. The associated alignment position is learned as a latent property of the model via backprop and requires no external alignment information during training. While the approach is tailored to the monotonic nature of TTS input-output alignment, it is still able to benefit from the flexible modeling power of interleaved multi-head self- and cross-attention operations. A system incorporating these improvements, which we call Very Attentive Tacotron, matches the naturalness and expressiveness of a baseline T5-based TTS system, while eliminating problems with repeated or dropped words and enabling generalization to any practical utterance length.</li>
</ul>

<h3>Title: Natural Language Processing for Analyzing Electronic Health Records and Clinical Notes in Cancer Research: A Review</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Bilal, Ameer Hamza, Nadia Malik</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22180">https://arxiv.org/abs/2410.22180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22180">https://arxiv.org/pdf/2410.22180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22180]] Natural Language Processing for Analyzing Electronic Health Records and Clinical Notes in Cancer Research: A Review(https://arxiv.org/abs/2410.22180)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Objective: This review aims to analyze the application of natural language processing (NLP) techniques in cancer research using electronic health records (EHRs) and clinical notes. This review addresses gaps in the existing literature by providing a broader perspective than previous studies focused on specific cancer types or applications. Methods: A comprehensive literature search was conducted using the Scopus database, identifying 94 relevant studies published between 2019 and 2024. Data extraction included study characteristics, cancer types, NLP methodologies, dataset information, performance metrics, challenges, and future directions. Studies were categorized based on cancer types and NLP applications. Results: The results showed a growing trend in NLP applications for cancer research, with breast, lung, and colorectal cancers being the most studied. Information extraction and text classification emerged as predominant NLP tasks. A shift from rule-based to advanced machine learning techniques, particularly transformer-based models, was observed. The Dataset sizes used in existing studies varied widely. Key challenges included the limited generalizability of proposed solutions and the need for improved integration into clinical workflows. Conclusion: NLP techniques show significant potential in analyzing EHRs and clinical notes for cancer research. However, future work should focus on improving model generalizability, enhancing robustness in handling complex clinical language, and expanding applications to understudied cancer types. Integration of NLP tools into clinical practice and addressing ethical considerations remain crucial for utilizing the full potential of NLP in enhancing cancer diagnosis, treatment, and patient outcomes.</li>
</ul>

<h3>Title: $r$Age-$k$: Communication-Efficient Federated Learning Using Age Factor</h3>
<ul>
<li><strong>Authors: </strong>Matin Mortaheb, Priyanka Kaswan, Sennur Ulukus</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, eess.SP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22192">https://arxiv.org/abs/2410.22192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22192">https://arxiv.org/pdf/2410.22192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22192]] $r$Age-$k$: Communication-Efficient Federated Learning Using Age Factor(https://arxiv.org/abs/2410.22192)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a collaborative approach where multiple clients, coordinated by a parameter server (PS), train a unified machine-learning model. The approach, however, suffers from two key challenges: data heterogeneity and communication overhead. Data heterogeneity refers to inconsistencies in model training arising from heterogeneous data at different clients. Communication overhead arises from the large volumes of parameter updates exchanged between the PS and clients. Existing solutions typically address these challenges separately. This paper introduces a new communication-efficient algorithm that uses the age of information metric to simultaneously tackle both limitations of FL. We introduce age vectors at the PS, which keep track of how often the different model parameters are updated from the clients. The PS uses this to selectively request updates for specific gradient indices from each client. Further, the PS employs age vectors to identify clients with statistically similar data and group them into clusters. The PS combines the age vectors of the clustered clients to efficiently coordinate gradient index updates among clients within a cluster. We evaluate our approach using the MNIST and CIFAR10 datasets in highly non-i.i.d. settings. The experimental results show that our proposed method can expedite training, surpassing other communication-efficient strategies in efficiency.</li>
</ul>

<h3>Title: ProMQA: Question Answering Dataset for Multimodal Procedural Activity Understanding</h3>
<ul>
<li><strong>Authors: </strong>Kimihiro Hasegawa, Wiradee Imrattanatrai, Zhi-Qi Cheng, Masaki Asada, Susan Holm, Yuran Wang, Ken Fukuda, Teruko Mitamura</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22211">https://arxiv.org/abs/2410.22211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22211">https://arxiv.org/pdf/2410.22211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22211]] ProMQA: Question Answering Dataset for Multimodal Procedural Activity Understanding(https://arxiv.org/abs/2410.22211)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Multimodal systems have great potential to assist humans in procedural activities, where people follow instructions to achieve their goals. Despite diverse application scenarios, systems are typically evaluated on traditional classification tasks, e.g., action recognition or temporal action segmentation. In this paper, we present a novel evaluation dataset, ProMQA, to measure system advancements in application-oriented scenarios. ProMQA consists of 401 multimodal procedural QA pairs on user recording of procedural activities coupled with their corresponding instruction. For QA annotation, we take a cost-effective human-LLM collaborative approach, where the existing annotation is augmented with LLM-generated QA pairs that are later verified by humans. We then provide the benchmark results to set the baseline performance on ProMQA. Our experiment reveals a significant gap between human performance and that of current systems, including competitive proprietary multimodal models. We hope our dataset sheds light on new aspects of models' multimodal understanding capabilities.</li>
</ul>

<h3>Title: LiVisSfM: Accurate and Robust Structure-from-Motion with LiDAR and Visual Cues</h3>
<ul>
<li><strong>Authors: </strong>Hanqing Jiang, Liyang Zhou, Zhuang Zhang, Yihao Yu, Guofeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22213">https://arxiv.org/abs/2410.22213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22213">https://arxiv.org/pdf/2410.22213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22213]] LiVisSfM: Accurate and Robust Structure-from-Motion with LiDAR and Visual Cues(https://arxiv.org/abs/2410.22213)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents an accurate and robust Structure-from-Motion (SfM) pipeline named LiVisSfM, which is an SfM-based reconstruction system that fully combines LiDAR and visual cues. Unlike most existing LiDAR-inertial odometry (LIO) and LiDAR-inertial-visual odometry (LIVO) methods relying heavily on LiDAR registration coupled with Inertial Measurement Unit (IMU), we propose a LiDAR-visual SfM method which innovatively carries out LiDAR frame registration to LiDAR voxel map in a Point-to-Gaussian residual metrics, combined with a LiDAR-visual BA and explicit loop closure in a bundle optimization way to achieve accurate and robust LiDAR pose estimation without dependence on IMU incorporation. Besides, we propose an incremental voxel updating strategy for efficient voxel map updating during the process of LiDAR frame registration and LiDAR-visual BA optimization. Experiments demonstrate the superior effectiveness of our LiVisSfM framework over state-of-the-art LIO and LIVO works on more accurate and robust LiDAR pose recovery and dense point cloud reconstruction of both public KITTI benchmark and a variety of self-captured dataset.</li>
</ul>

<h3>Title: Towards Unifying Understanding and Generation in the Era of Vision Foundation Models: A Survey from the Autoregression Perspective</h3>
<ul>
<li><strong>Authors: </strong>Shenghao Xie, Wenqiang Zu, Mingyang Zhao, Duo Su, Shilong Liu, Ruohua Shi, Guoqi Li, Shanghang Zhang, Lei Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22217">https://arxiv.org/abs/2410.22217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22217">https://arxiv.org/pdf/2410.22217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22217]] Towards Unifying Understanding and Generation in the Era of Vision Foundation Models: A Survey from the Autoregression Perspective(https://arxiv.org/abs/2410.22217)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Autoregression in large language models (LLMs) has shown impressive scalability by unifying all language tasks into the next token prediction paradigm. Recently, there is a growing interest in extending this success to vision foundation models. In this survey, we review the recent advances and discuss future directions for autoregressive vision foundation models. First, we present the trend for next generation of vision foundation models, i.e., unifying both understanding and generation in vision tasks. We then analyze the limitations of existing vision foundation models, and present a formal definition of autoregression with its advantages. Later, we categorize autoregressive vision foundation models from their vision tokenizers and autoregression backbones. Finally, we discuss several promising research challenges and directions. To the best of our knowledge, this is the first survey to comprehensively summarize autoregressive vision foundation models under the trend of unifying understanding and generation. A collection of related resources is available at this https URL.</li>
</ul>

<h3>Title: ContextIQ: A Multimodal Expert-Based Video Retrieval System for Contextual Advertising</h3>
<ul>
<li><strong>Authors: </strong>Ashutosh Chaubey, Anoubhav Agarwaal, Sartaki Sinha Roy, Aayush Agarwal, Susmita Ghose</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22233">https://arxiv.org/abs/2410.22233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22233">https://arxiv.org/pdf/2410.22233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22233]] ContextIQ: A Multimodal Expert-Based Video Retrieval System for Contextual Advertising(https://arxiv.org/abs/2410.22233)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Contextual advertising serves ads that are aligned to the content that the user is viewing. The rapid growth of video content on social platforms and streaming services, along with privacy concerns, has increased the need for contextual advertising. Placing the right ad in the right context creates a seamless and pleasant ad viewing experience, resulting in higher audience engagement and, ultimately, better ad monetization. From a technology standpoint, effective contextual advertising requires a video retrieval system capable of understanding complex video content at a very granular level. Current text-to-video retrieval models based on joint multimodal training demand large datasets and computational resources, limiting their practicality and lacking the key functionalities required for ad ecosystem integration. We introduce ContextIQ, a multimodal expert-based video retrieval system designed specifically for contextual advertising. ContextIQ utilizes modality-specific experts-video, audio, transcript (captions), and metadata such as objects, actions, emotion, etc.-to create semantically rich video representations. We show that our system, without joint training, achieves better or comparable results to state-of-the-art models and commercial solutions on multiple text-to-video retrieval benchmarks. Our ablation studies highlight the benefits of leveraging multiple modalities for enhanced video retrieval accuracy instead of using a vision-language model alone. Furthermore, we show how video retrieval systems such as ContextIQ can be used for contextual advertising in an ad ecosystem while also addressing concerns related to brand safety and filtering inappropriate content.</li>
</ul>

<h3>Title: Auditing $f$-Differential Privacy in One Run</h3>
<ul>
<li><strong>Authors: </strong>Saeed Mahloujifar, Luca Melis, Kamalika Chaudhuri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22235">https://arxiv.org/abs/2410.22235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22235">https://arxiv.org/pdf/2410.22235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22235]] Auditing $f$-Differential Privacy in One Run(https://arxiv.org/abs/2410.22235)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Empirical auditing has emerged as a means of catching some of the flaws in the implementation of privacy-preserving algorithms. Existing auditing mechanisms, however, are either computationally inefficient requiring multiple runs of the machine learning algorithms or suboptimal in calculating an empirical privacy. In this work, we present a tight and efficient auditing procedure and analysis that can effectively assess the privacy of mechanisms. Our approach is efficient; similar to the recent work of Steinke, Nasr, and Jagielski (2023), our auditing procedure leverages the randomness of examples in the input dataset and requires only a single run of the target mechanism. And it is more accurate; we provide a novel analysis that enables us to achieve tight empirical privacy estimates by using the hypothesized $f$-DP curve of the mechanism, which provides a more accurate measure of privacy than the traditional $\epsilon,\delta$ differential privacy parameters. We use our auditing procure and analysis to obtain empirical privacy, demonstrating that our auditing procedure delivers tighter privacy estimates.</li>
</ul>

<h3>Title: DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Rakesh R. Menon, Shashank Srivastava</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22239">https://arxiv.org/abs/2410.22239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22239">https://arxiv.org/pdf/2410.22239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22239]] DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers(https://arxiv.org/abs/2410.22239)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite their high predictive accuracies, current machine learning systems often exhibit systematic biases stemming from annotation artifacts or insufficient support for certain classes in the dataset. Recent work proposes automatic methods for identifying and explaining systematic biases using keywords. We introduce DISCERN, a framework for interpreting systematic biases in text classifiers using language explanations. DISCERN iteratively generates precise natural language descriptions of systematic errors by employing an interactive loop between two large language models. Finally, we use the descriptions to improve classifiers by augmenting classifier training sets with synthetically generated instances or annotated examples via active learning. On three text-classification datasets, we demonstrate that language explanations from our framework induce consistent performance improvements that go beyond what is achievable with exemplars of systematic bias. Finally, in human evaluations, we show that users can interpret systematic biases more effectively (by over 25% relative) and efficiently when described through language explanations as opposed to cluster exemplars.</li>
</ul>

<h3>Title: Abrupt Learning in Transformers: A Case Study on Matrix Completion</h3>
<ul>
<li><strong>Authors: </strong>Pulkit Gopalani, Ekdeep Singh Lubana, Wei Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22244">https://arxiv.org/abs/2410.22244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22244">https://arxiv.org/pdf/2410.22244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22244]] Abrupt Learning in Transformers: A Case Study on Matrix Completion(https://arxiv.org/abs/2410.22244)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Recent analysis on the training dynamics of Transformers has unveiled an interesting characteristic: the training loss plateaus for a significant number of training steps, and then suddenly (and sharply) drops to near--optimal values. To understand this phenomenon in depth, we formulate the low-rank matrix completion problem as a masked language modeling (MLM) task, and show that it is possible to train a BERT model to solve this task to low error. Furthermore, the loss curve shows a plateau early in training followed by a sudden drop to near-optimal values, despite no changes in the training procedure or hyper-parameters. To gain interpretability insights into this sudden drop, we examine the model's predictions, attention heads, and hidden states before and after this transition. Concretely, we observe that (a) the model transitions from simply copying the masked input to accurately predicting the masked entries; (b) the attention heads transition to interpretable patterns relevant to the task; and (c) the embeddings and hidden states encode information relevant to the problem. We also analyze the training dynamics of individual model components to understand the sudden drop in loss.</li>
</ul>

<h3>Title: LipKernel: Lipschitz-Bounded Convolutional Neural Networks via Dissipative Layers</h3>
<ul>
<li><strong>Authors: </strong>Patricia Pauli, Ruigang Wang, Ian Manchester, Frank Allg√∂wer</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV, eess.SY, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22258">https://arxiv.org/abs/2410.22258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22258">https://arxiv.org/pdf/2410.22258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22258]] LipKernel: Lipschitz-Bounded Convolutional Neural Networks via Dissipative Layers(https://arxiv.org/abs/2410.22258)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose a novel layer-wise parameterization for convolutional neural networks (CNNs) that includes built-in robustness guarantees by enforcing a prescribed Lipschitz bound. Each layer in our parameterization is designed to satisfy a linear matrix inequality (LMI), which in turn implies dissipativity with respect to a specific supply rate. Collectively, these layer-wise LMIs ensure Lipschitz boundedness for the input-output mapping of the neural network, yielding a more expressive parameterization than through spectral bounds or orthogonal layers. Our new method LipKernel directly parameterizes dissipative convolution kernels using a 2-D Roesser-type state space model. This means that the convolutional layers are given in standard form after training and can be evaluated without computational overhead. In numerical experiments, we show that the run-time using our method is orders of magnitude faster than state-of-the-art Lipschitz-bounded networks that parameterize convolutions in the Fourier domain, making our approach particularly attractive for improving robustness of learning-based real-time perception or control in robotics, autonomous vehicles, or automation systems. We focus on CNNs, and in contrast to previous works, our approach accommodates a wide variety of layers typically used in CNNs, including 1-D and 2-D convolutional layers, maximum and average pooling layers, as well as strided and dilated convolutions and zero padding. However, our approach naturally extends beyond CNNs as we can incorporate any layer that is incrementally dissipative.</li>
</ul>

<h3>Title: NCA-Morph: Medical Image Registration with Neural Cellular Automata</h3>
<ul>
<li><strong>Authors: </strong>Amin Ranem, John Kalkhof, Anirban Mukhopadhyay</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22265">https://arxiv.org/abs/2410.22265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22265">https://arxiv.org/pdf/2410.22265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22265]] NCA-Morph: Medical Image Registration with Neural Cellular Automata(https://arxiv.org/abs/2410.22265)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Medical image registration is a critical process that aligns various patient scans, facilitating tasks like diagnosis, surgical planning, and tracking. Traditional optimization based methods are slow, prompting the use of Deep Learning (DL) techniques, such as VoxelMorph and Transformer-based strategies, for faster results. However, these DL methods often impose significant resource demands. In response to these challenges, we present NCA-Morph, an innovative approach that seamlessly blends DL with a bio-inspired communication and networking approach, enabled by Neural Cellular Automata (NCAs). NCA-Morph not only harnesses the power of DL for efficient image registration but also builds a network of local communications between cells and respective voxels over time, mimicking the interaction observed in living systems. In our extensive experiments, we subject NCA-Morph to evaluations across three distinct 3D registration tasks, encompassing Brain, Prostate and Hippocampus images from both healthy and diseased patients. The results showcase NCA-Morph's ability to achieve state-of-the-art performance. Notably, NCA-Morph distinguishes itself as a lightweight architecture with significantly fewer parameters; 60% and 99.7% less than VoxelMorph and TransMorph. This characteristic positions NCA-Morph as an ideal solution for resource-constrained medical applications, such as primary care settings and operating rooms.</li>
</ul>

<h3>Title: Fourier Head: Helping Large Language Models Learn Complex Probability Distributions</h3>
<ul>
<li><strong>Authors: </strong>Nate Gillman, Daksh Aggarwal, Michael Freeman, Saurabh Singh, Chen Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22269">https://arxiv.org/abs/2410.22269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22269">https://arxiv.org/pdf/2410.22269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22269]] Fourier Head: Helping Large Language Models Learn Complex Probability Distributions(https://arxiv.org/abs/2410.22269)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>As the quality of large language models has improved, there has been increased interest in using them to model non-linguistic tokens. For example, the Decision Transformer recasts agentic decision making as a sequence modeling problem, using a decoder-only LLM to model the distribution over the discrete action space for an Atari agent. However, when adapting LLMs to non-linguistic domains, it remains unclear if softmax over discrete bins captures the continuous structure of the tokens and the potentially complex distributions needed for high quality token generation. We introduce a neural network layer, constructed using Fourier series, which we can easily substitute for any linear layer if we want the outputs to have a more continuous structure. We perform extensive analysis on synthetic datasets, as well as on large-scale decision making and time series forecasting tasks. We also provide theoretical evidence that this layer can better learn signal from data while ignoring high-frequency noise. All of our results support the effectiveness of our proposed Fourier head in scenarios where the underlying data distribution has a natural continuous structure. For example, the Fourier head improves a Decision Transformer agent's returns by 46% on the Atari Seaquest game, and increases a state-of-the-art times series foundation model's forecasting performance by 3.5% across 20 benchmarks unseen during training.</li>
</ul>

<h3>Title: Embedding-based classifiers can detect prompt injection attacks</h3>
<ul>
<li><strong>Authors: </strong>Md. Ahsan Ayub, Subhabrata Majumdar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22284">https://arxiv.org/abs/2410.22284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22284">https://arxiv.org/pdf/2410.22284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22284]] Embedding-based classifiers can detect prompt injection attacks(https://arxiv.org/abs/2410.22284)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are seeing significant adoption in every type of organization due to their exceptional generative capabilities. However, LLMs are found to be vulnerable to various adversarial attacks, particularly prompt injection attacks, which trick them into producing harmful or inappropriate content. Adversaries execute such attacks by crafting malicious prompts to deceive the LLMs. In this paper, we propose a novel approach based on embedding-based Machine Learning (ML) classifiers to protect LLM-based applications against this severe threat. We leverage three commonly used embedding models to generate embeddings of malicious and benign prompts and utilize ML classifiers to predict whether an input prompt is malicious. Out of several traditional ML methods, we achieve the best performance with classifiers built using Random Forest and XGBoost. Our classifiers outperform state-of-the-art prompt injection classifiers available in open-source implementations, which use encoder-only neural networks.</li>
</ul>

<h3>Title: Fine-Tuning LLMs for Code Mutation: A New Era of Cyber Threats</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Setak, Pooria Madani</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22293">https://arxiv.org/abs/2410.22293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22293">https://arxiv.org/pdf/2410.22293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22293]] Fine-Tuning LLMs for Code Mutation: A New Era of Cyber Threats(https://arxiv.org/abs/2410.22293)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have significantly improved their capabilities in natural language processing and code synthesis, enabling more complex applications across different fields. This paper explores the application of LLMs in the context of code mutation, a process where the structure of program code is altered without changing its functionality. Traditionally, code mutation has been employed to increase software robustness in mission-critical applications. Additionally, mutation engines have been exploited by malware developers to evade the signature-based detection methods employed by malware detection systems. Existing code mutation engines, often used by such threat actors, typically result in only limited variations in the malware, which can still be identified through static code analysis. However, the agility demonstrated by an LLM-based code synthesizer could significantly change this threat landscape by allowing for more complex code mutations that are not easily detected using static analysis. One can increase variations of codes synthesized by a pre-trained LLM through fine-tuning and retraining. This process is what we refer to as code mutation training. In this paper, we propose a novel definition of code mutation training tailored for pre-trained LLM-based code synthesizers and demonstrate this training on a lightweight pre-trained model. Our approach involves restructuring (i.e., mutating) code at the subroutine level, which allows for more manageable mutations while maintaining the semantic integrity verified through unit testing. Our experimental results illustrate the effectiveness of our approach in improving code mutation capabilities of LLM-based program synthesizers in producing varied and functionally correct code solutions, showcasing their potential to transform the landscape of code mutation and the threats associated with it.</li>
</ul>

<h3>Title: LLMs are Highly-Constrained Biophysical Sequence Optimizers</h3>
<ul>
<li><strong>Authors: </strong>Angelica Chen, Samuel D. Stanton, Robert G. Alberstein, Andrew M. Watkins, Richard Bonneau, Vladimir Gligorijevi, Kyunghyun Cho, Nathan C. Frey</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22296">https://arxiv.org/abs/2410.22296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22296">https://arxiv.org/pdf/2410.22296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22296]] LLMs are Highly-Constrained Biophysical Sequence Optimizers(https://arxiv.org/abs/2410.22296)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently shown significant potential in various biological tasks such as protein engineering and molecule design. These tasks typically involve black-box discrete sequence optimization, where the challenge lies in generating sequences that are not only biologically feasible but also adhere to hard fine-grained constraints. However, LLMs often struggle with such constraints, especially in biological contexts where verifying candidate solutions is costly and time-consuming. In this study, we explore the possibility of employing LLMs as highly-constrained bilevel optimizers through a methodology we refer to as Language Model Optimization with Margin Expectation (LLOME). This approach combines both offline and online optimization, utilizing limited oracle evaluations to iteratively enhance the sequences generated by the LLM. We additionally propose a novel training objective -- Margin-Aligned Expectation (MargE) -- that trains the LLM to smoothly interpolate between the reward and reference distributions. Lastly, we introduce a synthetic test suite that bears strong geometric similarity to real biophysical problems and enables rapid evaluation of LLM optimizers without time-consuming lab validation. Our findings reveal that, in comparison to genetic algorithm baselines, LLMs achieve significantly lower regret solutions while requiring fewer test function evaluations. However, we also observe that LLMs exhibit moderate miscalibration, are susceptible to generator collapse, and have difficulty finding the optimal solution when no explicit ground truth rewards are available.</li>
</ul>

<h3>Title: $\mathsf{OPA}$: One-shot Private Aggregation with Single Client Interaction and its Applications to Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Harish Karthikeyan, Antigoni Polychroniadou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22303">https://arxiv.org/abs/2410.22303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22303">https://arxiv.org/pdf/2410.22303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22303]] $\mathsf{OPA}$: One-shot Private Aggregation with Single Client Interaction and its Applications to Federated Learning(https://arxiv.org/abs/2410.22303)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, federate</a></li>
<li><strong>Abstract: </strong>Our work aims to minimize interaction in secure computation due to the high cost and challenges associated with communication rounds, particularly in scenarios with many clients. In this work, we revisit the problem of secure aggregation in the single-server setting where a single evaluation server can securely aggregate client-held individual inputs. Our key contribution is the introduction of One-shot Private Aggregation ($\mathsf{OPA}$) where clients speak only once (or even choose not to speak) per aggregation evaluation. Since each client communicates only once per aggregation, this simplifies managing dropouts and dynamic participation, contrasting with multi-round protocols and aligning with plaintext secure aggregation, where clients interact only once. We construct $\mathsf{OPA}$ based on LWR, LWE, class groups, DCR and demonstrate applications to privacy-preserving Federated Learning (FL) where clients \emph{speak once}. This is a sharp departure from prior multi-round FL protocols whose study was initiated by Bonawitz et al. (CCS, 2017). Moreover, unlike the YOSO (You Only Speak Once) model for general secure computation, $\mathsf{OPA}$ eliminates complex committee selection protocols to achieve adaptive security. Beyond asymptotic improvements, $\mathsf{OPA}$ is practical, outperforming state-of-the-art solutions. We benchmark logistic regression classifiers for two datasets, while also building an MLP classifier to train on MNIST, CIFAR-10, and CIFAR-100 datasets. We build two flavors of $\caps$ (1) from (threshold) key homomorphic PRF and (2) from seed homomorphic PRG and secret sharing.</li>
</ul>

<h3>Title: Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning</h3>
<ul>
<li><strong>Authors: </strong>Yihe Deng, Paul Mineiro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22304">https://arxiv.org/abs/2410.22304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22304">https://arxiv.org/pdf/2410.22304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22304]] Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning(https://arxiv.org/abs/2410.22304)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mathematical reasoning is a crucial capability for Large Language Models (LLMs), yet generating detailed and accurate reasoning traces remains a significant challenge. This paper introduces a novel approach to produce high-quality reasoning traces for LLM fine-tuning using online learning \textbf{Flows}. Our method employs an incremental output production Flow, where component LLMs collaboratively construct solutions through iterative communication. We train the Flow using online Direct Preference Optimization (DPO) learning with rollouts, generating DPO pairs for each training example and updating models in real-time. We directly compare the quality of reasoning traces generated by our method with those produced through direct model inference, demonstrating the effectiveness of our approach in improving LLM performance in mathematical reasoning tasks.</li>
</ul>

<h3>Title: SVIP: Towards Verifiable Inference of Open-source Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yifan Sun, Yuhang Li, Yue Zhang, Yuchen Jin, Huan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22307">https://arxiv.org/abs/2410.22307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22307">https://arxiv.org/pdf/2410.22307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22307]] SVIP: Towards Verifiable Inference of Open-source Large Language Models(https://arxiv.org/abs/2410.22307)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Open-source Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language understanding and generation, leading to widespread adoption across various domains. However, their increasing model sizes render local deployment impractical for individual users, pushing many to rely on computing service providers for inference through a blackbox API. This reliance introduces a new risk: a computing provider may stealthily substitute the requested LLM with a smaller, less capable model without consent from users, thereby delivering inferior outputs while benefiting from cost savings. In this paper, we formalize the problem of verifiable inference for LLMs. Existing verifiable computing solutions based on cryptographic or game-theoretic techniques are either computationally uneconomical or rest on strong assumptions. We introduce SVIP, a secret-based verifiable LLM inference protocol that leverages intermediate outputs from LLM as unique model identifiers. By training a proxy task on these outputs and requiring the computing provider to return both the generated text and the processed intermediate outputs, users can reliably verify whether the computing provider is acting honestly. In addition, the integration of a secret mechanism further enhances the security of our protocol. We thoroughly analyze our protocol under multiple strong and adaptive adversarial scenarios. Our extensive experiments demonstrate that SVIP is accurate, generalizable, computationally efficient, and resistant to various attacks. Notably, SVIP achieves false negative rates below 5% and false positive rates below 3%, while requiring less than 0.01 seconds per query for verification.</li>
</ul>

<h3>Title: Convex Formulations for Training Two-Layer ReLU Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Karthik Prakhya, Tolga Birdal, Alp Yurtsever</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22311">https://arxiv.org/abs/2410.22311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22311">https://arxiv.org/pdf/2410.22311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22311]] Convex Formulations for Training Two-Layer ReLU Neural Networks(https://arxiv.org/abs/2410.22311)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Solving non-convex, NP-hard optimization problems is crucial for training machine learning models, including neural networks. However, non-convexity often leads to black-box machine learning models with unclear inner workings. While convex formulations have been used for verifying neural network robustness, their application to training neural networks remains less explored. In response to this challenge, we reformulate the problem of training infinite-width two-layer ReLU networks as a convex completely positive program in a finite-dimensional (lifted) space. Despite the convexity, solving this problem remains NP-hard due to the complete positivity constraint. To overcome this challenge, we introduce a semidefinite relaxation that can be solved in polynomial time. We then experimentally evaluate the tightness of this relaxation, demonstrating its competitive performance in test accuracy across a range of classification tasks.</li>
</ul>

<h3>Title: Natural Language Inference Improves Compositionality in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Paola Cascante-Bonilla, Yu Hou, Yang Trista Cao, Hal Daum√© III, Rachel Rudinger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22315">https://arxiv.org/abs/2410.22315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22315">https://arxiv.org/pdf/2410.22315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22315]] Natural Language Inference Improves Compositionality in Vision-Language Models(https://arxiv.org/abs/2410.22315)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Compositional reasoning in Vision-Language Models (VLMs) remains challenging as these models often struggle to relate objects, attributes, and spatial relationships. Recent methods aim to address these limitations by relying on the semantics of the textual description, using Large Language Models (LLMs) to break them down into subsets of questions and answers. However, these methods primarily operate on the surface level, failing to incorporate deeper lexical understanding while introducing incorrect assumptions generated by the LLM. In response to these issues, we present Caption Expansion with Contradictions and Entailments (CECE), a principled approach that leverages Natural Language Inference (NLI) to generate entailments and contradictions from a given premise. CECE produces lexically diverse sentences while maintaining their core meaning. Through extensive experiments, we show that CECE enhances interpretability and reduces overreliance on biased or superficial features. By balancing CECE along the original premise, we achieve significant improvements over previous methods without requiring additional fine-tuning, producing state-of-the-art results on benchmarks that score agreement with human judgments for image-text alignment, and achieving an increase in performance on Winoground of +19.2% (group score) and +12.9% on EqBen (group score) over the best prior work (finetuned with targeted data).</li>
</ul>

<h3>Title: Online Detecting LLM-Generated Texts via Sequential Hypothesis Testing by Betting</h3>
<ul>
<li><strong>Authors: </strong>Can Chen, Jun-Kun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22318">https://arxiv.org/abs/2410.22318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22318">https://arxiv.org/pdf/2410.22318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22318]] Online Detecting LLM-Generated Texts via Sequential Hypothesis Testing by Betting(https://arxiv.org/abs/2410.22318)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Developing algorithms to differentiate between machine-generated texts and human-written texts has garnered substantial attention in recent years. Existing methods in this direction typically concern an offline setting where a dataset containing a mix of real and machine-generated texts is given upfront, and the task is to determine whether each sample in the dataset is from a large language model (LLM) or a human. However, in many practical scenarios, sources such as news websites, social media accounts, or on other forums publish content in a streaming fashion. Therefore, in this online scenario, how to quickly and accurately determine whether the source is an LLM with strong statistical guarantees is crucial for these media or platforms to function effectively and prevent the spread of misinformation and other potential misuse of LLMs. To tackle the problem of online detection, we develop an algorithm based on the techniques of sequential hypothesis testing by betting that not only builds upon and complements existing offline detection techniques but also enjoys statistical guarantees, which include a controlled false positive rate and the expected time to correctly identify a source as an LLM. Experiments were conducted to demonstrate the effectiveness of our method.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
