<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-06-18</h1>
<h3>Title: CDST: Color Disentangled Style Transfer for Universal Style Reference Customization</h3>
<ul>
<li><strong>Authors: </strong>Shiwen Zhang, Zhuowei Chen, Lang Chen, Yanze Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13770">https://arxiv.org/abs/2506.13770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13770">https://arxiv.org/pdf/2506.13770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13770]] CDST: Color Disentangled Style Transfer for Universal Style Reference Customization(https://arxiv.org/abs/2506.13770)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Color Disentangled Style Transfer (CDST), a novel and efficient two-stream style transfer training paradigm which completely isolates color from style and forces the style stream to be color-blinded. With one same model, CDST unlocks universal style transfer capabilities in a tuning-free manner during inference. Especially, the characteristics-preserved style transfer with style and content references is solved in the tuning-free way for the first time. CDST significantly improves the style similarity by multi-feature image embeddings compression and preserves strong editing capability via our new CDST style definition inspired by Diffusion UNet disentanglement law. By conducting thorough qualitative and quantitative experiments and human evaluations, we demonstrate that CDST achieves state-of-the-art results on various style transfer tasks.</li>
</ul>

<h3>Title: LittleBit: Ultra Low-Bit Quantization via Latent Factorization</h3>
<ul>
<li><strong>Authors: </strong>Banseok Lee, Dongkyu Kim, Youngcheon You, Youngmin Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13771">https://arxiv.org/abs/2506.13771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13771">https://arxiv.org/pdf/2506.13771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13771]] LittleBit: Ultra Low-Bit Quantization via Latent Factorization(https://arxiv.org/abs/2506.13771)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. This establishes a superior size-performance trade-off, with kernel-level benchmarks indicating potential for a 5$\times$ speedup compared to FP16. LittleBit paves the way for deploying powerful LLMs in resource-constrained environments.</li>
</ul>

<h3>Title: MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhenyan Lu, Daliang Xu, Dongqi Cai, Zexi Li, Wei Liu, Fangming Liu, Shangguang Wang, Mengwei Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13772">https://arxiv.org/abs/2506.13772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13772">https://arxiv.org/pdf/2506.13772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13772]] MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs(https://arxiv.org/abs/2506.13772)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are deployed on mobile devices to power killer applications such as intelligent assistants. LLMs pre-trained on general corpora often hallucinate when handling personalized or unseen queries, leading to incorrect or outdated responses. Knowledge editing addresses this by identifying and adjusting a small crucial portion of model weights, without compromising the general knowledge. However, prior knowledge editing methods are impractical to run on local devices due to the resource-heavy backpropagation (BP) needed for updates. We present MobiEdit, the first mobile knowledge editing framework that enables efficient LLM personalization on commercial off-the-shelf (COTS) mobile devices. MobiEdit replaces full-precision BP with quantized forward-only gradient estimation, thus compatible with the energy-efficient mobile neural processing units (NPUs). MobiEdit replaces full-precision backpropagation with quantized forward-only gradient estimation, making it compatible with energy-efficient mobile NPUs. To further improve gradient estimation efficiency, we introduce two optimizations: an early stoping mechanism that adaptively terminates editing upon success and a prefix cache that reuses computation across steps. Our approach enables real-time editing of a 3B-parameter model (Qwen2.5-3B-Instruct) on COTS mobile devices with 7.6$\times$ less memory, 14.7 $\times$ less energy and 3.6$\times$ less latency compared to previous knowledge editing methods.</li>
</ul>

<h3>Title: Hidden Bias in the Machine: Stereotypes in Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Sedat Porikli, Vedat Porikli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13780">https://arxiv.org/abs/2506.13780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13780">https://arxiv.org/pdf/2506.13780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13780]] Hidden Bias in the Machine: Stereotypes in Text-to-Image Models(https://arxiv.org/abs/2506.13780)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.</li>
</ul>

<h3>Title: ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries</h3>
<ul>
<li><strong>Authors: </strong>Zhou Chen, Xiao Wang, Yuanhong Liao, Ming Lin, Yuqi Bai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13796">https://arxiv.org/abs/2506.13796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13796">https://arxiv.org/pdf/2506.13796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13796]] ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries(https://arxiv.org/abs/2506.13796)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.</li>
</ul>

<h3>Title: Hybrid Meta-Learning Framework for Anomaly Forecasting in Nonlinear Dynamical Systems via Physics-Inspired Simulation and Deep Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Abdullah Burkan Bereketoglu</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, physics.acc-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13828">https://arxiv.org/abs/2506.13828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13828">https://arxiv.org/pdf/2506.13828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13828]] Hybrid Meta-Learning Framework for Anomaly Forecasting in Nonlinear Dynamical Systems via Physics-Inspired Simulation and Deep Ensembles(https://arxiv.org/abs/2506.13828)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>We propose a hybrid meta-learning framework for forecasting and anomaly detection in nonlinear dynamical systems characterized by nonstationary and stochastic behavior. The approach integrates a physics-inspired simulator that captures nonlinear growth-relaxation dynamics with random perturbations, representative of many complex physical, industrial, and cyber-physical systems. We use CNN-LSTM architectures for spatio-temporal feature extraction, Variational Autoencoders (VAE) for unsupervised anomaly scoring, and Isolation Forests for residual-based outlier detection in addition to a Dual-Stage Attention Recurrent Neural Network (DA-RNN) for one-step forecasting on top of the generated simulation data. To create composite anomaly forecasts, these models are combined using a meta-learner that combines forecasting outputs, reconstruction errors, and residual scores. The hybrid ensemble performs better than standalone models in anomaly localization, generalization, and robustness to nonlinear deviations, according to simulation-based experiments. The framework provides a broad, data-driven approach to early defect identification and predictive monitoring in nonlinear systems, which may be applied to a variety of scenarios where complete physical models might not be accessible.</li>
</ul>

<h3>Title: Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Jitian Zhao, Chenghui Li, Frederic Sala, Karl Rohe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13831">https://arxiv.org/abs/2506.13831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13831">https://arxiv.org/pdf/2506.13831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13831]] Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation(https://arxiv.org/abs/2506.13831)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Concept-based approaches, which aim to identify human-understandable concepts within a model's internal representations, are a promising method for interpreting embeddings from deep neural network models, such as CLIP. While these approaches help explain model behavior, current methods lack statistical rigor, making it challenging to validate identified concepts and compare different techniques. To address this challenge, we introduce a hypothesis testing framework that quantifies rotation-sensitive structures within the CLIP embedding space. Once such structures are identified, we propose a post-hoc concept decomposition method. Unlike existing approaches, it offers theoretical guarantees that discovered concepts represent robust, reproducible patterns (rather than method-specific artifacts) and outperforms other techniques in terms of reconstruction error. Empirically, we demonstrate that our concept-based decomposition algorithm effectively balances reconstruction accuracy with concept interpretability and helps mitigate spurious cues in data. Applied to a popular spurious correlation dataset, our method yields a 22.6% increase in worst-group accuracy after removing spurious background concepts.</li>
</ul>

<h3>Title: Evolvable Conditional Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Zhao Wei, Chin Chun Ooi, Abhishek Gupta, Jian Cheng Wong, Pao-Hsiung Chiu, Sheares Xue Wen Toh, Yew-Soon Ong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13834">https://arxiv.org/abs/2506.13834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13834">https://arxiv.org/pdf/2506.13834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13834]] Evolvable Conditional Diffusion(https://arxiv.org/abs/2506.13834)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper presents an evolvable conditional diffusion method such that black-box, non-differentiable multi-physics models, as are common in domains like computational fluid dynamics and electromagnetics, can be effectively used for guiding the generative process to facilitate autonomous scientific discovery. We formulate the guidance as an optimization problem where one optimizes for a desired fitness function through updates to the descriptive statistic for the denoising distribution, and derive an evolution-guided approach from first principles through the lens of probabilistic evolution. Interestingly, the final derived update algorithm is analogous to the update as per common gradient-based guided diffusion models, but without ever having to compute any derivatives. We validate our proposed evolvable diffusion algorithm in two AI for Science scenarios: the automated design of fluidic topology and meta-surface. Results demonstrate that this method effectively generates designs that better satisfy specific optimization objectives without reliance on differentiable proxies, providing an effective means of guidance-based diffusion that can capitalize on the wealth of black-box, non-differentiable multi-physics numerical models common across Science.</li>
</ul>

<h3>Title: Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study</h3>
<ul>
<li><strong>Authors: </strong>Dang Viet Anh Nguyen, Carlos Lima Azevedo, Tomer Toledo, Filipe Rodrigues</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13836">https://arxiv.org/abs/2506.13836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13836">https://arxiv.org/pdf/2506.13836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13836]] Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study(https://arxiv.org/abs/2506.13836)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a promising approach for improving urban mobility. However, its robustness under real-world disruptions such as traffic incidents remains largely underexplored. In this study, we introduce T-REX, an open-source, SUMO-based simulation framework for training and evaluating RL-TSC methods under dynamic, incident scenarios. T-REX models realistic network-level performance considering drivers' probabilistic rerouting, speed adaptation, and contextual lane-changing, enabling the simulation of congestion propagation under incidents. To assess robustness, we propose a suite of metrics that extend beyond conventional traffic efficiency measures. Through extensive experiments across synthetic and real-world networks, we showcase T-REX for the evaluation of several state-of-the-art RL-TSC methods under multiple real-world deployment paradigms. Our findings show that while independent value-based and decentralized pressure-based methods offer fast convergence and generalization in stable traffic conditions and homogeneous networks, their performance degrades sharply under incident-driven distribution shifts. In contrast, hierarchical coordination methods tend to offer more stable and adaptable performance in large-scale, irregular networks, benefiting from their structured decision-making architecture. However, this comes with the trade-off of slower convergence and higher training complexity. These findings highlight the need for robustness-aware design and evaluation in RL-TSC research. T-REX contributes to this effort by providing an open, standardized and reproducible platform for benchmarking RL methods under dynamic and disruptive traffic scenarios.</li>
</ul>

<h3>Title: Fake it till You Make it: Reward Modeling as Discriminative Prediction</h3>
<ul>
<li><strong>Authors: </strong>Runtao Liu, Jiahao Zhan, Yingqing He, Chen Wei, Alan Yuille, Qifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13846">https://arxiv.org/abs/2506.13846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13846">https://arxiv.org/pdf/2506.13846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13846]] Fake it till You Make it: Reward Modeling as Discriminative Prediction(https://arxiv.org/abs/2506.13846)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).</li>
</ul>

<h3>Title: StaQ it! Growing neural networks for Policy Mirror Descent</h3>
<ul>
<li><strong>Authors: </strong>Alena Shilova, Alex Davey, Brahim Driss, Riad Akrour</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13862">https://arxiv.org/abs/2506.13862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13862">https://arxiv.org/pdf/2506.13862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13862]] StaQ it! Growing neural networks for Policy Mirror Descent(https://arxiv.org/abs/2506.13862)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In Reinforcement Learning (RL), regularization has emerged as a popular tool both in theory and practice, typically based either on an entropy bonus or a Kullback-Leibler divergence that constrains successive policies. In practice, these approaches have been shown to improve exploration, robustness and stability, giving rise to popular Deep RL algorithms such as SAC and TRPO. Policy Mirror Descent (PMD) is a theoretical framework that solves this general regularized policy optimization problem, however the closed-form solution involves the sum of all past Q-functions, which is intractable in practice. We propose and analyze PMD-like algorithms that only keep the last $M$ Q-functions in memory, and show that for finite and large enough $M$, a convergent algorithm can be derived, introducing no error in the policy update, unlike prior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong theoretical guarantees and is competitive with deep RL baselines, while exhibiting less performance oscillation, paving the way for fully stable deep RL algorithms and providing a testbed for experimentation with Policy Mirror Descent.</li>
</ul>

<h3>Title: Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles</h3>
<ul>
<li><strong>Authors: </strong>Antara Raaghavi Bhattacharya, Isabel Papadimitriou, Kathryn Davidson, David Alvarez-Melis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13886">https://arxiv.org/abs/2506.13886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13886">https://arxiv.org/pdf/2506.13886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13886]] Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles(https://arxiv.org/abs/2506.13886)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\times$, etc, as in "twenty + three"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.</li>
</ul>

<h3>Title: VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training</h3>
<ul>
<li><strong>Authors: </strong>Jipeng Zhang, Kehao Miao, Renjie Pi, Zhaowei Wang, Runtao Liu, Rui Pan, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13888">https://arxiv.org/abs/2506.13888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13888">https://arxiv.org/pdf/2506.13888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13888]] VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training(https://arxiv.org/abs/2506.13888)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large language models but remains underexplored for Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing structured feedback, yet training effective VL-RMs faces two major challenges. First, the bootstrapping dilemma arises as high-quality training data depends on already strong VL models, creating a cycle where self-generated supervision reinforces existing biases. Second, modality bias and negative example amplification occur when VL models hallucinate incorrect visual attributes, leading to flawed preference data that further misguides training. To address these issues, we propose an iterative training framework leveraging vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling. Our approach refines preference datasets, enhances structured critiques, and iteratively improves reasoning. Experiments across VL-RM benchmarks demonstrate superior performance in hallucination detection and multimodal reasoning, advancing VL model alignment with reinforcement learning.</li>
</ul>

<h3>Title: Scaling Algorithm Distillation for Continuous Control with Mamba</h3>
<ul>
<li><strong>Authors: </strong>Samuel Beaussant, Mehdi Mounsif</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13892">https://arxiv.org/abs/2506.13892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13892">https://arxiv.org/pdf/2506.13892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13892]] Scaling Algorithm Distillation for Continuous Control with Mamba(https://arxiv.org/abs/2506.13892)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Algorithm Distillation (AD) was recently proposed as a new approach to perform In-Context Reinforcement Learning (ICRL) by modeling across-episodic training histories autoregressively with a causal transformer model. However, due to practical limitations induced by the attention mechanism, experiments were bottlenecked by the transformer's quadratic complexity and limited to simple discrete environments with short time horizons. In this work, we propose leveraging the recently proposed Selective Structured State Space Sequence (S6) models, which achieved state-of-the-art (SOTA) performance on long-range sequence modeling while scaling linearly in sequence length. Through four complex and continuous Meta Reinforcement Learning environments, we demonstrate the overall superiority of Mamba, a model built with S6 layers, over a transformer model for AD. Additionally, we show that scaling AD to very long contexts can improve ICRL performance and make it competitive even with a SOTA online meta RL baseline.</li>
</ul>

<h3>Title: EmoNews: A Spoken Dialogue System for Expressive News Conversations</h3>
<ul>
<li><strong>Authors: </strong>Ryuki Matsuura, Shikhar Bharadwaj, Jiarui Liu, Dhatchi Kunde Govindarajan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13894">https://arxiv.org/abs/2506.13894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13894">https://arxiv.org/pdf/2506.13894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13894]] EmoNews: A Spoken Dialogue System for Expressive News Conversations(https://arxiv.org/abs/2506.13894)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We develop a task-oriented spoken dialogue system (SDS) that regulates emotional speech based on contextual cues to enable more empathetic news conversations. Despite advancements in emotional text-to-speech (TTS) techniques, task-oriented emotional SDSs remain underexplored due to the compartmentalized nature of SDS and emotional TTS research, as well as the lack of standardized evaluation metrics for social goals. We address these challenges by developing an emotional SDS for news conversations that utilizes a large language model (LLM)-based sentiment analyzer to identify appropriate emotions and PromptTTS to synthesize context-appropriate emotional speech. We also propose subjective evaluation scale for emotional SDSs and judge the emotion regulation performance of the proposed and baseline systems. Experiments showed that our emotional SDS outperformed a baseline system in terms of the emotion regulation and engagement. These results suggest the critical role of speech emotion for more engaging conversations. All our source code is open-sourced at this https URL</li>
</ul>

<h3>Title: A Dual-Layer Image Encryption Framework Using Chaotic AES with Dynamic S-Boxes and Steganographic QR Codes</h3>
<ul>
<li><strong>Authors: </strong>Md Rishadul Bayesh, Dabbrata Das, Md Ahadullah</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13895">https://arxiv.org/abs/2506.13895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13895">https://arxiv.org/pdf/2506.13895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13895]] A Dual-Layer Image Encryption Framework Using Chaotic AES with Dynamic S-Boxes and Steganographic QR Codes(https://arxiv.org/abs/2506.13895)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, robust, diffusion</a></li>
<li><strong>Abstract: </strong>This paper presents a robust image encryption and key distribution framework that integrates an enhanced AES-128 algorithm with chaos theory and advanced steganographic techniques for dual-layer security. The encryption engine features a dynamic ShiftRows operation controlled by a logistic map, variable S-boxes generated from a two-dimensional Henon map for substitution and key expansion, and feedback chaining with post-encryption XOR diffusion to improve confusion, diffusion, and key sensitivity. To address secure key delivery, the scheme introduces dual-key distribution via steganographically modified QR codes. A static key and an AES-encrypted dynamic session key are embedded with a covert hint message using least significant bit (LSB) steganography. This design ensures the dynamic key can only be decrypted after reconstructing the static key from the hidden message, offering multi-factor protection against interception. Experimental results demonstrate the framework outperforms existing chaos-based and hybrid AES methods, achieving near-ideal entropy (7.997), minimal pixel correlation, and strong differential resistance with NPCR (>99.6%) and UACI (50.1%). Encrypted images show uniform histograms and robustness against noise and data loss. The framework offers a scalable, secure solution for sensitive image transmission in applications such as surveillance, medical imaging, and digital forensics, bridging the gap between cryptographic strength and safe key distribution.</li>
</ul>

<h3>Title: DeSPITE: Exploring Contrastive Deep Skeleton-Pointcloud-IMU-Text Embeddings for Advanced Point Cloud Human Activity Understanding</h3>
<ul>
<li><strong>Authors: </strong>Thomas Kreutz, Max Mühlhäuser, Alejandro Sanchez Guinea</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13897">https://arxiv.org/abs/2506.13897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13897">https://arxiv.org/pdf/2506.13897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13897]] DeSPITE: Exploring Contrastive Deep Skeleton-Pointcloud-IMU-Text Embeddings for Advanced Point Cloud Human Activity Understanding(https://arxiv.org/abs/2506.13897)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Despite LiDAR (Light Detection and Ranging) being an effective privacy-preserving alternative to RGB cameras to perceive human activities, it remains largely underexplored in the context of multi-modal contrastive pre-training for human activity understanding (e.g., human activity recognition (HAR), retrieval, or person re-identification (RE-ID)). To close this gap, our work explores learning the correspondence between LiDAR point clouds, human skeleton poses, IMU data, and text in a joint embedding space. More specifically, we present DeSPITE, a Deep Skeleton-Pointcloud-IMU-Text Embedding model, which effectively learns a joint embedding space across these four modalities through noise contrastive estimation. At the heart of our empirical exploration, we have combined the existing LIPD and Babel datasets, which enabled us to synchronize data of all four modalities, allowing us to explore the learning of a new joint embedding space. Our experiments demonstrate novel human activity understanding tasks for point cloud sequences enabled through DeSPITE, including Skeleton<->Pointcloud<->IMU matching, retrieval, and temporal moment retrieval. Furthermore, we show that DeSPITE is an effective pre-training strategy for point cloud HAR through experiments in MSR-Action3D and HMPEAR.</li>
</ul>

<h3>Title: Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations</h3>
<ul>
<li><strong>Authors: </strong>Abhilekh Borah, Chhavi Sharma, Danush Khanna, Utkarsh Bhatt, Gurpreet Singh, Hasnat Md Abdullah, Raghav Kaushik Ravi, Vinija Jain, Jyoti Patel, Shubham Singh, Vasu Sharma, Arpita Vats, Rahul Raja, Aman Chadha, Amitava Das</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13901">https://arxiv.org/abs/2506.13901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13901">https://arxiv.org/pdf/2506.13901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13901]] Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations(https://arxiv.org/abs/2506.13901)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking. To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing. Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.</li>
</ul>

<h3>Title: Enhancing interpretability of rule-based classifiers through feature graphs</h3>
<ul>
<li><strong>Authors: </strong>Christel Sirocchi, Damiano Verda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13903">https://arxiv.org/abs/2506.13903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13903">https://arxiv.org/pdf/2506.13903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13903]] Enhancing interpretability of rule-based classifiers through feature graphs(https://arxiv.org/abs/2506.13903)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability</a></li>
<li><strong>Abstract: </strong>In domains where transparency and trustworthiness are crucial, such as healthcare, rule-based systems are widely used and often preferred over black-box models for decision support systems due to their inherent interpretability. However, as rule-based models grow complex, discerning crucial features, understanding their interactions, and comparing feature contributions across different rule sets becomes challenging. To address this, we propose a comprehensive framework for estimating feature contributions in rule-based systems, introducing a graph-based feature visualisation strategy, a novel feature importance metric agnostic to rule-based predictors, and a distance metric for comparing rule sets based on feature contributions. By experimenting on two clinical datasets and four rule-based methods (decision trees, logic learning machines, association rules, and neural networks with rule extraction), we showcase our method's capability to uncover novel insights on the combined predictive value of clinical features, both at the dataset and class-specific levels. These insights can aid in identifying new risk factors, signature genes, and potential biomarkers, and determining the subset of patient information that should be prioritised to enhance diagnostic accuracy. Comparative analysis of the proposed feature importance score with state-of-the-art methods on 15 public benchmarks demonstrates competitive performance and superior robustness. The method implementation is available on GitHub: this https URL.</li>
</ul>

<h3>Title: GITO: Graph-Informed Transformer Operator for Learning Complex Partial Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Milad Ramezankhani, Janak M. Patel, Anirudh Deodhar, Dagnachew Birru</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13906">https://arxiv.org/abs/2506.13906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13906">https://arxiv.org/pdf/2506.13906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13906]] GITO: Graph-Informed Transformer Operator for Learning Complex Partial Differential Equations(https://arxiv.org/abs/2506.13906)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present a novel graph-informed transformer operator (GITO) architecture for learning complex partial differential equation systems defined on irregular geometries and non-uniform meshes. GITO consists of two main modules: a hybrid graph transformer (HGT) and a transformer neural operator (TNO). HGT leverages a graph neural network (GNN) to encode local spatial relationships and a transformer to capture long-range dependencies. A self-attention fusion layer integrates the outputs of the GNN and transformer to enable more expressive feature learning on graph-structured data. TNO module employs linear-complexity cross-attention and self-attention layers to map encoded input functions to predictions at arbitrary query locations, ensuring discretization invariance and enabling zero-shot super-resolution across any mesh. Empirical results on benchmark PDE tasks demonstrate that GITO outperforms existing transformer-based neural operators, paving the way for efficient, mesh-agnostic surrogate solvers in engineering applications.</li>
</ul>

<h3>Title: Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using the Example of Screw-Fastening Process Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Xinyuan Tu, Haocheng Zhang, Tao Chengxu, Zuyi Chen (Friedrich-Alexander-Universität Erlangen, Germany)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13909">https://arxiv.org/abs/2506.13909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13909">https://arxiv.org/pdf/2506.13909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13909]] Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using the Example of Screw-Fastening Process Monitoring(https://arxiv.org/abs/2506.13909)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Few-shot learning (FSL) has shown promise in vision but remains largely unexplored for \emph{industrial} time-series data, where annotating every new defect is prohibitively expensive. We present a systematic FSL study on screw-fastening process monitoring, using a 2\,300-sample multivariate torque dataset that covers 16 uni- and multi-factorial defect types. Beyond benchmarking, we introduce a \textbf{label-aware episodic sampler} that collapses multi-label sequences into multiple single-label tasks, keeping the output dimensionality fixed while preserving combinatorial label information. Two FSL paradigms are investigated: the metric-based \emph{Prototypical Network} and the gradient-based \emph{Model-Agnostic Meta-Learning} (MAML), each paired with three backbones: 1D CNN, InceptionTime and the 341 M-parameter transformer \emph{Moment}. On 10-shot, 3-way evaluation, the InceptionTime + Prototypical Network combination achieves a \textbf{0.944 weighted F1} in the multi-class regime and \textbf{0.935} in the multi-label regime, outperforming finetuned Moment by up to 5.3\% while requiring two orders of magnitude fewer parameters and training time. Across all backbones, metric learning consistently surpasses MAML, and our label-aware sampling yields an additional 1.7\% F1 over traditional class-based sampling. These findings challenge the assumption that large foundation models are always superior: when data are scarce, lightweight CNN architectures augmented with simple metric learning not only converge faster but also generalize better. We release code, data splits and pre-trained weights to foster reproducible research and to catalyze the adoption of FSL in high-value manufacturing inspection.</li>
</ul>

<h3>Title: Intelligent Image Sensing for Crime Analysis: A ML Approach towards Enhanced Violence Detection and Investigation</h3>
<ul>
<li><strong>Authors: </strong>Aritra Dutta, Pushpita Boral, G Suseela</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13910">https://arxiv.org/abs/2506.13910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13910">https://arxiv.org/pdf/2506.13910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13910]] Intelligent Image Sensing for Crime Analysis: A ML Approach towards Enhanced Violence Detection and Investigation(https://arxiv.org/abs/2506.13910)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The increasing global crime rate, coupled with substantial human and property losses, highlights the limitations of traditional surveillance methods in promptly detecting diverse and unexpected acts of violence. Addressing this pressing need for automatic violence detection, we leverage Machine Learning to detect and categorize violent events in video streams. This paper introduces a comprehensive framework for violence detection and classification, employing Supervised Learning for both binary and multi-class violence classification. The detection model relies on 3D Convolutional Neural Networks, while the classification model utilizes the separable convolutional 3D model for feature extraction and bidirectional LSTM for temporal processing. Training is conducted on a diverse customized datasets with frame-level annotations, incorporating videos from surveillance cameras, human recordings, hockey fight, sohas and wvd dataset across various platforms. Additionally, a camera module integrated with raspberry pi is used to capture live video feed, which is sent to the ML model for processing. Thus, demonstrating improved performance in terms of computational resource efficiency and accuracy.</li>
</ul>

<h3>Title: HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment</h3>
<ul>
<li><strong>Authors: </strong>Numair Nadeem, Saeed Anwar, Muhammad Hamza Asad, Abdul Bais</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13925">https://arxiv.org/abs/2506.13925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13925">https://arxiv.org/pdf/2506.13925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13925]] HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment(https://arxiv.org/abs/2506.13925)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification between similar classes, poor generalization and boundary localization. Vision-Language Models offer robust, domain-invariant semantics but lack the spatial grounding required for dense prediction. We introduce HierVL, a unified framework that bridges this gap by integrating abstract text embeddings into a mask-transformer architecture tailored for semi-supervised segmentation. HierVL features three novel components: a Hierarchical Semantic Query Generator that filters and projects abstract class embeddings into multi-scale queries to suppress irrelevant classes and handle intra-class variability; a Cross-Modal Spatial Alignment Module that aligns semantic queries with pixel features for sharper boundaries under sparse supervision; and a Dual-Query Transformer Decoder that fuses semantic and instance-level queries to prevent instance collapse. We also introduce targeted regularization losses that maintain vision-language alignment throughout training to reinforce semantic grounding. HierVL establishes a new state-of-the-art by achieving a +4.4% mean improvement of the intersection over the union on COCO (with 232 labeled images), +3.1% on Pascal VOC (with 92 labels), +5.9% on ADE20 (with 158 labels) and +1.8% on Cityscapes (with 100 labels), demonstrating better performance under 1% supervision on four benchmark datasets. Our results show that language-guided segmentation closes the label efficiency gap and unlocks new levels of fine-grained, instance-aware generalization.</li>
</ul>

<h3>Title: ReinDSplit: Reinforced Dynamic Split Learning for Pest Recognition in Precision Agriculture</h3>
<ul>
<li><strong>Authors: </strong>Vishesh Kumar Tanwar, Soumik Sarkar, Asheesh K. Singh, Sajal K. Das</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13935">https://arxiv.org/abs/2506.13935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13935">https://arxiv.org/pdf/2506.13935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13935]] ReinDSplit: Reinforced Dynamic Split Learning for Pest Recognition in Precision Agriculture(https://arxiv.org/abs/2506.13935)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>To empower precision agriculture through distributed machine learning (DML), split learning (SL) has emerged as a promising paradigm, partitioning deep neural networks (DNNs) between edge devices and servers to reduce computational burdens and preserve data privacy. However, conventional SL frameworks' one-split-fits-all strategy is a critical limitation in agricultural ecosystems where edge insect monitoring devices exhibit vast heterogeneity in computational power, energy constraints, and connectivity. This leads to straggler bottlenecks, inefficient resource utilization, and compromised model performance. Bridging this gap, we introduce ReinDSplit, a novel reinforcement learning (RL)-driven framework that dynamically tailors DNN split points for each device, optimizing efficiency without sacrificing accuracy. Specifically, a Q-learning agent acts as an adaptive orchestrator, balancing workloads and latency thresholds across devices to mitigate computational starvation or overload. By framing split layer selection as a finite-state Markov decision process, ReinDSplit convergence ensures that highly constrained devices contribute meaningfully to model training over time. Evaluated on three insect classification datasets using ResNet18, GoogleNet, and MobileNetV2, ReinDSplit achieves 94.31% accuracy with MobileNetV2. Beyond agriculture, ReinDSplit pioneers a paradigm shift in SL by harmonizing RL for resource efficiency, privacy, and scalability in heterogeneous environments.</li>
</ul>

<h3>Title: ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection</h3>
<ul>
<li><strong>Authors: </strong>Shang-Chi Tsai, Seiya Kawano, Angel Garcia Contreras, Koichiro Yoshino, Yun-Nung Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13956">https://arxiv.org/abs/2506.13956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13956">https://arxiv.org/pdf/2506.13956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13956]] ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection(https://arxiv.org/abs/2506.13956)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.</li>
</ul>

<h3>Title: Toward Explainable Offline RL: Analyzing Representations in Intrinsically Motivated Decision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Guiducci, Antonio Rizzo, Giovanna Maria Dimitri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13958">https://arxiv.org/abs/2506.13958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13958">https://arxiv.org/pdf/2506.13958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13958]] Toward Explainable Offline RL: Analyzing Representations in Intrinsically Motivated Decision Transformers(https://arxiv.org/abs/2506.13958)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer</a></li>
<li><strong>Abstract: </strong>Elastic Decision Transformers (EDTs) have proved to be particularly successful in offline reinforcement learning, offering a flexible framework that unifies sequence modeling with decision-making under uncertainty. Recent research has shown that incorporating intrinsic motivation mechanisms into EDTs improves performance across exploration tasks, yet the representational mechanisms underlying these improvements remain unexplored. In this paper, we introduce a systematic post-hoc explainability framework to analyze how intrinsic motivation shapes learned embeddings in EDTs. Through statistical analysis of embedding properties (including covariance structure, vector magnitudes, and orthogonality), we reveal that different intrinsic motivation variants create fundamentally different representational structures. Our analysis demonstrates environment-specific correlation patterns between embedding metrics and performance that explain why intrinsic motivation improves policy learning. These findings show that intrinsic motivation operates beyond simple exploration bonuses, acting as a representational prior that shapes embedding geometry in biologically plausible ways, creating environment-specific organizational structures that facilitate better decision-making.</li>
</ul>

<h3>Title: Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble</h3>
<ul>
<li><strong>Authors: </strong>Zhiqi Wang, Chengyu Zhang, Yuetian Chen, Nathalie Baracaldo, Swanand Kadhe, Lei Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13972">https://arxiv.org/abs/2506.13972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13972">https://arxiv.org/pdf/2506.13972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13972]] Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble(https://arxiv.org/abs/2506.13972)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, membership infer</a></li>
<li><strong>Abstract: </strong>Membership inference attacks (MIAs) pose a significant threat to the privacy of machine learning models and are widely used as tools for privacy assessment, auditing, and machine unlearning. While prior MIA research has primarily focused on performance metrics such as AUC, accuracy, and TPR@low FPR - either by developing new methods to enhance these metrics or using them to evaluate privacy solutions - we found that it overlooks the disparities among different attacks. These disparities, both between distinct attack methods and between multiple instantiations of the same method, have crucial implications for the reliability and completeness of MIAs as privacy evaluation tools. In this paper, we systematically investigate these disparities through a novel framework based on coverage and stability analysis. Extensive experiments reveal significant disparities in MIAs, their potential causes, and their broader implications for privacy evaluation. To address these challenges, we propose an ensemble framework with three distinct strategies to harness the strengths of state-of-the-art MIAs while accounting for their disparities. This framework not only enables the construction of more powerful attacks but also provides a more robust and comprehensive methodology for privacy evaluation.</li>
</ul>

<h3>Title: AI shares emotion with humans across languages and cultures</h3>
<ul>
<li><strong>Authors: </strong>Xiuwen Wu, Hao Wang, Zhiang Yan, Xiaohan Tang, Pengfei Xu, Wai-Ting Siok, Ping Li, Jia-Hong Gao, Bingjiang Lyu, Lang Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13978">https://arxiv.org/abs/2506.13978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13978">https://arxiv.org/pdf/2506.13978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13978]] AI shares emotion with humans across languages and cultures(https://arxiv.org/abs/2506.13978)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Effective and safe human-machine collaboration requires the regulated and meaningful exchange of emotions between humans and artificial intelligence (AI). Current AI systems based on large language models (LLMs) can provide feedback that makes people feel heard. Yet it remains unclear whether LLMs represent emotion in language as humans do, or whether and how the emotional tone of their output can be controlled. We assess human-AI emotional alignment across linguistic-cultural groups and model-families, using interpretable LLM features translated from concept-sets for over twenty nuanced emotion categories (including six basic emotions). Our analyses reveal that LLM-derived emotion spaces are structurally congruent with human perception, underpinned by the fundamental affective dimensions of valence and arousal. Furthermore, these emotion-related features also accurately predict large-scale behavioural data on word ratings along these two core dimensions, reflecting both universal and language-specific patterns. Finally, by leveraging steering vectors derived solely from human-centric emotion concepts, we show that model expressions can be stably and naturally modulated across distinct emotion categories, which provides causal evidence that human emotion concepts can be used to systematically induce LLMs to produce corresponding affective states when conveying content. These findings suggest AI not only shares emotional representations with humans but its affective outputs can be precisely guided using psychologically grounded emotion concepts.</li>
</ul>

<h3>Title: HAELT: A Hybrid Attentive Ensemble Learning Transformer Framework for High-Frequency Stock Price Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Thanh Dan Bui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13981">https://arxiv.org/abs/2506.13981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13981">https://arxiv.org/pdf/2506.13981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13981]] HAELT: A Hybrid Attentive Ensemble Learning Transformer Framework for High-Frequency Stock Price Forecasting(https://arxiv.org/abs/2506.13981)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>High-frequency stock price prediction is challenging due to non-stationarity, noise, and volatility. To tackle these issues, we propose the Hybrid Attentive Ensemble Learning Transformer (HAELT), a deep learning framework combining a ResNet-based noise-mitigation module, temporal self-attention for dynamic focus on relevant history, and a hybrid LSTM-Transformer core that captures both local and long-range dependencies. These components are adaptively ensembled based on recent performance. Evaluated on hourly Apple Inc. (AAPL) data from Jan 2024 to May 2025, HAELT achieves the highest F1-Score on the test set, effectively identifying both upward and downward price movements. This demonstrates HAELT's potential for robust, practical financial forecasting and algorithmic trading.</li>
</ul>

<h3>Title: Quantum-Informed Contrastive Learning with Dynamic Mixup Augmentation for Class-Imbalanced Expert Systems</h3>
<ul>
<li><strong>Authors: </strong>Md Abrar Jahin, Adiba Abid, M. F. Mridha</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13987">https://arxiv.org/abs/2506.13987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13987">https://arxiv.org/pdf/2506.13987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13987]] Quantum-Informed Contrastive Learning with Dynamic Mixup Augmentation for Class-Imbalanced Expert Systems(https://arxiv.org/abs/2506.13987)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Expert systems often operate in domains characterized by class-imbalanced tabular data, where detecting rare but critical instances is essential for safety and reliability. While conventional approaches, such as cost-sensitive learning, oversampling, and graph neural networks, provide partial solutions, they suffer from drawbacks like overfitting, label noise, and poor generalization in low-density regions. To address these challenges, we propose QCL-MixNet, a novel Quantum-Informed Contrastive Learning framework augmented with k-nearest neighbor (kNN) guided dynamic mixup for robust classification under imbalance. QCL-MixNet integrates three core innovations: (i) a Quantum Entanglement-inspired layer that models complex feature interactions through sinusoidal transformations and gated attention, (ii) a sample-aware mixup strategy that adaptively interpolates feature representations of semantically similar instances to enhance minority class representation, and (iii) a hybrid loss function that unifies focal reweighting, supervised contrastive learning, triplet margin loss, and variance regularization to improve both intra-class compactness and inter-class separability. Extensive experiments on 18 real-world imbalanced datasets (binary and multi-class) demonstrate that QCL-MixNet consistently outperforms 20 state-of-the-art machine learning, deep learning, and GNN-based baselines in macro-F1 and recall, often by substantial margins. Ablation studies further validate the critical role of each architectural component. Our results establish QCL-MixNet as a new benchmark for tabular imbalance handling in expert systems. Theoretical analyses reinforce its expressiveness, generalization, and optimization robustness.</li>
</ul>

<h3>Title: AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science</h3>
<ul>
<li><strong>Authors: </strong>An Luo, Xun Xian, Jin Du, Fangqiao Tian, Ganghua Wang, Ming Zhong, Shengchun Zhao, Xuan Bi, Zirui Liu, Jiawei Zhou, Jayanth Srinivasa, Ashish Kundu, Charles Fleming, Mingyi Hong, Jie Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13992">https://arxiv.org/abs/2506.13992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13992">https://arxiv.org/pdf/2506.13992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13992]] AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science(https://arxiv.org/abs/2506.13992)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems.</li>
</ul>

<h3>Title: Mapping Farmed Landscapes from Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Michelangelo Conserva, Alex Wilson, Charlotte Stanton, Vishal Batchu, Varun Gulshan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13993">https://arxiv.org/abs/2506.13993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13993">https://arxiv.org/pdf/2506.13993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13993]] Mapping Farmed Landscapes from Remote Sensing(https://arxiv.org/abs/2506.13993)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Effective management of agricultural landscapes is critical for meeting global biodiversity targets, but efforts are hampered by the absence of detailed, large-scale ecological maps. To address this, we introduce Farmscapes, the first large-scale (covering most of England), high-resolution (25cm) map of rural landscape features, including ecologically vital elements like hedgerows, woodlands, and stone walls. This map was generated using a deep learning segmentation model trained on a novel, dataset of 942 manually annotated tiles derived from aerial imagery. Our model accurately identifies key habitats, achieving high f1-scores for woodland (96\%) and farmed land (95\%), and demonstrates strong capability in segmenting linear features, with an F1-score of 72\% for hedgerows. By releasing the England-wide map on Google Earth Engine, we provide a powerful, open-access tool for ecologists and policymakers. This work enables data-driven planning for habitat restoration, supports the monitoring of initiatives like the EU Biodiversity Strategy, and lays the foundation for advanced analysis of landscape connectivity.</li>
</ul>

<h3>Title: Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Siyu Chen, Heejune Sheen, Xuyuan Xiong, Tianhao Wang, Zhuoran Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14002">https://arxiv.org/abs/2506.14002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14002">https://arxiv.org/pdf/2506.14002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14002]] Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders(https://arxiv.org/abs/2506.14002)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically \highlight{prove that this algorithm correctly recovers all monosemantic features} when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and \highlight{demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters}. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.</li>
</ul>

<h3>Title: Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs</h3>
<ul>
<li><strong>Authors: </strong>Yiwei Chen, Soumyadeep Pal, Yimeng Zhang, Qing Qu, Sijia Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14003">https://arxiv.org/abs/2506.14003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14003">https://arxiv.org/pdf/2506.14003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14003]] Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs(https://arxiv.org/abs/2506.14003)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>Machine unlearning (MU) for large language models (LLMs), commonly referred to as LLM unlearning, seeks to remove specific undesirable data or knowledge from a trained model, while maintaining its performance on standard tasks. While unlearning plays a vital role in protecting data privacy, enforcing copyright, and mitigating sociotechnical harms in LLMs, we identify a new vulnerability post-unlearning: unlearning trace detection. We discover that unlearning leaves behind persistent ''fingerprints'' in LLMs, detectable traces in both model behavior and internal representations. These traces can be identified from output responses, even when prompted with forget-irrelevant inputs. Specifically, a simple supervised classifier can reliably determine whether a model has undergone unlearning based solely on its textual outputs. Further analysis shows that these traces are embedded in intermediate activations and propagate nonlinearly to the final layer, forming low-dimensional, learnable manifolds in activation space. Through extensive experiments, we show that forget-relevant prompts enable over 90% accuracy in detecting unlearning traces across all model sizes. Even with forget-irrelevant inputs, large LLMs maintain high detectability, demonstrating the broad applicability of unlearning trace detection. These findings reveal that unlearning leaves measurable signatures, introducing a new risk of reverse-engineering forgotten information when a model is identified as unlearned given an input query. Codes are available at [this URL](this https URL).</li>
</ul>

<h3>Title: Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text</h3>
<ul>
<li><strong>Authors: </strong>Amr Mohamed, Yang Zhang, Michalis Vazirgiannis, Guokan Shang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14012">https://arxiv.org/abs/2506.14012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14012">https://arxiv.org/pdf/2506.14012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14012]] Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text(https://arxiv.org/abs/2506.14012)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English text$\unicode{x2013}$even under linguistic constraints$\unicode{x2013}$embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation.</li>
</ul>

<h3>Title: Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation</h3>
<ul>
<li><strong>Authors: </strong>Nick Yiwen Huang, Akin Caliskan, Berkay Kicanaoglu, James Tompkin, Hyeongwoo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14015">https://arxiv.org/abs/2506.14015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14015">https://arxiv.org/pdf/2506.14015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14015]] Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation(https://arxiv.org/abs/2506.14015)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We consider the problem of disentangling 3D from large vision-language models, which we show on generative 3D portraits. This allows free-form text control of appearance attributes like age, hair style, and glasses, and 3D geometry control of face expression and camera pose. In this setting, we assume we use a pre-trained large vision-language model (LVLM; CLIP) to generate from a smaller 2D dataset with no additional paired labels and with a pre-defined 3D morphable model (FLAME). First, we disentangle using canonicalization to a 2D reference frame from a deformable neural 3D triplane representation. But another form of entanglement arises from the significant noise in the LVLM's embedding space that describes irrelevant features. This damages output quality and diversity, but we overcome this with a Jacobian regularization that can be computed efficiently with a stochastic approximator. Compared to existing methods, our approach produces portraits with added text and 3D control, where portraits remain consistent when either control is changed. Broadly, this approach lets creators control 3D generators on their own 2D face data without needing resources to label large data or train large models.</li>
</ul>

<h3>Title: Bures-Wasserstein Flow Matching for Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Keyue Jiang, Jiahao Cui, Xiaowen Dong, Laura Toni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14020">https://arxiv.org/abs/2506.14020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14020">https://arxiv.org/pdf/2506.14020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14020]] Bures-Wasserstein Flow Matching for Graph Generation(https://arxiv.org/abs/2506.14020)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Graph generation has emerged as a critical task in fields ranging from molecule design to drug discovery. Contemporary approaches, notably diffusion and flow-based models, have achieved solid graph generative performance through constructing a probability path that interpolates between a reference distribution and the data distribution. However, these methods typically model the evolution of individual nodes and edges independently and use linear interpolations to build the path assuming that the data lie in Euclidean space. We show that this is suboptimal given the intrinsic non-Euclidean structure and interconnected patterns of graphs, and it poses risks to the sampling convergence. To build a better probability path, we model the joint evolution of the nodes and edges by representing graphs as connected systems parameterized by Markov random fields (MRF). We then leverage the optimal transport displacement between MRF objects to design the probability path for graph generation. Based on this, we introduce BWFlow, a flow-matching framework for graph generation that respects the underlying geometry of graphs and provides smooth velocities in the probability path. The novel framework can be adapted to both continuous and discrete flow-matching algorithms. Experimental evaluations in plain graph generation and 2D/3D molecule generation validate the effectiveness of BWFlow in graph generation with competitive performance, stable training, and guaranteed sampling convergence.</li>
</ul>

<h3>Title: MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Xueqing Peng, Lingfei Qian, Yan Wang, Ruoyu Xiang, Yueru He, Yang Ren, Mingyang Jiang, Jeff Zhao, Huan He, Yi Han, Yun Feng, Yuechen Jiang, Yupeng Cao, Haohang Li, Yangyang Yu, Xiaoyu Wang, Penglei Gao, Shengyuan Lin, Keyi Wang, Shanshan Yang, Yilun Zhao, Zhiwei Liu, Peng Lu, Jerry Huang, Suyuchen Wang, Triantafillos Papadopoulos, Polydoros Giannouris, Efstathia Soufleri, Nuo Chen, Guojun Xiong, Zhiyang Deng, Yijia Zhao, Mingquan Lin, Meikang Qiu, Kaleb E Smith, Arman Cohan, Xiao-Yang Liu, Jimin Huang, Alejandro Lopez-Lira, Xi Chen, Junichi Tsujii, Jian-Yun Nie, Sophia Ananiadou, Qianqian Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14028">https://arxiv.org/abs/2506.14028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14028">https://arxiv.org/pdf/2506.14028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14028]] MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation(https://arxiv.org/abs/2506.14028)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have accelerated progress in financial NLP and applications, yet existing benchmarks remain limited to monolingual and unimodal settings, often over-relying on simple tasks and failing to reflect the complexity of real-world financial communication. We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain, evaluating LLMs across modalities (text, vision, audio) and linguistic settings (monolingual, bilingual, multilingual) on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs; and EnglishOCR and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to extract and reason over information from visual-text financial documents. Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets. Extensive evaluation of 22 state-of-the-art models reveals that even the strongest models, despite their general multimodal and multilingual capabilities, struggle dramatically when faced with complex cross-lingual and multimodal tasks in financial domain. MultiFinBen is publicly released to foster transparent, reproducible, and inclusive progress in financial studies and applications.</li>
</ul>

<h3>Title: Robust Physics-Informed Neural Network Approach for Estimating Heterogeneous Elastic Properties from Noisy Displacement Data</h3>
<ul>
<li><strong>Authors: </strong>Tatthapong Srikitrungruang, Sina Aghaee Dabaghan Fard, Matthew Lemon, Jaesung Lee, Yuxiao Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14036">https://arxiv.org/abs/2506.14036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14036">https://arxiv.org/pdf/2506.14036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14036]] Robust Physics-Informed Neural Network Approach for Estimating Heterogeneous Elastic Properties from Noisy Displacement Data(https://arxiv.org/abs/2506.14036)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurately estimating spatially heterogeneous elasticity parameters, particularly Young's modulus and Poisson's ratio, from noisy displacement measurements remains significantly challenging in inverse elasticity problems. Existing inverse estimation techniques are often limited by instability, pronounced sensitivity to measurement noise, and difficulty in recovering absolute-scale Young's modulus. This work presents a novel Inverse Elasticity Physics-Informed Neural Network (IE-PINN) specifically designed to robustly reconstruct heterogeneous distributions of elasticity parameters from noisy displacement data based on linear elasticity physics. IE-PINN integrates three distinct neural network architectures dedicated to separately modeling displacement fields, strain fields, and elasticity distributions, thereby significantly enhancing stability and accuracy against measurement noise. Additionally, a two-phase estimation strategy is introduced: the first phase recovers relative spatial distributions of Young's modulus and Poisson's ratio, and the second phase calibrates the absolute scale of Young's modulus using imposed loading boundary conditions. Additional methodological innovations, including positional encoding, sine activation functions, and a sequential pretraining protocol, further enhance the model's performance and robustness. Extensive numerical experiments demonstrate that IE-PINN effectively overcomes critical limitations encountered by existing methods, delivering accurate absolute-scale elasticity estimations even under severe noise conditions. This advancement holds substantial potential for clinical imaging diagnostics and mechanical characterization, where measurements typically encounter substantial noise.</li>
</ul>

<h3>Title: An Interdisciplinary Review of Commonsense Reasoning and Intent Detection</h3>
<ul>
<li><strong>Authors: </strong>Md Nazmus Sakib</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14040">https://arxiv.org/abs/2506.14040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14040">https://arxiv.org/pdf/2506.14040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14040]] An Interdisciplinary Review of Commonsense Reasoning and Intent Detection(https://arxiv.org/abs/2506.14040)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This review explores recent advances in commonsense reasoning and intent detection, two key challenges in natural language understanding. We analyze 28 papers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and application. Commonsense reasoning is reviewed across zero-shot learning, cultural adaptation, structured evaluation, and interactive contexts. Intent detection is examined through open-set models, generative formulations, clustering, and human-centered systems. By bridging insights from NLP and HCI, we highlight emerging trends toward more adaptive, multilingual, and context-aware models, and identify key gaps in grounding, generalization, and benchmark design.</li>
</ul>

<h3>Title: Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications</h3>
<ul>
<li><strong>Authors: </strong>David Kogan, Max Schumacher, Sam Nguyen, Masanori Suzuki, Melissa Smith, Chloe Sophia Bellows, Jared Bernstein</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14046">https://arxiv.org/abs/2506.14046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14046">https://arxiv.org/pdf/2506.14046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14046]] Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications(https://arxiv.org/abs/2506.14046)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development.</li>
</ul>

<h3>Title: Scientifically-Interpretable Reasoning Network (ScIReN): Uncovering the Black-Box of Nature</h3>
<ul>
<li><strong>Authors: </strong>Joshua Fan, Haodi Xu, Feng Tao, Md Nasim, Marc Grimson, Yiqi Luo, Carla P. Gomes</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14054">https://arxiv.org/abs/2506.14054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14054">https://arxiv.org/pdf/2506.14054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14054]] Scientifically-Interpretable Reasoning Network (ScIReN): Uncovering the Black-Box of Nature(https://arxiv.org/abs/2506.14054)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Neural networks are a powerful tool for learning patterns from data. However, they do not respect known scientific laws, nor can they reveal novel scientific insights due to their black-box nature. In contrast, scientific reasoning distills biological or physical principles from observations and controlled experiments, and quantitatively interprets them with process-based models made of mathematical equations. Yet, process-based models rely on numerous free parameters that must be set in an ad-hoc manner, and thus often fit observations poorly in cross-scale predictions. While prior work has embedded process-based models in conventional neural networks, discovering interpretable relationships between parameters in process-based models and input features is still a grand challenge for scientific discovery. We thus propose Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent framework that combines interpretable neural and process-based reasoning. An interpretable encoder predicts scientifically-meaningful latent parameters, which are then passed through a differentiable process-based decoder to predict labeled output variables. ScIReN also uses a novel hard-sigmoid constraint layer to restrict latent parameters to meaningful ranges defined by scientific prior knowledge, further enhancing its interpretability. While the embedded process-based model enforces established scientific knowledge, the encoder reveals new scientific mechanisms and relationships hidden in conventional black-box models. We apply ScIReN on two tasks: simulating the flow of organic carbon through soils, and modeling ecosystem respiration from plants. In both tasks, ScIReN outperforms black-box networks in predictive accuracy while providing substantial scientific interpretability -- it can infer latent scientific mechanisms and their relationships with input features.</li>
</ul>

<h3>Title: SoK: Advances and Open Problems in Web Tracking</h3>
<ul>
<li><strong>Authors: </strong>Yash Vekaria (1), Yohan Beugin (2), Shaoor Munir (1), Gunes Acar (3), Nataliia Bielova (4), Steven Englehardt (5), Umar Iqbal (6), Alexandros Kapravelos (7), Pierre Laperdrix (8), Nick Nikiforakis (9), Jason Polakis (10), Franziska Roesner (11), Zubair Shafiq (1), Sebastian Zimmeck (12) ((1) University of California Davis, USA, (2) University of Wisconsin-Madison, USA, (3) Radboud University, Netherlands, (4) Inria Centre at Universite Cote d'Azur, France, (5) Independent Researcher, USA, (6) Washington University in St. Louis, USA, (7) North Carolina State University, USA, (8) Centre National de la Recherche Scientifique, France, (9) Stony Brook University, USA, (10) University of Illinois Chicago, USA, (11) University of Washington, USA, (12) Wesleyan University, USA)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14057">https://arxiv.org/abs/2506.14057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14057">https://arxiv.org/pdf/2506.14057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14057]] SoK: Advances and Open Problems in Web Tracking(https://arxiv.org/abs/2506.14057)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Web tracking is a pervasive and opaque practice that enables personalized advertising, retargeting, and conversion tracking. Over time, it has evolved into a sophisticated and invasive ecosystem, employing increasingly complex techniques to monitor and profile users across the web. The research community has a long track record of analyzing new web tracking techniques, designing and evaluating the effectiveness of countermeasures, and assessing compliance with privacy regulations. Despite a substantial body of work on web tracking, the literature remains fragmented across distinctly scoped studies, making it difficult to identify overarching trends, connect new but related techniques, and identify research gaps in the field. Today, web tracking is undergoing a once-in-a-generation transformation, driven by fundamental shifts in the advertising industry, the adoption of anti-tracking countermeasures by browsers, and the growing enforcement of emerging privacy regulations. This Systematization of Knowledge (SoK) aims to consolidate and synthesize this wide-ranging research, offering a comprehensive overview of the technical mechanisms, countermeasures, and regulations that shape the modern and rapidly evolving web tracking landscape. This SoK also highlights open challenges and outlines directions for future research, aiming to serve as a unified reference and introductory material for researchers, practitioners, and policymakers alike.</li>
</ul>

<h3>Title: Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data</h3>
<ul>
<li><strong>Authors: </strong>Iona Carslaw, Sivan Milton, Nicolas Navarre, Ciyang Qing, Wataru Uegaki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14064">https://arxiv.org/abs/2506.14064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14064">https://arxiv.org/pdf/2506.14064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14064]] Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data(https://arxiv.org/abs/2506.14064)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>For linguists, embedded clauses have been of special interest because of their intricate distribution of syntactic and semantic features. Yet, current research relies on schematically created language examples to investigate these constructions, missing out on statistical information and naturally-occurring examples that can be gained from large language corpora. Thus, we present a methodological approach for detecting and annotating naturally-occurring examples of English embedded clauses in large-scale text data using constituency parsing and a set of parsing heuristics. Our tool has been evaluated on our dataset Golden Embedded Clause Set (GECS), which includes hand-annotated examples of naturally-occurring English embedded clause sentences. Finally, we present a large-scale dataset of naturally-occurring English embedded clauses which we have extracted from the open-source corpus Dolma using our extraction tool.</li>
</ul>

<h3>Title: A Regret Perspective on Online Selective Generation</h3>
<ul>
<li><strong>Authors: </strong>Minjae Lee, Yoonjae Jung, Sangdon Park</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14067">https://arxiv.org/abs/2506.14067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14067">https://arxiv.org/pdf/2506.14067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14067]] A Regret Perspective on Online Selective Generation(https://arxiv.org/abs/2506.14067)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language generative models increasingly interact with humans, while their falsified responses raise concerns. To address this hallucination effect, selectively abstaining from answering, called selective generation, provides an effective way for generators to control the hallucination when it is unsure of their answers. However, as selective generators are interacting under non-stochastic environments and having partial feedback from users on selective generation (e.g., thumbs up or down on the selected answer), learning methods for selective generation under such practical setups are crucial but currently missing. To address these limitations, we propose an online learning algorithm for selective generation under partial feedback. In particular, as learning under partial feedback is well-studied by multi-armed bandit problems, we reduce selective generation to bandits and provide a novel conversion lemma from bandits back to selective generation to leverage any known bandit algorithms and theoretical properties. This mainly connects regret guarantees of bandits to false discovery rate (FDR) guarantees of selective generation for controlling hallucination. However, naively exploiting known bandit algorithms and their regret bounds suffers from slow convergence speed in practice due the nature of partial feedback. To overcome this, we exploit a unique structure of arms in selective generation for feedback unlocking, i.e., unlocking unknown feedback from observed feedback. We theoretically and empirically evaluate the efficacy of the proposed online selective generation algorithm under partial feedback over diverse data environment setups, resulting in controlling a desired FDR, while maintaining reasonable selection efficiency, i.e., the ratio of non-abstaining answers, compared to baselines.</li>
</ul>

<h3>Title: Comprehensive Verilog Design Problems: A Next-Generation Benchmark Dataset for Evaluating Large Language Models and Agents on RTL Design and Verification</h3>
<ul>
<li><strong>Authors: </strong>Nathaniel Pinckney, Chenhui Deng, Chia-Tung Ho, Yun-Da Tsai, Mingjie Liu, Wenfei Zhou, Brucek Khailany, Haoxing Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14074">https://arxiv.org/abs/2506.14074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14074">https://arxiv.org/pdf/2506.14074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14074]] Comprehensive Verilog Design Problems: A Next-Generation Benchmark Dataset for Evaluating Large Language Models and Agents on RTL Design and Verification(https://arxiv.org/abs/2506.14074)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We present the Comprehensive Verilog Design Problems (CVDP) benchmark, a new dataset and infrastructure to advance LLM and agent research in hardware design and verification. CVDP includes 783 problems across 13 task categories, covering RTL generation, verification, debugging, specification alignment, and technical Q&A authored by experienced hardware engineers. Problems are offered in both non-agentic and agentic formats. The benchmark introduces more realistic and challenging contexts than prior work, with state-of-the-art models achieving no more than 34% pass@1 on code generation. Agentic tasks$\unicode{x2013}$especially those involving RTL reuse and verification$\unicode{x2013}$are particularly difficult. Evaluation uses open-source tools and model scoring infrastructure, with comprehension tasks assessed via BLEU and LLM-based judging. CVDP reveals substantial gaps in current model capabilities, underscoring the need for continued research toward robust, real-world hardware design automation.</li>
</ul>

<h3>Title: Transformers Learn Faster with Semantic Focus</h3>
<ul>
<li><strong>Authors: </strong>Parikshit Ram, Kenneth L. Clarkson, Tim Klinger, Shashanka Ubaru, Alexander G. Gray</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14095">https://arxiv.org/abs/2506.14095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14095">https://arxiv.org/pdf/2506.14095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14095]] Transformers Learn Faster with Semantic Focus(https://arxiv.org/abs/2506.14095)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Various forms of sparse attention have been explored to mitigate the quadratic computational and memory cost of the attention mechanism in transformers. We study sparse transformers not through a lens of efficiency but rather in terms of learnability and generalization. Empirically studying a range of attention mechanisms, we find that input-dependent sparse attention models appear to converge faster and generalize better than standard attention models, while input-agnostic sparse attention models show no such benefits -- a phenomenon that is robust across architectural and optimization hyperparameter choices. This can be interpreted as demonstrating that concentrating a model's "semantic focus" with respect to the tokens currently being considered (in the form of input-dependent sparse attention) accelerates learning. We develop a theoretical characterization of the conditions that explain this behavior. We establish a connection between the stability of the standard softmax and the loss function's Lipschitz properties, then show how sparsity affects the stability of the softmax and the subsequent convergence and generalization guarantees resulting from the attention mechanism. This allows us to theoretically establish that input-agnostic sparse attention does not provide any benefits. We also characterize conditions when semantic focus (input-dependent sparse attention) can provide improved guarantees, and we validate that these conditions are in fact met in our empirical evaluations.</li>
</ul>

<h3>Title: Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems</h3>
<ul>
<li><strong>Authors: </strong>Sanjeda Akter, Ibne Farabi Shihab, Anuj Sharma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14096">https://arxiv.org/abs/2506.14096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14096">https://arxiv.org/pdf/2506.14096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14096]] Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems(https://arxiv.org/abs/2506.14096)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.</li>
</ul>

<h3>Title: Toward a Graph Foundation Model: Pre-Training Transformers With Random Walks</h3>
<ul>
<li><strong>Authors: </strong>Ziyuan Tang, Jie Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14098">https://arxiv.org/abs/2506.14098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14098">https://arxiv.org/pdf/2506.14098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14098]] Toward a Graph Foundation Model: Pre-Training Transformers With Random Walks(https://arxiv.org/abs/2506.14098)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>A foundation model like GPT elicits many emergent abilities, owing to the pre-training with broad inclusion of data and the use of the powerful Transformer architecture. While foundation models in natural languages are prevalent, can we build similar models for graphs? This paper describes an approach toward a graph foundation model that is pre-trained with diverse graph datasets by adapting the Transformer backbone. A central challenge toward this end is how a sequence model encodes graphs of varying sizes and from different domains. We propose representing a node as multiple random walks, such that the Transformer can extract node representations from sequences, which in turn form edge and graph representations. We develop a novel context prediction loss for these random walks and theoretically analyze their expressive power in distinguishing neighborhoods and graphs. We also demonstrate the pre-training of our model and its adaptation to downstream tasks, showcasing its potential as a foundation for processing and reasoning with graph-structured data.</li>
</ul>

<h3>Title: Abstract Meaning Representation for Hospital Discharge Summarization</h3>
<ul>
<li><strong>Authors: </strong>Paul Landes, Sitara Rao, Aaron Jeremy Chaise, Barbara Di Eugenio</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14101">https://arxiv.org/abs/2506.14101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14101">https://arxiv.org/pdf/2506.14101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14101]] Abstract Meaning Representation for Hospital Discharge Summarization(https://arxiv.org/abs/2506.14101)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Achilles heel of Large Language Models (LLMs) is hallucination, which has drastic consequences for the clinical domain. This is particularly important with regards to automatically generating discharge summaries (a lengthy medical document that summarizes a hospital in-patient visit). Automatically generating these summaries would free physicians to care for patients and reduce documentation burden. The goal of this work is to discover new methods that combine language-based graphs and deep learning models to address provenance of content and trustworthiness in automatic summarization. Our method shows impressive reliability results on the publicly available Medical Information Mart for Intensive III (MIMIC-III) corpus and clinical notes written by physicians at Anonymous Hospital. rovide our method, generated discharge ary output examples, source code and trained models.</li>
</ul>

<h3>Title: Evaluating Loss Functions for Graph Neural Networks: Towards Pretraining and Generalization</h3>
<ul>
<li><strong>Authors: </strong>Khushnood Abbas, Ruizhe Hou, Zhou Wengang, Dong Shi, Niu Ling, Satyaki Nan, Alireza Abbasi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14114">https://arxiv.org/abs/2506.14114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14114">https://arxiv.org/pdf/2506.14114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14114]] Evaluating Loss Functions for Graph Neural Networks: Towards Pretraining and Generalization(https://arxiv.org/abs/2506.14114)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) became useful for learning on non-Euclidean data. However, their best performance depends on choosing the right model architecture and the training objective, also called the loss function. Researchers have studied these parts separately, but a large-scale evaluation has not looked at how GNN models and many loss functions work together across different tasks. To fix this, we ran a thorough study - it included seven well-known GNN architectures. We also used a large group of 30 single plus mixed loss functions. The study looked at both inductive and transductive settings. Our evaluation spanned three distinct real-world datasets, assessing performance in both inductive and transductive settings using 21 comprehensive evaluation metrics. From these extensive results (detailed in supplementary information 1 \& 2), we meticulously analyzed the top ten model-loss combinations for each metric based on their average rank. Our findings reveal that, especially for the inductive case: 1) Hybrid loss functions generally yield superior and more robust performance compared to single loss functions, indicating the benefit of multi-objective optimization. 2) The GIN architecture always showed the highest-level average performance, especially with Cross-Entropy loss. 3) Although some combinations had overall lower average ranks, models such as GAT, particularly with certain hybrid losses, demonstrated incredible specialized strengths, maximizing the most top-1 results among the individual metrics, emphasizing subtle strengths for particular task demands. 4) On the other hand, the MPNN architecture typically lagged behind the scenarios it was tested against.</li>
</ul>

<h3>Title: FADPNet: Frequency-Aware Dual-Path Network for Face Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Siyu Xu, Wenjie Li, Guangwei Gao, Jian Yang, Guo-Jun Qi, Chia-Wen Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14121">https://arxiv.org/abs/2506.14121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14121">https://arxiv.org/pdf/2506.14121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14121]] FADPNet: Frequency-Aware Dual-Path Network for Face Super-Resolution(https://arxiv.org/abs/2506.14121)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Face super-resolution (FSR) under limited computational costs remains an open problem. Existing approaches typically treat all facial pixels equally, resulting in suboptimal allocation of computational resources and degraded FSR performance. CNN is relatively sensitive to high-frequency facial features, such as component contours and facial outlines. Meanwhile, Mamba excels at capturing low-frequency features like facial color and fine-grained texture, and does so with lower complexity than Transformers. Motivated by these observations, we propose FADPNet, a Frequency-Aware Dual-Path Network that decomposes facial features into low- and high-frequency components and processes them via dedicated branches. For low-frequency regions, we introduce a Mamba-based Low-Frequency Enhancement Block (LFEB), which combines state-space attention with squeeze-and-excitation operations to extract low-frequency global interactions and emphasize informative channels. For high-frequency regions, we design a CNN-based Deep Position-Aware Attention (DPA) module to enhance spatially-dependent structural details, complemented by a lightweight High-Frequency Refinement (HFR) module that further refines frequency-specific representations. Through the above designs, our method achieves an excellent balance between FSR quality and model efficiency, outperforming existing approaches.</li>
</ul>

<h3>Title: Sampling from Your Language Model One Byte at a Time</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Hayase, Alisa Liu, Noah A. Smith, Sewoong Oh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.FL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14123">https://arxiv.org/abs/2506.14123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14123">https://arxiv.org/pdf/2506.14123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14123]] Sampling from Your Language Model One Byte at a Time(https://arxiv.org/abs/2506.14123)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. However, prior work has shown that tokenization can introduce distortion into the model's generations. For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. This Prompt Boundary Problem (PBP) also arises in languages such as Chinese and in code generation, where tokens often do not line up with syntactic boundaries. Additionally mismatching tokenizers often hinder model composition and interoperability. For example, it is not possible to directly ensemble models with different tokenizers due to their mismatching vocabularies. To address these issues, we present an inference-time method to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM, without changing its generative distribution at the text level. Our method efficient solves the PBP and is also able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time as well as transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals.</li>
</ul>

<h3>Title: KDMOS:Knowledge Distillation for Motion Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chunyu Cao, Jintao Cheng, Zeyu Chen, Linfan Zhan, Rui Fan, Zhijian He, Xiaoyu Tang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14130">https://arxiv.org/abs/2506.14130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14130">https://arxiv.org/pdf/2506.14130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14130]] KDMOS:Knowledge Distillation for Motion Segmentation(https://arxiv.org/abs/2506.14130)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at this https URL.</li>
</ul>

<h3>Title: Leveraging Predictive Equivalence in Decision Trees</h3>
<ul>
<li><strong>Authors: </strong>Hayden McTavish, Zachery Boner, Jon Donnelly, Margo Seltzer, Cynthia Rudin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14143">https://arxiv.org/abs/2506.14143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14143">https://arxiv.org/pdf/2506.14143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14143]] Leveraging Predictive Equivalence in Decision Trees(https://arxiv.org/abs/2506.14143)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Decision trees are widely used for interpretable machine learning due to their clearly structured reasoning process. However, this structure belies a challenge we refer to as predictive equivalence: a given tree's decision boundary can be represented by many different decision trees. The presence of models with identical decision boundaries but different evaluation processes makes model selection challenging. The models will have different variable importance and behave differently in the presence of missing values, but most optimization procedures will arbitrarily choose one such model to return. We present a boolean logical representation of decision trees that does not exhibit predictive equivalence and is faithful to the underlying decision boundary. We apply our representation to several downstream machine learning tasks. Using our representation, we show that decision trees are surprisingly robust to test-time missingness of feature values; we address predictive equivalence's impact on quantifying variable importance; and we present an algorithm to optimize the cost of reaching predictions.</li>
</ul>

<h3>Title: SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability</h3>
<ul>
<li><strong>Authors: </strong>Juho Bai, Inwook Shim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14144">https://arxiv.org/abs/2506.14144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14144">https://arxiv.org/pdf/2506.14144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14144]] SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability(https://arxiv.org/abs/2506.14144)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer~(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models~(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH/UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50\% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: this https URL.</li>
</ul>

<h3>Title: S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tao He, Guang Huang, Yu Yang, Tianshi Xu, Sicheng Zhao, Guiguang Ding, Pengyang Wang, Feng Tian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14158">https://arxiv.org/abs/2506.14158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14158">https://arxiv.org/pdf/2506.14158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14158]] S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models(https://arxiv.org/abs/2506.14158)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.</li>
</ul>

<h3>Title: MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind</h3>
<ul>
<li><strong>Authors: </strong>Yanlin Li, Hao Liu, Huimin Liu, Yinwei Wei, Yupeng Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14161">https://arxiv.org/abs/2506.14161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14161">https://arxiv.org/pdf/2506.14161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14161]] MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind(https://arxiv.org/abs/2506.14161)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity for reasoning about mental states, yet failures in this capacity often manifest as systematic implicit bias. Evaluating this bias is challenging, as conventional direct-query methods are susceptible to social desirability effects and fail to capture its subtle, multi-dimensional nature. To this end, we propose an evaluation framework that leverages the Stereotype Content Model (SCM) to reconceptualize bias as a multi-dimensional failure in ToM across Competence, Sociability, and Morality. The framework introduces two indirect tasks: the Word Association Bias Test (WABT) to assess implicit lexical associations and the Affective Attribution Test (AAT) to measure covert affective leanings, both designed to probe latent stereotypes without triggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs demonstrate our framework's capacity to reveal complex bias structures, including pervasive sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification, thereby providing a more robust methodology for identifying the structural nature of implicit bias.</li>
</ul>

<h3>Title: Structured and Informed Probabilistic Modeling with the Thermodynamic Kolmogorov-Arnold Model</h3>
<ul>
<li><strong>Authors: </strong>Prithvi Raj</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14167">https://arxiv.org/abs/2506.14167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14167">https://arxiv.org/pdf/2506.14167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14167]] Structured and Informed Probabilistic Modeling with the Thermodynamic Kolmogorov-Arnold Model(https://arxiv.org/abs/2506.14167)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We adapt the Kolmogorov-Arnold Representation Theorem to generative modeling by reinterpreting its inner functions as a Markov Kernel between probability spaces via inverse transform sampling. We present a generative model that is interpretable, easy to design, and efficient. Our approach couples a Kolmogorov-Arnold Network generator with independent energy-based priors, trained via Maximum Likelihood. Inverse sampling enables fast inference, while prior knowledge can be incorporated before training to better align priors with posteriors, thereby improving learning efficiency and sample quality. The learned prior is also recoverable and visualizable post-training, offering an empirical Bayes perspective. To address inflexibility and mitigate prior-posterior mismatch, we introduce scalable extensions based on mixture distributions and Langevin Monte Carlo methods, admitting a trade-off between flexibility and training efficiency. Our contributions connect classical representation theorems with modern probabilistic modeling, while balancing training stability, inference speed, and the quality and diversity of generations.</li>
</ul>

<h3>Title: VideoMAR: Autoregressive Video Generatio with Continuous Tokens</h3>
<ul>
<li><strong>Authors: </strong>Hu Yu, Biao Gong, Hangjie Yuan, DanDan Zheng, Weilong Chai, Jingdong Chen, Kecheng Zheng, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14168">https://arxiv.org/abs/2506.14168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14168">https://arxiv.org/pdf/2506.14168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14168]] VideoMAR: Autoregressive Video Generatio with Continuous Tokens(https://arxiv.org/abs/2506.14168)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\%$), training data ($0.5\%$), and GPU resources ($0.2\%$).</li>
</ul>

<h3>Title: A multi-stage augmented multimodal interaction network for fish feeding intensity quantification</h3>
<ul>
<li><strong>Authors: </strong>Shulong Zhang, Mingyuan Yao, Jiayin Zhao, Xiao Liu, Haihua Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14170">https://arxiv.org/abs/2506.14170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14170">https://arxiv.org/pdf/2506.14170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14170]] A multi-stage augmented multimodal interaction network for fish feeding intensity quantification(https://arxiv.org/abs/2506.14170)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>In recirculating aquaculture systems, accurate and effective assessment of fish feeding intensity is crucial for reducing feed costs and calculating optimal feeding times. However, current studies have limitations in modality selection, feature extraction and fusion, and co-inference for decision making, which restrict further improvement in the accuracy, applicability and reliability of multimodal fusion models. To address this problem, this study proposes a Multi-stage Augmented Multimodal Interaction Network (MAINet) for quantifying fish feeding intensity. Firstly, a general feature extraction framework is proposed to efficiently extract feature information from input image, audio and water wave datas. Second, an Auxiliary-modality Reinforcement Primary-modality Mechanism (ARPM) is designed for inter-modal interaction and generate enhanced features, which consists of a Channel Attention Fusion Network (CAFN) and a Dual-mode Attention Fusion Network (DAFN). Finally, an Evidence Reasoning (ER) rule is introduced to fuse the output results of each modality and make decisions, thereby completing the quantification of fish feeding intensity. The experimental results show that the constructed MAINet reaches 96.76%, 96.78%, 96.79% and 96.79% in accuracy, precision, recall and F1-Score respectively, and its performance is significantly higher than the comparison models. Compared with models that adopt single-modality, dual-modality fusion and different decision-making fusion methods, it also has obvious advantages. Meanwhile, the ablation experiments further verified the key role of the proposed improvement strategy in improving the robustness and feature utilization efficiency of model, which can effectively improve the accuracy of the quantitative results of fish feeding intensity.</li>
</ul>

<h3>Title: GRAM: A Generative Foundation Reward Model for Reward Generalization</h3>
<ul>
<li><strong>Authors: </strong>Chenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, Qiaozhi He, Murun Yang, Bei Li, Tong Xiao, Chunliang Zhang, Tongran Liu, Jingbo Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14175">https://arxiv.org/abs/2506.14175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14175">https://arxiv.org/pdf/2506.14175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14175]] GRAM: A Generative Foundation Reward Model for Reward Generalization(https://arxiv.org/abs/2506.14175)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.</li>
</ul>

<h3>Title: Meta-SurDiff: Classification Diffusion Model Optimized by Meta Learning is Reliable for Online Surgical Phase Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yufei Li, Jirui Wu, Long Tian, Liming Wang, Xiaonan Liu, Zijun Liu, Xiyang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14181">https://arxiv.org/abs/2506.14181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14181">https://arxiv.org/pdf/2506.14181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14181]] Meta-SurDiff: Classification Diffusion Model Optimized by Meta Learning is Reliable for Online Surgical Phase Recognition(https://arxiv.org/abs/2506.14181)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Online surgical phase recognition has drawn great attention most recently due to its potential downstream applications closely related to human life and health. Despite deep models have made significant advances in capturing the discriminative long-term dependency of surgical videos to achieve improved recognition, they rarely account for exploring and modeling the uncertainty in surgical videos, which should be crucial for reliable online surgical phase recognition. We categorize the sources of uncertainty into two types, frame ambiguity in videos and unbalanced distribution among surgical phases, which are inevitable in surgical videos. To address this pivot issue, we introduce a meta-learning-optimized classification diffusion model (Meta-SurDiff), to take full advantage of the deep generative model and meta-learning in achieving precise frame-level distribution estimation for reliable online surgical phase recognition. For coarse recognition caused by ambiguous video frames, we employ a classification diffusion model to assess the confidence of recognition results at a finer-grained frame-level instance. For coarse recognition caused by unbalanced phase distribution, we use a meta-learning based objective to learn the diffusion model, thus enhancing the robustness of classification boundaries for different surgical this http URL establish effectiveness of Meta-SurDiff in online surgical phase recognition through extensive experiments on five widely used datasets using more than four practical metrics. The datasets include Cholec80, AutoLaparo, M2Cai16, OphNet, and NurViD, where OphNet comes from ophthalmic surgeries, NurViD is the daily care dataset, while the others come from laparoscopic surgeries. We will release the code upon acceptance.</li>
</ul>

<h3>Title: Egocentric Human-Object Interaction Detection: A New Benchmark and Method</h3>
<ul>
<li><strong>Authors: </strong>Kunyuan Deng, Yi Wang, Lap-Pui Chau</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14189">https://arxiv.org/abs/2506.14189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14189">https://arxiv.org/pdf/2506.14189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14189]] Egocentric Human-Object Interaction Detection: A New Benchmark and Method(https://arxiv.org/abs/2506.14189)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Understanding the interaction between humans and objects has gained much attention in recent years. Existing human-object interaction (HOI) detection methods mainly focus on the third-person perspectives, overlooking a more intuitive way from the egocentric view of HOI, namely Ego-HOI. This paper introduces an Ego-HOIBench, a new dataset to promote the benchmarking and development of Ego-HOI detection. Our Ego-HOIBench comprises more than 27K egocentric images with high-quality hand-verb-object triplet annotations across 123 fine-grained interaction categories and locations, covering a rich diversity of scenarios, object types, and hand configurations in daily activities. In addition, we explore and adapt third-person HOI detection methods to Ego-HOIBench and illustrate the challenges of hand-occluded objects and the complexity of single- and two-hand interactions. To build a new baseline, we propose a Hand Geometry and Interactivity Refinement (HGIR) scheme, which leverages hand pose and geometric information as valuable cues for interpreting interactions. Specifically, the HGIR scheme explicitly extracts global hand geometric features from the estimated hand pose proposals and refines the interaction-specific features using pose-interaction attention. This scheme enables the model to obtain a robust and powerful interaction representation, significantly improving the Ego-HOI detection capability. Our approach is lightweight and effective, and it can be easily applied to HOI baselines in a plug-and-play manner to achieve state-of-the-art results on Ego-HOIBench. Our project is available at: this https URL</li>
</ul>

<h3>Title: A Variational Information Theoretic Approach to Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Sudeepta Mondal, Zhuolin Jiang, Ganesh Sundaramoorthi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14194">https://arxiv.org/abs/2506.14194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14194">https://arxiv.org/pdf/2506.14194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14194]] A Variational Information Theoretic Approach to Out-of-Distribution Detection(https://arxiv.org/abs/2506.14194)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>We present a theory for the construction of out-of-distribution (OOD) detection features for neural networks. We introduce random features for OOD through a novel information-theoretic loss functional consisting of two terms, the first based on the KL divergence separates resulting in-distribution (ID) and OOD feature distributions and the second term is the Information Bottleneck, which favors compressed features that retain the OOD information. We formulate a variational procedure to optimize the loss and obtain OOD features. Based on assumptions on OOD distributions, one can recover properties of existing OOD features, i.e., shaping functions. Furthermore, we show that our theory can predict a new shaping function that out-performs existing ones on OOD benchmarks. Our theory provides a general framework for constructing a variety of new features with clear explainability.</li>
</ul>

<h3>Title: MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Junghwan Kim, Kieun Park, Sohee Park, Hyunggug Kim, Bongwon Suh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14199">https://arxiv.org/abs/2506.14199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14199">https://arxiv.org/pdf/2506.14199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14199]] MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment(https://arxiv.org/abs/2506.14199)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Literary translation requires preserving cultural nuances and stylistic elements, which traditional metrics like BLEU and METEOR fail to assess due to their focus on lexical overlap. This oversight neglects the narrative consistency and stylistic fidelity that are crucial for literary works. To address this, we propose MAS-LitEval, a multi-agent system using Large Language Models (LLMs) to evaluate translations based on terminology, narrative, and style. We tested MAS-LitEval on translations of The Little Prince and A Connecticut Yankee in King Arthur's Court, generated by various LLMs, and compared it to traditional metrics. \textbf{MAS-LitEval} outperformed these metrics, with top models scoring up to 0.890 in capturing literary nuances. This work introduces a scalable, nuanced framework for Translation Quality Assessment (TQA), offering a practical tool for translators and researchers.</li>
</ul>

<h3>Title: DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Makoto Shing, Takuya Akiba</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14202">https://arxiv.org/abs/2506.14202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14202">https://arxiv.org/pdf/2506.14202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14202]] DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion(https://arxiv.org/abs/2506.14202)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Training large neural networks with end-to-end backpropagation creates significant memory bottlenecks, limiting accessibility to state-of-the-art AI research. We propose $\textit{DiffusionBlocks}$, a novel training framework that interprets neural network blocks as performing denoising operations in a continuous-time diffusion process. By partitioning the network into independently trainable blocks and optimizing noise level assignments based on equal cumulative probability mass, our approach achieves significant memory efficiency while maintaining competitive performance compared to traditional backpropagation in generative tasks. Experiments on image generation and language modeling tasks demonstrate memory reduction proportional to the number of blocks while achieving superior performance. DiffusionBlocks provides a promising pathway for democratizing access to large-scale neural network training with limited computational resources.</li>
</ul>

<h3>Title: Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Jongho Kim, Romain Storaï, Seung-won Hwang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14203">https://arxiv.org/abs/2506.14203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14203">https://arxiv.org/pdf/2506.14203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14203]] Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation(https://arxiv.org/abs/2506.14203)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this study, we investigate the potential of language models (LMs) in aiding patients experiencing anomia, a difficulty identifying the names of items. Identifying the intended target item from patient's circumlocution involves the two challenges of term failure and error: (1) The terms relevant to identifying the item remain unseen. (2) What makes the challenge unique is inherent perturbed terms by semantic paraphasia, which are not exactly related to the target item, hindering the identification process. To address each, we propose robustifying the model from semantically paraphasic errors and enhancing the model with unseen terms with gradient-based selective augmentation. Specifically, the gradient value controls augmented data quality amid semantic errors, while the gradient variance guides the inclusion of unseen but relevant terms. Due to limited domain-specific datasets, we evaluate the model on the Tip-of-the-Tongue dataset as an intermediary task and then apply our findings to real patient data from AphasiaBank. Our results demonstrate strong performance against baselines, aiding anomia patients by addressing the outlined challenges.</li>
</ul>

<h3>Title: CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Jia-Chen Zhang, Zheng Zhou, Yu-Jie Xiong, Chun-Ming Xia, Fei Dai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14206">https://arxiv.org/abs/2506.14206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14206">https://arxiv.org/pdf/2506.14206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14206]] CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation(https://arxiv.org/abs/2506.14206)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Training data has been proven to be one of the most critical components in training generative AI. However, obtaining high-quality data remains challenging, with data privacy issues presenting a significant hurdle. To address the need for high-quality data. Synthesize data has emerged as a mainstream solution, demonstrating impressive performance in areas such as images, audio, and video. Generating mixed-type data, especially high-quality tabular data, still faces significant challenges. These primarily include its inherent heterogeneous data types, complex inter-variable relationships, and intricate column-wise distributions. In this paper, we introduce CausalDiffTab, a diffusion model-based generative model specifically designed to handle mixed tabular data containing both numerical and categorical features, while being more flexible in capturing complex interactions among variables. We further propose a hybrid adaptive causal regularization method based on the principle of Hierarchical Prior Fusion. This approach adaptively controls the weight of causal regularization, enhancing the model's performance without compromising its generative capabilities. Comprehensive experiments conducted on seven datasets demonstrate that CausalDiffTab outperforms baseline methods across all metrics. Our code is publicly available at: this https URL.</li>
</ul>

<h3>Title: Chaining Event Spans for Temporal Relation Grounding</h3>
<ul>
<li><strong>Authors: </strong>Jongho Kim, Dohyeon Lee, Minsoo Kim, Seung-won Hwang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14213">https://arxiv.org/abs/2506.14213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14213">https://arxiv.org/pdf/2506.14213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14213]] Chaining Event Spans for Temporal Relation Grounding(https://arxiv.org/abs/2506.14213)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Accurately understanding temporal relations between events is a critical building block of diverse tasks, such as temporal reading comprehension (TRC) and relation extraction (TRE). For example in TRC, we need to understand the temporal semantic differences between the following two questions that are lexically near-identical: "What finished right before the decision?" or "What finished right after the decision?". To discern the two questions, existing solutions have relied on answer overlaps as a proxy label to contrast similar and dissimilar questions. However, we claim that answer overlap can lead to unreliable results, due to spurious overlaps of two dissimilar questions with coincidentally identical answers. To address the issue, we propose a novel approach that elicits proper reasoning behaviors through a module for predicting time spans of events. We introduce the Timeline Reasoning Network (TRN) operating in a two-step inductive reasoning process: In the first step model initially answers each question with semantic and syntactic information. The next step chains multiple questions on the same event to predict a timeline, which is then used to ground the answers. Results on the TORQUE and TB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms previous methods by effectively resolving the spurious overlaps using the predicted timeline.</li>
</ul>

<h3>Title: TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift</h3>
<ul>
<li><strong>Authors: </strong>Dipesh Tharu Mahato, Rohan Poudel, Pramod Dhungana</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14217">https://arxiv.org/abs/2506.14217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14217">https://arxiv.org/pdf/2506.14217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14217]] TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift(https://arxiv.org/abs/2506.14217)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Deep neural networks often achieve high accuracy, but ensuring their reliability under adversarial and distributional shifts remains a pressing challenge. We propose TriGuard, a unified safety evaluation framework that combines (1) formal robustness verification, (2) attribution entropy to quantify saliency concentration, and (3) a novel Attribution Drift Score measuring explanation stability. TriGuard reveals critical mismatches between model accuracy and interpretability: verified models can still exhibit unstable reasoning, and attribution-based signals provide complementary safety insights beyond adversarial accuracy. Extensive experiments across three datasets and five architectures show how TriGuard uncovers subtle fragilities in neural reasoning. We further demonstrate that entropy-regularized training reduces explanation drift without sacrificing performance. TriGuard advances the frontier in robust, interpretable model evaluation.</li>
</ul>

<h3>Title: Can Large Language Models Improve Spectral Graph Neural Networks?</h3>
<ul>
<li><strong>Authors: </strong>Kangkang Lu, Yanhua Yu, Zhiyong Huang, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14220">https://arxiv.org/abs/2506.14220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14220">https://arxiv.org/pdf/2506.14220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14220]] Can Large Language Models Improve Spectral Graph Neural Networks?(https://arxiv.org/abs/2506.14220)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Spectral Graph Neural Networks (SGNNs) have attracted significant attention due to their ability to approximate arbitrary filters. They typically rely on supervision from downstream tasks to adaptively learn appropriate filters. However, under label-scarce conditions, SGNNs may learn suboptimal filters, leading to degraded performance. Meanwhile, the remarkable success of Large Language Models (LLMs) has inspired growing interest in exploring their potential within the GNN domain. This naturally raises an important question: \textit{Can LLMs help overcome the limitations of SGNNs and enhance their performance?} In this paper, we propose a novel approach that leverages LLMs to estimate the homophily of a given graph. The estimated homophily is then used to adaptively guide the design of polynomial spectral filters, thereby improving the expressiveness and adaptability of SGNNs across diverse graph structures. Specifically, we introduce a lightweight pipeline in which the LLM generates homophily-aware priors, which are injected into the filter coefficients to better align with the underlying graph topology. Extensive experiments on benchmark datasets demonstrate that our LLM-driven SGNN framework consistently outperforms existing baselines under both homophilic and heterophilic settings, with minimal computational and monetary overhead.</li>
</ul>

<h3>Title: Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team</h3>
<ul>
<li><strong>Authors: </strong>Md Tanzib Hosain, Salman Rahman, Md Kishor Morol, Md Rizwan Parvez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14234">https://arxiv.org/abs/2506.14234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14234">https://arxiv.org/pdf/2506.14234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14234]] Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team(https://arxiv.org/abs/2506.14234)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at this https URL.</li>
</ul>

<h3>Title: Cross-Modal Geometric Hierarchy Fusion: An Implicit-Submap Driven Framework for Resilient 3D Place Recognition</h3>
<ul>
<li><strong>Authors: </strong>Xiaohui Jiang, Haijiang Zhu, Chadei Li, Fulin Tang, Ning An</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14243">https://arxiv.org/abs/2506.14243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14243">https://arxiv.org/pdf/2506.14243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14243]] Cross-Modal Geometric Hierarchy Fusion: An Implicit-Submap Driven Framework for Resilient 3D Place Recognition(https://arxiv.org/abs/2506.14243)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>LiDAR-based place recognition serves as a crucial enabler for long-term autonomy in robotics and autonomous driving systems. Yet, prevailing methodologies relying on handcrafted feature extraction face dual challenges: (1) Inconsistent point cloud density, induced by ego-motion dynamics and environmental disturbances during repeated traversals, leads to descriptor instability, and (2) Representation fragility stems from reliance on single-level geometric abstractions that lack discriminative power in structurally complex scenarios. To address these limitations, we propose a novel framework that redefines 3D place recognition through density-agnostic geometric reasoning. Specifically, we introduce an implicit 3D representation based on elastic points, which is immune to the interference of original scene point cloud density and achieves the characteristic of uniform distribution. Subsequently, we derive the occupancy grid and normal vector information of the scene from this implicit representation. Finally, with the aid of these two types of information, we obtain descriptors that fuse geometric information from both bird's-eye view (capturing macro-level spatial layouts) and 3D segment (encoding micro-scale surface geometries) perspectives. We conducted extensive experiments on numerous datasets (KITTI, KITTI-360, MulRan, NCLT) across diverse environments. The experimental results demonstrate that our method achieves state-of-the-art performance. Moreover, our approach strikes an optimal balance between accuracy, runtime, and memory optimization for historical maps, showcasing excellent Resilient and scalability. Our code will be open-sourced in the future.</li>
</ul>

<h3>Title: Re-Initialization Token Learning for Tool-Augmented Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Li, Liu Liu, Baosheng Yu, Jiayan Qiu, Yibing Zhan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14248">https://arxiv.org/abs/2506.14248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14248">https://arxiv.org/pdf/2506.14248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14248]] Re-Initialization Token Learning for Tool-Augmented Large Language Models(https://arxiv.org/abs/2506.14248)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains.</li>
</ul>

<h3>Title: Convergence-Privacy-Fairness Trade-Off in Personalized Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Xiyu Zhao, Qimei Cui, Weicai Li, Wei Ni, Ekram Hossain, Quan Z. Sheng, Xiaofeng Tao, Ping Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14251">https://arxiv.org/abs/2506.14251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14251">https://arxiv.org/pdf/2506.14251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14251]] Convergence-Privacy-Fairness Trade-Off in Personalized Federated Learning(https://arxiv.org/abs/2506.14251)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate, fair</a></li>
<li><strong>Abstract: </strong>Personalized federated learning (PFL), e.g., the renowned Ditto, strikes a balance between personalization and generalization by conducting federated learning (FL) to guide personalized learning (PL). While FL is unaffected by personalized model training, in Ditto, PL depends on the outcome of the FL. However, the clients' concern about their privacy and consequent perturbation of their local models can affect the convergence and (performance) fairness of PL. This paper presents PFL, called DP-Ditto, which is a non-trivial extension of Ditto under the protection of differential privacy (DP), and analyzes the trade-off among its privacy guarantee, model convergence, and performance distribution fairness. We also analyze the convergence upper bound of the personalized models under DP-Ditto and derive the optimal number of global aggregations given a privacy budget. Further, we analyze the performance fairness of the personalized models, and reveal the feasibility of optimizing DP-Ditto jointly for convergence and fairness. Experiments validate our analysis and demonstrate that DP-Ditto can surpass the DP-perturbed versions of the state-of-the-art PFL models, such as FedAMP, pFedMe, APPLE, and FedALA, by over 32.71% in fairness and 9.66% in accuracy.</li>
</ul>

<h3>Title: synth-dacl: Does Synthetic Defect Data Enhance Segmentation Accuracy and Robustness for Real-World Bridge Inspections?</h3>
<ul>
<li><strong>Authors: </strong>Johannes Flotzinger, Fabian Deuser, Achref Jaziri, Heiko Neumann, Norbert Oswald, Visvanathan Ramesh, Thomas Braml</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14255">https://arxiv.org/abs/2506.14255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14255">https://arxiv.org/pdf/2506.14255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14255]] synth-dacl: Does Synthetic Defect Data Enhance Segmentation Accuracy and Robustness for Real-World Bridge Inspections?(https://arxiv.org/abs/2506.14255)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Adequate bridge inspection is increasingly challenging in many countries due to growing ailing stocks, compounded with a lack of staff and financial resources. Automating the key task of visual bridge inspection, classification of defects and building components on pixel level, improves efficiency, increases accuracy and enhances safety in the inspection process and resulting building assessment. Models overtaking this task must cope with an assortment of real-world conditions. They must be robust to variations in image quality, as well as background texture, as defects often appear on surfaces of diverse texture and degree of weathering. dacl10k is the largest and most diverse dataset for real-world concrete bridge inspections. However, the dataset exhibits class imbalance, which leads to notably poor model performance particularly when segmenting fine-grained classes such as cracks and cavities. This work introduces "synth-dacl", a compilation of three novel dataset extensions based on synthetic concrete textures. These extensions are designed to balance class distribution in dacl10k and enhance model performance, especially for crack and cavity segmentation. When incorporating the synth-dacl extensions, we observe substantial improvements in model robustness across 15 perturbed test sets. Notably, on the perturbed test set, a model trained on dacl10k combined with all synthetic extensions achieves a 2% increase in mean IoU, F1 score, Recall, and Precision compared to the same model trained solely on dacl10k.</li>
</ul>

<h3>Title: Comparison of Two Methods for Stationary Incident Detection Based on Background Image</h3>
<ul>
<li><strong>Authors: </strong>Deepak Ghimire, Joonwhoan Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14256">https://arxiv.org/abs/2506.14256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14256">https://arxiv.org/pdf/2506.14256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14256]] Comparison of Two Methods for Stationary Incident Detection Based on Background Image(https://arxiv.org/abs/2506.14256)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In general, background subtraction-based methods are used to detect moving objects in visual tracking applications. In this paper, we employed a background subtraction-based scheme to detect the temporarily stationary objects. We proposed two schemes for stationary object detection, and we compare those in terms of detection performance and computational complexity. In the first approach, we used a single background, and in the second approach, we used dual backgrounds, generated with different learning rates, in order to detect temporarily stopped objects. Finally, we used normalized cross correlation (NCC) based image comparison to monitor and track the detected stationary object in a video scene. The proposed method is robust with partial occlusion, short-time fully occlusion, and illumination changes, and it can operate in real time.</li>
</ul>

<h3>Title: RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?</h3>
<ul>
<li><strong>Authors: </strong>Rohan Gupta, Erik Jenner</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14261">https://arxiv.org/abs/2506.14261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14261">https://arxiv.org/pdf/2506.14261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14261]] RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?(https://arxiv.org/abs/2506.14261)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Latent-space monitors aim to detect undesirable behaviours in large language models by leveraging internal model representations rather than relying solely on black-box outputs. These methods have shown promise in identifying behaviours such as deception and unsafe completions, but a critical open question remains: can LLMs learn to evade such monitors? To study this, we introduce RL-Obfuscation, in which LLMs are finetuned via reinforcement learning to bypass latent-space monitors while maintaining coherent generations. We apply RL-Obfuscation to LLMs ranging from 7B to 14B parameters and evaluate evasion success against a suite of monitors. We find that token-level latent-space monitors are highly vulnerable to this attack. More holistic monitors, such as max-pooling or attention-based probes, remain robust. Moreover, we show that adversarial policies trained to evade a single static monitor generalise to unseen monitors of the same type. Finally, we study how the policy learned by RL bypasses these monitors and find that the model can also learn to repurpose tokens to mean something different internally.</li>
</ul>

<h3>Title: Knowledge Adaptation as Posterior Correction</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Emtiyaz Khan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14262">https://arxiv.org/abs/2506.14262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14262">https://arxiv.org/pdf/2506.14262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14262]] Knowledge Adaptation as Posterior Correction(https://arxiv.org/abs/2506.14262)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Adaptation is the holy grail of intelligence, but even the best AI models (like GPT) lack the adaptivity of toddlers. So the question remains: how can machines adapt quickly? Despite a lot of progress on model adaptation to facilitate continual and federated learning, as well as model merging, editing, unlearning, etc., little is known about the mechanisms by which machines can naturally learn to adapt in a similar way as humans and animals. Here, we show that all such adaptation methods can be seen as different ways of `correcting' the approximate posteriors. More accurate posteriors lead to smaller corrections, which in turn imply quicker adaptation. The result is obtained by using a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023) where interference created during adaptation is characterized by the natural-gradient mismatch over the past data. We present many examples to demonstrate the use of posterior-correction as a natural mechanism for the machines to learn to adapt quickly.</li>
</ul>

<h3>Title: Towards Robust Learning to Optimize with Theoretical Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Qingyu Song, Wei Lin, Juncheng Wang, Hong Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14263">https://arxiv.org/abs/2506.14263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14263">https://arxiv.org/pdf/2506.14263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14263]] Towards Robust Learning to Optimize with Theoretical Guarantees(https://arxiv.org/abs/2506.14263)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Learning to optimize (L2O) is an emerging technique to solve mathematical optimization problems with learning-based methods. Although with great success in many real-world scenarios such as wireless communications, computer networks, and electronic design, existing L2O works lack theoretical demonstration of their performance and robustness in out-of-distribution (OOD) scenarios. We address this gap by providing comprehensive proofs. First, we prove a sufficient condition for a robust L2O model with homogeneous convergence rates over all In-Distribution (InD) instances. We assume an L2O model achieves robustness for an InD scenario. Based on our proposed methodology of aligning OOD problems to InD problems, we also demonstrate that the L2O model's convergence rate in OOD scenarios will deteriorate by an equation of the L2O model's input features. Moreover, we propose an L2O model with a concise gradient-only feature construction and a novel gradient-based history modeling method. Numerical simulation demonstrates that our proposed model outperforms the state-of-the-art baseline in both InD and OOD scenarios and achieves up to 10 $\times$ convergence speedup. The code of our method can be found from this https URL.</li>
</ul>

<h3>Title: Exploring Non-contrastive Self-supervised Representation Learning for Image-based Profiling</h3>
<ul>
<li><strong>Authors: </strong>Siran Dai, Qianqian Xu, Peisong Wen, Yang Liu, Qingming Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14265">https://arxiv.org/abs/2506.14265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14265">https://arxiv.org/pdf/2506.14265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14265]] Exploring Non-contrastive Self-supervised Representation Learning for Image-based Profiling(https://arxiv.org/abs/2506.14265)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Image-based cell profiling aims to create informative representations of cell images. This technique is critical in drug discovery and has greatly advanced with recent improvements in computer vision. Inspired by recent developments in non-contrastive Self-Supervised Learning (SSL), this paper provides an initial exploration into training a generalizable feature extractor for cell images using such methods. However, there are two major challenges: 1) There is a large difference between the distributions of cell images and natural images, causing the view-generation process in existing SSL methods to fail; and 2) Unlike typical scenarios where each representation is based on a single image, cell profiling often involves multiple input images, making it difficult to effectively combine all available information. To overcome these challenges, we propose SSLProfiler, a non-contrastive SSL framework specifically designed for cell profiling. We introduce specialized data augmentation and representation post-processing methods tailored to cell images, which effectively address the issues mentioned above and result in a robust feature extractor. With these improvements, SSLProfiler won the Cell Line Transferability challenge at CVPR 2025.</li>
</ul>

<h3>Title: Leader360V: The Large-scale, Real-world 360 Video Dataset for Multi-task Learning in Diverse Environment</h3>
<ul>
<li><strong>Authors: </strong>Weiming Zhang, Dingwen Xiao, Aobotao Dai, Yexin Liu, Tianbo Pan, Shiqi Wen, Lei Chen, Lin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14271">https://arxiv.org/abs/2506.14271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14271">https://arxiv.org/pdf/2506.14271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14271]] Leader360V: The Large-scale, Real-world 360 Video Dataset for Multi-task Learning in Diverse Environment(https://arxiv.org/abs/2506.14271)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>360 video captures the complete surrounding scenes with the ultra-large field of view of 360X180. This makes 360 scene understanding tasks, eg, segmentation and tracking, crucial for appications, such as autonomous driving, robotics. With the recent emergence of foundation models, the community is, however, impeded by the lack of large-scale, labelled real-world datasets. This is caused by the inherent spherical properties, eg, severe distortion in polar regions, and content discontinuities, rendering the annotation costly yet complex. This paper introduces Leader360V, the first large-scale, labeled real-world 360 video datasets for instance segmentation and tracking. Our datasets enjoy high scene diversity, ranging from indoor and urban settings to natural and dynamic outdoor scenes. To automate annotation, we design an automatic labeling pipeline, which subtly coordinates pre-trained 2D segmentors and large language models to facilitate the labeling. The pipeline operates in three novel stages. Specifically, in the Initial Annotation Phase, we introduce a Semantic- and Distortion-aware Refinement module, which combines object mask proposals from multiple 2D segmentors with LLM-verified semantic labels. These are then converted into mask prompts to guide SAM2 in generating distortion-aware masks for subsequent frames. In the Auto-Refine Annotation Phase, missing or incomplete regions are corrected either by applying the SDR again or resolving the discontinuities near the horizontal borders. The Manual Revision Phase finally incorporates LLMs and human annotators to further refine and validate the annotations. Extensive user studies and evaluations demonstrate the effectiveness of our labeling pipeline. Meanwhile, experiments confirm that Leader360V significantly enhances model performance for 360 video segmentation and tracking, paving the way for more scalable 360 scene understanding.</li>
</ul>

<h3>Title: From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents</h3>
<ul>
<li><strong>Authors: </strong>Seongbo Jang, Minjin Jeon, Jaehoon Lee, Seonghyeon Lee, Dongha Lee, Hwanjo Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14285">https://arxiv.org/abs/2506.14285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14285">https://arxiv.org/pdf/2506.14285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14285]] From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents(https://arxiv.org/abs/2506.14285)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While research on dialogue response generation has primarily focused on generating coherent responses conditioning on textual context, the critical question of when to respond grounded on the temporal context remains underexplored. To bridge this gap, we propose a novel task called timely dialogue response generation and introduce the TimelyChat benchmark, which evaluates the capabilities of language models to predict appropriate time intervals and generate time-conditioned responses. Additionally, we construct a large-scale training dataset by leveraging unlabeled event knowledge from a temporal commonsense knowledge graph and employing a large language model (LLM) to synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent designed to proactively predict time intervals and generate timely responses that align with those intervals. Experimental results show that Timer outperforms prompting-based LLMs and other fine-tuned baselines in both turn-level and dialogue-level evaluations. We publicly release our data, model, and code.</li>
</ul>

<h3>Title: Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent</h3>
<ul>
<li><strong>Authors: </strong>Xueyang Feng, Jingsen Zhang, Jiakai Tang, Wei Li, Guohao Cai, Xu Chen, Quanyu Dai, Yue Zhu, Zhenhua Dong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14302">https://arxiv.org/abs/2506.14302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14302">https://arxiv.org/pdf/2506.14302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14302]] Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent(https://arxiv.org/abs/2506.14302)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have significantly propelled the development of Conversational Recommendation Agents (CRAs). However, these agents often generate short-sighted responses that fail to sustain user guidance and meet expectations. Although preference optimization has proven effective in aligning LLMs with user expectations, it remains costly and performs poorly in multi-turn dialogue. To address this challenge, we introduce a novel multi-turn preference optimization (MTPO) paradigm ECPO, which leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction throughout multi-turn dialogues, uncovering the underlying causes of dissatisfaction. These causes can be utilized to support targeted optimization of unsatisfactory responses, thereby achieving turn-level preference optimization. ECPO ingeniously eliminates the significant sampling overhead of existing MTPO methods while ensuring the optimization process drives meaningful improvements. To support ECPO, we introduce an LLM-based user simulator, AILO, to simulate user feedback and perform expectation confirmation during conversational recommendations. Experimental results show that ECPO significantly enhances CRA's interaction capabilities, delivering notable improvements in both efficiency and effectiveness over existing MTPO methods.</li>
</ul>

<h3>Title: Fair for a few: Improving Fairness in Doubly Imbalanced Datasets</h3>
<ul>
<li><strong>Authors: </strong>Ata Yalcin, Asli Umay Ozturk, Yigit Sever, Viktoria Pauw, Stephan Hachinger, Ismail Hakki Toroslu, Pinar Karagoz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14306">https://arxiv.org/abs/2506.14306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14306">https://arxiv.org/pdf/2506.14306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14306]] Fair for a few: Improving Fairness in Doubly Imbalanced Datasets(https://arxiv.org/abs/2506.14306)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Fairness has been identified as an important aspect of Machine Learning and Artificial Intelligence solutions for decision making. Recent literature offers a variety of approaches for debiasing, however many of them fall short when the data collection is imbalanced. In this paper, we focus on a particular case, fairness in doubly imbalanced datasets, such that the data collection is imbalanced both for the label and the groups in the sensitive attribute. Firstly, we present an exploratory analysis to illustrate limitations in debiasing on a doubly imbalanced dataset. Then, a multi-criteria based solution is proposed for finding the most suitable sampling and distribution for label and sensitive attribute, in terms of fairness and classification accuracy</li>
</ul>

<h3>Title: FRIDU: Functional Map Refinement with Guided Image Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Avigail Cohen Rimon, Mirela Ben-Chen, Or Litany</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14322">https://arxiv.org/abs/2506.14322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14322">https://arxiv.org/pdf/2506.14322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14322]] FRIDU: Functional Map Refinement with Guided Image Diffusion(https://arxiv.org/abs/2506.14322)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose a novel approach for refining a given correspondence map between two shapes. A correspondence map represented as a functional map, namely a change of basis matrix, can be additionally treated as a 2D image. With this perspective, we train an image diffusion model directly in the space of functional maps, enabling it to generate accurate maps conditioned on an inaccurate initial map. The training is done purely in the functional space, and thus is highly efficient. At inference time, we use the pointwise map corresponding to the current functional map as guidance during the diffusion process. The guidance can additionally encourage different functional map objectives, such as orthogonality and commutativity with the Laplace-Beltrami operator. We show that our approach is competitive with state-of-the-art methods of map refinement and that guided diffusion models provide a promising pathway to functional map processing.</li>
</ul>

<h3>Title: Vulnerability Disclosure or Notification? Best Practices for Reaching Stakeholders at Scale</h3>
<ul>
<li><strong>Authors: </strong>Ting-Han Chen, Jeroen van der Ham-de Vos</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14323">https://arxiv.org/abs/2506.14323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14323">https://arxiv.org/pdf/2506.14323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14323]] Vulnerability Disclosure or Notification? Best Practices for Reaching Stakeholders at Scale(https://arxiv.org/abs/2506.14323)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Security researchers are interested in security vulnerabilities, but these security vulnerabilities create risks for stakeholders. Coordinated Vulnerability Disclosure has been an accepted best practice for many years in disclosing newly discovered vulnerabilities. This practice has mostly worked, but it can become challenging when there are many different parties involved. There has also been research into known vulnerabilities, using datasets or active scans to discover how many machines are still vulnerable. The ethical guidelines suggest that researchers also make an effort to notify the owners of these machines. We posit that this differs from vulnerability disclosure, but rather the practice of vulnerability notification. This practice has some similarities with vulnerability disclosure but should be distinguished from it, providing other challenges and requiring a different approach. Based on our earlier disclosure experience and on prior work documenting their disclosure and notification operations, we provide a meta-review on vulnerability disclosure and notification to observe the shifts in strategies in recent years. We assess how researchers initiated their messaging and examine the outcomes. We then compile the best practices for the existing disclosure guidelines and for notification operations.</li>
</ul>

<h3>Title: LLM-Powered Intent-Based Categorization of Phishing Emails</h3>
<ul>
<li><strong>Authors: </strong>Even Eilertsen, Vasileios Mavroeidis, Gudmund Grov</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14337">https://arxiv.org/abs/2506.14337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14337">https://arxiv.org/pdf/2506.14337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14337]] LLM-Powered Intent-Based Categorization of Phishing Emails(https://arxiv.org/abs/2506.14337)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Phishing attacks remain a significant threat to modern cybersecurity, as they successfully deceive both humans and the defense mechanisms intended to protect them. Traditional detection systems primarily focus on email metadata that users cannot see in their inboxes. Additionally, these systems struggle with phishing emails, which experienced users can often identify empirically by the text alone. This paper investigates the practical potential of Large Language Models (LLMs) to detect these emails by focusing on their intent. In addition to the binary classification of phishing emails, the paper introduces an intent-type taxonomy, which is operationalized by the LLMs to classify emails into distinct categories and, therefore, generate actionable threat information. To facilitate our work, we have curated publicly available datasets into a custom dataset containing a mix of legitimate and phishing emails. Our results demonstrate that existing LLMs are capable of detecting and categorizing phishing emails, underscoring their potential in this domain.</li>
</ul>

<h3>Title: Quantum Enhanced Entropy Pool for Cryptographic Applications and Proofs</h3>
<ul>
<li><strong>Authors: </strong>Buniechukwu Njoku, Sonai Biswas, Milad Ghadimi, Mohammad Shojafar, Gabriele Gradoni, Riccardo Bassoli, Frank H. P. Fitzek</a></li>
<li><strong>Subjects: </strong>cs.CR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14340">https://arxiv.org/abs/2506.14340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14340">https://arxiv.org/pdf/2506.14340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14340]] Quantum Enhanced Entropy Pool for Cryptographic Applications and Proofs(https://arxiv.org/abs/2506.14340)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>This paper investigates the integration of quantum randomness into Verifiable Random Functions (VRFs) using the Ed25519 elliptic curve to strengthen cryptographic security. By replacing traditional pseudorandom number generators with quantum entropy sources, we assess the impact on key security and performance metrics, including execution time, and resource usage. Our approach simulates a modified VRF setup where initialization keys are derived from a quantum random number generator source (QRNG). The results show that while QRNGs could enhance the unpredictability and verifiability of VRFs, their incorporation introduces challenges related to temporal and computational overhead. This study provides valuable insights into the trade-offs of leveraging quantum randomness in API-driven cryptographic systems and offers a potential path toward more secure and efficient protocol design. The QRNG-based system shows increased (key generation times from 50 to 400+ microseconds, verification times from 500 to 3500 microseconds) and higher CPU usage (17% to 30%) compared to the more consistent performance of a Go-based VRF (key generation times below 200 microseconds, verification times under 2000 microseconds, CPU usage below 10%), highlighting trade-offs in computational efficiency and resource demands.</li>
</ul>

<h3>Title: A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Bruno Martins, Piotr Szymański, Piotr Gramacki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14345">https://arxiv.org/abs/2506.14345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14345">https://arxiv.org/pdf/2506.14345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14345]] A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis(https://arxiv.org/abs/2506.14345)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The emergence of Large Language Models (LLMs) has transformed information access, with current LLMs also powering deep research systems that can generate comprehensive report-style answers, through planned iterative search, retrieval, and reasoning. Still, current deep research systems lack the geo-temporal capabilities that are essential for answering context-rich questions involving geographic and/or temporal constraints, frequently occurring in domains like public health, environmental science, or socio-economic analysis. This paper reports our vision towards next generation systems, identifying important technical, infrastructural, and evaluative challenges in integrating geo-temporal reasoning into deep research pipelines. We argue for augmenting retrieval and synthesis processes with the ability to handle geo-temporal constraints, supported by open and reproducible infrastructures and rigorous evaluation protocols. Our vision outlines a path towards more advanced and geo-temporally aware deep research systems, of potential impact to the future of AI-driven information access.</li>
</ul>

<h3>Title: FGA-NN: Film Grain Analysis Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Zoubida Ameur, Frédéric Lefebvre, Philippe De Lagrange, Miloš Radosavljević</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14350">https://arxiv.org/abs/2506.14350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14350">https://arxiv.org/pdf/2506.14350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14350]] FGA-NN: Film Grain Analysis Neural Network(https://arxiv.org/abs/2506.14350)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Film grain, once a by-product of analog film, is now present in most cinematographic content for aesthetic reasons. However, when such content is compressed at medium to low bitrates, film grain is lost due to its random nature. To preserve artistic intent while compressing efficiently, film grain is analyzed and modeled before encoding and synthesized after decoding. This paper introduces FGA-NN, the first learning-based film grain analysis method to estimate conventional film grain parameters compatible with conventional synthesis. Quantitative and qualitative results demonstrate FGA-NN's superior balance between analysis accuracy and synthesis complexity, along with its robustness and applicability.</li>
</ul>

<h3>Title: DGG-XNet: A Hybrid Deep Learning Framework for Multi-Class Brain Disease Classification with Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Sumshun Nahar Eity, Mahin Montasir Afif, Tanisha Fairooz, Md. Mortuza Ahmmed, Md Saef Ullah Miah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14367">https://arxiv.org/abs/2506.14367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14367">https://arxiv.org/pdf/2506.14367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14367]] DGG-XNet: A Hybrid Deep Learning Framework for Multi-Class Brain Disease Classification with Explainable AI(https://arxiv.org/abs/2506.14367)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, generative</a></li>
<li><strong>Abstract: </strong>Accurate diagnosis of brain disorders such as Alzheimer's disease and brain tumors remains a critical challenge in medical imaging. Conventional methods based on manual MRI analysis are often inefficient and error-prone. To address this, we propose DGG-XNet, a hybrid deep learning model integrating VGG16 and DenseNet121 to enhance feature extraction and classification. DenseNet121 promotes feature reuse and efficient gradient flow through dense connectivity, while VGG16 contributes strong hierarchical spatial representations. Their fusion enables robust multiclass classification of neurological conditions. Grad-CAM is applied to visualize salient regions, enhancing model transparency. Trained on a combined dataset from BraTS 2021 and Kaggle, DGG-XNet achieved a test accuracy of 91.33\%, with precision, recall, and F1-score all exceeding 91\%. These results highlight DGG-XNet's potential as an effective and interpretable tool for computer-aided diagnosis (CAD) of neurodegenerative and oncological brain disorders.</li>
</ul>

<h3>Title: ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection</h3>
<ul>
<li><strong>Authors: </strong>Lucile Favero, Daniel Frases, Juan Antonio Pérez-Ortiz, Tanja Käser, Nuria Oliver</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14371">https://arxiv.org/abs/2506.14371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14371">https://arxiv.org/pdf/2506.14371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14371]] ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection(https://arxiv.org/abs/2506.14371)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The widespread adoption of chat interfaces based on Large Language Models (LLMs) raises concerns about promoting superficial learning and undermining the development of critical thinking skills. Instead of relying on LLMs purely for retrieving factual information, this work explores their potential to foster deeper reasoning by generating critical questions that challenge unsupported or vague claims in debate interventions. This study is part of a shared task of the 12th Workshop on Argument Mining, co-located with ACL 2025, focused on automatic critical question generation. We propose a two-step framework involving two small-scale open source language models: a Questioner that generates multiple candidate questions and a Judge that selects the most relevant ones. Our system ranked first in the shared task competition, demonstrating the potential of the proposed LLM-based approach to encourage critical engagement with argumentative texts.</li>
</ul>

<h3>Title: Discrete JEPA: Learning Discrete Token Representations without Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Junyeob Baek, Hosung Lee, Christopher Hoang, Mengye Ren, Sungjin Ahn</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14373">https://arxiv.org/abs/2506.14373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14373">https://arxiv.org/pdf/2506.14373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14373]] Discrete JEPA: Learning Discrete Token Representations without Reconstruction(https://arxiv.org/abs/2506.14373)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The cornerstone of cognitive intelligence lies in extracting hidden patterns from observations and leveraging these principles to systematically predict future outcomes. However, current image tokenization methods demonstrate significant limitations in tasks requiring symbolic abstraction and logical reasoning capabilities essential for systematic inference. To address this challenge, we propose Discrete-JEPA, extending the latent predictive coding framework with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning tasks. Discrete-JEPA dramatically outperforms baselines on visual symbolic prediction tasks, while striking visual evidence reveals the spontaneous emergence of deliberate systematic patterns within the learned semantic token space. Though an initial model, our approach promises a significant impact for advancing Symbolic world modeling and planning capabilities in artificial intelligence systems.</li>
</ul>

<h3>Title: Excessive Reasoning Attack on Reasoning LLMs</h3>
<ul>
<li><strong>Authors: </strong>Wai Man Si, Mingjie Li, Michael Backes, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14374">https://arxiv.org/abs/2506.14374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14374">https://arxiv.org/pdf/2506.14374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14374]] Excessive Reasoning Attack on Reasoning LLMs(https://arxiv.org/abs/2506.14374)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Recent reasoning large language models (LLMs), such as OpenAI o1 and DeepSeek-R1, exhibit strong performance on complex tasks through test-time inference scaling. However, prior studies have shown that these models often incur significant computational costs due to excessive reasoning, such as frequent switching between reasoning trajectories (e.g., underthinking) or redundant reasoning on simple questions (e.g., overthinking). In this work, we expose a novel threat: adversarial inputs can be crafted to exploit excessive reasoning behaviors and substantially increase computational overhead without compromising model utility. Therefore, we propose a novel loss framework consisting of three components: (1) Priority Cross-Entropy Loss, a modification of the standard cross-entropy objective that emphasizes key tokens by leveraging the autoregressive nature of LMs; (2) Excessive Reasoning Loss, which encourages the model to initiate additional reasoning paths during inference; and (3) Delayed Termination Loss, which is designed to extend the reasoning process and defer the generation of final outputs. We optimize and evaluate our attack for the GSM8K and ORCA datasets on DeepSeek-R1-Distill-LLaMA and DeepSeek-R1-Distill-Qwen. Empirical results demonstrate a 3x to 9x increase in reasoning length with comparable utility performance. Furthermore, our crafted adversarial inputs exhibit transferability, inducing computational overhead in o3-mini, o1-mini, DeepSeek-R1, and QWQ models.</li>
</ul>

<h3>Title: DepthSeg: Depth prompting in remote sensing semantic segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ning Zhou, Shanxiong Chen, Mingting Zhou, Haigang Sui, Lieyun Hu, Han Li, Li Hua, Qiming Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14382">https://arxiv.org/abs/2506.14382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14382">https://arxiv.org/pdf/2506.14382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14382]] DepthSeg: Depth prompting in remote sensing semantic segmentation(https://arxiv.org/abs/2506.14382)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Remote sensing semantic segmentation is crucial for extracting detailed land surface information, enabling applications such as environmental monitoring, land use planning, and resource assessment. In recent years, advancements in artificial intelligence have spurred the development of automatic remote sensing semantic segmentation methods. However, the existing semantic segmentation methods focus on distinguishing spectral characteristics of different objects while ignoring the differences in the elevation of the different targets. This results in land cover misclassification in complex scenarios involving shadow occlusion and spectral confusion. In this paper, we introduce a depth prompting two-dimensional (2D) remote sensing semantic segmentation framework (DepthSeg). It automatically models depth/height information from 2D remote sensing images and integrates it into the semantic segmentation framework to mitigate the effects of spectral confusion and shadow occlusion. During the feature extraction phase of DepthSeg, we introduce a lightweight adapter to enable cost-effective fine-tuning of the large-parameter vision transformer encoder pre-trained by natural images. In the depth prompting phase, we propose a depth prompter to model depth/height features explicitly. In the semantic prediction phase, we introduce a semantic classification decoder that couples the depth prompts with high-dimensional land-cover features, enabling accurate extraction of land-cover types. Experiments on the LiuZhou dataset validate the advantages of the DepthSeg framework in land cover mapping tasks. Detailed ablation studies further highlight the significance of the depth prompts in remote sensing semantic segmentation.</li>
</ul>

<h3>Title: GrFormer: A Novel Transformer on Grassmann Manifold for Infrared and Visible Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Huan Kang, Hui Li, Xiao-Jun Wu, Tianyang Xu, Rui Wang, Chunyang Cheng, Josef Kittler</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14384">https://arxiv.org/abs/2506.14384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14384">https://arxiv.org/pdf/2506.14384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14384]] GrFormer: A Novel Transformer on Grassmann Manifold for Infrared and Visible Image Fusion(https://arxiv.org/abs/2506.14384)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In the field of image fusion, promising progress has been made by modeling data from different modalities as linear subspaces. However, in practice, the source images are often located in a non-Euclidean space, where the Euclidean methods usually cannot encapsulate the intrinsic topological structure. Typically, the inner product performed in the Euclidean space calculates the algebraic similarity rather than the semantic similarity, which results in undesired attention output and a decrease in fusion performance. While the balance of low-level details and high-level semantics should be considered in infrared and visible image fusion task. To address this issue, in this paper, we propose a novel attention mechanism based on Grassmann manifold for infrared and visible image fusion (GrFormer). Specifically, our method constructs a low-rank subspace mapping through projection constraints on the Grassmann manifold, compressing attention features into subspaces of varying rank levels. This forces the features to decouple into high-frequency details (local low-rank) and low-frequency semantics (global low-rank), thereby achieving multi-scale semantic fusion. Additionally, to effectively integrate the significant information, we develop a cross-modal fusion strategy (CMS) based on a covariance mask to maximise the complementary properties between different modalities and to suppress the features with high correlation, which are deemed redundant. The experimental results demonstrate that our network outperforms SOTA methods both qualitatively and quantitatively on multiple image fusion benchmarks. The codes are available at this https URL.</li>
</ul>

<h3>Title: Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Conrad Orglmeister, Erik Bochinski, Volker Eiselein, Elvira Fleig</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14390">https://arxiv.org/abs/2506.14390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14390">https://arxiv.org/pdf/2506.14390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14390]] Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection(https://arxiv.org/abs/2506.14390)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Understanding the decision-making and trusting the reliability of Deep Machine Learning Models is crucial for adopting such methods to safety-relevant applications. We extend self-explainable Prototypical Variational models with autoencoder-based out-of-distribution (OOD) detection: A Variational Autoencoder is applied to learn a meaningful latent space which can be used for distance-based classification, likelihood estimation for OOD detection, and reconstruction. The In-Distribution (ID) region is defined by a Gaussian mixture distribution with learned prototypes representing the center of each mode. Furthermore, a novel restriction loss is introduced that promotes a compact ID region in the latent space without collapsing it into single points. The reconstructive capabilities of the Autoencoder ensure the explainability of the prototypes and the ID region of the classifier, further aiding the discrimination of OOD samples. Extensive evaluations on common OOD detection benchmarks as well as a large-scale dataset from a real-world railway application demonstrate the usefulness of the approach, outperforming previous methods.</li>
</ul>

<h3>Title: HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control</h3>
<ul>
<li><strong>Authors: </strong>Yaqiao Zhu, Hongkai Wen, Geyong Min, Man Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14391">https://arxiv.org/abs/2506.14391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14391">https://arxiv.org/pdf/2506.14391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14391]] HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control(https://arxiv.org/abs/2506.14391)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Efficient traffic signal control (TSC) is essential for mitigating urban congestion, yet existing reinforcement learning (RL) methods face challenges in scaling to large networks while maintaining global coordination. Centralized RL suffers from scalability issues, while decentralized approaches often lack unified objectives, resulting in limited network-level efficiency. In this paper, we propose HiLight, a hierarchical reinforcement learning framework with global adversarial guidance for large-scale TSC. HiLight consists of a high-level Meta-Policy, which partitions the traffic network into subregions and generates sub-goals using a Transformer-LSTM architecture, and a low-level Sub-Policy, which controls individual intersections with global awareness. To improve the alignment between global planning and local execution, we introduce an adversarial training mechanism, where the Meta-Policy generates challenging yet informative sub-goals, and the Sub-Policy learns to surpass these targets, leading to more effective coordination. We evaluate HiLight across both synthetic and real-world benchmarks, and additionally construct a large-scale Manhattan network with diverse traffic conditions, including peak transitions, adverse weather, and holiday surges. Experimental results show that HiLight exhibits significant advantages in large-scale scenarios and remains competitive across standard benchmarks of varying sizes.</li>
</ul>

<h3>Title: Consensus Power Inequality: A Comparative Study of Blockchain Networks</h3>
<ul>
<li><strong>Authors: </strong>Kamil Tylinski, Abylay Satybaldy, Paolo Tasca</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14393">https://arxiv.org/abs/2506.14393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14393">https://arxiv.org/pdf/2506.14393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14393]] Consensus Power Inequality: A Comparative Study of Blockchain Networks(https://arxiv.org/abs/2506.14393)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, fair</a></li>
<li><strong>Abstract: </strong>The distribution of consensus power is a cornerstone of decentralisation, influencing the security, resilience, and fairness of blockchain networks while ensuring equitable impact among participants. This study provides a rigorous evaluation of consensus power inequality across five prominent blockchain networks - Bitcoin, Ethereum, Cardano, Hedera, and Algorand - using data collected from January 2022 to July 2024. Leveraging established economic metrics, including the Gini coefficient and Theil index, the research quantitatively assesses how power is distributed among blockchain network participants. A robust dataset, capturing network-specific characteristics such as mining pools, staking patterns, and consensus nodes, forms the foundation of the analysis, enabling meaningful comparisons across diverse architectures. Through an in-depth comparative study, the paper identifies key disparities in consensus power distribution. Hedera and Bitcoin demonstrate more balanced power distribution, aligning closely with the principles of decentralisation. Ethereum and Cardano demonstrate moderate levels of inequality. However, contrary to expectations, Ethereum has become more concentrated following its transition to Proof-of-Stake. Meanwhile, Algorand shows a pronounced centralisation of power. Moreover, the findings highlight the structural and operational drivers of inequality, including economic barriers, governance models, and network effects, offering actionable insights for more equitable network design. This study establishes a methodological framework for evaluating blockchain consensus power inequality, emphasising the importance of targeted strategies to ensure fairer power distribution and enhancing the sustainability of decentralised systems. Future research will build on these findings by integrating additional metrics and examining the influence of emerging consensus mechanisms.</li>
</ul>

<h3>Title: Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yeonkyoung So, Gyuseong Lee, Sungmok Jung, Joonhak Lee, JiA Kang, Sangho Kim, Jaejin Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14397">https://arxiv.org/abs/2506.14397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14397">https://arxiv.org/pdf/2506.14397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14397]] Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding(https://arxiv.org/abs/2506.14397)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Negation is a fundamental linguistic phenomenon that poses persistent challenges for Large Language Models (LLMs), particularly in tasks requiring deep semantic understanding. Existing benchmarks often treat negation as a side case within broader tasks like natural language inference, resulting in a lack of benchmarks that exclusively target negation understanding. In this work, we introduce \textbf{Thunder-NUBench}, a novel benchmark explicitly designed to assess sentence-level negation understanding in LLMs. Thunder-NUBench goes beyond surface-level cue detection by contrasting standard negation with structurally diverse alternatives such as local negation, contradiction, and paraphrase. The benchmark consists of manually curated sentence-negation pairs and a multiple-choice dataset that enables in-depth evaluation of models' negation understanding.</li>
</ul>

<h3>Title: Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tian Xia, Fabio De Sousa Ribeiro, Rajat R Rasal, Avinash Kori, Raghav Mehta, Ben Glocker</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14399">https://arxiv.org/abs/2506.14399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14399">https://arxiv.org/pdf/2506.14399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14399]] Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models(https://arxiv.org/abs/2506.14399)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.</li>
</ul>

<h3>Title: One Size Fits None: Rethinking Fairness in Medical AI</h3>
<ul>
<li><strong>Authors: </strong>Roland Roller, Michael Hahn, Ajay Madhavan Ravichandran, Bilgin Osmanodja, Florian Oetke, Zeineb Sassi, Aljoscha Burchardt, Klaus Netter, Klemens Budde, Anne Herrmann, Tobias Strapatsas, Peter Dabrock, Sebastian Möller</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14400">https://arxiv.org/abs/2506.14400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14400">https://arxiv.org/pdf/2506.14400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14400]] One Size Fits None: Rethinking Fairness in Medical AI(https://arxiv.org/abs/2506.14400)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) models are increasingly used to support clinical decision-making. However, real-world medical datasets are often noisy, incomplete, and imbalanced, leading to performance disparities across patient subgroups. These differences raise fairness concerns, particularly when they reinforce existing disadvantages for marginalized groups. In this work, we analyze several medical prediction tasks and demonstrate how model performance varies with patient characteristics. While ML models may demonstrate good overall performance, we argue that subgroup-level evaluation is essential before integrating them into clinical workflows. By conducting a performance analysis at the subgroup level, differences can be clearly identified-allowing, on the one hand, for performance disparities to be considered in clinical practice, and on the other hand, for these insights to inform the responsible development of more effective models. Thereby, our work contributes to a practical discussion around the subgroup-sensitive development and deployment of medical ML models and the interconnectedness of fairness and transparency.</li>
</ul>

<h3>Title: Causally Steered Diffusion for Automated Video Counterfactual Generation</h3>
<ul>
<li><strong>Authors: </strong>Nikos Spyrou, Athanasios Vlontzos, Paraskevas Pegios, Thomas Melistas, Nefeli Gkouti, Yannis Panagakis, Giorgos Papanastasiou, Sotirios A. Tsaftaris</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14404">https://arxiv.org/abs/2506.14404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14404">https://arxiv.org/pdf/2506.14404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14404]] Causally Steered Diffusion for Automated Video Counterfactual Generation(https://arxiv.org/abs/2506.14404)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic "what-if" video scenarios in diverse areas such as healthcare and digital media.</li>
</ul>

<h3>Title: Compositional Attribute Imbalance in Vision Datasets</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Chen, Yanbiao Ma, Andi Zhang, Weidong Tang, Wei Dai, Bowei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14418">https://arxiv.org/abs/2506.14418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14418">https://arxiv.org/pdf/2506.14418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14418]] Compositional Attribute Imbalance in Vision Datasets(https://arxiv.org/abs/2506.14418)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Visual attribute imbalance is a common yet underexplored issue in image classification, significantly impacting model performance and generalization. In this work, we first define the first-level and second-level attributes of images and then introduce a CLIP-based framework to construct a visual attribute dictionary, enabling automatic evaluation of image attributes. By systematically analyzing both single-attribute imbalance and compositional attribute imbalance, we reveal how the rarity of attributes affects model performance. To tackle these challenges, we propose adjusting the sampling probability of samples based on the rarity of their compositional attributes. This strategy is further integrated with various data augmentation techniques (such as CutMix, Fmix, and SaliencyMix) to enhance the model's ability to represent rare attributes. Extensive experiments on benchmark datasets demonstrate that our method effectively mitigates attribute imbalance, thereby improving the robustness and fairness of deep neural networks. Our research highlights the importance of modeling visual attribute distributions and provides a scalable solution for long-tail image classification tasks.</li>
</ul>

<h3>Title: Toward Rich Video Human-Motion2D Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruihao Xi, Xuekuan Wang, Yongcheng Li, Shuhua Li, Zichen Wang, Yiwei Wang, Feng Wei, Cairong Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14428">https://arxiv.org/abs/2506.14428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14428">https://arxiv.org/pdf/2506.14428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14428]] Toward Rich Video Human-Motion2D Generation(https://arxiv.org/abs/2506.14428)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating realistic and controllable human motions, particularly those involving rich multi-character interactions, remains a significant challenge due to data scarcity and the complexities of modeling inter-personal dynamics. To address these limitations, we first introduce a new large-scale rich video human motion 2D dataset (Motion2D-Video-150K) comprising 150,000 video sequences. Motion2D-Video-150K features a balanced distribution of diverse single-character and, crucially, double-character interactive actions, each paired with detailed textual descriptions. Building upon this dataset, we propose a novel diffusion-based rich video human motion2D generation (RVHM2D) model. RVHM2D incorporates an enhanced textual conditioning mechanism utilizing either dual text encoders (CLIP-L/B) or T5-XXL with both global and local features. We devise a two-stage training strategy: the model is first trained with a standard diffusion objective, and then fine-tuned using reinforcement learning with an FID-based reward to further enhance motion realism and text alignment. Extensive experiments demonstrate that RVHM2D achieves leading performance on the Motion2D-Video-150K benchmark in generating both single and interactive double-character scenarios.</li>
</ul>

<h3>Title: LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiaoran Liu, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14429">https://arxiv.org/abs/2506.14429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14429">https://arxiv.org/pdf/2506.14429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14429]] LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs(https://arxiv.org/abs/2506.14429)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \textbf{\textit{stable perplexity}} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \textbf{\textit{local perception}} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.</li>
</ul>

<h3>Title: Model compression using knowledge distillation with integrated gradients</h3>
<ul>
<li><strong>Authors: </strong>David E. Hernandez, Jose Chang, Torbjörn E. M. Nordling</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14440">https://arxiv.org/abs/2506.14440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14440">https://arxiv.org/pdf/2506.14440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14440]] Model compression using knowledge distillation with integrated gradients(https://arxiv.org/abs/2506.14440)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Model compression is critical for deploying deep learning models on resource-constrained devices. We introduce a novel method enhancing knowledge distillation with integrated gradients (IG) as a data augmentation strategy. Our approach overlays IG maps onto input images during training, providing student models with deeper insights into teacher models' decision-making processes. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented knowledge distillation achieves 92.6% testing accuracy with a 4.1x compression factor-a significant 1.1 percentage point improvement ($p<0.001$) over non-distilled models (91.5%). This compression reduces inference time from 140 ms to 13 ms. Our method precomputes IG maps before training, transforming substantial runtime costs into a one-time preprocessing step. Our comprehensive experiments include: (1) comparisons with attention transfer, revealing complementary benefits when combined with our approach; (2) Monte Carlo simulations confirming statistical robustness; (3) systematic evaluation of compression factor versus accuracy trade-offs across a wide range (2.2x-1122x); and (4) validation on an ImageNet subset aligned with CIFAR-10 classes, demonstrating generalisability beyond the initial dataset. These extensive ablation studies confirm that IG-based knowledge distillation consistently outperforms conventional approaches across varied architectures and compression ratios. Our results establish this framework as a viable compression technique for real-world deployment on edge devices while maintaining competitive accuracy.</li>
</ul>

<h3>Title: How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison</h3>
<ul>
<li><strong>Authors: </strong>Jiayin Wang, Zhiquang Guo, Weizhi Ma, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14448">https://arxiv.org/abs/2506.14448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14448">https://arxiv.org/pdf/2506.14448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14448]] How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison(https://arxiv.org/abs/2506.14448)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As evaluation designs of large language models may shape our trajectory toward artificial general intelligence, comprehensive and forward-looking assessment is essential. Existing benchmarks primarily assess static knowledge, while intelligence also entails the ability to rapidly learn from experience. To this end, we advocate for the evaluation of Test-time Learning, the capacity to improve performance in experience-based, reasoning-intensive tasks during test time. In this work, we propose semantic games as effective testbeds for evaluating test-time learning, due to their resistance to saturation and inherent demand for strategic reasoning. We introduce an objective evaluation framework that compares model performance under both limited and cumulative experience settings, and contains four forms of experience representation. To provide a comparative baseline, we recruit eight human participants to complete the same task. Results show that LLMs exhibit measurable test-time learning capabilities; however, their improvements are less stable under cumulative experience and progress more slowly than those observed in humans. These findings underscore the potential of LLMs as general-purpose learning machines, while also revealing a substantial intellectual gap between models and humans, irrespective of how well LLMs perform on static benchmarks.</li>
</ul>

<h3>Title: Adapting Lightweight Vision Language Models for Radiological Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Aditya Shourya, Michel Dumontier, Chang Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14451">https://arxiv.org/abs/2506.14451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14451">https://arxiv.org/pdf/2506.14451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14451]] Adapting Lightweight Vision Language Models for Radiological Visual Question Answering(https://arxiv.org/abs/2506.14451)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis.</li>
</ul>

<h3>Title: MalGuard: Towards Real-Time, Accurate, and Actionable Detection of Malicious Packages in PyPI Ecosystem</h3>
<ul>
<li><strong>Authors: </strong>Xingan Gao, Xiaobing Sun, Sicong Cao, Kaifeng Huang, Di Wu, Xiaolei Liu, Xingwei Lin, Yang Xiang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14466">https://arxiv.org/abs/2506.14466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14466">https://arxiv.org/pdf/2506.14466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14466]] MalGuard: Towards Real-Time, Accurate, and Actionable Detection of Malicious Packages in PyPI Ecosystem(https://arxiv.org/abs/2506.14466)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, explainability, large language model</a></li>
<li><strong>Abstract: </strong>Malicious package detection has become a critical task in ensuring the security and stability of the PyPI. Existing detection approaches have focused on advancing model selection, evolving from traditional machine learning (ML) models to large language models (LLMs). However, as the complexity of the model increases, the time consumption also increases, which raises the question of whether a lightweight model achieves effective detection. Through empirical research, we demonstrate that collecting a sufficiently comprehensive feature set enables even traditional ML models to achieve outstanding performance. However, with the continuous emergence of new malicious packages, considerable human and material resources are required for feature analysis. Also, traditional ML model-based approaches lack of explainability to malicious this http URL, we propose a novel approach MalGuard based on graph centrality analysis and the LIME (Local Interpretable Model-agnostic Explanations) algorithm to detect malicious this http URL overcome the above two challenges, we leverage graph centrality analysis to extract sensitive APIs automatically to replace manual analysis. To understand the sensitive APIs, we further refine the feature set using LLM and integrate the LIME algorithm with ML models to provide explanations for malicious packages. We evaluated MalGuard against six SOTA baselines with the same settings. Experimental results show that our proposed MalGuard, improves precision by 0.5%-33.2% and recall by 1.8%-22.1%. With MalGuard, we successfully identified 113 previously unknown malicious packages from a pool of 64,348 newly-uploaded packages over a five-week period, and 109 out of them have been removed by the PyPI official.</li>
</ul>

<h3>Title: Dense360: Dense Understanding from Omnidirectional Panoramas</h3>
<ul>
<li><strong>Authors: </strong>Yikang Zhou, Tao Zhang, Dizhe Zhang, Shunping Ji, Xiangtai Li, Lu Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14471">https://arxiv.org/abs/2506.14471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14471">https://arxiv.org/pdf/2506.14471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14471]] Dense360: Dense Understanding from Omnidirectional Panoramas(https://arxiv.org/abs/2506.14471)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) require comprehensive visual inputs to achieve dense understanding of the physical world. While existing MLLMs demonstrate impressive world understanding capabilities through limited field-of-view (FOV) visual inputs (e.g., 70 degree), we take the first step toward dense understanding from omnidirectional panoramas. We first introduce an omnidirectional panoramas dataset featuring a comprehensive suite of reliability-scored annotations. Specifically, our dataset contains 160K panoramas with 5M dense entity-level captions, 1M unique referring expressions, and 100K entity-grounded panoramic scene descriptions. Compared to multi-view alternatives, panoramas can provide more complete, compact, and continuous scene representations through equirectangular projections (ERP). However, the use of ERP introduces two key challenges for MLLMs: i) spatial continuity along the circle of latitude, and ii) latitude-dependent variation in information density. We address these challenges through ERP-RoPE, a position encoding scheme specifically designed for panoramic ERP. In addition, we introduce Dense360-Bench, the first benchmark for evaluating MLLMs on omnidirectional captioning and grounding, establishing a comprehensive framework for advancing dense visual-language understanding in panoramic settings.</li>
</ul>

<h3>Title: LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data</h3>
<ul>
<li><strong>Authors: </strong>Eyal German, Sagiv Antebi, Edan Habler, Asaf Shabtai, Yuval Elovici</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14474">https://arxiv.org/abs/2506.14474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14474">https://arxiv.org/pdf/2506.14474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14474]] LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data(https://arxiv.org/abs/2506.14474)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, steal, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can be trained or fine-tuned on data obtained without the owner's consent. Verifying whether a specific LLM was trained on particular data instances or an entire dataset is extremely challenging. Dataset watermarking addresses this by embedding identifiable modifications in training data to detect unauthorized use. However, existing methods often lack stealth, making them relatively easy to detect and remove. In light of these limitations, we propose LexiMark, a novel watermarking technique designed for text and documents, which embeds synonym substitutions for carefully selected high-entropy words. Our method aims to enhance an LLM's memorization capabilities on the watermarked text without altering the semantic integrity of the text. As a result, the watermark is difficult to detect, blending seamlessly into the text with no visible markers, and is resistant to removal due to its subtle, contextually appropriate substitutions that evade automated and manual detection. We evaluated our method using baseline datasets from recent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral 7B, Pythia 6.9B, as well as three smaller variants from the Pythia family (160M, 410M, and 1B). Our evaluation spans multiple training settings, including continued pretraining and fine-tuning scenarios. The results demonstrate significant improvements in AUROC scores compared to existing methods, underscoring our method's effectiveness in reliably verifying whether unauthorized watermarked data was used in LLM training.</li>
</ul>

<h3>Title: ReDASH: Fast and efficient Scaling in Arithmetic Garbled Circuits for Secure Outsourced Inference</h3>
<ul>
<li><strong>Authors: </strong>Felix Maurer, Jonas Sander, Thomas Eisenbarth</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14489">https://arxiv.org/abs/2506.14489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14489">https://arxiv.org/pdf/2506.14489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14489]] ReDASH: Fast and efficient Scaling in Arithmetic Garbled Circuits for Secure Outsourced Inference(https://arxiv.org/abs/2506.14489)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>ReDash extends Dash's arithmetic garbled circuits to provide a more flexible and efficient framework for secure outsourced inference. By introducing a novel garbled scaling gadget based on a generalized base extension for the residue number system, ReDash removes Dash's limitation of scaling exclusively by powers of two. This enables arbitrary scaling factors drawn from the residue number system's modular base, allowing for tailored quantization schemes and more efficient model evaluation. Through the new $\text{ScaleQuant}^+$ quantization mechanism, ReDash supports optimized modular bases that can significantly reduce the overhead of arithmetic operations during convolutional neural network inference. ReDash achieves up to a 33-fold speedup in overall inference time compared to Dash Despite these enhancements, ReDash preserves the robust security guarantees of arithmetic garbling. By delivering both performance gains and quantization flexibility, ReDash expands the practicality of garbled convolutional neural network inference.</li>
</ul>

<h3>Title: LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops</h3>
<ul>
<li><strong>Authors: </strong>Jiyuan Fu, Kaixun Jiang, Lingyi Hong, Jinglun Li, Haijing Guo, Dingkang Yang, Zhaoyu Chen, Wenqiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14493">https://arxiv.org/abs/2506.14493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14493">https://arxiv.org/pdf/2506.14493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14493]] LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops(https://arxiv.org/abs/2506.14493)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, generative, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have shown great promise but require substantial computational resources during inference. Attackers can exploit this by inducing excessive output, leading to resource exhaustion and service degradation. Prior energy-latency attacks aim to increase generation time by broadly shifting the output token distribution away from the EOS token, but they neglect the influence of token-level Part-of-Speech (POS) characteristics on EOS and sentence-level structural patterns on output counts, limiting their efficacy. To address this, we propose LingoLoop, an attack designed to induce MLLMs to generate excessively verbose and repetitive sequences. First, we find that the POS tag of a token strongly affects the likelihood of generating an EOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to postpone EOS token generation by adjusting attention weights guided by POS information. Second, we identify that constraining output diversity to induce repetitive loops is effective for sustained generation. We introduce a Generative Path Pruning Mechanism that limits the magnitude of hidden states, encouraging the model to produce persistent loops. Extensive experiments demonstrate LingoLoop can increase generated tokens by up to 30 times and energy consumption by a comparable factor on models like Qwen2.5-VL-3B, consistently driving MLLMs towards their maximum generation limits. These findings expose significant MLLMs' vulnerabilities, posing challenges for their reliable deployment. The code will be released publicly following the paper's acceptance.</li>
</ul>

<h3>Title: I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs</h3>
<ul>
<li><strong>Authors: </strong>Yu Qi, Lipeng Gu, Honghua Chen, Liangliang Nan, Mingqiang Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14495">https://arxiv.org/abs/2506.14495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14495">https://arxiv.org/pdf/2506.14495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14495]] I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs(https://arxiv.org/abs/2506.14495)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing 3D visual grounding methods rely on precise text prompts to locate objects within 3D scenes. Speech, as a natural and intuitive modality, offers a promising alternative. Real-world speech inputs, however, often suffer from transcription errors due to accents, background noise, and varying speech rates, limiting the applicability of existing 3DVG methods. To address these challenges, we propose \textbf{SpeechRefer}, a novel 3DVG framework designed to enhance performance in the presence of noisy and ambiguous speech-to-text transcriptions. SpeechRefer integrates seamlessly with xisting 3DVG models and introduces two key innovations. First, the Speech Complementary Module captures acoustic similarities between phonetically related words and highlights subtle distinctions, generating complementary proposal scores from the speech signal. This reduces dependence on potentially erroneous transcriptions. Second, the Contrastive Complementary Module employs contrastive learning to align erroneous text features with corresponding speech features, ensuring robust performance even when transcription errors dominate. Extensive experiments on the SpeechRefer and peechNr3D datasets demonstrate that SpeechRefer improves the performance of existing 3DVG methods by a large margin, which highlights SpeechRefer's potential to bridge the gap between noisy speech inputs and reliable 3DVG, enabling more intuitive and practical multimodal systems.</li>
</ul>

<h3>Title: MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution</h3>
<ul>
<li><strong>Authors: </strong>Zhiwen Shao, Yifan Cheng, Feiran Li, Yong Zhou, Xuequan Lu, Yuan Xie, Lizhuang Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14511">https://arxiv.org/abs/2506.14511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14511">https://arxiv.org/pdf/2506.14511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14511]] MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution(https://arxiv.org/abs/2506.14511)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Facial micro-expression recognition (MER) is a challenging problem, due to transient and subtle micro-expression (ME) actions. Most existing methods depend on hand-crafted features, key frames like onset, apex, and offset frames, or deep networks limited by small-scale and low-diversity datasets. In this paper, we propose an end-to-end micro-action-aware deep learning framework with advantages from transformer, graph convolution, and vanilla convolution. In particular, we propose a novel F5C block composed of fully-connected convolution and channel correspondence convolution to directly extract local-global features from a sequence of raw frames, without the prior knowledge of key frames. The transformer-style fully-connected convolution is proposed to extract local features while maintaining global receptive fields, and the graph-style channel correspondence convolution is introduced to model the correlations among feature patterns. Moreover, MER, optical flow estimation, and facial landmark detection are jointly trained by sharing the local-global features. The two latter tasks contribute to capturing facial subtle action information for MER, which can alleviate the impact of insufficient training data. Extensive experiments demonstrate that our framework (i) outperforms the state-of-the-art MER methods on CASME II, SAMM, and SMIC benchmarks, (ii) works well for optical flow estimation and facial landmark detection, and (iii) can capture facial subtle muscle actions in local regions associated with MEs. The code is available at this https URL.</li>
</ul>

<h3>Title: SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Zijian Song, Xiaoxin Lin, Qiuming Huang, Guangrun Wang, Liang Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14512">https://arxiv.org/abs/2506.14512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14512">https://arxiv.org/pdf/2506.14512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14512]] SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks(https://arxiv.org/abs/2506.14512)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are experiencing rapid advancements in complex reasoning, exhibiting remarkable generalization in mathematics and programming. In contrast, while spatial intelligence is fundamental for Vision-Language Models (VLMs) in real-world interaction, the systematic evaluation of their complex reasoning ability within spatial contexts remains underexplored. To bridge this gap, we introduce SIRI-Bench, a benchmark designed to evaluate VLMs' spatial intelligence through video-based reasoning tasks. SIRI-Bench comprises nearly 1K video-question-answer triplets, where each problem is embedded in a realistic 3D scene and captured by video. By carefully designing questions and corresponding 3D scenes, our benchmark ensures that solving the questions requires both spatial comprehension for extracting information and high-level reasoning for deriving solutions, making it a challenging benchmark for evaluating VLMs. To facilitate large-scale data synthesis, we develop an Automatic Scene Creation Engine. This engine, leveraging multiple specialized LLM agents, can generate realistic 3D scenes from abstract math problems, ensuring faithfulness to the original descriptions. Experimental results reveal that state-of-the-art VLMs struggle significantly on SIRI-Bench, underscoring the challenge of spatial reasoning. We hope that our study will bring researchers' attention to spatially grounded reasoning and advance VLMs in visual problem-solving.</li>
</ul>

<h3>Title: Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Prabhav Sanga, Jaskaran Singh, Arun K. Dubey</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14515">https://arxiv.org/abs/2506.14515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14515">https://arxiv.org/pdf/2506.14515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14515]] Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning(https://arxiv.org/abs/2506.14515)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>As machine learning systems increasingly rely on data subject to privacy regulation, selectively unlearning specific information from trained models has become essential. In image classification, this involves removing the influence of particular training samples, semantic classes, or visual styles without full retraining. We introduce \textbf{Forget-Aligned Model Reconstruction (FAMR)}, a theoretically grounded and computationally efficient framework for post-hoc unlearning in deep image classifiers. FAMR frames forgetting as a constrained optimization problem that minimizes a uniform-prediction loss on the forget set while anchoring model parameters to their original values via an $\ell_2$ penalty. A theoretical analysis links FAMR's solution to influence-function-based retraining approximations, with bounds on parameter and output deviation. Empirical results on class forgetting tasks using CIFAR-10 and ImageNet-100 demonstrate FAMR's effectiveness, with strong performance retention and minimal computational overhead. The framework generalizes naturally to concept and style erasure, offering a scalable and certifiable route to efficient post-hoc forgetting in vision models.</li>
</ul>

<h3>Title: VisLanding: Monocular 3D Perception for UAV Safe Landing via Depth-Normal Synergy</h3>
<ul>
<li><strong>Authors: </strong>Zhuoyue Tan, Boyong He, Yuxiang Ji, Liaoni Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14525">https://arxiv.org/abs/2506.14525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14525">https://arxiv.org/pdf/2506.14525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14525]] VisLanding: Monocular 3D Perception for UAV Safe Landing via Depth-Normal Synergy(https://arxiv.org/abs/2506.14525)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents VisLanding, a monocular 3D perception-based framework for safe UAV (Unmanned Aerial Vehicle) landing. Addressing the core challenge of autonomous UAV landing in complex and unknown environments, this study innovatively leverages the depth-normal synergy prediction capabilities of the Metric3D V2 model to construct an end-to-end safe landing zones (SLZ) estimation framework. By introducing a safe zone segmentation branch, we transform the landing zone estimation task into a binary semantic segmentation problem. The model is fine-tuned and annotated using the WildUAV dataset from a UAV perspective, while a cross-domain evaluation dataset is constructed to validate the model's robustness. Experimental results demonstrate that VisLanding significantly enhances the accuracy of safe zone identification through a depth-normal joint optimization mechanism, while retaining the zero-shot generalization advantages of Metric3D V2. The proposed method exhibits superior generalization and robustness in cross-domain testing compared to other approaches. Furthermore, it enables the estimation of landing zone area by integrating predicted depth and normal information, providing critical decision-making support for practical applications.</li>
</ul>

<h3>Title: Automated Decision-Making on Networks with LLMs through Knowledge-Guided Evolution</h3>
<ul>
<li><strong>Authors: </strong>Xiaohan Zheng, Lanning Wei, Yong Li, Quanming Yao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14529">https://arxiv.org/abs/2506.14529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14529">https://arxiv.org/pdf/2506.14529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14529]] Automated Decision-Making on Networks with LLMs through Knowledge-Guided Evolution(https://arxiv.org/abs/2506.14529)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Effective decision-making on networks often relies on learning from graph-structured data, where Graph Neural Networks (GNNs) play a central role, but they take efforts to configure and tune. In this demo, we propose LLMNet, showing how to design GNN automated through Large Language Models. Our system develops a set of agents that construct graph-related knowlege bases and then leverages Retrieval-Augmented Generation (RAG) to support automated configuration and refinement of GNN models through a knowledge-guided evolution process. These agents, equipped with specialized knowledge bases, extract insights into tasks and graph structures by interacting with the knowledge bases. Empirical results show LLMNet excels in twelve datasets across three graph learning tasks, validating its effectiveness of GNN model designing.</li>
</ul>

<h3>Title: M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Can Zheng, Jiguang He, Chung G. Kang, Guofa Cai, Zitong Yu, Merouane Debbah</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14532">https://arxiv.org/abs/2506.14532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14532">https://arxiv.org/pdf/2506.14532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14532]] M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models(https://arxiv.org/abs/2506.14532)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel neural network framework called M2BeamLLM for beam prediction in millimeter-wave (mmWave) massive multi-input multi-output (mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data, including images, radar, LiDAR, and GPS, leveraging the powerful reasoning capabilities of large language models (LLMs) such as GPT-2 for beam prediction. By combining sensing data encoding, multimodal alignment and fusion, and supervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam prediction accuracy and robustness, demonstrably outperforming traditional deep learning (DL) models in both standard and few-shot scenarios. Furthermore, its prediction performance consistently improves with increased diversity in sensing modalities. Our study provides an efficient and intelligent beam prediction solution for vehicle-to-infrastructure (V2I) mmWave communication systems.</li>
</ul>

<h3>Title: Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs</h3>
<ul>
<li><strong>Authors: </strong>Gerardo A. Flores, Alyssa H. Smith, Julia A. Fukuyama, Ashia C. Wilson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14540">https://arxiv.org/abs/2506.14540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14540">https://arxiv.org/pdf/2506.14540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14540]] Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs(https://arxiv.org/abs/2506.14540)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Machine learning-based decision support systems are increasingly deployed in clinical settings, where probabilistic scoring functions are used to inform and prioritize patient management decisions. However, widely used scoring rules, such as accuracy and AUC-ROC, fail to adequately reflect key clinical priorities, including calibration, robustness to distributional shifts, and sensitivity to asymmetric error costs. In this work, we propose a principled yet practical evaluation framework for selecting calibrated thresholded classifiers that explicitly accounts for the uncertainty in class prevalences and domain-specific cost asymmetries often found in clinical settings. Building on the theory of proper scoring rules, particularly the Schervish representation, we derive an adjusted variant of cross-entropy (log score) that averages cost-weighted performance over clinically relevant ranges of class balance. The resulting evaluation is simple to apply, sensitive to clinical deployment conditions, and designed to prioritize models that are both calibrated and robust to real-world variations.</li>
</ul>

<h3>Title: Exploring Diffusion with Test-Time Training on Efficient Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Rongchang Lu, Tianduo Luo, Yunzhi Zhang, Conghan Yue, Pei Yang, Guibao Liu, Changyang Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14541">https://arxiv.org/abs/2506.14541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14541">https://arxiv.org/pdf/2506.14541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14541]] Exploring Diffusion with Test-Time Training on Efficient Image Restoration(https://arxiv.org/abs/2506.14541)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image restoration faces challenges including ineffective feature fusion, computational bottlenecks and inefficient diffusion processes. To address these, we propose DiffRWKVIR, a novel framework unifying Test-Time Training (TTT) with efficient diffusion. Our approach introduces three key innovations: (1) Omni-Scale 2D State Evolution extends RWKV's location-dependent parameterization to hierarchical multi-directional 2D scanning, enabling global contextual awareness with linear complexity O(L); (2) Chunk-Optimized Flash Processing accelerates intra-chunk parallelism by 3.2x via contiguous chunk processing (O(LCd) complexity), reducing sequential dependencies and computational overhead; (3) Prior-Guided Efficient Diffusion extracts a compact Image Prior Representation (IPR) in only 5-20 steps, proving 45% faster training/inference than DiffIR while solving computational inefficiency in denoising. Evaluated across super-resolution and inpainting benchmarks (Set5, Set14, BSD100, Urban100, Places365), DiffRWKVIR outperforms SwinIR, HAT, and MambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency metrics. Our method establishes a new paradigm for adaptive, high-efficiency image restoration with optimized hardware utilization.</li>
</ul>

<h3>Title: DreamLight: Towards Harmonious and Consistent Image Relighting</h3>
<ul>
<li><strong>Authors: </strong>Yong Liu, Wenpeng Xiao, Qianqian Wang, Junlin Chen, Shiyin Wang, Yitong Wang, Xinglong Wu, Yansong Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14549">https://arxiv.org/abs/2506.14549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14549">https://arxiv.org/pdf/2506.14549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14549]] DreamLight: Towards Harmonious and Consistent Image Relighting(https://arxiv.org/abs/2506.14549)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce a model named DreamLight for universal image relighting in this work, which can seamlessly composite subjects into a new background while maintaining aesthetic uniformity in terms of lighting and color tone. The background can be specified by natural images (image-based relighting) or generated from unlimited text prompts (text-based relighting). Existing studies primarily focus on image-based relighting, while with scant exploration into text-based scenarios. Some works employ intricate disentanglement pipeline designs relying on environment maps to provide relevant information, which grapples with the expensive data cost required for intrinsic decomposition and light source. Other methods take this task as an image translation problem and perform pixel-level transformation with autoencoder architecture. While these methods have achieved decent harmonization effects, they struggle to generate realistic and natural light interaction effects between the foreground and background. To alleviate these challenges, we reorganize the input data into a unified format and leverage the semantic prior provided by the pretrained diffusion model to facilitate the generation of natural results. Moreover, we propose a Position-Guided Light Adapter (PGLA) that condenses light information from different directions in the background into designed light query embeddings, and modulates the foreground with direction-biased masked attention. In addition, we present a post-processing module named Spectral Foreground Fixer (SFF) to adaptively reorganize different frequency components of subject and relighted background, which helps enhance the consistency of foreground appearance. Extensive comparisons and user study demonstrate that our DreamLight achieves remarkable relighting performance.</li>
</ul>

<h3>Title: Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images</h3>
<ul>
<li><strong>Authors: </strong>David Butler, Adrian Hilton, Gustavo Carneiro</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14560">https://arxiv.org/abs/2506.14560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14560">https://arxiv.org/pdf/2506.14560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14560]] Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images(https://arxiv.org/abs/2506.14560)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion</a></li>
<li><strong>Abstract: </strong>Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2\%, achieving an AUC of 0.71 in predicting knee OA progression while offering ~9% faster inference time.</li>
</ul>

<h3>Title: AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Di He, Ajay Jaiswal, Songjun Tu, Li Shen, Ganzhao Yuan, Shiwei Liu, Lu Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14562">https://arxiv.org/abs/2506.14562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14562">https://arxiv.org/pdf/2506.14562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14562]] AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs(https://arxiv.org/abs/2506.14562)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.</li>
</ul>

<h3>Title: Single-Example Learning in a Mixture of GPDMs with Latent Geometries</h3>
<ul>
<li><strong>Authors: </strong>Jesse St. Amand, Leonardo Gizzi, Martin A. Giese</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14563">https://arxiv.org/abs/2506.14563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14563">https://arxiv.org/pdf/2506.14563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14563]] Single-Example Learning in a Mixture of GPDMs with Latent Geometries(https://arxiv.org/abs/2506.14563)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, generative</a></li>
<li><strong>Abstract: </strong>We present the Gaussian process dynamical mixture model (GPDMM) and show its utility in single-example learning of human motion data. The Gaussian process dynamical model (GPDM) is a form of the Gaussian process latent variable model (GPLVM), but optimized with a hidden Markov model dynamical prior. The GPDMM combines multiple GPDMs in a probabilistic mixture-of-experts framework, utilizing embedded geometric features to allow for diverse sequences to be encoded in a single latent space, enabling the categorization and generation of each sequence class. GPDMs and our mixture model are particularly advantageous in addressing the challenges of modeling human movement in scenarios where data is limited and model interpretability is vital, such as in patient-specific medical applications like prosthesis control. We score the GPDMM on classification accuracy and generative ability in single-example learning, showcase model variations, and benchmark it against LSTMs, VAEs, and transformers.</li>
</ul>

<h3>Title: Anonymous Authentication using Attribute-based Encryption</h3>
<ul>
<li><strong>Authors: </strong>Nouha Oualha</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14566">https://arxiv.org/abs/2506.14566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14566">https://arxiv.org/pdf/2506.14566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14566]] Anonymous Authentication using Attribute-based Encryption(https://arxiv.org/abs/2506.14566)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>In today's digital age, personal data is constantly at risk of compromise. Attribute-Based Encryption (ABE) has emerged as a promising approach to privacy-preserving data protection. This paper proposes an anonymous authentication mechanism based on ABE, which allows users to authenticate without revealing their identity. The mechanism adds a privacy-preserving layer by enabling authorization based solely on user attributes. The proposed approach is implemented using OpenID Connect, demonstrating its feasibility in real-world systems.</li>
</ul>

<h3>Title: TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Mingkang Zhu, Xi Chen, Zhongdao Wang, Bei Yu, Hengshuang Zhao, Jiaya Jia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14574">https://arxiv.org/abs/2506.14574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14574">https://arxiv.org/pdf/2506.14574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14574]] TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization(https://arxiv.org/abs/2506.14574)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at this https URL.</li>
</ul>

<h3>Title: SoK: Privacy-Enhancing Technologies in Artificial Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Nouha Oualha</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14576">https://arxiv.org/abs/2506.14576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14576">https://arxiv.org/pdf/2506.14576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14576]] SoK: Privacy-Enhancing Technologies in Artificial Intelligence(https://arxiv.org/abs/2506.14576)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>As artificial intelligence (AI) continues to permeate various sectors, safeguarding personal and sensitive data has become increasingly crucial. To address these concerns, privacy-enhancing technologies (PETs) have emerged as a suite of digital tools that enable data collection and processing while preserving privacy. This paper explores the current landscape of data privacy in the context of AI, reviews the integration of PETs within AI systems, and assesses both their achievements and the challenges that remain.</li>
</ul>

<h3>Title: Object-Centric Neuro-Argumentative Learning</h3>
<ul>
<li><strong>Authors: </strong>Abdul Rahman Jacob, Avinash Kori, Emanuele De Angelis, Ben Glocker, Maurizio Proietti, Francesca Toni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14577">https://arxiv.org/abs/2506.14577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14577">https://arxiv.org/pdf/2506.14577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14577]] Object-Centric Neuro-Argumentative Learning(https://arxiv.org/abs/2506.14577)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Over the last decade, as we rely more on deep learning technologies to make critical decisions, concerns regarding their safety, reliability and interpretability have emerged. We introduce a novel Neural Argumentative Learning (NAL) architecture that integrates Assumption-Based Argumentation (ABA) with deep learning for image analysis. Our architecture consists of neural and symbolic components. The former segments and encodes images into facts using object-centric learning, while the latter applies ABA learning to develop ABA frameworks enabling predictions with images. Experiments on synthetic data show that the NAL architecture can be competitive with a state-of-the-art alternative.</li>
</ul>

<h3>Title: GenerationPrograms: Fine-grained Attribution with Executable Programs</h3>
<ul>
<li><strong>Authors: </strong>David Wan, Eran Hirsch, Elias Stengel-Eskin, Ido Dagan, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14580">https://arxiv.org/abs/2506.14580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14580">https://arxiv.org/pdf/2506.14580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14580]] GenerationPrograms: Fine-grained Attribution with Executable Programs(https://arxiv.org/abs/2506.14580)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Recent large language models (LLMs) achieve impressive performance in source-conditioned text generation but often fail to correctly provide fine-grained attributions for their outputs, undermining verifiability and trust. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable "code agent" architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program's specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both the document level and sentence level across two long-form question-answering tasks and a multi-document summarization task. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality.</li>
</ul>

<h3>Title: Busting the Paper Ballot: Voting Meets Adversarial Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Kaleel Mahmood, Caleb Manicke, Ethan Rathbun, Aayushi Verma, Sohaib Ahmad, Nicholas Stamatakis, Laurent Michel, Benjamin Fuller</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14582">https://arxiv.org/abs/2506.14582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14582">https://arxiv.org/pdf/2506.14582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14582]] Busting the Paper Ballot: Voting Meets Adversarial Machine Learning(https://arxiv.org/abs/2506.14582)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, transformer</a></li>
<li><strong>Abstract: </strong>We show the security risk associated with using machine learning classifiers in United States election tabulators. The central classification task in election tabulation is deciding whether a mark does or does not appear on a bubble associated to an alternative in a contest on the ballot. Barretto et al. (E-Vote-ID 2021) reported that convolutional neural networks are a viable option in this field, as they outperform simple feature-based classifiers. Our contributions to election security can be divided into four parts. To demonstrate and analyze the hypothetical vulnerability of machine learning models on election tabulators, we first introduce four new ballot datasets. Second, we train and test a variety of different models on our new datasets. These models include support vector machines, convolutional neural networks (a basic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third, using our new datasets and trained models, we demonstrate that traditional white box attacks are ineffective in the voting domain due to gradient masking. Our analyses further reveal that gradient masking is a product of numerical instability. We use a modified difference of logits ratio loss to overcome this issue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct attacks with the adversarial examples generated using our new methods. In traditional adversarial machine learning, a high (50% or greater) attack success rate is ideal. However, for certain elections, even a 5% attack success rate can flip the outcome of a race. We show such an impact is possible in the physical domain. We thoroughly discuss attack realism, and the challenges and practicality associated with printing and scanning ballot adversarial examples.</li>
</ul>

<h3>Title: Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet's Performance with Automatically Generated Document Images</h3>
<ul>
<li><strong>Authors: </strong>Krishna Sahukara, Zineddine Bettouche, Andreas Fischer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14583">https://arxiv.org/abs/2506.14583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14583">https://arxiv.org/pdf/2506.14583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14583]] Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet's Performance with Automatically Generated Document Images(https://arxiv.org/abs/2506.14583)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Document pages captured by smartphones or scanners often contain tables, yet manual extraction is slow and error-prone. We introduce an automated LaTeX-based pipeline that synthesizes realistic two-column pages with visually diverse table layouts and aligned ground-truth masks. The generated corpus augments the real-world Marmot benchmark and enables a systematic resolution study of TableNet. Training TableNet on our synthetic data achieves a pixel-wise XOR error of 4.04% on our synthetic test set with a 256x256 input resolution, and 4.33% with 1024x1024. The best performance on the Marmot benchmark is 9.18% (at 256x256), while cutting manual annotation effort through automation.</li>
</ul>

<h3>Title: SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification</h3>
<ul>
<li><strong>Authors: </strong>Shuo Yang, Bardh Prenkaj, Gjergji Kasneci</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14587">https://arxiv.org/abs/2506.14587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14587">https://arxiv.org/pdf/2506.14587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14587]] SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification(https://arxiv.org/abs/2506.14587)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Shortcut learning undermines model generalization to out-of-distribution data. While the literature attributes shortcuts to biases in superficial features, we show that imbalances in the semantic distribution of sample embeddings induce spurious semantic correlations, compromising model robustness. To address this issue, we propose SCISSOR (Semantic Cluster Intervention for Suppressing ShORtcut), a Siamese network-based debiasing approach that remaps the semantic space by discouraging latent clusters exploited as shortcuts. Unlike prior data-debiasing approaches, SCISSOR eliminates the need for data augmentation and rewriting. We evaluate SCISSOR on 6 models across 4 benchmarks: Chest-XRay and Not-MNIST in computer vision, and GYAFC and Yelp in NLP tasks. Compared to several baselines, SCISSOR reports +5.3 absolute points in F1 score on GYAFC, +7.3 on Yelp, +7.7 on Chest-XRay, and +1 on Not-MNIST. SCISSOR is also highly advantageous for lightweight models with ~9.5% improvement on F1 for ViT on computer vision datasets and ~11.9% for BERT on NLP. Our study redefines the landscape of model generalization by addressing overlooked semantic biases, establishing SCISSOR as a foundational framework for mitigating shortcut learning and fostering more robust, bias-resistant AI systems.</li>
</ul>

<h3>Title: PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Ming Xu, Xu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14596">https://arxiv.org/abs/2506.14596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14596">https://arxiv.org/pdf/2506.14596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14596]] PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human Pose Estimation(https://arxiv.org/abs/2506.14596)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Existing monocular 3D pose estimation methods primarily rely on joint positional features, while overlooking intrinsic directional and angular correlations within the skeleton. As a result, they often produce implausible poses under joint occlusions or rapid motion changes. To address these challenges, we propose the PoseGRAF framework. We first construct a dual graph convolutional structure that separately processes joint and bone graphs, effectively capturing their local dependencies. A Cross-Attention module is then introduced to model interdependencies between bone directions and joint features. Building upon this, a dynamic fusion module is designed to adaptively integrate both feature types by leveraging the relational dependencies between joints and bones. An improved Transformer encoder is further incorporated in a residual manner to generate the final output. Experimental results on the Human3.6M and MPI-INF-3DHP datasets show that our method exceeds state-of-the-art approaches. Additional evaluations on in-the-wild videos further validate its generalizability. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: Deep Learning Surrogates for Real-Time Gas Emission Inversion</h3>
<ul>
<li><strong>Authors: </strong>Thomas Newman, Christopher Nemeth, Matthew Jones, Philip Jonathan</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14597">https://arxiv.org/abs/2506.14597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14597">https://arxiv.org/pdf/2506.14597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14597]] Deep Learning Surrogates for Real-Time Gas Emission Inversion(https://arxiv.org/abs/2506.14597)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Real-time identification and quantification of greenhouse-gas emissions under transient atmospheric conditions is a critical challenge in environmental monitoring. We introduce a spatio-temporal inversion framework that embeds a deep-learning surrogate of computational fluid dynamics (CFD) within a sequential Monte Carlo algorithm to perform Bayesian inference of both emission rate and source location in dynamic flow fields. By substituting costly numerical solvers with a multilayer perceptron trained on high-fidelity CFD outputs, our surrogate captures spatial heterogeneity and temporal evolution of gas dispersion, while delivering near-real-time predictions. Validation on the Chilbolton methane release dataset demonstrates comparable accuracy to full CFD solvers and Gaussian plume models, yet achieves orders-of-magnitude faster runtimes. Further experiments under simulated obstructed-flow scenarios confirm robustness in complex environments. This work reconciles physical fidelity with computational feasibility, offering a scalable solution for industrial emissions monitoring and other time-sensitive spatio-temporal inversion tasks in environmental and scientific modeling.</li>
</ul>

<h3>Title: Align Your Flow: Scaling Continuous-Time Flow Map Distillation</h3>
<ul>
<li><strong>Authors: </strong>Amirmojtaba Sabour, Sanja Fidler, Karsten Kreis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14603">https://arxiv.org/abs/2506.14603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14603">https://arxiv.org/pdf/2506.14603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14603]] Align Your Flow: Scaling Continuous-Time Flow Map Distillation(https://arxiv.org/abs/2506.14603)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.</li>
</ul>

<h3>Title: Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching</h3>
<ul>
<li><strong>Authors: </strong>Giacomo Meanti, Thomas Ryckeboer, Michael Arbel, Julien Mairal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14605">https://arxiv.org/abs/2506.14605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14605">https://arxiv.org/pdf/2506.14605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14605]] Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching(https://arxiv.org/abs/2506.14605)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work addresses image restoration tasks through the lens of inverse problems using unpaired datasets. In contrast to traditional approaches -- which typically assume full knowledge of the forward model or access to paired degraded and ground-truth images -- the proposed method operates under minimal assumptions and relies only on small, unpaired datasets. This makes it particularly well-suited for real-world scenarios, where the forward model is often unknown or misspecified, and collecting paired data is costly or infeasible. The method leverages conditional flow matching to model the distribution of degraded observations, while simultaneously learning the forward model via a distribution-matching loss that arises naturally from the framework. Empirically, it outperforms both single-image blind and unsupervised approaches on deblurring and non-uniform point spread function (PSF) calibration tasks. It also matches state-of-the-art performance on blind super-resolution. We also showcase the effectiveness of our method with a proof of concept for lens calibration: a real-world application traditionally requiring time-consuming experiments and specialized equipment. In contrast, our approach achieves this with minimal data acquisition effort.</li>
</ul>

<h3>Title: Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Heakl, Sarim Hashmi, Chaimaa Abi, Celine Lee, Abdulrahman Mahmoud</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AR, cs.LG, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14606">https://arxiv.org/abs/2506.14606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14606">https://arxiv.org/pdf/2506.14606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14606]] Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees(https://arxiv.org/abs/2506.14606)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.</li>
</ul>

<h3>Title: Expressive Score-Based Priors for Distribution Matching with Geometry-Preserving Regularization</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Gong, Jim Lim, David I. Inouye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14607">https://arxiv.org/abs/2506.14607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14607">https://arxiv.org/pdf/2506.14607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14607]] Expressive Score-Based Priors for Distribution Matching with Geometry-Preserving Regularization(https://arxiv.org/abs/2506.14607)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, diffusion</a></li>
<li><strong>Abstract: </strong>Distribution matching (DM) is a versatile domain-invariant representation learning technique that has been applied to tasks such as fair classification, domain adaptation, and domain translation. Non-parametric DM methods struggle with scalability and adversarial DM approaches suffer from instability and mode collapse. While likelihood-based methods are a promising alternative, they often impose unnecessary biases through fixed priors or require explicit density models (e.g., flows) that can be challenging to train. We address this limitation by introducing a novel approach to training likelihood-based DM using expressive score-based prior distributions. Our key insight is that gradient-based DM training only requires the prior's score function -- not its density -- allowing us to train the prior via denoising score matching. This approach eliminates biases from fixed priors (e.g., in VAEs), enabling more effective use of geometry-preserving regularization, while avoiding the challenge of learning an explicit prior density model (e.g., a flow-based prior). Our method also demonstrates better stability and computational efficiency compared to other diffusion-based priors (e.g., LSGM). Furthermore, experiments demonstrate superior performance across multiple tasks, establishing our score-based method as a stable and effective approach to distribution matching. Source code available at this https URL.</li>
</ul>

<h3>Title: Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenchen Yuan, Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14625">https://arxiv.org/abs/2506.14625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14625">https://arxiv.org/pdf/2506.14625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14625]] Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models(https://arxiv.org/abs/2506.14625)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.</li>
</ul>

<h3>Title: VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Md. Adnanul Islam, Md. Faiyaz Abdullah Sayeedi, Md. Asaduzzaman Shuvo, Muhammad Ziaur Rahman, Shahanur Rahman Bappy, Raiyan Rahman, Swakkhar Shatabda</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14629">https://arxiv.org/abs/2506.14629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14629">https://arxiv.org/pdf/2506.14629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14629]] VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning(https://arxiv.org/abs/2506.14629)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme "Prevention is Better than Cure", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: this https URL</li>
</ul>

<h3>Title: AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation</h3>
<ul>
<li><strong>Authors: </strong>Leah von der Heyde, Anna-Carolina Haensch, Bernd Weiß, Jessika Daikeler</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14634">https://arxiv.org/abs/2506.14634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14634">https://arxiv.org/pdf/2506.14634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14634]] AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation(https://arxiv.org/abs/2506.14634)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.</li>
</ul>

<h3>Title: Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot</h3>
<ul>
<li><strong>Authors: </strong>Xiang Cheng, Chengyan Pan, Minjun Zhao, Deyang Li, Fangchao Liu, Xinyu Zhang, Xiao Zhang, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14641">https://arxiv.org/abs/2506.14641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14641">https://arxiv.org/pdf/2506.14641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14641]] Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot(https://arxiv.org/abs/2506.14641)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \texttt{Qwen2.5-Max} and \texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars.</li>
</ul>

<h3>Title: Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments</h3>
<ul>
<li><strong>Authors: </strong>. Pazzaglia, V. Vendetti, L. D. Comencini, F. Deriu, V. Modugno</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14645">https://arxiv.org/abs/2506.14645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14645">https://arxiv.org/pdf/2506.14645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14645]] Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments(https://arxiv.org/abs/2506.14645)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The increasing sophistication of large language models (LLMs) has sparked growing concerns regarding their potential role in exacerbating ideological polarization through the automated generation of persuasive and biased content. This study explores the extent to which fine-tuned LLMs can replicate and amplify polarizing discourse within online environments. Using a curated dataset of politically charged discussions extracted from Reddit, we fine-tune an open-source LLM to produce context-aware and ideologically aligned responses. The model's outputs are evaluated through linguistic analysis, sentiment scoring, and human annotation, with particular attention to credibility and rhetorical alignment with the original discourse. The results indicate that, when trained on partisan data, LLMs are capable of producing highly plausible and provocative comments, often indistinguishable from those written by humans. These findings raise significant ethical questions about the use of AI in political discourse, disinformation, and manipulation campaigns. The paper concludes with a discussion of the broader implications for AI governance, platform regulation, and the development of detection tools to mitigate adversarial fine-tuning risks.</li>
</ul>

<h3>Title: GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors</h3>
<ul>
<li><strong>Authors: </strong>Hengyuan Zhang, Xinrong Chen, Yingmin Qiu, Xiao Liang, Ziyue Li, Guanyu Wang, Weiping Li, Tong Mo, Wenyue Li, Hayden Kwok-Hay So, Ngai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14646">https://arxiv.org/abs/2506.14646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14646">https://arxiv.org/pdf/2506.14646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14646]] GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors(https://arxiv.org/abs/2506.14646)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), offer an efficient way to adapt large language models with reduced computational costs. However, their performance is limited by the small number of trainable parameters. Recent work combines LoRA with the Mixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two limitations remain in hindering the full exploitation of its potential: 1) the influence of downstream tasks when assigning expert numbers, and 2) the uniform rank assignment across all LoRA experts, which restricts representational diversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained layer-wise expert numbers and ranks allocation strategy with GuidedSelection Vectors (GSVs). GSVs are learned via a prior bilevel optimization process to capture both model- and task-specific needs, and are then used to allocate optimal expert numbers and ranks. Experiments on three backbone models across diverse benchmarks show that GuiLoMo consistently achieves superior or comparable performance to all baselines. Further analysis offers key insights into how expert numbers and ranks vary across layers and tasks, highlighting the benefits of adaptive expert configuration. Our code is available at this https URL.</li>
</ul>

<h3>Title: Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ling Li, Yao Zhou, Yuxuan Liang, Fugee Tsung, Jiaheng Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14674">https://arxiv.org/abs/2506.14674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14674">https://arxiv.org/pdf/2506.14674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14674]] Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models(https://arxiv.org/abs/2506.14674)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Previous methods for image geo-localization have typically treated the task as either classification or retrieval, often relying on black-box decisions that lack interpretability. The rise of large vision-language models (LVLMs) has enabled a rethinking of geo-localization as a reasoning-driven task grounded in visual cues. However, two major challenges persist. On the data side, existing reasoning-focused datasets are primarily based on street-view imagery, offering limited scene diversity and constrained viewpoints. On the modeling side, current approaches predominantly rely on supervised fine-tuning, which yields only marginal improvements in reasoning capabilities. To address these challenges, we propose a novel pipeline that constructs a reasoning-oriented geo-localization dataset, MP16-Reason, using diverse social media images. We introduce GLOBE, Group-relative policy optimization for Locatability assessment and Optimized visual-clue reasoning, yielding Bi-objective geo-Enhancement for the VLM in recognition and reasoning. GLOBE incorporates task-specific rewards that jointly enhance locatability assessment, visual clue reasoning, and geolocation accuracy. Both qualitative and quantitative results demonstrate that GLOBE outperforms state-of-the-art open-source LVLMs on geo-localization tasks, particularly in diverse visual scenes, while also generating more insightful and interpretable reasoning trajectories.</li>
</ul>

<h3>Title: Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality</h3>
<ul>
<li><strong>Authors: </strong>Yuto Harada, Yusuke Yamauchi, Yusuke Oda, Yohei Oseki, Yusuke Miyao, Yu Takagi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14681">https://arxiv.org/abs/2506.14681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14681">https://arxiv.org/pdf/2506.14681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14681]] Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality(https://arxiv.org/abs/2506.14681)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) is a critical step in aligning large language models (LLMs) with human instructions and values, yet many aspects of SFT remain poorly understood. We trained a wide range of base models on a variety of datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in 1,000+ SFT models under controlled conditions. We then identified the dataset properties that matter most and examined the layer-wise modifications introduced by SFT. Our findings reveal that some training-task synergies persist across all models while others vary substantially, emphasizing the importance of model-specific strategies. Moreover, we demonstrate that perplexity consistently predicts SFT effectiveness--often surpassing superficial similarity between trained data and benchmark--and that mid-layer weight changes correlate most strongly with performance gains. We will release these 1,000+ SFT models and benchmark results to accelerate further research.</li>
</ul>

<h3>Title: AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ads Dawson, Rob Mulla, Nick Landers, Shane Caldwell</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14682">https://arxiv.org/abs/2506.14682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14682">https://arxiv.org/pdf/2506.14682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14682]] AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models(https://arxiv.org/abs/2506.14682)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>We introduce AIRTBench, an AI red teaming benchmark for evaluating language models' ability to autonomously discover and exploit Artificial Intelligence and Machine Learning (AI/ML) security vulnerabilities. The benchmark consists of 70 realistic black-box capture-the-flag (CTF) challenges from the Crucible challenge environment on the Dreadnode platform, requiring models to write python code to interact with and compromise AI systems. Claude-3.7-Sonnet emerged as the clear leader, solving 43 challenges (61% of the total suite, 46.9% overall success rate), with Gemini-2.5-Pro following at 39 challenges (56%, 34.3% overall), GPT-4.5-Preview at 34 challenges (49%, 36.9% overall), and DeepSeek R1 at 29 challenges (41%, 26.9% overall). Our evaluations show frontier models excel at prompt injection attacks (averaging 49% success rates) but struggle with system exploitation and model inversion challenges (below 26%, even for the best performers). Frontier models are far outpacing open-source alternatives, with the best truly open-source model (Llama-4-17B) solving 7 challenges (10%, 1.0% overall), though demonstrating specialized capabilities on certain hard challenges. Compared to human security researchers, large language models (LLMs) solve challenges with remarkable efficiency completing in minutes what typically takes humans hours or days-with efficiency advantages of over 5,000x on hard challenges. Our contribution fills a critical gap in the evaluation landscape, providing the first comprehensive benchmark specifically designed to measure and track progress in autonomous AI red teaming capabilities.</li>
</ul>

<h3>Title: FocalClick-XL: Towards Unified and High-quality Interactive Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xi Chen, Hengshuang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14686">https://arxiv.org/abs/2506.14686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14686">https://arxiv.org/pdf/2506.14686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14686]] FocalClick-XL: Towards Unified and High-quality Interactive Segmentation(https://arxiv.org/abs/2506.14686)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Interactive segmentation enables users to extract binary masks of target objects through simple interactions such as clicks, scribbles, and boxes. However, existing methods often support only limited interaction forms and struggle to capture fine details. In this paper, we revisit the classical coarse-to-fine design of FocalClick and introduce significant extensions. Inspired by its multi-stage strategy, we propose a novel pipeline, FocalClick-XL, to address these challenges simultaneously. Following the emerging trend of large-scale pretraining, we decompose interactive segmentation into meta-tasks that capture different levels of information -- context, object, and detail -- assigning a dedicated subnet to each this http URL decomposition allows each subnet to undergo scaled pretraining with independent data and supervision, maximizing its effectiveness. To enhance flexibility, we share context- and detail-level information across different interaction forms as common knowledge while introducing a prompting layer at the object level to encode specific interaction types. As a result, FocalClick-XL achieves state-of-the-art performance on click-based benchmarks and demonstrates remarkable adaptability to diverse interaction formats, including boxes, scribbles, and coarse masks. Beyond binary mask generation, it is also capable of predicting alpha mattes with fine-grained details, making it a versatile and powerful tool for interactive segmentation.</li>
</ul>

<h3>Title: YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object Detection Framework</h3>
<ul>
<li><strong>Authors: </strong>Dahang Wan, Rongsheng Lu, Yang Fang, Xianli Lang, Shuangbao Shu, Jingjing Chen, Siyuan Shen, Ting Xu, Zecong Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14696">https://arxiv.org/abs/2506.14696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14696">https://arxiv.org/pdf/2506.14696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14696]] YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object Detection Framework(https://arxiv.org/abs/2506.14696)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multispectral object detection, which integrates information from multiple bands, can enhance detection accuracy and environmental adaptability, holding great application potential across various fields. Although existing methods have made progress in cross-modal interaction, low-light conditions, and model lightweight, there are still challenges like the lack of a unified single-stage framework, difficulty in balancing performance and fusion strategy, and unreasonable modality weight allocation. To address these, based on the YOLOv11 framework, we present YOLOv11-RGBT, a new comprehensive multimodal object detection framework. We designed six multispectral fusion modes and successfully applied them to models from YOLOv3 to YOLOv12 and RT-DETR. After reevaluating the importance of the two modalities, we proposed a P3 mid-fusion strategy and multispectral controllable fine-tuning (MCF) strategy for multispectral models. These improvements optimize feature fusion, reduce redundancy and mismatches, and boost overall model performance. Experiments show our framework excels on three major open-source multispectral object detection datasets, like LLVIP and FLIR. Particularly, the multispectral controllable fine-tuning strategy significantly enhanced model adaptability and robustness. On the FLIR dataset, it consistently improved YOLOv11 models' mAP by 3.41%-5.65%, reaching a maximum of 47.61%, verifying the framework and strategies' effectiveness. The code is available at: this https URL.</li>
</ul>

<h3>Title: Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data</h3>
<ul>
<li><strong>Authors: </strong>Anton Changalidis, Aki Härmä</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14704">https://arxiv.org/abs/2506.14704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14704">https://arxiv.org/pdf/2506.14704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14704]] Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data(https://arxiv.org/abs/2506.14704)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>This paper studies how the model architecture and data configurations influence the empirical memorization capacity of generative transformers. The models are trained using synthetic text datasets derived from the Systematized Nomenclature of Medicine (SNOMED) knowledge graph: triplets, representing static connections, and sequences, simulating complex relation patterns. The results show that embedding size is the primary determinant of learning speed and capacity, while additional layers provide limited benefits and may hinder performance on simpler datasets. Activation functions play a crucial role, and Softmax demonstrates greater stability and capacity. Furthermore, increasing the complexity of the data set seems to improve the final memorization. These insights improve our understanding of transformer memory mechanisms and provide a framework for optimizing model design with structured real-world data.</li>
</ul>

<h3>Title: Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Ni Ou, Zhuo Chen, Xinru Zhang, Junzheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14706">https://arxiv.org/abs/2506.14706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14706">https://arxiv.org/pdf/2506.14706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14706]] Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion(https://arxiv.org/abs/2506.14706)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Cameras and LiDAR are essential sensors for autonomous vehicles. The fusion of camera and LiDAR data addresses the limitations of individual sensors but relies on precise extrinsic calibration. Recently, numerous end-to-end calibration methods have been proposed; however, most predict extrinsic parameters in a single step and lack iterative optimization capabilities. To address the increasing demand for higher accuracy, we propose a versatile iterative framework based on surrogate diffusion. This framework can enhance the performance of any calibration method without requiring architectural modifications. Specifically, the initial extrinsic parameters undergo iterative refinement through a denoising process, in which the original calibration method serves as a surrogate denoiser to estimate the final extrinsics at each step. For comparative analysis, we selected four state-of-the-art calibration methods as surrogate denoisers and compared the results of our diffusion process with those of two other iterative approaches. Extensive experiments demonstrate that when integrated with our diffusion model, all calibration methods achieve higher accuracy, improved robustness, and greater stability compared to other iterative techniques and their single-step counterparts.</li>
</ul>

<h3>Title: DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning</h3>
<ul>
<li><strong>Authors: </strong>Kunal Swami, Debtanu Gupta, Amrit Kumar Muduli, Chirag Jaiswal, Pankaj Kumar Bajpai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14709">https://arxiv.org/abs/2506.14709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14709">https://arxiv.org/pdf/2506.14709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14709]] DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning(https://arxiv.org/abs/2506.14709)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Depth estimation is crucial for intelligent systems, enabling applications from autonomous navigation to augmented reality. While traditional stereo and active depth sensors have limitations in cost, power, and robustness, dual-pixel (DP) technology, ubiquitous in modern cameras, offers a compelling alternative. This paper introduces DiFuse-Net, a novel modality decoupled network design for disentangled RGB and DP based depth estimation. DiFuse-Net features a window bi-directional parallax attention mechanism (WBiPAM) specifically designed to capture the subtle DP disparity cues unique to smartphone cameras with small aperture. A separate encoder extracts contextual information from the RGB image, and these features are fused to enhance depth prediction. We also propose a Cross-modal Transfer Learning (CmTL) mechanism to utilize large-scale RGB-D datasets in the literature to cope with the limitations of obtaining large-scale RGB-DP-D dataset. Our evaluation and comparison of the proposed method demonstrates its superiority over the DP and stereo-based baseline methods. Additionally, we contribute a new, high-quality, real-world RGB-DP-D training dataset, named Dual-Camera Dual-Pixel (DCDP) dataset, created using our novel symmetric stereo camera hardware setup, stereo calibration and rectification protocol, and AI stereo disparity estimation method.</li>
</ul>

<h3>Title: Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ring Team, Bin Hu, Cai Chen, Deng Zhao, Ding Liu, Dingnan Jin, Feng Zhu, Hao Dai, Hongzhi Luan, Jia Guo, Jiaming Liu, Jiewei Wu, Jun Mei, Jun Zhou, Junbo Zhao, Junwu Xiong, Kaihong Zhang, Kuan Xu, Lei Liang, Liang Jiang, Liangcheng Fu, Longfei Zheng, Qiang Gao, Qing Cui, Quan Wan, Shaomian Zheng, Shuaicheng Li, Tongkai Yang, Wang Ren, Xiaodong Yan, Xiaopei Wan, Xiaoyun Feng, Xin Zhao, Xinxing Yang, Xinyu Kong, Xuemin Yang, Yang Li, Yingting Wu, Yongkang Liu, Zhankai Xu, Zhenduo Zhang, Zhenglei Zhou, Zhenyu Huang, Zhiqiang Zhang, Zihao Wang, Zujie Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14731">https://arxiv.org/abs/2506.14731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14731">https://arxiv.org/pdf/2506.14731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14731]] Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs(https://arxiv.org/abs/2506.14731)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.</li>
</ul>

<h3>Title: SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Ziqiao Peng, Wentao Hu, Junyuan Ma, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Hui Tian, Jun He, Hongyan Liu, Zhaoxin Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14742">https://arxiv.org/abs/2506.14742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14742">https://arxiv.org/pdf/2506.14742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14742]] SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting(https://arxiv.org/abs/2506.14742)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the ''devil'' in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: this https URL.</li>
</ul>

<h3>Title: Cost-Aware Routing for Efficient Text-To-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Qinchan (Wing)Li, Kenneth Chen, Changyue (Tina)Su, Wittawat Jitkrittum, Qi Sun, Patsorn Sangkloy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14753">https://arxiv.org/abs/2506.14753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14753">https://arxiv.org/pdf/2506.14753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14753]] Cost-Aware Routing for Efficient Text-To-Image Generation(https://arxiv.org/abs/2506.14753)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are well known for their ability to generate a high-fidelity image for an input prompt through an iterative denoising process. Unfortunately, the high fidelity also comes at a high computational cost due the inherently sequential generative process. In this work, we seek to optimally balance quality and computational cost, and propose a framework to allow the amount of computation to vary for each prompt, depending on its complexity. Each prompt is automatically routed to the most appropriate text-to-image generation function, which may correspond to a distinct number of denoising steps of a diffusion model, or a disparate, independent text-to-image model. Unlike uniform cost reduction techniques (e.g., distillation, model quantization), our approach achieves the optimal trade-off by learning to reserve expensive choices (e.g., 100+ denoising steps) only for a few complex prompts, and employ more economical choices (e.g., small distilled model) for less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB that by learning to route to nine already-trained text-to-image models, our approach is able to deliver an average quality that is higher than that achievable by any of these models alone.</li>
</ul>

<h3>Title: Scaling-Up the Pretraining of the Earth Observation Foundation Model PhilEO to the MajorTOM Dataset</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Dionelis, Jente Bosmans, Riccardo Musto, Giancarlo Paoletti, Simone Sarti, Giacomo Cascarano, Casper Fibaek, Luke Camilleri, Bertrand Le Saux, Nicolas Longépé</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14765">https://arxiv.org/abs/2506.14765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14765">https://arxiv.org/pdf/2506.14765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14765]] Scaling-Up the Pretraining of the Earth Observation Foundation Model PhilEO to the MajorTOM Dataset(https://arxiv.org/abs/2506.14765)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Today, Earth Observation (EO) satellites generate massive volumes of data, with the Copernicus Sentinel-2 constellation alone producing approximately 1.6TB per day. To fully exploit this information, it is essential to pretrain EO Foundation Models (FMs) on large unlabeled datasets, enabling efficient fine-tuning for several different downstream tasks with minimal labeled data. In this work, we present the scaling-up of our recently proposed EO Foundation Model, PhilEO Geo-Aware U-Net, on the unlabeled 23TB dataset MajorTOM, which covers the vast majority of the Earth's surface, as well as on the specialized subset FastTOM 2TB that does not include oceans and ice. We develop and study various PhilEO model variants with different numbers of parameters and architectures. Finally, we fine-tune the models on the PhilEO Bench for road density estimation, building density pixel-wise regression, and land cover semantic segmentation, and we evaluate the performance. Our results demonstrate that for all n-shots for road density regression, the PhilEO 44M MajorTOM 23TB model outperforms PhilEO Globe 0.5TB 44M. We also show that for most n-shots for road density estimation and building density regression, PhilEO 200M FastTOM outperforms all the other models. The effectiveness of both dataset and model scaling is validated using the PhilEO Bench. We also study the impact of architecture scaling, transitioning from U-Net Convolutional Neural Networks (CNN) to Vision Transformers (ViT).</li>
</ul>

<h3>Title: ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM</h3>
<ul>
<li><strong>Authors: </strong>Yujun Wang, Jinhe Bi, Yunpu Ma, Soeren Pirk</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14766">https://arxiv.org/abs/2506.14766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14766">https://arxiv.org/pdf/2506.14766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14766]] ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM(https://arxiv.org/abs/2506.14766)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Model (MLLM) often suffer from hallucinations. They over-rely on partial cues and generate incorrect responses. Recently, methods like Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD) have been proposed to mitigate hallucinations by contrasting predictions from perturbed or negatively prefixed inputs against original outputs. In this work, we uncover that methods like VCD and ICD fundamentally influence internal attention dynamics of the model. This observation suggests that their effectiveness may not stem merely from surface-level modifications to logits but from deeper shifts in attention distribution. Inspired by this insight, we propose an attention-steerable contrastive decoding framework that directly intervenes in attention mechanisms of the model to offer a more principled approach to mitigating hallucinations. Our experiments across multiple MLLM architectures and diverse decoding methods demonstrate that our approach significantly reduces hallucinations and improves the performance on benchmarks such as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing performance on standard VQA benchmarks.</li>
</ul>

<h3>Title: A Variational Framework for Improving Naturalness in Generative Spoken Language Models</h3>
<ul>
<li><strong>Authors: </strong>Li-Wei Chen, Takuya Higuchi, Zakaria Aldeneh, Ahmed Hussen Abdelaziz, Alexander Rudnicky</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14767">https://arxiv.org/abs/2506.14767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14767">https://arxiv.org/pdf/2506.14767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14767]] A Variational Framework for Improving Naturalness in Generative Spoken Language Models(https://arxiv.org/abs/2506.14767)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at this https URL.</li>
</ul>

<h3>Title: CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Jiahua Ma, Yiran Qin, Yixiong Li, Xuanqi Liao, Yulan Guo, Ruimao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14769">https://arxiv.org/abs/2506.14769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14769">https://arxiv.org/pdf/2506.14769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14769]] CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion(https://arxiv.org/abs/2506.14769)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution. To address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning. To further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution. Extensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded input observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
