<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Gradient-Free Textual Inversion. (arXiv:2304.05818v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05818">http://arxiv.org/abs/2304.05818</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05818] Gradient-Free Textual Inversion](http://arxiv.org/abs/2304.05818) #secure</code></li>
<li>Summary: <p>Recent works on personalized text-to-image generation usually learn to bind a
special token with specific subjects or styles of a few given images by tuning
its embedding through gradient descent. It is natural to question whether we
can optimize the textual inversions by only accessing the process of model
inference. As only requiring the forward computation to determine the textual
inversion retains the benefits of less GPU memory, simple deployment, and
secure access for scalable models. In this paper, we introduce a
\emph{gradient-free} framework to optimize the continuous textual inversion in
an iterative evolutionary strategy. Specifically, we first initialize an
appropriate token embedding for textual inversion with the consideration of
visual and text vocabulary information. Then, we decompose the optimization of
evolutionary strategy into dimension reduction of searching space and
non-convex gradient-free optimization in subspace, which significantly
accelerates the optimization process with negligible performance loss.
Experiments in several applications demonstrate that the performance of
text-to-image model equipped with our proposed gradient-free method is
comparable to that of gradient-based counterparts with variant GPU/CPU
platforms, flexible employment, as well as computational efficiency.
</p></li>
</ul>

<h3>Title: Automated and Secure Onboarding for System of Systems. (arXiv:2304.05778v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05778">http://arxiv.org/abs/2304.05778</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05778] Automated and Secure Onboarding for System of Systems](http://arxiv.org/abs/2304.05778) #secure</code></li>
<li>Summary: <p>The Internet of Things (IoT) is rapidly changing the number of connected
devices and the way they interact with each other. This increases the need for
an automated and secure onboarding procedure for IoT devices, systems and
services. Device manufacturers are entering the market with internet connected
devices, ranging from small sensors to production devices, which are subject of
security threats specific to IoT. The onboarding procedure is required to
introduce a new device in a System of Systems (SoS) without compromising the
already onboarded devices and the underlying infrastructure. Onboarding is the
process of providing access to the network and registering the components for
the first time in an IoT/SoS framework, thus creating a chain of trust from the
hardware device to its hosted software systems and their provided services. The
large number and diversity of device hardware, software systems and running
services raises the challenge to establish a generic onboarding procedure. In
this paper, we present an automated and secure onboarding procedure for SoS. We
have implemented the onboarding procedure in the Eclipse Arrowhead framework.
However, it can be easily adapted for other IoT/SoS frameworks that are based
on Service-oriented Architecture (SoA) principles. The automated onboarding
procedure ensures a secure and trusted communication between the new IoT
devices and the Eclipse Arrowhead framework. We show its application in a smart
charging use case and perform a security assessment.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: End-to-End O-RAN Security Architecture, Threat Surface, Coverage, and the Case of the Open Fronthaul. (arXiv:2304.05513v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05513">http://arxiv.org/abs/2304.05513</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05513] End-to-End O-RAN Security Architecture, Threat Surface, Coverage, and the Case of the Open Fronthaul](http://arxiv.org/abs/2304.05513) #security</code></li>
<li>Summary: <p>O-RAN establishes an advanced radio access network (RAN) architecture that
supports inter-operable, multi-vendor, and artificial intelligence (AI)
controlled wireless access networks. The unique components, interfaces, and
technologies of O-RAN differentiate it from the 3GPP RAN. Because O-RAN
supports 3GPP protocols, currently 4G and 5G, while offering additional network
interfaces and controllers, it has a larger attack surface. The O-RAN security
requirements, vulnerabilities, threats, and countermeasures must be carefully
assessed for it to become a platform for 5G Advanced and future 6G wireless.
This article presents the ongoing standardization activities of the O-RAN
Alliance for modeling the potential threats to the network and to the open
fronthaul interface, in particular. We identify end-to-end security threats and
discuss those on the open fronthaul in more detail. We then provide
recommendations for countermeasures to tackle the identified security risks and
encourage industry to establish standards and best practices for safe and
secure implementations of the open fronthaul interface.
</p></li>
</ul>

<h3>Title: Generative Adversarial Networks-Driven Cyber Threat Intelligence Detection Framework for Securing Internet of Things. (arXiv:2304.05644v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05644">http://arxiv.org/abs/2304.05644</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05644] Generative Adversarial Networks-Driven Cyber Threat Intelligence Detection Framework for Securing Internet of Things](http://arxiv.org/abs/2304.05644) #security</code></li>
<li>Summary: <p>While the benefits of 6G-enabled Internet of Things (IoT) are numerous,
providing high-speed, low-latency communication that brings new opportunities
for innovation and forms the foundation for continued growth in the IoT
industry, it is also important to consider the security challenges and risks
associated with the technology. In this paper, we propose a two-stage intrusion
detection framework for securing IoTs, which is based on two detectors. In the
first stage, we propose an adversarial training approach using generative
adversarial networks (GAN) to help the first detector train on robust features
by supplying it with adversarial examples as validation sets. Consequently, the
classifier would perform very well against adversarial attacks. Then, we
propose a deep learning (DL) model for the second detector to identify
intrusions. We evaluated the proposed approach's efficiency in terms of
detection accuracy and robustness against adversarial attacks. Experiment
results with a new cyber security dataset demonstrate the effectiveness of the
proposed methodology in detecting both intrusions and persistent adversarial
examples with a weighted avg of 96%, 95%, 95%, and 95% for precision, recall,
f1-score, and accuracy, respectively.
</p></li>
</ul>

<h3>Title: Automated Information Flow Analysis for Integrated Computing-in-Memory Modules. (arXiv:2304.05682v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05682">http://arxiv.org/abs/2304.05682</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05682] Automated Information Flow Analysis for Integrated Computing-in-Memory Modules](http://arxiv.org/abs/2304.05682) #security</code></li>
<li>Summary: <p>Novel non-volatile memory (NVM) technologies offer high-speed and
high-density data storage. In addition, they overcome the von Neumann
bottleneck by enabling computing-in-memory (CIM). Various computer
architectures have been proposed to integrate CIM blocks in their design,
forming a mixed-signal system to combine the computational benefits of CIM with
the robustness of conventional CMOS. Novel electronic design automation (EDA)
tools are necessary to design and manufacture these so-called neuromorphic
systems. Furthermore, EDA tools must consider the impact of security
vulnerabilities, as hardware security attacks have increased in recent years.
Existing information flow analysis (IFA) frameworks offer an automated
tool-suite to uphold the confidentiality property for sensitive data during the
design of hardware. However, currently available mixed-signal EDA tools are not
capable of analyzing the information flow of neuromorphic systems. To
illustrate the shortcomings, we develop information flow protocols for NVMs
that can be easily integrated in the already existing tool-suites. We show the
limitation of the state-of-the-art by analyzing the flow from sensitive signals
through multiple memristive crossbar structures to potential untrusted
components and outputs. Finally, we provide a thorough discussion of the merits
and flaws of the mixed-signal IFA frameworks on neuromorphic systems.
</p></li>
</ul>

<h3>Title: A Security Evaluation Framework for Software-Defined Network Architectures in Data Center Environments. (arXiv:2304.05776v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05776">http://arxiv.org/abs/2304.05776</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05776] A Security Evaluation Framework for Software-Defined Network Architectures in Data Center Environments](http://arxiv.org/abs/2304.05776) #security</code></li>
<li>Summary: <p>The importance of cloud computing has grown over the last years, which
resulted in a significant increase of Data Center (DC) network requirements.
Virtualisation is one of the key drivers of that transformation and enables a
massive deployment of computing resources, which exhausts server capacity
limits. Furthermore, the increased network endpoints need to be handled
dynamically and centrally to facilitate cloud computing functionalities.
Traditional DCs barely satisfy those demands because of their inherent
limitations based on the network topology. Software-Defined Networks (SDN)
promise to meet the increasing network requirements for cloud applications by
decoupling control functionalities from data forwarding. Although SDN solutions
add more flexibility to DC networks, they also pose new vulnerabilities with a
high impact due to the centralised architecture. In this paper we propose an
evaluation framework for assessing the security level of SDN architectures in
four different stages. Furthermore, we show in an experimental study, how the
framework can be used for mapping SDN threats with associated vulnerabilities
and necessary mitigations in conjunction with risk and impact classification.
The proposed framework helps administrators to evaluate the network security
level, to apply countermeasures for identified SDN threats, and to meet the
networks security requirements.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Few Shot Semantic Segmentation: a review of methodologies and open challenges. (arXiv:2304.05832v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05832">http://arxiv.org/abs/2304.05832</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05832] Few Shot Semantic Segmentation: a review of methodologies and open challenges](http://arxiv.org/abs/2304.05832) #privacy</code></li>
<li>Summary: <p>Semantic segmentation assigns category labels to each pixel in an image,
enabling breakthroughs in fields such as autonomous driving and robotics. Deep
Neural Networks have achieved high accuracies in semantic segmentation but
require large training datasets. Some domains have difficulties building such
datasets due to rarity, privacy concerns, and the need for skilled annotators.
Few-Shot Learning (FSL) has emerged as a new research stream that allows models
to learn new tasks from a few samples. This contribution provides an overview
of FSL in semantic segmentation (FSS), proposes a new taxonomy, and describes
current limitations and outlooks.
</p></li>
</ul>

<h3>Title: Echo of Neighbors: Privacy Amplification for Personalized Private Federated Learning with Shuffle Model. (arXiv:2304.05516v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05516">http://arxiv.org/abs/2304.05516</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05516] Echo of Neighbors: Privacy Amplification for Personalized Private Federated Learning with Shuffle Model](http://arxiv.org/abs/2304.05516) #privacy</code></li>
<li>Summary: <p>Federated Learning, as a popular paradigm for collaborative training, is
vulnerable against privacy attacks. Different privacy levels regarding users'
attitudes need to be satisfied locally, while a strict privacy guarantee for
the global model is also required centrally. Personalized Local Differential
Privacy (PLDP) is suitable for preserving users' varying local privacy, yet
only provides a central privacy guarantee equivalent to the worst-case local
privacy level. Thus, achieving strong central privacy as well as personalized
local privacy with a utility-promising model is a challenging problem. In this
work, a general framework (APES) is built up to strengthen model privacy under
personalized local privacy by leveraging the privacy amplification effect of
the shuffle model. To tighten the privacy bound, we quantify the heterogeneous
contributions to the central privacy user by user. The contributions are
characterized by the ability of generating "echos" from the perturbation of
each user, which is carefully measured by proposed methods Neighbor Divergence
and Clip-Laplace Mechanism. Furthermore, we propose a refined framework
(S-APES) with the post-sparsification technique to reduce privacy loss in
high-dimension scenarios. To the best of our knowledge, the impact of shuffling
on personalized local privacy is considered for the first time. We provide a
strong privacy amplification effect, and the bound is tighter than the baseline
result based on existing methods for uniform local privacy. Experiments
demonstrate that our frameworks ensure comparable or higher accuracy for the
global model.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Wild Face Anti-Spoofing Challenge 2023: Benchmark and Results. (arXiv:2304.05753v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05753">http://arxiv.org/abs/2304.05753</a></li>
<li>Code URL: <a href="https://github.com/deepinsight/insightface/tree/master/challenges/cvpr23-fas-wild">https://github.com/deepinsight/insightface/tree/master/challenges/cvpr23-fas-wild</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05753] Wild Face Anti-Spoofing Challenge 2023: Benchmark and Results](http://arxiv.org/abs/2304.05753) #attack</code></li>
<li>Summary: <p>Face anti-spoofing (FAS) is an essential mechanism for safeguarding the
integrity of automated face recognition systems. Despite substantial
advancements, the generalization of existing approaches to real-world
applications remains challenging. This limitation can be attributed to the
scarcity and lack of diversity in publicly available FAS datasets, which often
leads to overfitting during training or saturation during testing. In terms of
quantity, the number of spoof subjects is a critical determinant. Most datasets
comprise fewer than 2,000 subjects. With regard to diversity, the majority of
datasets consist of spoof samples collected in controlled environments using
repetitive, mechanical processes. This data collection methodology results in
homogenized samples and a dearth of scenario diversity. To address these
shortcomings, we introduce the Wild Face Anti-Spoofing (WFAS) dataset, a
large-scale, diverse FAS dataset collected in unconstrained settings. Our
dataset encompasses 853,729 images of 321,751 spoof subjects and 529,571 images
of 148,169 live subjects, representing a substantial increase in quantity.
Moreover, our dataset incorporates spoof data obtained from the internet,
spanning a wide array of scenarios and various commercial sensors, including 17
presentation attacks (PAs) that encompass both 2D and 3D forms. This novel data
collection strategy markedly enhances FAS data diversity. Leveraging the WFAS
dataset and Protocol 1 (Known-Type), we host the Wild Face Anti-Spoofing
Challenge at the CVPR2023 workshop. Additionally, we meticulously evaluate
representative methods using Protocol 1 and Protocol 2 (Unknown-Type). Through
an in-depth examination of the challenge outcomes and benchmark baselines, we
provide insightful analyses and propose potential avenues for future research.
The dataset is released under Insightface.
</p></li>
</ul>

<h3>Title: Cost-damage analysis of attack trees. (arXiv:2304.05812v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05812">http://arxiv.org/abs/2304.05812</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05812] Cost-damage analysis of attack trees](http://arxiv.org/abs/2304.05812) #attack</code></li>
<li>Summary: <p>Attack trees (ATs) are a widely deployed modelling technique to categorize
potential attacks on a system. An attacker of such a system aims at doing as
much damage as possible, but might be limited by a cost budget. The maximum
possible damage for a given cost budget is an important security metric of a
system. In this paper, we find the maximum damage given a cost budget by
modelling this problem with ATs, both in deterministic and probabilistic
settings. We show that the general problem is NP-complete, and provide
heuristics to solve it. For general ATs these are based on integer linear
programming. However when the AT is tree-structured, then one can instead use a
faster bottom-up approach. We also extend these methods to other problems
related to the cost-damage tradeoff, such as the cost-damage Pareto front.
</p></li>
</ul>

<h3>Title: Exploiting Logic Locking for a Neural Trojan Attack on Machine Learning Accelerators. (arXiv:2304.06017v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.06017">http://arxiv.org/abs/2304.06017</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.06017] Exploiting Logic Locking for a Neural Trojan Attack on Machine Learning Accelerators](http://arxiv.org/abs/2304.06017) #attack</code></li>
<li>Summary: <p>Logic locking has been proposed to safeguard intellectual property (IP)
during chip fabrication. Logic locking techniques protect hardware IP by making
a subset of combinational modules in a design dependent on a secret key that is
withheld from untrusted parties. If an incorrect secret key is used, a set of
deterministic errors is produced in locked modules, restricting unauthorized
use. A common target for logic locking is neural accelerators, especially as
machine-learning-as-a-service becomes more prevalent. In this work, we explore
how logic locking can be used to compromise the security of a neural
accelerator it protects. Specifically, we show how the deterministic errors
caused by incorrect keys can be harnessed to produce neural-trojan-style
backdoors. To do so, we first outline a motivational attack scenario where a
carefully chosen incorrect key, which we call a trojan key, produces
misclassifications for an attacker-specified input class in a locked
accelerator. We then develop a theoretically-robust attack methodology to
automatically identify trojan keys. To evaluate this attack, we launch it on
several locked accelerators. In our largest benchmark accelerator, our attack
identified a trojan key that caused a 74\% decrease in classification accuracy
for attacker-specified trigger inputs, while degrading accuracy by only 1.7\%
for other inputs on average.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Boosting Cross-task Transferability of Adversarial Patches with Visual Relations. (arXiv:2304.05402v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05402">http://arxiv.org/abs/2304.05402</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05402] Boosting Cross-task Transferability of Adversarial Patches with Visual Relations](http://arxiv.org/abs/2304.05402) #robust</code></li>
<li>Summary: <p>The transferability of adversarial examples is a crucial aspect of evaluating
the robustness of deep learning systems, particularly in black-box scenarios.
Although several methods have been proposed to enhance cross-model
transferability, little attention has been paid to the transferability of
adversarial examples across different tasks. This issue has become increasingly
relevant with the emergence of foundational multi-task AI systems such as
Visual ChatGPT, rendering the utility of adversarial samples generated by a
single task relatively limited. Furthermore, these systems often entail
inferential functions beyond mere recognition-like tasks. To address this gap,
we propose a novel Visual Relation-based cross-task Adversarial Patch
generation method called VRAP, which aims to evaluate the robustness of various
visual tasks, especially those involving visual reasoning, such as Visual
Question Answering and Image Captioning. VRAP employs scene graphs to combine
object recognition-based deception with predicate-based relations elimination,
thereby disrupting the visual reasoning information shared among inferential
tasks. Our extensive experiments demonstrate that VRAP significantly surpasses
previous methods in terms of black-box transferability across diverse visual
reasoning tasks.
</p></li>
</ul>

<h3>Title: Isolated Sign Language Recognition based on Tree Structure Skeleton Images. (arXiv:2304.05403v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05403">http://arxiv.org/abs/2304.05403</a></li>
<li>Code URL: <a href="https://github.com/davidlainesv/sl-tssi-densenet">https://github.com/davidlainesv/sl-tssi-densenet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05403] Isolated Sign Language Recognition based on Tree Structure Skeleton Images](http://arxiv.org/abs/2304.05403) #robust</code></li>
<li>Summary: <p>Sign Language Recognition (SLR) systems aim to be embedded in video stream
platforms to recognize the sign performed in front of a camera. SLR research
has taken advantage of recent advances in pose estimation models to use
skeleton sequences estimated from videos instead of RGB information to predict
signs. This approach can make HAR-related tasks less complex and more robust to
diverse backgrounds, lightning conditions, and physical appearances. In this
work, we explore the use of a spatio-temporal skeleton representation such as
Tree Structure Skeleton Image (TSSI) as an alternative input to improve the
accuracy of skeleton-based models for SLR. TSSI converts a skeleton sequence
into an RGB image where the columns represent the joints of the skeleton in a
depth-first tree traversal order, the rows represent the temporal evolution of
the joints, and the three channels represent the (x, y, z) coordinates of the
joints. We trained a DenseNet-121 using this type of input and compared it with
other skeleton-based deep learning methods using a large-scale American Sign
Language (ASL) dataset, WLASL. Our model (SL-TSSI-DenseNet) overcomes the
state-of-the-art of other skeleton-based models. Moreover, when including data
augmentation our proposal achieves better results than both skeleton-based and
RGB-based models. We evaluated the effectiveness of our model on the Ankara
University Turkish Sign Language (TSL) dataset, AUTSL, and a Mexican Sign
Language (LSM) dataset. On the AUTSL dataset, the model achieves similar
results to the state-of-the-art of other skeleton-based models. On the LSM
dataset, the model achieves higher results than the baseline. Code has been
made available at: https://github.com/davidlainesv/SL-TSSI-DenseNet.
</p></li>
</ul>

<h3>Title: SceneCalib: Automatic Targetless Calibration of Cameras and Lidars in Autonomous Driving. (arXiv:2304.05530v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05530">http://arxiv.org/abs/2304.05530</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05530] SceneCalib: Automatic Targetless Calibration of Cameras and Lidars in Autonomous Driving](http://arxiv.org/abs/2304.05530) #robust</code></li>
<li>Summary: <p>Accurate camera-to-lidar calibration is a requirement for sensor data fusion
in many 3D perception tasks. In this paper, we present SceneCalib, a novel
method for simultaneous self-calibration of extrinsic and intrinsic parameters
in a system containing multiple cameras and a lidar sensor. Existing methods
typically require specially designed calibration targets and human operators,
or they only attempt to solve for a subset of calibration parameters. We
resolve these issues with a fully automatic method that requires no explicit
correspondences between camera images and lidar point clouds, allowing for
robustness to many outdoor environments. Furthermore, the full system is
jointly calibrated with explicit cross-camera constraints to ensure that
camera-to-camera and camera-to-lidar extrinsic parameters are consistent.
</p></li>
</ul>

<h3>Title: Factorized Inverse Path Tracing for Efficient and Accurate Material-Lighting Estimation. (arXiv:2304.05669v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05669">http://arxiv.org/abs/2304.05669</a></li>
<li>Code URL: <a href="https://github.com/lwwu2/fipt">https://github.com/lwwu2/fipt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05669] Factorized Inverse Path Tracing for Efficient and Accurate Material-Lighting Estimation](http://arxiv.org/abs/2304.05669) #robust</code></li>
<li>Summary: <p>Inverse path tracing has recently been applied to joint material and lighting
estimation, given geometry and multi-view HDR observations of an indoor scene.
However, it has two major limitations: path tracing is expensive to compute,
and ambiguities exist between reflection and emission. We propose a novel
Factorized Inverse Path Tracing (FIPT) method which utilizes a factored light
transport formulation and finds emitters driven by rendering errors. Our
algorithm enables accurate material and lighting optimization faster than
previous work, and is more effective at resolving ambiguities. The exhaustive
experiments on synthetic scenes show that our method (1) outperforms
state-of-the-art indoor inverse rendering and relighting methods particularly
in the presence of complex illumination effects; (2) speeds up inverse path
tracing optimization to less than an hour. We further demonstrate robustness to
noisy inputs through material and lighting estimates that allow plausible
relighting in a real scene. The source code is available at:
https://github.com/lwwu2/fipt
</p></li>
</ul>

<h3>Title: Mesh2Tex: Generating Mesh Textures from Image Queries. (arXiv:2304.05868v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05868">http://arxiv.org/abs/2304.05868</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05868] Mesh2Tex: Generating Mesh Textures from Image Queries](http://arxiv.org/abs/2304.05868) #robust</code></li>
<li>Summary: <p>Remarkable advances have been achieved recently in learning neural
representations that characterize object geometry, while generating textured
objects suitable for downstream applications and 3D rendering remains at an
early stage. In particular, reconstructing textured geometry from images of
real objects is a significant challenge -- reconstructed geometry is often
inexact, making realistic texturing a significant challenge. We present
Mesh2Tex, which learns a realistic object texture manifold from uncorrelated
collections of 3D object geometry and photorealistic RGB images, by leveraging
a hybrid mesh-neural-field texture representation. Our texture representation
enables compact encoding of high-resolution textures as a neural field in the
barycentric coordinate system of the mesh faces. The learned texture manifold
enables effective navigation to generate an object texture for a given 3D
object geometry that matches to an input RGB image, which maintains robustness
even under challenging real-world scenarios where the mesh geometry
approximates an inexact match to the underlying geometry in the RGB image.
Mesh2Tex can effectively generate realistic object textures for an object mesh
to match real images observations towards digitization of real environments,
significantly improving over previous state of the art.
</p></li>
</ul>

<h3>Title: Unicom: Universal and Compact Representation Learning for Image Retrieval. (arXiv:2304.05884v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05884">http://arxiv.org/abs/2304.05884</a></li>
<li>Code URL: <a href="https://github.com/deepglint/unicom">https://github.com/deepglint/unicom</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05884] Unicom: Universal and Compact Representation Learning for Image Retrieval](http://arxiv.org/abs/2304.05884) #robust</code></li>
<li>Summary: <p>Modern image retrieval methods typically rely on fine-tuning pre-trained
encoders to extract image-level descriptors. However, the most widely used
models are pre-trained on ImageNet-1K with limited classes. The pre-trained
feature representation is therefore not universal enough to generalize well to
the diverse open-world classes. In this paper, we first cluster the large-scale
LAION400M into one million pseudo classes based on the joint textual and visual
features extracted by the CLIP model. Due to the confusion of label
granularity, the automatically clustered dataset inevitably contains heavy
inter-class conflict. To alleviate such conflict, we randomly select partial
inter-class prototypes to construct the margin-based softmax loss. To further
enhance the low-dimensional feature representation, we randomly select partial
feature dimensions when calculating the similarities between embeddings and
class-wise prototypes. The dual random partial selections are with respect to
the class dimension and the feature dimension of the prototype matrix, making
the classification conflict-robust and the feature embedding compact. Our
method significantly outperforms state-of-the-art unsupervised and supervised
image retrieval approaches on multiple benchmarks. The code and pre-trained
models are released to facilitate future research
https://github.com/deepglint/unicom.
</p></li>
</ul>

<h3>Title: Are Local Features All You Need for Cross-Domain Visual Place Recognition?. (arXiv:2304.05887v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05887">http://arxiv.org/abs/2304.05887</a></li>
<li>Code URL: <a href="https://github.com/gbarbarani/re-ranking-for-vpr">https://github.com/gbarbarani/re-ranking-for-vpr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05887] Are Local Features All You Need for Cross-Domain Visual Place Recognition?](http://arxiv.org/abs/2304.05887) #robust</code></li>
<li>Summary: <p>Visual Place Recognition is a task that aims to predict the coordinates of an
image (called query) based solely on visual clues. Most commonly, a retrieval
approach is adopted, where the query is matched to the most similar images from
a large database of geotagged photos, using learned global descriptors. Despite
recent advances, recognizing the same place when the query comes from a
significantly different distribution is still a major hurdle for state of the
art retrieval methods. Examples are heavy illumination changes (e.g. night-time
images) or substantial occlusions (e.g. transient objects). In this work we
explore whether re-ranking methods based on spatial verification can tackle
these challenges, following the intuition that local descriptors are inherently
more robust than global features to domain shifts. To this end, we provide a
new, comprehensive benchmark on current state of the art models. We also
introduce two new demanding datasets with night and occluded queries, to be
matched against a city-wide database. Code and datasets are available at
https://github.com/gbarbarani/re-ranking-for-VPR.
</p></li>
</ul>

<h3>Title: Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera. (arXiv:2304.06019v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.06019">http://arxiv.org/abs/2304.06019</a></li>
<li>Code URL: <a href="https://github.com/jnjaby/alignformer">https://github.com/jnjaby/alignformer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.06019] Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera](http://arxiv.org/abs/2304.06019) #robust</code></li>
<li>Summary: <p>Due to the difficulty in collecting large-scale and perfectly aligned paired
training data for Under-Display Camera (UDC) image restoration, previous
methods resort to monitor-based image systems or simulation-based methods,
sacrificing the realness of the data and introducing domain gaps. In this work,
we revisit the classic stereo setup for training data collection -- capturing
two images of the same scene with one UDC and one standard camera. The key idea
is to "copy" details from a high-quality reference image and "paste" them on
the UDC image. While being able to generate real training pairs, this setting
is susceptible to spatial misalignment due to perspective and depth of field
changes. The problem is further compounded by the large domain discrepancy
between the UDC and normal images, which is unique to UDC restoration. In this
paper, we mitigate the non-trivial domain discrepancy and spatial misalignment
through a novel Transformer-based framework that generates well-aligned yet
high-quality target data for the corresponding UDC input. This is made possible
through two carefully designed components, namely, the Domain Alignment Module
(DAM) and Geometric Alignment Module (GAM), which encourage robust and accurate
discovery of correspondence between the UDC and normal views. Extensive
experiments show that high-quality and well-aligned pseudo UDC training pairs
are beneficial for training a robust restoration network. Code and the dataset
are available at https://github.com/jnjaby/AlignFormer.
</p></li>
</ul>

<h3>Title: chatIPCC: Grounding Conversational AI in Climate Science. (arXiv:2304.05510v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05510">http://arxiv.org/abs/2304.05510</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05510] chatIPCC: Grounding Conversational AI in Climate Science](http://arxiv.org/abs/2304.05510) #robust</code></li>
<li>Summary: <p>Large Language Models (LLMs) have made significant progress in recent years,
achieving remarkable results in question-answering tasks (QA). However, they
still face two major challenges: hallucination and outdated information after
the training phase. These challenges take center stage in critical domains like
climate change, where obtaining accurate and up-to-date information from
reliable sources in a limited time is essential and difficult. To overcome
these barriers, one potential solution is to provide LLMs with access to
external, scientifically accurate, and robust sources (long-term memory) to
continuously update their knowledge and prevent the propagation of inaccurate,
incorrect, or outdated information. In this study, we enhanced GPT-4 by
integrating the information from the Sixth Assessment Report of the
Intergovernmental (IPCC AR6), the most comprehensive, up-to-date, and reliable
source in this domain. We present our conversational AI prototype, available at
www.chatclimate.ai/ipcc and demonstrate its ability to answer challenging
questions accurately in three different QA scenarios: asking from 1) GPT-4, 2)
chatIPCC, and 3) hybrid chatIPCC. The answers and their sources were evaluated
by our team of IPCC authors, who used their expert knowledge to score the
accuracy of the answers from 1 (very-low) to 5 (very-high). The evaluation
showed that the hybrid chatIPCC provided more accurate answers, highlighting
the effectiveness of our solution. This approach can be easily scaled for
chatbots in specific domains, enabling the delivery of reliable and accurate
information.
</p></li>
</ul>

<h3>Title: ReDWINE: A Clinical Datamart with Text Analytical Capabilities to Facilitate Rehabilitation Research. (arXiv:2304.05929v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05929">http://arxiv.org/abs/2304.05929</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05929] ReDWINE: A Clinical Datamart with Text Analytical Capabilities to Facilitate Rehabilitation Research](http://arxiv.org/abs/2304.05929) #robust</code></li>
<li>Summary: <p>Rehabilitation research focuses on determining the components of a treatment
intervention, the mechanism of how these components lead to recovery and
rehabilitation, and ultimately the optimal intervention strategies to maximize
patients' physical, psychologic, and social functioning. Traditional randomized
clinical trials that study and establish new interventions face several
challenges, such as high cost and time commitment. Observational studies that
use existing clinical data to observe the effect of an intervention have shown
several advantages over RCTs. Electronic Health Records (EHRs) have become an
increasingly important resource for conducting observational studies. To
support these studies, we developed a clinical research datamart, called
ReDWINE (Rehabilitation Datamart With Informatics iNfrastructure for rEsearch),
that transforms the rehabilitation-related EHR data collected from the UPMC
health care system to the Observational Health Data Sciences and Informatics
(OHDSI) Observational Medical Outcomes Partnership (OMOP) Common Data Model
(CDM) to facilitate rehabilitation research. The standardized EHR data stored
in ReDWINE will further reduce the time and effort required by investigators to
pool, harmonize, clean, and analyze data from multiple sources, leading to more
robust and comprehensive research findings. ReDWINE also includes deployment of
data visualization and data analytics tools to facilitate cohort definition and
clinical data analysis. These include among others the Open Health Natural
Language Processing (OHNLP) toolkit, a high-throughput NLP pipeline, to provide
text analytical capabilities at scale in ReDWINE. Using this comprehensive
representation of patient data in ReDWINE for rehabilitation research will
facilitate real-world evidence for health interventions and outcomes.
</p></li>
</ul>

<h3>Title: Dynamic Mixed Membership Stochastic Block Model for Weighted Labeled Networks. (arXiv:2304.05894v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05894">http://arxiv.org/abs/2304.05894</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05894] Dynamic Mixed Membership Stochastic Block Model for Weighted Labeled Networks](http://arxiv.org/abs/2304.05894) #robust</code></li>
<li>Summary: <p>Most real-world networks evolve over time. Existing literature proposes
models for dynamic networks that are either unlabeled or assumed to have a
single membership structure. On the other hand, a new family of Mixed
Membership Stochastic Block Models (MMSBM) allows to model static labeled
networks under the assumption of mixed-membership clustering. In this work, we
propose to extend this later class of models to infer dynamic labeled networks
under a mixed membership assumption. Our approach takes the form of a temporal
prior on the model's parameters. It relies on the single assumption that
dynamics are not abrupt. We show that our method significantly differs from
existing approaches, and allows to model more complex systems --dynamic labeled
networks. We demonstrate the robustness of our method with several experiments
on both synthetic and real-world datasets. A key interest of our approach is
that it needs very few training data to yield good results. The performance
gain under challenging conditions broadens the variety of possible applications
of automated learning tools --as in social sciences, which comprise many fields
where small datasets are a major obstacle to the introduction of machine
learning methods.
</p></li>
</ul>

<h3>Title: Maximum-likelihood Estimators in Physics-Informed Neural Networks for High-dimensional Inverse Problems. (arXiv:2304.05991v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05991">http://arxiv.org/abs/2304.05991</a></li>
<li>Code URL: <a href="https://github.com/gusmaogabriels/kinn">https://github.com/gusmaogabriels/kinn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05991] Maximum-likelihood Estimators in Physics-Informed Neural Networks for High-dimensional Inverse Problems](http://arxiv.org/abs/2304.05991) #robust</code></li>
<li>Summary: <p>Physics-informed neural networks (PINNs) have proven a suitable mathematical
scaffold for solving inverse ordinary (ODE) and partial differential equations
(PDE). Typical inverse PINNs are formulated as soft-constrained multi-objective
optimization problems with several hyperparameters. In this work, we
demonstrate that inverse PINNs can be framed in terms of maximum-likelihood
estimators (MLE) to allow explicit error propagation from interpolation to the
physical model space through Taylor expansion, without the need of
hyperparameter tuning. We explore its application to high-dimensional coupled
ODEs constrained by differential algebraic equations that are common in
transient chemical and biological kinetics. Furthermore, we show that
singular-value decomposition (SVD) of the ODE coupling matrices (reaction
stoichiometry matrix) provides reduced uncorrelated subspaces in which PINNs
solutions can be represented and over which residuals can be projected.
Finally, SVD bases serve as preconditioners for the inversion of covariance
matrices in this hyperparameter-free robust application of MLE to
``kinetics-informed neural networks''.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: On the Adversarial Inversion of Deep Biometric Representations. (arXiv:2304.05561v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05561">http://arxiv.org/abs/2304.05561</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05561] On the Adversarial Inversion of Deep Biometric Representations](http://arxiv.org/abs/2304.05561) #biometric</code></li>
<li>Summary: <p>Biometric authentication service providers often claim that it is not
possible to reverse-engineer a user's raw biometric sample, such as a
fingerprint or a face image, from its mathematical (feature-space)
representation. In this paper, we investigate this claim on the specific
example of deep neural network (DNN) embeddings. Inversion of DNN embeddings
has been investigated for explaining deep image representations or synthesizing
normalized images. Existing studies leverage full access to all layers of the
original model, as well as all possible information on the original dataset.
For the biometric authentication use case, we need to investigate this under
adversarial settings where an attacker has access to a feature-space
representation but no direct access to the exact original dataset nor the
original learned model. Instead, we assume varying degree of attacker's
background knowledge about the distribution of the dataset as well as the
original learned model (architecture and training process). In these cases, we
show that the attacker can exploit off-the-shelf DNN models and public
datasets, to mimic the behaviour of the original learned model to varying
degrees of success, based only on the obtained representation and attacker's
prior knowledge. We propose a two-pronged attack that first infers the original
DNN by exploiting the model footprint on the embedding, and then reconstructs
the raw data by using the inferred model. We show the practicality of the
attack on popular DNNs trained for two prominent biometric modalities, face and
fingerprint recognition. The attack can effectively infer the original
recognition model (mean accuracy 83\% for faces, 86\% for fingerprints), and
can craft effective biometric reconstructions that are successfully
authenticated with 1-vs-1 authentication accuracy of up to 92\% for some
models.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: SuperpixelGraph: Semi-automatic generation of building footprint through semantic-sensitive superpixel and neural graph networks. (arXiv:2304.05661v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05661">http://arxiv.org/abs/2304.05661</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05661] SuperpixelGraph: Semi-automatic generation of building footprint through semantic-sensitive superpixel and neural graph networks](http://arxiv.org/abs/2304.05661) #extraction</code></li>
<li>Summary: <p>Most urban applications necessitate building footprints in the form of
concise vector graphics with sharp boundaries rather than pixel-wise raster
images. This need contrasts with the majority of existing methods, which
typically generate over-smoothed footprint polygons. Editing these
automatically produced polygons can be inefficient, if not more time-consuming
than manual digitization. This paper introduces a semi-automatic approach for
building footprint extraction through semantically-sensitive superpixels and
neural graph networks. Drawing inspiration from object-based classification
techniques, we first learn to generate superpixels that are not only
boundary-preserving but also semantically-sensitive. The superpixels respond
exclusively to building boundaries rather than other natural objects, while
simultaneously producing semantic segmentation of the buildings. These
intermediate superpixel representations can be naturally considered as nodes
within a graph. Consequently, graph neural networks are employed to model the
global interactions among all superpixels and enhance the representativeness of
node features for building segmentation. Classical approaches are utilized to
extract and regularize boundaries for the vectorized building footprints.
Utilizing minimal clicks and straightforward strokes, we efficiently accomplish
accurate segmentation outcomes, eliminating the necessity for editing polygon
vertices. Our proposed approach demonstrates superior precision and efficacy,
as validated by experimental assessments on various public benchmark datasets.
We observe a 10\% enhancement in the metric for superpixel clustering and an
8\% increment in vector graphics evaluation, when compared with established
techniques. Additionally, we have devised an optimized and sophisticated
pipeline for interactive editing, poised to further augment the overall quality
of the results.
</p></li>
</ul>

<h3>Title: SketchANIMAR: Sketch-based 3D Animal Fine-Grained Retrieval. (arXiv:2304.05731v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05731">http://arxiv.org/abs/2304.05731</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05731] SketchANIMAR: Sketch-based 3D Animal Fine-Grained Retrieval](http://arxiv.org/abs/2304.05731) #extraction</code></li>
<li>Summary: <p>The retrieval of 3D objects has gained significant importance in recent years
due to its broad range of applications in computer vision, computer graphics,
virtual reality, and augmented reality. However, the retrieval of 3D objects
presents significant challenges due to the intricate nature of 3D models, which
can vary in shape, size, and texture, and have numerous polygons and vertices.
To this end, we introduce a novel SHREC challenge track that focuses on
retrieving relevant 3D animal models from a dataset using sketch queries and
expedites accessing 3D models through available sketches. Furthermore, a new
dataset named ANIMAR was constructed in this study, comprising a collection of
711 unique 3D animal models and 140 corresponding sketch queries. Our contest
requires participants to retrieve 3D models based on complex and detailed
sketches. We receive satisfactory results from eight teams and 204 runs.
Although further improvement is necessary, the proposed task has the potential
to incentivize additional research in the domain of 3D object retrieval,
potentially yielding benefits for a wide range of applications. We also provide
insights into potential areas of future research, such as improving techniques
for feature extraction and matching, and creating more diverse datasets to
evaluate retrieval performance.
</p></li>
</ul>

<h3>Title: DUFormer: A Novel Architecture for Power Line Segmentation of Aerial Images. (arXiv:2304.05821v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05821">http://arxiv.org/abs/2304.05821</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05821] DUFormer: A Novel Architecture for Power Line Segmentation of Aerial Images](http://arxiv.org/abs/2304.05821) #extraction</code></li>
<li>Summary: <p>Power lines pose a significant safety threat to unmanned aerial vehicles
(UAVs) operating at low altitudes. However, detecting power lines in aerial
images is challenging due to the small size of the foreground data (i.e., power
lines) and the abundance of background information. To address this challenge,
we propose DUFormer, a semantic segmentation algorithm designed specifically
for power line detection in aerial images. We assume that performing sufficient
feature extraction with a convolutional neural network (CNN) that has a strong
inductive bias is beneficial for training an efficient Transformer model. To
this end, we propose a heavy token encoder responsible for overlapping feature
re-mining and tokenization. The encoder comprises a pyramid CNN feature
extraction module and a power line feature enhancement module. Following
sufficient feature extraction for power lines, the feature fusion is carried
out, and then the Transformer block is used for global modeling. The final
segmentation result is obtained by fusing local and global features in the
decode head. Additionally, we demonstrate the significance of the joint
multi-weight loss function in power line segmentation. The experimental results
demonstrate that our proposed method achieves the state-of-the-art performance
in power line segmentation on the publicly available TTPLA dataset.
</p></li>
</ul>

<h3>Title: MED-VT: Multiscale Encoder-Decoder Video Transformer with Application to Object Segmentation. (arXiv:2304.05930v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05930">http://arxiv.org/abs/2304.05930</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05930] MED-VT: Multiscale Encoder-Decoder Video Transformer with Application to Object Segmentation](http://arxiv.org/abs/2304.05930) #extraction</code></li>
<li>Summary: <p>Multiscale video transformers have been explored in a wide variety of vision
tasks. To date, however, the multiscale processing has been confined to the
encoder or decoder alone. We present a unified multiscale encoder-decoder
transformer that is focused on dense prediction tasks in videos. Multiscale
representation at both encoder and decoder yields key benefits of implicit
extraction of spatiotemporal features (i.e. without reliance on input optical
flow) as well as temporal consistency at encoding and coarseto-fine detection
for high-level (e.g. object) semantics to guide precise localization at
decoding. Moreover, we propose a transductive learning scheme through
many-to-many label propagation to provide temporally consistent predictions. We
showcase our Multiscale Encoder-Decoder Video Transformer (MED-VT) on Automatic
Video Object Segmentation (AVOS) and actor/action segmentation, where we
outperform state-of-the-art approaches on multiple benchmarks using only raw
images, without using optical flow.
</p></li>
</ul>

<h3>Title: Fast vehicle detection algorithm based on lightweight YOLO7-tiny. (arXiv:2304.06002v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.06002">http://arxiv.org/abs/2304.06002</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.06002] Fast vehicle detection algorithm based on lightweight YOLO7-tiny](http://arxiv.org/abs/2304.06002) #extraction</code></li>
<li>Summary: <p>The swift and precise detection of vehicles holds significant research
significance in intelligent transportation systems (ITS). However, current
vehicle detection algorithms encounter challenges such as high computational
complexity, low detection rate, and limited feasibility on mobile devices. To
address these issues, this paper proposes a lightweight vehicle detection
algorithm for YOLOv7-tiny called Ghost-YOLOv7. The model first scales the width
multiple to 0.5 and replaces the standard convolution of the backbone network
with Ghost convolution to achieve a lighter network and improve the detection
speed; secondly, a Ghost bi-directional feature pyramid network (Ghost-BiFPN)
neck network is designed to enhance feature extraction capability of the
algorithm and enrich semantic information; thirdly, a Ghost Decouoled Head
(GDH) is employed for accurate prediction of vehicle location and class,
enhancing model accuracy; finally, a coordinate attention mechanism is
introduced in the output layer to suppress environmental interference, and the
WIoU loss function is employed to enhance the detection accuracy further.
Experimental results on the PASCAL VOC dataset demonstrate that Ghost-YOLOv7
outperforms the original YOLOv7-tiny model, achieving a 29.8% reduction in
computation, 37.3% reduction in the number of parameters, 35.1% reduction in
model weights, and 1.1% higher mean average precision (mAP), while achieving a
detection speed of 428 FPS. These results validate the effectiveness of the
proposed method.
</p></li>
</ul>

<h3>Title: GPr-Net: Geometric Prototypical Network for Point Cloud Few-Shot Learning. (arXiv:2304.06007v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.06007">http://arxiv.org/abs/2304.06007</a></li>
<li>Code URL: <a href="https://github.com/tejasanvekar/gpr-net">https://github.com/tejasanvekar/gpr-net</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.06007] GPr-Net: Geometric Prototypical Network for Point Cloud Few-Shot Learning](http://arxiv.org/abs/2304.06007) #extraction</code></li>
<li>Summary: <p>In the realm of 3D-computer vision applications, point cloud few-shot
learning plays a critical role. However, it poses an arduous challenge due to
the sparsity, irregularity, and unordered nature of the data. Current methods
rely on complex local geometric extraction techniques such as convolution,
graph, and attention mechanisms, along with extensive data-driven pre-training
tasks. These approaches contradict the fundamental goal of few-shot learning,
which is to facilitate efficient learning. To address this issue, we propose
GPr-Net (Geometric Prototypical Network), a lightweight and computationally
efficient geometric prototypical network that captures the intrinsic topology
of point clouds and achieves superior performance. Our proposed method, IGI++
(Intrinsic Geometry Interpreter++) employs vector-based hand-crafted intrinsic
geometry interpreters and Laplace vectors to extract and evaluate point cloud
morphology, resulting in improved representations for FSL (Few-Shot Learning).
Additionally, Laplace vectors enable the extraction of valuable features from
point clouds with fewer points. To tackle the distribution drift challenge in
few-shot metric learning, we leverage hyperbolic space and demonstrate that our
approach handles intra and inter-class variance better than existing point
cloud few-shot learning methods. Experimental results on the ModelNet40 dataset
show that GPr-Net outperforms state-of-the-art methods in few-shot learning on
point clouds, achieving utmost computational efficiency that is $170\times$
better than all existing works. The code is publicly available at
https://github.com/TejasAnvekar/GPr-Net.
</p></li>
</ul>

<h3>Title: Zero-shot Temporal Relation Extraction with ChatGPT. (arXiv:2304.05454v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05454">http://arxiv.org/abs/2304.05454</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05454] Zero-shot Temporal Relation Extraction with ChatGPT](http://arxiv.org/abs/2304.05454) #extraction</code></li>
<li>Summary: <p>The goal of temporal relation extraction is to infer the temporal relation
between two events in the document. Supervised models are dominant in this
task. In this work, we investigate ChatGPT's ability on zero-shot temporal
relation extraction. We designed three different prompt techniques to break
down the task and evaluate ChatGPT. Our experiments show that ChatGPT's
performance has a large gap with that of supervised methods and can heavily
rely on the design of prompts. We further demonstrate that ChatGPT can infer
more small relation classes correctly than supervised methods. The current
shortcomings of ChatGPT on temporal relation extraction are also discussed in
this paper. We found that ChatGPT cannot keep consistency during temporal
inference and it fails in actively long-dependency temporal inference.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Zero-Knowledge Proof-based Practical Federated Learning on Blockchain. (arXiv:2304.05590v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05590">http://arxiv.org/abs/2304.05590</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05590] Zero-Knowledge Proof-based Practical Federated Learning on Blockchain](http://arxiv.org/abs/2304.05590) #federate</code></li>
<li>Summary: <p>Since the concern of privacy leakage extremely discourages user participation
in sharing data, federated learning has gradually become a promising technique
for both academia and industry for achieving collaborative learning without
leaking information about the local data. Unfortunately, most federated
learning solutions cannot efficiently verify the execution of each
participant's local machine learning model and protect the privacy of user
data, simultaneously. In this article, we first propose a Zero-Knowledge
Proof-based Federated Learning (ZKP-FL) scheme on blockchain. It leverages
zero-knowledge proof for both the computation of local data and the aggregation
of local model parameters, aiming to verify the computation process without
requiring the plaintext of the local data. We further propose a Practical
ZKP-FL (PZKP-FL) scheme to support fraction and non-linear operations.
Specifically, we explore a Fraction-Integer mapping function, and use Taylor
expansion to efficiently handle non-linear operations while maintaining the
accuracy of the federated learning model. We also analyze the security of
PZKP-FL. Performance analysis demonstrates that the whole running time of the
PZKP-FL scheme is approximately less than one minute in parallel execution.
</p></li>
</ul>

<h3>Title: A Game-theoretic Framework for Federated Learning. (arXiv:2304.05836v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05836">http://arxiv.org/abs/2304.05836</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05836] A Game-theoretic Framework for Federated Learning](http://arxiv.org/abs/2304.05836) #federate</code></li>
<li>Summary: <p>In federated learning, benign participants aim to optimize a global model
collaboratively. However, the risk of \textit{privacy leakage} cannot be
ignored in the presence of \textit{semi-honest} adversaries. Existing research
has focused either on designing protection mechanisms or on inventing attacking
mechanisms. While the battle between defenders and attackers seems
never-ending, we are concerned with one critical question: is it possible to
prevent potential attacks in advance? To address this, we propose the first
game-theoretic framework that considers both FL defenders and attackers in
terms of their respective payoffs, which include computational costs, FL model
utilities, and privacy leakage risks. We name this game the Federated Learning
Security Game (FLSG), in which neither defenders nor attackers are aware of all
participants' payoffs.
</p></li>
</ul>

<p>To handle the \textit{incomplete information} inherent in this situation, we
propose associating the FLSG with an \textit{oracle} that has two primary
responsibilities. First, the oracle provides lower and upper bounds of the
payoffs for the players. Second, the oracle acts as a correlation device,
privately providing suggested actions to each player. With this novel
framework, we analyze the optimal strategies of defenders and attackers.
Furthermore, we derive and demonstrate conditions under which the attacker, as
a rational decision-maker, should always follow the oracle's suggestion
\textit{not to attack}.
</p>

<h3>Title: GraphGANFed: A Federated Generative Framework for Graph-Structured Molecules Towards Efficient Drug Discovery. (arXiv:2304.05498v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05498">http://arxiv.org/abs/2304.05498</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05498] GraphGANFed: A Federated Generative Framework for Graph-Structured Molecules Towards Efficient Drug Discovery](http://arxiv.org/abs/2304.05498) #federate</code></li>
<li>Summary: <p>Recent advances in deep learning have accelerated its use in various
applications, such as cellular image analysis and molecular discovery. In
molecular discovery, a generative adversarial network (GAN), which comprises a
discriminator to distinguish generated molecules from existing molecules and a
generator to generate new molecules, is one of the premier technologies due to
its ability to learn from a large molecular data set efficiently and generate
novel molecules that preserve similar properties. However, different
pharmaceutical companies may be unwilling or unable to share their local data
sets due to the geo-distributed and sensitive nature of molecular data sets,
making it impossible to train GANs in a centralized manner. In this paper, we
propose a Graph convolutional network in Generative Adversarial Networks via
Federated learning (GraphGANFed) framework, which integrates graph
convolutional neural Network (GCN), GAN, and federated learning (FL) as a whole
system to generate novel molecules without sharing local data sets. In
GraphGANFed, the discriminator is implemented as a GCN to better capture
features from molecules represented as molecular graphs, and FL is used to
train both the discriminator and generator in a distributive manner to preserve
data privacy. Extensive simulations are conducted based on the three bench-mark
data sets to demonstrate the feasibility and effectiveness of GraphGANFed. The
molecules generated by GraphGANFed can achieve high novelty (=100) and
diversity (> 0.9). The simulation results also indicate that 1) a lower
complexity discriminator model can better avoid mode collapse for a smaller
data set, 2) there is a tradeoff among different evaluation metrics, and 3)
having the right dropout ratio of the generator and discriminator can avoid
mode collapse.
</p></li>
</ul>

<h3>Title: Edge-cloud Collaborative Learning with Federated and Centralized Features. (arXiv:2304.05871v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05871">http://arxiv.org/abs/2304.05871</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05871] Edge-cloud Collaborative Learning with Federated and Centralized Features](http://arxiv.org/abs/2304.05871) #federate</code></li>
<li>Summary: <p>Federated learning (FL) is a popular way of edge computing that doesn't
compromise users' privacy. Current FL paradigms assume that data only resides
on the edge, while cloud servers only perform model averaging. However, in
real-life situations such as recommender systems, the cloud server has the
ability to store historical and interactive features. In this paper, our
proposed Edge-Cloud Collaborative Knowledge Transfer Framework (ECCT) bridges
the gap between the edge and cloud, enabling bi-directional knowledge transfer
between both, sharing feature embeddings and prediction logits. ECCT
consolidates various benefits, including enhancing personalization, enabling
model heterogeneity, tolerating training asynchronization, and relieving
communication burdens. Extensive experiments on public and industrial datasets
demonstrate ECCT's effectiveness and potential for use in academia and
industry.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Auditing ICU Readmission Rates in an Clinical Database: An Analysis of Risk Factors and Clinical Outcomes. (arXiv:2304.05986v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05986">http://arxiv.org/abs/2304.05986</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05986] Auditing ICU Readmission Rates in an Clinical Database: An Analysis of Risk Factors and Clinical Outcomes](http://arxiv.org/abs/2304.05986) #fair</code></li>
<li>Summary: <p>This study presents a machine learning (ML) pipeline for clinical data
classification in the context of a 30-day readmission problem, along with a
fairness audit on subgroups based on sensitive attributes. A range of ML models
are used for classification and the fairness audit is conducted on the model
predictions. The fairness audit uncovers disparities in equal opportunity,
predictive parity, false positive rate parity, and false negative rate parity
criteria on the MIMIC III dataset based on attributes such as gender,
ethnicity, language, and insurance group. The results identify disparities in
the model's performance across different groups and highlights the need for
better fairness and bias mitigation strategies. The study suggests the need for
collaborative efforts among researchers, policymakers, and practitioners to
address bias and fairness in artificial intelligence (AI) systems.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Optimal Interpretability-Performance Trade-off of Classification Trees with Black-Box Reinforcement Learning. (arXiv:2304.05839v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05839">http://arxiv.org/abs/2304.05839</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05839] Optimal Interpretability-Performance Trade-off of Classification Trees with Black-Box Reinforcement Learning](http://arxiv.org/abs/2304.05839) #interpretability</code></li>
<li>Summary: <p>Interpretability of AI models allows for user safety checks to build trust in
these models. In particular, decision trees (DTs) provide a global view on the
learned model and clearly outlines the role of the features that are critical
to classify a given data. However, interpretability is hindered if the DT is
too large. To learn compact trees, a Reinforcement Learning (RL) framework has
been recently proposed to explore the space of DTs. A given supervised
classification task is modeled as a Markov decision problem (MDP) and then
augmented with additional actions that gather information about the features,
equivalent to building a DT. By appropriately penalizing these actions, the RL
agent learns to optimally trade-off size and performance of a DT. However, to
do so, this RL agent has to solve a partially observable MDP. The main
contribution of this paper is to prove that it is sufficient to solve a fully
observable problem to learn a DT optimizing the interpretability-performance
trade-off. As such any planning or RL algorithm can be used. We demonstrate the
effectiveness of this approach on a set of classical supervised classification
datasets and compare our approach with other interpretability-performance
optimizing methods.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: CLIP Surgery for Better Explainability with Enhancement in Open-Vocabulary Tasks. (arXiv:2304.05653v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05653">http://arxiv.org/abs/2304.05653</a></li>
<li>Code URL: <a href="https://github.com/xmed-lab/clip_surgery">https://github.com/xmed-lab/clip_surgery</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05653] CLIP Surgery for Better Explainability with Enhancement in Open-Vocabulary Tasks](http://arxiv.org/abs/2304.05653) #explainability</code></li>
<li>Summary: <p>Contrastive Language-Image Pre-training (CLIP) is a powerful multimodal large
vision model that has demonstrated significant benefits for downstream tasks,
including many zero-shot learning and text-guided vision tasks. However, we
notice some severe problems regarding the model's explainability, which
undermines its credibility and impedes related tasks. Specifically, we find
CLIP prefers the background regions than the foregrounds according to the
predicted similarity map, which contradicts human understanding. Besides, there
are obvious noisy activations on the visualization results at irrelevant
positions. To address these two issues, we conduct in-depth analyses and reveal
the reasons with new findings and evidences. Based on these insights, we
propose the CLIP Surgery, a method that enables surgery-like modifications for
the inference architecture and features, for better explainability and
enhancement in multiple open-vocabulary tasks. The proposed method has
significantly improved the explainability of CLIP for both convolutional
networks and vision transformers, surpassing existing methods by large margins.
Besides, our approach also demonstrates remarkable improvements in
open-vocabulary segmentation and multi-label recognition tasks. For examples,
the mAP improvement on NUS-Wide multi-label recognition is 4.41% without any
additional training, and our CLIP Surgery surpasses the state-of-the-art method
by 8.74% at mIoU on Cityscapes open-vocabulary semantic segmentation.
Furthermore, our method benefits other tasks including multimodal visualization
and interactive segmentation like Segment Anything Model (SAM). The code is
available at https://github.com/xmed-lab/CLIP_Surgery
</p></li>
</ul>

<h3>Title: Communicating Uncertainty in Machine Learning Explanations: A Visualization Analytics Approach for Predictive Process Monitoring. (arXiv:2304.05736v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05736">http://arxiv.org/abs/2304.05736</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05736] Communicating Uncertainty in Machine Learning Explanations: A Visualization Analytics Approach for Predictive Process Monitoring](http://arxiv.org/abs/2304.05736) #explainability</code></li>
<li>Summary: <p>As data-driven intelligent systems advance, the need for reliable and
transparent decision-making mechanisms has become increasingly important.
Therefore, it is essential to integrate uncertainty quantification and model
explainability approaches to foster trustworthy business and operational
process analytics. This study explores how model uncertainty can be effectively
communicated in global and local post-hoc explanation approaches, such as
Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE)
plots. In addition, this study examines appropriate visualization analytics
approaches to facilitate such methodological integration. By combining these
two research directions, decision-makers can not only justify the plausibility
of explanation-driven actionable insights but also validate their reliability.
Finally, the study includes expert interviews to assess the suitability of the
proposed approach and designed interface for a real-world predictive process
monitoring problem in the manufacturing domain.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: CamDiff: Camouflage Image Augmentation via Diffusion Model. (arXiv:2304.05469v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05469">http://arxiv.org/abs/2304.05469</a></li>
<li>Code URL: <a href="https://github.com/drlxj/camdiff">https://github.com/drlxj/camdiff</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05469] CamDiff: Camouflage Image Augmentation via Diffusion Model](http://arxiv.org/abs/2304.05469) #diffusion</code></li>
<li>Summary: <p>The burgeoning field of camouflaged object detection (COD) seeks to identify
objects that blend into their surroundings. Despite the impressive performance
of recent models, we have identified a limitation in their robustness, where
existing methods may misclassify salient objects as camouflaged ones, despite
these two characteristics being contradictory. This limitation may stem from
lacking multi-pattern training images, leading to less saliency robustness. To
address this issue, we introduce CamDiff, a novel approach inspired by
AI-Generated Content (AIGC) that overcomes the scarcity of multi-pattern
training images. Specifically, we leverage the latent diffusion model to
synthesize salient objects in camouflaged scenes, while using the zero-shot
image classification ability of the Contrastive Language-Image Pre-training
(CLIP) model to prevent synthesis failures and ensure the synthesized object
aligns with the input prompt. Consequently, the synthesized image retains its
original camouflage label while incorporating salient objects, yielding
camouflage samples with richer characteristics. The results of user studies
show that the salient objects in the scenes synthesized by our framework
attract the user's attention more; thus, such samples pose a greater challenge
to the existing COD models. Our approach enables flexible editing and efficient
large-scale dataset generation at a low cost. It significantly enhances COD
baselines' training and testing phases, emphasizing robustness across diverse
domains. Our newly-generated datasets and source code are available at
https://github.com/drlxj/CamDiff.
</p></li>
</ul>

<h3>Title: Improving Diffusion Models for Scene Text Editing with Dual Encoders. (arXiv:2304.05568v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05568">http://arxiv.org/abs/2304.05568</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05568] Improving Diffusion Models for Scene Text Editing with Dual Encoders](http://arxiv.org/abs/2304.05568) #diffusion</code></li>
<li>Summary: <p>Scene text editing is a challenging task that involves modifying or inserting
specified texts in an image while maintaining its natural and realistic
appearance. Most previous approaches to this task rely on style-transfer models
that crop out text regions and feed them into image transfer models, such as
GANs. However, these methods are limited in their ability to change text style
and are unable to insert texts into images. Recent advances in diffusion models
have shown promise in overcoming these limitations with text-conditional image
editing. However, our empirical analysis reveals that state-of-the-art
diffusion models struggle with rendering correct text and controlling text
style. To address these problems, we propose DIFFSTE to improve pre-trained
diffusion models with a dual encoder design, which includes a character encoder
for better text legibility and an instruction encoder for better style control.
An instruction tuning framework is introduced to train our model to learn the
mapping from the text instruction to the corresponding image with either the
specified style or the style of the surrounding texts in the background. Such a
training method further brings our method the zero-shot generalization ability
to the following three scenarios: generating text with unseen font variation,
e.g., italic and bold, mixing different fonts to construct a new font, and
using more relaxed forms of natural language as the instructions to guide the
generation task. We evaluate our approach on five datasets and demonstrate its
superior performance in terms of text correctness, image naturalness, and style
controllability. Our code is publicly available.
https://github.com/UCSB-NLP-Chang/DiffSTE
</p></li>
</ul>

<h3>Title: InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions. (arXiv:2304.05684v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05684">http://arxiv.org/abs/2304.05684</a></li>
<li>Code URL: <a href="https://github.com/tr3e/intergen">https://github.com/tr3e/intergen</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05684] InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions](http://arxiv.org/abs/2304.05684) #diffusion</code></li>
<li>Summary: <p>We have recently seen tremendous progress in diffusion advances for
generating realistic human motions. Yet, they largely disregard the rich
multi-human interactions. In this paper, we present InterGen, an effective
diffusion-based approach that incorporates human-to-human interactions into the
motion diffusion process, which enables layman users to customize high-quality
two-person interaction motions, with only text guidance. We first contribute a
multimodal dataset, named InterHuman. It consists of about 107M frames for
diverse two-person interactions, with accurate skeletal motions and 16,756
natural language descriptions. For the algorithm side, we carefully tailor the
motion diffusion model to our two-person interaction setting. To handle the
symmetry of human identities during interactions, we propose two cooperative
transformer-based denoisers that explicitly share weights, with a mutual
attention mechanism to further connect the two denoising processes. Then, we
propose a novel representation for motion input in our interaction diffusion
model, which explicitly formulates the global relations between the two
performers in the world frame. We further introduce two novel regularization
terms to encode spatial relations, equipped with a corresponding damping scheme
during the training of our interaction diffusion model. Extensive experiments
validate the effectiveness and generalizability of InterGen. Notably, it can
generate more diverse and compelling two-person motions than previous methods
and enables various downstream applications for human interactions.
</p></li>
</ul>

<h3>Title: Exploring Diffusion Models for Unsupervised Video Anomaly Detection. (arXiv:2304.05841v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05841">http://arxiv.org/abs/2304.05841</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05841] Exploring Diffusion Models for Unsupervised Video Anomaly Detection](http://arxiv.org/abs/2304.05841) #diffusion</code></li>
<li>Summary: <p>This paper investigates the performance of diffusion models for video anomaly
detection (VAD) within the most challenging but also the most operational
scenario in which the data annotations are not used. As being sparse, diverse,
contextual, and often ambiguous, detecting abnormal events precisely is a very
ambitious task. To this end, we rely only on the information-rich
spatio-temporal data, and the reconstruction power of the diffusion models such
that a high reconstruction error is utilized to decide the abnormality.
Experiments performed on two large-scale video anomaly detection datasets
demonstrate the consistent improvement of the proposed method over the
state-of-the-art generative models while in some cases our method achieves
better scores than the more complex models. This is the first study using a
diffusion model and examining its parameters' influence to present guidance for
VAD in surveillance scenarios.
</p></li>
</ul>

<h3>Title: Cancer-Net BCa-S: Breast Cancer Grade Prediction using Volumetric Deep Radiomic Features from Synthetic Correlated Diffusion Imaging. (arXiv:2304.05899v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05899">http://arxiv.org/abs/2304.05899</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05899] Cancer-Net BCa-S: Breast Cancer Grade Prediction using Volumetric Deep Radiomic Features from Synthetic Correlated Diffusion Imaging](http://arxiv.org/abs/2304.05899) #diffusion</code></li>
<li>Summary: <p>The prevalence of breast cancer continues to grow, affecting about 300,000
females in the United States in 2023. However, there are different levels of
severity of breast cancer requiring different treatment strategies, and hence,
grading breast cancer has become a vital component of breast cancer diagnosis
and treatment planning. Specifically, the gold-standard Scarff-Bloom-Richardson
(SBR) grade has been shown to consistently indicate a patient's response to
chemotherapy. Unfortunately, the current method to determine the SBR grade
requires removal of some cancer cells from the patient which can lead to stress
and discomfort along with costly expenses. In this paper, we study the efficacy
of deep learning for breast cancer grading based on synthetic correlated
diffusion (CDI$^s$) imaging, a new magnetic resonance imaging (MRI) modality
and found that it achieves better performance on SBR grade prediction compared
to those learnt using gold-standard imaging modalities. Hence, we introduce
Cancer-Net BCa-S, a volumetric deep radiomics approach for predicting SBR grade
based on volumetric CDI$^s$ data. Given the promising results, this proposed
method to identify the severity of the cancer would allow for better treatment
decisions without the need for a biopsy. Cancer-Net BCa-S has been made
publicly available as part of a global open-source initiative for advancing
machine learning for cancer care.
</p></li>
</ul>

<h3>Title: SpectralDiff: Hyperspectral Image Classification with Spectral-Spatial Diffusion Models. (arXiv:2304.05961v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05961">http://arxiv.org/abs/2304.05961</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05961] SpectralDiff: Hyperspectral Image Classification with Spectral-Spatial Diffusion Models](http://arxiv.org/abs/2304.05961) #diffusion</code></li>
<li>Summary: <p>Hyperspectral image (HSI) classification is an important topic in the field
of remote sensing, and has a wide range of applications in Earth science. HSIs
contain hundreds of continuous bands, which are characterized by high dimension
and high correlation between adjacent bands. The high dimension and redundancy
of HSI data bring great difficulties to HSI classification. In recent years, a
large number of HSI feature extraction and classification methods based on deep
learning have been proposed. However, their ability to model the global
relationships among samples in both spatial and spectral domains is still
limited. In order to solve this problem, an HSI classification method with
spectral-spatial diffusion models is proposed. The proposed method realizes the
reconstruction of spectral-spatial distribution of the training samples with
the forward and reverse spectral-spatial diffusion process, thus modeling the
global spatial-spectral relationship between samples. Then, we use the
spectral-spatial denoising network of the reverse process to extract the
unsupervised diffusion features. Features extracted by the spectral-spatial
diffusion models can achieve cross-sample perception from the reconstructed
distribution of the training samples, thus obtaining better classification
performance. Experiments on three public HSI datasets show that the proposed
method can achieve better performance than the state-of-the-art methods. The
source code and the pre-trained spectral-spatial diffusion model will be
publicly available at https://github.com/chenning0115/SpectralDiff.
</p></li>
</ul>

<h3>Title: Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views. (arXiv:2304.06024v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.06024">http://arxiv.org/abs/2304.06024</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.06024] Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views](http://arxiv.org/abs/2304.06024) #diffusion</code></li>
<li>Summary: <p>Automatic perception of human behaviors during social interactions is crucial
for AR/VR applications, and an essential component is estimation of plausible
3D human pose and shape of our social partners from the egocentric view. One of
the biggest challenges of this task is severe body truncation due to close
social distances in egocentric scenarios, which brings large pose ambiguities
for unseen body parts. To tackle this challenge, we propose a novel
scene-conditioned diffusion method to model the body pose distribution.
Conditioned on the 3D scene geometry, the diffusion model generates bodies in
plausible human-scene interactions, with the sampling guided by a physics-based
collision score to further resolve human-scene inter-penetrations. The
classifier-free training enables flexible sampling with different conditions
and enhanced diversity. A visibility-aware graph convolution model guided by
per-joint visibility serves as the diffusion denoiser to incorporate
inter-joint dependencies and per-body-part control. Extensive evaluations show
that our method generates bodies in plausible interactions with 3D scenes,
achieving both superior accuracy for visible joints and diversity for invisible
body parts. The code will be available at
https://sanweiliti.github.io/egohmr/egohmr.html.
</p></li>
</ul>

<h3>Title: DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion. (arXiv:2304.06025v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.06025">http://arxiv.org/abs/2304.06025</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.06025] DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion](http://arxiv.org/abs/2304.06025) #diffusion</code></li>
<li>Summary: <p>We present DreamPose, a diffusion-based method for generating animated
fashion videos from still images. Given an image and a sequence of human body
poses, our method synthesizes a video containing both human and fabric motion.
To achieve this, we transform a pretrained text-to-image model (Stable
Diffusion) into a pose-and-image guided video synthesis model, using a novel
finetuning strategy, a set of architectural changes to support the added
conditioning signals, and techniques to encourage temporal consistency. We
fine-tune on a collection of fashion videos from the UBC Fashion dataset. We
evaluate our method on a variety of clothing styles and poses, and demonstrate
that our method produces state-of-the-art results on fashion video animation.
Video results are available on our project page.
</p></li>
</ul>

<h3>Title: Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA. (arXiv:2304.06027v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.06027">http://arxiv.org/abs/2304.06027</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.06027] Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA](http://arxiv.org/abs/2304.06027) #diffusion</code></li>
<li>Summary: <p>Recent works demonstrate a remarkable ability to customize text-to-image
diffusion models while only providing a few example images. What happens if you
try to customize such models using multiple, fine-grained concepts in a
sequential (i.e., continual) manner? In our work, we show that recent
state-of-the-art customization of text-to-image models suffer from catastrophic
forgetting when new concepts arrive sequentially. Specifically, when adding a
new concept, the ability to generate high quality images of past, similar
concepts degrade. To circumvent this forgetting, we propose a new method,
C-LoRA, composed of a continually self-regularized low-rank adaptation in cross
attention layers of the popular Stable Diffusion model. Furthermore, we use
customization prompts which do not include the word of the customized object
(i.e., "person" for a human face dataset) and are initialized as completely
random embeddings. Importantly, our method induces only marginal additional
parameter costs and requires no storage of user data for replay. We show that
C-LoRA not only outperforms several baselines for our proposed setting of
text-to-image continual customization, which we refer to as Continual
Diffusion, but that we achieve a new state-of-the-art in the well-established
rehearsal-free continual learning setting for image classification. The high
achieving performance of C-LoRA in two separate domains positions it as a
compelling solution for a wide range of applications, and we believe it has
significant potential for practical impact.
</p></li>
</ul>

<h3>Title: Diffusion models with location-scale noise. (arXiv:2304.05907v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.05907">http://arxiv.org/abs/2304.05907</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.05907] Diffusion models with location-scale noise](http://arxiv.org/abs/2304.05907) #diffusion</code></li>
<li>Summary: <p>Diffusion Models (DMs) are powerful generative models that add Gaussian noise
to the data and learn to remove it. We wanted to determine which noise
distribution (Gaussian or non-Gaussian) led to better generated data in DMs.
Since DMs do not work by design with non-Gaussian noise, we built a framework
that allows reversing a diffusion process with non-Gaussian location-scale
noise. We use that framework to show that the Gaussian distribution performs
the best over a wide range of other distributions (Laplace, Uniform, t,
Generalized-Gaussian).
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
