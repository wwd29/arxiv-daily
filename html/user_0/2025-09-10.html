<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-10</h1>
<h3>Title: Individualized and Interpretable Sleep Forecasting via a Two-Stage Adaptive Spatial-Temporal Model</h3>
<ul>
<li><strong>Authors: </strong>Xueyi Wang, Elisabeth Wilhelm</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06974">https://arxiv.org/abs/2509.06974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06974">https://arxiv.org/pdf/2509.06974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06974]] Individualized and Interpretable Sleep Forecasting via a Two-Stage Adaptive Spatial-Temporal Model(https://arxiv.org/abs/2509.06974)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>Sleep quality significantly impacts well-being. Therefore, healthcare providers and individuals need accessible and reliable forecasting tools for preventive interventions. This paper introduces an interpretable, individualized two-stage adaptive spatial-temporal model for predicting sleep quality scores. Our proposed framework combines multi-scale convolutional layers to model spatial interactions across multiple input variables, recurrent layers and attention mechanisms to capture long-term temporal dependencies, and a two-stage domain adaptation strategy to enhance generalization. The first adaptation stage is applied during training to mitigate overfitting on the training set. In the second stage, a source-free test-time adaptation mechanism is employed to adapt the model to new users without requiring labels. We conducted various experiments with five input window sizes (3, 5, 7, 9, and 11 days) and five prediction window sizes (1, 3, 5, 7, and 9 days). Our model consistently outperformed time series forecasting baseline approaches, including Long Short-Term Memory (LSTM), Informer, PatchTST, and TimesNet. The best performance was achieved with a three-day input window and a one-day prediction window, yielding a root mean square error (RMSE) of 0.216. Furthermore, the model demonstrated good predictive performance even for longer forecasting horizons (e.g, with a 0.257 RMSE for a three-day prediction window), highlighting its practical utility for real-world applications. We also conducted an explainability analysis to examine how different features influence sleep quality. These findings proved that the proposed framework offers a robust, adaptive, and explainable solution for personalized sleep forecasting using sparse data from commercial wearable devices.</li>
</ul>

<h3>Title: A Knowledge-Guided Cross-Modal Feature Fusion Model for Local Traffic Demand Prediction</h3>
<ul>
<li><strong>Authors: </strong>Lingyu Zhang, Pengfei Xu, Guobin Wu, Jian Liang, Ruiyang Dong, Yunhai Wang, Xuan Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06976">https://arxiv.org/abs/2509.06976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06976">https://arxiv.org/pdf/2509.06976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06976]] A Knowledge-Guided Cross-Modal Feature Fusion Model for Local Traffic Demand Prediction(https://arxiv.org/abs/2509.06976)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Traffic demand prediction plays a critical role in intelligent transportation systems. Existing traffic prediction models primarily rely on temporal traffic data, with limited efforts incorporating human knowledge and experience for urban traffic demand forecasting. However, in real-world scenarios, traffic knowledge and experience derived from human daily life significantly influence precise traffic prediction. Such knowledge and experiences can guide the model in uncovering latent patterns within traffic data, thereby enhancing the accuracy and robustness of predictions. To this end, this paper proposes integrating structured temporal traffic data with textual data representing human knowledge and experience, resulting in a novel knowledge-guided cross-modal feature representation learning (KGCM) model for traffic demand prediction. Based on regional transportation characteristics, we construct a prior knowledge dataset using a large language model combined with manual authoring and revision, covering both regional and global knowledge and experiences. The KGCM model then learns multimodal data features through designed local and global adaptive graph networks, as well as a cross-modal feature fusion mechanism. A proposed reasoning-based dynamic update strategy enables dynamic optimization of the graph model's parameters, achieving optimal performance. Experiments on multiple traffic datasets demonstrate that our model accurately predicts future traffic demand and outperforms existing state-of-the-art (SOTA) models.</li>
</ul>

<h3>Title: RLFactory: A Plug-and-Play Reinforcement Learning Post-Training Framework for LLM Multi-Turn Tool-Use</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Chai, Guojun Yin, Zekun Xu, Chuhuai Yue, Yi Jia, Siyu Xia, Xiaohan Wang, Jiwen Jiang, Xiaoguang Li, Chengqi Dong, Hang He, Wei Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06980">https://arxiv.org/abs/2509.06980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06980">https://arxiv.org/pdf/2509.06980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06980]] RLFactory: A Plug-and-Play Reinforcement Learning Post-Training Framework for LLM Multi-Turn Tool-Use(https://arxiv.org/abs/2509.06980)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models excel at basic reasoning but struggle with tasks that require interaction with external tools. We present RLFactory, a plug-and-play reinforcement learning post-training framework for multi-round tool use. RLFactory tackles (i) tool-call stability and adaptability amid tool heterogeneity and interface issues via an asyncio-based asynchronous caller and a decoupled tool/training architecture, and (ii) diverse evaluation needs via a reward layer supporting rule-based, model-judgment, and tool-verification signals. It reconstructs the MDP by introducing observation markers from tool feedback, closing the loop among model, tools, and environment, and implements a generate-parse-invoke-update workflow for dynamic policy optimization. On Search-R1 with Qwen3-4B, RLFactory achieves a 0.486 test score on the Natural Questions (NQ) dataset, surpassing larger models trained with similar techniques (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training throughput by 6.8x. RLFactory provides a low-barrier, highly adaptable framework for strengthening multi-round tool use of LLMs in real-world scenarios. Code: this https URL.</li>
</ul>

<h3>Title: CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention</h3>
<ul>
<li><strong>Authors: </strong>Xiaomeng Hu, Fei Huang, Chenhan Yuan, Junyang Lin, Tsung-Yi Ho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06982">https://arxiv.org/abs/2509.06982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06982">https://arxiv.org/pdf/2509.06982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06982]] CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention(https://arxiv.org/abs/2509.06982)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly deployed in real-world applications, ensuring the safety of their outputs during decoding has become a critical challenge. However, existing decoding-time interventions, such as Contrastive Decoding, often force a severe trade-off between safety and response quality. In this work, we propose CARE, a novel framework for decoding-time safety alignment that integrates three key components: (1) a guard model for real-time safety monitoring, enabling detection of potentially unsafe content; (2) a rollback mechanism with a token buffer to correct unsafe outputs efficiently at an earlier stage without disrupting the user experience; and (3) a novel introspection-based intervention strategy, where the model generates self-reflective critiques of its previous outputs and incorporates these reflections into the context to guide subsequent decoding steps. The framework achieves a superior safety-quality trade-off by using its guard model for precise interventions, its rollback mechanism for timely corrections, and our novel introspection method for effective self-correction. Experimental results demonstrate that our framework achieves a superior balance of safety, quality, and efficiency, attaining a low harmful response rate and minimal disruption to the user experience while maintaining high response quality.</li>
</ul>

<h3>Title: FediLoRA: Heterogeneous LoRA for Federated Multimodal Fine-tuning under Missing Modalities</h3>
<ul>
<li><strong>Authors: </strong>Lishan Yang, Nam Kha Nguygen, Po Hu, Wei Emma Zhang, Yanjun Shu, Mong Yuan Sim, Weitong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06984">https://arxiv.org/abs/2509.06984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06984">https://arxiv.org/pdf/2509.06984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06984]] FediLoRA: Heterogeneous LoRA for Federated Multimodal Fine-tuning under Missing Modalities(https://arxiv.org/abs/2509.06984)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Foundation models have demonstrated remarkable performance across a wide range of tasks, yet their large parameter sizes pose challenges for practical deployment, especially in decentralized environments. Parameter-efficient fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), reduces local computing and memory overhead, making it attractive for federated learning. However, existing federated LoRA methods typically assume uniform rank configurations and unimodal inputs, overlooking two key real-world challenges: (1) heterogeneous client resources have different LoRA ranks, and (2) multimodal data settings with potentially missing modalities. In this work, we propose FediLoRA, a simple yet effective framework for federated multimodal fine-tuning under heterogeneous LoRA ranks and missing modalities. FediLoRA introduces a dimension-wise aggregation strategy that reweights LoRA updates without information dilution during aggregation. It also includes a lightweight layer-wise model editing method that selectively incorporates global parameters to repair local components which improves both client and global model performances. Experimental results on three multimodal benchmark datasets demonstrate that FediLoRA achieves superior performance over competitive baselines in both global and personalized settings, particularly in the presence of modality incompleteness.</li>
</ul>

<h3>Title: CellPainTR: Generalizable Representation Learning for Cross-Dataset Cell Painting Analysis</h3>
<ul>
<li><strong>Authors: </strong>Cedric Caruzzo, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06986">https://arxiv.org/abs/2509.06986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06986">https://arxiv.org/pdf/2509.06986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06986]] CellPainTR: Generalizable Representation Learning for Cross-Dataset Cell Painting Analysis(https://arxiv.org/abs/2509.06986)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Large-scale biological discovery requires integrating massive, heterogeneous datasets like those from the JUMP Cell Painting consortium, but technical batch effects and a lack of generalizable models remain critical roadblocks. To address this, we introduce CellPainTR, a Transformer-based architecture designed to learn foundational representations of cellular morphology that are robust to batch effects. Unlike traditional methods that require retraining on new data, CellPainTR's design, featuring source-specific context tokens, allows for effective out-of-distribution (OOD) generalization to entirely unseen datasets without fine-tuning. We validate CellPainTR on the large-scale JUMP dataset, where it outperforms established methods like ComBat and Harmony in both batch integration and biological signal preservation. Critically, we demonstrate its robustness through a challenging OOD task on the unseen Bray et al. dataset, where it maintains high performance despite significant domain and feature shifts. Our work represents a significant step towards creating truly foundational models for image-based profiling, enabling more reliable and scalable cross-study biological analysis.</li>
</ul>

<h3>Title: FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection</h3>
<ul>
<li><strong>Authors: </strong>Alexey Zhukov (UB, CNRS, Bordeaux INP, Inria, LaBRI), Jenny Benois-Pineau (UB, CNRS, Bordeaux INP, Inria, LaBRI), Amira Youssef (SNCF Réseau), Akka Zemmari (UB, CNRS, Bordeaux INP, Inria, LaBRI), Mohamed Mosbah (UB, CNRS, Bordeaux INP, Inria, LaBRI), Virginie Taillandier</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06987">https://arxiv.org/abs/2509.06987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06987">https://arxiv.org/pdf/2509.06987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06987]] FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection(https://arxiv.org/abs/2509.06987)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multimodal fusion is a multimedia technique that has become popular in the wide range of tasks where image information is accompanied by a signal/audio. The latter may not convey highly semantic information, such as speech or music, but some measures such as audio signal recorded by mics in the goal to detect rail structure elements or defects. While classical detection approaches such as You Only Look Once (YOLO) family detectors can be efficiently deployed for defect detection on the image modality, the single modality approaches remain limited. They yield an overdetection in case of the appearance similar to normal structural elements. The paper proposes a new multimodal fusion architecture built on the basis of domain rules with YOLO and Vision transformer backbones. It integrates YOLOv8n for rapid object detection with a Vision Transformer (ViT) to combine feature maps extracted from multiple layers (7, 16, and 19) and synthesised audio representations for two defect classes: rail Rupture and Surface defect. Fusion is performed between audio and image. Experimental evaluation on a real-world railway dataset demonstrates that our multimodal fusion improves precision and overall accuracy by 0.2 points compared to the vision-only approach. Student's unpaired t-test also confirms statistical significance of differences in the mean accuracy.</li>
</ul>

<h3>Title: Frustratingly Easy Feature Reconstruction for Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Yingsheng Wang, Shuo Lu, Jian Liang, Aihua Zheng, Ran He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06988">https://arxiv.org/abs/2509.06988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06988">https://arxiv.org/pdf/2509.06988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06988]] Frustratingly Easy Feature Reconstruction for Out-of-Distribution Detection(https://arxiv.org/abs/2509.06988)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) detection helps models identify data outside the training categories, crucial for security applications. While feature-based post-hoc methods address this by evaluating data differences in the feature space without changing network parameters, they often require access to training data, which may not be suitable for some data privacy scenarios. This may not be suitable in scenarios where data privacy protection is a concern. In this paper, we propose a simple yet effective post-hoc method, termed Classifier-based Feature Reconstruction (ClaFR), from the perspective of subspace projection. It first performs an orthogonal decomposition of the classifier's weights to extract the class-known subspace, then maps the original data features into this subspace to obtain new data representations. Subsequently, the OOD score is determined by calculating the feature reconstruction error of the data within the subspace. Compared to existing OOD detection algorithms, our method does not require access to training data while achieving leading performance on multiple OOD benchmarks. Our code is released at this https URL.</li>
</ul>

<h3>Title: FedAPT: Federated Adversarial Prompt Tuning for Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kun Zhai, Siheng Chen, Xingjun Ma, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06992">https://arxiv.org/abs/2509.06992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06992">https://arxiv.org/pdf/2509.06992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06992]] FedAPT: Federated Adversarial Prompt Tuning for Vision-Language Models(https://arxiv.org/abs/2509.06992)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Prompt Tuning (FPT) is an efficient method for cross-client collaborative fine-tuning of large Vision-Language Models (VLMs). However, models tuned using FPT are vulnerable to adversarial attacks, leading to misclassification in downstream tasks. In this work, we introduce Federated Adversarial Prompt Tuning (\textbf{FedAPT}), a novel method designed to enhance the adversarial robustness of FPT. We identify a key issue in FedAPT under non-independent and identically distributed (non-IID) settings: a \textit{class information gap} between clients and the global model. Clients rely solely on limited local label information to generate adversarial samples for training, while the global model must defend against adversarial attacks from global labels. To address this issue, we propose a \textbf{class-aware prompt generator} that generates visual prompts from text prompts. This generator is guided by a \emph{Global Label Embedding} (serving as a ``beacon") which encodes cross-client label information to create more globally-aligned visual prompts. Additionally, we propose a \textbf{cross-layer generator sharing} strategy to enhance prompt coupling across different layers of the model, further boosting adversarial robustness. Extensive experiments on multiple image classification datasets demonstrate the superiority of FedAPT in improving adversarial robustness, outperforming existing methods by a large margin. FedAPT also exhibits exceptional generalization in cross-domain and cross-dataset scenarios, indicating its effectiveness in real-world applications.</li>
</ul>

<h3>Title: The Protocol Genome A Self Supervised Learning Framework from DICOM Headers</h3>
<ul>
<li><strong>Authors: </strong>Jimmy Joseph</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06995">https://arxiv.org/abs/2509.06995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06995">https://arxiv.org/pdf/2509.06995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06995]] The Protocol Genome A Self Supervised Learning Framework from DICOM Headers(https://arxiv.org/abs/2509.06995)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce the Protocol Genome, a self-supervised learning system that learns correlations from DICOM headers and achieves AUROC 0.901 (vs 0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation. Our method also improves calibration and robustness across modalities (CT, MRI, CXR) and vendors. Clinical imaging is funneled through PACS/DICOM, where procedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slice thickness) have consequences for contrast, noise, and artifact. These latent confounders impede the generalization of image-only networks across sites. We consider structured DICOM headers as a label and learn protocol-aware but clinically robust image representations. Protocol Genome obtains tokenized embeddings of de-identified header fields and models them along with image features using: (1) protocol-image contrastive learning, (2) masked protocol prediction, and (3) protocol-protocol translation. With 1.26M studies (7 health systems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CT triage for PE, (B) brain MRI glioma grading, and (C) chest radiograph cardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as well as ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041: cardiomegaly) is associated with higher external AUROC; 25-37% calibration improvements are obtained (p < 0.01, DeLong tests). While the gains may be task-dependent, they are preserved with 10-20% of labeled data. From a clinical point of view, the technique reduces false positives at protocol borders and is applicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish a model card and deployment guide, complete with both de-identification and bias audits.</li>
</ul>

<h3>Title: Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems</h3>
<ul>
<li><strong>Authors: </strong>Jie Zhang, Ting Xu, Gelei Deng, Runyi Hu, Han Qiu, Tianwei Zhang, Qing Guo, Ivor Tsang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06996">https://arxiv.org/abs/2509.06996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06996">https://arxiv.org/pdf/2509.06996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06996]] Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems(https://arxiv.org/abs/2509.06996)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, segmentation</a></li>
<li><strong>Abstract: </strong>Writing is a universal cultural technology that reuses vision for symbolic communication. Humans display striking resilience: we readily recognize words even when characters are fragmented, fused, or partially occluded. This paper investigates whether advanced vision language models (VLMs) share this resilience. We construct two psychophysics inspired benchmarks across distinct writing systems, Chinese logographs and English alphabetic words, by splicing, recombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli for models while remaining legible to humans. Despite strong performance on clean text, contemporary VLMs show a severe drop under these perturbations, frequently producing unrelated or incoherent outputs. The pattern suggests a structural limitation: models heavily leverage generic visual invariances but under rely on compositional priors needed for robust literacy. We release stimuli generation code, prompts, and evaluation protocols to facilitate transparent replication and follow up work. Our findings motivate architectures and training strategies that encode symbol segmentation, composition, and binding across scripts, and they delineate concrete challenges for deploying multimodal systems in education, accessibility, cultural heritage, and security.</li>
</ul>

<h3>Title: K-Syn: K-space Data Synthesis in Ultra Low-data Regimes</h3>
<ul>
<li><strong>Authors: </strong>Guan Yu, Zhang Jianhua, Liang Dong, Liu Qiegen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06997">https://arxiv.org/abs/2509.06997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06997">https://arxiv.org/pdf/2509.06997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06997]] K-Syn: K-space Data Synthesis in Ultra Low-data Regimes(https://arxiv.org/abs/2509.06997)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Owing to the inherently dynamic and complex characteristics of cardiac magnetic resonance (CMR) imaging, high-quality and diverse k-space data are rarely available in practice, which in turn hampers robust reconstruction of dynamic cardiac MRI. To address this challenge, we perform feature-level learning directly in the frequency domain and employ a temporal-fusion strategy as the generative guidance to synthesize k-space data. Specifically, leveraging the global representation capacity of the Fourier transform, the frequency domain can be considered a natural global feature space. Therefore, unlike traditional methods that use pixel-level convolution for feature learning and modeling in the image domain, this letter focuses on feature-level modeling in the frequency domain, enabling stable and rich generation even with ultra low-data regimes. Moreover, leveraging the advantages of feature-level modeling in the frequency domain, we integrate k-space data across time frames with multiple fusion strategies to steer and further optimize the generative trajectory. Experimental results demonstrate that the proposed method possesses strong generative ability in low-data regimes, indicating practical potential to alleviate data scarcity in dynamic MRI reconstruction.</li>
</ul>

<h3>Title: Not All Splits Are Equal: Rethinking Attribute Generalization Across Unrelated Categories</h3>
<ul>
<li><strong>Authors: </strong>Liviu Nicolae Fircă, Antonio Bărbălau, Dan Oneata, Elena Burceanu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06998">https://arxiv.org/abs/2509.06998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06998">https://arxiv.org/pdf/2509.06998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06998]] Not All Splits Are Equal: Rethinking Attribute Generalization Across Unrelated Categories(https://arxiv.org/abs/2509.06998)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Can models generalize attribute knowledge across semantically and perceptually dissimilar categories? While prior work has addressed attribute prediction within narrow taxonomic or visually similar domains, it remains unclear whether current models can abstract attributes and apply them to conceptually distant categories. This work presents the first explicit evaluation for the robustness of the attribute prediction task under such conditions, testing whether models can correctly infer shared attributes between unrelated object types: e.g., identifying that the attribute "has four legs" is common to both "dogs" and "chairs". To enable this evaluation, we introduce train-test split strategies that progressively reduce correlation between training and test sets, based on: LLM-driven semantic grouping, embedding similarity thresholding, embedding-based clustering, and supercategory-based partitioning using ground-truth labels. Results show a sharp drop in performance as the correlation between training and test categories decreases, indicating strong sensitivity to split design. Among the evaluated methods, clustering yields the most effective trade-off, reducing hidden correlations while preserving learnability. These findings offer new insights into the limitations of current representations and inform future benchmark construction for attribute reasoning.</li>
</ul>

<h3>Title: Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ahmed R. Sadik, Mariusz Bujny</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07010">https://arxiv.org/abs/2509.07010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07010">https://arxiv.org/pdf/2509.07010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07010]] Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models(https://arxiv.org/abs/2509.07010)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models are increasingly capable of interpreting multimodal inputs to generate complex 3D shapes, yet robust methods to evaluate geometric and structural fidelity remain underdeveloped. This paper introduces a human in the loop framework for the quantitative evaluation of LLM generated 3D models, supporting applications such as democratization of CAD design, reverse engineering of legacy designs, and rapid prototyping. We propose a comprehensive suite of similarity and complexity metrics, including volumetric accuracy, surface alignment, dimensional fidelity, and topological intricacy, to benchmark generated models against ground truth CAD references. Using an L bracket component as a case study, we systematically compare LLM performance across four input modalities: 2D orthographic views, isometric sketches, geometric structure trees, and code based correction prompts. Our findings demonstrate improved generation fidelity with increased semantic richness, with code level prompts achieving perfect reconstruction across all metrics. A key contribution of this work is demonstrating that our proposed quantitative evaluation approach enables significantly faster convergence toward the ground truth, especially compared to traditional qualitative methods based solely on visual inspection and human intuition. This work not only advances the understanding of AI assisted shape synthesis but also provides a scalable methodology to validate and refine generative models for diverse CAD applications.</li>
</ul>

<h3>Title: Random Forest Stratified K-Fold Cross Validation on SYN DoS Attack SD-IoV</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Arif Hakimi Zamrai, Kamaludin Mohd Yusof</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07016">https://arxiv.org/abs/2509.07016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07016">https://arxiv.org/pdf/2509.07016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07016]] Random Forest Stratified K-Fold Cross Validation on SYN DoS Attack SD-IoV(https://arxiv.org/abs/2509.07016)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>In response to the prevalent concern of TCP SYN flood attacks within the context of Software-Defined Internet of Vehicles (SD-IoV), this study addresses the significant challenge of network security in rapidly evolving vehicular communication systems. This research focuses on optimizing a Random Forest Classifier model to achieve maximum accuracy and minimal detection time, thereby enhancing vehicular network security. The methodology involves preprocessing a dataset containing SYN attack instances, employing feature scaling and label encoding techniques, and applying Stratified K-Fold cross-validation to target key metrics such as accuracy, precision, recall, and F1-score. This research achieved an average value of 0.999998 for all metrics with a SYN DoS attack detection time of 0.24 seconds. Results show that the fine-tuned Random Forest model, configured with 20 estimators and a depth of 10, effectively differentiates between normal and malicious traffic with high accuracy and minimal detection time, which is crucial for SD-IoV networks. This approach marks a significant advancement and introduces a state-of-the-art algorithm in detecting SYN flood attacks, combining high accuracy with minimal detection time. It contributes to vehicular network security by providing a robust solution against TCP SYN flood attacks while maintaining network efficiency and reliability.</li>
</ul>

<h3>Title: 1 bit is all we need: binary normalized neural networks</h3>
<ul>
<li><strong>Authors: </strong>Eduardo Lobo Lustoda Cabral, Paulo Pirozelli, Larissa Driemeier</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07025">https://arxiv.org/abs/2509.07025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07025">https://arxiv.org/pdf/2509.07025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07025]] 1 bit is all we need: binary normalized neural networks(https://arxiv.org/abs/2509.07025)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The increasing size of large neural network models, specifically language models and foundational image models, poses deployment challenges, prompting efforts to reduce memory requirements and enhance computational efficiency. These efforts are critical to ensure practical deployment and effective utilization of these models across various applications. In this work, a novel type of neural network layers and models is developed that uses only single-bit parameters. In this novel type of models all parameters of all layers, including kernel weights and biases, only have values equal to zero or one. This novel type of models uses layers named as binary normalized layer. These binary normalized layers can be of any type, such as fully connected, convolutional, attention, etc., and they consist of slight variations of the corresponding conventional layers. To show the effectiveness of the binary normalized layers, two different models are configured to solve a multiclass image classification problem and a language decoder to predict the next token of a sequence. The model to solve the image classification has convolutional and fully connected layers, and the language model is composed of transformer blocks with multi-head attention. The results show that models with binary normalized layers present almost the same results obtained by equivalent models with real 32-bit parameters. The binary normalized layers allow to develop models that use 32 times less memory than current models and have equivalent performance. Besides, the binary normalized layers can be easily implemented on current computers using 1-bit arrays, and do not require the development of dedicated electronic hardware. This novel type of layers opens a new era for large neural network models with reduced memory requirements that can be deployed using simple and cheap hardware, such as mobile devices or only cpus.</li>
</ul>

<h3>Title: Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Jisung Hwang, Jaihoon Kim, Minhyuk Sung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07027">https://arxiv.org/abs/2509.07027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07027">https://arxiv.org/pdf/2509.07027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07027]] Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models(https://arxiv.org/abs/2509.07027)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose a novel regularization loss that enforces standard Gaussianity, encouraging samples to align with a standard Gaussian distribution. This facilitates a range of downstream tasks involving optimization in the latent space of text-to-image models. We treat elements of a high-dimensional sample as one-dimensional standard Gaussian variables and define a composite loss that combines moment-based regularization in the spatial domain with power spectrum-based regularization in the spectral domain. Since the expected values of moments and power spectrum distributions are analytically known, the loss promotes conformity to these properties. To ensure permutation invariance, the losses are applied to randomly permuted inputs. Notably, existing Gaussianity-based regularizations fall within our unified framework: some correspond to moment losses of specific orders, while the previous covariance-matching loss is equivalent to our spectral loss but incurs higher time complexity due to its spatial-domain computation. We showcase the application of our regularization in generative modeling for test-time reward alignment with a text-to-image model, specifically to enhance aesthetics and text alignment. Our regularization outperforms previous Gaussianity regularization, effectively prevents reward hacking and accelerates convergence.</li>
</ul>

<h3>Title: Methodological Insights into Structural Causal Modelling and Uncertainty-Aware Forecasting for Economic Indicators</h3>
<ul>
<li><strong>Authors: </strong>Federico Cerutti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07036">https://arxiv.org/abs/2509.07036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07036">https://arxiv.org/pdf/2509.07036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07036]] Methodological Insights into Structural Causal Modelling and Uncertainty-Aware Forecasting for Economic Indicators(https://arxiv.org/abs/2509.07036)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a methodological approach to financial time series analysis by combining causal discovery and uncertainty-aware forecasting. As a case study, we focus on four key U.S. macroeconomic indicators -- GDP, economic growth, inflation, and unemployment -- and we apply the LPCMCI framework with Gaussian Process Distance Correlation (GPDC) to uncover dynamic causal relationships in quarterly data from 1970 to 2021. Our results reveal a robust unidirectional causal link from economic growth to GDP and highlight the limited connectivity of inflation, suggesting the influence of latent factors. Unemployment exhibits strong autoregressive dependence, motivating its use as a case study for probabilistic forecasting. Leveraging the Chronos framework, a large language model trained for time series, we perform zero-shot predictions on unemployment. This approach delivers accurate forecasts one and two quarters ahead, without requiring task-specific training. Crucially, the model's uncertainty-aware predictions yield 90\% confidence intervals, enabling effective anomaly detection through statistically principled deviation analysis. This study demonstrates the value of combining causal structure learning with probabilistic language models to inform economic policy and enhance forecasting robustness.</li>
</ul>

<h3>Title: Benchmarking Vision Transformers and CNNs for Thermal Photovoltaic Fault Detection with Explainable AI Validation</h3>
<ul>
<li><strong>Authors: </strong>Serra Aksoy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07039">https://arxiv.org/abs/2509.07039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07039">https://arxiv.org/pdf/2509.07039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07039]] Benchmarking Vision Transformers and CNNs for Thermal Photovoltaic Fault Detection with Explainable AI Validation(https://arxiv.org/abs/2509.07039)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Artificial intelligence deployment for automated photovoltaic (PV) monitoring faces interpretability barriers that limit adoption in energy infrastructure applications. While deep learning achieves high accuracy in thermal fault detection, validation that model decisions align with thermal physics principles remains lacking, creating deployment hesitancy where understanding model reasoning is critical. This study provides a systematic comparison of convolutional neural networks (ResNet-18, EfficientNet-B0) and vision transformers (ViT-Tiny, Swin-Tiny) for thermal PV fault detection, using XRAI saliency analysis to assess alignment with thermal physics principles. This represents the first systematic comparison of CNNs and vision transformers for thermal PV fault detection with physics-validated interpretability. Evaluation on 20,000 infrared images spanning normal operation and 11 fault categories shows that Swin Transformer achieves the highest performance (94% binary accuracy; 73% multiclass accuracy) compared to CNN approaches. XRAI analysis reveals that models learn physically meaningful features, such as localized hotspots for cell defects, linear thermal paths for diode failures, and thermal boundaries for vegetation shading, consistent with expected thermal signatures. However, performance varies significantly across fault types: electrical faults achieve strong detection (F1-scores >0.90) while environmental factors like soiling remain challenging (F1-scores 0.20-0.33), indicating limitations imposed by thermal imaging resolution. The thermal physics-guided interpretability approach provides methodology for validating AI decision-making in energy monitoring applications, addressing deployment barriers in renewable energy infrastructure.</li>
</ul>

<h3>Title: SAM$^{*}$: Task-Adaptive SAM with Physics-Guided Rewards</h3>
<ul>
<li><strong>Authors: </strong>Kamyar Barakati, Utkarsh Pratiush, Sheryl L. Sanchez, Aditya Raghavan, Delia J. Milliron, Mahshid Ahmadi, Philip D. Rack, Sergei V. Kalinin</a></li>
<li><strong>Subjects: </strong>cs.CV, cond-mat.mtrl-sci, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07047">https://arxiv.org/abs/2509.07047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07047">https://arxiv.org/pdf/2509.07047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07047]] SAM$^{*}$: Task-Adaptive SAM with Physics-Guided Rewards(https://arxiv.org/abs/2509.07047)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Image segmentation is a critical task in microscopy, essential for accurately analyzing and interpreting complex visual data. This task can be performed using custom models trained on domain-specific datasets, transfer learning from pre-trained models, or foundational models that offer broad applicability. However, foundational models often present a considerable number of non-transparent tuning parameters that require extensive manual optimization, limiting their usability for real-time streaming data analysis. Here, we introduce a reward function-based optimization to fine-tune foundational models and illustrate this approach for SAM (Segment Anything Model) framework by Meta. The reward functions can be constructed to represent the physics of the imaged system, including particle size distributions, geometries, and other criteria. By integrating a reward-driven optimization framework, we enhance SAM's adaptability and performance, leading to an optimized variant, SAM$^{*}$, that better aligns with the requirements of diverse segmentation tasks and particularly allows for real-time streaming data segmentation. We demonstrate the effectiveness of this approach in microscopy imaging, where precise segmentation is crucial for analyzing cellular structures, material interfaces, and nanoscale features.</li>
</ul>

<h3>Title: Automated Evaluation of Gender Bias Across 13 Large Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Juan Manuel Contreras</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07050">https://arxiv.org/abs/2509.07050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07050">https://arxiv.org/pdf/2509.07050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07050]] Automated Evaluation of Gender Bias Across 13 Large Multimodal Models(https://arxiv.org/abs/2509.07050)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Large multimodal models (LMMs) have revolutionized text-to-image generation, but they risk perpetuating the harmful social biases in their training data. Prior work has identified gender bias in these models, but methodological limitations prevented large-scale, comparable, cross-model analysis. To address this gap, we introduce the Aymara Image Fairness Evaluation, a benchmark for assessing social bias in AI-generated images. We test 13 commercially available LMMs using 75 procedurally-generated, gender-neutral prompts to generate people in stereotypically-male, stereotypically-female, and non-stereotypical professions. We then use a validated LLM-as-a-judge system to score the 965 resulting images for gender representation. Our results reveal (p < .001 for all): 1) LMMs systematically not only reproduce but actually amplify occupational gender stereotypes relative to real-world labor data, generating men in 93.0% of images for male-stereotyped professions but only 22.5% for female-stereotyped professions; 2) Models exhibit a strong default-male bias, generating men in 68.3% of the time for non-stereotyped professions; and 3) The extent of bias varies dramatically across models, with overall male representation ranging from 46.7% to 73.3%. Notably, the top-performing model de-amplified gender stereotypes and approached gender parity, achieving the highest fairness scores. This variation suggests high bias is not an inevitable outcome but a consequence of design choices. Our work provides the most comprehensive cross-model benchmark of gender bias to date and underscores the necessity of standardized, automated evaluation tools for promoting accountability and fairness in AI development.</li>
</ul>

<h3>Title: The Signalgate Case is Waiving a Red Flag to All Organizational and Behavioral Cybersecurity Leaders, Practitioners, and Researchers: Are We Receiving the Signal Amidst the Noise?</h3>
<ul>
<li><strong>Authors: </strong>Paul Benjamin Lowry, Gregory D. Moody, Robert Willison, Clay Posey</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07053">https://arxiv.org/abs/2509.07053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07053">https://arxiv.org/pdf/2509.07053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07053]] The Signalgate Case is Waiving a Red Flag to All Organizational and Behavioral Cybersecurity Leaders, Practitioners, and Researchers: Are We Receiving the Signal Amidst the Noise?(https://arxiv.org/abs/2509.07053)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense</a></li>
<li><strong>Abstract: </strong>The Signalgate incident of March 2025, wherein senior US national security officials inadvertently disclosed sensitive military operational details via the encrypted messaging platform Signal, highlights critical vulnerabilities in organizational security arising from human error, governance gaps, and the misuse of technology. Although smaller in scale when compared to historical breaches involving billions of records, Signalgate illustrates critical systemic issues often overshadowed by a focus on external cyber threats. Employing a case-study approach and systematic review grounded in the NIST Cybersecurity Framework, we analyze the incident to identify patterns of human-centric vulnerabilities and governance challenges common to organizational security failures. Findings emphasize three critical points. (1) Organizational security depends heavily on human behavior, with internal actors often serving as the weakest link despite advanced technical defenses; (2) Leadership tone strongly influences organizational security culture and efficacy, and (3) widespread reliance on technical solutions without sufficient investments in human and organizational factors leads to ineffective practices and wasted resources. From these observations, we propose actionable recommendations for enhancing organizational and national security, including strong leadership engagement, comprehensive adoption of zero-trust architectures, clearer accountability structures, incentivized security behaviors, and rigorous oversight. Particularly during periods of organizational transition, such as mergers or large-scale personnel changes, additional measures become particularly important. Signalgate underscores the need for leaders and policymakers to reorient cybersecurity strategies toward addressing governance, cultural, and behavioral risks.</li>
</ul>

<h3>Title: Sequentially Auditing Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Tomás González, Mateo Dulce-Rubio, Aaditya Ramdas, Mónica Ribero</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07055">https://arxiv.org/abs/2509.07055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07055">https://arxiv.org/pdf/2509.07055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07055]] Sequentially Auditing Differential Privacy(https://arxiv.org/abs/2509.07055)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We propose a practical sequential test for auditing differential privacy guarantees of black-box mechanisms. The test processes streams of mechanisms' outputs providing anytime-valid inference while controlling Type I error, overcoming the fixed sample size limitation of previous batch auditing methods. Experiments show this test detects violations with sample sizes that are orders of magnitude smaller than existing methods, reducing this number from 50K to a few hundred examples, across diverse realistic mechanisms. Notably, it identifies DP-SGD privacy violations in \textit{under} one training run, unlike prior methods needing full model training.</li>
</ul>

<h3>Title: Faster VGGT with Block-Sparse Global Attention</h3>
<ul>
<li><strong>Authors: </strong>Chung-Shien Brian Wang, Christian Schmidt, Jens Piekenbrinck, Bastian Leibe</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07120">https://arxiv.org/abs/2509.07120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07120">https://arxiv.org/pdf/2509.07120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07120]] Faster VGGT with Block-Sparse Global Attention(https://arxiv.org/abs/2509.07120)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Efficient and accurate feed-forward multi-view reconstruction has long been an important task in computer vision. Recent transformer-based models like VGGT and $\pi^3$ have achieved impressive results with simple architectures, yet they face an inherent runtime bottleneck, due to the quadratic complexity of the global attention layers, that limits the scalability to large image sets. In this paper, we empirically analyze the global attention matrix of these models and observe that probability mass concentrates on a small subset of patch-patch interactions that correspond to cross-view geometric matches. Motivated by the structured attention and inspired by recent advancement in large language models, we propose a replacement for the dense global attention operation based on highly optimized block-sparse kernels, yielding up to $4\times$ faster inference with comparable task performance. Our retrofit requires no retraining of the backbone, extends to both VGGT and $\pi^3$, and supports large image collections. Evaluations on a comprehensive suite of multi-view benchmarks demonstrate the effectiveness of our approach.</li>
</ul>

<h3>Title: Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry</h3>
<ul>
<li><strong>Authors: </strong>Soruya Saha, Md Nurul Absurd, Saptarshi Debroy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07130">https://arxiv.org/abs/2509.07130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07130">https://arxiv.org/pdf/2509.07130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07130]] Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry(https://arxiv.org/abs/2509.07130)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Visual-Inertial Odometry (VIO) supports immersive Virtual Reality (VR) by fusing camera and Inertial Measurement Unit (IMU) data for real-time pose. However, current trend of offloading VIO to edge servers can lead server-side threat surface where subtle pose spoofing can accumulate into substantial drift, while evading heuristic checks. In this paper, we study this threat and present an unsupervised, label-free detection and recovery mechanism. The proposed model is trained on attack-free sessions to learn temporal regularities of motion to detect runtime deviations and initiate recovery to restore pose consistency. We evaluate the approach in a realistic offloaded-VIO environment using ILLIXR testbed across multiple spoofing intensities. Experimental results in terms of well-known performance metrics show substantial reductions in trajectory and pose error compared to a no-defense baseline.</li>
</ul>

<h3>Title: SoK: Security and Privacy of AI Agents for Blockchain</h3>
<ul>
<li><strong>Authors: </strong>Nicolò Romandini, Carlo Mazzocca, Kai Otsuki, Rebecca Montanari</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07131">https://arxiv.org/abs/2509.07131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07131">https://arxiv.org/pdf/2509.07131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07131]] SoK: Security and Privacy of AI Agents for Blockchain(https://arxiv.org/abs/2509.07131)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Blockchain and smart contracts have garnered significant interest in recent years as the foundation of a decentralized, trustless digital ecosystem, thereby eliminating the need for traditional centralized authorities. Despite their central role in powering Web3, their complexity still presents significant barriers for non-expert users. To bridge this gap, Artificial Intelligence (AI)-based agents have emerged as valuable tools for interacting with blockchain environments, supporting a range of tasks, from analyzing on-chain data and optimizing transaction strategies to detecting vulnerabilities within smart contracts. While interest in applying AI to blockchain is growing, the literature still lacks a comprehensive survey that focuses specifically on the intersection with AI agents. Most of the related work only provides general considerations, without focusing on any specific domain. This paper addresses this gap by presenting the first Systematization of Knowledge dedicated to AI-driven systems for blockchain, with a special focus on their security and privacy dimensions, shedding light on their applications, limitations, and future research directions.</li>
</ul>

<h3>Title: MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations</h3>
<ul>
<li><strong>Authors: </strong>Ruggero Marino Lazzaroni, Alessandro Angioi, Michelangelo Puliga, Davide Sanna, Roberto Marras</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07135">https://arxiv.org/abs/2509.07135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07135">https://arxiv.org/pdf/2509.07135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07135]] MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations(https://arxiv.org/abs/2509.07135)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) show increasing potential in education, yet benchmarks for non-English languages in specialized domains remain scarce. We introduce MedBench-IT, the first comprehensive benchmark for evaluating LLMs on Italian medical university entrance examinations. Sourced from Edizioni Simone, a leading preparatory materials publisher, MedBench-IT comprises 17,410 expert-written multiple-choice questions across six subjects (Biology, Chemistry, Logic, General Culture, Mathematics, Physics) and three difficulty levels. We evaluated diverse models including proprietary LLMs (GPT-4o, Claude series) and resource-efficient open-source alternatives (<30B parameters) focusing on practical deployability. Beyond accuracy, we conducted rigorous reproducibility tests (88.86% response consistency, varying by subject), ordering bias analysis (minimal impact), and reasoning prompt evaluation. We also examined correlations between question readability and model performance, finding a statistically significant but small inverse relationship. MedBench-IT provides a crucial resource for Italian NLP community, EdTech developers, and practitioners, offering insights into current capabilities and standardized evaluation methodology for this critical domain.</li>
</ul>

<h3>Title: Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiyin Tan, Jennifer D'Souza</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07142">https://arxiv.org/abs/2509.07142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07142">https://arxiv.org/pdf/2509.07142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07142]] Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models(https://arxiv.org/abs/2509.07142)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This study presents a framework for automated evaluation of dynamically evolving topic models using Large Language Models (LLMs). Topic modeling is essential for organizing and retrieving scholarly content in digital library systems, helping users navigate complex and evolving knowledge domains. However, widely used automated metrics, such as coherence and diversity, often capture only narrow statistical patterns and fail to explain semantic failures in practice. We introduce a purpose-oriented evaluation framework that employs nine LLM-based metrics spanning four key dimensions of topic quality: lexical validity, intra-topic semantic soundness, inter-topic structural soundness, and document-topic alignment soundness. The framework is validated through adversarial and sampling-based protocols, and is applied across datasets spanning news articles, scholarly publications, and social media posts, as well as multiple topic modeling methods and open-source LLMs. Our analysis shows that LLM-based metrics provide interpretable, robust, and task-relevant assessments, uncovering critical weaknesses in topic models such as redundancy and semantic drift, which are often missed by traditional metrics. These results support the development of scalable, fine-grained evaluation tools for maintaining topic relevance in dynamic datasets. All code and data supporting this work are accessible at this https URL.</li>
</ul>

<h3>Title: Measuring Uncertainty in Transformer Circuits with Effective Information Consistency</h3>
<ul>
<li><strong>Authors: </strong>Anatoly A. Krasnovsky</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07149">https://arxiv.org/abs/2509.07149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07149">https://arxiv.org/pdf/2509.07149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07149]] Measuring Uncertainty in Transformer Circuits with Effective Information Consistency(https://arxiv.org/abs/2509.07149)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Mechanistic interpretability has identified functional subgraphs within large language models (LLMs), known as Transformer Circuits (TCs), that appear to implement specific algorithms. Yet we lack a formal, single-pass way to quantify when an active circuit is behaving coherently and thus likely trustworthy. Building on prior systems-theoretic proposals, we specialize a sheaf/cohomology and causal emergence perspective to TCs and introduce the Effective-Information Consistency Score (EICS). EICS combines (i) a normalized sheaf inconsistency computed from local Jacobians and activations, with (ii) a Gaussian EI proxy for circuit-level causal emergence derived from the same forward state. The construction is white-box, single-pass, and makes units explicit so that the score is dimensionless. We further provide practical guidance on score interpretation, computational overhead (with fast and exact modes), and a toy sanity-check analysis. Empirical validation on LLM tasks is deferred.</li>
</ul>

<h3>Title: PLaID++: A Preference Aligned Language Model for Targeted Inorganic Materials Design</h3>
<ul>
<li><strong>Authors: </strong>Andy Xu, Rohan Desai, Larry Wang, Gabriel Hope, Ethan Ritz</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07150">https://arxiv.org/abs/2509.07150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07150">https://arxiv.org/pdf/2509.07150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07150]] PLaID++: A Preference Aligned Language Model for Targeted Inorganic Materials Design(https://arxiv.org/abs/2509.07150)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Discovering novel materials is critical for technological advancements such as solar cells, batteries, and carbon capture. However, the development of new materials is constrained by a slow and expensive trial-and-error process. To accelerate this pipeline, we introduce PLaID++, a Large Language Model (LLM) fine-tuned for stable and property-guided crystal generation. We fine-tune Qwen-2.5 7B to generate crystal structures using a novel Wyckoff-based text representation. We show that generation can be effectively guided with a reinforcement learning technique based on Direct Preference Optimization (DPO), with sampled structures categorized by their stability, novelty, and space group. By encoding symmetry constraints directly into text and guiding model outputs towards desirable chemical space, PLaID++ generates structures that are thermodynamically stable, unique, and novel at a $\sim$50\% greater rate than prior methods and conditionally generates structures with desired space group properties. Our experiments highlight the effectiveness of iterative DPO, achieving $\sim$115\% and $\sim$50\% improvements in unconditional and space group conditioned generation, respectively, compared to fine-tuning alone. Our work demonstrates the potential of adapting post-training techniques from natural language processing to materials design, paving the way for targeted and efficient discovery of novel materials.</li>
</ul>

<h3>Title: Towards EnergyGPT: A Large Language Model Specialized for the Energy Sector</h3>
<ul>
<li><strong>Authors: </strong>Amal Chebbi, Babajide Kolade</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07177">https://arxiv.org/abs/2509.07177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07177">https://arxiv.org/pdf/2509.07177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07177]] Towards EnergyGPT: A Large Language Model Specialized for the Energy Sector(https://arxiv.org/abs/2509.07177)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models have demonstrated impressive capabilities across various domains. However, their general-purpose nature often limits their effectiveness in specialized fields such as energy, where deep technical expertise and precise domain knowledge are essential. In this paper, we introduce EnergyGPT, a domain-specialized language model tailored for the energy sector, developed by fine-tuning LLaMA 3.1-8B model using Supervised Fine-Tuning on a high-quality, curated corpus of energy-related texts. We present a complete development pipeline, including data collection and curation, model fine-tuning, benchmark design and LLM-judge choice, evaluation and deployment. Through this work, we demonstrate that our training strategy enables improvements in domain relevance and performance without the need for large-scale infrastructure. By evaluating the performance of the model using domain-specific question-answering benchmarks, our results demonstrate that EnergyGPT outperforms the base model in most of the energy-related language understanding and generation tasks.</li>
</ul>

<h3>Title: Realism to Deception: Investigating Deepfake Detectors Against Face Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Saad Saeed, Ijaz Ul Haq, Khalid Malik</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07178">https://arxiv.org/abs/2509.07178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07178">https://arxiv.org/pdf/2509.07178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07178]] Realism to Deception: Investigating Deepfake Detectors Against Face Enhancement(https://arxiv.org/abs/2509.07178)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, biometric</a></li>
<li><strong>Abstract: </strong>Face enhancement techniques are widely used to enhance facial appearance. However, they can inadvertently distort biometric features, leading to significant decrease in the accuracy of deepfake detectors. This study hypothesizes that these techniques, while improving perceptual quality, can degrade the performance of deepfake detectors. To investigate this, we systematically evaluate whether commonly used face enhancement methods can serve an anti-forensic role by reducing detection accuracy. We use both traditional image processing methods and advanced GAN-based enhancements to evaluate the robustness of deepfake detectors. We provide a comprehensive analysis of the effectiveness of these enhancement techniques, focusing on their impact on Naïve, Spatial, and Frequency-based detection methods. Furthermore, we conduct adversarial training experiments to assess whether exposure to face enhancement transformations improves model robustness. Experiments conducted on the FaceForensics++, DeepFakeDetection, and CelebDF-v2 datasets indicate that even basic enhancement filters can significantly reduce detection accuracy achieving ASR up to 64.63\%. In contrast, GAN-based techniques further exploit these vulnerabilities, achieving ASR up to 75.12\%. Our results demonstrate that face enhancement methods can effectively function as anti-forensic tools, emphasizing the need for more resilient and adaptive forensic methods.</li>
</ul>

<h3>Title: Dimensionally Reduced Open-World Clustering: DROWCULA</h3>
<ul>
<li><strong>Authors: </strong>Erencem Ozbey, Dimitrios I. Diochnos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07184">https://arxiv.org/abs/2509.07184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07184">https://arxiv.org/pdf/2509.07184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07184]] Dimensionally Reduced Open-World Clustering: DROWCULA(https://arxiv.org/abs/2509.07184)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Working with annotated data is the cornerstone of supervised learning. Nevertheless, providing labels to instances is a task that requires significant human effort. Several critical real-world applications make things more complicated because no matter how many labels may have been identified in a task of interest, it could be the case that examples corresponding to novel classes may appear in the future. Not unsurprisingly, prior work in this, so-called, `open-world' context has focused a lot on semi-supervised approaches. Focusing on image classification, somehow paradoxically, we propose a fully unsupervised approach to the problem of determining the novel categories in a particular dataset. Our approach relies on estimating the number of clusters using Vision Transformers, which utilize attention mechanisms to generate vector embeddings. Furthermore, we incorporate manifold learning techniques to refine these embeddings by exploiting the intrinsic geometry of the data, thereby enhancing the overall image clustering performance. Overall, we establish new State-of-the-Art results on single-modal clustering and Novel Class Discovery on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet. We do so, both when the number of clusters is known or unknown ahead of time. The code is available at: this https URL.</li>
</ul>

<h3>Title: DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge</h3>
<ul>
<li><strong>Authors: </strong>Zonghai Yao, Michael Sun, Won Seok Jang, Sunjae Kwon, Soie Kwon, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07188">https://arxiv.org/abs/2509.07188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07188">https://arxiv.org/pdf/2509.07188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07188]] DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge(https://arxiv.org/abs/2509.07188)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Discharge communication is a critical yet underexplored component of patient care, where the goal shifts from diagnosis to education. While recent large language model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they fail to evaluate models' ability to support patients after the visit. We introduce DischargeSim, a novel benchmark that evaluates LLMs on their ability to act as personalized discharge educators. DischargeSim simulates post-visit, multi-turn conversations between LLM-driven DoctorAgents and PatientAgents with diverse psychosocial profiles (e.g., health literacy, education, emotion). Interactions are structured across six clinically grounded discharge topics and assessed along three axes: (1) dialogue quality via automatic and LLM-as-judge evaluation, (2) personalized document generation including free-text summaries and structured AHRQ checklists, and (3) patient comprehension through a downstream multiple-choice exam. Experiments across 18 LLMs reveal significant gaps in discharge education capability, with performance varying widely across patient profiles. Notably, model size does not always yield better education outcomes, highlighting trade-offs in strategy use and content prioritization. DischargeSim offers a first step toward benchmarking LLMs in post-visit clinical education and promoting equitable, personalized patient support.</li>
</ul>

<h3>Title: Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Zahra Atf, Peter R Lewis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07190">https://arxiv.org/abs/2509.07190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07190">https://arxiv.org/pdf/2509.07190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07190]] Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation(https://arxiv.org/abs/2509.07190)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used in high-stakes settings, where explaining uncertainty is both technical and ethical. Probabilistic methods are often opaque and misaligned with expectations of transparency. We propose a framework based on rule-based moral principles for handling uncertainty in LLM-generated text. Using insights from moral psychology and virtue ethics, we define rules such as precaution, deference, and responsibility to guide responses under epistemic or aleatoric uncertainty. These rules are encoded in a lightweight Prolog engine, where uncertainty levels (low, medium, high) trigger aligned system actions with plain-language rationales. Scenario-based simulations benchmark rule coverage, fairness, and trust calibration. Use cases in clinical and legal domains illustrate how moral reasoning can improve trust and interpretability. Our approach offers a transparent, lightweight alternative to probabilistic models for socially responsible natural language generation.</li>
</ul>

<h3>Title: Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data</h3>
<ul>
<li><strong>Authors: </strong>Yiyue Chen, Usman Akram, Chianing Wang, Haris Vikalo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07198">https://arxiv.org/abs/2509.07198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07198">https://arxiv.org/pdf/2509.07198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07198]] Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data(https://arxiv.org/abs/2509.07198)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Motivated by the high resource costs and privacy concerns associated with centralized machine learning, federated learning (FL) has emerged as an efficient alternative that enables clients to collaboratively train a global model while keeping their data local. However, in real-world deployments, client data distributions often evolve over time and differ significantly across clients, introducing heterogeneity that degrades the performance of standard FL algorithms. In this work, we introduce Fed-REACT, a federated learning framework designed for heterogeneous and evolving client data. Fed-REACT combines representation learning with evolutionary clustering in a two-stage process: (1) in the first stage, each client learns a local model to extracts feature representations from its data; (2) in the second stage, the server dynamically groups clients into clusters based on these representations and coordinates cluster-wise training of task-specific models for downstream objectives such as classification or regression. We provide a theoretical analysis of the representation learning stage, and empirically demonstrate that Fed-REACT achieves superior accuracy and robustness on real-world datasets.</li>
</ul>

<h3>Title: XBusNet: Text-Guided Breast Ultrasound Segmentation via Multimodal Vision-Language Learning</h3>
<ul>
<li><strong>Authors: </strong>Raja Mallina, Bryar Shareef</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07213">https://arxiv.org/abs/2509.07213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07213">https://arxiv.org/pdf/2509.07213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07213]] XBusNet: Text-Guided Breast Ultrasound Segmentation via Multimodal Vision-Language Learning(https://arxiv.org/abs/2509.07213)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Background: Precise breast ultrasound (BUS) segmentation supports reliable measurement, quantitative analysis, and downstream classification, yet remains difficult for small or low-contrast lesions with fuzzy margins and speckle noise. Text prompts can add clinical context, but directly applying weakly localized text-image cues (e.g., CAM/CLIP-derived signals) tends to produce coarse, blob-like responses that smear boundaries unless additional mechanisms recover fine edges. Methods: We propose XBusNet, a novel dual-prompt, dual-branch multimodal model that combines image features with clinically grounded text. A global pathway based on a CLIP Vision Transformer encodes whole-image semantics conditioned on lesion size and location, while a local U-Net pathway emphasizes precise boundaries and is modulated by prompts that describe shape, margin, and Breast Imaging Reporting and Data System (BI-RADS) terms. Prompts are assembled automatically from structured metadata, requiring no manual clicks. We evaluate on the Breast Lesions USG (BLU) dataset using five-fold cross-validation. Primary metrics are Dice and Intersection over Union (IoU); we also conduct size-stratified analyses and ablations to assess the roles of the global and local paths and the text-driven modulation. Results: XBusNet achieves state-of-the-art performance on BLU, with mean Dice of 0.8765 and IoU of 0.8149, outperforming six strong baselines. Small lesions show the largest gains, with fewer missed regions and fewer spurious activations. Ablation studies show complementary contributions of global context, local boundary modeling, and prompt-based modulation. Conclusions: A dual-prompt, dual-branch multimodal design that merges global semantics with local precision yields accurate BUS segmentation masks and improves robustness for small, low-contrast lesions.</li>
</ul>

<h3>Title: Explaining How Quantization Disparately Skews a Model</h3>
<ul>
<li><strong>Authors: </strong>Abhimanyu Bellam, Jung-Eun Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07222">https://arxiv.org/abs/2509.07222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07222">https://arxiv.org/pdf/2509.07222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07222]] Explaining How Quantization Disparately Skews a Model(https://arxiv.org/abs/2509.07222)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Post Training Quantization (PTQ) is widely adopted due to its high compression capacity and speed with minimal impact on accuracy. However, we observed that disparate impacts are exacerbated by quantization, especially for minority groups. Our analysis explains that in the course of quantization there is a chain of factors attributed to a disparate impact across groups during forward and backward passes. We explore how the changes in weights and activations induced by quantization cause cascaded impacts in the network, resulting in logits with lower variance, increased loss, and compromised group accuracies. We extend our study to verify the influence of these impacts on group gradient norms and eigenvalues of the Hessian matrix, providing insights into the state of the network from an optimization point of view. To mitigate these effects, we propose integrating mixed precision Quantization Aware Training (QAT) with dataset sampling methods and weighted loss functions, therefore providing fair deployment of quantized neural networks.</li>
</ul>

<h3>Title: All You Need Is A Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching</h3>
<ul>
<li><strong>Authors: </strong>Ze Sheng, Qingxiao Xu, Jianwei Huang, Matthew Woodcock, Heqing Huang, Alastair F. Donaldson, Guofei Gu, Jeff Huang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07225">https://arxiv.org/abs/2509.07225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07225">https://arxiv.org/pdf/2509.07225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07225]] All You Need Is A Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching(https://arxiv.org/abs/2509.07225)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Our team, All You Need Is A Fuzzing Brain, was one of seven finalists in DARPA's Artificial Intelligence Cyber Challenge (AIxCC), placing fourth in the final round. During the competition, we developed a Cyber Reasoning System (CRS) that autonomously discovered 28 security vulnerabilities - including six previously unknown zero-days - in real-world open-source C and Java projects, and successfully patched 14 of them. The complete CRS is open source at this https URL. This paper provides a detailed technical description of our CRS, with an emphasis on its LLM-powered components and strategies. Building on AIxCC, we further introduce a public leaderboard for benchmarking state-of-the-art LLMs on vulnerability detection and patching tasks, derived from the AIxCC dataset. The leaderboard is available at this https URL.</li>
</ul>

<h3>Title: Systematic Optimization of Open Source Large Language Models for Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Pranav Pawar, Dhwaj Jain, Varun Gupta, Kaustav Dedhia, Dashrath Kale, Sudhir Dhekane</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07238">https://arxiv.org/abs/2509.07238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07238">https://arxiv.org/pdf/2509.07238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07238]] Systematic Optimization of Open Source Large Language Models for Mathematical Reasoning(https://arxiv.org/abs/2509.07238)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a practical investigation into fine-tuning model parameters for mathematical reasoning tasks through experimenting with various configurations including randomness control, reasoning depth, and sampling strategies, careful tuning demonstrates substantial improvements in efficiency as well as performance. A holistically optimized framework is introduced for five state-of-the-art models on mathematical reasoning tasks, exhibiting significant performance boosts while maintaining solution correctness. Through systematic parameter optimization across Qwen2.5-72B, Llama-3.1-70B, DeepSeek-V3, Mixtral-8x22B, and Yi-Lightning, consistent efficiency gains are demonstrated with 100% optimization success rate. The methodology achieves an average 29.4% reduction in computational cost and 23.9% improvement in inference speed across all tested models. This framework systematically searches parameter spaces including temperature (0.1-0.5), reasoning steps (4-12), planning periods (1-4), and nucleus sampling (0.85-0.98), determining optimal configurations through testing on mathematical reasoning benchmarks. Critical findings show that lower temperature regimes (0.1-0.4) and reduced reasoning steps (4-6) consistently enhance efficiency without compromising accuracy. DeepSeek-V3 achieves the highest accuracy at 98%, while Mixtral-8x22B delivers the most cost-effective performance at 361.5 tokens per accurate response. Key contributions include: (1) the first comprehensive optimization study for five diverse SOTA models in mathematical reasoning, (2) a standardized production-oriented parameter optimization framework, (3) discovery of universal optimization trends applicable across model architectures, and (4) production-ready configurations with extensive performance characterization.</li>
</ul>

<h3>Title: IP-Basis PINNs: Efficient Multi-Query Inverse Parameter Estimation</h3>
<ul>
<li><strong>Authors: </strong>Shalev Manor, Mohammad Kohandel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07245">https://arxiv.org/abs/2509.07245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07245">https://arxiv.org/pdf/2509.07245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07245]] IP-Basis PINNs: Efficient Multi-Query Inverse Parameter Estimation(https://arxiv.org/abs/2509.07245)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Solving inverse problems with Physics-Informed Neural Networks (PINNs) is computationally expensive for multi-query scenarios, as each new set of observed data requires a new, expensive training procedure. We present Inverse-Parameter Basis PINNs (IP-Basis PINNs), a meta-learning framework that extends the foundational work of Desai et al. (2022) to enable rapid and efficient inference for inverse problems. Our method employs an offline-online decomposition: a deep network is first trained offline to produce a rich set of basis functions that span the solution space of a parametric differential equation. For each new inverse problem online, this network is frozen, and solutions and parameters are inferred by training only a lightweight linear output layer against observed data. Key innovations that make our approach effective for inverse problems include: (1) a novel online loss formulation for simultaneous solution reconstruction and parameter identification, (2) a significant reduction in computational overhead via forward-mode automatic differentiation for PDE loss evaluation, and (3) a non-trivial validation and early-stopping mechanism for robust offline training. We demonstrate the efficacy of IP-Basis PINNs on three diverse benchmarks, including an extension to universal PINNs for unknown functional terms-showing consistent performance across constant and functional parameter estimation, a significant speedup per query over standard PINNs, and robust operation with scarce and noisy data.</li>
</ul>

<h3>Title: GCond: Gradient Conflict Resolution via Accumulation-based Stabilization for Large-Scale Multi-Task Learning</h3>
<ul>
<li><strong>Authors: </strong>Evgeny Alves Limarenko, Anastasiia Alexandrovna Studenikina</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07252">https://arxiv.org/abs/2509.07252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07252">https://arxiv.org/pdf/2509.07252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07252]] GCond: Gradient Conflict Resolution via Accumulation-based Stabilization for Large-Scale Multi-Task Learning(https://arxiv.org/abs/2509.07252)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In multi-task learning (MTL), gradient conflict poses a significant challenge. Effective methods for addressing this problem, including PCGrad, CAGrad, and GradNorm, in their original implementations are computationally demanding, which significantly limits their application in modern large models and transformers. We propose Gradient Conductor (GCond), a method that builds upon PCGrad principles by combining them with gradient accumulation and an adaptive arbitration mechanism. We evaluated GCond on self-supervised learning tasks using MobileNetV3-Small and ConvNeXt architectures on the ImageNet 1K dataset and a combined head and neck CT scan dataset, comparing the proposed method against baseline linear combinations and state-of-the-art gradient conflict resolution methods. The stochastic mode of GCond achieved a two-fold computational speedup while maintaining optimization quality, and demonstrated superior performance across all evaluated metrics, achieving lower L1 and SSIM losses compared to other methods on both datasets. GCond exhibited high scalability, being successfully applied to both compact models (MobileNetV3-Small) and large architectures (ConvNeXt-tiny and ConvNeXt-Base). It also showed compatibility with modern optimizers such as AdamW and Lion/LARS. Therefore, GCond offers a scalable and efficient solution to the problem of gradient conflicts in multi-task learning.</li>
</ul>

<h3>Title: LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade</h3>
<ul>
<li><strong>Authors: </strong>Aida Kostikova, Ole Pütz, Steffen Eger, Olga Sabelfeld, Benjamin Paassen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07274">https://arxiv.org/abs/2509.07274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07274">https://arxiv.org/pdf/2509.07274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07274]] LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade(https://arxiv.org/abs/2509.07274)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Migration has been a core topic in German political debate, from millions of expellees post World War II over labor migration to refugee movements in the recent past. Studying political speech regarding such wide-ranging phenomena in depth traditionally required extensive manual annotations, limiting the scope of analysis to small subsets of the data. Large language models (LLMs) have the potential to partially automate even complex annotation tasks. We provide an extensive evaluation of a multiple LLMs in annotating (anti-)solidarity subtypes in German parliamentary debates compared to a large set of thousands of human reference annotations (gathered over a year). We evaluate the influence of model size, prompting differences, fine-tuning, historical versus contemporary data; and we investigate systematic errors. Beyond methodological evaluation, we also interpret the resulting annotations from a social science lense, gaining deeper insight into (anti-)solidarity trends towards migrants in the German post-World War II period and recent past. Our data reveals a high degree of migrant-directed solidarity in the postwar period, as well as a strong trend towards anti-solidarity in the German parliament since 2015, motivating further research. These findings highlight the promise of LLMs for political text analysis and the importance of migration debates in Germany, where demographic decline and labor shortages coexist with rising polarization.</li>
</ul>

<h3>Title: Breast Cancer Detection in Thermographic Images via Diffusion-Based Augmentation and Nonlinear Feature Fusion</h3>
<ul>
<li><strong>Authors: </strong>Sepehr Salem, M. Moein Esfahani, Jingyu Liu, Vince Calhoun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07277">https://arxiv.org/abs/2509.07277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07277">https://arxiv.org/pdf/2509.07277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07277]] Breast Cancer Detection in Thermographic Images via Diffusion-Based Augmentation and Nonlinear Feature Fusion(https://arxiv.org/abs/2509.07277)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Data scarcity hinders deep learning for medical imaging. We propose a framework for breast cancer classification in thermograms that addresses this using a Diffusion Probabilistic Model (DPM) for data augmentation. Our DPM-based augmentation is shown to be superior to both traditional methods and a ProGAN baseline. The framework fuses deep features from a pre-trained ResNet-50 with handcrafted nonlinear features (e.g., Fractal Dimension) derived from U-Net segmented tumors. An XGBoost classifier trained on these fused features achieves 98.0\% accuracy and 98.1\% sensitivity. Ablation studies and statistical tests confirm that both the DPM augmentation and the nonlinear feature fusion are critical, statistically significant components of this success. This work validates the synergy between advanced generative models and interpretable features for creating highly accurate medical diagnostic tools.</li>
</ul>

<h3>Title: Learning Generalized Hamiltonian Dynamics with Stability from Noisy Trajectory Data</h3>
<ul>
<li><strong>Authors: </strong>Luke McLennan, Yi Wang, Ryan Farell, Minh Nguyen, Chandrajit Bajaj</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07280">https://arxiv.org/abs/2509.07280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07280">https://arxiv.org/pdf/2509.07280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07280]] Learning Generalized Hamiltonian Dynamics with Stability from Noisy Trajectory Data(https://arxiv.org/abs/2509.07280)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce a robust framework for learning various generalized Hamiltonian dynamics from noisy, sparse phase-space data and in an unsupervised manner based on variational Bayesian inference. Although conservative, dissipative, and port-Hamiltonian systems might share the same initial total energy of a closed system, it is challenging for a single Hamiltonian network model to capture the distinctive and varying motion dynamics and physics of a phase space, from sampled observational phase space trajectories. To address this complicated Hamiltonian manifold learning challenge, we extend sparse symplectic, random Fourier Gaussian processes learning with predictive successive numerical estimations of the Hamiltonian landscape, using a generalized form of state and conjugate momentum Hamiltonian dynamics, appropriate to different classes of conservative, dissipative and port-Hamiltonian physical systems. In addition to the kernelized evidence lower bound (ELBO) loss for data fidelity, we incorporate stability and conservation constraints as additional hyper-parameter balanced loss terms to regularize the model's multi-gradients, enforcing physics correctness for improved prediction accuracy with bounded uncertainty.</li>
</ul>

<h3>Title: ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers</h3>
<ul>
<li><strong>Authors: </strong>Jeff Shen, Lindsay Smith</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07282">https://arxiv.org/abs/2509.07282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07282">https://arxiv.org/pdf/2509.07282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07282]] ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers(https://arxiv.org/abs/2509.07282)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>We present cryptogram solving as an ideal testbed for studying neural network generalization in combinatorially complex domains. In this task, models must decrypt text encoded with substitution ciphers, choosing from 26! possible mappings without explicit access to the cipher. We develop ALICE (an Architecture for Learning Interpretable Cryptogram dEcipherment): a simple encoder-only Transformer that sets a new state-of-the-art for both accuracy and speed on this decryption problem. Surprisingly, ALICE generalizes to unseen ciphers after training on only ${\sim}1500$ unique ciphers, a minute fraction ($3.7 \times 10^{-24}$) of the possible cipher space. To enhance interpretability, we introduce a novel bijective decoding head that explicitly models permutations via the Gumbel-Sinkhorn method, enabling direct extraction of learned cipher mappings. Through early exit analysis, we reveal how ALICE progressively refines its predictions in a way that appears to mirror common human strategies for this task: early layers employ frequency-based heuristics, middle layers form word structures, and final layers correct individual characters. Our architectural innovations and analysis methods extend beyond cryptograms to any domain with bijective mappings and combinatorial structure, offering new insights into neural network generalization and interpretability.</li>
</ul>

<h3>Title: Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm</h3>
<ul>
<li><strong>Authors: </strong>Yan Pang, Wenlong Meng, Xiaojing Liao, Tianhao Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07287">https://arxiv.org/abs/2509.07287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07287">https://arxiv.org/pdf/2509.07287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07287]] Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm(https://arxiv.org/abs/2509.07287)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, steal, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid development of large language models, the potential threat of their malicious use, particularly in generating phishing content, is becoming increasingly prevalent. Leveraging the capabilities of LLMs, malicious users can synthesize phishing emails that are free from spelling mistakes and other easily detectable features. Furthermore, such models can generate topic-specific phishing messages, tailoring content to the target domain and increasing the likelihood of success. Detecting such content remains a significant challenge, as LLM-generated phishing emails often lack clear or distinguishable linguistic features. As a result, most existing semantic-level detection approaches struggle to identify them reliably. While certain LLM-based detection methods have shown promise, they suffer from high computational costs and are constrained by the performance of the underlying language model, making them impractical for large-scale deployment. In this work, we aim to address this issue. We propose Paladin, which embeds trigger-tag associations into vanilla LLM using various insertion strategies, creating them into instrumented LLMs. When an instrumented LLM generates content related to phishing, it will automatically include detectable tags, enabling easier identification. Based on the design on implicit and explicit triggers and tags, we consider four distinct scenarios in our work. We evaluate our method from three key perspectives: stealthiness, effectiveness, and robustness, and compare it with existing baseline methods. Experimental results show that our method outperforms the baselines, achieving over 90% detection accuracy across all scenarios.</li>
</ul>

<h3>Title: zkUnlearner: A Zero-Knowledge Framework for Verifiable Unlearning with Multi-Granularity and Forgery-Resistance</h3>
<ul>
<li><strong>Authors: </strong>Nan Wang, Nan Wu, Xiangyu Hui, Jiafan Wang, Xin Yuan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07290">https://arxiv.org/abs/2509.07290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07290">https://arxiv.org/pdf/2509.07290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07290]] zkUnlearner: A Zero-Knowledge Framework for Verifiable Unlearning with Multi-Granularity and Forgery-Resistance(https://arxiv.org/abs/2509.07290)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack</a></li>
<li><strong>Abstract: </strong>As the demand for exercising the "right to be forgotten" grows, the need for verifiable machine unlearning has become increasingly evident to ensure both transparency and accountability. We present {\em zkUnlearner}, the first zero-knowledge framework for verifiable machine unlearning, specifically designed to support {\em multi-granularity} and {\em forgery-resistance}. First, we propose a general computational model that employs a {\em bit-masking} technique to enable the {\em selectivity} of existing zero-knowledge proofs of training for gradient descent algorithms. This innovation enables not only traditional {\em sample-level} unlearning but also more advanced {\em feature-level} and {\em class-level} unlearning. Our model can be translated to arithmetic circuits, ensuring compatibility with a broad range of zero-knowledge proof systems. Furthermore, our approach overcomes key limitations of existing methods in both efficiency and privacy. Second, forging attacks present a serious threat to the reliability of unlearning. Specifically, in Stochastic Gradient Descent optimization, gradients from unlearned data, or from minibatches containing it, can be forged using alternative data samples or minibatches that exclude it. We propose the first effective strategies to resist state-of-the-art forging attacks. Finally, we benchmark a zkSNARK-based instantiation of our framework and perform comprehensive performance evaluations to validate its practicality.</li>
</ul>

<h3>Title: Reconstruction Alignment Improves Unified Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Ji Xie, Trevor Darrell, Luke Zettlemoyer, XuDong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07295">https://arxiv.org/abs/2509.07295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07295">https://arxiv.org/pdf/2509.07295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07295]] Reconstruction Alignment Improves Unified Multimodal Models(https://arxiv.org/abs/2509.07295)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense "text prompts," providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73$\rightarrow$0.90) and DPGBench (80.93$\rightarrow$88.15), while also boosting editing benchmarks (ImgEdit 3.38$\rightarrow$3.75, GEdit 6.94$\rightarrow$7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs</li>
</ul>

<h3>Title: Basis Vector Metric: A Method for Robust Open-Ended State Change Detection</h3>
<ul>
<li><strong>Authors: </strong>David Oprea, Sam Powers</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07308">https://arxiv.org/abs/2509.07308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07308">https://arxiv.org/pdf/2509.07308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07308]] Basis Vector Metric: A Method for Robust Open-Ended State Change Detection(https://arxiv.org/abs/2509.07308)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We test a new method, which we will abbreviate using the acronym BVM (Basis Vectors Method), in its ability to judge the state changes in images through using language embeddings. We used the MIT-States dataset, containing about 53,000 images, to gather all of our data, which has 225 nouns and 115 adjectives, with each noun having about 9 different adjectives, forming approximately 1000 noun-adjective pairs. For our first experiment, we test our method's ability to determine the state of each noun class separately against other metrics for comparison. These metrics are cosine similarity, dot product, product quantization, binary index, Naive Bayes, and a custom neural network. Among these metrics, we found that our proposed BVM performs the best in classifying the states for each noun. We then perform a second experiment where we try using BVM to determine if it can differentiate adjectives from one another for each adjective separately. We compared the abilities of BVM to differentiate adjectives against the proposed method the MIT-States paper suggests: using a logistic regression model. In the end, we did not find conclusive evidence that our BVM metric could perform better than the logistic regression model at discerning adjectives. Yet, we were able to find evidence for possible improvements to our method; this leads to the chance of increasing our method's accuracy through certain changes in our methodologies.</li>
</ul>

<h3>Title: Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations</h3>
<ul>
<li><strong>Authors: </strong>Sihyun Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07311">https://arxiv.org/abs/2509.07311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07311">https://arxiv.org/pdf/2509.07311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07311]] Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations(https://arxiv.org/abs/2509.07311)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have been driven by pretraining, supervised fine tuning (SFT), and alignment tuning. Among these, SFT plays a crucial role in transforming a model 's general knowledge into structured responses tailored to specific tasks. However, there is no clearly established methodology for effective training data selection. Simply increasing the volume of data does not guarantee performance improvements, while preprocessing, sampling, and validation require substantial time and cost. To address this issue, a variety of data selection methods have been proposed. Among them, knowledge based selection approaches identify suitable training data by analyzing the model 's responses. Nevertheless, these methods typically rely on prompt engineering, making them sensitive to variations and incurring additional costs for prompt design. In this study, we propose Knowledge Analysis via Model Internal Representations (KAMIR), a novel approach that overcomes these limitations by analyzing data based on the model 's internal representations. KAMIR computes similarities between the hidden states of each layer (block) and the final hidden states for a given input to assess the data. Unlike prior methods that were largely limited to multiple choice tasks, KAMIR can be applied to a wide range of tasks such as machine reading comprehension and summarization. Moreover, it selects data useful for training based on the model 's familiarity with the input, even with a small dataset and a simple classifier architecture. Experiments across diverse task datasets demonstrate that training with less familiar data leads to better generalization performance.</li>
</ul>

<h3>Title: SafeToolBench: Pioneering a Prospective Benchmark to Evaluating Tool Utilization Safety in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hongfei Xia, Hongru Wang, Zeming Liu, Qian Yu, Yuhang Guo, Haifeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07315">https://arxiv.org/abs/2509.07315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07315">https://arxiv.org/pdf/2509.07315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07315]] SafeToolBench: Pioneering a Prospective Benchmark to Evaluating Tool Utilization Safety in LLMs(https://arxiv.org/abs/2509.07315)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have exhibited great performance in autonomously calling various tools in external environments, leading to better problem solving and task automation capabilities. However, these external tools also amplify potential risks such as financial loss or privacy leakage with ambiguous or malicious user instructions. Compared to previous studies, which mainly assess the safety awareness of LLMs after obtaining the tool execution results (i.e., retrospective evaluation), this paper focuses on prospective ways to assess the safety of LLM tool utilization, aiming to avoid irreversible harm caused by directly executing tools. To this end, we propose SafeToolBench, the first benchmark to comprehensively assess tool utilization security in a prospective manner, covering malicious user instructions and diverse practical toolsets. Additionally, we propose a novel framework, SafeInstructTool, which aims to enhance LLMs' awareness of tool utilization security from three perspectives (i.e., \textit{User Instruction, Tool Itself, and Joint Instruction-Tool}), leading to nine detailed dimensions in total. We experiment with four LLMs using different methods, revealing that existing approaches fail to capture all risks in tool utilization. In contrast, our framework significantly enhances LLMs' self-awareness, enabling a more safe and trustworthy tool utilization.</li>
</ul>

<h3>Title: Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation</h3>
<ul>
<li><strong>Authors: </strong>Nakyung Lee, Yeongoon Kim, Minhae Oh, Suhwan Kim, Jin Woo Koo, Hyewon Jo, Jungwoo Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07324">https://arxiv.org/abs/2509.07324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07324">https://arxiv.org/pdf/2509.07324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07324]] Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation(https://arxiv.org/abs/2509.07324)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based self-attention mechanism serves as the core of modern language models, yet it often suffers from localization, where attentions collapse onto a limited subset of tokens and fail to capture long-range dependencies. To address this issue, we propose Self-Attention One-step Belief Propagation (SAOBP), a refinement framework that injects multi-hop relationships through a belief propagation process. To interpret and quantify these interactions, we introduce Global Token Dependency (GTD) that captures the relative contribution of multihop connections within the attention graph. Empirical results indicate that SAOBP helps prevent entropy collapse in deeper layers and adaptively maintains GTD at task-appropriate levels, thereby supporting improvements in model performance. Importantly, we observe competitive gains in small-scale models, highlighting its potential for improving inference quality in resource-constrained scenarios.</li>
</ul>

<h3>Title: CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation</h3>
<ul>
<li><strong>Authors: </strong>Alyssa Unell, Noel C. F. Codella, Sam Preston, Peniel Argaw, Wen-wai Yim, Zelalem Gero, Cliff Wong, Rajesh Jena, Eric Horvitz, Amanda K. Hall, Ruican Rachel Zhong, Jiachen Li, Shrey Jain, Mu Wei, Matthew Lungren, Hoifung Poon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07325">https://arxiv.org/abs/2509.07325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07325">https://arxiv.org/pdf/2509.07325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07325]] CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation(https://arxiv.org/abs/2509.07325)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The National Comprehensive Cancer Network (NCCN) provides evidence-based guidelines for cancer treatment. Translating complex patient presentations into guideline-compliant treatment recommendations is time-intensive, requires specialized expertise, and is prone to error. Advances in large language model (LLM) capabilities promise to reduce the time required to generate treatment recommendations and improve accuracy. We present an LLM agent-based approach to automatically generate guideline-concordant treatment trajectories for patients with non-small cell lung cancer (NSCLC). Our contributions are threefold. First, we construct a novel longitudinal dataset of 121 cases of NSCLC patients that includes clinical encounters, diagnostic results, and medical histories, each expertly annotated with the corresponding NCCN guideline trajectories by board-certified oncologists. Second, we demonstrate that existing LLMs possess domain-specific knowledge that enables high-quality proxy benchmark generation for both model development and evaluation, achieving strong correlation (Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks. Third, we develop a hybrid approach combining expensive human annotations with model consistency information to create both the agent framework that predicts the relevant guidelines for a patient, as well as a meta-classifier that verifies prediction accuracy with calibrated confidence scores for treatment recommendations (AUROC=0.800), a critical capability for communicating the accuracy of outputs, custom-tailoring tradeoffs in performance, and supporting regulatory compliance. This work establishes a framework for clinically viable LLM-based guideline adherence systems that balance accuracy, interpretability, and regulatory requirements while reducing annotation costs, providing a scalable pathway toward automated clinical decision support.</li>
</ul>

<h3>Title: DEPF: A UAV Multispectral Object Detector with Dual-Domain Enhancement and Priority-Guided Mamba Fusion</h3>
<ul>
<li><strong>Authors: </strong>Shucong Li, Zhenyu Liu, Zijie Hong, Zhiheng Zhou, Xianghai Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07327">https://arxiv.org/abs/2509.07327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07327">https://arxiv.org/pdf/2509.07327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07327]] DEPF: A UAV Multispectral Object Detector with Dual-Domain Enhancement and Priority-Guided Mamba Fusion(https://arxiv.org/abs/2509.07327)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multispectral remote sensing object detection is one of the important application of unmanned aerial vehicle (UAV). However, it faces three challenges. Firstly, the low-light remote sensing images reduce the complementarity during multi-modality fusion. Secondly, the local small target modeling is interfered with redundant information in the fusion stage easily. Thirdly, due to the quadratic computational complexity, it is hard to apply the transformer-based methods on the UAV platform. To address these limitations, motivated by Mamba with linear complexity, a UAV multispectral object detector with dual-domain enhancement and priority-guided mamba fusion (DEPF) is proposed. Firstly, to enhance low-light remote sensing images, Dual-Domain Enhancement Module (DDE) is designed, which contains Cross-Scale Wavelet Mamba (CSWM) and Fourier Details Recovery block (FDR). CSWM applies cross-scale mamba scanning for the low-frequency components to enhance the global brightness of images, while FDR constructs spectrum recovery network to enhance the frequency spectra features for recovering the texture-details. Secondly, to enhance local target modeling and reduce the impact of redundant information during fusion, Priority-Guided Mamba Fusion Module (PGMF) is designed. PGMF introduces the concept of priority scanning, which starts from local targets features according to the priority scores obtained from modality difference. Experiments on DroneVehicle dataset and VEDAI dataset reports that, DEPF performs well on object detection, comparing with state-of-the-art methods. Our code is available in the supplementary material.</li>
</ul>

<h3>Title: FedTeddi: Temporal Drift and Divergence Aware Scheduling for Timely Federated Edge Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Bai, Yuxuan Sun, Tan Chen, Wei Chen, Sheng Zhou, Zhisheng Niu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07342">https://arxiv.org/abs/2509.07342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07342">https://arxiv.org/pdf/2509.07342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07342]] FedTeddi: Temporal Drift and Divergence Aware Scheduling for Timely Federated Edge Learning(https://arxiv.org/abs/2509.07342)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated edge learning (FEEL) enables collaborative model training across distributed clients over wireless networks without exposing raw data. While most existing studies assume static datasets, in real-world scenarios clients may continuously collect data with time-varying and non-independent and identically distributed (non-i.i.d.) characteristics. A critical challenge is how to adapt models in a timely yet efficient manner to such evolving data. In this paper, we propose FedTeddi, a temporal-drift-and-divergence-aware scheduling algorithm that facilitates fast convergence of FEEL under dynamic data evolution and communication resource limits. We first quantify the temporal dynamics and non-i.i.d. characteristics of data using temporal drift and collective divergence, respectively, and represent them as the Earth Mover's Distance (EMD) of class distributions for classification tasks. We then propose a novel optimization objective and develop a joint scheduling and bandwidth allocation algorithm, enabling the FEEL system to learn from new data quickly without forgetting previous knowledge. Experimental results show that our algorithm achieves higher test accuracy and faster convergence compared to benchmark methods, improving the rate of convergence by 58.4% on CIFAR-10 and 49.2% on CIFAR-100 compared to random scheduling.</li>
</ul>

<h3>Title: PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Tang, Yi Yang, Ahmed Abbasi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07370">https://arxiv.org/abs/2509.07370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07370">https://arxiv.org/pdf/2509.07370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07370]] PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions(https://arxiv.org/abs/2509.07370)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) demonstrate remarkable capabilities across various fields. These developments have led to more direct communication between humans and LLMs in various situations, such as social companionship and psychological support. However, LLMs often exhibit limitations in emotional perception and social competence during real-world conversations. These limitations partly originate from their inability to adapt their communication style and emotional expression to different social and task contexts. In this work, we introduce PersonaFuse, a novel LLM post-training framework that enables LLMs to adapt and express different personalities for varying situations. Inspired by Trait Activation Theory and the Big Five personality model, PersonaFuse employs a Mixture-of-Expert architecture that combines persona adapters with a dynamic routing network, enabling contextual trait expression. Experimental results show that PersonaFuse substantially outperforms baseline models across multiple dimensions of social-emotional intelligence. Importantly, these gains are achieved without sacrificing general reasoning ability or model safety, which remain common limitations of direct prompting and supervised fine-tuning approaches. PersonaFuse also delivers consistent improvements in downstream human-centered applications, such as mental health counseling and review-based customer service. Finally, human preference evaluations against leading LLMs, including GPT-4o and DeepSeek, demonstrate that PersonaFuse achieves competitive response quality despite its comparatively smaller model size. These findings demonstrate that PersonaFuse~offers a theoretically grounded and practical approach for developing social-emotional enhanced LLMs, marking a significant advancement toward more human-centric AI systems.</li>
</ul>

<h3>Title: Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents</h3>
<ul>
<li><strong>Authors: </strong>Sankalp Tattwadarshi Swain, Anshika Krishnatray, Dhruv Kumar, Jagat Sesh Challa</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07389">https://arxiv.org/abs/2509.07389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07389">https://arxiv.org/pdf/2509.07389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07389]] Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents(https://arxiv.org/abs/2509.07389)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing evaluation studies on linguistic competence of large language models (LLM agents) have focused primarily on vocabulary learning, morphological rule induction, syntactic generalization, pragmatic inference, and cross-linguistic transfer. However, none assess whether LLM agents can acquire a language through pattern recognition and interactive feedback, a central feature of human language acquisition. We propose a novel experimental framework in which an LLM agent is evaluated on its ability to acquire and use a newly constructed language (Tinkatongue) in conversation with a bot that understands only Tinkatongue. Our findings show that LLM agents fail to establish a conversation within 100 responses, yet they adopt distinct strategies that mirror human approaches to language learning. The results suggest a new direction for evaluation benchmarks and open pathways to model designs that learn more effectively from interactive feedback.</li>
</ul>

<h3>Title: The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Yi-Jie Cheng, Oscar Chew, Yun-Nung Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07399">https://arxiv.org/abs/2509.07399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07399">https://arxiv.org/pdf/2509.07399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07399]] The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering(https://arxiv.org/abs/2509.07399)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Integrating knowledge graphs (KGs) into the reasoning processes of large language models (LLMs) has emerged as a promising approach to mitigate hallucination. However, existing work in this area often relies on proprietary or extremely large models, limiting accessibility and scalability. In this study, we investigate the capabilities of existing integration methods for small language models (SLMs) in KG-based question answering and observe that their performance is often constrained by their limited ability to traverse and reason over knowledge graphs. To address this limitation, we propose leveraging simple and efficient exploration modules to handle knowledge graph traversal in place of the language model itself. Experiment results demonstrate that these lightweight modules effectively improve the performance of small language models on knowledge graph question answering tasks. Source code: this https URL.</li>
</ul>

<h3>Title: LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction</h3>
<ul>
<li><strong>Authors: </strong>Weichu Liu, Jing Xiong, Yuxuan Hu, Zixuan Li, Minghuan Tan, Ningning Mao, Chenyang Zhao, Zhongwei Wan, Chaofan Tao, Wendong Xu, Hui Shen, Chengming Li, Lingpeng Kong, Ngai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07403">https://arxiv.org/abs/2509.07403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07403">https://arxiv.org/pdf/2509.07403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07403]] LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction(https://arxiv.org/abs/2509.07403)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) make significant progress in Emotional Intelligence (EI) and long-context understanding. However, existing benchmarks tend to overlook certain aspects of EI in long-context scenarios, especially under realistic, practical settings where interactions are lengthy, diverse, and often noisy. To move towards such realistic settings, we present LongEmotion, a benchmark specifically designed for long-context EI tasks. It covers a diverse set of tasks, including Emotion Classification, Emotion Detection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion Expression. On average, the input length for these tasks reaches 8,777 tokens, with long-form generation required for Emotion Expression. To enhance performance under realistic constraints, we incorporate Retrieval-Augmented Generation (RAG) and Collaborative Emotional Modeling (CoEM), and compare them with standard prompt-based methods. Unlike conventional approaches, our RAG method leverages both the conversation context and the large language model itself as retrieval sources, avoiding reliance on external knowledge bases. The CoEM method further improves performance by decomposing the task into five stages, integrating both retrieval augmentation and limited knowledge injection. Experimental results show that both RAG and CoEM consistently enhance EI-related performance across most long-context tasks, advancing LLMs toward more practical and real-world EI applications. Furthermore, we conducted a comparative case study experiment on the GPT series to demonstrate the differences among various models in terms of EI. Code is available on GitHub at this https URL, and the project page can be found at this https URL.</li>
</ul>

<h3>Title: EMORF-II: Adaptive EM-based Outlier-Robust Filtering with Correlated Measurement Noise</h3>
<ul>
<li><strong>Authors: </strong>Arslan Majal, Aamir Hussain Chughtai, Muhammad Tahir</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07415">https://arxiv.org/abs/2509.07415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07415">https://arxiv.org/pdf/2509.07415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07415]] EMORF-II: Adaptive EM-based Outlier-Robust Filtering with Correlated Measurement Noise(https://arxiv.org/abs/2509.07415)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a learning-based outlier-robust filter for a general setup where the measurement noise can be correlated. Since it is an enhanced version of EM-based outlier robust filter (EMORF), we call it as EMORF-II. As it is equipped with an additional powerful feature to learn the outlier characteristics during inference along with outlier-detection, EMORF-II has improved outlier-mitigation capability. Numerical experiments confirm performance gains as compared to the state-of-the-art methods in terms of accuracy with an increased computational overhead. However, thankfully the computational complexity order remains at par with other practical methods making it a useful choice for diverse applications.</li>
</ul>

<h3>Title: The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward</h3>
<ul>
<li><strong>Authors: </strong>Long Li, Jiaran Hao, Jason Klein Liu, Zhijian Zhou, Xiaoyu Tan, Wei Chu, Zhe Wang, Shirui Pan, Chao Qu, Yuan Qi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07430">https://arxiv.org/abs/2509.07430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07430">https://arxiv.org/pdf/2509.07430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07430]] The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward(https://arxiv.org/abs/2509.07430)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A central paradox in fine-tuning Large Language Models (LLMs) with Reinforcement Learning with Verifiable Reward (RLVR) is the frequent degradation of multi-attempt performance (Pass@k) despite improvements in single-attempt accuracy (Pass@1). This is often accompanied by catastrophic forgetting, where models lose previously acquired skills. While various methods have been proposed, the choice and function of the divergence term have been surprisingly unexamined as a proactive solution. We argue that standard RLVR objectives -- both those using the mode-seeking reverse KL-divergence and those forgoing a divergence term entirely -- lack a crucial mechanism for knowledge retention. The reverse-KL actively accelerates this decay by narrowing the policy, while its absence provides no safeguard against the model drifting from its diverse knowledge base. We propose a fundamental shift in perspective: using the divergence term itself as the solution. Our framework, Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences (like forward-KL and JS-divergence) to function as a rehearsal mechanism. By continuously referencing the initial policy, this approach forces the model to maintain broad solution coverage. Extensive experiments on math and SQL generation demonstrate that DPH-RL not only resolves the Pass@k degradation but improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is more training-efficient because it computes f-divergence using generator functions, requiring only sampling from the initial policy and no online reference model. Our work highlights a crucial, overlooked axis for improving RLVR, demonstrating that the proper selection of a divergence measure is a powerful tool for building more general and diverse reasoning models.</li>
</ul>

<h3>Title: DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation</h3>
<ul>
<li><strong>Authors: </strong>Ze-Xin Yin, Jiaxiong Qiu, Liu Liu, Xinjie Wang, Wei Sui, Zhizhong Su, Jian Yang, Jin Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07435">https://arxiv.org/abs/2509.07435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07435">https://arxiv.org/pdf/2509.07435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07435]] DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation(https://arxiv.org/abs/2509.07435)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The labor- and experience-intensive creation of 3D assets with physically based rendering (PBR) materials demands an autonomous 3D asset creation pipeline. However, most existing 3D generation methods focus on geometry modeling, either baking textures into simple vertex colors or leaving texture synthesis to post-processing with image diffusion models. To achieve end-to-end PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter (LGAA), a novel framework that unifies the modeling of geometry and PBR materials by exploiting multi-view (MV) diffusion priors from a novel perspective. The LGAA features a modular design with three components. Specifically, the LGAA Wrapper reuses and adapts network layers from MV diffusion models, which encapsulate knowledge acquired from billions of images, enabling better convergence in a data-efficient manner. To incorporate multiple diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated post-processing procedure to effectively extract high-quality, relightable mesh assets from the resulting 2DGS. Extensive quantitative and qualitative experiments demonstrate the superior performance of LGAA with both text-and image-conditioned MV diffusion models. Additionally, the modular design enables flexible incorporation of multiple diffusion priors, and the knowledge-preserving scheme leads to efficient convergence trained on merely 69k multi-view instances. Our code, pre-trained weights, and the dataset used will be publicly available via our project page: this https URL.</li>
</ul>

<h3>Title: In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting</h3>
<ul>
<li><strong>Authors: </strong>Taiying Peng, Jiacheng Hua, Miao Liu, Feng Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07447">https://arxiv.org/abs/2509.07447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07447">https://arxiv.org/pdf/2509.07447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07447]] In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting(https://arxiv.org/abs/2509.07447)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The emergence of advanced multimodal large language models (MLLMs) has significantly enhanced AI assistants' ability to process complex information across modalities. Recently, egocentric videos, by directly capturing user focus, actions, and context in an unified coordinate, offer an exciting opportunity to enable proactive and personalized AI user experiences with MLLMs. However, existing benchmarks overlook the crucial role of gaze as an indicator of user intent. To address this gap, we introduce EgoGazeVQA, an egocentric gaze-guided video question answering benchmark that leverages gaze information to improve the understanding of longer daily-life videos. EgoGazeVQA consists of gaze-based QA pairs generated by MLLMs and refined by human annotators. Our experiments reveal that existing MLLMs struggle to accurately interpret user intentions. In contrast, our gaze-guided intent prompting methods significantly enhance performance by integrating spatial, temporal, and intent-related cues. We further conduct experiments on gaze-related fine-tuning and analyze how gaze estimation accuracy impacts prompting effectiveness. These results underscore the value of gaze for more personalized and effective AI assistants in egocentric settings.</li>
</ul>

<h3>Title: GLEAM: Learning to Match and Explain in Cross-View Geo-Localization</h3>
<ul>
<li><strong>Authors: </strong>Xudong Lu, Zhi Zheng, Yi Wan, Yongxiang Yao, Annan Wang, Renrui Zhang, Panwang Xia, Qiong Wu, Qingyun Li, Weifeng Lin, Xiangyu Zhao, Xue Yang, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07450">https://arxiv.org/abs/2509.07450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07450">https://arxiv.org/pdf/2509.07450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07450]] GLEAM: Learning to Match and Explain in Cross-View Geo-Localization(https://arxiv.org/abs/2509.07450)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Cross-View Geo-Localization (CVGL) focuses on identifying correspondences between images captured from distinct perspectives of the same geographical location. However, existing CVGL approaches are typically restricted to a single view or modality, and their direct visual matching strategy lacks interpretability: they merely predict whether two images correspond, without explaining the rationale behind the match. In this paper, we present GLEAM-C, a foundational CVGL model that unifies multiple views and modalities-including UAV imagery, street maps, panoramic views, and ground photographs-by aligning them exclusively with satellite imagery. Our framework enhances training efficiency through optimized implementation while achieving accuracy comparable to prior modality-specific CVGL models through a two-phase training strategy. Moreover, to address the lack of interpretability in traditional CVGL methods, we leverage the reasoning capabilities of multimodal large language models (MLLMs) to propose a new task, GLEAM-X, which combines cross-view correspondence prediction with explainable reasoning. To support this task, we construct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Pro to generate training and testing data. The test set is further refined through detailed human revision, enabling systematic evaluation of explainable cross-view reasoning and advancing transparency and scalability in geo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGL pipeline that integrates multi-modal, multi-view alignment with interpretable correspondence analysis, unifying accurate cross-view matching with explainable reasoning and advancing Geo-Localization by enabling models to better Explain And Match. Code and datasets used in this work will be made publicly accessible at this https URL.</li>
</ul>

<h3>Title: XOCT: Enhancing OCT to OCTA Translation via Cross-Dimensional Supervised Multi-Scale Feature Learning</h3>
<ul>
<li><strong>Authors: </strong>Pooya Khosravi, Kun Han, Anthony T. Wu, Arghavan Rezvani, Zexin Feng, Xiaohui Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07455">https://arxiv.org/abs/2509.07455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07455">https://arxiv.org/pdf/2509.07455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07455]] XOCT: Enhancing OCT to OCTA Translation via Cross-Dimensional Supervised Multi-Scale Feature Learning(https://arxiv.org/abs/2509.07455)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Optical Coherence Tomography Angiography (OCTA) and its derived en-face projections provide high-resolution visualization of the retinal and choroidal vasculature, which is critical for the rapid and accurate diagnosis of retinal diseases. However, acquiring high-quality OCTA images is challenging due to motion sensitivity and the high costs associated with software modifications for conventional OCT devices. Moreover, current deep learning methods for OCT-to-OCTA translation often overlook the vascular differences across retinal layers and struggle to reconstruct the intricate, dense vascular details necessary for reliable diagnosis. To overcome these limitations, we propose XOCT, a novel deep learning framework that integrates Cross-Dimensional Supervision (CDS) with a Multi-Scale Feature Fusion (MSFF) network for layer-aware vascular reconstruction. Our CDS module leverages 2D layer-wise en-face projections, generated via segmentation-weighted z-axis averaging, as supervisory signals to compel the network to learn distinct representations for each retinal layer through fine-grained, targeted guidance. Meanwhile, the MSFF module enhances vessel delineation through multi-scale feature extraction combined with a channel reweighting strategy, effectively capturing vascular details at multiple spatial scales. Our experiments on the OCTA-500 dataset demonstrate XOCT's improvements, especially for the en-face projections which are significant for clinical evaluation of retinal pathologies, underscoring its potential to enhance OCTA accessibility, reliability, and diagnostic value for ophthalmic disease detection and monitoring. The code is available at this https URL.</li>
</ul>

<h3>Title: Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting</h3>
<ul>
<li><strong>Authors: </strong>Sai Siddhartha Chary Aylapuram, Veeraraju Elluru, Shivang Agarwal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07456">https://arxiv.org/abs/2509.07456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07456">https://arxiv.org/pdf/2509.07456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07456]] Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting(https://arxiv.org/abs/2509.07456)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair</a></li>
<li><strong>Abstract: </strong>Deep neural networks often rely on spurious correlations in training data, leading to biased or unfair predictions in safety-critical domains such as medicine and autonomous driving. While conventional bias mitigation typically requires retraining from scratch or redesigning data pipelines, recent advances in machine unlearning provide a promising alternative for post-hoc model correction. In this work, we investigate \textit{Bias-Aware Machine Unlearning}, a paradigm that selectively removes biased samples or feature representations to mitigate diverse forms of bias in vision models. Building on privacy-preserving unlearning techniques, we evaluate various strategies including Gradient Ascent, LoRA, and Teacher-Student distillation. Through empirical analysis on three benchmark datasets, CUB-200-2011 (pose bias), CIFAR-10 (synthetic patch bias), and CelebA (gender bias in smile detection), we demonstrate that post-hoc unlearning can substantially reduce subgroup disparities, with improvements in demographic parity of up to \textbf{94.86\%} on CUB-200, \textbf{30.28\%} on CIFAR-10, and \textbf{97.37\%} on CelebA. These gains are achieved with minimal accuracy loss and with methods scoring an average of 0.62 across the 3 settings on the joint evaluation of utility, fairness, quality, and privacy. Our findings establish machine unlearning as a practical framework for enhancing fairness in deployed vision systems without necessitating full retraining.</li>
</ul>

<h3>Title: A Decade-long Landscape of Advanced Persistent Threats: Longitudinal Analysis and Global Trends</h3>
<ul>
<li><strong>Authors: </strong>Shakhzod Yuldoshkhujaev (1), Mijin Jeon (1), Doowon Kim (2), Nick Nikiforakis (3), Hyungjoon Koo (1) ((1) Sungkyunkwan University, (2) University of Tennessee, (3) Stony Brook University)</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07457">https://arxiv.org/abs/2509.07457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07457">https://arxiv.org/pdf/2509.07457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07457]] A Decade-long Landscape of Advanced Persistent Threats: Longitudinal Analysis and Global Trends(https://arxiv.org/abs/2509.07457)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>An advanced persistent threat (APT) refers to a covert, long-term cyberattack, typically conducted by state-sponsored actors, targeting critical sectors and often remaining undetected for long periods. In response, collective intelligence from around the globe collaborates to identify and trace surreptitious activities, generating substantial documentation on APT campaigns publicly available on the web. While prior works predominantly focus on specific aspects of APT cases, such as detection, evaluation, cyber threat intelligence, and dataset creation, limited attention has been devoted to revisiting and investigating these scattered dossiers in a longitudinal manner. The objective of our study is to fill the gap by offering a macro perspective, connecting key insights and global trends in past APT attacks. We systematically analyze six reliable sources-three focused on technical reports and another three on threat actors-examining 1,509 APT dossiers (24,215 pages) spanning 2014-2023, and identifying 603 unique APT groups worldwide. To efficiently unearth relevant information, we employ a hybrid methodology that combines rule-based information retrieval with large-language-model-based search techniques. Our longitudinal analysis reveals shifts in threat actor activities, global attack vectors, changes in targeted sectors, and relationships between cyberattacks and significant events such as elections or wars, which provide insights into historical patterns in APT evolution. Over the past decade, 154 countries have been affected, primarily using malicious documents and spear phishing as dominant initial infiltration vectors, with a noticeable decline in zero-day exploitation since 2016. Furthermore, we present our findings through interactive visualization tools, such as an APT map or flow diagram, to facilitate intuitive understanding of global patterns and trends in APT activities.</li>
</ul>

<h3>Title: Biometric Bound Credentials for Age Verification</h3>
<ul>
<li><strong>Authors: </strong>Norman Poh, Daryl Burns</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07465">https://arxiv.org/abs/2509.07465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07465">https://arxiv.org/pdf/2509.07465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07465]] Biometric Bound Credentials for Age Verification(https://arxiv.org/abs/2509.07465)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, biometric, fair</a></li>
<li><strong>Abstract: </strong>Age verification is increasingly critical for regulatory compliance, user trust, and the protection of minors online. Historically, solutions have struggled with poor accuracy, intrusiveness, and significant security risks. More recently, concerns have shifted toward privacy, surveillance, fairness, and the need for transparent, trustworthy systems. In this paper, we propose Biometric Bound Credentials (BBCreds) as a privacy-preserving approach that cryptographically binds age credentials to an individual's biometric features without storing biometric templates. This ensures only the legitimate, physically present user can access age-restricted services, prevents credential sharing, and addresses both legacy and emerging challenges in age verification. enhances privacy.</li>
</ul>

<h3>Title: From Scarcity to Efficiency: Investigating the Effects of Data Augmentation on African Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Mardiyyah Oduwole, Oluwatosin Olajide, Jamiu Suleiman, Faith Hunja, Busayo Awobade, Fatimo Adebanjo, Comfort Akanni, Chinonyelum Igwe, Peace Ododo, Promise Omoigui, Steven Kolawole, Abraham Owodunni</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07471">https://arxiv.org/abs/2509.07471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07471">https://arxiv.org/pdf/2509.07471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07471]] From Scarcity to Efficiency: Investigating the Effects of Data Augmentation on African Machine Translation(https://arxiv.org/abs/2509.07471)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The linguistic diversity across the African continent presents different challenges and opportunities for machine translation. This study explores the effects of data augmentation techniques in improving translation systems in low-resource African languages. We focus on two data augmentation techniques: sentence concatenation with back translation and switch-out, applying them across six African languages. Our experiments show significant improvements in machine translation performance, with a minimum increase of 25\% in BLEU score across all six this http URL provide a comprehensive analysis and highlight the potential of these techniques to improve machine translation systems for low-resource languages, contributing to the development of more robust translation systems for under-resourced languages.</li>
</ul>

<h3>Title: ANYPORTAL: Zero-Shot Consistent Video Background Replacement</h3>
<ul>
<li><strong>Authors: </strong>Wenshuo Gao, Xicheng Lan, Shuai Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07472">https://arxiv.org/abs/2509.07472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07472">https://arxiv.org/pdf/2509.07472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07472]] ANYPORTAL: Zero-Shot Consistent Video Background Replacement(https://arxiv.org/abs/2509.07472)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite the rapid advancements in video generation technology, creating high-quality videos that precisely align with user intentions remains a significant challenge. Existing methods often fail to achieve fine-grained control over video details, limiting their practical applicability. We introduce ANYPORTAL, a novel zero-shot framework for video background replacement that leverages pre-trained diffusion models. Our framework collaboratively integrates the temporal prior of video diffusion models with the relighting capabilities of image diffusion models in a zero-shot setting. To address the critical challenge of foreground consistency, we propose a Refinement Projection Algorithm, which enables pixel-level detail manipulation to ensure precise foreground preservation. ANYPORTAL is training-free and overcomes the challenges of achieving foreground consistency and temporally coherent relighting. Experimental results demonstrate that ANYPORTAL achieves high-quality results on consumer-grade GPUs, offering a practical and efficient solution for video content creation and editing.</li>
</ul>

<h3>Title: HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention</h3>
<ul>
<li><strong>Authors: </strong>Saumya Goswami, Siddharth Kurra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07475">https://arxiv.org/abs/2509.07475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07475">https://arxiv.org/pdf/2509.07475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07475]] HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention(https://arxiv.org/abs/2509.07475)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Detecting content that contradicts or is unsupported by a given source text is a critical challenge for the safe deployment of generative language models. We introduce HALT-RAG, a post-hoc verification system designed to identify hallucinations in the outputs of Retrieval-Augmented Generation (RAG) pipelines. Our flexible and task-adaptable framework uses a universal feature set derived from an ensemble of two frozen, off-the-shelf Natural Language Inference (NLI) models and lightweight lexical signals. These features are used to train a simple, calibrated, and task-adapted meta-classifier. Using a rigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and produce unbiased estimates, we evaluate our system on the HaluEval benchmark. By pairing our universal feature set with a lightweight, task-adapted classifier and a precision-constrained decision policy, HALT-RAG achieves strong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA, and dialogue tasks, respectively. The system's well-calibrated probabilities enable a practical abstention mechanism, providing a reliable tool for balancing model performance with safety requirements.</li>
</ul>

<h3>Title: MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification</h3>
<ul>
<li><strong>Authors: </strong>Patrick Wienholt, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07477">https://arxiv.org/abs/2509.07477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07477">https://arxiv.org/pdf/2509.07477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07477]] MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification(https://arxiv.org/abs/2509.07477)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Deep neural networks excel in radiological image classification but frequently suffer from poor interpretability, limiting clinical acceptance. We present MedicalPatchNet, an inherently self-explainable architecture for chest X-ray classification that transparently attributes decisions to distinct image regions. MedicalPatchNet splits images into non-overlapping patches, independently classifies each patch, and aggregates predictions, enabling intuitive visualization of each patch's diagnostic contribution without post-hoc techniques. Trained on the CheXpert dataset (223,414 images), MedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908) of EfficientNet-B0, while substantially improving interpretability: MedicalPatchNet demonstrates substantially improved interpretability with higher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with Grad-CAM) on the CheXlocalize dataset. By providing explicit, reliable explanations accessible even to non-AI experts, MedicalPatchNet mitigates risks associated with shortcut learning, thus improving clinical trust. Our model is publicly available with reproducible training and inference scripts and contributes to safer, explainable AI-assisted diagnostics across medical imaging domains. We make the code publicly available: this https URL</li>
</ul>

<h3>Title: LINR Bridge: Vector Graphic Animation via Neural Implicits and Video Diffusion Priors</h3>
<ul>
<li><strong>Authors: </strong>Wenshuo Gao, Xicheng Lan, Luyao Zhang, Shuai Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07484">https://arxiv.org/abs/2509.07484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07484">https://arxiv.org/pdf/2509.07484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07484]] LINR Bridge: Vector Graphic Animation via Neural Implicits and Video Diffusion Priors(https://arxiv.org/abs/2509.07484)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Vector graphics, known for their scalability and user-friendliness, provide a unique approach to visual content compared to traditional pixel-based images. Animation of these graphics, driven by the motion of their elements, offers enhanced comprehensibility and controllability but often requires substantial manual effort. To automate this process, we propose a novel method that integrates implicit neural representations with text-to-video diffusion models for vector graphic animation. Our approach employs layered implicit neural representations to reconstruct vector graphics, preserving their inherent properties such as infinite resolution and precise color and shape constraints, which effectively bridges the large domain gap between vector graphics and diffusion models. The neural representations are then optimized using video score distillation sampling, which leverages motion priors from pretrained text-to-video diffusion models. Finally, the vector graphics are warped to match the representations resulting in smooth animation. Experimental results validate the effectiveness of our method in generating vivid and natural vector graphic animations, demonstrating significant improvement over existing techniques that suffer from limitations in flexibility and animation quality.</li>
</ul>

<h3>Title: Generating Transferrable Adversarial Examples via Local Mixing and Logits Optimization for Remote Sensing Object Recognition</h3>
<ul>
<li><strong>Authors: </strong>Chun Liu, Hailong Wang, Bingqian Zhu, Panpan Ding, Zheng Zheng, Tao Xu, Zhigang Han, Jiayao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07495">https://arxiv.org/abs/2509.07495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07495">https://arxiv.org/pdf/2509.07495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07495]] Generating Transferrable Adversarial Examples via Local Mixing and Logits Optimization for Remote Sensing Object Recognition(https://arxiv.org/abs/2509.07495)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, posing significant security threats to their deployment in remote sensing applications. Research on adversarial attacks not only reveals model vulnerabilities but also provides critical insights for enhancing robustness. Although current mixing-based strategies have been proposed to increase the transferability of adversarial examples, they either perform global blending or directly exchange a region in the images, which may destroy global semantic features and mislead the optimization of adversarial examples. Furthermore, their reliance on cross-entropy loss for perturbation optimization leads to gradient diminishing during iterative updates, compromising adversarial example quality. To address these limitations, we focus on non-targeted attacks and propose a novel framework via local mixing and logits optimization. First, we present a local mixing strategy to generate diverse yet semantically consistent inputs. Different from MixUp, which globally blends two images, and MixCut, which stitches images together, our method merely blends local regions to preserve global semantic information. Second, we adapt the logit loss from targeted attacks to non-targeted scenarios, mitigating the gradient vanishing problem of cross-entropy loss. Third, a perturbation smoothing loss is applied to suppress high-frequency noise and enhance transferability. Extensive experiments on FGSCR-42 and MTARSI datasets demonstrate superior performance over 12 state-of-the-art methods across 6 surrogate models. Notably, with ResNet as the surrogate on MTARSI, our method achieves a 17.28% average improvement in black-box attack success rate.</li>
</ul>

<h3>Title: Conv4Rec: A 1-by-1 Convolutional AutoEncoder for User Profiling through Joint Analysis of Implicit and Explicit Feedbacks</h3>
<ul>
<li><strong>Authors: </strong>Antoine Ledent, Petr Kasalický, Rodrigo Alves, Hady W. Lauw</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07499">https://arxiv.org/abs/2509.07499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07499">https://arxiv.org/pdf/2509.07499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07499]] Conv4Rec: A 1-by-1 Convolutional AutoEncoder for User Profiling through Joint Analysis of Implicit and Explicit Feedbacks(https://arxiv.org/abs/2509.07499)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We introduce a new convolutional AutoEncoder architecture for user modelling and recommendation tasks with several improvements over the state of the art. Firstly, our model has the flexibility to learn a set of associations and combinations between different interaction types in a way that carries over to each user and item. Secondly, our model is able to learn jointly from both the explicit ratings and the implicit information in the sampling pattern (which we refer to as `implicit feedback'). It can also make separate predictions for the probability of consuming content and the likelihood of granting it a high rating if observed. This not only allows the model to make predictions for both the implicit and explicit feedback, but also increases the informativeness of the predictions: in particular, our model can identify items which users would not have been likely to consume naturally, but would be likely to enjoy if exposed to them. Finally, we provide several generalization bounds for our model, which to the best of our knowledge, are among the first generalization bounds for auto-encoders in a Recommender Systems setting; we also show that optimizing our loss function guarantees the recovery of the exact sampling distribution over interactions up to a small error in total variation. In experiments on several real-life datasets, we achieve state-of-the-art performance on both the implicit and explicit feedback prediction tasks despite relying on a single model for both, and benefiting from additional interpretability in the form of individual predictions for the probabilities of each possible rating.</li>
</ul>

<h3>Title: Backdoor Attacks and Defenses in Computer Vision Domain: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Bilal Hussain Abbasi, Yanjun Zhang, Leo Zhang, Shang Gao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07504">https://arxiv.org/abs/2509.07504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07504">https://arxiv.org/pdf/2509.07504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07504]] Backdoor Attacks and Defenses in Computer Vision Domain: A Survey(https://arxiv.org/abs/2509.07504)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, attack, segmentation</a></li>
<li><strong>Abstract: </strong>Backdoor (trojan) attacks embed hidden, controllable behaviors into machine-learning models so that models behave normally on benign inputs but produce attacker-chosen outputs when a trigger is present. This survey reviews the rapidly growing literature on backdoor attacks and defenses in the computer-vision domain. We introduce a multi-dimensional taxonomy that organizes attacks and defenses by injection stage (dataset poisoning, model/parameter modification, inference-time injection), trigger type (patch, blended/frequency, semantic, transformation), labeling strategy (dirty-label vs. clean-label / feature-collision), representation stage (instance-specific, manifold/class-level, neuron/parameter hijacking, distributed encodings), and target task (classification, detection, segmentation, video, multimodal). For each axis we summarize representative methods, highlight evaluation practices, and discuss where defenses succeed or fail. For example, many classical sanitization and reverse-engineering tools are effective against reusable patch attacks but struggle with input-aware, sample-specific, or parameter-space backdoors and with transfer via compromised pre-trained encoders or hardware bit-flips. We synthesize trends, identify persistent gaps (supply-chain and hardware threats, certifiable defenses, cross-task benchmarks), and propose practical guidelines for threat-aware evaluation and layered defenses. This survey aims to orient researchers and practitioners to the current threat landscape and pressing research directions in secure computer vision.</li>
</ul>

<h3>Title: Extension of Spatial k-Anonymity: New Metrics for Assessing the Anonymity of Geomasked Data Considering Realistic Attack Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Simon Cremer, Lydia Jehmlich, Rainer Lenz</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07505">https://arxiv.org/abs/2509.07505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07505">https://arxiv.org/pdf/2509.07505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07505]] Extension of Spatial k-Anonymity: New Metrics for Assessing the Anonymity of Geomasked Data Considering Realistic Attack Scenarios(https://arxiv.org/abs/2509.07505)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>Spatial data are gaining increasing importance in many areas of research. Particularly spatial health data are becoming increasingly important for medical research, for example, to better understand relationships between environmental factors and disease patterns. However, their use is often restricted by legal data protection regulations, since georeferenced personal information carries a high risk of re-identification of individuals. To address this issue, what are called geomasking methods are applied to guarantee data protection through targeted displacement of individual data points, while simultaneously maintaining analytical validity within a tolerable range. In the current literature the degree of anonymity of such anonymized georeferenced datasets is often measured by the so-called metric of spatial k-anonymity. However, this metric has considerable shortcomings, particularly regarding its resilience against realistic data attack scenarios. This article classifies the potential data attack scenarios in the context of anonymized georeferenced microdata and introduces appropriate metrics that enable a comprehensive assessment of anonymity adapted to potential data attack scenarios.</li>
</ul>

<h3>Title: ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Zihan Chen, Lei Shi, Weize Wu, Qiji Zhou, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07512">https://arxiv.org/abs/2509.07512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07512">https://arxiv.org/pdf/2509.07512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07512]] ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval(https://arxiv.org/abs/2509.07512)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Many contemporary data-driven research efforts in the natural sciences, such as chemistry and materials science, require large-scale, high-performance entity recognition from scientific datasets. Large language models (LLMs) have increasingly been adopted to solve the entity recognition task, with the same trend being observed on all-spectrum NLP tasks. The prevailing entity recognition LLMs rely on fine-tuned technology, yet the fine-tuning process often incurs significant cost. To achieve a best performance-cost trade-off, we propose ALLabel, a three-stage framework designed to select the most informative and representative samples in preparing the demonstrations for LLM modeling. The annotated examples are used to construct a ground-truth retrieval corpus for LLM in-context learning. By sequentially employing three distinct active learning strategies, ALLabel consistently outperforms all baselines under the same annotation budget across three specialized domain datasets. Experimental results also demonstrate that selectively annotating only 5\%-10\% of the dataset with ALLabel can achieve performance comparable to the method annotating the entire dataset. Further analyses and ablation studies verify the effectiveness and generalizability of our proposal.</li>
</ul>

<h3>Title: RoseCDL: Robust and Scalable Convolutional Dictionary Learning for Rare-event Detection</h3>
<ul>
<li><strong>Authors: </strong>Jad Yehya, Mansour Benbakoura, Cédric Allain, Benoît Malezieux, Matthieu Kowalski, Thomas Moreau</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07523">https://arxiv.org/abs/2509.07523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07523">https://arxiv.org/pdf/2509.07523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07523]] RoseCDL: Robust and Scalable Convolutional Dictionary Learning for Rare-event Detection(https://arxiv.org/abs/2509.07523)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Identifying recurring patterns and rare events in large-scale signals is a fundamental challenge in fields such as astronomy, physical simulations, and biomedical science. Convolutional Dictionary Learning (CDL) offers a powerful framework for modeling local structures in signals, but its use for detecting rare or anomalous events remains largely unexplored. In particular, CDL faces two key challenges in this setting: high computational cost and sensitivity to artifacts and outliers. In this paper, we introduce RoseCDL, a scalable and robust CDL algorithm designed for unsupervised rare event detection in long signals. RoseCDL combines stochastic windowing for efficient training on large datasets with inline outlier detection to enhance robustness and isolate anomalous patterns. This reframes CDL as a practical tool for event discovery and characterization in real-world signals, extending its role beyond traditional tasks like compression or denoising.</li>
</ul>

<h3>Title: Universal Few-Shot Spatial Control for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kiet T. Nguyen, Chanhuyk Lee, Donggyun Kim, Dong Hoon Lee, Seunghoon Hong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07530">https://arxiv.org/abs/2509.07530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07530">https://arxiv.org/pdf/2509.07530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07530]] Universal Few-Shot Spatial Control for Diffusion Models(https://arxiv.org/abs/2509.07530)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Spatial conditioning in pretrained text-to-image diffusion models has significantly improved fine-grained control over the structure of generated images. However, existing control adapters exhibit limited adaptability and incur high training costs when encountering novel spatial control conditions that differ substantially from the training tasks. To address this limitation, we propose Universal Few-Shot Control (UFC), a versatile few-shot control adapter capable of generalizing to novel spatial conditions. Given a few image-condition pairs of an unseen task and a query condition, UFC leverages the analogy between query and support conditions to construct task-specific control features, instantiated by a matching mechanism and an update on a small set of task-specific parameters. Experiments on six novel spatial control tasks show that UFC, fine-tuned with only 30 annotated examples of novel tasks, achieves fine-grained control consistent with the spatial conditions. Notably, when fine-tuned with 0.1% of the full training data, UFC achieves competitive performance with the fully supervised baselines in various control tasks. We also show that UFC is applicable agnostically to various diffusion backbones and demonstrate its effectiveness on both UNet and DiT architectures. Code is available at this https URL.</li>
</ul>

<h3>Title: HU-based Foreground Masking for 3D Medical Masked Image Modeling</h3>
<ul>
<li><strong>Authors: </strong>Jin Lee, Vu Dang, Gwang-Hyun Yu, Anh Le, Zahid Rahman, Jin-Ho Jang, Heonzoo Lee, Kun-Yung Kim, Jin-Sul Kim, Jin-Young Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07534">https://arxiv.org/abs/2509.07534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07534">https://arxiv.org/pdf/2509.07534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07534]] HU-based Foreground Masking for 3D Medical Masked Image Modeling(https://arxiv.org/abs/2509.07534)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>While Masked Image Modeling (MIM) has revolutionized fields of computer vision, its adoption in 3D medical image computing has been limited by the use of random masking, which overlooks the density of anatomical objects. To address this limitation, we enhance the pretext task with a simple yet effective masking strategy. Leveraging Hounsfield Unit (HU) measurements, we implement an HU-based Foreground Masking, which focuses on the intensity distribution of visceral organs and excludes non-tissue regions, such as air and fluid, that lack diagnostically meaningful features. Extensive experiments on five public 3D medical imaging datasets demonstrate that our masking consistently improves performance, both in quality of segmentation and Dice score (BTCV:~84.64\%, Flare22:~92.43\%, MM-WHS:~90.67\%, Amos22:~88.64\%, BraTS:~78.55\%). These results underscore the importance of domain-centric MIM and suggest a promising direction for representation learning in medical image segmentation. Implementation is available at this http URL.</li>
</ul>

<h3>Title: PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from One-shot Unposed Image</h3>
<ul>
<li><strong>Authors: </strong>Peng Li, Yisheng He, Yingdong Hu, Yuan Dong, Weihao Yuan, Yuan Liu, Zilong Dong, Yike Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07552">https://arxiv.org/abs/2509.07552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07552">https://arxiv.org/pdf/2509.07552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07552]] PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from One-shot Unposed Image(https://arxiv.org/abs/2509.07552)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>We present a feed-forward framework for Gaussian full-head synthesis from a single unposed image. Unlike previous work that relies on time-consuming GAN inversion and test-time optimization, our framework can reconstruct the Gaussian full-head model given a single unposed image in a single forward pass. This enables fast reconstruction and rendering during inference. To mitigate the lack of large-scale 3D head assets, we propose a large-scale synthetic dataset from trained 3D GANs and train our framework using only synthetic data. For efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian head generation pipeline, where sparse points from the FLAME model interact with the image features by transformer blocks for feature extraction and coarse shape reconstruction, which are then densified for high-fidelity reconstruction. To fully leverage the prior knowledge residing in pretrained 3D GANs for effective reconstruction, we propose a dual-branch framework that effectively aggregates the structured spherical triplane feature and unstructured point-based features for more effective Gaussian head reconstruction. Experimental results show the effectiveness of our framework towards existing work.</li>
</ul>

<h3>Title: VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents</h3>
<ul>
<li><strong>Authors: </strong>Zheng Wu, Heyuan Huang, Xingyu Lou, Xiangmou Qu, Pengzhou Cheng, Zongru Wu, Weiwen Liu, Weinan Zhang, Jun Wang, Zhaoxiang Wang, Zhuosheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07553">https://arxiv.org/abs/2509.07553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07553">https://arxiv.org/pdf/2509.07553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07553]] VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents(https://arxiv.org/abs/2509.07553)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid progress of multimodal large language models, operating system (OS) agents become increasingly capable of automating tasks through on-device graphical user interfaces (GUIs). However, most existing OS agents are designed for idealized settings, whereas real-world environments often present untrustworthy conditions. To mitigate risks of over-execution in such scenarios, we propose a query-driven human-agent-GUI interaction framework that enables OS agents to decide when to query humans for more reliable task completion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy OS agent trained with a two-stage learning paradigm that falicitate the decoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent autonomously executes actions in normal conditions while proactively querying humans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves the average step-wise success rate by 20.64\% in untrustworthy scenarios over the state-of-the-art, without compromising normal performance. Analysis highlights VeriOS-Agent's rationality, generalizability, and scalability. The codes, datasets and models are available at this https URL.</li>
</ul>

<h3>Title: Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Yi Liu, Xiangrong Zhu, Xiangyu Liu, Wei Wei, Wei Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07555">https://arxiv.org/abs/2509.07555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07555">https://arxiv.org/pdf/2509.07555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07555]] Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition(https://arxiv.org/abs/2509.07555)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In a rapidly evolving world where information updates swiftly, knowledge in large language models (LLMs) becomes outdated quickly. Retraining LLMs is not a cost-effective option, making knowledge editing (KE) without modifying parameters particularly necessary. We find that although existing retrieval-augmented generation (RAG)-based KE methods excel at editing simple knowledge, they struggle with KE in multi-hop question answering due to the issue of "edit skipping", which refers to skipping the relevant edited fact in inference. In addition to the diversity of natural language expressions of knowledge, edit skipping also arises from the mismatch between the granularity of LLMs in problem-solving and the facts in the edited memory. To address this issue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing method with guided decomposition (IRAKE) through the guidance from single edited facts and entire edited cases. Experimental results demonstrate that IRAKE mitigates the failure of editing caused by edit skipping and outperforms state-of-the-art methods for KE in multi-hop question answering.</li>
</ul>

<h3>Title: $ΔL$ Normalization: Rethink Loss Aggregation in RLVR</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan He, Xufang Luo, Yike Zhang, Yuqing Yang, Lili Qiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07558">https://arxiv.org/abs/2509.07558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07558">https://arxiv.org/pdf/2509.07558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07558]] $ΔL$ Normalization: Rethink Loss Aggregation in RLVR(https://arxiv.org/abs/2509.07558)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose $\Delta L$ Normalization, a simple yet effective loss aggregation method tailored to the characteristic of dynamic generation lengths in Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has demonstrated strong potential in improving the reasoning capabilities of large language models (LLMs), but a major challenge lies in the large variability of response lengths during training, which leads to high gradient variance and unstable optimization. Although previous methods such as GRPO, DAPO, and Dr. GRPO introduce different loss normalization terms to address this issue, they either produce biased estimates or still suffer from high gradient variance. By analyzing the effect of varying lengths on policy loss both theoretically and empirically, we reformulate the problem as finding a minimum-variance unbiased estimator. Our proposed $\Delta L$ Normalization not only provides an unbiased estimate of the true policy loss but also minimizes gradient variance in theory. Extensive experiments show that it consistently achieves superior results across different model sizes, maximum lengths, and tasks. Our code will be made public at this https URL.</li>
</ul>

<h3>Title: uGMM-NN: Univariate Gaussian Mixture Model Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Zakeria Sharif Ali</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07569">https://arxiv.org/abs/2509.07569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07569">https://arxiv.org/pdf/2509.07569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07569]] uGMM-NN: Univariate Gaussian Mixture Model Neural Network(https://arxiv.org/abs/2509.07569)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces the Univariate Gaussian Mixture Model Neural Network (uGMM-NN), a novel neural architecture that embeds probabilistic reasoning directly into the computational units of deep networks. Unlike traditional neurons, which apply weighted sums followed by fixed nonlinearities, each uGMM-NN node parameterizes its activations as a univariate Gaussian mixture, with learnable means, variances, and mixing coefficients. This design enables richer representations by capturing multimodality and uncertainty at the level of individual neurons, while retaining the scalability of standard feedforward networks. We demonstrate that uGMM-NN can achieve competitive discriminative performance compared to conventional multilayer perceptrons, while additionally offering a probabilistic interpretation of activations. The proposed framework provides a foundation for integrating uncertainty-aware components into modern neural architectures, opening new directions for both discriminative and generative modeling.</li>
</ul>

<h3>Title: Homogenization with Guaranteed Bounds via Primal-Dual Physically Informed Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Liya Gaynutdinova, Martin Doškář, Ondřej Rokoš, Ivana Pultarová</a></li>
<li><strong>Subjects: </strong>cs.LG, math.AP, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07579">https://arxiv.org/abs/2509.07579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07579">https://arxiv.org/pdf/2509.07579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07579]] Homogenization with Guaranteed Bounds via Primal-Dual Physically Informed Neural Networks(https://arxiv.org/abs/2509.07579)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Physics-informed neural networks (PINNs) have shown promise in solving partial differential equations (PDEs) relevant to multiscale modeling, but they often fail when applied to materials with discontinuous coefficients, such as media with piecewise constant properties. This paper introduces a dual formulation for the PINN framework to improve the reliability of the homogenization of periodic thermo-conductive composites, for both strong and variational (weak) formulations. The dual approach facilitates the derivation of guaranteed upper and lower error bounds, enabling more robust detection of PINN failure. We compare standard PINNs applied to smoothed material approximations with variational PINNs (VPINNs) using both spectral and neural network-based test functions. Our results indicate that while strong-form PINNs may outperform VPINNs in controlled settings, they are sensitive to material discontinuities and may fail without clear diagnostics. In contrast, VPINNs accommodate piecewise constant material parameters directly but require careful selection of test functions to avoid instability. Dual formulation serves as a reliable indicator of convergence quality, and its integration into PINN frameworks enhances their applicability to homogenization problems in micromechanics.</li>
</ul>

<h3>Title: Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Yusuke Hirota, Ryo Hachiuma, Boyi Li, Ximing Lu, Michael Ross Boone, Boris Ivanovic, Yejin Choi, Marco Pavone, Yu-Chiang Frank Wang, Noa Garcia, Yuta Nakashima, Chao-Han Huck Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07596">https://arxiv.org/abs/2509.07596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07596">https://arxiv.org/pdf/2509.07596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07596]] Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation(https://arxiv.org/abs/2509.07596)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Gender bias in vision-language foundation models (VLMs) raises concerns about their safe deployment and is typically evaluated using benchmarks with gender annotations on real-world images. However, as these benchmarks often contain spurious correlations between gender and non-gender features, such as objects and backgrounds, we identify a critical oversight in gender bias evaluation: Do spurious features distort gender bias evaluation? To address this question, we systematically perturb non-gender features across four widely used benchmarks (COCO-gender, FACET, MIAP, and PHASE) and various VLMs to quantify their impact on bias evaluation. Our findings reveal that even minimal perturbations, such as masking just 10% of objects or weakly blurring backgrounds, can dramatically alter bias scores, shifting metrics by up to 175% in generative VLMs and 43% in CLIP variants. This suggests that current bias evaluations often reflect model responses to spurious features rather than gender bias, undermining their reliability. Since creating spurious feature-free benchmarks is fundamentally challenging, we recommend reporting bias metrics alongside feature-sensitivity measurements to enable a more reliable bias assessment.</li>
</ul>

<h3>Title: Transformer-Based Approach to Optimal Sensor Placement for Structural Health Monitoring of Probe Cards</h3>
<ul>
<li><strong>Authors: </strong>Mehdi Bejani, Marco Mauri, Daniele Acconcia, Simone Todaro, Stefano Mariani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07603">https://arxiv.org/abs/2509.07603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07603">https://arxiv.org/pdf/2509.07603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07603]] Transformer-Based Approach to Optimal Sensor Placement for Structural Health Monitoring of Probe Cards(https://arxiv.org/abs/2509.07603)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>This paper presents an innovative Transformer-based deep learning strategy for optimizing the placement of sensors aiming at structural health monitoring of semiconductor probe cards. Failures in probe cards, including substrate cracks and loosened screws, would critically affect semiconductor manufacturing yield and reliability. Some failure modes could be detected by equipping a probe card with adequate sensors. Frequency response functions from simulated failure scenarios are adopted within a finite element model of a probe card. A comprehensive dataset, enriched by physics-informed scenario expansion and physics-aware statistical data augmentation, is exploited to train a hybrid Convolutional Neural Network and Transformer model. The model achieves high accuracy (99.83%) in classifying the probe card health states (baseline, loose screw, crack) and an excellent crack detection recall (99.73%). Model robustness is confirmed through a rigorous framework of 3 repetitions of 10-fold stratified cross-validation. The attention mechanism also pinpoints critical sensor locations: an analysis of the attention weights offers actionable insights for designing efficient, cost-effective monitoring systems by optimizing sensor configurations. This research highlights the capability of attention-based deep learning to advance proactive maintenance, enhancing operational reliability and yield in semiconductor manufacturing.</li>
</ul>

<h3>Title: Beyond Rebalancing: Benchmarking Binary Classifiers Under Class Imbalance Without Rebalancing Techniques</h3>
<ul>
<li><strong>Authors: </strong>Ali Nawaz, Amir Ahmad, Shehroz S. Khan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07605">https://arxiv.org/abs/2509.07605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07605">https://arxiv.org/pdf/2509.07605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07605]] Beyond Rebalancing: Benchmarking Binary Classifiers Under Class Imbalance Without Rebalancing Techniques(https://arxiv.org/abs/2509.07605)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Class imbalance poses a significant challenge to supervised classification, particularly in critical domains like medical diagnostics and anomaly detection where minority class instances are rare. While numerous studies have explored rebalancing techniques to address this issue, less attention has been given to evaluating the performance of binary classifiers under imbalance when no such techniques are applied. Therefore, the goal of this study is to assess the performance of binary classifiers "as-is", without performing any explicit rebalancing. Specifically, we systematically evaluate the robustness of a diverse set of binary classifiers across both real-world and synthetic datasets, under progressively reduced minority class sizes, using one-shot and few-shot scenarios as baselines. Our approach also explores varying data complexities through synthetic decision boundary generation to simulate real-world conditions. In addition to standard classifiers, we include experiments using undersampling, oversampling strategies, and one-class classification (OCC) methods to examine their behavior under severe imbalance. The results confirm that classification becomes more difficult as data complexity increases and the minority class size decreases. While traditional classifiers deteriorate under extreme imbalance, advanced models like TabPFN and boosting-based ensembles retain relatively higher performance and better generalization compared to traditional classifiers. Visual interpretability and evaluation metrics further validate these findings. Our work offers valuable guidance on model selection for imbalanced learning, providing insights into classifier robustness without dependence on explicit rebalancing techniques.</li>
</ul>

<h3>Title: Enhanced cast-128 with adaptive s-box optimization via neural networks for image protection</h3>
<ul>
<li><strong>Authors: </strong>Fadhil Abbas Fadhil, Maryam Mahdi Alhusseini, Mohammad-Reza Feizi-Derakhshi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07606">https://arxiv.org/abs/2509.07606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07606">https://arxiv.org/pdf/2509.07606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07606]] Enhanced cast-128 with adaptive s-box optimization via neural networks for image protection(https://arxiv.org/abs/2509.07606)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack, robust</a></li>
<li><strong>Abstract: </strong>An improved CAST-128 encryption algorithm, which is done by implementing chaos-based adaptive S-box generation using Logistic sine Map (LSM), has been provided in this paper because of the increasing requirements of efficient and smart image encryption mechanisms. The study aims to address the drawbacks of static S-box models commonly used in traditional cryptographic systems, which are susceptible to linear and differential attacks. In the proposed scheme, the dynamic, non-linear, invertible, and highly cryptographic strength S-boxes are generated through a hybrid chaotic system that may have high non-linearity, strong and rigorous avalanche characteristics, and low differential uniformity. The process here is that the LSM is used to produce S-boxes having key-dependent parameters that are stuffed into the CAST-128 structure to encrypt the image in a block-wise manner. The performance of the encryption is assessed utilizing a set of standard grayscale images. The metrics that are used to evaluate the security are entropy, NPCR, UACI, PSNR, and histogram analysis. Outcomes indicate that randomness, resistance to statistical attacks, and country of encryption are significantly improved compared to the original CAST-128. The study is theoretically and practically relevant since it presents a lightweight S-box generation approach driven by chaos, which can increase the level of robustness of the image encryptions without enlisting machine learning. The system may be applied to secure communications, surveillance systems, and medical image protection on a real-time basis.</li>
</ul>

<h3>Title: FlexEmu: Towards Flexible MCU Peripheral Emulation (Extended Version)</h3>
<ul>
<li><strong>Authors: </strong>Chongqing Lei, Zhen Ling, Xiangyu Xu, Shaofeng Li, Guangchi Liu, Kai Dong, Junzhou Luo</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07615">https://arxiv.org/abs/2509.07615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07615">https://arxiv.org/pdf/2509.07615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07615]] FlexEmu: Towards Flexible MCU Peripheral Emulation (Extended Version)(https://arxiv.org/abs/2509.07615)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Microcontroller units (MCUs) are widely used in embedded devices due to their low power consumption and cost-effectiveness. MCU firmware controls these devices and is vital to the security of embedded systems. However, performing dynamic security analyses for MCU firmware has remained challenging due to the lack of usable execution environments -- existing dynamic analyses cannot run on physical devices (e.g., insufficient computational resources), while building emulators is costly due to the massive amount of heterogeneous hardware, especially peripherals. Our work is based on the insight that MCU peripherals can be modeled in a two-fold manner. At the structural level, peripherals have diverse implementations but we can use a limited set of primitives to abstract peripherals because their hardware implementations are based on common hardware concepts. At the semantic level, peripherals have diverse functionalities. However, we can use a single unified semantic model to describe the same kind of peripherals because they exhibit similar functionalities. Building on this, we propose FlexEmu, a flexible MCU peripheral emulation framework. Once semantic models are created, FlexEmu automatically extracts peripheral-specific details to instantiate models and generate emulators accordingly. We have successfully applied FlexEmu to model 12 kinds of MCU peripherals. Our evaluation on 90 firmware samples across 15 different MCU platforms shows that the automatically generated emulators can faithfully replicate hardware behaviors and achieve a 98.48% unit test passing rate, outperforming state-of-the-art approaches. To demonstrate the implications of FlexEmu on firmware security, we use the generated emulators to fuzz three popular RTOSes and uncover 10 previously unknown bugs.</li>
</ul>

<h3>Title: MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Libo Ren, Yee Man Ng, Lifeng Han</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07622">https://arxiv.org/abs/2509.07622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07622">https://arxiv.org/pdf/2509.07622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07622]] MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs(https://arxiv.org/abs/2509.07622)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Efficient communication between patients and clinicians plays an important role in shared decision-making. However, clinical reports are often lengthy and filled with clinical jargon, making it difficult for domain experts to identify important aspects in the document efficiently. This paper presents the methodology we applied in the MultiClinSUM shared task for summarising clinical case documents. We used an Iterative Self-Prompting technique on large language models (LLMs) by asking LLMs to generate task-specific prompts and refine them via example-based few-shot learning. Furthermore, we used lexical and embedding space metrics, ROUGE and BERT-score, to guide the model fine-tuning with epochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved ROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P, R, F1) from the official evaluation on 3,396 clinical case reports from various specialties extracted from open journals. The high BERTscore indicates that the model produced semantically equivalent output summaries compared to the references, even though the overlap at the exact lexicon level is lower, as reflected in the lower ROUGE scores. This work sheds some light on how perspective-aware ISP (PA-ISP) can be deployed for clinical report summarisation and support better communication between patients and clinicians.</li>
</ul>

<h3>Title: Self-Supervised Cross-Encoder for Neurodegenerative Disease Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Fangqi Cheng, Yingying Zhao, Xiaochen Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07623">https://arxiv.org/abs/2509.07623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07623">https://arxiv.org/pdf/2509.07623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07623]] Self-Supervised Cross-Encoder for Neurodegenerative Disease Diagnosis(https://arxiv.org/abs/2509.07623)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Deep learning has shown significant potential in diagnosing neurodegenerative diseases from MRI data. However, most existing methods rely heavily on large volumes of labeled data and often yield representations that lack interpretability. To address both challenges, we propose a novel self-supervised cross-encoder framework that leverages the temporal continuity in longitudinal MRI scans for supervision. This framework disentangles learned representations into two components: a static representation, constrained by contrastive learning, which captures stable anatomical features; and a dynamic representation, guided by input-gradient regularization, which reflects temporal changes and can be effectively fine-tuned for downstream classification tasks. Experimental results on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset demonstrate that our method achieves superior classification accuracy and improved interpretability. Furthermore, the learned representations exhibit strong zero-shot generalization on the Open Access Series of Imaging Studies (OASIS) dataset and cross-task generalization on the Parkinson Progression Marker Initiative (PPMI) dataset. The code for the proposed method will be made publicly available.</li>
</ul>

<h3>Title: Embedded Off-Switches for AI Compute</h3>
<ul>
<li><strong>Authors: </strong>James Petrie</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07637">https://arxiv.org/abs/2509.07637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07637">https://arxiv.org/pdf/2509.07637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07637]] Embedded Off-Switches for AI Compute(https://arxiv.org/abs/2509.07637)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>To address the risks of increasingly capable AI systems, we introduce a hardware-level off-switch that embeds thousands of independent "security blocks" in each AI accelerator. This massively redundant architecture is designed to prevent unauthorized chip use, even against sophisticated physical attacks. Our main security block design uses public key cryptography to check the authenticity of authorization licenses, and randomly generated nonces to prevent replay attacks. We evaluate attack vectors and present additional security block variants that could be added for greater robustness. Security blocks can be built with standard circuit components, ensuring compatibility with existing semiconductor manufacturing processes. With embedded security blocks, the next generation of AI accelerators could be more robustly defended against dangerous misuse.</li>
</ul>

<h3>Title: Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity</h3>
<ul>
<li><strong>Authors: </strong>Sung Ju Lee, Nam Ik Cho</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07647">https://arxiv.org/abs/2509.07647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07647">https://arxiv.org/pdf/2509.07647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07647]] Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity(https://arxiv.org/abs/2509.07647)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>Semantic watermarking techniques for latent diffusion models (LDMs) are robust against regeneration attacks, but often suffer from detection performance degradation due to the loss of frequency integrity. To tackle this problem, we propose a novel embedding method called Hermitian Symmetric Fourier Watermarking (SFW), which maintains frequency integrity by enforcing Hermitian symmetry. Additionally, we introduce a center-aware embedding strategy that reduces the vulnerability of semantic watermarking due to cropping attacks by ensuring robust information retention. To validate our approach, we apply these techniques to existing semantic watermarking schemes, enhancing their frequency-domain structures for better robustness and retrieval accuracy. Extensive experiments demonstrate that our methods achieve state-of-the-art verification and identification performance, surpassing previous approaches across various attack scenarios. Ablation studies confirm the impact of SFW on detection capabilities, the effectiveness of the center-aware embedding against cropping, and how message capacity influences identification accuracy. Notably, our method achieves the highest detection accuracy while maintaining superior image fidelity, as evidenced by FID and CLIP scores. Conclusively, our proposed SFW is shown to be an effective framework for balancing robustness and image fidelity, addressing the inherent trade-offs in semantic watermarking. Code available at this https URL</li>
</ul>

<h3>Title: Graph-based Integrated Gradients for Explaining Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Lachlan Simpson, Kyle Millar, Adriel Cheng, Cheng-Chew Lim, Hong Gunn Chew</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07648">https://arxiv.org/abs/2509.07648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07648">https://arxiv.org/pdf/2509.07648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07648]] Graph-based Integrated Gradients for Explaining Graph Neural Networks(https://arxiv.org/abs/2509.07648)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Integrated Gradients (IG) is a common explainability technique to address the black-box problem of neural networks. Integrated gradients assumes continuous data. Graphs are discrete structures making IG ill-suited to graphs. In this work, we introduce graph-based integrated gradients (GB-IG); an extension of IG to graphs. We demonstrate on four synthetic datasets that GB-IG accurately identifies crucial structural components of the graph used in classification tasks. We further demonstrate on three prevalent real-world graph datasets that GB-IG outperforms IG in highlighting important features for node classification tasks.</li>
</ul>

<h3>Title: Leveraging Digital Twin-as-a-Service Towards Continuous and Automated Cybersecurity Certification</h3>
<ul>
<li><strong>Authors: </strong>Ioannis Koufos, Abdul Rehman Qureshi, Adrian Asensio, Allen Abishek, Efstathios Zaragkas, Ricard Vilalta, Maria Souvalioti, George Xilouris, Michael-Alexandros Kourtis</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07649">https://arxiv.org/abs/2509.07649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07649">https://arxiv.org/pdf/2509.07649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07649]] Leveraging Digital Twin-as-a-Service Towards Continuous and Automated Cybersecurity Certification(https://arxiv.org/abs/2509.07649)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Traditional risk assessments rely on manual audits and system scans, often causing operational disruptions and leaving security gaps. To address these challenges, this work presents Security Digital Twin-as-a-Service (SDT-aaS), a novel approach that leverages Digital Twin (DT) technology for automated, non-intrusive security compliance. SDT-aaS enables real-time security assessments by mirroring real-world assets, collecting compliance artifacts, and creating machine-readable evidence. The proposed work is a scalable and interoperable solution that supports open standards like CycloneDX and Web of Things (WoT), facilitating seamless integration and efficient compliance management. Empirical results from a moderate-scale infrastructure use case demonstrate its feasibility and performance, paving the way for efficient, on-demand cybersecurity governance with minimal operational impact.</li>
</ul>

<h3>Title: Beyond Motion Cues and Structural Sparsity: Revisiting Small Moving Target Detection</h3>
<ul>
<li><strong>Authors: </strong>Guoyi Zhang, Siyang Chen, Guangsheng Xu, Zhihua Shen, Han Wang, Xiaohu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07654">https://arxiv.org/abs/2509.07654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07654">https://arxiv.org/pdf/2509.07654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07654]] Beyond Motion Cues and Structural Sparsity: Revisiting Small Moving Target Detection(https://arxiv.org/abs/2509.07654)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust</a></li>
<li><strong>Abstract: </strong>Small moving target detection is crucial for many defense applications but remains highly challenging due to low signal-to-noise ratios, ambiguous visual cues, and cluttered backgrounds. In this work, we propose a novel deep learning framework that differs fundamentally from existing approaches, which often rely on target-specific features or motion cues and tend to lack robustness in complex environments. Our key insight is that small target detection and background discrimination are inherently coupled, even cluttered video backgrounds often exhibit strong low-rank structures that can serve as stable priors for detection. We reformulate the task as a tensor-based low-rank and sparse decomposition problem and conduct a theoretical analysis of the background, target, and noise components to guide model design. Building on these insights, we introduce TenRPCANet, a deep neural network that requires minimal assumptions about target characteristics. Specifically, we propose a tokenization strategy that implicitly enforces multi-order tensor low-rank priors through a self-attention mechanism. This mechanism captures both local and non-local self-similarity to model the low-rank background without relying on explicit iterative optimization. In addition, inspired by the sparse component update in tensor RPCA, we design a feature refinement module to enhance target saliency. The proposed method achieves state-of-the-art performance on two highly distinct and challenging tasks: multi-frame infrared small target detection and space object detection. These results demonstrate both the effectiveness and the generalizability of our approach.</li>
</ul>

<h3>Title: MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Xixi Wu, Yanchao Tan, Nan Hou, Ruiyang Zhang, Hong Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07666">https://arxiv.org/abs/2509.07666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07666">https://arxiv.org/pdf/2509.07666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07666]] MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval(https://arxiv.org/abs/2509.07666)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Document Understanding is a foundational AI capability with broad applications, and Document Question Answering (DocQA) is a key evaluation task. Traditional methods convert the document into text for processing by Large Language Models (LLMs), but this process strips away critical multi-modal information like figures. While Large Vision-Language Models (LVLMs) address this limitation, their constrained input size makes multi-page document comprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate this by selecting relevant pages, but they rely solely on semantic relevance, ignoring logical connections between pages and the query, which is essential for reasoning. To this end, we propose MoLoRAG, a logic-aware retrieval framework for multi-modal, multi-page document understanding. By constructing a page graph that captures contextual relationships between pages, a lightweight VLM performs graph traversal to retrieve relevant pages, including those with logical connections often overlooked. This approach combines semantic and logical relevance to deliver more accurate retrieval. After retrieval, the top-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance flexibility, MoLoRAG offers two variants: a training-free solution for easy deployment and a fine-tuned version to improve logical relevance checking. Experiments on four DocQA datasets demonstrate average improvements of 9.68% in accuracy over LVLM direct inference and 7.44% in retrieval precision over baselines. Codes and datasets are released at this https URL.</li>
</ul>

<h3>Title: Nearest Neighbor Projection Removal Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Himanshu Singh, A. V. Subramanyam, Shivank Rajput, Mohan Kankanhalli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07673">https://arxiv.org/abs/2509.07673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07673">https://arxiv.org/pdf/2509.07673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07673]] Nearest Neighbor Projection Removal Adversarial Training(https://arxiv.org/abs/2509.07673)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks have exhibited impressive performance in image classification tasks but remain vulnerable to adversarial examples. Standard adversarial training enhances robustness but typically fails to explicitly address inter-class feature overlap, a significant contributor to adversarial susceptibility. In this work, we introduce a novel adversarial training framework that actively mitigates inter-class proximity by projecting out inter-class dependencies from adversarial and clean samples in the feature space. Specifically, our approach first identifies the nearest inter-class neighbors for each adversarial sample and subsequently removes projections onto these neighbors to enforce stronger feature separability. Theoretically, we demonstrate that our proposed logits correction reduces the Lipschitz constant of neural networks, thereby lowering the Rademacher complexity, which directly contributes to improved generalization and robustness. Extensive experiments across standard benchmarks including CIFAR-10, CIFAR-100, and SVHN show that our method demonstrates strong performance that is competitive with leading adversarial training techniques, highlighting significant achievements in both robust and clean accuracy. Our findings reveal the importance of addressing inter-class feature proximity explicitly to bolster adversarial robustness in DNNs.</li>
</ul>

<h3>Title: CAViAR: Critic-Augmented Video Agentic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Sachit Menon, Ahmet Iscen, Arsha Nagrani, Tobias Weyand, Carl Vondrick, Cordelia Schmid</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07680">https://arxiv.org/abs/2509.07680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07680">https://arxiv.org/pdf/2509.07680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07680]] CAViAR: Critic-Augmented Video Agentic Reasoning(https://arxiv.org/abs/2509.07680)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video understanding has seen significant progress in recent years, with models' performance on perception from short clips continuing to rise. Yet, multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show performance wanes for tasks requiring complex reasoning on videos as queries grow more complex and videos grow longer. In this work, we ask: can existing perception capabilities be leveraged to successfully perform more complex video reasoning? In particular, we develop a large language model agent given access to video modules as subagents or tools. Rather than following a fixed procedure to solve queries as in previous work such as Visual Programming, ViperGPT, and MoReVQA, the agent uses the results of each call to a module to determine subsequent steps. Inspired by work in the textual reasoning domain, we introduce a critic to distinguish between instances of successful and unsuccessful sequences from the agent. We show that the combination of our agent and critic achieve strong performance on the previously-mentioned datasets.</li>
</ul>

<h3>Title: SEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compression</h3>
<ul>
<li><strong>Authors: </strong>Chunhang Zheng, Zichang Ren, Dou Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07704">https://arxiv.org/abs/2509.07704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07704">https://arxiv.org/pdf/2509.07704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07704]] SEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compression(https://arxiv.org/abs/2509.07704)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recently, learned image compression has attracted considerable attention due to its superior performance over traditional methods. However, most existing approaches employ a single entropy model to estimate the probability distribution of pixel values across the entire image, which limits their ability to capture the diverse statistical characteristics of different semantic regions. To overcome this limitation, we propose Segmentation-Assisted Multi-Entropy Models for Lossless Image Compression (SEEC). Our framework utilizes semantic segmentation to guide the selection and adaptation of multiple entropy models, enabling more accurate probability distribution estimation for distinct semantic regions. Specifically, SEEC first extracts image features and then applies semantic segmentation to identify different regions, each assigned a specialized entropy model to better capture its unique statistical properties. Finally, a multi-channel discrete logistic mixture likelihood is employed to model the pixel value distributions effectively. Experimental results on benchmark datasets demonstrate that SEEC achieves state-of-the-art compression ratios while introducing only minimal encoding and decoding latency. With superior performance, the proposed model also supports Regions of Interest (ROIs) coding condition on the provided segmentation mask. Our code is available at this https URL.</li>
</ul>

<h3>Title: IBN: An Interpretable Bidirectional-Modeling Network for Multivariate Time Series Forecasting with Variable Missing</h3>
<ul>
<li><strong>Authors: </strong>Shusen Ma, Tianhao Zhang, Qijiu Xia, Yun-Bo Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07725">https://arxiv.org/abs/2509.07725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07725">https://arxiv.org/pdf/2509.07725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07725]] IBN: An Interpretable Bidirectional-Modeling Network for Multivariate Time Series Forecasting with Variable Missing(https://arxiv.org/abs/2509.07725)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Multivariate time series forecasting (MTSF) often faces challenges from missing variables, which hinder conventional spatial-temporal graph neural networks in modeling inter-variable correlations. While GinAR addresses variable missing using attention-based imputation and adaptive graph learning for the first time, it lacks interpretability and fails to capture more latent temporal patterns due to its simple recursive units (RUs). To overcome these limitations, we propose the Interpretable Bidirectional-modeling Network (IBN), integrating Uncertainty-Aware Interpolation (UAI) and Gaussian kernel-based Graph Convolution (GGCN). IBN estimates the uncertainty of reconstructed values using MC Dropout and applies an uncertainty-weighted strategy to mitigate high-risk reconstructions. GGCN explicitly models spatial correlations among variables, while a bidirectional RU enhances temporal dependency modeling. Extensive experiments show that IBN achieves state-of-the-art forecasting performance under various missing-rate scenarios, providing a more reliable and interpretable framework for MTSF with missing variables. Code is available at: this https URL.</li>
</ul>

<h3>Title: M-BRe: Discovering Training Samples for Relation Extraction from Unlabeled Texts with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zexuan Li, Hongliang Dai, Piji Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07730">https://arxiv.org/abs/2509.07730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07730">https://arxiv.org/pdf/2509.07730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07730]] M-BRe: Discovering Training Samples for Relation Extraction from Unlabeled Texts with Large Language Models(https://arxiv.org/abs/2509.07730)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>For Relation Extraction (RE), the manual annotation of training data may be prohibitively expensive, since the sentences that contain the target relations in texts can be very scarce and difficult to find. It is therefore beneficial to develop an efficient method that can automatically extract training instances from unlabeled texts for training RE models. Recently, large language models (LLMs) have been adopted in various natural language processing tasks, with RE also benefiting from their advances. However, when leveraging LLMs for RE with predefined relation categories, two key challenges arise. First, in a multi-class classification setting, LLMs often struggle to comprehensively capture the semantics of every relation, leading to suboptimal results. Second, although employing binary classification for each relation individually can mitigate this issue, it introduces significant computational overhead, resulting in impractical time complexity for real-world applications. Therefore, this paper proposes a framework called M-BRe to extract training instances from unlabeled texts for RE. It utilizes three modules to combine the advantages of both of the above classification approaches: Relation Grouping, Relation Extraction, and Label Decision. Extensive experiments confirm its superior capability in discovering high-quality training samples from unlabeled texts for RE.</li>
</ul>

<h3>Title: Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts</h3>
<ul>
<li><strong>Authors: </strong>Rochana Prih Hastuti, Rian Adam Rajagede, Mansour Al Ghanim, Mengxin Zheng, Qian Lou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07755">https://arxiv.org/abs/2509.07755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07755">https://arxiv.org/pdf/2509.07755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07755]] Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts(https://arxiv.org/abs/2509.07755)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) adapted to sensitive domains such as medicine, their fluency raises safety risks, particularly regarding provenance and accountability. Watermarking embeds detectable patterns to mitigate these risks, yet its reliability in medical contexts remains untested. Existing benchmarks focus on detection-quality tradeoffs, overlooking factual risks under low-entropy settings often exploited by watermarking's reweighting strategy. We propose a medical-focused evaluation workflow that jointly assesses factual accuracy and coherence. Using GPT-Judger and further human validation, we introduce the Factuality-Weighted Score (FWS), a composite metric prioritizing factual accuracy beyond coherence to guide watermarking deployment in medical domains. Our evaluation shows current watermarking methods substantially compromise medical factuality, with entropy shifts degrading medical entity representation. These findings underscore the need for domain-aware watermarking approaches that preserve the integrity of medical content.</li>
</ul>

<h3>Title: Empirical Security Analysis of Software-based Fault Isolation through Controlled Fault Injection</h3>
<ul>
<li><strong>Authors: </strong>Nils Bars, Lukas Bernhard, Moritz Schloegel, Thorsten Holz</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07757">https://arxiv.org/abs/2509.07757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07757">https://arxiv.org/pdf/2509.07757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07757]] Empirical Security Analysis of Software-based Fault Isolation through Controlled Fault Injection(https://arxiv.org/abs/2509.07757)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>We use browsers daily to access all sorts of information. Because browsers routinely process scripts, media, and executable code from unknown sources, they form a critical security boundary between users and adversaries. A common attack vector is JavaScript, which exposes a large attack surface due to the sheer complexity of modern JavaScript engines. To mitigate these threats, modern engines increasingly adopt software-based fault isolation (SFI). A prominent example is Google's V8 heap sandbox, which represents the most widely deployed SFI mechanism, protecting billions of users across all Chromium-based browsers and countless applications built on this http URL and Electron. The heap sandbox splits the address space into two parts: one part containing trusted, security-sensitive metadata, and a sandboxed heap containing memory accessible to untrusted code. On a technical level, the sandbox enforces isolation by removing raw pointers and using translation tables to resolve references to trusted objects. Consequently, an attacker cannot corrupt trusted data even with full control of the sandboxed data, unless there is a bug in how code handles data from the sandboxed heap. Despite their widespread use, such SFI mechanisms have seen little security testing. In this work, we propose a new testing technique that models the security boundary of modern SFI implementations. Following the SFI threat model, we assume a powerful attacker who fully controls the sandbox's memory. We implement this by instrumenting memory loads originating in the trusted domain and accessing untrusted, attacker-controlled sandbox memory. We then inject faults into the loaded data, aiming to trigger memory corruption in the trusted domain. In a comprehensive evaluation, we identify 19 security bugs in V8 that enable an attacker to bypass the sandbox.</li>
</ul>

<h3>Title: AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents</h3>
<ul>
<li><strong>Authors: </strong>Haitao Hu, Peng Chen, Yanpeng Zhao, Yuqi Chen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07764">https://arxiv.org/abs/2509.07764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07764">https://arxiv.org/pdf/2509.07764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07764]] AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents(https://arxiv.org/abs/2509.07764)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been increasingly integrated into computer-use agents, which can autonomously operate tools on a user's computer to accomplish complex tasks. However, due to the inherently unstable and unpredictable nature of LLM outputs, they may issue unintended tool commands or incorrect inputs, leading to potentially harmful operations. Unlike traditional security risks stemming from insecure user prompts, tool execution results from LLM-driven decisions introduce new and unique security challenges. These vulnerabilities span across all components of a computer-use agent. To mitigate these risks, we propose AgentSentinel, an end-to-end, real-time defense framework designed to mitigate potential security threats on a user's computer. AgentSentinel intercepts all sensitive operations within agent-related services and halts execution until a comprehensive security audit is completed. Our security auditing mechanism introduces a novel inspection process that correlates the current task context with system traces generated during task execution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a benchmark consisting of 60 diverse attack scenarios across six attack categories. The benchmark demonstrates a 87% average attack success rate on four state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an average defense success rate of 79.6%, significantly outperforming all baseline defenses.</li>
</ul>

<h3>Title: Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Michele Joshua Maggini, Dhia Merzougui, Rabiraj Bandyopadhyay, Gaël Dias, Fabrice Maurel, Pablo Gamallo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07768">https://arxiv.org/abs/2509.07768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07768">https://arxiv.org/pdf/2509.07768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07768]] Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning(https://arxiv.org/abs/2509.07768)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The spread of fake news, polarizing, politically biased, and harmful content on online platforms has been a serious concern. With large language models becoming a promising approach, however, no study has properly benchmarked their performance across different models, usage methods, and languages. This study presents a comprehensive overview of different Large Language Models adaptation paradigms for the detection of hyperpartisan and fake news, harmful tweets, and political bias. Our experiments spanned 10 datasets and 5 different languages (English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and multiclass classification scenarios. We tested different strategies ranging from parameter efficient Fine-Tuning of language models to a variety of different In-Context Learning strategies and prompts. These included zero-shot prompts, codebooks, few-shot (with both randomly-selected and diversely-selected examples using Determinantal Point Processes), and Chain-of-Thought. We discovered that In-Context Learning often underperforms when compared to Fine-Tuning a model. This main finding highlights the importance of Fine-Tuning even smaller models on task-specific settings even when compared to the largest models evaluated in an In-Context Learning setup - in our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and Qwen2.5-7B-Instruct.</li>
</ul>

<h3>Title: XSRD-Net: EXplainable Stroke Relapse Detection</h3>
<ul>
<li><strong>Authors: </strong>Christian Gapp, Elias Tappeiner, Martin Welk, Karl Fritscher, Stephanie Mangesius, Constantin Eisenschink, Philipp Deisl, Michael Knoflach, Astrid E. Grams, Elke R. Gizewski, Rainer Schubert</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07772">https://arxiv.org/abs/2509.07772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07772">https://arxiv.org/pdf/2509.07772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07772]] XSRD-Net: EXplainable Stroke Relapse Detection(https://arxiv.org/abs/2509.07772)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Stroke is the second most frequent cause of death world wide with an annual mortality of around 5.5 million. Recurrence rates of stroke are between 5 and 25% in the first year. As mortality rates for relapses are extraordinarily high (40%) it is of utmost importance to reduce the recurrence rates. We address this issue by detecting patients at risk of stroke recurrence at an early stage in order to enable appropriate therapy planning. To this end we collected 3D intracranial CTA image data and recorded concomitant heart diseases, the age and the gender of stroke patients between 2010 and 2024. We trained single- and multimodal deep learning based neural networks for binary relapse detection (Task 1) and for relapse free survival (RFS) time prediction together with a subsequent classification (Task 2). The separation of relapse from non-relapse patients (Task 1) could be solved with tabular data (AUC on test dataset: 0.84). However, for the main task, the regression (Task 2), our multimodal XSRD-net processed the modalities vision:tabular with 0.68:0.32 according to modality contribution measures. The c-index with respect to relapses for the multimodal model reached 0.68, and the AUC is 0.71 for the test dataset. Final, deeper interpretability analysis results could highlight a link between both heart diseases (tabular) and carotid arteries (vision) for the detection of relapses and the prediction of the RFS time. This is a central outcome that we strive to strengthen with ongoing data collection and model retraining.</li>
</ul>

<h3>Title: HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Yimin Pan, Matthias Nießner, Tobias Kirschstein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07774">https://arxiv.org/abs/2509.07774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07774">https://arxiv.org/pdf/2509.07774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07774]] HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting(https://arxiv.org/abs/2509.07774)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Human hair reconstruction is a challenging problem in computer vision, with growing importance for applications in virtual reality and digital human modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient and explicit scene representations that naturally align with the structure of hair strands. In this work, we extend the 3DGS framework to enable strand-level hair geometry reconstruction from multi-view images. Our multi-stage pipeline first reconstructs detailed hair geometry using a differentiable Gaussian rasterizer, then merges individual Gaussian segments into coherent strands through a novel merging scheme, and finally refines and grows the strands under photometric supervision. While existing methods typically evaluate reconstruction quality at the geometric level, they often neglect the connectivity and topology of hair strands. To address this, we propose a new evaluation metric that serves as a proxy for assessing topological accuracy in strand reconstruction. Extensive experiments on both synthetic and real-world datasets demonstrate that our method robustly handles a wide range of hairstyles and achieves efficient reconstruction, typically completing within one hour. The project page can be found at: this https URL</li>
</ul>

<h3>Title: SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP</h3>
<ul>
<li><strong>Authors: </strong>Decheng Duan, Yingyi Zhang, Jitong Peng, Chengzhi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07801">https://arxiv.org/abs/2509.07801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07801">https://arxiv.org/pdf/2509.07801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07801]] SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP(https://arxiv.org/abs/2509.07801)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Structured information extraction from scientific literature is crucial for capturing core concepts and emerging trends in specialized fields. While existing datasets aid model development, most focus on specific publication sections due to domain complexity and the high cost of annotating scientific texts. To address this limitation, we introduce SciNLP - a specialized benchmark for full-text entity and relation extraction in the Natural Language Processing (NLP) domain. The dataset comprises 60 manually annotated full-text NLP publications, covering 7,072 entities and 1,826 relations. Compared to existing research, SciNLP is the first dataset providing full-text annotations of entities and their relationships in the NLP domain. To validate the effectiveness of SciNLP, we conducted comparative experiments with similar datasets and evaluated the performance of state-of-the-art supervised models on this dataset. Results reveal varying extraction capabilities of existing models across academic texts of different lengths. Cross-comparisons with existing datasets show that SciNLP achieves significant performance improvements on certain baseline models. Using models trained on SciNLP, we implemented automatic construction of a fine-grained knowledge graph for the NLP domain. Our KG has an average node degree of 3.2 per entity, indicating rich semantic topological information that enhances downstream applications. The dataset is publicly available at this https URL.</li>
</ul>

<h3>Title: Inner-product Functional Encryption with Fine-grained Revocation for Flexible EHR Sharing</h3>
<ul>
<li><strong>Authors: </strong>Yue Han, Jinguang Han, Liqun Chen, Chao Sun</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07804">https://arxiv.org/abs/2509.07804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07804">https://arxiv.org/pdf/2509.07804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07804]] Inner-product Functional Encryption with Fine-grained Revocation for Flexible EHR Sharing(https://arxiv.org/abs/2509.07804)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>E-health record (EHR) contains a vast amount of continuously growing medical data and enables medical institutions to access patient health data this http URL provides opportunities for medical data mining which has important applications in identifying high-risk patients and improving disease diagnosis, this http URL EHR contains sensitive patient information, how to protect patient privacy and enable mining on EHR data is important and this http URL public key encryption (PKE) can protect patient privacy, but cannot support flexible selective computation on encrypted EHR this http URL encryption (FE) allows authorised users to compute function values of encrypted data without releasing other information, hence supporting selective computation on encrypted data. Nevertheless, existing FE schemes do not support fine-grained revocation and update, so they are unsuitable for EHR system. In this paper,we first propose an inner-product functional encryption with fine-grained revocation (IPFE-FR) scheme, and then apply it to a flexible EHR sharing system. Our scheme possesses the following features:(1) a group manager can revoke a specific function computation of medical institutions on encrypted EHR data,instead of all function computation rights. (2) a revoked medical institution is not allowed to compute the function value of encrypted EHR data not only generated after the revocation, but also generated before the revocation. (3) secret keys issued to the same medical institution are bound together to prevent collusion attacks. The formal definition and security model of the IPFE-FR scheme are this http URL, we present a concrete construction and reduce its security to the Learning with Errors (LWE) assumption which is quantum-resistant. Finally, the theoretical analysis and experimental implementation of our scheme are conducted to show its efficiency.</li>
</ul>

<h3>Title: Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems</h3>
<ul>
<li><strong>Authors: </strong>Xiaolin Chen, Xuemeng Song, Haokun Wen, Weili Guan, Xiangyu Zhao, Liqiang Nie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07817">https://arxiv.org/abs/2509.07817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07817">https://arxiv.org/pdf/2509.07817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07817]] Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems(https://arxiv.org/abs/2509.07817)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Textual response generation is pivotal for multimodal \mbox{task-oriented} dialog systems, which aims to generate proper textual responses based on the multimodal context. While existing efforts have demonstrated remarkable progress, there still exist the following limitations: 1) \textit{neglect of unstructured review knowledge} and 2) \textit{underutilization of large language models (LLMs)}. Inspired by this, we aim to fully utilize dual knowledge (\textit{i.e., } structured attribute and unstructured review knowledge) with LLMs to promote textual response generation in multimodal task-oriented dialog systems. However, this task is non-trivial due to two key challenges: 1) \textit{dynamic knowledge type selection} and 2) \textit{intention-response decoupling}. To address these challenges, we propose a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for multimodal dialog systems (named DK2R). To be specific, DK2R first extracts both structured attribute and unstructured review knowledge from external knowledge base given the dialog context. Thereafter, DK2R uses an LLM to evaluate each knowledge type's utility by analyzing LLM-generated provisional probe responses. Moreover, DK2R separately summarizes the intention-oriented key clues via dedicated reasoning, which are further used as auxiliary signals to enhance LLM-based textual response generation. Extensive experiments conducted on a public dataset verify the superiority of DK2R. We have released the codes and parameters.</li>
</ul>

<h3>Title: Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model</h3>
<ul>
<li><strong>Authors: </strong>Zhuoxu Huang, Mingqi Gao, Jungong Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07825">https://arxiv.org/abs/2509.07825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07825">https://arxiv.org/pdf/2509.07825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07825]] Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model(https://arxiv.org/abs/2509.07825)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>3D object segmentation with Large Language Models (LLMs) has become a prevailing paradigm due to its broad semantics, task flexibility, and strong generalization. However, this paradigm is hindered by representation misalignment: LLMs process high-level semantic tokens, whereas 3D point clouds convey only dense geometric structures. In prior methods, misalignment limits both input and output. At the input stage, dense point patches require heavy pre-alignment, weakening object-level semantics and confusing similar distractors. At the output stage, predictions depend only on dense features without explicit geometric cues, leading to a loss of fine-grained accuracy. To address these limitations, we present the Point Linguist Model (PLM), a general framework that bridges the representation gap between LLMs and dense 3D point clouds without requiring large-scale pre-alignment between 3D-text or 3D-images. Specifically, we introduce Object-centric Discriminative Representation (OcDR), which learns object-centric tokens that capture target semantics and scene relations under a hard negative-aware training objective. This mitigates the misalignment between LLM tokens and 3D points, enhances resilience to distractors, and facilitates semantic-level reasoning within LLMs. For accurate segmentation, we introduce the Geometric Reactivation Decoder (GRD), which predicts masks by combining OcDR tokens carrying LLM-inferred geometry with corresponding dense features, preserving comprehensive dense features throughout the pipeline. Extensive experiments show that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and +6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gains across 7 benchmarks spanning 4 different tasks, demonstrating the effectiveness of comprehensive object-centric reasoning for robust 3D understanding.</li>
</ul>

<h3>Title: Deep Learning-Based Burned Area Mapping Using Bi-Temporal Siamese Networks and AlphaEarth Foundation Datasets</h3>
<ul>
<li><strong>Authors: </strong>Seyd Teymoor Seydi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07852">https://arxiv.org/abs/2509.07852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07852">https://arxiv.org/pdf/2509.07852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07852]] Deep Learning-Based Burned Area Mapping Using Bi-Temporal Siamese Networks and AlphaEarth Foundation Datasets(https://arxiv.org/abs/2509.07852)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate and timely mapping of burned areas is crucial for environmental monitoring, disaster management, and assessment of climate change. This study presents a novel approach to automated burned area mapping using the AlphaEArth dataset combined with the Siamese U-Net deep learning architecture. The AlphaEArth Dataset, comprising high-resolution optical and thermal infrared imagery with comprehensive ground-truth annotations, provides an unprecedented resource for training robust burned area detection models. We trained our model with the Monitoring Trends in Burn Severity (MTBS) dataset in the contiguous US and evaluated it with 17 regions cross in Europe. Our experimental results demonstrate that the proposed ensemble approach achieves superior performance with an overall accuracy of 95%, IoU of 0.6, and F1-score of 74% on the test dataset. The model successfully identifies burned areas across diverse ecosystems with complex background, showing particular strength in detecting partially burned vegetation and fire boundaries and its transferability and high generalization in burned area mapping. This research contributes to the advancement of automated fire damage assessment and provides a scalable solution for global burn area monitoring using the AlphaEarth dataset.</li>
</ul>

<h3>Title: D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via Layer-to-head Attention Diagnostics</h3>
<ul>
<li><strong>Authors: </strong>Tiancheng Yang, Lin Zhang, Jiaye Lin, Guimin Hu, Di Wang, Lijie Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07864">https://arxiv.org/abs/2509.07864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07864">https://arxiv.org/pdf/2509.07864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07864]] D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via Layer-to-head Attention Diagnostics(https://arxiv.org/abs/2509.07864)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) achieve strong performance on tasks like image captioning and visual question answering, but remain prone to hallucinations, where generated text conflicts with the visual input. Prior work links this partly to insufficient visual attention, but existing attention-based detectors and mitigation typically apply uniform adjustments across layers and heads, obscuring where errors originate. In this paper, we first show these methods fail to accurately localize problematic layers. Then, we introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags anomalous layers, and Image Attention Focus (IAF) which scores attention heads within those layers. Analysis shows that LIAE pinpoints faulty layers and IAF reliably ranks heads that warrant correction. Guided by these signals, we propose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a task-agnostic, attention-guided method that dynamically localizes and corrects errors during inference with negligible overhead. Results show our D-LEAF delivers a 53% relative improvement on standard captioning benchmarks, and on VQA both accuracy and F1-score improve by approximately 4%, substantially suppressing hallucinations while preserving efficiency.</li>
</ul>

<h3>Title: Are Humans as Brittle as Large Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Li, Sean Papay, Roman Klinger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07869">https://arxiv.org/abs/2509.07869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07869">https://arxiv.org/pdf/2509.07869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07869]] Are Humans as Brittle as Large Language Models?(https://arxiv.org/abs/2509.07869)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The output of large language models (LLM) is unstable, due to both non-determinism of the decoding process as well as to prompt brittleness. While the intrinsic non-determinism of LLM generation may mimic existing uncertainty in human annotations through distributional shifts in outputs, it is largely assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs. This raises the question: do human annotators show similar sensitivity to instruction changes? If so, should prompt brittleness in LLMs be considered problematic? One may alternatively hypothesize that prompt brittleness correctly reflects human annotation variances. To fill this research gap, we systematically compare the effects of prompt modifications on LLMs and identical instruction modifications for human annotators, focusing on the question of whether humans are similarly sensitive to prompt perturbations. To study this, we prompt both humans and LLMs for a set of text classification tasks conditioned on prompt variations. Our findings indicate that both humans and LLMs exhibit increased brittleness in response to specific types of prompt modifications, particularly those involving the substitution of alternative label sets or label formats. However, the distribution of human judgments is less affected by typographical errors and reversed label order than that of LLMs.</li>
</ul>

<h3>Title: Active Membership Inference Test (aMINT): Enhancing Model Auditability with Multi-Task Learning</h3>
<ul>
<li><strong>Authors: </strong>Daniel DeAlcala, Aythami Morales, Julian Fierrez, Gonzalo Mancera, Ruben Tolosana, Javier Ortega-Garcia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07879">https://arxiv.org/abs/2509.07879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07879">https://arxiv.org/pdf/2509.07879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07879]] Active Membership Inference Test (aMINT): Enhancing Model Auditability with Multi-Task Learning(https://arxiv.org/abs/2509.07879)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, membership infer, transformer</a></li>
<li><strong>Abstract: </strong>Active Membership Inference Test (aMINT) is a method designed to detect whether given data were used during the training of machine learning models. In Active MINT, we propose a novel multitask learning process that involves training simultaneously two models: the original or Audited Model, and a secondary model, referred to as the MINT Model, responsible for identifying the data used for training the Audited Model. This novel multi-task learning approach has been designed to incorporate the auditability of the model as an optimization objective during the training process of neural networks. The proposed approach incorporates intermediate activation maps as inputs to the MINT layers, which are trained to enhance the detection of training data. We present results using a wide range of neural networks, from lighter architectures such as MobileNet to more complex ones such as Vision Transformers, evaluated in 5 public benchmarks. Our proposed Active MINT achieves over 80% accuracy in detecting if given data was used for training, significantly outperforming previous approaches in the literature. Our aMINT and related methodological developments contribute to increasing transparency in AI models, facilitating stronger safeguards in AI deployments to achieve proper security, privacy, and copyright protection.</li>
</ul>

<h3>Title: From Detection to Mitigation: Addressing Gender Bias in Chinese Texts via Efficient Tuning and Voting-Based Rebalancing</h3>
<ul>
<li><strong>Authors: </strong>Chengyan Wu, Yiqiang Cai, Yufei Cheng, Yun Xue</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07889">https://arxiv.org/abs/2509.07889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07889">https://arxiv.org/pdf/2509.07889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07889]] From Detection to Mitigation: Addressing Gender Bias in Chinese Texts via Efficient Tuning and Voting-Based Rebalancing(https://arxiv.org/abs/2509.07889)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which focuses on sentence-level gender bias detection and mitigation in Chinese. The task aims to promote fairness and controllability in natural language generation by automatically detecting, classifying, and mitigating gender bias. To address this challenge, we adopt a fine-tuning approach based on large language models (LLMs), efficiently adapt to the bias detection task via Low-Rank Adaptation (LoRA). In terms of data processing, we construct a more balanced training set to alleviate class imbalance and introduce heterogeneous samples from multiple sources to enhance model generalization. For the detection and classification sub-tasks, we employ a majority voting strategy that integrates outputs from multiple expert models to boost performance. Additionally, to improve bias generation detection and mitigation, we design a multi-temperature sampling mechanism to capture potential variations in bias expression styles. Experimental results demonstrate the effectiveness of our approach in bias detection, classification, and mitigation. Our method ultimately achieves an average score of 47.90%, ranking fourth in the shared task.</li>
</ul>

<h3>Title: Biased Tales: Cultural and Topic Bias in Generating Children's Stories</h3>
<ul>
<li><strong>Authors: </strong>Donya Rooein, Vilém Zouhar, Debora Nozza, Dirk Hovy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07908">https://arxiv.org/abs/2509.07908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07908">https://arxiv.org/pdf/2509.07908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07908]] Biased Tales: Cultural and Topic Bias in Generating Children's Stories(https://arxiv.org/abs/2509.07908)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Stories play a pivotal role in human communication, shaping beliefs and morals, particularly in children. As parents increasingly rely on large language models (LLMs) to craft bedtime stories, the presence of cultural and gender stereotypes in these narratives raises significant concerns. To address this issue, we present Biased Tales, a comprehensive dataset designed to analyze how biases influence protagonists' attributes and story elements in LLM-generated stories. Our analysis uncovers striking disparities. When the protagonist is described as a girl (as compared to a boy), appearance-related attributes increase by 55.26%. Stories featuring non-Western children disproportionately emphasize cultural heritage, tradition, and family themes far more than those for Western children. Our findings highlight the role of sociocultural bias in making creative AI use more equitable and diverse.</li>
</ul>

<h3>Title: Uncovering Scaling Laws for Large Language Models via Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Arun Verma, Zhaoxuan Wu, Zijian Zhou, Xiaoqiang Lin, Zhiliang Chen, Rachael Hwee Ling Sim, Rui Qiao, Jingtan Wang, Nhung Bui, Xinyuan Niu, Wenyang Hu, Gregory Kang Ruey Lau, Zi-Yu Khoo, Zitong Zhao, Xinyi Xu, Apivich Hemachandra, See-Kiong Ng, Bryan Kian Hsiang Low</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07909">https://arxiv.org/abs/2509.07909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07909">https://arxiv.org/pdf/2509.07909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07909]] Uncovering Scaling Laws for Large Language Models via Inverse Problems(https://arxiv.org/abs/2509.07909)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are large-scale pretrained models that have achieved remarkable success across diverse domains. These successes have been driven by unprecedented complexity and scale in both data and computations. However, due to the high costs of training such models, brute-force trial-and-error approaches to improve LLMs are not feasible. Inspired by the success of inverse problems in uncovering fundamental scientific laws, this position paper advocates that inverse problems can also efficiently uncover scaling laws that guide the building of LLMs to achieve the desirable performance with significantly better cost-effectiveness.</li>
</ul>

<h3>Title: Object-level Correlation for Few-Shot Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chunlin Wen, Yu Zhang, Jie Fan, Hongyuan Zhu, Xiu-Shen Wei, Yijun Wang, Zhiqiang Kou, Shuzhou Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07917">https://arxiv.org/abs/2509.07917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07917">https://arxiv.org/pdf/2509.07917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07917]] Object-level Correlation for Few-Shot Segmentation(https://arxiv.org/abs/2509.07917)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Few-shot semantic segmentation (FSS) aims to segment objects of novel categories in the query images given only a few annotated support samples. Existing methods primarily build the image-level correlation between the support target object and the entire query image. However, this correlation contains the hard pixel noise, \textit{i.e.}, irrelevant background objects, that is intractable to trace and suppress, leading to the overfitting of the background. To address the limitation of this correlation, we imitate the biological vision process to identify novel objects in the object-level information. Target identification in the general objects is more valid than in the entire image, especially in the low-data regime. Inspired by this, we design an Object-level Correlation Network (OCNet) by establishing the object-level correlation between the support target object and query general objects, which is mainly composed of the General Object Mining Module (GOMM) and Correlation Construction Module (CCM). Specifically, GOMM constructs the query general object feature by learning saliency and high-level similarity cues, where the general objects include the irrelevant background objects and the target foreground object. Then, CCM establishes the object-level correlation by allocating the target prototypes to match the general object feature. The generated object-level correlation can mine the query target feature and suppress the hard pixel noise for the final prediction. Extensive experiments on PASCAL-${5}^{i}$ and COCO-${20}^{i}$ show that our model achieves the state-of-the-art performance.</li>
</ul>

<h3>Title: ScoreHOI: Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Ao Li, Jinpeng Liu, Yixuan Zhu, Yansong Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07920">https://arxiv.org/abs/2509.07920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07920">https://arxiv.org/pdf/2509.07920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07920]] ScoreHOI: Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion(https://arxiv.org/abs/2509.07920)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Joint reconstruction of human-object interaction marks a significant milestone in comprehending the intricate interrelations between humans and their surrounding environment. Nevertheless, previous optimization methods often struggle to achieve physically plausible reconstruction results due to the lack of prior knowledge about human-object interactions. In this paper, we introduce ScoreHOI, an effective diffusion-based optimizer that introduces diffusion priors for the precise recovery of human-object interactions. By harnessing the controllability within score-guided sampling, the diffusion model can reconstruct a conditional distribution of human and object pose given the image observation and object feature. During inference, the ScoreHOI effectively improves the reconstruction results by guiding the denoising process with specific physical constraints. Furthermore, we propose a contact-driven iterative refinement approach to enhance the contact plausibility and improve the reconstruction accuracy. Extensive evaluations on standard benchmarks demonstrate ScoreHOI's superior performance over state-of-the-art methods, highlighting its ability to achieve a precise and robust improvement in joint human-object interaction reconstruction.</li>
</ul>

<h3>Title: Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Moo Hyun Son, Juyoung Bae, Zelin Qiu, Jiale Peng, Kai Xin Li, Yifan Lin, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07923">https://arxiv.org/abs/2509.07923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07923">https://arxiv.org/pdf/2509.07923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07923]] Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation(https://arxiv.org/abs/2509.07923)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Digital dentistry represents a transformative shift in modern dental practice. The foundational step in this transformation is the accurate digital representation of the patient's dentition, which is obtained from segmented Cone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite the growing interest in digital dental technologies, existing segmentation methodologies frequently lack rigorous validation and demonstrate limited performance and clinical applicability. To the best of our knowledge, this is the first work to introduce a multimodal pretraining framework for tooth segmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning for pretraining that integrates volumetric (CBCT) and surface-based (IOS) modalities. By capturing modality-invariant representations through multimodal contrastive learning, our approach effectively models fine-grained anatomical features, enabling precise multi-class segmentation and accurate identification of Fédération Dentaire Internationale (FDI) tooth numbering. Along with the framework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset to date, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensive collection of independent datasets, representing the largest and most diverse evaluation to date. Our method achieves state-of-the-art performance in both internal and external testing, with an increase of 12\% for CBCT segmentation and 8\% for IOS segmentation in the Dice Similarity Coefficient (DSC). Furthermore, ToothMCL consistently surpasses existing approaches in tooth groups and demonstrates robust generalizability across varying imaging conditions and clinical scenarios.</li>
</ul>

<h3>Title: GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tuo Wang, Adithya Kulkarni, Tyler Cody, Peter A. Beling, Yujun Yan, Dawei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07925">https://arxiv.org/abs/2509.07925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07925">https://arxiv.org/pdf/2509.07925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07925]] GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models(https://arxiv.org/abs/2509.07925)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at this https URL.</li>
</ul>

<h3>Title: Feature Space Analysis by Guided Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Kimiaki Shirahama, Miki Yanobu, Kaduki Yamashita, Miho Ohsaki</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07936">https://arxiv.org/abs/2509.07936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07936">https://arxiv.org/pdf/2509.07936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07936]] Feature Space Analysis by Guided Diffusion Model(https://arxiv.org/abs/2509.07936)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>One of the key issues in Deep Neural Networks (DNNs) is the black-box nature of their internal feature extraction process. Targeting vision-related domains, this paper focuses on analysing the feature space of a DNN by proposing a decoder that can generate images whose features are guaranteed to closely match a user-specified feature. Owing to this guarantee that is missed in past studies, our decoder allows us to evidence which of various attributes in an image are encoded into a feature by the DNN, by generating images whose features are in proximity to that feature. Our decoder is implemented as a guided diffusion model that guides the reverse image generation of a pre-trained diffusion model to minimise the Euclidean distance between the feature of a clean image estimated at each step and the user-specified feature. One practical advantage of our decoder is that it can analyse feature spaces of different DNNs with no additional training and run on a single COTS GPU. The experimental results targeting CLIP's image encoder, ResNet-50 and vision transformer demonstrate that images generated by our decoder have features remarkably similar to the user-specified ones and reveal valuable insights into these DNNs' feature spaces.</li>
</ul>

<h3>Title: Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees</h3>
<ul>
<li><strong>Authors: </strong>Katsuaki Nakano, Reza Feyyazi, Shanchieh Jay Yang, Michael Zuzak</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07939">https://arxiv.org/abs/2509.07939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07939">https://arxiv.org/pdf/2509.07939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07939]] Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees(https://arxiv.org/abs/2509.07939)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have driven interest in automating cybersecurity penetration testing workflows, offering the promise of faster and more consistent vulnerability assessment for enterprise systems. Existing LLM agents for penetration testing primarily rely on self-guided reasoning, which can produce inaccurate or hallucinated procedural steps. As a result, the LLM agent may undertake unproductive actions, such as exploiting unused software libraries or generating cyclical responses that repeat prior tactics. In this work, we propose a guided reasoning pipeline for penetration testing LLM agents that incorporates a deterministic task tree built from the MITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the LLM's reaoning process to explicitly defined tactics, techniques, and procedures. This anchors reasoning in proven penetration testing methodologies and filters out ineffective actions by guiding the agent towards more productive attack procedures. To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios. Our proposed reasoning pipeline guided the LLM agent through 71.8\%, 72.8\%, and 78.6\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively. Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\%, 16.5\%, and 75.7\% of subtasks and required 86.2\%, 118.7\%, and 205.9\% more model queries. This suggests that incorporating a deterministic task tree into LLM reasoning pipelines can enhance the accuracy and efficiency of automated cybersecurity assessments</li>
</ul>

<h3>Title: ImportSnare: Directed "Code Manual" Hijacking in Retrieval-Augmented Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Kai Ye, Liangcai Su, Chenxiong Qian</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07941">https://arxiv.org/abs/2509.07941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07941">https://arxiv.org/pdf/2509.07941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07941]] ImportSnare: Directed "Code Manual" Hijacking in Retrieval-Augmented Code Generation(https://arxiv.org/abs/2509.07941)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Code generation has emerged as a pivotal capability of Large Language Models(LLMs), revolutionizing development efficiency for programmers of all skill levels. However, the complexity of data structures and algorithmic logic often results in functional deficiencies and security vulnerabilities in generated code, reducing it to a prototype requiring extensive manual debugging. While Retrieval-Augmented Generation (RAG) can enhance correctness and security by leveraging external code manuals, it simultaneously introduces new attack surfaces. In this paper, we pioneer the exploration of attack surfaces in Retrieval-Augmented Code Generation (RACG), focusing on malicious dependency hijacking. We demonstrate how poisoned documentation containing hidden malicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting dual trust chains: LLM reliance on RAG and developers' blind trust in LLM suggestions. To construct poisoned documents, we propose ImportSnare, a novel attack framework employing two synergistic strategies: 1)Position-aware beam search optimizes hidden ranking sequences to elevate poisoned documents in retrieval results, and 2)Multilingual inductive suggestions generate jailbreaking sequences to manipulate LLMs into recommending malicious dependencies. Through extensive experiments across Python, Rust, and JavaScript, ImportSnare achieves significant attack success rates (over 50% for popular libraries such as matplotlib and seaborn) in general, and is also able to succeed even when the poisoning ratio is as low as 0.01%, targeting both custom and real-world malicious packages. Our findings reveal critical supply chain risks in LLM-powered development, highlighting inadequate security alignment for code generation tasks. To support future research, we will release the multilingual benchmark suite and datasets. The project homepage is this https URL.</li>
</ul>

<h3>Title: Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Kasra Borazjani, Naji Khosravan, Rajeev Sahay, Bita Akram, Seyyedali Hosseinalipour</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07946">https://arxiv.org/abs/2509.07946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07946">https://arxiv.org/pdf/2509.07946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07946]] Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges(https://arxiv.org/abs/2509.07946)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, interpretability</a></li>
<li><strong>Abstract: </strong>Multi-modal multi-task (M3T) foundation models (FMs) have recently shown transformative potential in artificial intelligence, with emerging applications in education. However, their deployment in real-world educational settings is hindered by privacy regulations, data silos, and limited domain-specific data availability. We introduce M3T Federated Foundation Models (FedFMs) for education: a paradigm that integrates federated learning (FL) with M3T FMs to enable collaborative, privacy-preserving training across decentralized institutions while accommodating diverse modalities and tasks. Subsequently, this position paper aims to unveil M3T FedFMs as a promising yet underexplored approach to the education community, explore its potentials, and reveal its related future research directions. We outline how M3T FedFMs can advance three critical pillars of next-generation intelligent education systems: (i) privacy preservation, by keeping sensitive multi-modal student and institutional data local; (ii) personalization, through modular architectures enabling tailored models for students, instructors, and institutions; and (iii) equity and inclusivity, by facilitating participation from underrepresented and resource-constrained entities. We finally identify various open research challenges, including studying of (i) inter-institution heterogeneous privacy regulations, (ii) the non-uniformity of data modalities' characteristics, (iii) the unlearning approaches for M3T FedFMs, (iv) the continual learning frameworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which must be collectively addressed for practical deployment.</li>
</ul>

<h3>Title: ACE and Diverse Generalization via Selective Disagreement</h3>
<ul>
<li><strong>Authors: </strong>Oliver Daniels, Stuart Armstrong, Alexandre Maranhão, Mahirah Fairuz Rahman, Benjamin M. Marlin, Rebecca Gorman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07955">https://arxiv.org/abs/2509.07955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07955">https://arxiv.org/pdf/2509.07955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07955]] ACE and Diverse Generalization via Selective Disagreement(https://arxiv.org/abs/2509.07955)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks are notoriously sensitive to spurious correlations - where a model learns a shortcut that fails out-of-distribution. Existing work on spurious correlations has often focused on incomplete correlations,leveraging access to labeled instances that break the correlation. But in cases where the spurious correlations are complete, the correct generalization is fundamentally \textit{underspecified}. To resolve this underspecification, we propose learning a set of concepts that are consistent with training data but make distinct predictions on a subset of novel unlabeled inputs. Using a self-training approach that encourages \textit{confident} and \textit{selective} disagreement, our method ACE matches or outperforms existing methods on a suite of complete-spurious correlation benchmarks, while remaining robust to incomplete spurious correlations. ACE is also more configurable than prior approaches, allowing for straight-forward encoding of prior knowledge and principled unsupervised model selection. In an early application to language-model alignment, we find that ACE achieves competitive performance on the measurement tampering detection benchmark \textit{without} access to untrusted measurements. While still subject to important limitations, ACE represents significant progress towards overcoming underspecification.</li>
</ul>

<h3>Title: Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images</h3>
<ul>
<li><strong>Authors: </strong>Boammani Aser Lompo, Marc Haraoui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07966">https://arxiv.org/abs/2509.07966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07966">https://arxiv.org/pdf/2509.07966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07966]] Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images(https://arxiv.org/abs/2509.07966)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual reasoning over structured data such as tables is a critical capability for modern vision-language models (VLMs), yet current benchmarks remain limited in scale, diversity, or reasoning depth, especially when it comes to rendered table images. Addressing this gap, we introduce Visual-TableQA, a large-scale, open-domain multimodal dataset specifically designed to evaluate and enhance visual reasoning over complex tabular data. Our generation pipeline is modular, scalable, and fully autonomous, involving multiple reasoning LLMs collaborating across distinct roles: generation, validation, and inspiration. Visual-TableQA comprises 2.5k richly structured LaTeX-rendered tables and 6k reasoning-intensive QA pairs, all produced at a cost of under USD 100. To promote diversity and creativity, our pipeline performs multi-model collaborative data generation via cross-model prompting ('inspiration') and LLM-jury filtering. Stronger models seed layouts and topics that weaker models elaborate, collectively distilling diverse reasoning patterns and visual structures into the dataset. Empirical results show that models fine-tuned on Visual-TableQA generalize robustly to external benchmarks, outperforming several proprietary models despite the dataset's synthetic nature. The full pipeline and resources are publicly available at this https URL.</li>
</ul>

<h3>Title: SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Lukas Haas, Gal Yona, Giovanni D'Antonio, Sasha Goldshtein, Dipanjan Das</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07968">https://arxiv.org/abs/2509.07968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07968">https://arxiv.org/pdf/2509.07968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07968]] SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge(https://arxiv.org/abs/2509.07968)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It addresses critical limitations in OpenAI's benchmark, including noisy and incorrect labels, topical biases, and question redundancy. SimpleQA Verified was created through a rigorous multi-stage filtering process involving de-duplication, topic balancing, and source reconciliation to produce a more reliable and challenging evaluation set, alongside improvements in the autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a state-of-the-art F1-score of 55.6, outperforming other frontier models, including GPT-5. This work provides the research community with a higher-fidelity tool to track genuine progress in parametric model factuality and to mitigate hallucinations. The benchmark dataset, evaluation code, and leaderboard are available at: this https URL.</li>
</ul>

<h3>Title: One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zheng Geng, Nan Wang, Shaocong Xu, Chongjie Ye, Bohan Li, Zhaoxi Chen, Sida Peng, Hao Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07978">https://arxiv.org/abs/2509.07978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07978">https://arxiv.org/pdf/2509.07978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07978]] One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation(https://arxiv.org/abs/2509.07978)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Estimating the 6D pose of arbitrary unseen objects from a single reference image is critical for robotics operating in the long-tail of real-world instances. However, this setting is notoriously challenging: 3D models are rarely available, single-view reconstructions lack metric scale, and domain gaps between generated models and real-world images undermine robustness. We propose OnePoseViaGen, a pipeline that tackles these challenges through two key components. First, a coarse-to-fine alignment module jointly refines scale and pose by combining multi-view feature matching with render-and-compare refinement. Second, a text-guided generative domain randomization strategy diversifies textures, enabling effective fine-tuning of pose estimators with synthetic data. Together, these steps allow high-fidelity single-view 3D generation to support reliable one-shot 6D pose estimation. On challenging benchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves state-of-the-art performance far surpassing prior approaches. We further demonstrate robust dexterous grasping with a real robot hand, validating the practicality of our method in real-world manipulation. Project page: this https URL</li>
</ul>

<h3>Title: Visual Representation Alignment for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Heeji Yoon, Jaewoo Jung, Junwan Kim, Hyungyu Choi, Heeseong Shin, Sangbeom Lim, Honggyu An, Chaehyun Kim, Jisang Han, Donghyun Kim, Chanho Eom, Sunghwan Hong, Seungryong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07979">https://arxiv.org/abs/2509.07979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07979">https://arxiv.org/pdf/2509.07979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07979]] Visual Representation Alignment for Multimodal Large Language Models(https://arxiv.org/abs/2509.07979)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details during training. In this paper, we present VIsual Representation ALignment (VIRAL), a simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from the input vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments demonstrate consistent improvements across all tasks on widely adopted multimodal benchmarks. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs.</li>
</ul>

<h3>Title: Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Xinyu Yang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07980">https://arxiv.org/abs/2509.07980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07980">https://arxiv.org/pdf/2509.07980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07980]] Parallel-R1: Towards Parallel Thinking via Reinforcement Learning(https://arxiv.org/abs/2509.07980)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose \textbf{Parallel-R1}, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a \textbf{mid-training exploration scaffold}, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
