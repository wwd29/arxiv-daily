<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: A Deep Dive into VirusTotal: Characterizing and Clustering a Massive File Feed. (arXiv:2210.15973v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15973">http://arxiv.org/abs/2210.15973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15973] A Deep Dive into VirusTotal: Characterizing and Clustering a Massive File Feed](http://arxiv.org/abs/2210.15973)</code></li>
<li>Summary: <p>Online scanners analyze user-submitted files with a large number of security
tools and provide access to the analysis results. As the most popular online
scanner, VirusTotal (VT) is often used for determining if samples are
malicious, labeling samples with their family, hunting for new threats, and
collecting malware samples. We analyze 328M VT reports for 235M samples
collected for one year through the VT file feed. We use the reports to
characterize the VT file feed in depth and compare it with the telemetry of a
large security vendor. We answer questions such as How diverse is the feed?
Does it allow building malware datasets for different filetypes? How fresh are
the samples it provides? What is the distribution of malware families it sees?
Does that distribution really represent malware on user devices?
</p></li>
</ul>

<p>We then explore how to perform threat hunting at scale by investigating
scalable approaches that can produce high purity clusters on the 235M feed
samples. We investigate three clustering approaches: hierarchical agglomerative
clustering (HAC), a more scalable HAC variant for TLSH digests (HAC-T), and a
simple feature value grouping (FVG). Our results show that HAC-T and FVG using
selected features produce high precision clusters on ground truth datasets.
However, only FVG scales to the daily influx of samples in the feed. Moreover,
FVG takes 15 hours to cluster the whole dataset of 235M samples. Finally, we
use the produced clusters for threat hunting, namely for detecting 190K samples
thought to be benign (i.e., with zero detections) that may really be malicious
because they belong to 29K clusters where most samples are detected as
malicious.
</p>

<h2>privacy</h2>
<h3>Title: BRATsynthetic: Text De-identification using a Markov Chain Replacement Strategy for Surrogate Personal Identifying Information. (arXiv:2210.16125v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16125">http://arxiv.org/abs/2210.16125</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16125] BRATsynthetic: Text De-identification using a Markov Chain Replacement Strategy for Surrogate Personal Identifying Information](http://arxiv.org/abs/2210.16125)</code></li>
<li>Summary: <p>Objective: Implement and assess personal health identifying information (PHI)
substitution strategies and quantify their privacy preserving benefits.
</p></li>
</ul>

<p>Materials and Methods: We implement and assess 3 different `Hiding in Plain
Sight` (HIPS) strategies for PHI replacement including a standard Consistent
replacement strategy, a Random replacement strategy and a novel Markov
model-based strategy. We evaluate the privacy preserving benefits of these
strategies on a synthetic PHI distribution and real clinical corpora from 2
different institutions using a range of false negative error rates (FNER).
</p>
<p>Results: Using FNER ranging from 0.1% to 5% PHI leakage at the document level
could be reduced from 27.1% to 0.1% (0.1% FNER) and from 94.2% to 57.7% (5%
FNER) utilizing the Markov chain strategy versus the Consistent strategy on a
corpus containing a diverse set of notes from the University of Alabama at
Birmingham (UAB). The Markov chain substitution strategy also consistently
outperformed the Consistent and Random substitution strategies in a MIMIC
corpus of discharge summaries and on a range of synthetic clinical PHI
distributions. Discussion: We demonstrate that a Markov chain surrogate
generation strategy substantially reduces the chance of inadvertent PHI release
across a range of assumed PHI FNER and release our implementation
`BRATsynthetic` on Github.
</p>
<p>Conclusion: The Markov chain replacement strategy allows for the release of
larger de-identified corpora at the same risk level relative to corpora
released using a consistent HIPS strategy.
</p>

<h2>protect</h2>
<h3>Title: DELFI: Deep Mixture Models for Long-term Air Quality Forecasting in the Delhi National Capital Region. (arXiv:2210.15923v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15923">http://arxiv.org/abs/2210.15923</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15923] DELFI: Deep Mixture Models for Long-term Air Quality Forecasting in the Delhi National Capital Region](http://arxiv.org/abs/2210.15923)</code></li>
<li>Summary: <p>The identification and control of human factors in climate change is a
rapidly growing concern and robust, real-time air-quality monitoring and
forecasting plays a critical role in allowing effective policy formulation and
implementation. This paper presents DELFI, a novel deep learning-based mixture
model to make effective long-term predictions of Particulate Matter (PM) 2.5
concentrations. A key novelty in DELFI is its multi-scale approach to the
forecasting problem. The observation that point predictions are more suitable
in the short-term and probabilistic predictions in the long-term allows
accurate predictions to be made as much as 24 hours in advance. DELFI
incorporates meteorological data as well as pollutant-based features to ensure
a robust model that is divided into two parts: (i) a stack of three Long
Short-Term Memory (LSTM) networks that perform differential modelling of the
same window of past data, and (ii) a fully-connected layer enabling attention
to each of the components. Experimental evaluation based on deployment of 13
stations in the Delhi National Capital Region (Delhi-NCR) in India establishes
that DELFI offers far superior predictions especially in the long-term as
compared to even non-parametric baselines. The Delhi-NCR recorded the 3rd
highest PM levels amongst 39 mega-cities across the world during 2011-2015 and
DELFI's performance establishes it as a potential tool for effective long-term
forecasting of PM levels to enable public health management and environment
protection.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Improving Transferability of Adversarial Examples on Face Recognition with Beneficial Perturbation Feature Augmentation. (arXiv:2210.16117v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16117">http://arxiv.org/abs/2210.16117</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16117] Improving Transferability of Adversarial Examples on Face Recognition with Beneficial Perturbation Feature Augmentation](http://arxiv.org/abs/2210.16117)</code></li>
<li>Summary: <p>Face recognition (FR) models can be easily fooled by adversarial examples,
which are crafted by adding imperceptible perturbations on benign face images.
To improve the transferability of adversarial examples on FR models, we propose
a novel attack method called Beneficial Perturbation Feature Augmentation
Attack (BPFA), which reduces the overfitting of the adversarial examples to
surrogate FR models by the adversarial strategy. Specifically, in the
backpropagation step, BPFA records the gradients on pre-selected features and
uses the gradient on the input image to craft adversarial perturbation to be
added on the input image. In the next forward propagation step, BPFA leverages
the recorded gradients to add perturbations(i.e., beneficial perturbations)
that can be pitted against the adversarial perturbation added on the input
image on their corresponding features. The above two steps are repeated until
the last backpropagation step before the maximum number of iterations is
reached. The optimization process of the adversarial perturbation added on the
input image and the optimization process of the beneficial perturbations added
on the features correspond to a minimax two-player game. Extensive experiments
demonstrate that BPFA outperforms the state-of-the-art gradient-based
adversarial attacks on FR.
</p></li>
</ul>

<h3>Title: TAD: Transfer Learning-based Multi-Adversarial Detection of Evasion Attacks against Network Intrusion Detection Systems. (arXiv:2210.15700v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15700">http://arxiv.org/abs/2210.15700</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15700] TAD: Transfer Learning-based Multi-Adversarial Detection of Evasion Attacks against Network Intrusion Detection Systems](http://arxiv.org/abs/2210.15700)</code></li>
<li>Summary: <p>Nowadays, intrusion detection systems based on deep learning deliver
state-of-the-art performance. However, recent research has shown that specially
crafted perturbations, called adversarial examples, are capable of
significantly reducing the performance of these intrusion detection systems.
The objective of this paper is to design an efficient transfer learning-based
adversarial detector and then to assess the effectiveness of using multiple
strategically placed adversarial detectors compared to a single adversarial
detector for intrusion detection systems. In our experiments, we implement
existing state-of-the-art models for intrusion detection. We then attack those
models with a set of chosen evasion attacks. In an attempt to detect those
adversarial attacks, we design and implement multiple transfer learning-based
adversarial detectors, each receiving a subset of the information passed
through the IDS. By combining their respective decisions, we illustrate that
combining multiple detectors can further improve the detectability of
adversarial traffic compared to a single detector in the case of a parallel IDS
design.
</p></li>
</ul>

<h3>Title: Ethereum Proof-of-Stake under Scrutiny. (arXiv:2210.16070v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16070">http://arxiv.org/abs/2210.16070</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16070] Ethereum Proof-of-Stake under Scrutiny](http://arxiv.org/abs/2210.16070)</code></li>
<li>Summary: <p>Ethereum has undergone a recent change called \textit{the Merge}, which made
Ethereum a Proof-of-Stake blockchain shifting closer to BFT consensus.
Ethereum, which wished to keep the best of the two protocols designs (BFT and
Nakomoto-style), now has an involved consensus protocol as its core. The result
is a blockchain being possibly produced in a tree-like form while participants
try to finalize blocks. Several attacks jeopardizing liveness have been found
in this new setting. The Ethereum community has responded by creating a patch.
We discovered a new attack on the patched protocol. To support our analysis, we
propose a new formalization of the properties of liveness and availability of
the Ethereum blockchain, and we provide a pseudo-code. We believe this
formalization to be helpful for other analyses as well. Our results yield that
the Ethereum Proof-of-Stake has probabilistic liveness, influenced by the
parameter describing the time frame allowed for validators to change their mind
about the current main chain.
</p></li>
</ul>

<h3>Title: Local Model Reconstruction Attacks in Federated Learning and their Uses. (arXiv:2210.16205v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16205">http://arxiv.org/abs/2210.16205</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16205] Local Model Reconstruction Attacks in Federated Learning and their Uses](http://arxiv.org/abs/2210.16205)</code></li>
<li>Summary: <p>In this paper, we initiate the study of local model reconstruction attacks
for federated learning, where a honest-but-curious adversary eavesdrops the
messages exchanged between a targeted client and the server, and then
reconstructs the local/personalized model of the victim. The local model
reconstruction attack allows the adversary to trigger other classical attacks
in a more effective way, since the local model only depends on the client's
data and can leak more private information than the global model learned by the
server. Additionally, we propose a novel model-based attribute inference attack
in federated learning leveraging the local model reconstruction attack. We
provide an analytical lower-bound for this attribute inference attack.
Empirical results using real world datasets confirm that our local
reconstruction attack works well for both regression and classification tasks.
Moreover, we benchmark our novel attribute inference attack against the
state-of-the-art attacks in federated learning. Our attack results in higher
reconstruction accuracy especially when the clients' datasets are
heterogeneous. Our work provides a new angle for designing powerful and
explainable attacks to effectively quantify the privacy risk in FL.
</p></li>
</ul>

<h3>Title: On the Vulnerability of Data Points under Multiple Membership Inference Attacks and Target Models. (arXiv:2210.16258v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16258">http://arxiv.org/abs/2210.16258</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16258] On the Vulnerability of Data Points under Multiple Membership Inference Attacks and Target Models](http://arxiv.org/abs/2210.16258)</code></li>
<li>Summary: <p>Membership Inference Attacks (MIAs) infer whether a data point is in the
training data of a machine learning model. It is a threat while being in the
training data is private information of a data point. MIA correctly infers some
data points as members or non-members of the training data. Intuitively, data
points that MIA accurately detects are vulnerable. Considering those data
points may exist in different target models susceptible to multiple MIAs, the
vulnerability of data points under multiple MIAs and target models is worth
exploring.
</p></li>
</ul>

<p>This paper defines new metrics that can reflect the actual situation of data
points' vulnerability and capture vulnerable data points under multiple MIAs
and target models. From the analysis, MIA has an inference tendency to some
data points despite a low overall inference performance. Additionally, we
implement 54 MIAs, whose average attack accuracy ranges from 0.5 to 0.9, to
support our analysis with our scalable and flexible platform, Membership
Inference Attacks Platform (VMIAP). Furthermore, previous methods are
unsuitable for finding vulnerable data points under multiple MIAs and different
target models. Finally, we observe that the vulnerability is not characteristic
of the data point but related to the MIA and target model.
</p>

<h3>Title: Review on Classification Techniques used in Biophysiological Stress Monitoring. (arXiv:2210.16040v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16040">http://arxiv.org/abs/2210.16040</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16040] Review on Classification Techniques used in Biophysiological Stress Monitoring](http://arxiv.org/abs/2210.16040)</code></li>
<li>Summary: <p>Cardiovascular activities are directly related to the response of a body in a
stressed condition. Stress, based on its intensity, can be divided into two
types i.e. Acute stress (short-term stress) and Chronic stress (long-term
stress). Repeated acute stress and continuous chronic stress may play a vital
role in inflammation in the circulatory system and thus leads to a heart attack
or to a stroke. In this study, we have reviewed commonly used machine learning
classification techniques applied to different stress-indicating parameters
used in stress monitoring devices. These parameters include Photoplethysmograph
(PPG), Electrocardiographs (ECG), Electromyograph (EMG), Galvanic Skin Response
(GSR), Heart Rate Variation (HRV), skin temperature, respiratory rate,
Electroencephalograph (EEG) and salivary cortisol, used in stress monitoring
devices. This study also provides a discussion on choosing a classifier, which
depends upon a number of factors other than accuracy, like the number of
subjects involved in an experiment, type of signals processing and
computational limitations.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: BI AVAN: Brain inspired Adversarial Visual Attention Network. (arXiv:2210.15790v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15790">http://arxiv.org/abs/2210.15790</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15790] BI AVAN: Brain inspired Adversarial Visual Attention Network](http://arxiv.org/abs/2210.15790)</code></li>
<li>Summary: <p>Visual attention is a fundamental mechanism in the human brain, and it
inspires the design of attention mechanisms in deep neural networks. However,
most of the visual attention studies adopted eye-tracking data rather than the
direct measurement of brain activity to characterize human visual attention. In
addition, the adversarial relationship between the attention-related objects
and attention-neglected background in the human visual system was not fully
exploited. To bridge these gaps, we propose a novel brain-inspired adversarial
visual attention network (BI-AVAN) to characterize human visual attention
directly from functional brain activity. Our BI-AVAN model imitates the biased
competition process between attention-related/neglected objects to identify and
locate the visual objects in a movie frame the human brain focuses on in an
unsupervised manner. We use independent eye-tracking data as ground truth for
validation and experimental results show that our model achieves robust and
promising results when inferring meaningful human visual attention and mapping
the relationship between brain activities and visual stimuli. Our BI-AVAN model
contributes to the emerging field of leveraging the brain's functional
architecture to inspire and guide the model design in artificial intelligence
(AI), e.g., deep neural networks.
</p></li>
</ul>

<h3>Title: FUSSL: Fuzzy Uncertain Self Supervised Learning. (arXiv:2210.15818v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15818">http://arxiv.org/abs/2210.15818</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15818] FUSSL: Fuzzy Uncertain Self Supervised Learning](http://arxiv.org/abs/2210.15818)</code></li>
<li>Summary: <p>Self supervised learning (SSL) has become a very successful technique to
harness the power of unlabeled data, with no annotation effort. A number of
developed approaches are evolving with the goal of outperforming supervised
alternatives, which have been relatively successful. One main issue in SSL is
robustness of the approaches under different settings. In this paper, for the
first time, we recognize the fundamental limits of SSL coming from the use of a
single-supervisory signal. To address this limitation, we leverage the power of
uncertainty representation to devise a robust and general standard hierarchical
learning/training protocol for any SSL baseline, regardless of their
assumptions and approaches. Essentially, using the information bottleneck
principle, we decompose feature learning into a two-stage training procedure,
each with a distinct supervision signal. This double supervision approach is
captured in two key steps: 1) invariance enforcement to data augmentation, and
2) fuzzy pseudo labeling (both hard and soft annotation). This simple, yet,
effective protocol which enables cross-class/cluster feature learning, is
instantiated via an initial training of an ensemble of models through
invariance enforcement to data augmentation as first training phase, and then
assigning fuzzy labels to the original samples for the second training phase.
We consider multiple alternative scenarios with double supervision and evaluate
the effectiveness of our approach on recent baselines, covering four different
SSL paradigms, including geometrical, contrastive, non-contrastive, and
hard/soft whitening (redundancy reduction) baselines. Extensive experiments
under multiple settings show that the proposed training protocol consistently
improves the performance of the former baselines, independent of their
respective underlying principles.
</p></li>
</ul>

<h3>Title: Facial Action Unit Detection and Intensity Estimation from Self-supervised Representation. (arXiv:2210.15878v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15878">http://arxiv.org/abs/2210.15878</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15878] Facial Action Unit Detection and Intensity Estimation from Self-supervised Representation](http://arxiv.org/abs/2210.15878)</code></li>
<li>Summary: <p>As a fine-grained and local expression behavior measurement, facial action
unit (FAU) analysis (e.g., detection and intensity estimation) has been
documented for its time-consuming, labor-intensive, and error-prone annotation.
Thus a long-standing challenge of FAU analysis arises from the data scarcity of
manual annotations, limiting the generalization ability of trained models to a
large extent. Amounts of previous works have made efforts to alleviate this
issue via semi/weakly supervised methods and extra auxiliary information.
However, these methods still require domain knowledge and have not yet avoided
the high dependency on data annotation. This paper introduces a robust facial
representation model MAE-Face for AU analysis. Using masked autoencoding as the
self-supervised pre-training approach, MAE-Face first learns a high-capacity
model from a feasible collection of face images without additional data
annotations. Then after being fine-tuned on AU datasets, MAE-Face exhibits
convincing performance for both AU detection and AU intensity estimation,
achieving a new state-of-the-art on nearly all the evaluation results. Further
investigation shows that MAE-Face achieves decent performance even when
fine-tuned on only 1\% of the AU training set, strongly proving its robustness
and generalization performance.
</p></li>
</ul>

<h3>Title: Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing. (arXiv:2210.15889v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15889">http://arxiv.org/abs/2210.15889</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15889] Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing](http://arxiv.org/abs/2210.15889)</code></li>
<li>Summary: <p>Neural-symbolic computing (NeSy), which pursues the integration of the
symbolic and statistical paradigms of cognition, has been an active research
area of Artificial Intelligence (AI) for many years. As NeSy shows promise of
reconciling the advantages of reasoning and interpretability of symbolic
representation and robust learning in neural networks, it may serve as a
catalyst for the next generation of AI. In the present paper, we provide a
systematic overview of the important and recent developments of research on
NeSy AI. Firstly, we introduce study history and background concepts of this
area. Afterward, we categorize recent approaches along several main
characteristics that underline this research paradigm, including
neural-symbolic interrelation, neural architecture, knowledge representation,
and functionality. Then, we briefly discuss the successful application of
modern NeSy approaches in several domains. Finally, we identify the open
problems together with potential future research directions.
</p></li>
</ul>

<h3>Title: GeoGCN: Geometric Dual-domain Graph Convolution Network for Point Cloud Denoising. (arXiv:2210.15913v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15913">http://arxiv.org/abs/2210.15913</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15913] GeoGCN: Geometric Dual-domain Graph Convolution Network for Point Cloud Denoising](http://arxiv.org/abs/2210.15913)</code></li>
<li>Summary: <p>We propose GeoGCN, a novel geometric dual-domain graph convolution network
for point cloud denoising (PCD). Beyond the traditional wisdom of PCD, to fully
exploit the geometric information of point clouds, we define two kinds of
surface normals, one is called Real Normal (RN), and the other is Virtual
Normal (VN). RN preserves the local details of noisy point clouds while VN
avoids the global shape shrinkage during denoising. GeoGCN is a new PCD
paradigm that, 1) first regresses point positions by spatialbased GCN with the
help of VNs, 2) then estimates initial RNs by performing Principal Component
Analysis on the regressed points, and 3) finally regresses fine RNs by
normalbased GCN. Unlike existing PCD methods, GeoGCN not only exploits two
kinds of geometry expertise (i.e., RN and VN) but also benefits from training
data. Experiments validate that GeoGCN outperforms SOTAs in terms of both
noise-robustness and local-and-global feature preservation.
</p></li>
</ul>

<h3>Title: PSFormer: Point Transformer for 3D Salient Object Detection. (arXiv:2210.15933v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15933">http://arxiv.org/abs/2210.15933</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15933] PSFormer: Point Transformer for 3D Salient Object Detection](http://arxiv.org/abs/2210.15933)</code></li>
<li>Summary: <p>We propose PSFormer, an effective point transformer model for 3D salient
object detection. PSFormer is an encoder-decoder network that takes full
advantage of transformers to model the contextual information in both
multi-scale point- and scene-wise manners. In the encoder, we develop a Point
Context Transformer (PCT) module to capture region contextual features at the
point level; PCT contains two different transformers to excavate the
relationship among points. In the decoder, we develop a Scene Context
Transformer (SCT) module to learn context representations at the scene level;
SCT contains both Upsampling-and-Transformer blocks and Multi-context
Aggregation units to integrate the global semantic and multi-level features
from the encoder into the global scene context. Experiments show clear
improvements of PSFormer over its competitors and validate that PSFormer is
more robust to challenging cases such as small objects, multiple objects, and
objects with complex structures.
</p></li>
</ul>

<h3>Title: Matching entropy based disparity estimation from light field. (arXiv:2210.15948v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15948">http://arxiv.org/abs/2210.15948</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15948] Matching entropy based disparity estimation from light field](http://arxiv.org/abs/2210.15948)</code></li>
<li>Summary: <p>A major challenge for matching-based depth estimation is to prevent
mismatches in occlusion and smooth regions. An effective matching window
satisfying three characteristics: texture richness, disparity consistency and
anti-occlusion should be able to prevent mismatches to some extent. According
to these characteristics, we propose matching entropy in the spatial domain of
light field to measure the amount of correct information in a matching window,
which provides the criterion for matching window selection. Based on matching
entropy regularization, we establish an optimization model for depth estimation
with a matching cost fidelity term. To find the optimum, we propose a two-step
adaptive matching algorithm. First, the region type is adaptively determined to
identify occluding, occluded, smooth and textured regions. Then, the matching
entropy criterion is used to adaptively select the size and shape of matching
windows, as well as the visible viewpoints. The two-step process can reduce
mismatches and redundant calculations by selecting effective matching windows.
The experimental results on synthetic and real data show that the proposed
method can effectively improve the accuracy of depth estimation in occlusion
and smooth regions and has strong robustness for different noise levels.
Therefore, high-precision depth estimation from 4D light field data is
achieved.
</p></li>
</ul>

<h3>Title: Benchmarking performance of object detection under image distortions in an uncontrolled environment. (arXiv:2210.15999v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15999">http://arxiv.org/abs/2210.15999</a></li>
<li>Code URL: <a href="https://github.com/aymanbegh/benchmarking-performance">https://github.com/aymanbegh/benchmarking-performance</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15999] Benchmarking performance of object detection under image distortions in an uncontrolled environment](http://arxiv.org/abs/2210.15999)</code></li>
<li>Summary: <p>The robustness of object detection algorithms plays a prominent role in
real-world applications, especially in uncontrolled environments due to
distortions during image acquisition. It has been proven that the performance
of object detection methods suffers from in-capture distortions. In this study,
we present a performance evaluation framework for the state-of-the-art object
detection methods using a dedicated dataset containing images with various
distortions at different levels of severity. Furthermore, we propose an
original strategy of image distortion generation applied to the MS-COCO dataset
that combines some local and global distortions to reach much better
performances. We have shown that training using the proposed dataset improves
the robustness of object detection by 31.5\%. Finally, we provide a custom
dataset including natural images distorted from MS-COCO to perform a more
reliable evaluation of the robustness against common distortions. The database
and the generation source codes of the different distortions are made publicly
available
</p></li>
</ul>

<h3>Title: Addressing Bias in Face Detectors using Decentralised Data collection with incentives. (arXiv:2210.16024v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16024">http://arxiv.org/abs/2210.16024</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16024] Addressing Bias in Face Detectors using Decentralised Data collection with incentives](http://arxiv.org/abs/2210.16024)</code></li>
<li>Summary: <p>Recent developments in machine learning have shown that successful models do
not rely only on huge amounts of data but the right kind of data. We show in
this paper how this data-centric approach can be facilitated in a decentralized
manner to enable efficient data collection for algorithms. Face detectors are a
class of models that suffer heavily from bias issues as they have to work on a
large variety of different data. We also propose a face detection and
anonymization approach using a hybrid MultiTask Cascaded CNN with FaceNet
Embeddings to benchmark multiple datasets to describe and evaluate the bias in
the models towards different ethnicities, gender, and age groups along with
ways to enrich fairness in a decentralized system of data labeling, correction,
and verification by users to create a robust pipeline for model retraining.
</p></li>
</ul>

<h3>Title: Rawgment: Noise-Accounted RAW Augmentation Enables Recognition in a Wide Variety of Environments. (arXiv:2210.16046v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16046">http://arxiv.org/abs/2210.16046</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16046] Rawgment: Noise-Accounted RAW Augmentation Enables Recognition in a Wide Variety of Environments](http://arxiv.org/abs/2210.16046)</code></li>
<li>Summary: <p>Image recognition models that can work in challenging environments (e.g.,
extremely dark, blurry, or high dynamic range conditions) must be useful.
However, creating a training dataset for such environments is expensive and
hard due to the difficulties of data collection and annotation. It is desirable
if we could get a robust model without the need of hard-to-obtain dataset. One
simple approach is to apply data augmentation such as color jitter and blur to
standard RGB (sRGB) images in simple scenes. Unfortunately, this approach
struggles to yield realistic images in terms of pixel intensity and noise
distribution due to not considering the non-linearity of Image Signal Processor
(ISP) and noise characteristics of an image sensor. Instead, we propose a
noise-accounted RAW image augmentation method. In essence, color jitter and
blur augmentation are applied to a RAW image before applying non-linear ISP,
yielding realistic intensity. Furthermore, we introduce a noise amount
alignment method that calibrates the domain gap in noise property caused by the
augmentation. We show that our proposed noise-accounted RAW augmentation method
doubles the image recognition accuracy in challenging environments only with
simple training data.
</p></li>
</ul>

<h3>Title: Localized Randomized Smoothing for Collective Robustness Certification. (arXiv:2210.16140v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16140">http://arxiv.org/abs/2210.16140</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16140] Localized Randomized Smoothing for Collective Robustness Certification](http://arxiv.org/abs/2210.16140)</code></li>
<li>Summary: <p>Models for image segmentation, node classification and many other tasks map a
single input to multiple labels. By perturbing this single shared input (e.g.
the image) an adversary can manipulate several predictions (e.g. misclassify
several pixels). Collective robustness certification is the task of provably
bounding the number of robust predictions under this threat model. The only
dedicated method that goes beyond certifying each output independently is
limited to strictly local models, where each prediction is associated with a
small receptive field. We propose a more general collective robustness
certificate for all types of models and further show that this approach is
beneficial for the larger class of softly local models, where each output is
dependent on the entire input but assigns different levels of importance to
different input regions (e.g. based on their proximity in the image). The
certificate is based on our novel localized randomized smoothing approach,
where the random perturbation strength for different input regions is
proportional to their importance for the outputs. Localized smoothing
Pareto-dominates existing certificates on both image segmentation and node
classification tasks, simultaneously offering higher accuracy and stronger
guarantees.
</p></li>
</ul>

<h3>Title: Boulders Identification on Small Bodies Under Varying Illumination Conditions. (arXiv:2210.16283v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16283">http://arxiv.org/abs/2210.16283</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16283] Boulders Identification on Small Bodies Under Varying Illumination Conditions](http://arxiv.org/abs/2210.16283)</code></li>
<li>Summary: <p>The capability to detect boulders on the surface of small bodies is
beneficial for vision-based applications such as navigation and hazard
detection during critical operations. This task is challenging due to the wide
assortment of irregular shapes, the characteristics of the boulders population,
and the rapid variability in the illumination conditions. The authors address
this challenge by designing a multi-step training approach to develop a
data-driven image processing pipeline to robustly detect and segment boulders
scattered over the surface of a small body. Due to the limited availability of
labeled image-mask pairs, the developed methodology is supported by two
artificial environments designed in Blender specifically for this work. These
are used to generate a large amount of synthetic image-label sets, which are
made publicly available to the image processing community. The methodology
presented addresses the challenges of varying illumination conditions,
irregular shapes, fast training time, extensive exploration of the architecture
design space, and domain gap between synthetic and real images from previously
flown missions. The performance of the developed image processing pipeline is
tested both on synthetic and real images, exhibiting good performances, and
high generalization capabilities
</p></li>
</ul>

<h3>Title: Leveraging Label Correlations in a Multi-label Setting: A Case Study in Emotion. (arXiv:2210.15842v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15842">http://arxiv.org/abs/2210.15842</a></li>
<li>Code URL: <a href="https://github.com/gchochla/demux-memo">https://github.com/gchochla/demux-memo</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15842] Leveraging Label Correlations in a Multi-label Setting: A Case Study in Emotion](http://arxiv.org/abs/2210.15842)</code></li>
<li>Summary: <p>Detecting emotions expressed in text has become critical to a range of
fields. In this work, we investigate ways to exploit label correlations in
multi-label emotion recognition models to improve emotion detection. First, we
develop two modeling approaches to the problem in order to capture word
associations of the emotion words themselves, by either including the emotions
in the input, or by leveraging Masked Language Modeling (MLM). Second, we
integrate pairwise constraints of emotion representations as regularization
terms alongside the classification loss of the models. We split these terms
into two categories, local and global. The former dynamically change based on
the gold labels, while the latter remain static during training. We demonstrate
state-of-the-art performance across Spanish, English, and Arabic in SemEval
2018 Task 1 E-c using monolingual BERT-based models. On top of better
performance, we also demonstrate improved robustness. Code is available at
https://github.com/gchochla/Demux-MEmo.
</p></li>
</ul>

<h3>Title: RoChBert: Towards Robust BERT Fine-tuning for Chinese. (arXiv:2210.15944v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15944">http://arxiv.org/abs/2210.15944</a></li>
<li>Code URL: <a href="https://github.com/zzh-z/rochbert">https://github.com/zzh-z/rochbert</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15944] RoChBert: Towards Robust BERT Fine-tuning for Chinese](http://arxiv.org/abs/2210.15944)</code></li>
<li>Summary: <p>Despite of the superb performance on a wide range of tasks, pre-trained
language models (e.g., BERT) have been proved vulnerable to adversarial texts.
In this paper, we present RoChBERT, a framework to build more Robust BERT-based
models by utilizing a more comprehensive adversarial graph to fuse Chinese
phonetic and glyph features into pre-trained representations during
fine-tuning. Inspired by curriculum learning, we further propose to augment the
training dataset with adversarial texts in combination with intermediate
samples. Extensive experiments demonstrate that RoChBERT outperforms previous
methods in significant ways: (i) robust -- RoChBERT greatly improves the model
robustness without sacrificing accuracy on benign texts. Specifically, the
defense lowers the success rates of unlimited and limited attacks by 59.43% and
39.33% respectively, while remaining accuracy of 93.30%; (ii) flexible --
RoChBERT can easily extend to various language models to solve different
downstream tasks with excellent performance; and (iii) efficient -- RoChBERT
can be directly applied to the fine-tuning stage without pre-training language
model from scratch, and the proposed data augmentation method is also low-cost.
</p></li>
</ul>

<h3>Title: BEBERT: Efficient and robust binary ensemble BERT. (arXiv:2210.15976v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15976">http://arxiv.org/abs/2210.15976</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15976] BEBERT: Efficient and robust binary ensemble BERT](http://arxiv.org/abs/2210.15976)</code></li>
<li>Summary: <p>Pre-trained BERT models have achieved impressive accuracy on natural language
processing (NLP) tasks. However, their excessive amount of parameters hinders
them from efficient deployment on edge devices. Binarization of the BERT models
can significantly alleviate this issue but comes with a severe accuracy drop
compared with their full-precision counterparts. In this paper, we propose an
efficient and robust binary ensemble BERT (BEBERT) to bridge the accuracy gap.
To the best of our knowledge, this is the first work employing ensemble
techniques on binary BERTs, yielding BEBERT, which achieves superior accuracy
while retaining computational efficiency. Furthermore, we remove the knowledge
distillation procedures during ensemble to speed up the training process
without compromising accuracy. Experimental results on the GLUE benchmark show
that the proposed BEBERT significantly outperforms the existing binary BERT
models in accuracy and robustness with a 2x speedup on training time. Moreover,
our BEBERT has only a negligible accuracy loss of 0.3% compared to the
full-precision baseline while saving 15x and 13x in FLOPs and model size,
respectively. In addition, BEBERT also outperforms other compressed BERTs in
accuracy by up to 6.7%.
</p></li>
</ul>

<h3>Title: Probing for targeted syntactic knowledge through grammatical error detection. (arXiv:2210.16228v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16228">http://arxiv.org/abs/2210.16228</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16228] Probing for targeted syntactic knowledge through grammatical error detection](http://arxiv.org/abs/2210.16228)</code></li>
<li>Summary: <p>Targeted studies testing knowledge of subject-verb agreement (SVA) indicate
that pre-trained language models encode syntactic information. We assert that
if models robustly encode subject-verb agreement, they should be able to
identify when agreement is correct and when it is incorrect. To that end, we
propose grammatical error detection as a diagnostic probe to evaluate
token-level contextual representations for their knowledge of SVA. We evaluate
contextual representations at each layer from five pre-trained English language
models: BERT, XLNet, GPT-2, RoBERTa, and ELECTRA. We leverage public annotated
training data from both English second language learners and Wikipedia edits,
and report results on manually crafted stimuli for subject-verb agreement. We
find that masked language models linearly encode information relevant to the
detection of SVA errors, while the autoregressive models perform on par with
our baseline. However, we also observe a divergence in performance when probes
are trained on different training sets, and when they are evaluated on
different syntactic constructions, suggesting the information pertaining to SVA
error detection is not robustly encoded.
</p></li>
</ul>

<h3>Title: Investigating Ensemble Methods for Model Robustness Improvement of Text Classifiers. (arXiv:2210.16298v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16298">http://arxiv.org/abs/2210.16298</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16298] Investigating Ensemble Methods for Model Robustness Improvement of Text Classifiers](http://arxiv.org/abs/2210.16298)</code></li>
<li>Summary: <p>Large pre-trained language models have shown remarkable performance over the
past few years. These models, however, sometimes learn superficial features
from the dataset and cannot generalize to the distributions that are dissimilar
to the training scenario. There have been several approaches proposed to reduce
model's reliance on these bias features which can improve model robustness in
the out-of-distribution setting. However, existing methods usually use a fixed
low-capacity model to deal with various bias features, which ignore the
learnability of those features. In this paper, we analyze a set of existing
bias features and demonstrate there is no single model that works best for all
the cases. We further show that by choosing an appropriate bias model, we can
obtain a better robustness result than baselines with a more sophisticated
model design.
</p></li>
</ul>

<h3>Title: DICTION: DynamIC robusT whIte bOx watermarkiNg scheme. (arXiv:2210.15745v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15745">http://arxiv.org/abs/2210.15745</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15745] DICTION: DynamIC robusT whIte bOx watermarkiNg scheme](http://arxiv.org/abs/2210.15745)</code></li>
<li>Summary: <p>Deep neural network (DNN) watermarking is a suitable method for protecting
the ownership of deep learning (DL) models derived from computationally
intensive processes and painstakingly compiled and annotated datasets. It
secretly embeds an identifier (watermark) within the model, which can be
retrieved by the owner to prove ownership. In this paper, we first provide a
unified framework for white box DNN watermarking schemes. It includes current
state-of-the art methods outlining their theoretical inter-connections. In
second, we introduce DICTION, a new white-box Dynamic Robust watermarking
scheme, we derived from this framework. Its main originality stands on a
generative adversarial network (GAN) strategy where the watermark extraction
function is a DNN trained as a GAN discriminator, and the target model to
watermark as a GAN generator taking a GAN latent space as trigger set input.
DICTION can be seen as a generalization of DeepSigns which, to the best of
knowledge, is the only other Dynamic white-box watermarking scheme from the
literature. Experiments conducted on the same model test set as Deepsigns
demonstrate that our scheme achieves much better performance. Especially, and
contrarily to DeepSigns, with DICTION one can increase the watermark capacity
while preserving at best the model accuracy and ensuring simultaneously a
strong robustness against a wide range of watermark removal and detection
attacks.
</p></li>
</ul>

<h3>Title: Noise Injection Node Regularization for Robust Learning. (arXiv:2210.15764v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15764">http://arxiv.org/abs/2210.15764</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15764] Noise Injection Node Regularization for Robust Learning](http://arxiv.org/abs/2210.15764)</code></li>
<li>Summary: <p>We introduce Noise Injection Node Regularization (NINR), a method of
injecting structured noise into Deep Neural Networks (DNN) during the training
stage, resulting in an emergent regularizing effect. We present theoretical and
empirical evidence for substantial improvement in robustness against various
test data perturbations for feed-forward DNNs when trained under NINR. The
novelty in our approach comes from the interplay of adaptive noise injection
and initialization conditions such that noise is the dominant driver of
dynamics at the start of training. As it simply requires the addition of
external nodes without altering the existing network structure or optimization
algorithms, this method can be easily incorporated into many standard problem
specifications. We find improved stability against a number of data
perturbations, including domain shifts, with the most dramatic improvement
obtained for unstructured noise, where our technique outperforms other existing
methods such as Dropout or $L_2$ regularization, in some cases. We further show
that desirable generalization properties on clean data are generally
maintained.
</p></li>
</ul>

<h3>Title: Toward Reliable Neural Specifications. (arXiv:2210.16114v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16114">http://arxiv.org/abs/2210.16114</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16114] Toward Reliable Neural Specifications](http://arxiv.org/abs/2210.16114)</code></li>
<li>Summary: <p>Having reliable specifications is an unavoidable challenge in achieving
verifiable correctness, robustness, and interpretability of AI systems.
Existing specifications for neural networks are in the paradigm of data as
specification. That is, the local neighborhood centering around a reference
input is considered to be correct (or robust). However, our empirical study
shows that such a specification is extremely overfitted since usually no data
points from the testing set lie in the certified region of the reference input,
making them impractical for real-world applications. We propose a new family of
specifications called neural representation as specification, which uses the
intrinsic information of neural networks - neural activation patterns (NAP),
rather than input data to specify the correctness and/or robustness of neural
network predictions. We present a simple statistical approach to mining
dominant neural activation patterns. We analyze NAPs from a statistical point
of view and find that a single NAP can cover a large number of training and
testing data points whereas ad hoc data-as-specification only covers the given
reference data point. To show the effectiveness of discovered NAPs, we formally
verify several important properties, such as various types of
misclassifications will never happen for a given NAP, and there is no-ambiguity
between different NAPs. We show that by using NAP, we can verify the prediction
of the entire input space, while still recalling 84% of the data. Thus, we
argue that using NAPs is a more reliable and extensible specification for
neural network verification.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Exploring Spatial-Temporal Features for Deepfake Detection and Localization. (arXiv:2210.15872v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15872">http://arxiv.org/abs/2210.15872</a></li>
<li>Code URL: <a href="https://github.com/highwaywu/st-ddl">https://github.com/highwaywu/st-ddl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15872] Exploring Spatial-Temporal Features for Deepfake Detection and Localization](http://arxiv.org/abs/2210.15872)</code></li>
<li>Summary: <p>With the continuous research on Deepfake forensics, recent studies have
attempted to provide the fine-grained localization of forgeries, in addition to
the coarse classification at the video-level. However, the detection and
localization performance of existing Deepfake forensic methods still have
plenty of room for further improvement. In this work, we propose a
Spatial-Temporal Deepfake Detection and Localization (ST-DDL) network that
simultaneously explores spatial and temporal features for detecting and
localizing forged regions. Specifically, we design a new Anchor-Mesh Motion
(AMM) algorithm to extract temporal (motion) features by modeling the precise
geometric movements of the facial micro-expression. Compared with traditional
motion extraction methods (e.g., optical flow) designed to simulate
large-moving objects, our proposed AMM could better capture the
small-displacement facial features. The temporal features and the spatial
features are then fused in a Fusion Attention (FA) module based on a
Transformer architecture for the eventual Deepfake forensic tasks. The
superiority of our ST-DDL network is verified by experimental comparisons with
several state-of-the-art competitors, in terms of both video- and pixel-level
detection and localization performance. Furthermore, to impel the future
development of Deepfake forensics, we build a public forgery dataset consisting
of 6000 videos, with many new features such as using widely-used commercial
software (e.g., After Effects) for the production, providing online social
networks transmitted versions, and splicing multi-source videos. The source
code and dataset are available at https://github.com/HighwayWu/ST-DDL.
</p></li>
</ul>

<h3>Title: Comparison of Stereo Matching Algorithms for the Development of Disparity Map. (arXiv:2210.15926v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15926">http://arxiv.org/abs/2210.15926</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15926] Comparison of Stereo Matching Algorithms for the Development of Disparity Map](http://arxiv.org/abs/2210.15926)</code></li>
<li>Summary: <p>Stereo Matching is one of the classical problems in computer vision for the
extraction of 3D information but still controversial for accuracy and
processing costs. The use of matching techniques and cost functions is crucial
in the development of the disparity map. This paper presents a comparative
study of six different stereo matching algorithms including Block Matching
(BM), Block Matching with Dynamic Programming (BMDP), Belief Propagation (BP),
Gradient Feature Matching (GF), Histogram of Oriented Gradient (HOG), and the
proposed method. Also three cost functions namely Mean Squared Error (MSE), Sum
of Absolute Differences (SAD), Normalized Cross-Correlation (NCC) were used and
compared. The stereo images used in this study were from the Middlebury Stereo
Datasets provided with perfect and imperfect calibrations. Results show that
the selection of matching function is quite important and also depends on the
images properties. Results showed that the BP algorithm in most cases provided
better results getting accuracies over 95%.
</p></li>
</ul>

<h3>Title: Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction. (arXiv:2210.15843v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15843">http://arxiv.org/abs/2210.15843</a></li>
<li>Code URL: <a href="https://github.com/hustminslab/bip">https://github.com/hustminslab/bip</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15843] Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction](http://arxiv.org/abs/2210.15843)</code></li>
<li>Summary: <p>Recently, prompt-tuning has attracted growing interests in event argument
extraction (EAE). However, the existing prompt-tuning methods have not achieved
satisfactory performance due to the lack of consideration of entity
information. In this paper, we propose a bi-directional iterative prompt-tuning
method for EAE, where the EAE task is treated as a cloze-style task to take
full advantage of entity information and pre-trained language models (PLMs).
Furthermore, our method explores event argument interactions by introducing the
argument roles of contextual entities into prompt construction. Since template
and verbalizer are two crucial components in a cloze-style prompt, we propose
to utilize the role label semantic knowledge to construct a semantic verbalizer
and design three kinds of templates for the EAE task. Experiments on the ACE
2005 English dataset with standard and low-resource settings show that the
proposed method significantly outperforms the peer state-of-the-art methods.
Our code is available at https://github.com/HustMinsLab/BIP.
</p></li>
</ul>

<h3>Title: DORE: Document Ordered Relation Extraction based on Generative Framework. (arXiv:2210.16064v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16064">http://arxiv.org/abs/2210.16064</a></li>
<li>Code URL: <a href="https://github.com/ayyyq/dore">https://github.com/ayyyq/dore</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16064] DORE: Document Ordered Relation Extraction based on Generative Framework](http://arxiv.org/abs/2210.16064)</code></li>
<li>Summary: <p>In recent years, there is a surge of generation-based information extraction
work, which allows a more direct use of pre-trained language models and
efficiently captures output dependencies. However, previous generative methods
using lexical representation do not naturally fit document-level relation
extraction (DocRE) where there are multiple entities and relational facts. In
this paper, we investigate the root cause of the underwhelming performance of
the existing generative DocRE models and discover that the culprit is the
inadequacy of the training paradigm, instead of the capacities of the models.
We propose to generate a symbolic and ordered sequence from the relation matrix
which is deterministic and easier for model to learn. Moreover, we design a
parallel row generation method to process overlong target sequences. Besides,
we introduce several negative sampling strategies to improve the performance
with balanced signals. Experimental results on four datasets show that our
proposed method can improve the performance of the generative DocRE models. We
have released our code at https://github.com/ayyyq/DORE.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedVMR: A New Federated Learning method for Video Moment Retrieval. (arXiv:2210.15977v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15977">http://arxiv.org/abs/2210.15977</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15977] FedVMR: A New Federated Learning method for Video Moment Retrieval](http://arxiv.org/abs/2210.15977)</code></li>
<li>Summary: <p>Despite the great success achieved, existing video moment retrieval (VMR)
methods are developed under the assumption that data are centralizedly stored.
However, in real-world applications, due to the inherent nature of data
generation and privacy concerns, data are often distributed on different silos,
bringing huge challenges to effective large-scale training. In this work, we
try to overcome above limitation by leveraging the recent success of federated
learning. As the first that is explored in VMR field, the new task is defined
as video moment retrieval with distributed data. Then, a novel federated
learning method named FedVMR is proposed to facilitate large-scale and secure
training of VMR models in decentralized environment. Experiments on benchmark
datasets demonstrate its effectiveness. This work is the very first attempt to
enable safe and efficient VMR training in decentralized scene, which is hoped
to pave the way for further study in the related research field.
</p></li>
</ul>

<h3>Title: Federated Learning for Chronic Obstructive Pulmonary Disease Classification with Partial Personalized Attention Mechanism. (arXiv:2210.16142v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16142">http://arxiv.org/abs/2210.16142</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16142] Federated Learning for Chronic Obstructive Pulmonary Disease Classification with Partial Personalized Attention Mechanism](http://arxiv.org/abs/2210.16142)</code></li>
<li>Summary: <p>Chronic Obstructive Pulmonary Disease (COPD) is the fourth leading cause of
death worldwide. Yet, COPD diagnosis heavily relies on spirometric examination
as well as functional airway limitation, which may cause a considerable portion
of COPD patients underdiagnosed especially at the early stage. Recent advance
in deep learning (DL) has shown their promising potential in COPD
identification from CT images. However, with heterogeneous syndromes and
distinct phenotypes, DL models trained with CTs from one data center fail to
generalize on images from another center. Due to privacy regularizations, a
collaboration of distributed CT images into one centralized center is not
feasible. Federated learning (FL) approaches enable us to train with
distributed private data. Yet, routine FL solutions suffer from performance
degradation in the case where COPD CTs are not independent and identically
distributed (Non-IID). To address this issue, we propose a novel personalized
federated learning (PFL) method based on vision transformer (ViT) for
distributed and heterogeneous COPD CTs. To be more specific, we partially
personalize some heads in multiheaded self-attention layers to learn the
personalized attention for local data and retain the other heads shared to
extract the common attention. To the best of our knowledge, this is the first
proposal of a PFL framework specifically for ViT to identify COPD. Our
evaluation of a dataset set curated from six medical centers shows our method
outperforms the PFL approaches for convolutional neural networks.
</p></li>
</ul>

<h3>Title: Completely Heterogeneous Federated Learning. (arXiv:2210.15865v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15865">http://arxiv.org/abs/2210.15865</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15865] Completely Heterogeneous Federated Learning](http://arxiv.org/abs/2210.15865)</code></li>
<li>Summary: <p>Federated learning (FL) faces three major difficulties: cross-domain,
heterogeneous models, and non-i.i.d. labels scenarios. Existing FL methods fail
to handle the above three constraints at the same time, and the level of
privacy protection needs to be lowered (e.g., the model architecture and data
category distribution can be shared). In this work, we propose the challenging
"completely heterogeneous" scenario in FL, which refers to that each client
will not expose any private information including feature space, model
architecture, and label distribution. We then devise an FL framework based on
parameter decoupling and data-free knowledge distillation to solve the problem.
Experiments show that our proposed method achieves high performance in
completely heterogeneous scenarios where other approaches fail.
</p></li>
</ul>

<h3>Title: Prototype-Based Layered Federated Cross-Modal Hashing. (arXiv:2210.15678v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15678">http://arxiv.org/abs/2210.15678</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15678] Prototype-Based Layered Federated Cross-Modal Hashing](http://arxiv.org/abs/2210.15678)</code></li>
<li>Summary: <p>Recently, deep cross-modal hashing has gained increasing attention. However,
in many practical cases, data are distributed and cannot be collected due to
privacy concerns, which greatly reduces the cross-modal hashing performance on
each client. And due to the problems of statistical heterogeneity, model
heterogeneity, and forcing each client to accept the same parameters, applying
federated learning to cross-modal hash learning becomes very tricky. In this
paper, we propose a novel method called prototype-based layered federated
cross-modal hashing. Specifically, the prototype is introduced to learn the
similarity between instances and classes on server, reducing the impact of
statistical heterogeneity (non-IID) on different clients. And we monitor the
distance between local and global prototypes to further improve the
performance. To realize personalized federated learning, a hypernetwork is
deployed on server to dynamically update different layers' weights of local
model. Experimental results on benchmark datasets show that our method
outperforms state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Federated Learning with Intermediate Representation Regularization. (arXiv:2210.15827v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15827">http://arxiv.org/abs/2210.15827</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15827] Federated Learning with Intermediate Representation Regularization](http://arxiv.org/abs/2210.15827)</code></li>
<li>Summary: <p>In contrast to centralized model training that involves data collection,
federated learning (FL) enables remote clients to collaboratively train a model
without exposing their private data. However, model performance usually
degrades in FL due to the heterogeneous data generated by clients of diverse
characteristics. One promising strategy to maintain good performance is by
limiting the local training from drifting far away from the global model.
Previous studies accomplish this by regularizing the distance between the
representations learned by the local and global models. However, they only
consider representations from the early layers of a model or the layer
preceding the output layer. In this study, we introduce FedIntR, which provides
a more fine-grained regularization by integrating the representations of
intermediate layers into the local training process. Specifically, FedIntR
computes a regularization term that encourages the closeness between the
intermediate layer representations of the local and global models.
Additionally, FedIntR automatically determines the contribution of each layer's
representation to the regularization term based on the similarity between local
and global representations. We conduct extensive experiments on various
datasets to show that FedIntR can achieve equivalent or higher performance
compared to the state-of-the-art approaches.
</p></li>
</ul>

<h3>Title: Efficient and Light-Weight Federated Learning via Asynchronous Distributed Dropout. (arXiv:2210.16105v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16105">http://arxiv.org/abs/2210.16105</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16105] Efficient and Light-Weight Federated Learning via Asynchronous Distributed Dropout](http://arxiv.org/abs/2210.16105)</code></li>
<li>Summary: <p>Asynchronous learning protocols have regained attention lately, especially in
the Federated Learning (FL) setup, where slower clients can severely impede the
learning process. Herein, we propose \texttt{AsyncDrop}, a novel asynchronous
FL framework that utilizes dropout regularization to handle device
heterogeneity in distributed settings. Overall, \texttt{AsyncDrop} achieves
better performance compared to state of the art asynchronous methodologies,
while resulting in less communication and training time overheads. The key idea
revolves around creating ``submodels'' out of the global model, and
distributing their training to workers, based on device heterogeneity. We
rigorously justify that such an approach can be theoretically characterized. We
implement our approach and compare it against other asynchronous baselines,
both by design and by adapting existing synchronous FL algorithms to
asynchronous scenarios. Empirically, \texttt{AsyncDrop} reduces the
communication cost and training time, while matching or improving the final
test accuracy in diverse non-i.i.d. FL scenarios.
</p></li>
</ul>

<h3>Title: Imitation Learning-based Implicit Semantic-aware Communication Networks: Multi-layer Representation and Collaborative Reasoning. (arXiv:2210.16118v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16118">http://arxiv.org/abs/2210.16118</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16118] Imitation Learning-based Implicit Semantic-aware Communication Networks: Multi-layer Representation and Collaborative Reasoning](http://arxiv.org/abs/2210.16118)</code></li>
<li>Summary: <p>Semantic communication has recently attracted significant interest from both
industry and academia due to its potential to transform the existing
data-focused communication architecture towards a more generally intelligent
and goal-oriented semantic-aware networking system. Despite its promising
potential, semantic communications and semantic-aware networking are still at
their infancy. Most existing works focus on transporting and delivering the
explicit semantic information, e.g., labels or features of objects, that can be
directly identified from the source signal. The original definition of
semantics as well as recent results in cognitive neuroscience suggest that it
is the implicit semantic information, in particular the hidden relations
connecting different concepts and feature items that plays the fundamental role
in recognizing, communicating, and delivering the real semantic meanings of
messages. Motivated by this observation, we propose a novel reasoning-based
implicit semantic-aware communication network architecture that allows multiple
tiers of CDC and edge servers to collaborate and support efficient semantic
encoding, decoding, and interpretation for end-users. We introduce a new
multi-layer representation of semantic information taking into consideration
both the hierarchical structure of implicit semantics as well as the
personalized inference preference of individual users. We model the semantic
reasoning process as a reinforcement learning process and then propose an
imitation-based semantic reasoning mechanism learning (iRML) solution for the
edge servers to leaning a reasoning policy that imitates the inference behavior
of the source user. A federated GCN-based collaborative reasoning solution is
proposed to allow multiple edge servers to jointly construct a shared semantic
interpretation model based on decentralized knowledge datasets.
</p></li>
</ul>

<h3>Title: M3FGM:a node masking and multi-granularity message passing-based federated graph model for spatial-temporal data prediction. (arXiv:2210.16193v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16193">http://arxiv.org/abs/2210.16193</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16193] M3FGM:a node masking and multi-granularity message passing-based federated graph model for spatial-temporal data prediction](http://arxiv.org/abs/2210.16193)</code></li>
<li>Summary: <p>Researchers are solving the challenges of spatial-temporal prediction by
combining Federated Learning (FL) and graph models with respect to the
constrain of privacy and security. However, there are still several issues left
unattended: 1) Clients might not be able to access the server during inference
phase; 2) The graph of clients designed manually in the server model may not
reveal the proper relationship between clients. This paper proposes a new
embeddings aggregation structured FL approach named node Masking and
Multi-granularity Message passing-based Federated Graph Model (M3FGM) for the
above issues. The server model of M3FGM employs a MaskNode layer to simulate
the case of offline clients. We also redesign the decoder of the client model
using a dual-sub-decoders structure so that each client model can use its local
data to predict independently when offline. As for the second issue, A new GNN
layer named Multi-Granularity Message Passing (MGMP) allows each client node to
perceive global and local information.We conducted extensive experiments in two
different scenarios on two real traffic datasets. Results show that the
proposed model outperforms the baselines and variant models, achieves the best
results in both scenarios.
</p></li>
</ul>

<h3>Title: Federated Learning based Energy Demand Prediction with Clustered Aggregation. (arXiv:2210.15850v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15850">http://arxiv.org/abs/2210.15850</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15850] Federated Learning based Energy Demand Prediction with Clustered Aggregation](http://arxiv.org/abs/2210.15850)</code></li>
<li>Summary: <p>To reduce negative environmental impacts, power stations and energy grids
need to optimize the resources required for power production. Thus, predicting
the energy consumption of clients is becoming an important part of every energy
management system. Energy usage information collected by the clients' smart
homes can be used to train a deep neural network to predict the future energy
demand. Collecting data from a large number of distributed clients for
centralized model training is expensive in terms of communication resources. To
take advantage of distributed data in edge systems, centralized training can be
replaced by federated learning where each client only needs to upload model
updates produced by training on its local data. These model updates are
aggregated into a single global model by the server. But since different
clients can have different attributes, model updates can have diverse weights
and as a result, it can take a long time for the aggregated global model to
converge. To speed up the convergence process, we can apply clustering to group
clients based on their properties and aggregate model updates from the same
cluster together to produce a cluster specific global model. In this paper, we
propose a recurrent neural network based energy demand predictor, trained with
federated learning on clustered clients to take advantage of distributed data
and speed up the convergence process.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Fairness Certificates for Differentially Private Classification. (arXiv:2210.16242v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.16242">http://arxiv.org/abs/2210.16242</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.16242] Fairness Certificates for Differentially Private Classification](http://arxiv.org/abs/2210.16242)</code></li>
<li>Summary: <p>In this work, we theoretically study the impact of differential privacy on
fairness in binary classification. We prove that, given a class of models,
popular group fairness measures are pointwise Lipschitz-continuous with respect
to the parameters of the model. This result is a consequence of a more general
statement on the probability that a decision function makes a negative
prediction conditioned on an arbitrary event (such as membership to a sensitive
group), which may be of independent interest. We use the aforementioned
Lipschitz property to prove a high probability bound showing that, given enough
examples, the fairness level of private models is close to the one of their
non-private counterparts.
</p></li>
</ul>

<h3>Title: Mitigating Health Disparities in EHR via Deconfounder. (arXiv:2210.15901v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.15901">http://arxiv.org/abs/2210.15901</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.15901] Mitigating Health Disparities in EHR via Deconfounder](http://arxiv.org/abs/2210.15901)</code></li>
<li>Summary: <p>Health disparities, or inequalities between different patient demographics,
are becoming crucial in medical decision-making, especially in Electronic
Health Record (EHR) predictive modeling. To ensure the fairness of sensitive
attributes, conventional studies mainly adopt calibration or re-weighting
methods to balance the performance on among different demographic groups.
However, we argue that these methods have some limitations. First, these
methods usually mean a trade-off between the model's performance and fairness.
Second, many methods completely attribute unfairness to the data collection
process, which lacks substantial evidence. In this paper, we provide an
empirical study to discover the possibility of using deconfounder to address
the disparity issue in healthcare. Our study can be summarized in two parts.
The first part is a pilot study demonstrating the exacerbation of disparity
when unobserved confounders exist. The second part proposed a novel framework,
Parity Medical Deconfounder (PriMeD), to deal with the disparity issue in
healthcare datasets. Inspired by the deconfounder theory, PriMeD adopts a
Conditional Variational Autoencoder (CVAE) to learn latent factors (substitute
confounders) for observational data, and extensive experiments are provided to
show its effectiveness.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
