<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Recent Advances in the Internet of Medical Things (IoMT) Systems Security. (arXiv:2302.04439v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04439">http://arxiv.org/abs/2302.04439</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04439] Recent Advances in the Internet of Medical Things (IoMT) Systems Security](http://arxiv.org/abs/2302.04439) #security</code></li>
<li>Summary: <p>The rapid evolutions in micro-computing, mini-hardware manufacturing, and
machine to machine (M2M) communications have enabled novel Internet of Things
(IoT) solutions to reshape many networking applications. Healthcare systems are
among these applications that have been revolutionized with IoT, introducing an
IoT branch known as the Internet of Medical Things (IoMT) systems. IoMT systems
allow remote monitoring of patients with chronic diseases. Thus, it can provide
timely patients' diagnostic that can save their life in case of emergencies.
However, security in these critical systems is a major challenge facing their
wide utilization. In this paper, we present state-of-the-art techniques to
secure IoMT systems' data during collection, transmission, and storage. We
comprehensively overview IoMT systems' potential attacks, including physical
and network attacks. Our findings reveal that most security techniques do not
consider various types of attacks. Hence, we propose a security framework that
combines several security techniques. The framework covers IoMT security
requirements and can mitigate most of its known attacks.
</p></li>
</ul>

<h3>Title: Imperceptible Sample-Specific Backdoor to DNN with Denoising Autoencoder. (arXiv:2302.04457v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04457">http://arxiv.org/abs/2302.04457</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04457] Imperceptible Sample-Specific Backdoor to DNN with Denoising Autoencoder](http://arxiv.org/abs/2302.04457) #security</code></li>
<li>Summary: <p>The backdoor attack poses a new security threat to deep neural networks.
Existing backdoor often relies on visible universal trigger to make the
backdoored model malfunction, which are not only usually visually suspicious to
human but also catchable by mainstream countermeasures. We propose an
imperceptible sample-specific backdoor that the trigger varies from sample to
sample and invisible. Our trigger generation is automated through a desnoising
autoencoder that is fed with delicate but pervasive features (i.e., edge
patterns per images). We extensively experiment our backdoor attack on ImageNet
and MS-Celeb-1M, which demonstrates stable and nearly 100% (i.e., 99.8%) attack
success rate with negligible impact on the clean data accuracy of the infected
model. The denoising autoeconder based trigger generator is reusable or
transferable across tasks (e.g., from ImageNet to MS-Celeb-1M), whilst the
trigger has high exclusiveness (i.e., a trigger generated for one sample is not
applicable to another sample). Besides, our proposed backdoored model has
achieved high evasiveness against mainstream backdoor defenses such as Neural
Cleanse, STRIP, SentiNet and Fine-Pruning.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Offsite-Tuning: Transfer Learning without Full Model. (arXiv:2302.04870v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04870">http://arxiv.org/abs/2302.04870</a></li>
<li>Code URL: <a href="https://github.com/mit-han-lab/offsite-tuning">https://github.com/mit-han-lab/offsite-tuning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04870] Offsite-Tuning: Transfer Learning without Full Model](http://arxiv.org/abs/2302.04870) #privacy</code></li>
<li>Summary: <p>Transfer learning is important for foundation models to adapt to downstream
tasks. However, many foundation models are proprietary, so users must share
their data with model owners to fine-tune the models, which is costly and raise
privacy concerns. Moreover, fine-tuning large foundation models is
computation-intensive and impractical for most downstream users. In this paper,
we propose Offsite-Tuning, a privacy-preserving and efficient transfer learning
framework that can adapt billion-parameter foundation models to downstream data
without access to the full model. In offsite-tuning, the model owner sends a
light-weight adapter and a lossy compressed emulator to the data owner, who
then fine-tunes the adapter on the downstream data with the emulator's
assistance. The fine-tuned adapter is then returned to the model owner, who
plugs it into the full model to create an adapted foundation model.
Offsite-tuning preserves both parties' privacy and is computationally more
efficient than the existing fine-tuning methods that require access to the full
model weights. We demonstrate the effectiveness of offsite-tuning on various
large language and vision foundation models. Offsite-tuning can achieve
comparable accuracy as full model fine-tuning while being privacy-preserving
and efficient, achieving 6.5x speedup and 5.6x memory reduction. Code is
available at https://github.com/mit-han-lab/offsite-tuning.
</p></li>
</ul>

<h3>Title: Measuring the Privacy Leakage via Graph Reconstruction Attacks on Simplicial Neural Networks (Student Abstract). (arXiv:2302.04373v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04373">http://arxiv.org/abs/2302.04373</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04373] Measuring the Privacy Leakage via Graph Reconstruction Attacks on Simplicial Neural Networks (Student Abstract)](http://arxiv.org/abs/2302.04373) #privacy</code></li>
<li>Summary: <p>In this paper, we measure the privacy leakage via studying whether graph
representations can be inverted to recover the graph used to generate them via
graph reconstruction attack (GRA). We propose a GRA that recovers a graph's
adjacency matrix from the representations via a graph decoder that minimizes
the reconstruction loss between the partial graph and the reconstructed graph.
We study three types of representations that are trained on the graph, i.e.,
representations output from graph convolutional network (GCN), graph attention
network (GAT), and our proposed simplicial neural network (SNN) via a
higher-order combinatorial Laplacian. Unlike the first two types of
representations that only encode pairwise relationships, the third type of
representation, i.e., SNN outputs, encodes higher-order interactions (e.g.,
homological features) between nodes. We find that the SNN outputs reveal the
lowest privacy-preserving ability to defend the GRA, followed by those of GATs
and GCNs, which indicates the importance of building more private
representations with higher-order node information that could defend the
potential threats, such as GRAs.
</p></li>
</ul>

<h3>Title: Privacy-Preserving Representation Learning for Text-Attributed Networks with Simplicial Complexes. (arXiv:2302.04383v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04383">http://arxiv.org/abs/2302.04383</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04383] Privacy-Preserving Representation Learning for Text-Attributed Networks with Simplicial Complexes](http://arxiv.org/abs/2302.04383) #privacy</code></li>
<li>Summary: <p>Although recent network representation learning (NRL) works in
text-attributed networks demonstrated superior performance for various graph
inference tasks, learning network representations could always raise privacy
concerns when nodes represent people or human-related variables. Moreover,
standard NRLs that leverage structural information from a graph proceed by
first encoding pairwise relationships into learned representations and then
analysing its properties. This approach is fundamentally misaligned with
problems where the relationships involve multiple points, and topological
structure must be encoded beyond pairwise interactions. Fortunately, the
machinery of topological data analysis (TDA) and, in particular, simplicial
neural networks (SNNs) offer a mathematically rigorous framework to learn
higher-order interactions between nodes. It is critical to investigate if the
representation outputs from SNNs are more vulnerable compared to regular
representation outputs from graph neural networks (GNNs) via pairwise
interactions. In my dissertation, I will first study learning the
representations with text attributes for simplicial complexes (RT4SC) via SNNs.
Then, I will conduct research on two potential attacks on the representation
outputs from SNNs: (1) membership inference attack, which infers whether a
certain node of a graph is inside the training data of the GNN model; and (2)
graph reconstruction attacks, which infer the confidential edges of a
text-attributed network. Finally, I will study a privacy-preserving
deterministic differentially private alternating direction method of multiplier
to learn secure representation outputs from SNNs that capture multi-scale
relationships and facilitate the passage from local structure to global
invariant features on text-attributed networks.
</p></li>
</ul>

<h3>Title: Practical Privacy Preservation in a Mobile Cloud Environment. (arXiv:2302.04463v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04463">http://arxiv.org/abs/2302.04463</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04463] Practical Privacy Preservation in a Mobile Cloud Environment](http://arxiv.org/abs/2302.04463) #privacy</code></li>
<li>Summary: <p>The proliferation of smartphone devices has led to the emergence of powerful
user services from enabling interactions with friends and business associates
to mapping, finding nearby businesses and alerting users in real-time.
Moreover, users do not realize that continuously sharing their trajectory data
with online systems may end up revealing a great amount of information in terms
of their behavior, mobility patterns and social relationships. Thus, addressing
these privacy risks is a fundamental challenge. In this work, we present
$TP^3$, a Privacy Protection system for Trajectory analytics. Our contributions
are the following: (1) we model a new type of attack, namely 'social link
exploitation attack', (2) we utilize the coresets theory, a fast and accurate
technique which approximates well the original data using a small data set, and
running queries on the coreset produces similar results to the original data,
and (3) we employ the Serverless computing paradigm to accommodate a set of
privacy operations for achieving high system performance with minimized
provisioning costs, while preserving the users' privacy. We have developed
these techniques in our $TP^3$ system that works with state-of-the-art
trajectory analytics apps and applies different types of privacy operations.
Our detailed experimental evaluation illustrates that our approach is both
efficient and practical.
</p></li>
</ul>

<h3>Title: Distributed Learning with Curious and Adversarial Machines. (arXiv:2302.04787v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04787">http://arxiv.org/abs/2302.04787</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04787] Distributed Learning with Curious and Adversarial Machines](http://arxiv.org/abs/2302.04787) #privacy</code></li>
<li>Summary: <p>The ubiquity of distributed machine learning (ML) in sensitive public domain
applications calls for algorithms that protect data privacy, while being robust
to faults and adversarial behaviors. Although privacy and robustness have been
extensively studied independently in distributed ML, their synthesis remains
poorly understood. We present the first tight analysis of the error incurred by
any algorithm ensuring robustness against a fraction of adversarial machines,
as well as differential privacy (DP) for honest machines' data against any
other curious entity. Our analysis exhibits a fundamental trade-off between
privacy, robustness, and utility. Surprisingly, we show that the cost of this
trade-off is marginal compared to that of the classical privacy-utility
trade-off. To prove our lower bound, we consider the case of mean estimation,
subject to distributed DP and robustness constraints, and devise reductions to
centralized estimation of one-way marginals. We prove our matching upper bound
by presenting a new distributed ML algorithm using a high-dimensional robust
aggregation rule. The latter amortizes the dependence on the dimension in the
error (caused by adversarial workers and DP), while being agnostic to the
statistical properties of the data.
</p></li>
</ul>

<h3>Title: Pushing the Boundaries of Private, Large-Scale Query Answering. (arXiv:2302.04833v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04833">http://arxiv.org/abs/2302.04833</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04833] Pushing the Boundaries of Private, Large-Scale Query Answering](http://arxiv.org/abs/2302.04833) #privacy</code></li>
<li>Summary: <p>We address the problem of efficiently and effectively answering large numbers
of queries on a sensitive dataset while ensuring differential privacy (DP). We
separately analyze this problem in two distinct settings, grounding our work in
a state-of-the-art DP mechanism for large-scale query answering: the Relaxed
Adaptive Projection (RAP) mechanism.
</p></li>
</ul>

<p>The first setting is a classic setting in DP literature where all queries are
known to the mechanism in advance. Within this setting, we identify challenges
in the RAP mechanism's original analysis, then overcome them with an enhanced
implementation and analysis. We then extend the capabilities of the RAP
mechanism to be able to answer a more general and powerful class of queries
(r-of-k thresholds) than previously considered. Empirically evaluating this
class, we find that the mechanism is able to answer orders of magnitude larger
sets of queries than prior works, and does so quickly and with high utility.
</p>
<p>We then define a second setting motivated by real-world considerations and
whose definition is inspired by work in the field of machine learning. In this
new setting, a mechanism is only given partial knowledge of queries that will
be posed in the future, and it is expected to answer these future-posed queries
with high utility. We formally define this setting and how to measure a
mechanism's utility within it. We then comprehensively empirically evaluate the
RAP mechanism's utility within this new setting. From this evaluation, we find
that even with weak partial knowledge of the future queries that will be posed,
the mechanism is able to efficiently and effectively answer arbitrary queries
posed in the future. Taken together, the results from these two settings
advance the state of the art on differentially private large-scale query
answering.
</p>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Mathematical Modeling of Cyber Resilience. (arXiv:2302.04413v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04413">http://arxiv.org/abs/2302.04413</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04413] Mathematical Modeling of Cyber Resilience](http://arxiv.org/abs/2302.04413) #defense</code></li>
<li>Summary: <p>We identify quantitative characteristics of responses to cyber compromises
that can be learned from repeatable, systematic experiments. We model a vehicle
equipped with an autonomous cyber-defense system and which also has some
inherent physical resilience features. When attacked by malware, this ensemble
of cyber-physical features (i.e., "bonware") strives to resist and recover from
the performance degradation caused by the malware's attack. We propose
parsimonious continuous models, and develop stochastic models to aid in
quantifying systems' resilience to cyber attacks.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Exploiting Certified Defences to Attack Randomised Smoothing. (arXiv:2302.04379v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04379">http://arxiv.org/abs/2302.04379</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04379] Exploiting Certified Defences to Attack Randomised Smoothing](http://arxiv.org/abs/2302.04379) #attack</code></li>
<li>Summary: <p>In guaranteeing that no adversarial examples exist within a bounded region,
certification mechanisms play an important role in neural network robustness.
Concerningly, this work demonstrates that the certification mechanisms
themselves introduce a new, heretofore undiscovered attack surface, that can be
exploited by attackers to construct smaller adversarial perturbations. While
these attacks exist outside the certification region in no way invalidate
certifications, minimising a perturbation's norm significantly increases the
level of difficulty associated with attack detection. In comparison to baseline
attacks, our new framework yields smaller perturbations more than twice as
frequently as any other approach, resulting in an up to $34 \%$ reduction in
the median perturbation norm. That this approach also requires $90 \%$ less
computational time than approaches like PGD. That these reductions are possible
suggests that exploiting this new attack vector would allow attackers to more
frequently construct hard to detect adversarial attacks, by exploiting the very
systems designed to defend deployed models.
</p></li>
</ul>

<h3>Title: Forensic Log Based Detection For Keystroke Injection "BadUsb" Attacks. (arXiv:2302.04541v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04541">http://arxiv.org/abs/2302.04541</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04541] Forensic Log Based Detection For Keystroke Injection "BadUsb" Attacks](http://arxiv.org/abs/2302.04541) #attack</code></li>
<li>Summary: <p>This document describes an experiment with main purpose to detect BadUSB
attacks that utilize external Human Interaction Device hardware gadgets to
inject keystrokes and acquire remote code execution. One of the main goals, is
to detect such activity based on behavioral factors and allow everyone with a
basic set of cognitive capabilities ,regardless of the user being a human or a
computer, to identify anomalous speed related indicators but also correlate
such speed changes with other elements such as commonly malicious processes
like powershell processes being called in close proximity timing-wise, PnP
device events occurring correlated with driver images loaded.
</p></li>
</ul>

<h3>Title: SoK: A Data-driven View on Methods to Detect Reflective Amplification DDoS Attacks Using Honeypots. (arXiv:2302.04614v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04614">http://arxiv.org/abs/2302.04614</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04614] SoK: A Data-driven View on Methods to Detect Reflective Amplification DDoS Attacks Using Honeypots](http://arxiv.org/abs/2302.04614) #attack</code></li>
<li>Summary: <p>In this paper, we revisit the use of honeypots for detecting reflective
amplification attacks. These measurement tools require careful design of both
data collection and data analysis including cautious threshold inference. We
survey common amplification honeypot platforms as well as the underlying
methods to infer attack detection thresholds and to extract knowledge from the
data. By systematically exploring the threshold space, we find most honeypot
platforms produce comparable results despite their different configurations.
Moreover, by applying data from a large-scale honeypot deployment, network
telescopes, and a real-world baseline obtained from a leading DDoS mitigation
provider, we question the fundamental assumption of honeypot research that
convergence of observations can imply their completeness. Conclusively we
derive guidance on precise, reproducible honeypot research, and present open
challenges.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Contour Completion using Deep Structural Priors. (arXiv:2302.04447v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04447">http://arxiv.org/abs/2302.04447</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04447] Contour Completion using Deep Structural Priors](http://arxiv.org/abs/2302.04447) #robust</code></li>
<li>Summary: <p>Humans can easily perceive illusory contours and complete missing forms in
fragmented shapes. This work investigates whether such capability can arise in
convolutional neural networks (CNNs) using deep structural priors computed
directly from images. In this work, we present a framework that completes
disconnected contours and connects fragmented lines and curves. In our
framework, we propose a model that does not even need to know which regions of
the contour are eliminated. We introduce an iterative process that completes an
incomplete image and we propose novel measures that guide this to find regions
it needs to complete. Our model trains on a single image and fills in the
contours with no additional training data. Our work builds a robust framework
to achieve contour completion using deep structural priors and extensively
investigate how such a model could be implemented.
</p></li>
</ul>

<h3>Title: Toward Extremely Lightweight Distracted Driver Recognition With Distillation-Based Neural Architecture Search and Knowledge Transfer. (arXiv:2302.04527v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04527">http://arxiv.org/abs/2302.04527</a></li>
<li>Code URL: <a href="https://github.com/dichao-liu/lightweight_distracted_driver_recognition_with_distillation-based_nas_and_knowledge_transfer">https://github.com/dichao-liu/lightweight_distracted_driver_recognition_with_distillation-based_nas_and_knowledge_transfer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04527] Toward Extremely Lightweight Distracted Driver Recognition With Distillation-Based Neural Architecture Search and Knowledge Transfer](http://arxiv.org/abs/2302.04527) #robust</code></li>
<li>Summary: <p>The number of traffic accidents has been continuously increasing in recent
years worldwide. Many accidents are caused by distracted drivers, who take
their attention away from driving. Motivated by the success of Convolutional
Neural Networks (CNNs) in computer vision, many researchers developed CNN-based
algorithms to recognize distracted driving from a dashcam and warn the driver
against unsafe behaviors. However, current models have too many parameters,
which is unfeasible for vehicle-mounted computing. This work proposes a novel
knowledge-distillation-based framework to solve this problem. The proposed
framework first constructs a high-performance teacher network by progressively
strengthening the robustness to illumination changes from shallow to deep
layers of a CNN. Then, the teacher network is used to guide the architecture
searching process of a student network through knowledge distillation. After
that, we use the teacher network again to transfer knowledge to the student
network by knowledge distillation. Experimental results on the Statefarm
Distracted Driver Detection Dataset and AUC Distracted Driver Dataset show that
the proposed approach is highly effective for recognizing distracted driving
behaviors from photos: (1) the teacher network's accuracy surpasses the
previous best accuracy; (2) the student network achieves very high accuracy
with only 0.42M parameters (around 55% of the previous most lightweight model).
Furthermore, the student network architecture can be extended to a
spatial-temporal 3D CNN for recognizing distracted driving from video clips.
The 3D student network largely surpasses the previous best accuracy with only
2.03M parameters on the Drive&amp;Act Dataset. The source code is available at
https://github.com/Dichao-Liu/Lightweight_Distracted_Driver_Recognition_with_Distillation-Based_NAS_and_Knowledge_Transfer.
</p></li>
</ul>

<h3>Title: MAPS: A Noise-Robust Progressive Learning Approach for Source-Free Domain Adaptive Keypoint Detection. (arXiv:2302.04589v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04589">http://arxiv.org/abs/2302.04589</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04589] MAPS: A Noise-Robust Progressive Learning Approach for Source-Free Domain Adaptive Keypoint Detection](http://arxiv.org/abs/2302.04589) #robust</code></li>
<li>Summary: <p>Existing cross-domain keypoint detection methods always require accessing the
source data during adaptation, which may violate the data privacy law and pose
serious security concerns. Instead, this paper considers a realistic problem
setting called source-free domain adaptive keypoint detection, where only the
well-trained source model is provided to the target domain. For the challenging
problem, we first construct a teacher-student learning baseline by stabilizing
the predictions under data augmentation and network ensembles. Built on this,
we further propose a unified approach, Mixup Augmentation and Progressive
Selection (MAPS), to fully exploit the noisy pseudo labels of unlabeled target
data during training. On the one hand, MAPS regularizes the model to favor
simple linear behavior in-between the target samples via self-mixup
augmentation, preventing the model from over-fitting to noisy predictions. On
the other hand, MAPS employs the self-paced learning paradigm and progressively
selects pseudo-labeled samples from <code>easy' to</code>hard' into the training process
to reduce noise accumulation. Results on four keypoint detection datasets show
that MAPS outperforms the baseline and achieves comparable or even better
results in comparison to previous non-source-free counterparts.
</p></li>
</ul>

<h3>Title: Weakly Supervised Human Skin Segmentation using Guidance Attention Mechanisms. (arXiv:2302.04625v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04625">http://arxiv.org/abs/2302.04625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04625] Weakly Supervised Human Skin Segmentation using Guidance Attention Mechanisms](http://arxiv.org/abs/2302.04625) #robust</code></li>
<li>Summary: <p>Human skin segmentation is a crucial task in computer vision and biometric
systems, yet it poses several challenges such as variability in skin color,
pose, and illumination. This paper presents a robust data-driven skin
segmentation method for a single image that addresses these challenges through
the integration of contextual information and efficient network design. In
addition to robustness and accuracy, the integration into real-time systems
requires a careful balance between computational power, speed, and performance.
The proposed method incorporates two attention modules, Body Attention and Skin
Attention, that utilize contextual information to improve segmentation results.
These modules draw attention to the desired areas, focusing on the body
boundaries and skin pixels, respectively. Additionally, an efficient network
architecture is employed in the encoder part to minimize computational power
while retaining high performance. To handle the issue of noisy labels in skin
datasets, the proposed method uses a weakly supervised training strategy,
relying on the Skin Attention module. The results of this study demonstrate
that the proposed method is comparable to, or outperforms, state-of-the-art
methods on benchmark datasets.
</p></li>
</ul>

<h3>Title: Real-Time Visual Feedback to Guide Benchmark Creation: A Human-and-Metric-in-the-Loop Workflow. (arXiv:2302.04434v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04434">http://arxiv.org/abs/2302.04434</a></li>
<li>Code URL: <a href="https://github.com/aarunku5/vaida-eacl-2023">https://github.com/aarunku5/vaida-eacl-2023</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04434] Real-Time Visual Feedback to Guide Benchmark Creation: A Human-and-Metric-in-the-Loop Workflow](http://arxiv.org/abs/2302.04434) #robust</code></li>
<li>Summary: <p>Recent research has shown that language models exploit `artifacts' in
benchmarks to solve tasks, rather than truly learning them, leading to inflated
model performance. In pursuit of creating better benchmarks, we propose VAIDA,
a novel benchmark creation paradigm for NLP, that focuses on guiding
crowdworkers, an under-explored facet of addressing benchmark idiosyncrasies.
VAIDA facilitates sample correction by providing realtime visual feedback and
recommendations to improve sample quality. Our approach is domain, model, task,
and metric agnostic, and constitutes a paradigm shift for robust, validated,
and dynamic benchmark creation via human-and-metric-in-the-loop workflows. We
evaluate via expert review and a user study with NASA TLX. We find that VAIDA
decreases effort, frustration, mental, and temporal demands of crowdworkers and
analysts, simultaneously increasing the performance of both user groups with a
45.8% decrease in the level of artifacts in created samples. As a by product of
our user study, we observe that created samples are adversarial across models,
leading to decreases of 31.3% (BERT), 22.5% (RoBERTa), 14.98% (GPT-3 fewshot)
in performance.
</p></li>
</ul>

<h3>Title: Data Augmentation for Robust Character Detection in Fantasy Novels. (arXiv:2302.04555v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04555">http://arxiv.org/abs/2302.04555</a></li>
<li>Code URL: <a href="https://github.com/compnet/ddaugner">https://github.com/compnet/ddaugner</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04555] Data Augmentation for Robust Character Detection in Fantasy Novels](http://arxiv.org/abs/2302.04555) #robust</code></li>
<li>Summary: <p>Named Entity Recognition (NER) is a low-level task often used as a foundation
for solving higher level NLP problems. In the context of character detection in
novels, NER false negatives can be an issue as they possibly imply missing
certain characters or relationships completely. In this article, we demonstrate
that applying a straightforward data augmentation technique allows training a
model achieving higher recall, at the cost of a certain amount of precision
regarding ambiguous entities. We show that this decrease in precision can be
mitigated by giving the model more local context, which resolves some of the
ambiguities.
</p></li>
</ul>

<h3>Title: Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study. (arXiv:2302.04618v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04618">http://arxiv.org/abs/2302.04618</a></li>
<li>Code URL: <a href="https://github.com/oceanypt/coldqa-tta">https://github.com/oceanypt/coldqa-tta</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04618] Robust Question Answering against Distribution Shifts with Test-Time Adaptation: An Empirical Study](http://arxiv.org/abs/2302.04618) #robust</code></li>
<li>Summary: <p>A deployed question answering (QA) model can easily fail when the test data
has a distribution shift compared to the training data. Robustness tuning (RT)
methods have been widely studied to enhance model robustness against
distribution shifts before model deployment. However, can we improve a model
after deployment? To answer this question, we evaluate test-time adaptation
(TTA) to improve a model after deployment. We first introduce COLDQA, a unified
evaluation benchmark for robust QA against text corruption and changes in
language and domain. We then evaluate previous TTA methods on COLDQA and
compare them to RT methods. We also propose a novel TTA method called online
imitation learning (OIL). Through extensive experiments, we find that TTA is
comparable to RT methods, and applying TTA after RT can significantly boost the
performance on COLDQA. Our proposed OIL improves TTA to be more robust to
variation in hyper-parameters and test distributions over time.
</p></li>
</ul>

<h3>Title: Continuous Learning for Android Malware Detection. (arXiv:2302.04332v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04332">http://arxiv.org/abs/2302.04332</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04332] Continuous Learning for Android Malware Detection](http://arxiv.org/abs/2302.04332) #robust</code></li>
<li>Summary: <p>Machine learning methods can detect Android malware with very high accuracy.
However, these classifiers have an Achilles heel, concept drift: they rapidly
become out of date and ineffective, due to the evolution of malware apps and
benign apps. Our research finds that, after training an Android malware
classifier on one year's worth of data, the F1 score quickly dropped from 0.99
to 0.76 after 6 months of deployment on new test samples.
</p></li>
</ul>

<p>In this paper, we propose new methods to combat the concept drift problem of
Android malware classifiers. Since machine learning technique needs to be
continuously deployed, we use active learning: we select new samples for
analysts to label, and then add the labeled samples to the training set to
retrain the classifier. Our key idea is, similarity-based uncertainty is more
robust against concept drift. Therefore, we combine contrastive learning with
active learning. We propose a new hierarchical contrastive learning scheme, and
a new sample selection technique to continuously train the Android malware
classifier. Our evaluation shows that this leads to significant improvements,
compared to previously published methods for active learning. Our approach
reduces the false negative rate from 16% (for the best baseline) to 10%, while
maintaining the same false positive rate (0.6%). Also, our approach maintains
more consistent performance across a seven-year time period than past methods.
</p>

<h3>Title: Outlier-Robust Gromov Wasserstein for Graph Data. (arXiv:2302.04610v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04610">http://arxiv.org/abs/2302.04610</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04610] Outlier-Robust Gromov Wasserstein for Graph Data](http://arxiv.org/abs/2302.04610) #robust</code></li>
<li>Summary: <p>Gromov Wasserstein (GW) distance is a powerful tool for comparing and
aligning probability distributions supported on different metric spaces. It has
become the main modeling technique for aligning heterogeneous data for a wide
range of graph learning tasks. However, the GW distance is known to be highly
sensitive to outliers, which can result in large inaccuracies if the outliers
are given the same weight as other samples in the objective function. To
mitigate this issue, we introduce a new and robust version of the GW distance
called RGW. RGW features optimistically perturbed marginal constraints within a
$\varphi$-divergence based ambiguity set. To make the benefits of RGW more
accessible in practice, we develop a computationally efficient algorithm,
Bregman proximal alternating linearization minimization, with a theoretical
convergence guarantee. Through extensive experimentation, we validate our
theoretical results and demonstrate the effectiveness of RGW on real-world
graph learning tasks, such as subgraph matching and partial shape
correspondence.
</p></li>
</ul>

<h3>Title: Learning Mixtures of Markov Chains with Quality Guarantees. (arXiv:2302.04680v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04680">http://arxiv.org/abs/2302.04680</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04680] Learning Mixtures of Markov Chains with Quality Guarantees](http://arxiv.org/abs/2302.04680) #robust</code></li>
<li>Summary: <p>A large number of modern applications ranging from listening songs online and
browsing the Web to using a navigation app on a smartphone generate a plethora
of user trails. Clustering such trails into groups with a common sequence
pattern can reveal significant structure in human behavior that can lead to
improving user experience through better recommendations, and even prevent
suicides [LMCR14]. One approach to modeling this problem mathematically is as a
mixture of Markov chains. Recently, Gupta, Kumar and Vassilvitski [GKV16]
introduced an algorithm (GKV-SVD) based on the singular value decomposition
(SVD) that under certain conditions can perfectly recover a mixture of L chains
on n states, given only the distribution of trails of length 3 (3-trail).
</p></li>
</ul>

<p>In this work we contribute to the problem of unmixing Markov chains by
highlighting and addressing two important constraints of the GKV-SVD algorithm
[GKV16]: some chains in the mixture may not even be weakly connected, and
secondly in practice one does not know beforehand the true number of chains. We
resolve these issues in the Gupta et al. paper [GKV16]. Specifically, we
propose an algebraic criterion that enables us to choose a value of L
efficiently that avoids overfitting. Furthermore, we design a reconstruction
algorithm that outputs the true mixture in the presence of disconnected chains
and is robust to noise. We complement our theoretical results with experiments
on both synthetic and real data, where we observe that our method outperforms
the GKV-SVD algorithm. Finally, we empirically observe that combining an
EM-algorithm with our method performs best in practice, both in terms of
reconstruction error with respect to the distribution of 3-trails and the
mixture of Markov Chains.
</p>

<h3>Title: Equivariant MuZero. (arXiv:2302.04798v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04798">http://arxiv.org/abs/2302.04798</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04798] Equivariant MuZero](http://arxiv.org/abs/2302.04798) #robust</code></li>
<li>Summary: <p>Deep reinforcement learning repeatedly succeeds in closed, well-defined
domains such as games (Chess, Go, StarCraft). The next frontier is real-world
scenarios, where setups are numerous and varied. For this, agents need to learn
the underlying rules governing the environment, so as to robustly generalise to
conditions that differ from those they were trained on. Model-based
reinforcement learning algorithms, such as the highly successful MuZero, aim to
accomplish this by learning a world model. However, leveraging a world model
has not consistently shown greater generalisation capabilities compared to
model-free alternatives. In this work, we propose improving the data efficiency
and generalisation capabilities of MuZero by explicitly incorporating the
symmetries of the environment in its world-model architecture. We prove that,
so long as the neural networks used by MuZero are equivariant to a particular
symmetry group acting on the environment, the entirety of MuZero's
action-selection algorithm will also be equivariant to that group. We evaluate
Equivariant MuZero on procedurally-generated MiniPacman and on Chaser from the
ProcGen suite: training on a set of mazes, and then testing on unseen rotated
versions, demonstrating the benefits of equivariance. Further, we verify that
our performance improvements hold even when only some of the components of
Equivariant MuZero obey strict equivariance, which highlights the robustness of
our construction.
</p></li>
</ul>

<h3>Title: Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments. (arXiv:2302.04823v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04823">http://arxiv.org/abs/2302.04823</a></li>
<li>Code URL: <a href="https://github.com/gustavokcouto/hgail">https://github.com/gustavokcouto/hgail</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04823] Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments](http://arxiv.org/abs/2302.04823) #robust</code></li>
<li>Summary: <p>Deriving robust control policies for realistic urban navigation scenarios is
not a trivial task. In an end-to-end approach, these policies must map
high-dimensional images from the vehicle's cameras to low-level actions such as
steering and throttle. While pure Reinforcement Learning (RL) approaches are
based exclusively on rewards,Generative Adversarial Imitation Learning (GAIL)
agents learn from expert demonstrations while interacting with the environment,
which favors GAIL on tasks for which a reward signal is difficult to derive. In
this work, the hGAIL architecture was proposed to solve the autonomous
navigation of a vehicle in an end-to-end approach, mapping sensory perceptions
directly to low-level actions, while simultaneously learning mid-level input
representations of the agent's environment. The proposed hGAIL consists of an
hierarchical Adversarial Imitation Learning architecture composed of two main
modules: the GAN (Generative Adversarial Nets) which generates the Bird's-Eye
View (BEV) representation mainly from the images of three frontal cameras of
the vehicle, and the GAIL which learns to control the vehicle based mainly on
the BEV predictions from the GAN as input.Our experiments have shown that GAIL
exclusively from cameras (without BEV) fails to even learn the task, while
hGAIL, after training, was able to autonomously navigate successfully in all
intersections of the city.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04449">http://arxiv.org/abs/2302.04449</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04449] Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals](http://arxiv.org/abs/2302.04449) #extraction</code></li>
<li>Summary: <p>High sample complexity has long been a challenge for RL. On the other hand,
humans learn to perform tasks not only from interaction or demonstrations, but
also by reading unstructured text documents, e.g., instruction manuals.
Instruction manuals and wiki pages are among the most abundant data that could
inform agents of valuable features and policies or task-specific environmental
dynamics and reward structures. Therefore, we hypothesize that the ability to
utilize human-written instruction manuals to assist learning policies for
specific tasks should lead to a more efficient and better-performing agent.
</p></li>
</ul>

<p>We propose the Read and Reward framework. Read and Reward speeds up RL
algorithms on Atari games by reading manuals released by the Atari game
developers. Our framework consists of a QA Extraction module that extracts and
summarizes relevant information from the manual and a Reasoning module that
evaluates object-agent interactions based on information from the manual.
Auxiliary reward is then provided to a standard A2C RL agent, when interaction
is detected. When assisted by our design, A2C improves on 4 games in the Atari
environment with sparse rewards, and requires 1000x less training frames
compared to the previous SOTA Agent 57 on Skiing, the hardest game in Atari.
</p>

<h3>Title: Global Constraints with Prompting for Zero-Shot Event Argument Classification. (arXiv:2302.04459v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04459">http://arxiv.org/abs/2302.04459</a></li>
<li>Code URL: <a href="https://github.com/HKUST-KnowComp/Constraints-with-Prompting-for-Zero-Shot-EAC">https://github.com/HKUST-KnowComp/Constraints-with-Prompting-for-Zero-Shot-EAC</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04459] Global Constraints with Prompting for Zero-Shot Event Argument Classification](http://arxiv.org/abs/2302.04459) #extraction</code></li>
<li>Summary: <p>Determining the role of event arguments is a crucial subtask of event
extraction. Most previous supervised models leverage costly annotations, which
is not practical for open-domain applications. In this work, we propose to use
global constraints with prompting to effectively tackles event argument
classification without any annotation and task-specific training. Specifically,
given an event and its associated passage, the model first creates several new
passages by prefix prompts and cloze prompts, where prefix prompts indicate
event type and trigger span, and cloze prompts connect each candidate role with
the target argument span. Then, a pre-trained language model scores the new
passages, making the initial prediction. Our novel prompt templates can easily
adapt to all events and argument types without manual effort. Next, the model
regularizes the prediction by global constraints exploiting cross-task,
cross-argument, and cross-event relations. Extensive experiments demonstrate
our model's effectiveness: it outperforms the best zero-shot baselines by 12.5%
and 10.9% F1 on ACE and ERE with given argument spans and by 4.3% and 3.3% F1,
respectively, without given argument spans. We have made our code publicly
available.
</p></li>
</ul>

<h3>Title: Bag of Tricks for Training Data Extraction from Language Models. (arXiv:2302.04460v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04460">http://arxiv.org/abs/2302.04460</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04460] Bag of Tricks for Training Data Extraction from Language Models](http://arxiv.org/abs/2302.04460) #extraction</code></li>
<li>Summary: <p>With the advance of language models, privacy protection is receiving more
attention. Training data extraction is therefore of great importance, as it can
serve as a potential tool to assess privacy leakage. However, due to the
difficulty of this task, most of the existing methods are proof-of-concept and
still not effective enough. In this paper, we investigate and benchmark tricks
for improving training data extraction using a publicly available dataset.
Because most existing extraction methods use a pipeline of
generating-then-ranking, i.e., generating text candidates as potential training
data and then ranking them based on specific criteria, our research focuses on
the tricks for both text generation (e.g., sampling strategy) and text ranking
(e.g., token-level criteria). The experimental results show that several
previously overlooked tricks can be crucial to the success of training data
extraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricks
outperform the baseline by a large margin in most cases, providing a much
stronger baseline for future research.
</p></li>
</ul>

<h3>Title: Lightweight Transformers for Clinical Natural Language Processing. (arXiv:2302.04725v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04725">http://arxiv.org/abs/2302.04725</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04725] Lightweight Transformers for Clinical Natural Language Processing](http://arxiv.org/abs/2302.04725) #extraction</code></li>
<li>Summary: <p>Specialised pre-trained language models are becoming more frequent in NLP
since they can potentially outperform models trained on generic texts. BioBERT
and BioClinicalBERT are two examples of such models that have shown promise in
medical NLP tasks. Many of these models are overparametrised and
resource-intensive, but thanks to techniques like Knowledge Distillation (KD),
it is possible to create smaller versions that perform almost as well as their
larger counterparts. In this work, we specifically focus on development of
compact language models for processing clinical texts (i.e. progress notes,
discharge summaries etc). We developed a number of efficient lightweight
clinical transformers using knowledge distillation and continual learning, with
the number of parameters ranging from 15 million to 65 million. These models
performed comparably to larger models such as BioBERT and ClinicalBioBERT and
significantly outperformed other compact models trained on general or
biomedical data. Our extensive evaluation was done across several standard
datasets and covered a wide range of clinical text-mining tasks, including
Natural Language Inference, Relation Extraction, Named Entity Recognition, and
Sequence Classification. To our knowledge, this is the first comprehensive
study specifically focused on creating efficient and compact transformers for
clinical NLP tasks. The models and code used in this study can be found on our
Huggingface profile at https://huggingface.co/nlpie and Github page at
https://github.com/nlpie-research/Lightweight-Clinical-Transformers,
respectively, promoting reproducibility of our results.
</p></li>
</ul>

<h3>Title: Massively Multilingual Language Models for Cross Lingual Fact Extraction from Low Resource Indian Languages. (arXiv:2302.04790v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04790">http://arxiv.org/abs/2302.04790</a></li>
<li>Code URL: <a href="https://github.com/bhavyajeet/clfe">https://github.com/bhavyajeet/clfe</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04790] Massively Multilingual Language Models for Cross Lingual Fact Extraction from Low Resource Indian Languages](http://arxiv.org/abs/2302.04790) #extraction</code></li>
<li>Summary: <p>Massive knowledge graphs like Wikidata attempt to capture world knowledge
about multiple entities. Recent approaches concentrate on automatically
enriching these KGs from text. However a lot of information present in the form
of natural text in low resource languages is often missed out. Cross Lingual
Information Extraction aims at extracting factual information in the form of
English triples from low resource Indian Language text. Despite its massive
potential, progress made on this task is lagging when compared to Monolingual
Information Extraction. In this paper, we propose the task of Cross Lingual
Fact Extraction(CLFE) from text and devise an end-to-end generative approach
for the same which achieves an overall F1 score of 77.46.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Towards Model-Agnostic Federated Learning over Networks. (arXiv:2302.04363v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04363">http://arxiv.org/abs/2302.04363</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04363] Towards Model-Agnostic Federated Learning over Networks](http://arxiv.org/abs/2302.04363) #federate</code></li>
<li>Summary: <p>We present a model-agnostic federated learning method for decentralized data
with an intrinsic network structure. The network structure reflects
similarities between the (statistics of) local datasets and, in turn, their
associated local models. Our method is an instance of empirical risk
minimization, using a regularization term that is constructed from the network
structure of data. In particular, we require well-connected local models,
forming clusters, to yield similar predictions on a common test set. In
principle our method can be applied to any collection of local models. The only
restriction put on these local models is that they allow for efficient
implementation of regularized empirical risk minimization (training). Such
implementations might be available in the form of high-level programming
frameworks such as \texttt{scikit-learn}, \texttt{Keras} or \texttt{PyTorch}.
</p></li>
</ul>

<h3>Title: Towards Fairer and More Efficient Federated Learning via Multidimensional Personalized Edge Models. (arXiv:2302.04464v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04464">http://arxiv.org/abs/2302.04464</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04464] Towards Fairer and More Efficient Federated Learning via Multidimensional Personalized Edge Models](http://arxiv.org/abs/2302.04464) #federate</code></li>
<li>Summary: <p>Federated learning (FL) is an emerging technique that trains massive and
geographically distributed edge data while maintaining privacy. However, FL has
inherent challenges in terms of fairness and computational efficiency due to
the rising heterogeneity of edges, and thus usually result in sub-optimal
performance in recent state-of-the-art (SOTA) solutions. In this paper, we
propose a Customized Federated Learning (CFL) system to eliminate FL
heterogeneity from multiple dimensions. Specifically, CFL tailors personalized
models from the specially designed global model for each client, jointly guided
an online trained model-search helper and a novel aggregation algorithm.
Extensive experiments demonstrate that CFL has full-stack advantages for both
FL training and edge reasoning and significantly improves the SOTA performance
w.r.t. model accuracy (up to 7.2% in the non-heterogeneous environment and up
to 21.8% in the heterogeneous environment), efficiency, and FL fairness.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Mitigating Bias in Visual Transformers via Targeted Alignment. (arXiv:2302.04358v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04358">http://arxiv.org/abs/2302.04358</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04358] Mitigating Bias in Visual Transformers via Targeted Alignment](http://arxiv.org/abs/2302.04358) #fair</code></li>
<li>Summary: <p>As transformer architectures become increasingly prevalent in computer
vision, it is critical to understand their fairness implications. We perform
the first study of the fairness of transformers applied to computer vision and
benchmark several bias mitigation approaches from prior work. We visualize the
feature space of the transformer self-attention modules and discover that a
significant portion of the bias is encoded in the query matrix. With this
knowledge, we propose TADeT, a targeted alignment strategy for debiasing
transformers that aims to discover and remove bias primarily from query matrix
features. We measure performance using Balanced Accuracy and Standard Accuracy,
and fairness using Equalized Odds and Balanced Accuracy Difference. TADeT
consistently leads to improved fairness over prior work on multiple attribute
prediction tasks on the CelebA dataset, without compromising performance.
</p></li>
</ul>

<h3>Title: On Fairness and Stability: Is Estimator Variance a Friend or a Foe?. (arXiv:2302.04525v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04525">http://arxiv.org/abs/2302.04525</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04525] On Fairness and Stability: Is Estimator Variance a Friend or a Foe?](http://arxiv.org/abs/2302.04525) #fair</code></li>
<li>Summary: <p>The error of an estimator can be decomposed into a (statistical) bias term, a
variance term, and an irreducible noise term. When we do bias analysis,
formally we are asking the question: "how good are the predictions?" The role
of bias in the error decomposition is clear: if we trust the labels/targets,
then we would want the estimator to have as low bias as possible, in order to
minimize error. Fair machine learning is concerned with the question: "Are the
predictions equally good for different demographic/social groups?" This has
naturally led to a variety of fairness metrics that compare some measure of
statistical bias on subsets corresponding to socially privileged and socially
disadvantaged groups. In this paper we propose a new family of performance
measures based on group-wise parity in variance. We demonstrate when group-wise
statistical bias analysis gives an incomplete picture, and what group-wise
variance analysis can tell us in settings that differ in the magnitude of
statistical bias. We develop and release an open-source library that reconciles
uncertainty quantification techniques with fairness analysis, and use it to
conduct an extensive empirical analysis of our variance-based fairness measures
on standard benchmarks.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Q-Diffusion: Quantizing Diffusion Models. (arXiv:2302.04304v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04304">http://arxiv.org/abs/2302.04304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04304] Q-Diffusion: Quantizing Diffusion Models](http://arxiv.org/abs/2302.04304) #diffusion</code></li>
<li>Summary: <p>Diffusion models have recently achieved great success in synthesizing diverse
and high-fidelity images. However, sampling speed and memory constraints remain
a major barrier to the practical adoption of diffusion models as the generation
process for these models can be slow due to the need for iterative noise
estimation using complex neural networks. We propose a solution to this problem
by compressing the noise estimation network to accelerate the generation
process using post-training quantization (PTQ). While existing PTQ approaches
have not been able to effectively deal with the changing output distributions
of noise estimation networks in diffusion models over multiple time steps, we
are able to formulate a PTQ method that is specifically designed to handle the
unique multi-timestep structure of diffusion models with a data calibration
scheme using data sampled from different time steps. Experimental results show
that our proposed method is able to directly quantize full-precision diffusion
models into 8-bit or 4-bit models while maintaining comparable performance in a
training-free manner, achieving a FID change of at most 1.88. Our approach can
also be applied to text-guided image generation, and for the first time we can
run stable diffusion in 4-bit weights without losing much perceptual quality,
as shown in Figure 5 and Figure 9.
</p></li>
</ul>

<h3>Title: Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples. (arXiv:2302.04578v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04578">http://arxiv.org/abs/2302.04578</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04578] Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples](http://arxiv.org/abs/2302.04578) #diffusion</code></li>
<li>Summary: <p>Diffusion Models (DMs) achieve state-of-the-art performance in generative
tasks, boosting a wave in AI for Art. Despite the success of commercialization,
DMs meanwhile provide tools for copyright violations, where infringers benefit
from illegally using paintings created by human artists to train DMs and
generate novel paintings in a similar style. In this paper, we show that it is
possible to create an image $x'$ that is similar to an image $x$ for human
vision but unrecognizable for DMs. We build a framework to define and evaluate
this adversarial example for diffusion models. Based on the framework, we
further propose AdvDM, an algorithm to generate adversarial examples for DMs.
By optimizing upon different latent variables sampled from the reverse process
of DMs, AdvDM conducts a Monte-Carlo estimation of adversarial examples for
DMs. Extensive experiments show that the estimated adversarial examples can
effectively hinder DMs from extracting their features. Our method can be a
powerful tool for human artists to protect their copyright against infringers
with DM-based AI-for-Art applications.
</p></li>
</ul>

<h3>Title: Better Diffusion Models Further Improve Adversarial Training. (arXiv:2302.04638v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04638">http://arxiv.org/abs/2302.04638</a></li>
<li>Code URL: <a href="https://github.com/wzekai99/dm-improves-at">https://github.com/wzekai99/dm-improves-at</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04638] Better Diffusion Models Further Improve Adversarial Training](http://arxiv.org/abs/2302.04638) #diffusion</code></li>
<li>Summary: <p>It has been recognized that the data generated by the denoising diffusion
probabilistic model (DDPM) improves adversarial training. After two years of
rapid development in diffusion models, a question naturally arises: can better
diffusion models further improve adversarial training? This paper gives an
affirmative answer by employing the most recent diffusion model which has
higher efficiency ($\sim 20$ sampling steps) and image quality (lower FID
score) compared with DDPM. Our adversarially trained models achieve
state-of-the-art performance on RobustBench using only generated data (no
external datasets). Under the $\ell_\infty$-norm threat model with
$\epsilon=8/255$, our models achieve $70.69\%$ and $42.67\%$ robust accuracy on
CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous
state-of-the-art models by $+4.58\%$ and $+8.03\%$. Under the $\ell_2$-norm
threat model with $\epsilon=128/255$, our models achieve $84.86\%$ on CIFAR-10
($+4.44\%$). These results also beat previous works that use external data. Our
code is available at https://github.com/wzekai99/DM-Improves-AT.
</p></li>
</ul>

<h3>Title: Is This Loss Informative? Speeding Up Textual Inversion with Deterministic Objective Evaluation. (arXiv:2302.04841v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04841">http://arxiv.org/abs/2302.04841</a></li>
<li>Code URL: <a href="https://github.com/yandex-research/dvar">https://github.com/yandex-research/dvar</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04841] Is This Loss Informative? Speeding Up Textual Inversion with Deterministic Objective Evaluation](http://arxiv.org/abs/2302.04841) #diffusion</code></li>
<li>Summary: <p>Text-to-image generation models represent the next step of evolution in image
synthesis, offering natural means of flexible yet fine-grained control over the
result. One emerging area of research is the rapid adaptation of large
text-to-image models to smaller datasets or new visual concepts. However, the
most efficient method of adaptation, called textual inversion, has a known
limitation of long training time, which both restricts practical applications
and increases the experiment time for research. In this work, we study the
training dynamics of textual inversion, aiming to speed it up. We observe that
most concepts are learned at early stages and do not improve in quality later,
but standard model convergence metrics fail to indicate that. Instead, we
propose a simple early stopping criterion that only requires computing the
textual inversion loss on the same inputs for all training iterations. Our
experiments on both Latent Diffusion and Stable Diffusion models for 93
concepts demonstrate the competitive performance of our method, speeding
adaptation up to 15 times with no significant drops in quality.
</p></li>
</ul>

<h3>Title: UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models. (arXiv:2302.04867v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04867">http://arxiv.org/abs/2302.04867</a></li>
<li>Code URL: <a href="https://github.com/wl-zhao/unipc">https://github.com/wl-zhao/unipc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04867] UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models](http://arxiv.org/abs/2302.04867) #diffusion</code></li>
<li>Summary: <p>Diffusion probabilistic models (DPMs) have demonstrated a very promising
ability in high-resolution image synthesis. However, sampling from a
pre-trained DPM usually requires hundreds of model evaluations, which is
computationally expensive. Despite recent progress in designing high-order
solvers for DPMs, there still exists room for further speedup, especially in
extremely few steps (e.g., 5~10 steps). Inspired by the predictor-corrector for
ODE solvers, we develop a unified corrector (UniC) that can be applied after
any existing DPM sampler to increase the order of accuracy without extra model
evaluations, and derive a unified predictor (UniP) that supports arbitrary
order as a byproduct. Combining UniP and UniC, we propose a unified
predictor-corrector framework called UniPC for the fast sampling of DPMs, which
has a unified analytical form for any order and can significantly improve the
sampling quality over previous methods. We evaluate our methods through
extensive experiments including both unconditional and conditional sampling
using pixel-space and latent-space DPMs. Our UniPC can achieve 3.87 FID on
CIFAR10 (unconditional) and 7.51 FID on ImageNet 256$\times$256 (conditional)
with only 10 function evaluations. Code is available at
https://github.com/wl-zhao/UniPC
</p></li>
</ul>

<h3>Title: MedDiff: Generating Electronic Health Records using Accelerated Denoising Diffusion Model. (arXiv:2302.04355v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04355">http://arxiv.org/abs/2302.04355</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04355] MedDiff: Generating Electronic Health Records using Accelerated Denoising Diffusion Model](http://arxiv.org/abs/2302.04355) #diffusion</code></li>
<li>Summary: <p>Due to patient privacy protection concerns, machine learning research in
healthcare has been undeniably slower and limited than in other application
domains. High-quality, realistic, synthetic electronic health records (EHRs)
can be leveraged to accelerate methodological developments for research
purposes while mitigating privacy concerns associated with data sharing. The
current state-of-the-art model for synthetic EHR generation is generative
adversarial networks, which are notoriously difficult to train and can suffer
from mode collapse. Denoising Diffusion Probabilistic Models, a class of
generative models inspired by statistical thermodynamics, have recently been
shown to generate high-quality synthetic samples in certain domains. It is
unknown whether these can generalize to generation of large-scale,
high-dimensional EHRs. In this paper, we present a novel generative model based
on diffusion models that is the first successful application on electronic
health records. Our model proposes a mechanism to perform class-conditional
sampling to preserve label information. We also introduce a new sampling
strategy to accelerate the inference speed. We empirically show that our model
outperforms existing state-of-the-art synthetic EHR generation methods.
</p></li>
</ul>

<h3>Title: Geometry-Complete Diffusion for 3D Molecule Generation. (arXiv:2302.04313v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04313">http://arxiv.org/abs/2302.04313</a></li>
<li>Code URL: <a href="https://github.com/bioinfomachinelearning/bio-diffusion">https://github.com/bioinfomachinelearning/bio-diffusion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04313] Geometry-Complete Diffusion for 3D Molecule Generation](http://arxiv.org/abs/2302.04313) #diffusion</code></li>
<li>Summary: <p>Denoising diffusion probabilistic models (DDPMs) have recently taken the
field of generative modeling by storm, pioneering new state-of-the-art results
in disciplines such as computer vision and computational biology for diverse
tasks ranging from text-guided image generation to structure-guided protein
design. Along this latter line of research, methods such as those of Hoogeboom
et al. 2022 have been proposed for unconditionally generating 3D molecules
using equivariant graph neural networks (GNNs) within a DDPM framework. Toward
this end, we propose GCDM, a geometry-complete diffusion model that achieves
new state-of-the-art results for 3D molecule diffusion generation by leveraging
the representation learning strengths offered by GNNs that perform
geometry-complete message-passing. Our results with GCDM also offer preliminary
insights into how physical inductive biases impact the generative dynamics of
molecular DDPMs. The source code, data, and instructions to train new models or
reproduce our results are freely available at
https://github.com/BioinfoMachineLearning/bio-diffusion.
</p></li>
</ul>

<h3>Title: Geometry of Score Based Generative Models. (arXiv:2302.04411v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04411">http://arxiv.org/abs/2302.04411</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04411] Geometry of Score Based Generative Models](http://arxiv.org/abs/2302.04411) #diffusion</code></li>
<li>Summary: <p>In this work, we look at Score-based generative models (also called diffusion
generative models) from a geometric perspective. From a new view point, we
prove that both the forward and backward process of adding noise and generating
from noise are Wasserstein gradient flow in the space of probability measures.
We are the first to prove this connection. Our understanding of Score-based
(and Diffusion) generative models have matured and become more complete by
drawing ideas from different fields like Bayesian inference, control theory,
stochastic differential equation and Schrodinger bridge. However, many open
questions and challenges remain. One problem, for example, is how to decrease
the sampling time? We demonstrate that looking from geometric perspective
enables us to answer many of these questions and provide new interpretations to
some known results. Furthermore, geometric perspective enables us to devise an
intuitive geometric solution to the problem of faster sampling. By augmenting
traditional score-based generative models with a projection step, we show that
we can generate high quality images with significantly fewer sampling-steps.
</p></li>
</ul>

<h3>Title: Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion. (arXiv:2302.04451v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04451">http://arxiv.org/abs/2302.04451</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04451] Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion](http://arxiv.org/abs/2302.04451) #diffusion</code></li>
<li>Summary: <p>Graph neural networks are widely used tools for graph prediction tasks.
Motivated by their empirical performance, prior works have developed
generalization bounds for graph neural networks, which scale with graph
structures in terms of the maximum degree. In this paper, we present
generalization bounds that instead scale with the largest singular value of the
graph neural network's feature diffusion matrix. These bounds are numerically
much smaller than prior bounds for real-world graphs. We also construct a lower
bound of the generalization gap that matches our upper bound asymptotically. To
achieve these results, we analyze a unified model that includes prior works'
settings (i.e., convolutional and message-passing networks) and new settings
(i.e., graph isomorphism networks). Our key idea is to measure the stability of
graph neural networks against noise perturbations using Hessians. Empirically,
we find that Hessian-based measurements correlate with the observed
generalization gaps of graph neural networks accurately; Optimizing noise
stability properties for fine-tuning pretrained graph neural networks also
improves test performance on several graph-level classification tasks.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
