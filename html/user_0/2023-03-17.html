<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Trustera: A Live Conversation Redaction System. (arXiv:2303.09438v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09438">http://arxiv.org/abs/2303.09438</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09438] Trustera: A Live Conversation Redaction System](http://arxiv.org/abs/2303.09438) #secure</code></li>
<li>Summary: <p>Trustera, the first functional system that redacts personally identifiable
information (PII) in real-time spoken conversations to remove agents' need to
hear sensitive information while preserving the naturalness of live
customer-agent conversations. As opposed to post-call redaction, audio masking
starts as soon as the customer begins speaking to a PII entity. This
significantly reduces the risk of PII being intercepted or stored in insecure
data storage. Trustera's architecture consists of a pipeline of automatic
speech recognition, natural language understanding, and a live audio redactor
module. The system's goal is three-fold: redact entities that are PII, mask the
audio that goes to the agent, and at the same time capture the entity, so that
the captured PII can be used for a payment transaction or caller
identification. Trustera is currently being used by thousands of agents to
secure customers' sensitive information.
</p></li>
</ul>

<h3>Title: vFHE: Verifiable Fully Homomorphic Encryption with Blind Hash. (arXiv:2303.08886v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08886">http://arxiv.org/abs/2303.08886</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08886] vFHE: Verifiable Fully Homomorphic Encryption with Blind Hash](http://arxiv.org/abs/2303.08886) #secure</code></li>
<li>Summary: <p>Fully homomorphic encryption (FHE) is a powerful encryption technique that
allows for computation to be performed on ciphertext without the need for
decryption. FHE will thus enable privacy-preserving computation and a wide
range of applications, such as secure cloud computing on sensitive medical and
financial data, secure machine learning, etc. Prior research in FHE has largely
concentrated on improving its speed, and great stride has been made. However,
there has been a scarcity of research on addressing a major challenge of FHE
computation: client-side data owners cannot verify the integrity of the
calculations performed by the service and computation providers, hence cannot
be assured of the correctness of computation results. This is particularly
concerning when the service or computation provider may act in an
untrustworthy, unreliable, or malicious manner and tampers the computational
results. Prior work on ensuring FHE computational integrity has been
non-universal or incurring too much overhead. We propose vFHE to add
computational integrity to FHE without losing universality and without
incurring high performance overheads.
</p></li>
</ul>

<h3>Title: Cryptographic Primitives based on Compact Knapsack Problem. (arXiv:2303.08973v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08973">http://arxiv.org/abs/2303.08973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08973] Cryptographic Primitives based on Compact Knapsack Problem](http://arxiv.org/abs/2303.08973) #secure</code></li>
<li>Summary: <p>In the present paper, we extend previous results of an id scheme based on
compact knapsack problem defined by one equation. We present a sound three-move
id scheme based on compact knapsack problem defined by an integer matrix. We
study this problem by providing attacks based on lattices. Furthermore, we
provide the corresponding digital signature obtained by Fiat-Shamir transform
and we prove that is secure under ROM. These primitives are post quantum
resistant.
</p></li>
</ul>

<h3>Title: Web and Mobile Platforms for Managing Elections based on IoT And Machine Learning Algorithms. (arXiv:2303.09045v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09045">http://arxiv.org/abs/2303.09045</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09045] Web and Mobile Platforms for Managing Elections based on IoT And Machine Learning Algorithms](http://arxiv.org/abs/2303.09045) #secure</code></li>
<li>Summary: <p>The global pandemic situation has severely affected all countries. As a
result, almost all countries had to adjust to online technologies to continue
their processes. In addition, Sri Lanka is yearly spending ten billion on
elections. We have examined a proper way of minimizing the cost of hosting
these events online. To solve the existing problems and increase the time
potency and cost reduction we have used IoT and ML-based technologies.
IoT-based data will identify, register, and be used to secure from fraud, while
ML algorithms manipulate the election data and produce winning predictions,
weather-based voters attendance, and election violence. All the data will be
saved in cloud computing and a standard database to store and access the data.
This study mainly focuses on four aspects of an E-voting system. The most
frequent problems across the world in E-voting are the security, accuracy, and
reliability of the systems. E-government systems must be secured against
various cyber-attacks and ensure that only authorized users can access
valuable, and sometimes sensitive information. Being able to access a system
without passwords but using biometric details has been there for a while now,
however, our proposed system has a different approach to taking the
credentials, processing, and combining the images, reformatting and producing
the output, and tracking. In addition, we ensure to enhance e-voting safety.
While ML-based algorithms use different data sets and provide predictions in
advance.
</p></li>
</ul>

<h3>Title: MASCARA: Systematically Generating Memorable And Secure Passphrases. (arXiv:2303.09150v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09150">http://arxiv.org/abs/2303.09150</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09150] MASCARA: Systematically Generating Memorable And Secure Passphrases](http://arxiv.org/abs/2303.09150) #secure</code></li>
<li>Summary: <p>Passwords are the most common mechanism for authenticating users online.
However, studies have shown that users find it difficult to create and manage
secure passwords. To that end, passphrases are often recommended as a usable
alternative to passwords, which would potentially be easy to remember and hard
to guess. However, as we show, user-chosen passphrases fall short of being
secure, while state-of-the-art machine-generated passphrases are difficult to
remember. In this work, we aim to tackle the drawbacks of the systems that
generate passphrases for practical use. In particular, we address the problem
of generating secure and memorable passphrases and compare them against user
chosen passphrases in use. We identify and characterize 72, 999 user-chosen
in-use unique English passphrases from prior leaked password databases. Then we
leverage this understanding to create a novel framework for measuring
memorability and guessability of passphrases. Utilizing our framework, we
design MASCARA, which follows a constrained Markov generation process to create
passphrases that optimize for both memorability and guessability. Our
evaluation of passphrases shows that MASCARA-generated passphrases are harder
to guess than in-use user-generated passphrases, while being easier to remember
compared to state-of-the-art machine-generated passphrases. We conduct a
two-part user study with crowdsourcing platform Prolific to demonstrate that
users have highest memory-recall (and lowest error rate) while using MASCARA
passphrases. Moreover, for passphrases of length desired by the users, the
recall rate is 60-100% higher for MASCARA-generated passphrases compared to
current system-generated ones.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Fast and Accurate Object Detection on Asymmetrical Receptive Field. (arXiv:2303.08995v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08995">http://arxiv.org/abs/2303.08995</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08995] Fast and Accurate Object Detection on Asymmetrical Receptive Field](http://arxiv.org/abs/2303.08995) #security</code></li>
<li>Summary: <p>Object detection has been used in a wide range of industries. For example, in
autonomous driving, the task of object detection is to accurately and
efficiently identify and locate a large number of predefined classes of object
instances (vehicles, pedestrians, traffic signs, etc.) from videos of roads. In
robotics, the industry robot needs to recognize specific machine elements. In
the security field, the camera should accurately recognize each face of people.
With the wide application of deep learning, the accuracy and efficiency of
object detection have been greatly improved, but object detection based on deep
learning still faces challenges. Different applications of object detection
have different requirements, including highly accurate detection,
multi-category object detection, real-time detection, robustness to occlusions,
etc. To address the above challenges, based on extensive literature research,
this paper analyzes methods for improving and optimizing mainstream object
detection algorithms from the perspective of evolution of one-stage and
two-stage object detection algorithms. Furthermore, this article proposes
methods for improving object detection accuracy from the perspective of
changing receptive fields. The new model is based on the original YOLOv5 (You
Look Only Once) with some modifications. The structure of the head part of
YOLOv5 is modified by adding asymmetrical pooling layers. As a result, the
accuracy of the algorithm is improved while ensuring the speed. The
performances of the new model in this article are compared with original YOLOv5
model and analyzed from several parameters. And the evaluation of the new model
is presented in four situations. Moreover, the summary and outlooks are made on
the problems to be solved and the research directions in the future.
</p></li>
</ul>

<h3>Title: SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning. (arXiv:2303.09079v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09079">http://arxiv.org/abs/2303.09079</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09079] SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning](http://arxiv.org/abs/2303.09079) #security</code></li>
<li>Summary: <p>Self-supervised learning (SSL) is a commonly used approach to learning and
encoding data representations. By using a pre-trained SSL image encoder and
training a downstream classifier on top of it, impressive performance can be
achieved on various tasks with very little labeled data. The increasing usage
of SSL has led to an uptick in security research related to SSL encoders and
the development of various Trojan attacks. The danger posed by Trojan attacks
inserted in SSL encoders lies in their ability to operate covertly and spread
widely among various users and devices. The presence of backdoor behavior in
Trojaned encoders can inadvertently be inherited by downstream classifiers,
making it even more difficult to detect and mitigate the threat. Although
current Trojan detection methods in supervised learning can potentially
safeguard SSL downstream classifiers, identifying and addressing triggers in
the SSL encoder before its widespread dissemination is a challenging task. This
is because downstream tasks are not always known, dataset labels are not
available, and even the original training dataset is not accessible during the
SSL encoder Trojan detection. This paper presents an innovative technique
called SSL-Cleanse that is designed to detect and mitigate backdoor attacks in
SSL encoders. We evaluated SSL-Cleanse on various datasets using 300 models,
achieving an average detection success rate of 83.7% on ImageNet-100. After
mitigating backdoors, on average, backdoored encoders achieve 0.24% attack
success rate without great accuracy loss, proving the effectiveness of
SSL-Cleanse.
</p></li>
</ul>

<h3>Title: Generic Decoding of Restricted Errors. (arXiv:2303.08882v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08882">http://arxiv.org/abs/2303.08882</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08882] Generic Decoding of Restricted Errors](http://arxiv.org/abs/2303.08882) #security</code></li>
<li>Summary: <p>Several recently proposed code-based cryptosystems base their security on a
slightly generalized version of the classical (syndrome) decoding problem.
Namely, in the so-called restricted (syndrome) decoding problem, the error
values stem from a restricted set. In this paper, we propose new generic
decoders, that are inspired by subset sum solvers and tailored to the new
setting. The introduced algorithms take the restricted structure of the error
set into account in order to utilize the representation technique efficiently.
This leads to a considerable decrease in the security levels of recently
published code-based cryptosystems.
</p></li>
</ul>

<h3>Title: Security of Blockchains at Capacity. (arXiv:2303.09113v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09113">http://arxiv.org/abs/2303.09113</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09113] Security of Blockchains at Capacity](http://arxiv.org/abs/2303.09113) #security</code></li>
<li>Summary: <p>Given a network of nodes with certain communication and computation
capacities, what is the maximum rate at which a blockchain can run securely? We
study this question for proof-of-work (PoW) and proof-of-stake (PoS) longest
chain protocols under a 'bounded bandwidth' model which captures queuing and
processing delays due to high block rate relative to capacity, bursty release
of adversarial blocks, and in PoS, spamming due to equivocations.
</p></li>
</ul>

<p>We demonstrate that security of both PoW and PoS longest chain, when
operating at capacity, requires carefully designed scheduling policies that
correctly prioritize which blocks are processed first, as we show attack
strategies tailored to such policies. In PoS, we show an attack exploiting
equivocations, which highlights that the throughput of the PoS longest chain
protocol with a broad class of scheduling policies must decrease as the desired
security error probability decreases. At the same time, through an improved
analysis method, our work is the first to identify block production rates under
which PoW longest chain is secure in the bounded bandwidth setting. We also
present the first PoS longest chain protocol, SaPoS, which is secure with a
block production rate independent of the security error probability, by using
an 'equivocation removal' policy to prevent equivocation spamming.
</p>

<h2>privacy</h2>
<h3>Title: Image Classifiers Leak Sensitive Attributes About Their Classes. (arXiv:2303.09289v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09289">http://arxiv.org/abs/2303.09289</a></li>
<li>Code URL: <a href="https://github.com/lukasstruppek/class_attribute_inference_attacks">https://github.com/lukasstruppek/class_attribute_inference_attacks</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09289] Image Classifiers Leak Sensitive Attributes About Their Classes](http://arxiv.org/abs/2303.09289) #privacy</code></li>
<li>Summary: <p>Neural network-based image classifiers are powerful tools for computer vision
tasks, but they inadvertently reveal sensitive attribute information about
their classes, raising concerns about their privacy. To investigate this
privacy leakage, we introduce the first Class Attribute Inference Attack
(Caia), which leverages recent advances in text-to-image synthesis to infer
sensitive attributes of individual classes in a black-box setting, while
remaining competitive with related white-box attacks. Our extensive experiments
in the face recognition domain show that Caia can accurately infer undisclosed
sensitive attributes, such as an individual's hair color, gender and racial
appearance, which are not part of the training labels. Interestingly, we
demonstrate that adversarial robust models are even more vulnerable to such
privacy leakage than standard models, indicating that a trade-off between
robustness and privacy exists.
</p></li>
</ul>

<h3>Title: A Short Survey of Viewing Large Language Models in Legal Aspect. (arXiv:2303.09136v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09136">http://arxiv.org/abs/2303.09136</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09136] A Short Survey of Viewing Large Language Models in Legal Aspect](http://arxiv.org/abs/2303.09136) #privacy</code></li>
<li>Summary: <p>Large language models (LLMs) have transformed many fields, including natural
language processing, computer vision, and reinforcement learning. These models
have also made a significant impact in the field of law, where they are being
increasingly utilized to automate various legal tasks, such as legal judgement
prediction, legal document analysis, and legal document writing. However, the
integration of LLMs into the legal field has also raised several legal
problems, including privacy concerns, bias, and explainability. In this survey,
we explore the integration of LLMs into the field of law. We discuss the
various applications of LLMs in legal tasks, examine the legal challenges that
arise from their use, and explore the data resources that can be used to
specialize LLMs in the legal domain. Finally, we discuss several promising
directions and conclude this paper. By doing so, we hope to provide an overview
of the current state of LLMs in law and highlight the potential benefits and
challenges of their integration.
</p></li>
</ul>

<h3>Title: Text-to-ECG: 12-Lead Electrocardiogram Synthesis conditioned on Clinical Text Reports. (arXiv:2303.09395v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09395">http://arxiv.org/abs/2303.09395</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09395] Text-to-ECG: 12-Lead Electrocardiogram Synthesis conditioned on Clinical Text Reports](http://arxiv.org/abs/2303.09395) #privacy</code></li>
<li>Summary: <p>Electrocardiogram (ECG) synthesis is the area of research focused on
generating realistic synthetic ECG signals for medical use without concerns
over annotation costs or clinical data privacy restrictions. Traditional ECG
generation models consider a single ECG lead and utilize GAN-based generative
models. These models can only generate single lead samples and require separate
training for each diagnosis class. The diagnosis classes of ECGs are
insufficient to capture the intricate differences between ECGs depending on
various features (e.g. patient demographic details, co-existing diagnosis
classes, etc.). To alleviate these challenges, we present a text-to-ECG task,
in which textual inputs are used to produce ECG outputs. Then we propose
Auto-TTE, an autoregressive generative model conditioned on clinical text
reports to synthesize 12-lead ECGs, for the first time to our knowledge. We
compare the performance of our model with other representative models in
text-to-speech and text-to-image. Experimental results show the superiority of
our model in various quantitative evaluations and qualitative analysis.
Finally, we conduct a user study with three board-certified cardiologists to
confirm the fidelity and semantic alignment of generated samples. our code will
be available at https://github.com/TClife/text_to_ecg
</p></li>
</ul>

<h3>Title: Not Seen, Not Heard in the Digital World! Measuring Privacy Practices in Children's Apps. (arXiv:2303.09008v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09008">http://arxiv.org/abs/2303.09008</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09008] Not Seen, Not Heard in the Digital World! Measuring Privacy Practices in Children's Apps](http://arxiv.org/abs/2303.09008) #privacy</code></li>
<li>Summary: <p>The digital age has brought a world of opportunity to children. Connectivity
can be a game-changer for some of the world's most marginalized children.
However, while legislatures around the world have enacted regulations to
protect children's online privacy, and app stores have instituted various
protections, privacy in mobile apps remains a growing concern for parents and
wider society. In this paper, we explore the potential privacy issues and
threats that exist in these apps. We investigate 20,195 mobile apps from the
Google Play store that are designed particularly for children (Family apps) or
include children in their target user groups (Normal apps). Using both static
and dynamic analysis, we find that 4.47% of Family apps request location
permissions, even though collecting location information from children is
forbidden by the Play store, and 81.25% of Family apps use trackers (which are
not allowed in children's apps). Even major developers with 40+ kids apps on
the Play store use ad trackers. Furthermore, we find that most permission
request notifications are not well designed for children, and 19.25% apps have
inconsistent content age ratings across the different protection authorities.
Our findings suggest that, despite significant attention to children's privacy,
a large gap between regulatory provisions, app store policies, and actual
development practices exist. Our research sheds light for government
policymakers, app stores, and developers.
</p></li>
</ul>

<h3>Title: Privacy-Preserving Video Conferencing via Thermal-Generative Images. (arXiv:2303.09279v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09279">http://arxiv.org/abs/2303.09279</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09279] Privacy-Preserving Video Conferencing via Thermal-Generative Images](http://arxiv.org/abs/2303.09279) #privacy</code></li>
<li>Summary: <p>Due to the COVID-19 epidemic, video conferencing has evolved as a new
paradigm of communication and teamwork. However, private and personal
information can be easily leaked through cameras during video conferencing.
This includes leakage of a person's appearance as well as the contents in the
background. This paper proposes a novel way of using online low-resolution
thermal images as conditions to guide the synthesis of RGB images, bringing a
promising solution for real-time video conferencing when privacy leakage is a
concern. SPADE-SR (Spatially-Adaptive De-normalization with Self Resampling), a
variant of SPADE, is adopted to incorporate the spatial property of a thermal
heatmap and the non-thermal property of a normal, privacy-free pre-recorded RGB
image provided in a form of latent code. We create a PAIR-LRT-Human (LRT =
Low-Resolution Thermal) dataset to validate our claims. The result enables a
convenient way of video conferencing where users no longer need to groom
themselves and tidy up backgrounds for a short meeting. Additionally, it allows
a user to switch to a different appearance and background during a conference.
</p></li>
</ul>

<h3>Title: WebSHAP: Towards Explaining Any Machine Learning Models Anywhere. (arXiv:2303.09545v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09545">http://arxiv.org/abs/2303.09545</a></li>
<li>Code URL: <a href="https://github.com/poloclub/webshap">https://github.com/poloclub/webshap</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09545] WebSHAP: Towards Explaining Any Machine Learning Models Anywhere](http://arxiv.org/abs/2303.09545) #privacy</code></li>
<li>Summary: <p>As machine learning (ML) is increasingly integrated into our everyday Web
experience, there is a call for transparent and explainable web-based ML.
However, existing explainability techniques often require dedicated backend
servers, which limit their usefulness as the Web community moves toward
in-browser ML for lower latency and greater privacy. To address the pressing
need for a client-side explainability solution, we present WebSHAP, the first
in-browser tool that adapts the state-of-the-art model-agnostic explainability
technique SHAP to the Web environment. Our open-source tool is developed with
modern Web technologies such as WebGL that leverage client-side hardware
capabilities and make it easy to integrate into existing Web ML applications.
We demonstrate WebSHAP in a usage scenario of explaining ML-based loan approval
decisions to loan applicants. Reflecting on our work, we discuss the
opportunities and challenges for future research on transparent Web ML. WebSHAP
is available at https://github.com/poloclub/webshap.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Secret-Keeping in Question Answering. (arXiv:2303.09067v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09067">http://arxiv.org/abs/2303.09067</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09067] Secret-Keeping in Question Answering](http://arxiv.org/abs/2303.09067) #protect</code></li>
<li>Summary: <p>Existing question-answering research focuses on unanswerable questions in the
context of always providing an answer when a system can\dots but what about
cases where a system {\bf should not} answer a question. This can either be to
protect sensitive users or sensitive information. Many models expose sensitive
information under interrogation by an adversarial user. We seek to determine if
it is possible to teach a question-answering system to keep a specific fact
secret. We design and implement a proof-of-concept architecture and through our
evaluation determine that while possible, there are numerous directions for
future research to reduce system paranoia (false positives), information
leakage (false negatives) and extend the implementation of the work to more
complex problems with preserving secrecy in the presence of information
aggregation.
</p></li>
</ul>

<h3>Title: Copyright Protection and Accountability of Generative AI:Attack, Watermarking and Attribution. (arXiv:2303.09272v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09272">http://arxiv.org/abs/2303.09272</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09272] Copyright Protection and Accountability of Generative AI:Attack, Watermarking and Attribution](http://arxiv.org/abs/2303.09272) #protect</code></li>
<li>Summary: <p>Generative AI (e.g., Generative Adversarial Networks - GANs) has become
increasingly popular in recent years. However, Generative AI introduces
significant concerns regarding the protection of Intellectual Property Rights
(IPR) (resp. model accountability) pertaining to images (resp. toxic images)
and models (resp. poisoned models) generated. In this paper, we propose an
evaluation framework to provide a comprehensive overview of the current state
of the copyright protection measures for GANs, evaluate their performance
across a diverse range of GAN architectures, and identify the factors that
affect their performance and future research directions. Our findings indicate
that the current IPR protection methods for input images, model watermarking,
and attribution networks are largely satisfactory for a wide range of GANs. We
highlight that further attention must be directed towards protecting training
sets, as the current approaches fail to provide robust IPR protection and
provenance tracing on training sets.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Certifiable (Multi)Robustness Against Patch Attacks Using ERM. (arXiv:2303.08944v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08944">http://arxiv.org/abs/2303.08944</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08944] Certifiable (Multi)Robustness Against Patch Attacks Using ERM](http://arxiv.org/abs/2303.08944) #attack</code></li>
<li>Summary: <p>Consider patch attacks, where at test-time an adversary manipulates a test
image with a patch in order to induce a targeted misclassification. We consider
a recent defense to patch attacks, Patch-Cleanser (Xiang et al. [2022]). The
Patch-Cleanser algorithm requires a prediction model to have a ``two-mask
correctness'' property, meaning that the prediction model should correctly
classify any image when any two blank masks replace portions of the image.
Xiang et al. learn a prediction model to be robust to two-mask operations by
augmenting the training set with pairs of masks at random locations of training
images and performing empirical risk minimization (ERM) on the augmented
dataset.
</p></li>
</ul>

<p>However, in the non-realizable setting when no predictor is perfectly correct
on all two-mask operations on all images, we exhibit an example where ERM
fails. To overcome this challenge, we propose a different algorithm that
provably learns a predictor robust to all two-mask operations using an ERM
oracle, based on prior work by Feige et al. [2015]. We also extend this result
to a multiple-group setting, where we can learn a predictor that achieves low
robust loss on all groups simultaneously.
</p>

<h3>Title: Rethinking Model Ensemble in Transfer-based Adversarial Attacks. (arXiv:2303.09105v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09105">http://arxiv.org/abs/2303.09105</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09105] Rethinking Model Ensemble in Transfer-based Adversarial Attacks](http://arxiv.org/abs/2303.09105) #attack</code></li>
<li>Summary: <p>Deep learning models are vulnerable to adversarial examples. Transfer-based
adversarial attacks attract tremendous attention as they can identify the
weaknesses of deep learning models in a black-box manner. An effective strategy
to improve the transferability of adversarial examples is attacking an ensemble
of models. However, previous works simply average the outputs of different
models, lacking an in-depth analysis on how and why model ensemble can strongly
improve the transferability. In this work, we rethink the ensemble in
adversarial attacks and define the common weakness of model ensemble with the
properties of the flatness of loss landscape and the closeness to the local
optimum of each model. We empirically and theoretically show that these two
properties are strongly correlated with the transferability and propose a
Common Weakness Attack (CWA) to generate more transferable adversarial examples
by promoting these two properties. Experimental results on both image
classification and object detection tasks validate the effectiveness of our
approach to improve the adversarial transferability, especially when attacking
adversarially trained models.
</p></li>
</ul>

<h3>Title: DeeBBAA: A benchmark Deep Black Box Adversarial Attack against Cyber-Physical Power Systems. (arXiv:2303.09024v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09024">http://arxiv.org/abs/2303.09024</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09024] DeeBBAA: A benchmark Deep Black Box Adversarial Attack against Cyber-Physical Power Systems](http://arxiv.org/abs/2303.09024) #attack</code></li>
<li>Summary: <p>An increased energy demand, and environmental pressure to accommodate higher
levels of renewable energy and flexible loads like electric vehicles have led
to numerous smart transformations in the modern power systems. These
transformations make the cyber-physical power system highly susceptible to
cyber-adversaries targeting its numerous operations. In this work, a novel
black box adversarial attack strategy is proposed targeting the AC state
estimation operation of an unknown power system using historical data.
Specifically, false data is injected into the measurements obtained from a
small subset of the power system components which leads to significant
deviations in the state estimates. Experiments carried out on the IEEE 39 bus
and 118 bus test systems make it evident that the proposed strategy, called
DeeBBAA, can evade numerous conventional and state-of-the-art attack detection
mechanisms with very high probability.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: EvalAttAI: A Holistic Approach to Evaluating Attribution Maps in Robust and Non-Robust Models. (arXiv:2303.08866v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08866">http://arxiv.org/abs/2303.08866</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08866] EvalAttAI: A Holistic Approach to Evaluating Attribution Maps in Robust and Non-Robust Models](http://arxiv.org/abs/2303.08866) #robust</code></li>
<li>Summary: <p>The expansion of explainable artificial intelligence as a field of research
has generated numerous methods of visualizing and understanding the black box
of a machine learning model. Attribution maps are generally used to highlight
the parts of the input image that influence the model to make a specific
decision. On the other hand, the robustness of machine learning models to
natural noise and adversarial attacks is also being actively explored. This
paper focuses on evaluating methods of attribution mapping to find whether
robust neural networks are more explainable. We explore this problem within the
application of classification for medical imaging. Explainability research is
at an impasse. There are many methods of attribution mapping, but no current
consensus on how to evaluate them and determine the ones that are the best. Our
experiments on multiple datasets (natural and medical imaging) and various
attribution methods reveal that two popular evaluation metrics, Deletion and
Insertion, have inherent limitations and yield contradictory results. We
propose a new explainability faithfulness metric (called EvalAttAI) that
addresses the limitations of prior metrics. Using our novel evaluation, we
found that Bayesian deep neural networks using the Variational Density
Propagation technique were consistently more explainable when used with the
best performing attribution method, the Vanilla Gradient. However, in general,
various types of robust neural networks may not be more explainable, despite
these models producing more visually plausible attribution maps.
</p></li>
</ul>

<h3>Title: Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement. (arXiv:2303.08983v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08983">http://arxiv.org/abs/2303.08983</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08983] Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement](http://arxiv.org/abs/2303.08983) #robust</code></li>
<li>Summary: <p>We propose Dataset Reinforcement, a strategy to improve a dataset once such
that the accuracy of any model architecture trained on the reinforced dataset
is improved at no additional training cost for users. We propose a Dataset
Reinforcement strategy based on data augmentation and knowledge distillation.
Our generic strategy is designed based on extensive analysis across CNN- and
transformer-based models and performing large-scale study of distillation with
state-of-the-art models with various data augmentations. We create a reinforced
version of the ImageNet training dataset, called ImageNet+, as well as
reinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained
with ImageNet+ are more accurate, robust, and calibrated, and transfer well to
downstream tasks (e.g., segmentation and detection). As an example, the
accuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on
ImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the
ImageNet validation set is also reduced by 9.9%. Using this backbone with
Mask-RCNN for object detection on MS-COCO, the mean average precision improves
by 0.8%. We reach similar gains for MobileNets, ViTs, and Swin-Transformers.
For MobileNetV3 and Swin-Tiny we observe significant improvements on
ImageNet-R/A/C of up to 10% improved robustness. Models pretrained on ImageNet+
and fine-tuned on CIFAR-100+, Flowers-102+, and Food-101+, reach up to 3.4%
improved accuracy.
</p></li>
</ul>

<h3>Title: Unsupervised Facial Expression Representation Learning with Contrastive Local Warping. (arXiv:2303.09034v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09034">http://arxiv.org/abs/2303.09034</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09034] Unsupervised Facial Expression Representation Learning with Contrastive Local Warping](http://arxiv.org/abs/2303.09034) #robust</code></li>
<li>Summary: <p>This paper investigates unsupervised representation learning for facial
expression analysis. We think Unsupervised Facial Expression Representation
(UFER) deserves exploration and has the potential to address some key
challenges in facial expression analysis, such as scaling, annotation bias, the
discrepancy between discrete labels and continuous emotions, and model
pre-training. Such motivated, we propose a UFER method with contrastive local
warping (ContraWarping), which leverages the insight that the emotional
expression is robust to current global transformation (affine transformation,
color jitter, etc.) but can be easily changed by random local warping.
Therefore, given a facial image, ContraWarping employs some global
transformations and local warping to generate its positive and negative samples
and sets up a novel contrastive learning framework. Our in-depth investigation
shows that: 1) the positive pairs from global transformations may be exploited
with general self-supervised learning (e.g., BYOL) and already bring some
informative features, and 2) the negative pairs from local warping explicitly
introduce expression-related variation and further bring substantial
improvement. Based on ContraWarping, we demonstrate the benefit of UFER under
two facial expression analysis scenarios: facial expression recognition and
image retrieval. For example, directly using ContraWarping features for linear
probing achieves 79.14% accuracy on RAF-DB, significantly reducing the gap
towards the full-supervised counterpart (88.92% / 84.81% with/without
pre-training).
</p></li>
</ul>

<h3>Title: CoLo-CAM: Class Activation Mapping for Object Co-Localization in Weakly-Labeled Unconstrained Videos. (arXiv:2303.09044v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09044">http://arxiv.org/abs/2303.09044</a></li>
<li>Code URL: <a href="https://github.com/sbelharbi/colo-cam">https://github.com/sbelharbi/colo-cam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09044] CoLo-CAM: Class Activation Mapping for Object Co-Localization in Weakly-Labeled Unconstrained Videos](http://arxiv.org/abs/2303.09044) #robust</code></li>
<li>Summary: <p>Weakly-supervised video object localization (WSVOL) methods often rely on
visual and motion cues only, making them susceptible to inaccurate
localization. Recently, discriminative models via a temporal class activation
mapping (CAM) method have been explored. Although results are promising,
objects are assumed to have minimal movement leading to degradation in
performance for relatively long-term dependencies. In this paper, a novel
CoLo-CAM method for object localization is proposed to leverage spatiotemporal
information in activation maps without any assumptions about object movement.
Over a given sequence of frames, explicit joint learning of localization is
produced across these maps based on color cues, by assuming an object has
similar color across frames. The CAMs' activations are constrained to activate
similarly over pixels with similar colors, achieving co-localization. This
joint learning creates direct communication among pixels across all image
locations, and over all frames, allowing for transfer, aggregation, and
correction of learned localization. This is achieved by minimizing a color term
of a CRF loss over joint images/maps. In addition to our multi-frame
constraint, we impose per-frame local constraints including pseudo-labels, and
CRF loss in combination with a global size constraint to improve per-frame
localization. Empirical experiments on two challenging datasets for
unconstrained videos, YouTube-Objects, show the merits of our method, and its
robustness to long-term dependencies, leading to new state-of-the-art
localization performance. Public code: https://github.com/sbelharbi/colo-cam.
</p></li>
</ul>

<h3>Title: Robust Evaluation of Diffusion-Based Adversarial Purification. (arXiv:2303.09051v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09051">http://arxiv.org/abs/2303.09051</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09051] Robust Evaluation of Diffusion-Based Adversarial Purification](http://arxiv.org/abs/2303.09051) #robust</code></li>
<li>Summary: <p>We question the current evaluation practice on diffusion-based purification
methods. Diffusion-based purification methods aim to remove adversarial effects
from an input data point at test time. The approach gains increasing attention
as an alternative to adversarial training due to the disentangling between
training and testing. Well-known white-box attacks are often employed to
measure the robustness of the purification. However, it is unknown whether
these attacks are the most effective for the diffusion-based purification since
the attacks are often tailored for adversarial training. We analyze the current
practices and provide a new guideline for measuring the robustness of
purification methods against adversarial attacks. Based on our analysis, we
further propose a new purification strategy showing competitive results against
the state-of-the-art adversarial training approaches.
</p></li>
</ul>

<h3>Title: FindView: Precise Target View Localization Task for Look Around Agents. (arXiv:2303.09054v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09054">http://arxiv.org/abs/2303.09054</a></li>
<li>Code URL: <a href="https://github.com/haruishi43/look_around">https://github.com/haruishi43/look_around</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09054] FindView: Precise Target View Localization Task for Look Around Agents](http://arxiv.org/abs/2303.09054) #robust</code></li>
<li>Summary: <p>With the increase in demands for service robots and automated inspection,
agents need to localize in its surrounding environment to achieve more natural
communication with humans by shared contexts. In this work, we propose a novel
but straightforward task of precise target view localization for look around
agents called the FindView task. This task imitates the movements of PTZ
cameras or user interfaces for 360 degree mediums, where the observer must
"look around" to find a view that exactly matches the target. To solve this
task, we introduce a rule-based agent that heuristically finds the optimal view
and a policy learning agent that employs reinforcement learning to learn by
interacting with the 360 degree scene. Through extensive evaluations and
benchmarks, we conclude that learned methods have many advantages, in
particular precise localization that is robust to corruption and can be easily
deployed in novel scenes.
</p></li>
</ul>

<h3>Title: Reliable Image Dehazing by NeRF. (arXiv:2303.09153v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09153">http://arxiv.org/abs/2303.09153</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09153] Reliable Image Dehazing by NeRF](http://arxiv.org/abs/2303.09153) #robust</code></li>
<li>Summary: <p>We present an image dehazing algorithm with high quality, wide application,
and no data training or prior needed. We analyze the defects of the original
dehazing model, and propose a new and reliable dehazing reconstruction and
dehazing model based on the combination of optical scattering model and
computer graphics lighting rendering model. Based on the new haze model and the
images obtained by the cameras, we can reconstruct the three-dimensional space,
accurately calculate the objects and haze in the space, and use the
transparency relationship of haze to perform accurate haze removal. To obtain a
3D simulation dataset we used the Unreal 5 computer graphics rendering engine.
In order to obtain real shot data in different scenes, we used fog generators,
array cameras, mobile phones, underwater cameras and drones to obtain haze
data. We use formula derivation, simulation data set and real shot data set
result experimental results to prove the feasibility of the new method.
Compared with various other methods, we are far ahead in terms of calculation
indicators (4 dB higher quality average scene), color remains more natural, and
the algorithm is more robust in different scenarios and best in the subjective
perception.
</p></li>
</ul>

<h3>Title: Global Knowledge Calibration for Fast Open-Vocabulary Segmentation. (arXiv:2303.09181v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09181">http://arxiv.org/abs/2303.09181</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09181] Global Knowledge Calibration for Fast Open-Vocabulary Segmentation](http://arxiv.org/abs/2303.09181) #robust</code></li>
<li>Summary: <p>Recent advancements in pre-trained vision-language models, such as CLIP, have
enabled the segmentation of arbitrary concepts solely from textual inputs, a
process commonly referred to as open-vocabulary semantic segmentation (OVS).
However, existing OVS techniques confront a fundamental challenge: the trained
classifier tends to overfit on the base classes observed during training,
resulting in suboptimal generalization performance to unseen classes. To
mitigate this issue, recent studies have proposed the use of an additional
frozen pre-trained CLIP for classification. Nonetheless, this approach incurs
heavy computational overheads as the CLIP vision encoder must be repeatedly
forward-passed for each mask, rendering it impractical for real-world
applications. To address this challenge, our objective is to develop a fast OVS
model that can perform comparably or better without the extra computational
burden of the CLIP image encoder during inference. To this end, we propose a
core idea of preserving the generalizable representation when fine-tuning on
known classes. Specifically, we introduce a text diversification strategy that
generates a set of synonyms for each training category, which prevents the
learned representation from collapsing onto specific known category names.
Additionally, we employ a text-guided knowledge distillation method to preserve
the generalizable knowledge of CLIP. Extensive experiments demonstrate that our
proposed model achieves robust generalization performance across various
datasets. Furthermore, we perform a preliminary exploration of open-vocabulary
video segmentation and present a benchmark that can facilitate future
open-vocabulary research in the video domain.
</p></li>
</ul>

<h3>Title: MixCycle: Mixup Assisted Semi-Supervised 3D Single Object Tracking with Cycle Consistency. (arXiv:2303.09219v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09219">http://arxiv.org/abs/2303.09219</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09219] MixCycle: Mixup Assisted Semi-Supervised 3D Single Object Tracking with Cycle Consistency](http://arxiv.org/abs/2303.09219) #robust</code></li>
<li>Summary: <p>3D single object tracking (SOT) is an indispensable part of automated
driving. Existing approaches rely heavily on large, densely labeled datasets.
However, annotating point clouds is both costly and time-consuming. Inspired by
the great success of cycle tracking in unsupervised 2D SOT, we introduce the
first semi-supervised approach to 3D SOT. Specifically, we introduce two
cycle-consistency strategies for supervision: 1) Self tracking cycles, which
leverage labels to help the model converge better in the early stages of
training; 2) forward-backward cycles, which strengthen the tracker's robustness
to motion variations and the template noise caused by the template update
strategy. Furthermore, we propose a data augmentation strategy named SOTMixup
to improve the tracker's robustness to point cloud diversity. SOTMixup
generates training samples by sampling points in two point clouds with a mixing
rate and assigns a reasonable loss weight for training according to the mixing
rate. The resulting MixCycle approach generalizes to appearance matching-based
trackers. On the KITTI benchmark, based on the P2B tracker, MixCycle trained
with $\textbf{10%}$ labels outperforms P2B trained with $\textbf{100%}$ labels,
and achieves a $\textbf{28.4%}$ precision improvement when using $\textbf{1%}$
labels. Our code will be publicly released.
</p></li>
</ul>

<h3>Title: 3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive Segmentation of Heterogeneous Infant Brain MRI. (arXiv:2303.09373v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09373">http://arxiv.org/abs/2303.09373</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09373] 3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive Segmentation of Heterogeneous Infant Brain MRI](http://arxiv.org/abs/2303.09373) #robust</code></li>
<li>Summary: <p>Robust segmentation of infant brain MRI across multiple ages, modalities, and
sites remains challenging due to the intrinsic heterogeneity caused by
different MRI scanners, vendors, or acquisition sequences, as well as varying
stages of neurodevelopment. To address this challenge, previous studies have
explored domain adaptation (DA) algorithms from various perspectives, including
feature alignment, entropy minimization, contrast synthesis (style transfer),
and pseudo-labeling. This paper introduces a novel framework called MAPSeg
(Masked Autoencoding and Pseudo-labelling Segmentation) to address the
challenges of cross-age, cross-modality, and cross-site segmentation of
subcortical regions in infant brain MRI. Utilizing 3D masked autoencoding as
well as masked pseudo-labeling, the model is able to jointly learn from labeled
source domain data and unlabeled target domain data. We evaluated our framework
on expert-annotated datasets acquired from different ages and sites. MAPSeg
consistently outperformed other methods, including previous state-of-the-art
supervised baselines, domain generalization, and domain adaptation frameworks
in segmenting subcortical regions regardless of age, modality, or acquisition
site. The code and pretrained encoder will be publicly available at
https://github.com/XuzheZ/MAPSeg
</p></li>
</ul>

<h3>Title: All4One: Symbiotic Neighbour Contrastive Learning via Self-Attention and Redundancy Reduction. (arXiv:2303.09417v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09417">http://arxiv.org/abs/2303.09417</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09417] All4One: Symbiotic Neighbour Contrastive Learning via Self-Attention and Redundancy Reduction](http://arxiv.org/abs/2303.09417) #robust</code></li>
<li>Summary: <p>Nearest neighbour based methods have proved to be one of the most successful
self-supervised learning (SSL) approaches due to their high generalization
capabilities. However, their computational efficiency decreases when more than
one neighbour is used. In this paper, we propose a novel contrastive SSL
approach, which we call All4One, that reduces the distance between neighbour
representations using ''centroids'' created through a self-attention mechanism.
We use a Centroid Contrasting objective along with single Neighbour Contrasting
and Feature Contrasting objectives. Centroids help in learning contextual
information from multiple neighbours whereas the neighbour contrast enables
learning representations directly from the neighbours and the feature contrast
allows learning representations unique to the features. This combination
enables All4One to outperform popular instance discrimination approaches by
more than 1% on linear classification evaluation for popular benchmark datasets
and obtains state-of-the-art (SoTA) results. Finally, we show that All4One is
robust towards embedding dimensionalities and augmentations, surpassing NNCLR
and Barlow Twins by more than 5% on low dimensionality and weak augmentation
settings. The source code would be made available soon.
</p></li>
</ul>

<h3>Title: Logical Implications for Visual Question Answering Consistency. (arXiv:2303.09427v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09427">http://arxiv.org/abs/2303.09427</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09427] Logical Implications for Visual Question Answering Consistency](http://arxiv.org/abs/2303.09427) #robust</code></li>
<li>Summary: <p>Despite considerable recent progress in Visual Question Answering (VQA)
models, inconsistent or contradictory answers continue to cast doubt on their
true reasoning capabilities. However, most proposed methods use indirect
strategies or strong assumptions on pairs of questions and answers to enforce
model consistency. Instead, we propose a novel strategy intended to improve
model performance by directly reducing logical inconsistencies. To do this, we
introduce a new consistency loss term that can be used by a wide range of the
VQA models and which relies on knowing the logical relation between pairs of
questions and answers. While such information is typically not available in VQA
datasets, we propose to infer these logical relations using a dedicated
language model and use these in our proposed consistency loss function. We
conduct extensive experiments on the VQA Introspect and DME datasets and show
that our method brings improvements to state-of-the-art VQA models, while being
robust across different architectures and settings.
</p></li>
</ul>

<h3>Title: Towards Robust Bangla Complex Named Entity Recognition. (arXiv:2303.09306v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09306">http://arxiv.org/abs/2303.09306</a></li>
<li>Code URL: <a href="https://github.com/ramisa2108/bangla-complex-named-entity-recognition-challenge">https://github.com/ramisa2108/bangla-complex-named-entity-recognition-challenge</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09306] Towards Robust Bangla Complex Named Entity Recognition](http://arxiv.org/abs/2303.09306) #robust</code></li>
<li>Summary: <p>Named Entity Recognition (NER) is a fundamental task in natural language
processing that involves identifying and classifying named entities in text.
But much work hasn't been done for complex named entity recognition in Bangla,
despite being the seventh most spoken language globally. CNER is a more
challenging task than traditional NER as it involves identifying and
classifying complex and compound entities, which are not common in Bangla
language. In this paper, we present the winning solution of Bangla Complex
Named Entity Recognition Challenge - addressing the CNER task on BanglaCoNER
dataset using two different approaches, namely Conditional Random Fields (CRF)
and finetuning transformer based Deep Learning models such as BanglaBERT.
</p></li>
</ul>

<p>The dataset consisted of 15300 sentences for training and 800 sentences for
validation, in the .conll format. Exploratory Data Analysis (EDA) on the
dataset revealed that the dataset had 7 different NER tags, with notable
presence of English words, suggesting that the dataset is synthetic and likely
a product of translation.
</p>
<p>We experimented with a variety of feature combinations including Part of
Speech (POS) tags, word suffixes, Gazetteers, and cluster information from
embeddings, while also finetuning the BanglaBERT (large) model for NER. We
found that not all linguistic patterns are immediately apparent or even
intuitive to humans, which is why Deep Learning based models has proved to be
the more effective model in NLP, including CNER task. Our fine tuned BanglaBERT
(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our
study highlights the importance of Bangla Complex Named Entity Recognition,
particularly in the context of synthetic datasets. Our findings also
demonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in
Bangla language.
</p>

<h3>Title: Machine Learning for Flow Cytometry Data Analysis. (arXiv:2303.09007v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09007">http://arxiv.org/abs/2303.09007</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09007] Machine Learning for Flow Cytometry Data Analysis](http://arxiv.org/abs/2303.09007) #robust</code></li>
<li>Summary: <p>Flow cytometry mainly used for detecting the characteristics of a number of
biochemical substances based on the expression of specific markers in cells. It
is particularly useful for detecting membrane surface receptors, antigens,
ions, or during DNA/RNA expression. Not only can it be employed as a biomedical
research tool for recognising distinctive types of cells in mixed populations,
but it can also be used as a diagnostic tool for classifying abnormal cell
populations connected with disease. Modern flow cytometers can rapidly analyse
tens of thousands of cells at the same time while also measuring multiple
parameters from a single cell. However, the rapid development of flow
cytometers makes it challenging for conventional analysis methods to interpret
flow cytometry data. Researchers need to be able to distinguish
interesting-looking cell populations manually in multi-dimensional data
collected from millions of cells. Thus, it is essential to find a robust
approach for analysing flow cytometry data automatically, specifically in
identifying cell populations automatically. This thesis mainly concerns
discover the potential shortcoming of current automated-gating algorithms in
both real datasets and synthetic datasets. Three representative automated
clustering algorithms are selected to be applied, compared and evaluated by
completely and partially automated gating. A subspace clustering ProClus also
implemented in this thesis. The performance of ProClus in flow cytometry is not
well, but it is still a useful algorithm to detect noise.
</p></li>
</ul>

<h3>Title: Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling. (arXiv:2303.09033v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09033">http://arxiv.org/abs/2303.09033</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09033] Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling](http://arxiv.org/abs/2303.09033) #robust</code></li>
<li>Summary: <p>Most bandit algorithms assume that the reward variance or its upper bound is
known. While variance overestimation is usually safe and sound, it increases
regret. On the other hand, an underestimated variance may lead to linear regret
due to committing early to a suboptimal arm. This motivated prior works on
variance-aware frequentist algorithms. We lay foundations for the Bayesian
setting. In particular, we study multi-armed bandits with known and
\emph{unknown heterogeneous reward variances}, and develop Thompson sampling
algorithms for both and bound their Bayes regret. Our regret bounds decrease
with lower reward variances, which make learning easier. The bound for unknown
reward variances captures the effect of the prior on learning reward variances
and is the first of its kind. Our experiments show the superiority of
variance-aware Bayesian algorithms and also highlight their robustness.
</p></li>
</ul>

<h3>Title: Evaluation of distance-based approaches for forensic comparison: Application to hand odor evidence. (arXiv:2303.09126v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09126">http://arxiv.org/abs/2303.09126</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09126] Evaluation of distance-based approaches for forensic comparison: Application to hand odor evidence](http://arxiv.org/abs/2303.09126) #robust</code></li>
<li>Summary: <p>The issue of distinguishing between the same-source and different-source
hypotheses based on various types of traces is a generic problem in forensic
science. This problem is often tackled with Bayesian approaches, which are able
to provide a likelihood ratio that quantifies the relative strengths of
evidence supporting each of the two competing hypotheses. Here, we focus on
distance-based approaches, whose robustness and specifically whose capacity to
deal with high-dimensional evidence are very different, and need to be
evaluated and optimized. A unified framework for direct methods based on
estimating the likelihoods of the distance between traces under each of the two
competing hypotheses, and indirect methods using logistic regression to
discriminate between same-source and different-source distance distributions,
is presented. Whilst direct methods are more flexible, indirect methods are
more robust and quite natural in machine learning. Moreover, indirect methods
also enable the use of a vectorial distance, thus preventing the severe
information loss suffered by scalar distance approaches.Direct and indirect
methods are compared in terms of sensitivity, specificity and robustness, with
and without dimensionality reduction, with and without feature selection, on
the example of hand odor profiles, a novel and challenging type of evidence in
the field of forensics. Empirical evaluations on a large panel of 534 subjects
and their 1690 odor traces show the significant superiority of the indirect
methods, especially without dimensionality reduction, be it with or without
feature selection.
</p></li>
</ul>

<h3>Title: Adaptive Modeling of Uncertainties for Traffic Forecasting. (arXiv:2303.09273v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09273">http://arxiv.org/abs/2303.09273</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09273] Adaptive Modeling of Uncertainties for Traffic Forecasting](http://arxiv.org/abs/2303.09273) #robust</code></li>
<li>Summary: <p>Deep neural networks (DNNs) have emerged as a dominant approach for
developing traffic forecasting models. These models are typically trained to
minimize error on averaged test cases and produce a single-point prediction,
such as a scalar value for traffic speed or travel time. However, single-point
predictions fail to account for prediction uncertainty that is critical for
many transportation management scenarios, such as determining the best- or
worst-case arrival time. We present QuanTraffic, a generic framework to enhance
the capability of an arbitrary DNN model for uncertainty modeling. QuanTraffic
requires little human involvement and does not change the base DNN architecture
during deployment. Instead, it automatically learns a standard quantile
function during the DNN model training to produce a prediction interval for the
single-point prediction. The prediction interval defines a range where the true
value of the traffic prediction is likely to fall. Furthermore, QuanTraffic
develops an adaptive scheme that dynamically adjusts the prediction interval
based on the location and prediction window of the test input. We evaluated
QuanTraffic by applying it to five representative DNN models for traffic
forecasting across seven public datasets. We then compared QuanTraffic against
five uncertainty quantification methods. Compared to the baseline uncertainty
modeling techniques, QuanTraffic with base DNN architectures delivers
consistently better and more robust performance than the existing ones on the
reported datasets.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: A novel dual skip connection mechanism in U-Nets for building footprint extraction. (arXiv:2303.09064v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09064">http://arxiv.org/abs/2303.09064</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09064] A novel dual skip connection mechanism in U-Nets for building footprint extraction](http://arxiv.org/abs/2303.09064) #extraction</code></li>
<li>Summary: <p>The importance of building footprints and their inventory has been recognised
as an enabler for multiple societal problems. Extracting urban building
footprint is complex and requires semantic segmentation of very high-resolution
(VHR) earth observation (EO) images. U-Net is a common deep learning
architecture for such segmentation. It has seen several re-incarnation
including U-Net++ and U-Net3+ with a focus on multi-scale feature aggregation
with re-designed skip connections. However, the exploitation of multi-scale
information is still evolving. In this paper, we propose a dual skip connection
mechanism (DSCM) for U-Net and a dual full-scale skip connection mechanism
(DFSCM) for U-Net3+. The DSCM in U-Net doubles the features in the encoder and
passes them to the decoder for precise localisation. Similarly, the DFSCM
incorporates increased low-level context information with high-level semantics
from feature maps in different scales. The DSCM is further tested in ResUnet
and different scales of U-Net. The proposed mechanisms, therefore, produce
several novel networks that are evaluated in a benchmark WHU building dataset
and a multi-resolution dataset that we develop for the City of Melbourne. The
results on the benchmark dataset demonstrate 17.7% and 18.4% gain in F1 score
and Intersection over Union (IoU) compared to the state-of-the-art vanilla
U-Net3+. In the same experimental setup, DSCM on U-Net and ResUnet provides a
gain in five accuracy measures against the original networks. The codes will be
available in a GitHub link after peer review.
</p></li>
</ul>

<h3>Title: Visual-Linguistic Causal Intervention for Radiology Report Generation. (arXiv:2303.09117v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09117">http://arxiv.org/abs/2303.09117</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09117] Visual-Linguistic Causal Intervention for Radiology Report Generation](http://arxiv.org/abs/2303.09117) #extraction</code></li>
<li>Summary: <p>Automatic radiology report generation is essential for computer-aided
diagnosis and medication guidance. Importantly, automatic radiology report
generation (RRG) can relieve the heavy burden of radiologists by generating
medical reports automatically from visual-linguistic data relations. However,
due to the spurious correlations within image-text data induced by visual and
linguistic biases, it is challenging to generate accurate reports that reliably
describe abnormalities. Besides, the cross-modal confounder is usually
unobservable and difficult to be eliminated explicitly. In this paper, we
mitigate the cross-modal data bias for RRG from a new perspective, i.e.,
visual-linguistic causal intervention, and propose a novel Visual-Linguistic
Causal Intervention (VLCI) framework for RRG, which consists of a visual
deconfounding module (VDM) and a linguistic deconfounding module (LDM), to
implicitly deconfound the visual-linguistic confounder by causal front-door
intervention. Specifically, the VDM explores and disentangles the visual
confounder from the patch-based local and global features without object
detection due to the absence of universal clinic semantic extraction.
Simultaneously, the LDM eliminates the linguistic confounder caused by salient
visual features and high-frequency context without constructing specific
dictionaries. Extensive experiments on IU-Xray and MIMIC-CXR datasets show that
our VLCI outperforms the state-of-the-art RRG methods significantly. Source
code and models are available at https://github.com/WissingChen/VLCI.
</p></li>
</ul>

<h3>Title: EmotiEffNet Facial Features in Uni-task Emotion Recognition in Video at ABAW-5 competition. (arXiv:2303.09162v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09162">http://arxiv.org/abs/2303.09162</a></li>
<li>Code URL: <a href="https://github.com/HSE-asavchenko/face-emotion-recognition">https://github.com/HSE-asavchenko/face-emotion-recognition</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09162] EmotiEffNet Facial Features in Uni-task Emotion Recognition in Video at ABAW-5 competition](http://arxiv.org/abs/2303.09162) #extraction</code></li>
<li>Summary: <p>In this article, the results of our team for the fifth Affective Behavior
Analysis in-the-wild (ABAW) competition are presented. The usage of the
pre-trained convolutional networks from the EmotiEffNet family for frame-level
feature extraction is studied. In particular, we propose an ensemble of a
multi-layered perceptron and the LightAutoML-based classifier. The
post-processing by smoothing the results for sequential frames is implemented.
Experimental results for the large-scale Aff-Wild2 database demonstrate that
our model achieves a much greater macro-averaged F1-score for facial expression
recognition and action unit detection and concordance correlation coefficients
for valence/arousal estimation when compared to baseline.
</p></li>
</ul>

<h3>Title: Multimodal Feature Extraction and Fusion for Emotional Reaction Intensity Estimation and Expression Classification in Videos with Transformers. (arXiv:2303.09164v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09164">http://arxiv.org/abs/2303.09164</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09164] Multimodal Feature Extraction and Fusion for Emotional Reaction Intensity Estimation and Expression Classification in Videos with Transformers](http://arxiv.org/abs/2303.09164) #extraction</code></li>
<li>Summary: <p>In this paper, we present our solutions to the two sub-challenges of
Affective Behavior Analysis in the wild (ABAW) 2023: the Emotional Reaction
Intensity (ERI) Estimation Challenge and Expression (Expr) Classification
Challenge. ABAW 2023 focuses on the problem of affective behavior analysis in
the wild, with the goal of creating machines and robots that have the ability
to understand human feelings, emotions and behaviors, which can effectively
contribute to the advent of a more intelligent future. In our work, we use
different models and tools for the Hume-Reaction dataset to extract features of
various aspects, such as audio features, video features, etc. By analyzing,
combining, and studying these multimodal features, we effectively improve the
accuracy of the model for multimodal sentiment prediction. For the Emotional
Reaction Intensity (ERI) Estimation Challenge, our method shows excellent
results with a Pearson coefficient on the validation dataset, exceeding the
baseline method by 84 percent.
</p></li>
</ul>

<h3>Title: Grab What You Need: Rethinking Complex Table Structure Recognition with Flexible Components Deliberation. (arXiv:2303.09174v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09174">http://arxiv.org/abs/2303.09174</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09174] Grab What You Need: Rethinking Complex Table Structure Recognition with Flexible Components Deliberation](http://arxiv.org/abs/2303.09174) #extraction</code></li>
<li>Summary: <p>Recently, Table Structure Recognition (TSR) task, aiming at identifying table
structure into machine readable formats, has received increasing interest in
the community. While impressive success, most single table component-based
methods can not perform well on unregularized table cases distracted by not
only complicated inner structure but also exterior capture distortion. In this
paper, we raise it as Complex TSR problem, where the performance degeneration
of existing methods is attributable to their inefficient component usage and
redundant post-processing. To mitigate it, we shift our perspective from table
component extraction towards the efficient multiple components leverage, which
awaits further exploration in the field. Specifically, we propose a seminal
method, termed GrabTab, equipped with newly proposed Component Deliberator.
Thanks to its progressive deliberation mechanism, our GrabTab can flexibly
accommodate to most complex tables with reasonable components selected but
without complicated post-processing involved. Quantitative experimental results
on public benchmarks demonstrate that our method significantly outperforms the
state-of-the-arts, especially under more challenging scenes.
</p></li>
</ul>

<h3>Title: NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes. (arXiv:2303.09431v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09431">http://arxiv.org/abs/2303.09431</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09431] NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes](http://arxiv.org/abs/2303.09431) #extraction</code></li>
<li>Summary: <p>With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis
has recently made a big leap forward. At the core, NeRF proposes that each 3D
point can emit radiance, allowing to conduct view synthesis using
differentiable volumetric rendering. While neural radiance fields can
accurately represent 3D scenes for computing the image rendering, 3D meshes are
still the main scene representation supported by most computer graphics and
simulation pipelines, enabling tasks such as real time rendering and
physics-based simulations. Obtaining 3D meshes from neural radiance fields
still remains an open challenge since NeRFs are optimized for view synthesis,
not enforcing an accurate underlying geometry on the radiance field. We thus
propose a novel compact and flexible architecture that enables easy 3D surface
reconstruction from any NeRF-driven approach. Upon having trained the radiance
field, we distill the volumetric 3D representation into a Signed Surface
Approximation Network, allowing easy extraction of the 3D mesh and appearance.
Our final 3D mesh is physically accurate and can be rendered in real time on an
array of devices.
</p></li>
</ul>

<h3>Title: Applying unsupervised keyphrase methods on concepts extracted from discharge sheets. (arXiv:2303.08928v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08928">http://arxiv.org/abs/2303.08928</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08928] Applying unsupervised keyphrase methods on concepts extracted from discharge sheets](http://arxiv.org/abs/2303.08928) #extraction</code></li>
<li>Summary: <p>Clinical notes containing valuable patient information are written by
different health care providers with various scientific levels and writing
styles. It might be helpful for clinicians and researchers to understand what
information is essential when dealing with extensive electronic medical
records. Entities recognizing and mapping them to standard terminologies is
crucial in reducing ambiguity in processing clinical notes. Although named
entity recognition and entity linking are critical steps in clinical natural
language processing, they can also result in the production of repetitive and
low-value concepts. In other hand, all parts of a clinical text do not share
the same importance or content in predicting the patient's condition. As a
result, it is necessary to identify the section in which each content is
recorded and also to identify key concepts to extract meaning from clinical
texts. In this study, these challenges have been addressed by using clinical
natural language processing techniques. In addition, in order to identify key
concepts, a set of popular unsupervised key phrase extraction methods has been
verified and evaluated. Considering that most of the clinical concepts are in
the form of multi-word expressions and their accurate identification requires
the user to specify n-gram range, we have proposed a shortcut method to
preserve the structure of the expression based on TF-IDF. In order to evaluate
the pre-processing method and select the concepts, we have designed two types
of downstream tasks (multiple and binary classification) using the capabilities
of transformer-based models. The obtained results show the superiority of
proposed method in combination with SciBERT model, also offer an insight into
the efficacy of general extracting essential phrase methods for clinical notes.
</p></li>
</ul>

<h3>Title: GLEN: General-Purpose Event Detection for Thousands of Types. (arXiv:2303.09093v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09093">http://arxiv.org/abs/2303.09093</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09093] GLEN: General-Purpose Event Detection for Thousands of Types](http://arxiv.org/abs/2303.09093) #extraction</code></li>
<li>Summary: <p>The development of event extraction systems has been hindered by the absence
of wide-coverage, large-scale datasets. To make event extraction systems more
accessible, we build a general-purpose event detection dataset GLEN, which
covers 3,465 different event types, making it over 20x larger in ontology than
any current dataset. GLEN is created by utilizing the DWD Overlay, which
provides a mapping between Wikidata Qnodes and PropBank rolesets. This enables
us to use the abundant existing annotation for PropBank as distant supervision.
In addition, we also propose a new multi-stage event detection model
specifically designed to handle the large ontology size and partial labels in
GLEN. We show that our model exhibits superior performance (~10% F1 gain)
compared to both conventional classification baselines and newer
definition-based models. Finally, we perform error analysis and show that label
noise is still the largest challenge for improving performance.
</p></li>
</ul>

<h3>Title: The Scope of In-Context Learning for the Extraction of Medical Temporal Constraints. (arXiv:2303.09366v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09366">http://arxiv.org/abs/2303.09366</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09366] The Scope of In-Context Learning for the Extraction of Medical Temporal Constraints](http://arxiv.org/abs/2303.09366) #extraction</code></li>
<li>Summary: <p>Medications often impose temporal constraints on everyday patient activity.
Violations of such medical temporal constraints (MTCs) lead to a lack of
treatment adherence, in addition to poor health outcomes and increased
healthcare expenses. These MTCs are found in drug usage guidelines (DUGs) in
both patient education materials and clinical texts. Computationally
representing MTCs in DUGs will advance patient-centric healthcare applications
by helping to define safe patient activity patterns. We define a novel taxonomy
of MTCs found in DUGs and develop a novel context-free grammar (CFG) based
model to computationally represent MTCs from unstructured DUGs. Additionally,
we release three new datasets with a combined total of N = 836 DUGs labeled
with normalized MTCs. We develop an in-context learning (ICL) solution for
automatically extracting and normalizing MTCs found in DUGs, achieving an
average F1 score of 0.62 across all datasets. Finally, we rigorously
investigate ICL model performance against a baseline model, across datasets and
MTC types, and through in-depth error analysis.
</p></li>
</ul>

<h3>Title: A Multimodal Data-driven Framework for Anxiety Screening. (arXiv:2303.09041v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09041">http://arxiv.org/abs/2303.09041</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09041] A Multimodal Data-driven Framework for Anxiety Screening](http://arxiv.org/abs/2303.09041) #extraction</code></li>
<li>Summary: <p>Early screening for anxiety and appropriate interventions are essential to
reduce the incidence of self-harm and suicide in patients. Due to limited
medical resources, traditional methods that overly rely on physician expertise
and specialized equipment cannot simultaneously meet the needs for high
accuracy and model interpretability. Multimodal data can provide more objective
evidence for anxiety screening to improve the accuracy of models. The large
amount of noise in multimodal data and the unbalanced nature of the data make
the model prone to overfitting. However, it is a non-differentiable problem
when high-dimensional and multimodal feature combinations are used as model
inputs and incorporated into model training. This causes existing anxiety
screening methods based on machine learning and deep learning to be
inapplicable. Therefore, we propose a multimodal data-driven anxiety screening
framework, namely MMD-AS, and conduct experiments on the collected health data
of over 200 seafarers by smartphones. The proposed framework's feature
extraction, dimension reduction, feature selection, and anxiety inference are
jointly trained to improve the model's performance. In the feature selection
step, a feature selection method based on the Improved Fireworks Algorithm is
used to solve the non-differentiable problem of feature combination to remove
redundant features and search for the ideal feature subset. The experimental
results show that our framework outperforms the comparison methods.
</p></li>
</ul>

<h3>Title: Gate Recurrent Unit Network based on Hilbert-Schmidt Independence Criterion for State-of-Health Estimation. (arXiv:2303.09497v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09497">http://arxiv.org/abs/2303.09497</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09497] Gate Recurrent Unit Network based on Hilbert-Schmidt Independence Criterion for State-of-Health Estimation](http://arxiv.org/abs/2303.09497) #extraction</code></li>
<li>Summary: <p>State-of-health (SOH) estimation is a key step in ensuring the safe and
reliable operation of batteries. Due to issues such as varying data
distribution and sequence length in different cycles, most existing methods
require health feature extraction technique, which can be time-consuming and
labor-intensive. GRU can well solve this problem due to the simple structure
and superior performance, receiving widespread attentions. However, redundant
information still exists within the network and impacts the accuracy of SOH
estimation. To address this issue, a new GRU network based on Hilbert-Schmidt
Independence Criterion (GRU-HSIC) is proposed. First, a zero masking network is
used to transform all battery data measured with varying lengths every cycle
into sequences of the same length, while still retaining information about the
original data size in each cycle. Second, the Hilbert-Schmidt Independence
Criterion (HSIC) bottleneck, which evolved from Information Bottleneck (IB)
theory, is extended to GRU to compress the information from hidden layers. To
evaluate the proposed method, we conducted experiments on datasets from the
Center for Advanced Life Cycle Engineering (CALCE) of the University of
Maryland and NASA Ames Prognostics Center of Excellence. Experimental results
demonstrate that our model achieves higher accuracy than other recurrent
models.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: GLASU: A Communication-Efficient Algorithm for Federated Learning with Vertically Distributed Graph Data. (arXiv:2303.09531v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09531">http://arxiv.org/abs/2303.09531</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09531] GLASU: A Communication-Efficient Algorithm for Federated Learning with Vertically Distributed Graph Data](http://arxiv.org/abs/2303.09531) #federate</code></li>
<li>Summary: <p>Vertical federated learning (VFL) is a distributed learning paradigm, where
computing clients collectively train a model based on the partial features of
the same set of samples they possess. Current research on VFL focuses on the
case when samples are independent, but it rarely addresses an emerging scenario
when samples are interrelated through a graph. For graph-structured data, graph
neural networks (GNNs) are competitive machine learning models, but a naive
implementation in the VFL setting causes a significant communication overhead.
Moreover, the analysis of the training is faced with a challenge caused by the
biased stochastic gradients. In this paper, we propose a model splitting method
that splits a backbone GNN across the clients and the server and a
communication-efficient algorithm, GLASU, to train such a model. GLASU adopts
lazy aggregation and stale updates to skip aggregation when evaluating the
model and skip feature exchanges during training, greatly reducing
communication. We offer a theoretical analysis and conduct extensive numerical
experiments on real-world datasets, showing that the proposed algorithm
effectively trains a GNN model, whose performance matches that of the backbone
GNN when trained in a centralized manner.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Large Selective Kernel Network for Remote Sensing Object Detection. (arXiv:2303.09030v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09030">http://arxiv.org/abs/2303.09030</a></li>
<li>Code URL: <a href="https://github.com/zcablii/Large-Selective-Kernel-Network">https://github.com/zcablii/Large-Selective-Kernel-Network</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09030] Large Selective Kernel Network for Remote Sensing Object Detection](http://arxiv.org/abs/2303.09030) #fair</code></li>
<li>Summary: <p>Recent research on remote sensing object detection has largely focused on
improving the representation of oriented bounding boxes but has overlooked the
unique prior knowledge presented in remote sensing scenarios. Such prior
knowledge can be useful because tiny remote sensing objects may be mistakenly
detected without referencing a sufficiently long-range context, and the
long-range context required by different types of objects can vary. In this
paper, we take these priors into account and propose the Large Selective Kernel
Network (LSKNet). LSKNet can dynamically adjust its large spatial receptive
field to better model the ranging context of various objects in remote sensing
scenarios. To the best of our knowledge, this is the first time that large and
selective kernel mechanisms have been explored in the field of remote sensing
object detection. Without bells and whistles, LSKNet sets new state-of-the-art
scores on standard benchmarks, i.e., HRSC2016 (98.46\% mAP), DOTA-v1.0 (81.64\%
mAP) and FAIR1M-v1.0 (47.87\% mAP). Based on a similar technique, we rank 2nd
place in 2022 the Greater Bay Area International Algorithm Competition. Code is
available at https://github.com/zcablii/Large-Selective-Kernel-Network.
</p></li>
</ul>

<h3>Title: The NCI Imaging Data Commons as a platform for reproducible research in computational pathology. (arXiv:2303.09354v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09354">http://arxiv.org/abs/2303.09354</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09354] The NCI Imaging Data Commons as a platform for reproducible research in computational pathology](http://arxiv.org/abs/2303.09354) #fair</code></li>
<li>Summary: <p>Objective: Reproducibility is critical for translating machine learning-based
(ML) solutions in computational pathology (CompPath) into practice. However, an
increasing number of studies report difficulties in reproducing ML results. The
NCI Imaging Data Commons (IDC) is a public repository of >120 cancer image
collections, including >38,000 whole-slide images (WSIs), that is designed to
be used with cloud-based ML services. Here, we explore the potential of the IDC
to facilitate reproducibility of CompPath research.
</p></li>
</ul>

<p>Materials and Methods: The IDC realizes the FAIR principles: All images are
encoded according to the DICOM standard, persistently identified, discoverable
via rich metadata, and accessible via open tools. Taking advantage of this, we
implemented two experiments in which a representative ML-based method for
classifying lung tumor tissue was trained and/or evaluated on different
datasets from the IDC. To assess reproducibility, the experiments were run
multiple times with independent but identically configured sessions of common
ML services.
</p>
<p>Results: The AUC values of different runs of the same experiment were
generally consistent and in the same order of magnitude as a similar,
previously published study. However, there were occasional small variations in
AUC values of up to 0.044, indicating a practical limit to reproducibility.
</p>
<p>Discussion and conclusion: By realizing the FAIR principles, the IDC enables
other researchers to reuse exactly the same datasets. Cloud-based ML services
enable others to run CompPath experiments in an identically configured
computing environment without having to own high-performance hardware. The
combination of both makes it possible to approach the reproducibility limit.
</p>

<h3>Title: Tollywood Emotions: Annotation of Valence-Arousal in Telugu Song Lyrics. (arXiv:2303.09364v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09364">http://arxiv.org/abs/2303.09364</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09364] Tollywood Emotions: Annotation of Valence-Arousal in Telugu Song Lyrics](http://arxiv.org/abs/2303.09364) #fair</code></li>
<li>Summary: <p>Emotion recognition from a given music track has heavily relied on acoustic
features, social tags, and metadata but is seldom focused on lyrics. There are
no datasets of Indian language songs that contain both valence and arousal
manual ratings of lyrics. We present a new manually annotated dataset of Telugu
songs' lyrics collected from Spotify with valence and arousal annotated on a
discrete scale. A fairly high inter-annotator agreement was observed for both
valence and arousal. Subsequently, we create two music emotion recognition
models by using two classification techniques to identify valence, arousal and
respective emotion quadrant from lyrics. Support vector machine (SVM) with term
frequency-inverse document frequency (TF-IDF) features and fine-tuning the
pre-trained XLMRoBERTa (XLM-R) model were used for valence, arousal and
quadrant classification tasks. Fine-tuned XLMRoBERTa performs better than the
SVM by improving macro-averaged F1-scores of 54.69%, 67.61%, 34.13% to 77.90%,
80.71% and 58.33% for valence, arousal and quadrant classifications,
respectively, on 10-fold cross-validation. In addition, we compare our lyrics
annotations with Spotify's annotations of valence and energy (same as arousal),
which are based on entire music tracks. The implications of our findings are
discussed. Finally, we make the dataset publicly available with lyrics,
annotations and Spotify IDs.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: NAISR: A 3D Neural Additive Model for Interpretable Shape Representation. (arXiv:2303.09234v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09234">http://arxiv.org/abs/2303.09234</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09234] NAISR: A 3D Neural Additive Model for Interpretable Shape Representation](http://arxiv.org/abs/2303.09234) #interpretability</code></li>
<li>Summary: <p>Deep implicit functions (DIFs) have emerged as a powerful paradigm for many
computer vision tasks such as 3D shape reconstruction, generation,
registration, completion, editing, and understanding. However, given a set of
3D shapes with associated covariates there is at present no shape
representation method which allows to precisely represent the shapes while
capturing the individual dependencies on each covariate. Such a method would be
of high utility to researchers to discover knowledge hidden in a population of
shapes. We propose a 3D Neural Additive Model for Interpretable Shape
Representation (NAISR) which describes individual shapes by deforming a shape
atlas in accordance to the effect of disentangled covariates. Our approach
captures shape population trends and allows for patient-specific predictions
through shape transfer. NAISR is the first approach to combine the benefits of
deep implicit shape representations with an atlas deforming according to
specified covariates. Although our driving problem is the construction of an
airway atlas, NAISR is a general approach for modeling, representing, and
investigating shape populations. We evaluate NAISR with respect to shape
reconstruction, shape disentanglement, shape evolution, and shape transfer for
the pediatric upper airway. Our experiments demonstrate that NAISR achieves
competitive shape reconstruction performance while retaining interpretability.
</p></li>
</ul>

<h3>Title: TOT: Topology-Aware Optimal Transport For Multimodal Hate Detection. (arXiv:2303.09314v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09314">http://arxiv.org/abs/2303.09314</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09314] TOT: Topology-Aware Optimal Transport For Multimodal Hate Detection](http://arxiv.org/abs/2303.09314) #interpretability</code></li>
<li>Summary: <p>Multimodal hate detection, which aims to identify harmful content online such
as memes, is crucial for building a wholesome internet environment. Previous
work has made enlightening exploration in detecting explicit hate remarks.
However, most of their approaches neglect the analysis of implicit harm, which
is particularly challenging as explicit text markers and demographic visual
cues are often twisted or missing. The leveraged cross-modal attention
mechanisms also suffer from the distributional modality gap and lack logical
interpretability. To address these semantic gaps issues, we propose TOT: a
topology-aware optimal transport framework to decipher the implicit harm in
memes scenario, which formulates the cross-modal aligning problem as solutions
for optimal transportation plans. Specifically, we leverage an optimal
transport kernel method to capture complementary information from multiple
modalities. The kernel embedding provides a non-linear transformation ability
to reproduce a kernel Hilbert space (RKHS), which reflects significance for
eliminating the distributional modality gap. Moreover, we perceive the topology
information based on aligned representations to conduct bipartite graph path
reasoning. The newly achieved state-of-the-art performance on two publicly
available benchmark datasets, together with further visual analysis,
demonstrate the superiority of TOT in capturing implicit cross-modal alignment.
</p></li>
</ul>

<h3>Title: ToxVis: Enabling Interpretability of Implicit vs. Explicit Toxicity Detection Models with Interactive Visualization. (arXiv:2303.09402v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09402">http://arxiv.org/abs/2303.09402</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09402] ToxVis: Enabling Interpretability of Implicit vs](http://arxiv.org/abs/2303.09402) #interpretability</code></li>
<li>Summary: <p>The rise of hate speech on online platforms has led to an urgent need for
effective content moderation. However, the subjective and multi-faceted nature
of hateful online content, including implicit hate speech, poses significant
challenges to human moderators and content moderation systems. To address this
issue, we developed ToxVis, a visually interactive and explainable tool for
classifying hate speech into three categories: implicit, explicit, and
non-hateful. We fine-tuned two transformer-based models using RoBERTa, XLNET,
and GPT-3 and used deep learning interpretation techniques to provide
explanations for the classification results. ToxVis enables users to input
potentially hateful text and receive a classification result along with a
visual explanation of which words contributed most to the decision. By making
the classification process explainable, ToxVis provides a valuable tool for
understanding the nuances of hateful content and supporting more effective
content moderation. Our research contributes to the growing body of work aimed
at mitigating the harms caused by online hate speech and demonstrates the
potential for combining state-of-the-art natural language processing models
with interpretable deep learning techniques to address this critical issue.
Finally, ToxVis can serve as a resource for content moderators, social media
platforms, and researchers working to combat the spread of hate speech online.
</p></li>
</ul>

<h3>Title: Preoperative Prognosis Assessment of Lumbar Spinal Surgery for Low Back Pain and Sciatica Patients based on Multimodalities and Multimodal Learning. (arXiv:2303.09085v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09085">http://arxiv.org/abs/2303.09085</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09085] Preoperative Prognosis Assessment of Lumbar Spinal Surgery for Low Back Pain and Sciatica Patients based on Multimodalities and Multimodal Learning](http://arxiv.org/abs/2303.09085) #interpretability</code></li>
<li>Summary: <p>Low back pain (LBP) and sciatica may require surgical therapy when they are
symptomatic of severe pain. However, there is no effective measures to evaluate
the surgical outcomes in advance. This work combined elements of Eastern
medicine and machine learning, and developed a preoperative assessment tool to
predict the prognosis of lumbar spinal surgery in LBP and sciatica patients.
Standard operative assessments, traditional Chinese medicine body constitution
assessments, planned surgical approach, and vowel pronunciation recordings were
collected and stored in different modalities. Our work provides insights into
leveraging modality combinations, multimodals, and fusion strategies. The
interpretability of models and correlations between modalities were also
inspected. Based on the recruited 105 patients, we found that combining
standard operative assessments, body constitution assessments, and planned
surgical approach achieved the best performance in 0.81 accuracy. Our approach
is effective and can be widely applied in general practice due to simplicity
and effective.
</p></li>
</ul>

<h3>Title: Interpretability from a new lens: Integrating Stratification and Domain knowledge for Biomedical Applications. (arXiv:2303.09322v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09322">http://arxiv.org/abs/2303.09322</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09322] Interpretability from a new lens: Integrating Stratification and Domain knowledge for Biomedical Applications](http://arxiv.org/abs/2303.09322) #interpretability</code></li>
<li>Summary: <p>The use of machine learning (ML) techniques in the biomedical field has
become increasingly important, particularly with the large amounts of data
generated by the aftermath of the COVID-19 pandemic. However, due to the
complex nature of biomedical datasets and the use of black-box ML models, a
lack of trust and adoption by domain experts can arise. In response,
interpretable ML (IML) approaches have been developed, but the curse of
dimensionality in biomedical datasets can lead to model instability. This paper
proposes a novel computational strategy for the stratification of biomedical
problem datasets into k-fold cross-validation (CVs) and integrating domain
knowledge interpretation techniques embedded into the current state-of-the-art
IML frameworks. This approach can improve model stability, establish trust, and
provide explanations for outcomes generated by trained IML models.
Specifically, the model outcome, such as aggregated feature weight importance,
can be linked to further domain knowledge interpretations using techniques like
pathway functional enrichment, drug targeting, and repurposing databases.
Additionally, involving end-users and clinicians in focus group discussions
before and after the choice of IML framework can help guide testable
hypotheses, improve performance metrics, and build trustworthy and usable IML
solutions in the biomedical field. Overall, this study highlights the potential
of combining advanced computational techniques with domain knowledge
interpretation to enhance the effectiveness of IML solutions in the context of
complex biomedical datasets.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels. (arXiv:2303.08863v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08863">http://arxiv.org/abs/2303.08863</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08863] Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels](http://arxiv.org/abs/2303.08863) #diffusion</code></li>
<li>Summary: <p>Image-to-image reconstruction problems with free or inexpensive metadata in
the form of class labels appear often in biological and medical image domains.
Existing text-guided or style-transfer image-to-image approaches do not
translate to datasets where additional information is provided as discrete
classes. We introduce and implement a model which combines image-to-image and
class-guided denoising diffusion probabilistic models. We train our model on a
real-world dataset of microscopy images used for drug discovery, with and
without incorporating metadata labels. By exploring the properties of
image-to-image diffusion with relevant labels, we show that class-guided
image-to-image diffusion can improve the meaningful content of the
reconstructed images and outperform the unguided model in useful downstream
tasks.
</p></li>
</ul>

<h3>Title: Stochastic Segmentation with Conditional Categorical Diffusion Models. (arXiv:2303.08888v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08888">http://arxiv.org/abs/2303.08888</a></li>
<li>Code URL: <a href="https://github.com/larsdoorenbos/ccdm-stochastic-segmentation">https://github.com/larsdoorenbos/ccdm-stochastic-segmentation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08888] Stochastic Segmentation with Conditional Categorical Diffusion Models](http://arxiv.org/abs/2303.08888) #diffusion</code></li>
<li>Summary: <p>Semantic segmentation has made significant progress in recent years thanks to
deep neural networks, but the common objective of generating a single
segmentation output that accurately matches the image's content may not be
suitable for safety-critical domains such as medical diagnostics and autonomous
driving. Instead, multiple possible correct segmentation maps may be required
to reflect the true distribution of annotation maps. In this context,
stochastic semantic segmentation methods must learn to predict conditional
distributions of labels given the image, but this is challenging due to the
typically multimodal distributions, high-dimensional output spaces, and limited
annotation data. To address these challenges, we propose a conditional
categorical diffusion model (CCDM) for semantic segmentation based on Denoising
Diffusion Probabilistic Models. Our model is conditioned to the input image,
enabling it to generate multiple segmentation label maps that account for the
aleatoric uncertainty arising from divergent ground truth annotations. Our
experimental results show that CCDM achieves state-of-the-art performance on
LIDC, a stochastic semantic segmentation dataset, and outperforms established
baselines on the classical segmentation dataset Cityscapes.
</p></li>
</ul>

<h3>Title: Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation. (arXiv:2303.09119v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09119">http://arxiv.org/abs/2303.09119</a></li>
<li>Code URL: <a href="https://github.com/advocate99/diffgesture">https://github.com/advocate99/diffgesture</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09119] Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation](http://arxiv.org/abs/2303.09119) #diffusion</code></li>
<li>Summary: <p>Animating virtual avatars to make co-speech gestures facilitates various
applications in human-machine interaction. The existing methods mainly rely on
generative adversarial networks (GANs), which typically suffer from notorious
mode collapse and unstable training, thus making it difficult to learn accurate
audio-gesture joint distributions. In this work, we propose a novel
diffusion-based framework, named Diffusion Co-Speech Gesture (DiffGesture), to
effectively capture the cross-modal audio-to-gesture associations and preserve
temporal coherence for high-fidelity audio-driven co-speech gesture generation.
Specifically, we first establish the diffusion-conditional generation process
on clips of skeleton sequences and audio to enable the whole framework. Then, a
novel Diffusion Audio-Gesture Transformer is devised to better attend to the
information from multiple modalities and model the long-term temporal
dependency. Moreover, to eliminate temporal inconsistency, we propose an
effective Diffusion Gesture Stabilizer with an annealed noise sampling
strategy. Benefiting from the architectural advantages of diffusion models, we
further incorporate implicit classifier-free guidance to trade off between
diversity and gesture quality. Extensive experiments demonstrate that
DiffGesture achieves state-of-theart performance, which renders coherent
gestures with better mode coverage and stronger audio correlations. Code is
available at https://github.com/Advocate99/DiffGesture.
</p></li>
</ul>

<h3>Title: Fiber Tract Shape Measures Inform Prediction of Non-Imaging Phenotypes. (arXiv:2303.09124v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09124">http://arxiv.org/abs/2303.09124</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09124] Fiber Tract Shape Measures Inform Prediction of Non-Imaging Phenotypes](http://arxiv.org/abs/2303.09124) #diffusion</code></li>
<li>Summary: <p>Neuroimaging measures of the brain's white matter connections can enable the
prediction of non-imaging phenotypes, such as demographic and cognitive
measures. Existing works have investigated traditional microstructure and
connectivity measures from diffusion MRI tractography, without considering the
shape of the connections reconstructed by tractography. In this paper, we
investigate the potential of fiber tract shape features for predicting
non-imaging phenotypes, both individually and in combination with traditional
features. We focus on three basic shape features: length, diameter, and
elongation. Two different prediction methods are used, including a traditional
regression method and a deep-learning-based prediction method. Experiments use
an efficient two-stage fusion strategy for prediction using microstructure,
connectivity, and shape measures. To reduce predictive bias due to brain size,
normalized shape features are also investigated. Experimental results on the
Human Connectome Project (HCP) young adult dataset (n=1065) demonstrate that
individual shape features are predictive of non-imaging phenotypes. When
combined with microstructure and connectivity features, shape features
significantly improve performance for predicting the cognitive score TPVT (NIH
Toolbox picture vocabulary test). Overall, this study demonstrates that the
shape of fiber tracts contains useful information for the description and study
of the living human brain using machine learning.
</p></li>
</ul>

<h3>Title: DIRE for Diffusion-Generated Image Detection. (arXiv:2303.09295v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09295">http://arxiv.org/abs/2303.09295</a></li>
<li>Code URL: <a href="https://github.com/zhendongwang6/dire">https://github.com/zhendongwang6/dire</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09295] DIRE for Diffusion-Generated Image Detection](http://arxiv.org/abs/2303.09295) #diffusion</code></li>
<li>Summary: <p>Diffusion models have shown remarkable success in visual synthesis, but have
also raised concerns about potential abuse for malicious purposes. In this
paper, we seek to build a detector for telling apart real images from
diffusion-generated images. We find that existing detectors struggle to detect
images generated by diffusion models, even if we include generated images from
a specific diffusion model in their training data. To address this issue, we
propose a novel image representation called DIffusion Reconstruction Error
(DIRE), which measures the error between an input image and its reconstruction
counterpart by a pre-trained diffusion model. We observe that
diffusion-generated images can be approximately reconstructed by a diffusion
model while real images cannot. It provides a hint that DIRE can serve as a
bridge to distinguish generated and real images. DIRE provides an effective way
to detect images generated by most diffusion models, and it is general for
detecting generated images from unseen diffusion models and robust to various
perturbations. Furthermore, we establish a comprehensive diffusion-generated
benchmark including images generated by eight diffusion models to evaluate the
performance of diffusion-generated image detectors. Extensive experiments on
our collected benchmark demonstrate that DIRE exhibits superiority over
previous generated-image detectors. The code and dataset are available at
https://github.com/ZhendongWang6/DIRE.
</p></li>
</ul>

<h3>Title: Unified Multi-Modal Latent Diffusion for Joint Subject and Text Conditional Image Generation. (arXiv:2303.09319v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09319">http://arxiv.org/abs/2303.09319</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09319] Unified Multi-Modal Latent Diffusion for Joint Subject and Text Conditional Image Generation](http://arxiv.org/abs/2303.09319) #diffusion</code></li>
<li>Summary: <p>Language-guided image generation has achieved great success nowadays by using
diffusion models. However, texts can be less detailed to describe
highly-specific subjects such as a particular dog or a certain car, which makes
pure text-to-image generation not accurate enough to satisfy user requirements.
In this work, we present a novel Unified Multi-Modal Latent Diffusion
(UMM-Diffusion) which takes joint texts and images containing specified
subjects as input sequences and generates customized images with the subjects.
To be more specific, both input texts and images are encoded into one unified
multi-modal latent space, in which the input images are learned to be projected
to pseudo word embedding and can be further combined with text to guide image
generation. Besides, to eliminate the irrelevant parts of the input images such
as background or illumination, we propose a novel sampling technique of
diffusion models used by the image generator which fuses the results guided by
multi-modal input and pure text input. By leveraging the large-scale
pre-trained text-to-image generator and the designed image encoder, our method
is able to generate high-quality images with complex semantics from both
aspects of input texts and images.
</p></li>
</ul>

<h3>Title: DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars. (arXiv:2303.09375v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09375">http://arxiv.org/abs/2303.09375</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09375] DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars](http://arxiv.org/abs/2303.09375) #diffusion</code></li>
<li>Summary: <p>We present DINAR, an approach for creating realistic rigged fullbody avatars
from single RGB images. Similarly to previous works, our method uses neural
textures combined with the SMPL-X body model to achieve photo-realistic quality
of avatars while keeping them easy to animate and fast to infer. To restore the
texture, we use a latent diffusion model and show how such model can be trained
in the neural texture space. The use of the diffusion model allows us to
realistically reconstruct large unseen regions such as the back of a person
given the frontal view. The models in our pipeline are trained using 2D images
and videos only. In the experiments, our approach achieves state-of-the-art
rendering quality and good generalization to new poses and viewpoints. In
particular, the approach improves state-of-the-art on the SnapshotPeople public
benchmark.
</p></li>
</ul>

<h3>Title: DiffIR: Efficient Diffusion Model for Image Restoration. (arXiv:2303.09472v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09472">http://arxiv.org/abs/2303.09472</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09472] DiffIR: Efficient Diffusion Model for Image Restoration](http://arxiv.org/abs/2303.09472) #diffusion</code></li>
<li>Summary: <p>Diffusion model (DM) has achieved SOTA performance by modeling the image
synthesis process into a sequential application of a denoising network.
However, different from image synthesis generating each pixel from scratch,
most pixels of image restoration (IR) are given. Thus, for IR, traditional DMs
running massive iterations on a large model to estimate whole images or feature
maps is inefficient. To address this issue, we propose an efficient DM for IR
(DiffIR), which consists of a compact IR prior extraction network (CPEN),
dynamic IR transformer (DIRformer), and denoising network. Specifically, DiffIR
has two training stages: pretraining and training DM. In pretraining, we input
ground-truth images into CPEN$<em>{S1}$ to capture a compact IR prior
representation (IPR) to guide DIRformer. In the second stage, we train the DM
to directly estimate the same IRP as pretrained CPEN$</em>{S1}$ only using LQ
images. We observe that since the IPR is only a compact vector, DiffIR can use
fewer iterations than traditional DM to obtain accurate estimations and
generate more stable and realistic results. Since the iterations are few, our
DiffIR can adopt a joint optimization of CPEN$_{S2}$, DIRformer, and denoising
network, which can further reduce the estimation error influence. We conduct
extensive experiments on several IR tasks and achieve SOTA performance while
consuming less computational costs.
</p></li>
</ul>

<h3>Title: $P+$: Extended Textual Conditioning in Text-to-Image Generation. (arXiv:2303.09522v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09522">http://arxiv.org/abs/2303.09522</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09522] $P+$: Extended Textual Conditioning in Text-to-Image Generation](http://arxiv.org/abs/2303.09522) #diffusion</code></li>
<li>Summary: <p>We introduce an Extended Textual Conditioning space in text-to-image models,
referred to as $P+$. This space consists of multiple textual conditions,
derived from per-layer prompts, each corresponding to a layer of the denoising
U-net of the diffusion model.
</p></li>
</ul>

<p>We show that the extended space provides greater disentangling and control
over image synthesis. We further introduce Extended Textual Inversion (XTI),
where the images are inverted into $P+$, and represented by per-layer tokens.
</p>
<p>We show that XTI is more expressive and precise, and converges faster than
the original Textual Inversion (TI) space. The extended inversion method does
not involve any noticeable trade-off between reconstruction and editability and
induces more regular inversions.
</p>
<p>We conduct a series of extensive experiments to analyze and understand the
properties of the new space, and to showcase the effectiveness of our method
for personalizing text-to-image models. Furthermore, we utilize the unique
properties of this space to achieve previously unattainable results in
object-style mixing using text-to-image models. Project page:
https://prompt-plus.github.io
</p>

<h3>Title: FateZero: Fusing Attentions for Zero-shot Text-based Video Editing. (arXiv:2303.09535v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09535">http://arxiv.org/abs/2303.09535</a></li>
<li>Code URL: <a href="https://github.com/chenyangqiqi/fatezero">https://github.com/chenyangqiqi/fatezero</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09535] FateZero: Fusing Attentions for Zero-shot Text-based Video Editing](http://arxiv.org/abs/2303.09535) #diffusion</code></li>
<li>Summary: <p>The diffusion-based generative models have achieved remarkable success in
text-based image generation. However, since it contains enormous randomness in
generation progress, it is still challenging to apply such models for
real-world visual content editing, especially in videos. In this paper, we
propose FateZero, a zero-shot text-based editing method on real-world videos
without per-prompt training or use-specific mask. To edit videos consistently,
we propose several techniques based on the pre-trained models. Firstly, in
contrast to the straightforward DDIM inversion technique, our approach captures
intermediate attention maps during inversion, which effectively retain both
structural and motion information. These maps are directly fused in the editing
process rather than generated during denoising. To further minimize semantic
leakage of the source video, we then fuse self-attentions with a blending mask
obtained by cross-attention features from the source prompt. Furthermore, we
have implemented a reform of the self-attention mechanism in denoising UNet by
introducing spatial-temporal attention to ensure frame consistency. Yet
succinct, our method is the first one to show the ability of zero-shot
text-driven video style and local attribute editing from the trained
text-to-image model. We also have a better zero-shot shape-aware editing
ability based on the text-to-video model. Extensive experiments demonstrate our
superior temporal consistency and editing capability than previous works.
</p></li>
</ul>

<h3>Title: Diffusion-HPC: Generating Synthetic Images with Realistic Humans. (arXiv:2303.09541v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09541">http://arxiv.org/abs/2303.09541</a></li>
<li>Code URL: <a href="https://github.com/zzweng/diffusion_hpc">https://github.com/zzweng/diffusion_hpc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09541] Diffusion-HPC: Generating Synthetic Images with Realistic Humans](http://arxiv.org/abs/2303.09541) #diffusion</code></li>
<li>Summary: <p>Recent text-to-image generative models have exhibited remarkable abilities in
generating high-fidelity and photo-realistic images. However, despite the
visually impressive results, these models often struggle to preserve plausible
human structure in the generations. Due to this reason, while generative models
have shown promising results in aiding downstream image recognition tasks by
generating large volumes of synthetic data, they remain infeasible for
improving downstream human pose perception and understanding. In this work, we
propose Diffusion model with Human Pose Correction (Diffusion HPC), a
text-conditioned method that generates photo-realistic images with plausible
posed humans by injecting prior knowledge about human body structure. We show
that Diffusion HPC effectively improves the realism of human generations.
Furthermore, as the generations are accompanied by 3D meshes that serve as
ground truths, Diffusion HPC's generated image-mesh pairs are well-suited for
downstream human mesh recovery task, where a shortage of 3D training data has
long been an issue.
</p></li>
</ul>

<h3>Title: Efficient Diffusion Training via Min-SNR Weighting Strategy. (arXiv:2303.09556v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.09556">http://arxiv.org/abs/2303.09556</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.09556] Efficient Diffusion Training via Min-SNR Weighting Strategy](http://arxiv.org/abs/2303.09556) #diffusion</code></li>
<li>Summary: <p>Denoising diffusion models have been a mainstream approach for image
generation, however, training these models often suffers from slow convergence.
In this paper, we discovered that the slow convergence is partly due to
conflicting optimization directions between timesteps. To address this issue,
we treat the diffusion training as a multi-task learning problem, and introduce
a simple yet effective approach referred to as Min-SNR-$\gamma$. This method
adapts loss weights of timesteps based on clamped signal-to-noise ratios, which
effectively balances the conflicts among timesteps. Our results demonstrate a
significant improvement in converging speed, 3.4$\times$ faster than previous
weighting strategies. It is also more effective, achieving a new record FID
score of 2.06 on the ImageNet $256\times256$ benchmark using smaller
architectures than that employed in previous state-of-the-art.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
