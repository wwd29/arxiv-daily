<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: A Survey on Secure and Private Federated Learning Using Blockchain: Theory and Application in Resource-constrained Computing. (arXiv:2303.13727v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13727">http://arxiv.org/abs/2303.13727</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13727] A Survey on Secure and Private Federated Learning Using Blockchain: Theory and Application in Resource-constrained Computing](http://arxiv.org/abs/2303.13727) #secure</code></li>
<li>Summary: <p>Federated Learning (FL) has gained widespread popularity in recent years due
to the fast booming of advanced machine learning and artificial intelligence
along with emerging security and privacy threats. FL enables efficient model
generation from local data storage of the edge devices without revealing the
sensitive data to any entities. While this paradigm partly mitigates the
privacy issues of users' sensitive data, the performance of the FL process can
be threatened and reached a bottleneck due to the growing cyber threats and
privacy violation techniques. To expedite the proliferation of FL process, the
integration of blockchain for FL environments has drawn prolific attention from
the people of academia and industry. Blockchain has the potential to prevent
security and privacy threats with its decentralization, immutability,
consensus, and transparency characteristic. However, if the blockchain
mechanism requires costly computational resources, then the
resource-constrained FL clients cannot be involved in the training. Considering
that, this survey focuses on reviewing the challenges, solutions, and future
directions for the successful deployment of blockchain in resource-constrained
FL environments. We comprehensively review variant blockchain mechanisms that
are suitable for FL process and discuss their trade-offs for a limited resource
budget. Further, we extensively analyze the cyber threats that could be
observed in a resource-constrained FL environment, and how blockchain can play
a key role to block those cyber attacks. To this end, we highlight some
potential solutions towards the coupling of blockchain and federated learning
that can offer high levels of reliability, data privacy, and distributed
computing performance.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Scamming the Scammers: Using ChatGPT to Reply Mails for Wasting Time and Resources. (arXiv:2303.13521v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13521">http://arxiv.org/abs/2303.13521</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13521] Scamming the Scammers: Using ChatGPT to Reply Mails for Wasting Time and Resources](http://arxiv.org/abs/2303.13521) #security</code></li>
<li>Summary: <p>The use of Artificial Intelligence (AI) to support cybersecurity operations
is now a consolidated practice, e.g., to detect malicious code or configure
traffic filtering policies. The recent surge of AI, generative techniques and
frameworks with efficient natural language processing capabilities dramatically
magnifies the number of possible applications aimed at increasing the security
of the Internet. Specifically, the ability of ChatGPT to produce textual
contents while mimicking realistic human interactions can be used to mitigate
the plague of emails containing scams. Therefore, this paper investigates the
use of AI to engage scammers in automatized and pointless communications, with
the goal of wasting both their time and resources. Preliminary results showcase
that ChatGPT is able to decoy scammers, thus confirming that AI is an effective
tool to counteract threats delivered via mail. In addition, we highlight the
multitude of implications and open research questions to be addressed in the
perspective of the ubiquitous adoption of AI.
</p></li>
</ul>

<h3>Title: Physical Backdoor Trigger Activation of Autonomous Vehicle using Reachability Analysis. (arXiv:2303.13992v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13992">http://arxiv.org/abs/2303.13992</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13992] Physical Backdoor Trigger Activation of Autonomous Vehicle using Reachability Analysis](http://arxiv.org/abs/2303.13992) #security</code></li>
<li>Summary: <p>Recent studies reveal that Autonomous Vehicles (AVs) can be manipulated by
hidden backdoors, causing them to perform harmful actions when activated by
physical triggers. However, it is still unclear how these triggers can be
activated while adhering to traffic principles. Understanding this
vulnerability in a dynamic traffic environment is crucial. This work addresses
this gap by presenting physical trigger activation as a reachability problem of
controlled dynamic system. Our technique identifies security-critical areas in
traffic systems where trigger conditions for accidents can be reached, and
provides intended trajectories for how those conditions can be reached. Testing
on typical traffic scenarios showed the system can be successfully driven to
trigger conditions with near 100% activation rate. Our method benefits from
identifying AV vulnerability and enabling effective safety strategies.
</p></li>
</ul>

<h3>Title: PoisonedGNN: Backdoor Attack on Graph Neural Networks-based Hardware Security Systems. (arXiv:2303.14009v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14009">http://arxiv.org/abs/2303.14009</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14009] PoisonedGNN: Backdoor Attack on Graph Neural Networks-based Hardware Security Systems](http://arxiv.org/abs/2303.14009) #security</code></li>
<li>Summary: <p>Graph neural networks (GNNs) have shown great success in detecting
intellectual property (IP) piracy and hardware Trojans (HTs). However, the
machine learning community has demonstrated that GNNs are susceptible to data
poisoning attacks, which result in GNNs performing abnormally on graphs with
pre-defined backdoor triggers (realized using crafted subgraphs). Thus, it is
imperative to ensure that the adoption of GNNs should not introduce security
vulnerabilities in critical security frameworks.
</p></li>
</ul>

<p>Existing backdoor attacks on GNNs generate random subgraphs with specific
sizes/densities to act as backdoor triggers. However, for Boolean circuits,
backdoor triggers cannot be randomized since the added structures should not
affect the functionality of a design.
</p>
<p>We explore this threat and develop PoisonedGNN as the first backdoor attack
on GNNs in the context of hardware design. We design and inject backdoor
triggers into the register-transfer- or the gate-level representation of a
given design without affecting the functionality to evade some GNN-based
detection procedures. To demonstrate the effectiveness of PoisonedGNN, we
consider two case studies: (i) Hiding HTs and (ii) IP piracy. Our experiments
on TrustHub datasets demonstrate that PoisonedGNN can hide HTs and IP piracy
from advanced GNN-based detection platforms with an attack success rate of up
to 100%.
</p>

<h3>Title: Interpretable Anomaly Detection via Discrete Optimization. (arXiv:2303.14111v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14111">http://arxiv.org/abs/2303.14111</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14111] Interpretable Anomaly Detection via Discrete Optimization](http://arxiv.org/abs/2303.14111) #security</code></li>
<li>Summary: <p>Anomaly detection is essential in many application domains, such as cyber
security, law enforcement, medicine, and fraud protection. However, the
decision-making of current deep learning approaches is notoriously hard to
understand, which often limits their practical applicability. To overcome this
limitation, we propose a framework for learning inherently interpretable
anomaly detectors from sequential data. More specifically, we consider the task
of learning a deterministic finite automaton (DFA) from a given multi-set of
unlabeled sequences. We show that this problem is computationally hard and
develop two learning algorithms based on constraint optimization. Moreover, we
introduce novel regularization schemes for our optimization problems that
improve the overall interpretability of our DFAs. Using a prototype
implementation, we demonstrate that our approach shows promising results in
terms of accuracy and F1 score.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Primer: Fast Private Transformer Inference on Encrypted Data. (arXiv:2303.13679v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13679">http://arxiv.org/abs/2303.13679</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13679] Primer: Fast Private Transformer Inference on Encrypted Data](http://arxiv.org/abs/2303.13679) #privacy</code></li>
<li>Summary: <p>It is increasingly important to enable privacy-preserving inference for cloud
services based on Transformers. Post-quantum cryptographic techniques, e.g.,
fully homomorphic encryption (FHE), and multi-party computation (MPC), are
popular methods to support private Transformer inference. However, existing
works still suffer from prohibitively computational and communicational
overhead. In this work, we present, Primer, to enable a fast and accurate
Transformer over encrypted data for natural language processing tasks. In
particular, Primer is constructed by a hybrid cryptographic protocol optimized
for attention-based Transformer models, as well as techniques including
computation merge and tokens-first ciphertext packing. Comprehensive
experiments on encrypted language modeling show that Primer achieves
state-of-the-art accuracy and reduces the inference latency by 90.6% ~ 97.5%
over previous methods.
</p></li>
</ul>

<h3>Title: Differentially Private Synthetic Control. (arXiv:2303.14084v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14084">http://arxiv.org/abs/2303.14084</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14084] Differentially Private Synthetic Control](http://arxiv.org/abs/2303.14084) #privacy</code></li>
<li>Summary: <p>Synthetic control is a causal inference tool used to estimate the treatment
effects of an intervention by creating synthetic counterfactual data. This
approach combines measurements from other similar observations (i.e., donor
pool ) to predict a counterfactual time series of interest (i.e., target unit)
by analyzing the relationship between the target and the donor pool before the
intervention. As synthetic control tools are increasingly applied to sensitive
or proprietary data, formal privacy protections are often required. In this
work, we provide the first algorithms for differentially private synthetic
control with explicit error bounds. Our approach builds upon tools from
non-private synthetic control and differentially private empirical risk
minimization. We provide upper and lower bounds on the sensitivity of the
synthetic control query and provide explicit error bounds on the accuracy of
our private synthetic control algorithms. We show that our algorithms produce
accurate predictions for the target unit, and that the cost of privacy is
small. Finally, we empirically evaluate the performance of our algorithm, and
show favorable performance in a variety of parameter regimes, as well as
providing guidance to practitioners for hyperparameter tuning.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: PIAT: Parameter Interpolation based Adversarial Training for Image Classification. (arXiv:2303.13955v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13955">http://arxiv.org/abs/2303.13955</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13955] PIAT: Parameter Interpolation based Adversarial Training for Image Classification](http://arxiv.org/abs/2303.13955) #defense</code></li>
<li>Summary: <p>Adversarial training has been demonstrated to be the most effective approach
to defend against adversarial attacks. However, existing adversarial training
methods show apparent oscillations and overfitting issue in the training
process, degrading the defense efficacy. In this work, we propose a novel
framework, termed Parameter Interpolation based Adversarial Training (PIAT),
that makes full use of the historical information during training.
Specifically, at the end of each epoch, PIAT tunes the model parameters as the
interpolation of the parameters of the previous and current epochs. Besides, we
suggest to use the Normalized Mean Square Error (NMSE) to further improve the
robustness by aligning the clean and adversarial examples. Compared with other
regularization methods, NMSE focuses more on the relative magnitude of the
logits rather than the absolute magnitude. Extensive experiments on several
benchmark datasets and various networks show that our method could prominently
improve the model robustness and reduce the generalization error. Moreover, our
framework is general and could further boost the robust accuracy when combined
with other adversarial training methods.
</p></li>
</ul>

<h3>Title: Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing. (arXiv:2303.14077v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14077">http://arxiv.org/abs/2303.14077</a></li>
<li>Code URL: <a href="https://github.com/treelli/instance-adaptive-smoothness-enhanced-at">https://github.com/treelli/instance-adaptive-smoothness-enhanced-at</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14077] Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing](http://arxiv.org/abs/2303.14077) #defense</code></li>
<li>Summary: <p>Deep neural networks can be easily fooled into making incorrect predictions
through corruption of the input by adversarial perturbations:
human-imperceptible artificial noise. So far adversarial training has been the
most successful defense against such adversarial attacks. This work focuses on
improving adversarial training to boost adversarial robustness. We first
analyze, from an instance-wise perspective, how adversarial vulnerability
evolves during adversarial training. We find that during training an overall
reduction of adversarial loss is achieved by sacrificing a considerable
proportion of training samples to be more vulnerable to adversarial attack,
which results in an uneven distribution of adversarial vulnerability among
data. Such "uneven vulnerability", is prevalent across several popular robust
training methods and, more importantly, relates to overfitting in adversarial
training. Motivated by this observation, we propose a new adversarial training
method: Instance-adaptive Smoothness Enhanced Adversarial Training (ISEAT). It
jointly smooths both input and weight loss landscapes in an adaptive,
instance-specific, way to enhance robustness more for those samples with higher
adversarial vulnerability. Extensive experiments demonstrate the superiority of
our method over existing defense methods. Noticeably, our method, when combined
with the latest data augmentation and semi-supervised learning techniques,
achieves state-of-the-art robustness against $\ell_{\infty}$-norm constrained
attacks on CIFAR10 of 59.32% for Wide ResNet34-10 without extra data, and
61.55% for Wide ResNet28-10 with extra data. Code is available at
https://github.com/TreeLLi/Instance-adaptive-Smoothness-Enhanced-AT.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Physically Adversarial Infrared Patches with Learnable Shapes and Locations. (arXiv:2303.13868v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13868">http://arxiv.org/abs/2303.13868</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13868] Physically Adversarial Infrared Patches with Learnable Shapes and Locations](http://arxiv.org/abs/2303.13868) #attack</code></li>
<li>Summary: <p>Owing to the extensive application of infrared object detectors in the
safety-critical tasks, it is necessary to evaluate their robustness against
adversarial examples in the real world. However, current few physical infrared
attacks are complicated to implement in practical application because of their
complex transformation from digital world to physical world. To address this
issue, in this paper, we propose a physically feasible infrared attack method
called "adversarial infrared patches". Considering the imaging mechanism of
infrared cameras by capturing objects' thermal radiation, adversarial infrared
patches conduct attacks by attaching a patch of thermal insulation materials on
the target object to manipulate its thermal distribution. To enhance
adversarial attacks, we present a novel aggregation regularization to guide the
simultaneous learning for the patch' shape and location on the target object.
Thus, a simple gradient-based optimization can be adapted to solve for them. We
verify adversarial infrared patches in different object detection tasks with
various object detectors. Experimental results show that our method achieves
more than 90\% Attack Success Rate (ASR) versus the pedestrian detector and
vehicle detector in the physical environment, where the objects are captured in
different angles, distances, postures, and scenes. More importantly,
adversarial infrared patch is easy to implement, and it only needs 0.5 hours to
be constructed in the physical world, which verifies its effectiveness and
efficiency.
</p></li>
</ul>

<h3>Title: Effective black box adversarial attack with handcrafted kernels. (arXiv:2303.13887v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13887">http://arxiv.org/abs/2303.13887</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13887] Effective black box adversarial attack with handcrafted kernels](http://arxiv.org/abs/2303.13887) #attack</code></li>
<li>Summary: <p>We propose a new, simple framework for crafting adversarial examples for
black box attacks. The idea is to simulate the substitution model with a
non-trainable model compounded of just one layer of handcrafted convolutional
kernels and then train the generator neural network to maximize the distance of
the outputs for the original and generated adversarial image. We show that
fooling the prediction of the first layer causes the whole network to be fooled
and decreases its accuracy on adversarial inputs. Moreover, we do not train the
neural network to obtain the first convolutional layer kernels, but we create
them using the technique of F-transform. Therefore, our method is very time and
resource effective.
</p></li>
</ul>

<h3>Title: Vulnerability of Face Morphing Attacks: A Case Study on Lookalike and Identical Twins. (arXiv:2303.14004v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14004">http://arxiv.org/abs/2303.14004</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14004] Vulnerability of Face Morphing Attacks: A Case Study on Lookalike and Identical Twins](http://arxiv.org/abs/2303.14004) #attack</code></li>
<li>Summary: <p>Face morphing attacks have emerged as a potential threat, particularly in
automatic border control scenarios. Morphing attacks permit more than one
individual to use travel documents that can be used to cross borders using
automatic border control gates. The potential for morphing attacks depends on
the selection of data subjects (accomplice and malicious actors). This work
investigates lookalike and identical twins as the source of face morphing
generation. We present a systematic study on benchmarking the vulnerability of
Face Recognition Systems (FRS) to lookalike and identical twin morphing images.
Therefore, we constructed new face morphing datasets using 16 pairs of
identical twin and lookalike data subjects. Morphing images from lookalike and
identical twins are generated using a landmark-based method. Extensive
experiments are carried out to benchmark the attack potential of lookalike and
identical twins. Furthermore, experiments are designed to provide insights into
the impact of vulnerability with normal face morphing compared with lookalike
and identical twin face morphing.
</p></li>
</ul>

<h3>Title: How many dimensions are required to find an adversarial example?. (arXiv:2303.14173v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14173">http://arxiv.org/abs/2303.14173</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14173] How many dimensions are required to find an adversarial example?](http://arxiv.org/abs/2303.14173) #attack</code></li>
<li>Summary: <p>Past work exploring adversarial vulnerability have focused on situations
where an adversary can perturb all dimensions of model input. On the other
hand, a range of recent works consider the case where either (i) an adversary
can perturb a limited number of input parameters or (ii) a subset of modalities
in a multimodal problem. In both of these cases, adversarial examples are
effectively constrained to a subspace $V$ in the ambient input space
$\mathcal{X}$. Motivated by this, in this work we investigate how adversarial
vulnerability depends on $\dim(V)$. In particular, we show that the adversarial
success of standard PGD attacks with $\ell^p$ norm constraints behaves like a
monotonically increasing function of $\epsilon (\frac{\dim(V)}{\dim
\mathcal{X}})^{\frac{1}{q}}$ where $\epsilon$ is the perturbation budget and
$\frac{1}{p} + \frac{1}{q} =1$, provided $p > 1$ (the case $p=1$ presents
additional subtleties which we analyze in some detail). This functional form
can be easily derived from a simple toy linear model, and as such our results
land further credence to arguments that adversarial examples are endemic to
locally linear models on high dimensional spaces.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: MoGDE: Boosting Mobile Monocular 3D Object Detection with Ground Depth Estimation. (arXiv:2303.13561v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13561">http://arxiv.org/abs/2303.13561</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13561] MoGDE: Boosting Mobile Monocular 3D Object Detection with Ground Depth Estimation](http://arxiv.org/abs/2303.13561) #robust</code></li>
<li>Summary: <p>Monocular 3D object detection (Mono3D) in mobile settings (e.g., on a
vehicle, a drone, or a robot) is an important yet challenging task. Due to the
near-far disparity phenomenon of monocular vision and the ever-changing camera
pose, it is hard to acquire high detection accuracy, especially for far
objects. Inspired by the insight that the depth of an object can be well
determined according to the depth of the ground where it stands, in this paper,
we propose a novel Mono3D framework, called MoGDE, which constantly estimates
the corresponding ground depth of an image and then utilizes the estimated
ground depth information to guide Mono3D. To this end, we utilize a pose
detection network to estimate the pose of the camera and then construct a
feature map portraying pixel-level ground depth according to the 3D-to-2D
perspective geometry. Moreover, to improve Mono3D with the estimated ground
depth, we design an RGB-D feature fusion network based on the transformer
structure, where the long-range self-attention mechanism is utilized to
effectively identify ground-contacting points and pin the corresponding ground
depth to the image feature map. We conduct extensive experiments on the
real-world KITTI dataset. The results demonstrate that MoGDE can effectively
improve the Mono3D accuracy and robustness for both near and far objects. MoGDE
yields the best performance compared with the state-of-the-art methods by a
large margin and is ranked number one on the KITTI 3D benchmark.
</p></li>
</ul>

<h3>Title: NOPE: Novel Object Pose Estimation from a Single Image. (arXiv:2303.13612v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13612">http://arxiv.org/abs/2303.13612</a></li>
<li>Code URL: <a href="https://github.com/nv-nguyen/nope">https://github.com/nv-nguyen/nope</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13612] NOPE: Novel Object Pose Estimation from a Single Image](http://arxiv.org/abs/2303.13612) #robust</code></li>
<li>Summary: <p>The practicality of 3D object pose estimation remains limited for many
applications due to the need for prior knowledge of a 3D model and a training
period for new objects. To address this limitation, we propose an approach that
takes a single image of a new object as input and predicts the relative pose of
this object in new images without prior knowledge of the object's 3D model and
without requiring training time for new objects and categories. We achieve this
by training a model to directly predict discriminative embeddings for
viewpoints surrounding the object. This prediction is done using a simple U-Net
architecture with attention and conditioned on the desired pose, which yields
extremely fast inference. We compare our approach to state-of-the-art methods
and show it outperforms them both in terms of accuracy and robustness. Our
source code is publicly available at https://github.com/nv-nguyen/nope
</p></li>
</ul>

<h3>Title: Bringing Inputs to Shared Domains for 3D Interacting Hands Recovery in the Wild. (arXiv:2303.13652v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13652">http://arxiv.org/abs/2303.13652</a></li>
<li>Code URL: <a href="https://github.com/facebookresearch/interwild">https://github.com/facebookresearch/interwild</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13652] Bringing Inputs to Shared Domains for 3D Interacting Hands Recovery in the Wild](http://arxiv.org/abs/2303.13652) #robust</code></li>
<li>Summary: <p>Despite recent achievements, existing 3D interacting hands recovery methods
have shown results mainly on motion capture (MoCap) environments, not on
in-the-wild (ITW) ones. This is because collecting 3D interacting hands data in
the wild is extremely challenging, even for the 2D data. We present InterWild,
which brings MoCap and ITW samples to shared domains for robust 3D interacting
hands recovery in the wild with a limited amount of ITW 2D/3D interacting hands
data. 3D interacting hands recovery consists of two sub-problems: 1) 3D
recovery of each hand and 2) 3D relative translation recovery between two
hands. For the first sub-problem, we bring MoCap and ITW samples to a shared 2D
scale space. Although ITW datasets provide a limited amount of 2D/3D
interacting hands, they contain large-scale 2D single hand data. Motivated by
this, we use a single hand image as an input for the first sub-problem
regardless of whether two hands are interacting. Hence, interacting hands of
MoCap datasets are brought to the 2D scale space of single hands of ITW
datasets. For the second sub-problem, we bring MoCap and ITW samples to a
shared appearance-invariant space. Unlike the first sub-problem, 2D labels of
ITW datasets are not helpful for the second sub-problem due to the 3D
translation's ambiguity. Hence, instead of relying on ITW samples, we amplify
the generalizability of MoCap samples by taking only a geometric feature
without an image as an input for the second sub-problem. As the geometric
feature is invariant to appearances, MoCap and ITW samples do not suffer from a
huge appearance gap between the two datasets. The code is publicly available at
https://github.com/facebookresearch/InterWild.
</p></li>
</ul>

<h3>Title: Efficient Neural Architecture Search for Emotion Recognition. (arXiv:2303.13653v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13653">http://arxiv.org/abs/2303.13653</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13653] Efficient Neural Architecture Search for Emotion Recognition](http://arxiv.org/abs/2303.13653) #robust</code></li>
<li>Summary: <p>Automated human emotion recognition from facial expressions is a well-studied
problem and still remains a very challenging task. Some efficient or accurate
deep learning models have been presented in the literature. However, it is
quite difficult to design a model that is both efficient and accurate at the
same time. Moreover, identifying the minute feature variations in facial
regions for both macro and micro-expressions requires expertise in network
design. In this paper, we proposed to search for a highly efficient and robust
neural architecture for both macro and micro-level facial expression
recognition. To the best of our knowledge, this is the first attempt to design
a NAS-based solution for both macro and micro-expression recognition. We
produce lightweight models with a gradient-based architecture search algorithm.
To maintain consistency between macro and micro-expressions, we utilize dynamic
imaging and convert microexpression sequences into a single frame, preserving
the spatiotemporal features in the facial regions. The EmoNAS has evaluated
over 13 datasets (7 macro expression datasets: CK+, DISFA, MUG, ISED, OULU-VIS
CASIA, FER2013, RAF-DB, and 6 micro-expression datasets: CASME-I, CASME-II,
CAS(ME)2, SAMM, SMIC, MEGC2019 challenge). The proposed models outperform the
existing state-of-the-art methods and perform very well in terms of speed and
space complexity.
</p></li>
</ul>

<h3>Title: Low-frequency Image Deep Steganography: Manipulate the Frequency Distribution to Hide Secrets with Tenacious Robustness. (arXiv:2303.13713v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13713">http://arxiv.org/abs/2303.13713</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13713] Low-frequency Image Deep Steganography: Manipulate the Frequency Distribution to Hide Secrets with Tenacious Robustness](http://arxiv.org/abs/2303.13713) #robust</code></li>
<li>Summary: <p>Image deep steganography (IDS) is a technique that utilizes deep learning to
embed a secret image invisibly into a cover image to generate a container
image. However, the container images generated by convolutional neural networks
(CNNs) are vulnerable to attacks that distort their high-frequency components.
To address this problem, we propose a novel method called Low-frequency Image
Deep Steganography (LIDS) that allows frequency distribution manipulation in
the embedding process. LIDS extracts a feature map from the secret image and
adds it to the cover image to yield the container image. The container image is
not directly output by the CNNs, and thus, it does not contain high-frequency
artifacts. The extracted feature map is regulated by a frequency loss to ensure
that its frequency distribution mainly concentrates on the low-frequency
domain. To further enhance robustness, an attack layer is inserted to damage
the container image. The retrieval network then retrieves a recovered secret
image from a damaged container image. Our experiments demonstrate that LIDS
outperforms state-of-the-art methods in terms of robustness, while maintaining
high fidelity and specificity. By avoiding high-frequency artifacts and
manipulating the frequency distribution of the embedded feature map, LIDS
achieves improved robustness against attacks that distort the high-frequency
components of container images.
</p></li>
</ul>

<h3>Title: Progressively Optimized Local Radiance Fields for Robust View Synthesis. (arXiv:2303.13791v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13791">http://arxiv.org/abs/2303.13791</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13791] Progressively Optimized Local Radiance Fields for Robust View Synthesis](http://arxiv.org/abs/2303.13791) #robust</code></li>
<li>Summary: <p>We present an algorithm for reconstructing the radiance field of a
large-scale scene from a single casually captured video. The task poses two
core challenges. First, most existing radiance field reconstruction approaches
rely on accurate pre-estimated camera poses from Structure-from-Motion
algorithms, which frequently fail on in-the-wild videos. Second, using a
single, global radiance field with finite representational capacity does not
scale to longer trajectories in an unbounded scene. For handling unknown poses,
we jointly estimate the camera poses with radiance field in a progressive
manner. We show that progressive optimization significantly improves the
robustness of the reconstruction. For handling large unbounded scenes, we
dynamically allocate new local radiance fields trained with frames within a
temporal window. This further improves robustness (e.g., performs well even
under moderate pose drifts) and allows us to scale to large scenes. Our
extensive evaluation on the Tanks and Temples dataset and our collected outdoor
dataset, Static Hikes, show that our approach compares favorably with the
state-of-the-art.
</p></li>
</ul>

<h3>Title: Generalist: Decoupling Natural and Robust Generalization. (arXiv:2303.13813v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13813">http://arxiv.org/abs/2303.13813</a></li>
<li>Code URL: <a href="https://github.com/pku-ml/generalist">https://github.com/pku-ml/generalist</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13813] Generalist: Decoupling Natural and Robust Generalization](http://arxiv.org/abs/2303.13813) #robust</code></li>
<li>Summary: <p>Deep neural networks obtained by standard training have been constantly
plagued by adversarial examples. Although adversarial training demonstrates its
capability to defend against adversarial examples, unfortunately, it leads to
an inevitable drop in the natural generalization. To address the issue, we
decouple the natural generalization and the robust generalization from joint
training and formulate different training strategies for each one.
Specifically, instead of minimizing a global loss on the expectation over these
two generalization errors, we propose a bi-expert framework called
\emph{Generalist} where we simultaneously train base learners with task-aware
strategies so that they can specialize in their own fields. The parameters of
base learners are collected and combined to form a global learner at intervals
during the training process. The global learner is then distributed to the base
learners as initialized parameters for continued training. Theoretically, we
prove that the risks of Generalist will get lower once the base learners are
well trained. Extensive experiments verify the applicability of Generalist to
achieve high accuracy on natural examples while maintaining considerable
robustness to adversarial ones. Code is available at
https://github.com/PKU-ML/Generalist.
</p></li>
</ul>

<h3>Title: Anomaly Detection under Distribution Shift. (arXiv:2303.13845v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13845">http://arxiv.org/abs/2303.13845</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13845] Anomaly Detection under Distribution Shift](http://arxiv.org/abs/2303.13845) #robust</code></li>
<li>Summary: <p>Anomaly detection (AD) is a crucial machine learning task that aims to learn
patterns from a set of normal training samples to identify abnormal samples in
test data. Most existing AD studies assume that the training and test data are
drawn from the same data distribution, but the test data can have large
distribution shifts arising in many real-world applications due to different
natural variations such as new lighting conditions, object poses, or background
appearances, rendering existing AD methods ineffective in such cases. In this
paper, we consider the problem of anomaly detection under distribution shift
and establish performance benchmarks on three widely-used AD and
out-of-distribution (OOD) generalization datasets. We demonstrate that simple
adaptation of state-of-the-art OOD generalization methods to AD settings fails
to work effectively due to the lack of labeled anomaly data. We further
introduce a novel robust AD approach to diverse distribution shifts by
minimizing the distribution gap between in-distribution and OOD normal samples
in both the training and inference stages in an unsupervised way. Our extensive
empirical results on the three datasets show that our approach substantially
outperforms state-of-the-art AD methods and OOD generalization methods on data
with various distribution shifts, while maintaining the detection accuracy on
in-distribution data.
</p></li>
</ul>

<h3>Title: Feature Separation and Recalibration for Adversarial Robustness. (arXiv:2303.13846v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13846">http://arxiv.org/abs/2303.13846</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13846] Feature Separation and Recalibration for Adversarial Robustness](http://arxiv.org/abs/2303.13846) #robust</code></li>
<li>Summary: <p>Deep neural networks are susceptible to adversarial attacks due to the
accumulation of perturbations in the feature level, and numerous works have
boosted model robustness by deactivating the non-robust feature activations
that cause model mispredictions. However, we claim that these malicious
activations still contain discriminative cues and that with recalibration, they
can capture additional useful information for correct model predictions. To
this end, we propose a novel, easy-to-plugin approach named Feature Separation
and Recalibration (FSR) that recalibrates the malicious, non-robust activations
for more robust feature maps through Separation and Recalibration. The
Separation part disentangles the input feature map into the robust feature with
activations that help the model make correct predictions and the non-robust
feature with activations that are responsible for model mispredictions upon
adversarial attack. The Recalibration part then adjusts the non-robust
activations to restore the potentially useful cues for model predictions.
Extensive experiments verify the superiority of FSR compared to traditional
deactivation techniques and demonstrate that it improves the robustness of
existing adversarial training methods by up to 8.57% with small computational
overhead. Codes are available at https://github.com/wkim97/FSR.
</p></li>
</ul>

<h3>Title: Deformable Model Driven Neural Rendering for High-fidelity 3D Reconstruction of Human Heads Under Low-View Settings. (arXiv:2303.13855v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13855">http://arxiv.org/abs/2303.13855</a></li>
<li>Code URL: <a href="https://github.com/xubaixinxbx/high-fidelity-3d-reconstruction-of-human-heads">https://github.com/xubaixinxbx/high-fidelity-3d-reconstruction-of-human-heads</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13855] Deformable Model Driven Neural Rendering for High-fidelity 3D Reconstruction of Human Heads Under Low-View Settings](http://arxiv.org/abs/2303.13855) #robust</code></li>
<li>Summary: <p>We propose a robust method for learning neural implicit functions that can
reconstruct 3D human heads with high-fidelity geometry from low-view inputs. We
represent 3D human heads as the zero level-set of a composed signed distance
field that consists of a smooth template, a non-rigid deformation, and a
high-frequency displacement field. The template represents identity-independent
and expression-neutral features, which is trained on multiple individuals,
along with the deformation network. The displacement field encodes
identity-dependent geometric details, trained for each specific individual. We
train our network in two stages using a coarse-to-fine strategy without 3D
supervision. Our experiments demonstrate that the geometry decomposition and
two-stage training make our method robust and our model outperforms existing
methods in terms of reconstruction accuracy and novel view synthesis under
low-view settings. Additionally, the pre-trained template serves a good
initialization for our model to adapt to unseen individuals.
</p></li>
</ul>

<h3>Title: Robust Test-Time Adaptation in Dynamic Scenarios. (arXiv:2303.13899v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13899">http://arxiv.org/abs/2303.13899</a></li>
<li>Code URL: <a href="https://github.com/bit-da/rotta">https://github.com/bit-da/rotta</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13899] Robust Test-Time Adaptation in Dynamic Scenarios](http://arxiv.org/abs/2303.13899) #robust</code></li>
<li>Summary: <p>Test-time adaptation (TTA) intends to adapt the pretrained model to test
distributions with only unlabeled test data streams. Most of the previous TTA
methods have achieved great success on simple test data streams such as
independently sampled data from single or multiple distributions. However,
these attempts may fail in dynamic scenarios of real-world applications like
autonomous driving, where the environments gradually change and the test data
is sampled correlatively over time. In this work, we explore such practical
test data streams to deploy the model on the fly, namely practical test-time
adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA)
method against the complex data stream in PTTA. More specifically, we present a
robust batch normalization scheme to estimate the normalization statistics.
Meanwhile, a memory bank is utilized to sample category-balanced data with
consideration of timeliness and uncertainty. Further, to stabilize the training
procedure, we develop a time-aware reweighting strategy with a teacher-student
model. Extensive experiments prove that RoTTA enables continual testtime
adaptation on the correlatively sampled data streams. Our method is easy to
implement, making it a good choice for rapid deployment. The code is publicly
available at https://github.com/BIT-DA/RoTTA
</p></li>
</ul>

<h3>Title: CCL: Continual Contrastive Learning for LiDAR Place Recognition. (arXiv:2303.13952v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13952">http://arxiv.org/abs/2303.13952</a></li>
<li>Code URL: <a href="https://github.com/cloudcjf/ccl">https://github.com/cloudcjf/ccl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13952] CCL: Continual Contrastive Learning for LiDAR Place Recognition](http://arxiv.org/abs/2303.13952) #robust</code></li>
<li>Summary: <p>Place recognition is an essential and challenging task in loop closing and
global localization for robotics and autonomous driving applications.
Benefiting from the recent advances in deep learning techniques, the
performance of LiDAR place recognition (LPR) has been greatly improved.
However, current deep learning-based methods suffer from two major problems:
poor generalization ability and catastrophic forgetting. In this paper, we
propose a continual contrastive learning method, named CCL, to tackle the
catastrophic forgetting problem and generally improve the robustness of LPR
approaches. Our CCL constructs a contrastive feature pool and utilizes
contrastive loss to train more transferable representations of places. When
transferred into new environments, our CCL continuously reviews the contrastive
memory bank and applies a distribution-based knowledge distillation to maintain
the retrieval ability of the past data while continually learning to recognize
new places from the new data. We thoroughly evaluate our approach on Oxford,
MulRan, and PNV datasets using three different LPR methods. The experimental
results show that our CCL consistently improves the performance of different
methods in different environments outperforming the state-of-the-art continual
learning method. The implementation of our method has been released at
https://github.com/cloudcjf/CCL.
</p></li>
</ul>

<h3>Title: StereoScene: BEV-Assisted Stereo Matching Empowers 3D Semantic Scene Completion. (arXiv:2303.13959v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13959">http://arxiv.org/abs/2303.13959</a></li>
<li>Code URL: <a href="https://github.com/Arlo0o/StereoScene">https://github.com/Arlo0o/StereoScene</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13959] StereoScene: BEV-Assisted Stereo Matching Empowers 3D Semantic Scene Completion](http://arxiv.org/abs/2303.13959) #robust</code></li>
<li>Summary: <p>3D semantic scene completion (SSC) is an ill-posed task that requires
inferring a dense 3D scene from incomplete observations. Previous methods
either explicitly incorporate 3D geometric input or rely on learnt 3D prior
behind monocular RGB images. However, 3D sensors such as LiDAR are expensive
and intrusive while monocular cameras face challenges in modeling precise
geometry due to the inherent ambiguity. In this work, we propose StereoScene
for 3D Semantic Scene Completion (SSC), which explores taking full advantage of
light-weight camera inputs without resorting to any external 3D sensors. Our
key insight is to leverage stereo matching to resolve geometric ambiguity. To
improve its robustness in unmatched areas, we introduce bird's-eye-view (BEV)
representation to inspire hallucination ability with rich context information.
On top of the stereo and BEV representations, a mutual interactive aggregation
(MIA) module is carefully devised to fully unleash their power. Specifically, a
Bi-directional Interaction Transformer (BIT) augmented with confidence
re-weighting is used to encourage reliable prediction through mutual guidance
while a Dual Volume Aggregation (DVA) module is designed to facilitate
complementary aggregation. Experimental results on SemanticKITTI demonstrate
that the proposed StereoScene outperforms the state-of-the-art camera-based
methods by a large margin with a relative improvement of 26.9% in geometry and
38.6% in semantic.
</p></li>
</ul>

<h3>Title: DistractFlow: Improving Optical Flow Estimation via Realistic Distractions and Pseudo-Labeling. (arXiv:2303.14078v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14078">http://arxiv.org/abs/2303.14078</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14078] DistractFlow: Improving Optical Flow Estimation via Realistic Distractions and Pseudo-Labeling](http://arxiv.org/abs/2303.14078) #robust</code></li>
<li>Summary: <p>We propose a novel data augmentation approach, DistractFlow, for training
optical flow estimation models by introducing realistic distractions to the
input frames. Based on a mixing ratio, we combine one of the frames in the pair
with a distractor image depicting a similar domain, which allows for inducing
visual perturbations congruent with natural objects and scenes. We refer to
such pairs as distracted pairs. Our intuition is that using semantically
meaningful distractors enables the model to learn related variations and attain
robustness against challenging deviations, compared to conventional
augmentation schemes focusing only on low-level aspects and modifications. More
specifically, in addition to the supervised loss computed between the estimated
flow for the original pair and its ground-truth flow, we include a second
supervised loss defined between the distracted pair's flow and the original
pair's ground-truth flow, weighted with the same mixing ratio. Furthermore,
when unlabeled data is available, we extend our augmentation approach to
self-supervised settings through pseudo-labeling and cross-consistency
regularization. Given an original pair and its distracted version, we enforce
the estimated flow on the distracted pair to agree with the flow of the
original pair. Our approach allows increasing the number of available training
pairs significantly without requiring additional annotations. It is agnostic to
the model architecture and can be applied to training any optical flow
estimation models. Our extensive evaluations on multiple benchmarks, including
Sintel, KITTI, and SlowFlow, show that DistractFlow improves existing models
consistently, outperforming the latest state of the art.
</p></li>
</ul>

<h3>Title: Enhancing Multiple Reliability Measures via Nuisance-extended Information Bottleneck. (arXiv:2303.14096v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14096">http://arxiv.org/abs/2303.14096</a></li>
<li>Code URL: <a href="https://github.com/jh-jeong/nuisance_ib">https://github.com/jh-jeong/nuisance_ib</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14096] Enhancing Multiple Reliability Measures via Nuisance-extended Information Bottleneck](http://arxiv.org/abs/2303.14096) #robust</code></li>
<li>Summary: <p>In practical scenarios where training data is limited, many predictive
signals in the data can be rather from some biases in data acquisition (i.e.,
less generalizable), so that one cannot prevent a model from co-adapting on
such (so-called) "shortcut" signals: this makes the model fragile in various
distribution shifts. To bypass such failure modes, we consider an adversarial
threat model under a mutual information constraint to cover a wider class of
perturbations in training. This motivates us to extend the standard information
bottleneck to additionally model the nuisance information. We propose an
autoencoder-based training to implement the objective, as well as practical
encoder designs to facilitate the proposed hybrid discriminative-generative
training concerning both convolutional- and Transformer-based architectures.
Our experimental results show that the proposed scheme improves robustness of
learned representations (remarkably without using any domain-specific
knowledge), with respect to multiple challenging reliability measures. For
example, our model could advance the state-of-the-art on a recent challenging
OBJECTS benchmark in novelty detection by $78.4\% \rightarrow 87.2\%$ in AUROC,
while simultaneously enjoying improved corruption, background and (certified)
adversarial robustness. Code is available at
https://github.com/jh-jeong/nuisance_ib.
</p></li>
</ul>

<h3>Title: Object Motion Sensitivity: A Bio-inspired Solution to the Ego-motion Problem for Event-based Cameras. (arXiv:2303.14114v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14114">http://arxiv.org/abs/2303.14114</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14114] Object Motion Sensitivity: A Bio-inspired Solution to the Ego-motion Problem for Event-based Cameras](http://arxiv.org/abs/2303.14114) #robust</code></li>
<li>Summary: <p>Neuromorphic (event-based) image sensors draw inspiration from the
human-retina to create an electronic device that can process visual stimuli in
a way that closely resembles its biological counterpart. These sensors process
information significantly different than the traditional RGB sensors.
Specifically, the sensory information generated by event-based image sensors
are orders of magnitude sparser compared to that of RGB sensors. The first
generation of neuromorphic image sensors, Dynamic Vision Sensor (DVS), are
inspired by the computations confined to the photoreceptors and the first
retinal synapse. In this work, we highlight the capability of the second
generation of neuromorphic image sensors, Integrated Retinal Functionality in
CMOS Image Sensors (IRIS), which aims to mimic full retinal computations from
photoreceptors to output of the retina (retinal ganglion cells) for targeted
feature-extraction. The feature of choice in this work is Object Motion
Sensitivity (OMS) that is processed locally in the IRIS sensor. We study the
capability of OMS in solving the ego-motion problem of the event-based cameras.
Our results show that OMS can accomplish standard computer vision tasks with
similar efficiency to conventional RGB and DVS solutions but offers drastic
bandwidth reduction. This cuts the wireless and computing power budgets and
opens up vast opportunities in high-speed, robust, energy-efficient, and
low-bandwidth real-time decision making.
</p></li>
</ul>

<h3>Title: BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects. (arXiv:2303.14158v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14158">http://arxiv.org/abs/2303.14158</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14158] BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects](http://arxiv.org/abs/2303.14158) #robust</code></li>
<li>Summary: <p>We present a near real-time method for 6-DoF tracking of an unknown object
from a monocular RGBD video sequence, while simultaneously performing neural 3D
reconstruction of the object. Our method works for arbitrary rigid objects,
even when visual texture is largely absent. The object is assumed to be
segmented in the first frame only. No additional information is required, and
no assumption is made about the interaction agent. Key to our method is a
Neural Object Field that is learned concurrently with a pose graph optimization
process in order to robustly accumulate information into a consistent 3D
representation capturing both geometry and appearance. A dynamic pool of posed
memory frames is automatically maintained to facilitate communication between
these threads. Our approach handles challenging sequences with large pose
changes, partial and full occlusion, untextured surfaces, and specular
highlights. We show results on HO3D, YCBInEOAT, and BEHAVE datasets,
demonstrating that our method significantly outperforms existing approaches.
Project page: https://bundlesdf.github.io
</p></li>
</ul>

<h3>Title: Inherent Consistent Learning for Accurate Semi-supervised Medical Image Segmentation. (arXiv:2303.14175v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14175">http://arxiv.org/abs/2303.14175</a></li>
<li>Code URL: <a href="https://github.com/zhuye98/icl">https://github.com/zhuye98/icl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14175] Inherent Consistent Learning for Accurate Semi-supervised Medical Image Segmentation](http://arxiv.org/abs/2303.14175) #robust</code></li>
<li>Summary: <p>Semi-supervised medical image segmentation has attracted much attention in
recent years because of the high cost of medical image annotations. In this
paper, we propose a novel Inherent Consistent Learning (ICL) method, which aims
to learn robust semantic category representations through the semantic
consistency guidance of labeled and unlabeled data to help segmentation. In
practice, we introduce two external modules namely Supervised Semantic Proxy
Adaptor (SSPA) and Unsupervised Semantic Consistent Learner (USCL) that based
on the attention mechanism to align the semantic category representations of
labeled and unlabeled data, as well as update the global semantic
representations over the entire training set. The proposed ICL is a
plug-and-play scheme for various network architectures and the two modules are
not involved in the testing stage. Experimental results on three public
benchmarks show that the proposed method can outperform the state-of-the-art
especially when the number of annotated data is extremely limited. Code is
available at: https://github.com/zhuye98/ICL.git.
</p></li>
</ul>

<h3>Title: FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization. (arXiv:2303.14189v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14189">http://arxiv.org/abs/2303.14189</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14189] FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization](http://arxiv.org/abs/2303.14189) #robust</code></li>
<li>Summary: <p>The recent amalgamation of transformer and convolutional designs has led to
steady improvements in accuracy and efficiency of the models. In this work, we
introduce FastViT, a hybrid vision transformer architecture that obtains the
state-of-the-art latency-accuracy trade-off. To this end, we introduce a novel
token mixing operator, RepMixer, a building block of FastViT, that uses
structural reparameterization to lower the memory access cost by removing
skip-connections in the network. We further apply train-time
overparametrization and large kernel convolutions to boost accuracy and
empirically show that these choices have minimal effect on latency. We show
that - our model is 3.5x faster than CMT, a recent state-of-the-art hybrid
transformer architecture, 4.9x faster than EfficientNet, and 1.9x faster than
ConvNeXt on a mobile device for the same accuracy on the ImageNet dataset. At
similar latency, our model obtains 4.2% better Top-1 accuracy on ImageNet than
MobileOne. Our model consistently outperforms competing architectures across
several tasks -- image classification, detection, segmentation and 3D mesh
regression with significant improvement in latency on both a mobile device and
a desktop GPU. Furthermore, our model is highly robust to out-of-distribution
samples and corruptions, improving over competing robust models.
</p></li>
</ul>

<h3>Title: Adversarial Robustness and Feature Impact Analysis for Driver Drowsiness Detection. (arXiv:2303.13649v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13649">http://arxiv.org/abs/2303.13649</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13649] Adversarial Robustness and Feature Impact Analysis for Driver Drowsiness Detection](http://arxiv.org/abs/2303.13649) #robust</code></li>
<li>Summary: <p>Drowsy driving is a major cause of road accidents, but drivers are dismissive
of the impact that fatigue can have on their reaction times. To detect
drowsiness before any impairment occurs, a promising strategy is using Machine
Learning (ML) to monitor Heart Rate Variability (HRV) signals. This work
presents multiple experiments with different HRV time windows and ML models, a
feature impact analysis using Shapley Additive Explanations (SHAP), and an
adversarial robustness analysis to assess their reliability when processing
faulty input data and perturbed HRV signals. The most reliable model was
Extreme Gradient Boosting (XGB) and the optimal time window had between 120 and
150 seconds. Furthermore, SHAP enabled the selection of the 18 most impactful
features and the training of new smaller models that achieved a performance as
good as the initial ones. Despite the susceptibility of all models to
adversarial attacks, adversarial training enabled them to preserve
significantly higher results, especially XGB. Therefore, ML models can
significantly benefit from realistic adversarial training to provide a more
robust driver drowsiness detection.
</p></li>
</ul>

<h3>Title: Structural Imbalance Aware Graph Augmentation Learning. (arXiv:2303.13757v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13757">http://arxiv.org/abs/2303.13757</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13757] Structural Imbalance Aware Graph Augmentation Learning](http://arxiv.org/abs/2303.13757) #robust</code></li>
<li>Summary: <p>Graph machine learning (GML) has made great progress in node classification,
link prediction, graph classification and so on. However, graphs in reality are
often structurally imbalanced, that is, only a few hub nodes have a denser
local structure and higher influence. The imbalance may compromise the
robustness of existing GML models, especially in learning tail nodes. This
paper proposes a selective graph augmentation method (SAug) to solve this
problem. Firstly, a Pagerank-based sampling strategy is designed to identify
hub nodes and tail nodes in the graph. Secondly, a selective augmentation
strategy is proposed, which drops the noisy neighbors of hub nodes on one side,
and discovers the latent neighbors and generates pseudo neighbors for tail
nodes on the other side. It can also alleviate the structural imbalance between
two types of nodes. Finally, a GNN model will be retrained on the augmented
graph. Extensive experiments demonstrate that SAug can significantly improve
the backbone GNNs and achieve superior performance to its competitors of graph
augmentation methods and hub/tail aware methods.
</p></li>
</ul>

<h3>Title: Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs. (arXiv:2303.13763v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13763">http://arxiv.org/abs/2303.13763</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13763] Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs](http://arxiv.org/abs/2303.13763) #robust</code></li>
<li>Summary: <p>Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency
multilayer perceptrons~(MLPs) on graph tasks has become a hot research topic.
However, MLPs rely exclusively on the node features and fail to capture the
graph structural information. Previous methods address this issue by processing
graph edges into extra inputs for MLPs, but such graph structures may be
unavailable for various scenarios. To this end, we propose a Prototype-Guided
Knowledge Distillation~(PGKD) method, which does not require graph
edges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze the
graph structural information in GNN teachers, and distill such information from
GNNs to MLPs via prototypes in an edge-free setting. Experimental results on
popular graph benchmarks demonstrate the effectiveness and robustness of the
proposed PGKD.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Multimodal Adaptive Fusion of Face and Gait Features using Keyless attention based Deep Neural Networks for Human Identification. (arXiv:2303.13814v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13814">http://arxiv.org/abs/2303.13814</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13814] Multimodal Adaptive Fusion of Face and Gait Features using Keyless attention based Deep Neural Networks for Human Identification](http://arxiv.org/abs/2303.13814) #biometric</code></li>
<li>Summary: <p>Biometrics plays a significant role in vision-based surveillance
applications. Soft biometrics such as gait is widely used with face in
surveillance tasks like person recognition and re-identification. Nevertheless,
in practical scenarios, classical fusion techniques respond poorly to changes
in individual users and in the external environment. To this end, we propose a
novel adaptive multi-biometric fusion strategy for the dynamic incorporation of
gait and face biometric cues by leveraging keyless attention deep neural
networks. Various external factors such as viewpoint and distance to the
camera, are investigated in this study. Extensive experiments have shown
superior performanceof the proposed model compared with the state-of-the-art
model.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Extracting real estate values of rental apartment floor plans using graph convolutional networks. (arXiv:2303.13568v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13568">http://arxiv.org/abs/2303.13568</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13568] Extracting real estate values of rental apartment floor plans using graph convolutional networks](http://arxiv.org/abs/2303.13568) #extraction</code></li>
<li>Summary: <p>Access graphs that indicate adjacency relationships from the perspective of
flow lines of rooms are extracted automatically from a large number of floor
plan images of a family-oriented rental apartment complex in Osaka Prefecture,
Japan, based on a recently proposed access graph extraction method with slight
modifications. We define and implement a graph convolutional network (GCN) for
access graphs and propose a model to estimate the real estate value of access
graphs as the floor plan value. The model, which includes the floor plan value
and hedonic method using other general explanatory variables, is used to
estimate rents and their estimation accuracies are compared. In addition, the
features of the floor plan that explain the rent are analyzed from the learned
convolution network. Therefore, a new model for comprehensively estimating the
value of real estate floor plans is proposed and validated. The results show
that the proposed method significantly improves the accuracy of rent estimation
compared to that of conventional models, and it is possible to understand the
specific spatial configuration rules that influence the value of a floor plan
by analyzing the learned GCN.
</p></li>
</ul>

<h3>Title: Probability-based Global Cross-modal Upsampling for Pansharpening. (arXiv:2303.13659v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13659">http://arxiv.org/abs/2303.13659</a></li>
<li>Code URL: <a href="https://github.com/zeyu-zhu/pgcu">https://github.com/zeyu-zhu/pgcu</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13659] Probability-based Global Cross-modal Upsampling for Pansharpening](http://arxiv.org/abs/2303.13659) #extraction</code></li>
<li>Summary: <p>Pansharpening is an essential preprocessing step for remote sensing image
processing. Although deep learning (DL) approaches performed well on this task,
current upsampling methods used in these approaches only utilize the local
information of each pixel in the low-resolution multispectral (LRMS) image
while neglecting to exploit its global information as well as the cross-modal
information of the guiding panchromatic (PAN) image, which limits their
performance improvement. To address this issue, this paper develops a novel
probability-based global cross-modal upsampling (PGCU) method for
pan-sharpening. Precisely, we first formulate the PGCU method from a
probabilistic perspective and then design an efficient network module to
implement it by fully utilizing the information mentioned above while
simultaneously considering the channel specificity. The PGCU module consists of
three blocks, i.e., information extraction (IE), distribution and expectation
estimation (DEE), and fine adjustment (FA). Extensive experiments verify the
superiority of the PGCU method compared with other popular upsampling methods.
Additionally, experiments also show that the PGCU module can help improve the
performance of existing SOTA deep learning pansharpening methods. The codes are
available at https://github.com/Zeyu-Zhu/PGCU.
</p></li>
</ul>

<h3>Title: PFT-SSR: Parallax Fusion Transformer for Stereo Image Super-Resolution. (arXiv:2303.13807v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13807">http://arxiv.org/abs/2303.13807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13807] PFT-SSR: Parallax Fusion Transformer for Stereo Image Super-Resolution](http://arxiv.org/abs/2303.13807) #extraction</code></li>
<li>Summary: <p>Stereo image super-resolution aims to boost the performance of image
super-resolution by exploiting the supplementary information provided by
binocular systems. Although previous methods have achieved promising results,
they did not fully utilize the information of cross-view and intra-view. To
further unleash the potential of binocular images, in this letter, we propose a
novel Transformerbased parallax fusion module called Parallax Fusion
Transformer (PFT). PFT employs a Cross-view Fusion Transformer (CVFT) to
utilize cross-view information and an Intra-view Refinement Transformer (IVRT)
for intra-view feature refinement. Meanwhile, we adopted the Swin Transformer
as the backbone for feature extraction and SR reconstruction to form a pure
Transformer architecture called PFT-SSR. Extensive experiments and ablation
studies show that PFT-SSR achieves competitive results and outperforms most
SOTA methods. Source code is available at https://github.com/MIVRC/PFT-PyTorch.
</p></li>
</ul>

<h3>Title: Prior-RadGraphFormer: A Prior-Knowledge-Enhanced Transformer for Generating Radiology Graphs from X-Rays. (arXiv:2303.13818v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13818">http://arxiv.org/abs/2303.13818</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13818] Prior-RadGraphFormer: A Prior-Knowledge-Enhanced Transformer for Generating Radiology Graphs from X-Rays](http://arxiv.org/abs/2303.13818) #extraction</code></li>
<li>Summary: <p>The extraction of structured clinical information from free-text radiology
reports in the form of radiology graphs has been demonstrated to be a valuable
approach for evaluating the clinical correctness of report-generation methods.
However, the direct generation of radiology graphs from chest X-ray (CXR)
images has not been attempted. To address this gap, we propose a novel approach
called Prior-RadGraphFormer that utilizes a transformer model with prior
knowledge in the form of a probabilistic knowledge graph (PKG) to generate
radiology graphs directly from CXR images. The PKG models the statistical
relationship between radiology entities, including anatomical structures and
medical observations. This additional contextual information enhances the
accuracy of entity and relation extraction. The generated radiology graphs can
be applied to various downstream tasks, such as free-text or structured reports
generation and multi-label classification of pathologies. Our approach
represents a promising method for generating radiology graphs directly from CXR
images, and has significant potential for improving medical image analysis and
clinical decision-making.
</p></li>
</ul>

<h3>Title: Semantic Prompt for Few-Shot Image Recognition. (arXiv:2303.14123v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14123">http://arxiv.org/abs/2303.14123</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14123] Semantic Prompt for Few-Shot Image Recognition](http://arxiv.org/abs/2303.14123) #extraction</code></li>
<li>Summary: <p>Few-shot learning is a challenging problem since only a few examples are
provided to recognize a new class. Several recent studies exploit additional
semantic information, e.g. text embeddings of class names, to address the issue
of rare samples through combining semantic prototypes with visual prototypes.
However, these methods still suffer from the spurious visual features learned
from the rare support samples, resulting in limited benefits. In this paper, we
propose a novel Semantic Prompt (SP) approach for few-shot learning. Instead of
the naive exploitation of semantic information for remedying classifiers, we
explore leveraging semantic information as prompts to tune the visual feature
extraction network adaptively. Specifically, we design two complementary
mechanisms to insert semantic prompts into the feature extractor: one is to
enable the interaction between semantic prompts and patch embeddings along the
spatial dimension via self-attention, another is to supplement visual features
with the transformed semantic prompts along the channel dimension. By combining
these two mechanisms, the feature extractor presents a better ability to attend
to the class-specific features and obtains more generalized image
representations with merely a few support samples. Through extensive
experiments on four datasets, the proposed approach achieves promising results,
improving the 1-shot learning accuracy by 3.67% on average.
</p></li>
</ul>

<h3>Title: Overview of the ICASSP 2023 General Meeting Understanding and Generation Challenge (MUG). (arXiv:2303.13932v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13932">http://arxiv.org/abs/2303.13932</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13932] Overview of the ICASSP 2023 General Meeting Understanding and Generation Challenge (MUG)](http://arxiv.org/abs/2303.13932) #extraction</code></li>
<li>Summary: <p>ICASSP2023 General Meeting Understanding and Generation Challenge (MUG)
focuses on prompting a wide range of spoken language processing (SLP) research
on meeting transcripts, as SLP applications are critical to improve users'
efficiency in grasping important information in meetings. MUG includes five
tracks, including topic segmentation, topic-level and session-level extractive
summarization, topic title generation, keyphrase extraction, and action item
detection. To facilitate MUG, we construct and release a large-scale meeting
dataset, the AliMeeting4MUG Corpus.
</p></li>
</ul>

<h3>Title: MUG: A General Meeting Understanding and Generation Benchmark. (arXiv:2303.13939v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13939">http://arxiv.org/abs/2303.13939</a></li>
<li>Code URL: <a href="https://github.com/alibaba-damo-academy/spokennlp">https://github.com/alibaba-damo-academy/spokennlp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13939] MUG: A General Meeting Understanding and Generation Benchmark](http://arxiv.org/abs/2303.13939) #extraction</code></li>
<li>Summary: <p>Listening to long video/audio recordings from video conferencing and online
courses for acquiring information is extremely inefficient. Even after ASR
systems transcribe recordings into long-form spoken language documents, reading
ASR transcripts only partly speeds up seeking information. It has been observed
that a range of NLP applications, such as keyphrase extraction, topic
segmentation, and summarization, significantly improve users' efficiency in
grasping important information. The meeting scenario is among the most valuable
scenarios for deploying these spoken language processing (SLP) capabilities.
However, the lack of large-scale public meeting datasets annotated for these
SLP tasks severely hinders their advancement. To prompt SLP advancement, we
establish a large-scale general Meeting Understanding and Generation Benchmark
(MUG) to benchmark the performance of a wide range of SLP tasks, including
topic segmentation, topic-level and session-level extractive summarization and
topic title generation, keyphrase extraction, and action item detection. To
facilitate the MUG benchmark, we construct and release a large-scale meeting
dataset for comprehensive long-form SLP development, the AliMeeting4MUG Corpus,
which consists of 654 recorded Mandarin meeting sessions with diverse topic
coverage, with manual annotations for SLP tasks on manual transcripts of
meeting recordings. To the best of our knowledge, the AliMeeting4MUG Corpus is
so far the largest meeting corpus in scale and facilitates most SLP tasks. In
this paper, we provide a detailed introduction of this corpus, SLP tasks and
evaluation methods, baseline systems and their performance.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Learning on Heterogenous Data using Chest CT. (arXiv:2303.13567v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13567">http://arxiv.org/abs/2303.13567</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13567] Federated Learning on Heterogenous Data using Chest CT](http://arxiv.org/abs/2303.13567) #federate</code></li>
<li>Summary: <p>Large data have accelerated advances in AI. While it is well known that
population differences from genetics, sex, race, diet, and various
environmental factors contribute significantly to disease, AI studies in
medicine have largely focused on locoregional patient cohorts with less diverse
data sources. Such limitation stems from barriers to large-scale data share in
medicine and ethical concerns over data privacy. Federated learning (FL) is one
potential pathway for AI development that enables learning across hospitals
without data share. In this study, we show the results of various FL strategies
on one of the largest and most diverse COVID-19 chest CT datasets: 21
participating hospitals across five continents that comprise >10,000 patients
with >1 million images. We present three techniques: Fed Averaging (FedAvg),
Incremental Institutional Learning (IIL), and Cyclical Incremental
Institutional Learning (CIIL). We also propose an FL strategy that leverages
synthetically generated data to overcome class imbalances and data size
disparities across centers. We show that FL can achieve comparable performance
to Centralized Data Sharing (CDS) while maintaining high performance across
sites with small, underrepresented data. We investigate the strengths and
weaknesses for all technical approaches on this heterogeneous dataset including
the robustness to non-Independent and identically distributed (non-IID)
diversity of data. We also describe the sources of data heterogeneity such as
age, sex, and site locations in the context of FL and show how even among the
correctly labeled populations, disparities can arise due to these biases.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Efficient and Accurate Co-Visible Region Localization with Matching Key-Points Crop (MKPC): A Two-Stage Pipeline for Enhancing Image Matching Performance. (arXiv:2303.13794v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13794">http://arxiv.org/abs/2303.13794</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13794] Efficient and Accurate Co-Visible Region Localization with Matching Key-Points Crop (MKPC): A Two-Stage Pipeline for Enhancing Image Matching Performance](http://arxiv.org/abs/2303.13794) #fair</code></li>
<li>Summary: <p>Image matching is a classic and fundamental task in computer vision. In this
paper, under the hypothesis that the areas outside the co-visible regions carry
little information, we propose a matching key-points crop (MKPC) algorithm. The
MKPC locates, proposes and crops the critical regions, which are the co-visible
areas with great efficiency and accuracy. Furthermore, building upon MKPC, we
propose a general two-stage pipeline for image matching, which is compatible to
any image matching models or combinations. We experimented with plugging
SuperPoint + SuperGlue into the two-stage pipeline, whose results show that our
method enhances the performance for outdoor pose estimations. What's more, in a
fair comparative condition, our method outperforms the SOTA on Image Matching
Challenge 2022 Benchmark, which represents the hardest outdoor benchmark of
image matching currently.
</p></li>
</ul>

<h3>Title: Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint. (arXiv:2303.13790v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13790">http://arxiv.org/abs/2303.13790</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13790] Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint](http://arxiv.org/abs/2303.13790) #fair</code></li>
<li>Summary: <p>Clinical trials are indispensable in developing new treatments, but they face
obstacles in patient recruitment and retention, hindering the enrollment of
necessary participants. To tackle these challenges, deep learning frameworks
have been created to match patients to trials. These frameworks calculate the
similarity between patients and clinical trial eligibility criteria,
considering the discrepancy between inclusion and exclusion criteria. Recent
studies have shown that these frameworks outperform earlier approaches.
However, deep learning models may raise fairness issues in patient-trial
matching when certain sensitive groups of individuals are underrepresented in
clinical trials, leading to incomplete or inaccurate data and potential harm.
To tackle the issue of fairness, this work proposes a fair patient-trial
matching framework by generating a patient-criterion level fairness constraint.
The proposed framework considers the inconsistency between the embedding of
inclusion and exclusion criteria among patients of different sensitive groups.
The experimental results on real-world patient-trial and patient-criterion
matching tasks demonstrate that the proposed framework can successfully
alleviate the predictions that tend to be biased.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Regularization of polynomial networks for image recognition. (arXiv:2303.13896v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13896">http://arxiv.org/abs/2303.13896</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13896] Regularization of polynomial networks for image recognition](http://arxiv.org/abs/2303.13896) #interpretability</code></li>
<li>Summary: <p>Deep Neural Networks (DNNs) have obtained impressive performance across
tasks, however they still remain as black boxes, e.g., hard to theoretically
analyze. At the same time, Polynomial Networks (PNs) have emerged as an
alternative method with a promising performance and improved interpretability
but have yet to reach the performance of the powerful DNN baselines. In this
work, we aim to close this performance gap. We introduce a class of PNs, which
are able to reach the performance of ResNet across a range of six benchmarks.
We demonstrate that strong regularization is critical and conduct an extensive
study of the exact regularization schemes required to match performance. To
further motivate the regularization schemes, we introduce D-PolyNets that
achieve a higher-degree of expansion than previously proposed polynomial
networks. D-PolyNets are more parameter-efficient while achieving a similar
performance as other polynomial networks. We expect that our new models can
lead to an understanding of the role of elementwise activation functions (which
are no longer required for training PNs). The source code is available at
https://github.com/grigorisg9gr/regularized_polynomials.
</p></li>
</ul>

<h3>Title: Best of Both Worlds: Multimodal Contrastive Learning with Tabular and Imaging Data. (arXiv:2303.14080v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14080">http://arxiv.org/abs/2303.14080</a></li>
<li>Code URL: <a href="https://github.com/paulhager/mmcl-tabular-imaging">https://github.com/paulhager/mmcl-tabular-imaging</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14080] Best of Both Worlds: Multimodal Contrastive Learning with Tabular and Imaging Data](http://arxiv.org/abs/2303.14080) #interpretability</code></li>
<li>Summary: <p>Medical datasets and especially biobanks, often contain extensive tabular
data with rich clinical information in addition to images. In practice,
clinicians typically have less data, both in terms of diversity and scale, but
still wish to deploy deep learning solutions. Combined with increasing medical
dataset sizes and expensive annotation costs, the necessity for unsupervised
methods that can pretrain multimodally and predict unimodally has risen.
</p></li>
</ul>

<p>To address these needs, we propose the first self-supervised contrastive
learning framework that takes advantage of images and tabular data to train
unimodal encoders. Our solution combines SimCLR and SCARF, two leading
contrastive learning strategies, and is simple and effective. In our
experiments, we demonstrate the strength of our framework by predicting risks
of myocardial infarction and coronary artery disease (CAD) using cardiac MR
images and 120 clinical features from 40,000 UK Biobank subjects. Furthermore,
we show the generalizability of our approach to natural images using the DVM
car advertisement dataset.
</p>
<p>We take advantage of the high interpretability of tabular data and through
attribution and ablation experiments find that morphometric tabular features,
describing size and shape, have outsized importance during the contrastive
learning process and improve the quality of the learned embeddings. Finally, we
introduce a novel form of supervised contrastive learning, label as a feature
(LaaF), by appending the ground truth label as a tabular feature during
multimodal pretraining, outperforming all supervised contrastive baselines.
</p>

<h3>Title: Improving Prediction Performance and Model Interpretability through Attention Mechanisms from Basic and Applied Research Perspectives. (arXiv:2303.14116v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14116">http://arxiv.org/abs/2303.14116</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14116] Improving Prediction Performance and Model Interpretability through Attention Mechanisms from Basic and Applied Research Perspectives](http://arxiv.org/abs/2303.14116) #interpretability</code></li>
<li>Summary: <p>With the dramatic advances in deep learning technology, machine learning
research is focusing on improving the interpretability of model predictions as
well as prediction performance in both basic and applied research. While deep
learning models have much higher prediction performance than traditional
machine learning models, the specific prediction process is still difficult to
interpret and/or explain. This is known as the black-boxing of machine learning
models and is recognized as a particularly important problem in a wide range of
research fields, including manufacturing, commerce, robotics, and other
industries where the use of such technology has become commonplace, as well as
the medical field, where mistakes are not tolerated. This bulletin is based on
the summary of the author's dissertation. The research summarized in the
dissertation focuses on the attention mechanism, which has been the focus of
much attention in recent years, and discusses its potential for both basic
research in terms of improving prediction performance and interpretability, and
applied research in terms of evaluating it for real-world applications using
large data sets beyond the laboratory environment. The dissertation also
concludes with a summary of the implications of these findings for subsequent
research and future prospects in the field.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: End-to-End Diffusion Latent Optimization Improves Classifier Guidance. (arXiv:2303.13703v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13703">http://arxiv.org/abs/2303.13703</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13703] End-to-End Diffusion Latent Optimization Improves Classifier Guidance](http://arxiv.org/abs/2303.13703) #diffusion</code></li>
<li>Summary: <p>Classifier guidance -- using the gradients of an image classifier to steer
the generations of a diffusion model -- has the potential to dramatically
expand the creative control over image generation and editing. However,
currently classifier guidance requires either training new noise-aware models
to obtain accurate gradients or using a one-step denoising approximation of the
final generation, which leads to misaligned gradients and sub-optimal control.
We highlight this approximation's shortcomings and propose a novel guidance
method: Direct Optimization of Diffusion Latents (DOODL), which enables
plug-and-play guidance by optimizing diffusion latents w.r.t. the gradients of
a pre-trained classifier on the true generated pixels, using an invertible
diffusion process to achieve memory-efficient backpropagation. Showcasing the
potential of more precise guidance, DOODL outperforms one-step classifier
guidance on computational and human evaluation metrics across different forms
of guidance: using CLIP guidance to improve generations of complex prompts from
DrawBench, using fine-grained visual classifiers to expand the vocabulary of
Stable Diffusion, enabling image-conditioned generation with a CLIP visual
encoder, and improving image aesthetics using an aesthetic scoring network.
</p></li>
</ul>

<h3>Title: Conditional Image-to-Video Generation with Latent Flow Diffusion Models. (arXiv:2303.13744v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13744">http://arxiv.org/abs/2303.13744</a></li>
<li>Code URL: <a href="https://github.com/nihaomiao/cvpr23_lfdm">https://github.com/nihaomiao/cvpr23_lfdm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13744] Conditional Image-to-Video Generation with Latent Flow Diffusion Models](http://arxiv.org/abs/2303.13744) #diffusion</code></li>
<li>Summary: <p>Conditional image-to-video (cI2V) generation aims to synthesize a new
plausible video starting from an image (e.g., a person's face) and a condition
(e.g., an action class label like smile). The key challenge of the cI2V task
lies in the simultaneous generation of realistic spatial appearance and
temporal dynamics corresponding to the given image and condition. In this
paper, we propose an approach for cI2V using novel latent flow diffusion models
(LFDM) that synthesize an optical flow sequence in the latent space based on
the given condition to warp the given image. Compared to previous
direct-synthesis-based works, our proposed LFDM can better synthesize spatial
details and temporal motion by fully utilizing the spatial content of the given
image and warping it in the latent space according to the generated
temporally-coherent flow. The training of LFDM consists of two separate stages:
(1) an unsupervised learning stage to train a latent flow auto-encoder for
spatial content generation, including a flow predictor to estimate latent flow
between pairs of video frames, and (2) a conditional learning stage to train a
3D-UNet-based diffusion model (DM) for temporal latent flow generation. Unlike
previous DMs operating in pixel space or latent feature space that couples
spatial and temporal information, the DM in our LFDM only needs to learn a
low-dimensional latent flow space for motion generation, thus being more
computationally efficient. We conduct comprehensive experiments on multiple
datasets, where LFDM consistently outperforms prior arts. Furthermore, we show
that LFDM can be easily adapted to new domains by simply finetuning the image
decoder. Our code is available at https://github.com/nihaomiao/CVPR23_LFDM.
</p></li>
</ul>

<h3>Title: CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout. (arXiv:2303.13843v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13843">http://arxiv.org/abs/2303.13843</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13843] CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout](http://arxiv.org/abs/2303.13843) #diffusion</code></li>
<li>Summary: <p>Recent research endeavors have shown that combining neural radiance fields
(NeRFs) with pre-trained diffusion models holds great potential for text-to-3D
generation.However, a hurdle is that they often encounter guidance collapse
when rendering complex scenes from multi-object texts. Because the
text-to-image diffusion models are inherently unconstrained, making them less
competent to accurately associate object semantics with specific 3D structures.
To address this issue, we propose a novel framework, dubbed CompoNeRF, that
explicitly incorporates an editable 3D scene layout to provide effective
guidance at the single object (i.e., local) and whole scene (i.e., global)
levels. Firstly, we interpret the multi-object text as an editable 3D scene
layout containing multiple local NeRFs associated with the object-specific 3D
box coordinates and text prompt, which can be easily collected from users.
Then, we introduce a global MLP to calibrate the compositional latent features
from local NeRFs, which surprisingly improves the view consistency across
different local NeRFs. Lastly, we apply the text guidance on global and local
levels through their corresponding views to avoid guidance ambiguity. This way,
our CompoNeRF allows for flexible scene editing and re-composition of trained
local NeRFs into a new scene by manipulating the 3D layout or text prompt.
Leveraging the open-source Stable Diffusion model, our CompoNeRF can generate
faithful and editable text-to-3D results while opening a potential direction
for text-guided multi-object composition via the editable 3D scene layout.
</p></li>
</ul>

<h3>Title: Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. (arXiv:2303.13873v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13873">http://arxiv.org/abs/2303.13873</a></li>
<li>Code URL: <a href="https://github.com/Gorilla-Lab-SCUT/Fantasia3D">https://github.com/Gorilla-Lab-SCUT/Fantasia3D</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13873] Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation](http://arxiv.org/abs/2303.13873) #diffusion</code></li>
<li>Summary: <p>Automatic 3D content creation has achieved rapid progress recently due to the
availability of pre-trained, large language models and image diffusion models,
forming the emerging topic of text-to-3D content creation. Existing text-to-3D
methods commonly use implicit scene representations, which couple the geometry
and appearance via volume rendering and are suboptimal in terms of recovering
finer geometries and achieving photorealistic rendering; consequently, they are
less effective for generating high-quality 3D assets. In this work, we propose
a new method of Fantasia3D for high-quality text-to-3D content creation. Key to
Fantasia3D is the disentangled modeling and learning of geometry and
appearance. For geometry learning, we rely on a hybrid scene representation,
and propose to encode surface normal extracted from the representation as the
input of the image diffusion model. For appearance modeling, we introduce the
spatially varying bidirectional reflectance distribution function (BRDF) into
the text-to-3D task, and learn the surface material for photorealistic
rendering of the generated surface. Our disentangled framework is more
compatible with popular graphics engines, supporting relighting, editing, and
physical simulation of the generated 3D assets. We conduct thorough experiments
that show the advantages of our method over existing ones under different
text-to-3D task settings. Project page and source codes:
https://fantasia3d.github.io/.
</p></li>
</ul>

<h3>Title: CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images. (arXiv:2303.14126v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14126">http://arxiv.org/abs/2303.14126</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14126] CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images](http://arxiv.org/abs/2303.14126) #diffusion</code></li>
<li>Summary: <p>Recent technological advances in synthetic data have enabled the generation
of images with such high quality that human beings cannot tell the difference
between real-life photographs and Artificial Intelligence (AI) generated
images. Given the critical necessity of data reliability and authentication,
this article proposes to enhance our ability to recognise AI-generated images
through computer vision. Initially, a synthetic dataset is generated that
mirrors the ten classes of the already available CIFAR-10 dataset with latent
diffusion which provides a contrasting set of images for comparison to real
photographs. The model is capable of generating complex visual attributes, such
as photorealistic reflections in water. The two sets of data present as a
binary classification problem with regard to whether the photograph is real or
generated by AI. This study then proposes the use of a Convolutional Neural
Network (CNN) to classify the images into two categories; Real or Fake.
Following hyperparameter tuning and the training of 36 individual network
topologies, the optimal approach could correctly classify the images with
92.98% accuracy. Finally, this study implements explainable AI via Gradient
Class Activation Mapping to explore which features within the images are useful
for classification. Interpretation reveals interesting concepts within the
image, in particular, noting that the actual entity itself does not hold useful
information for classification; instead, the model focuses on small visual
imperfections in the background of the images. The complete dataset engineered
for this study, referred to as the CIFAKE dataset, is made publicly available
to the research community for future work.
</p></li>
</ul>

<h3>Title: MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion. (arXiv:2303.14139v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14139">http://arxiv.org/abs/2303.14139</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14139] MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion](http://arxiv.org/abs/2303.14139) #diffusion</code></li>
<li>Summary: <p>Reconstructing visual stimuli from measured functional magnetic resonance
imaging (fMRI) has been a meaningful and challenging task. Previous studies
have successfully achieved reconstructions with structures similar to the
original images, such as the outlines and size of some natural images. However,
these reconstructions lack explicit semantic information and are difficult to
discern. In recent years, many studies have utilized multi-modal pre-trained
models with stronger generative capabilities to reconstruct images that are
semantically similar to the original ones. However, these images have
uncontrollable structural information such as position and orientation. To
address both of the aforementioned issues simultaneously, we propose a
two-stage image reconstruction model called MindDiffuser, utilizing Stable
Diffusion. In Stage 1, the VQ-VAE latent representations and the CLIP text
embeddings decoded from fMRI are put into the image-to-image process of Stable
Diffusion, which yields a preliminary image that contains semantic and
structural information. In Stage 2, we utilize the low-level CLIP visual
features decoded from fMRI as supervisory information, and continually adjust
the two features in Stage 1 through backpropagation to align the structural
information. The results of both qualitative and quantitative analyses
demonstrate that our proposed model has surpassed the current state-of-the-art
models in terms of reconstruction results on Natural Scenes Dataset (NSD).
Furthermore, the results of ablation experiments indicate that each component
of our model is effective for image reconstruction.
</p></li>
</ul>

<h3>Title: Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior. (arXiv:2303.14184v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14184">http://arxiv.org/abs/2303.14184</a></li>
<li>Code URL: <a href="https://github.com/junshutang/Make-It-3D">https://github.com/junshutang/Make-It-3D</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14184] Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior](http://arxiv.org/abs/2303.14184) #diffusion</code></li>
<li>Summary: <p>In this work, we investigate the problem of creating high-fidelity 3D content
from only a single image. This is inherently challenging: it essentially
involves estimating the underlying 3D geometry while simultaneously
hallucinating unseen textures. To address this challenge, we leverage prior
knowledge from a well-trained 2D diffusion model to act as 3D-aware supervision
for 3D creation. Our approach, Make-It-3D, employs a two-stage optimization
pipeline: the first stage optimizes a neural radiance field by incorporating
constraints from the reference image at the frontal view and diffusion prior at
novel views; the second stage transforms the coarse model into textured point
clouds and further elevates the realism with diffusion prior while leveraging
the high-quality textures from the reference image. Extensive experiments
demonstrate that our method outperforms prior works by a large margin,
resulting in faithful reconstructions and impressive visual quality. Our method
presents the first attempt to achieve high-quality 3D creation from a single
image for general objects and enables various applications such as text-to-3D
creation and texture editing.
</p></li>
</ul>

<h3>Title: Enhancing Unsupervised Speech Recognition with Diffusion GANs. (arXiv:2303.13559v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.13559">http://arxiv.org/abs/2303.13559</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.13559] Enhancing Unsupervised Speech Recognition with Diffusion GANs](http://arxiv.org/abs/2303.13559) #diffusion</code></li>
<li>Summary: <p>We enhance the vanilla adversarial training method for unsupervised Automatic
Speech Recognition (ASR) by a diffusion-GAN. Our model (1) injects instance
noises of various intensities to the generator's output and unlabeled reference
text which are sampled from pretrained phoneme language models with a length
constraint, (2) asks diffusion timestep-dependent discriminators to separate
them, and (3) back-propagates the gradients to update the generator.
Word/phoneme error rate comparisons with wav2vec-U under Librispeech (3.1% for
test-clean and 5.6% for test-other), TIMIT and MLS datasets, show that our
enhancement strategies work effectively.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
