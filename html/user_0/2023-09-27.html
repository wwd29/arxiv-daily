<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: DECORAIT -- DECentralized Opt-in/out Registry for AI Training. (arXiv:2309.14400v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14400">http://arxiv.org/abs/2309.14400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14400]] DECORAIT -- DECentralized Opt-in/out Registry for AI Training(http://arxiv.org/abs/2309.14400)</code></li>
<li>Summary: <p>We present DECORAIT; a decentralized registry through which content creators
may assert their right to opt in or out of AI training as well as receive
reward for their contributions. Generative AI (GenAI) enables images to be
synthesized using AI models trained on vast amounts of data scraped from public
sources. Model and content creators who may wish to share their work openly
without sanctioning its use for training are thus presented with a data
governance challenge. Further, establishing the provenance of GenAI training
data is important to creatives to ensure fair recognition and reward for their
such use. We report a prototype of DECORAIT, which explores hierarchical
clustering and a combination of on/off-chain storage to create a scalable
decentralized registry to trace the provenance of GenAI training data in order
to determine training consent and reward creatives who contribute that data.
DECORAIT combines distributed ledger technology (DLT) with visual
fingerprinting, leveraging the emerging C2PA (Coalition for Content Provenance
and Authenticity) standard to create a secure, open registry through which
creatives may express consent and data ownership for GenAI.
</p></li>
</ul>

<h3>Title: A Public Key Infrastructure for 5G Service-Based Architecture. (arXiv:2309.14659v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14659">http://arxiv.org/abs/2309.14659</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14659]] A Public Key Infrastructure for 5G Service-Based Architecture(http://arxiv.org/abs/2309.14659)</code></li>
<li>Summary: <p>The 3GPP 5G Service-based Architecture (SBA) security specifications leave
several details on how to setup an appropriate Public Key Infrastructure (PKI)
for 5G SBA, unspecified. In this work, we propose 5G-SBA-PKI, a public key
infrastructure for secure inter-NF communication in 5G SBA core networks, where
NF refers to Network Functions. 5G-SBA-PKI is designed to include multiple
certificate authorities (with different scopes of operation and capabilities)
at different PLMN levels for certification operations and key exchange between
communicating NFs, where PLMN refers to a Public Land Mobile Network. We
conduct a formal analysis of 5G-SBA-PKI with respect to the desired security
properties using TAMARIN prover. Finally, we evaluate 5G-SBA-PKI's performance
with "pre-quantum" as well as quantum-safe cryptographic algorithms.
</p></li>
</ul>

<h3>Title: SyzTrust: State-aware Fuzzing on Trusted OS Designed for IoT Devices. (arXiv:2309.14742v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14742">http://arxiv.org/abs/2309.14742</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14742]] SyzTrust: State-aware Fuzzing on Trusted OS Designed for IoT Devices(http://arxiv.org/abs/2309.14742)</code></li>
<li>Summary: <p>Trusted Execution Environments (TEEs) embedded in IoT devices provide a
deployable solution to secure IoT applications at the hardware level. By
design, in TEEs, the Trusted Operating System (Trusted OS) is the primary
component. It enables the TEE to use security-based design techniques, such as
data encryption and identity authentication. Once a Trusted OS has been
exploited, the TEE can no longer ensure security. However, Trusted OSes for IoT
devices have received little security analysis, which is challenging from
several perspectives: (1) Trusted OSes are closed-source and have an
unfavorable environment for sending test cases and collecting feedback. (2)
Trusted OSes have complex data structures and require a stateful workflow,
which limits existing vulnerability detection tools. To address the challenges,
we present SyzTrust, the first state-aware fuzzing framework for vetting the
security of resource-limited Trusted OSes. SyzTrust adopts a hardware-assisted
framework to enable fuzzing Trusted OSes directly on IoT devices as well as
tracking state and code coverage non-invasively. SyzTrust utilizes composite
feedback to guide the fuzzer to effectively explore more states as well as to
increase the code coverage. We evaluate SyzTrust on Trusted OSes from three
major vendors: Samsung, Tsinglink Cloud, and Ali Cloud. These systems run on
Cortex M23/33 MCUs, which provide the necessary abstraction for embedded TEEs.
We discovered 70 previously unknown vulnerabilities in their Trusted OSes,
receiving 10 new CVEs so far. Furthermore, compared to the baseline, SyzTrust
has demonstrated significant improvements, including 66% higher code coverage,
651% higher state coverage, and 31% improved vulnerability-finding capability.
We report all discovered new vulnerabilities to vendors and open source
SyzTrust.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Incorporating Ensemble and Transfer Learning For An End-To-End Auto-Colorized Image Detection Model. (arXiv:2309.14478v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14478">http://arxiv.org/abs/2309.14478</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14478]] Incorporating Ensemble and Transfer Learning For An End-To-End Auto-Colorized Image Detection Model(http://arxiv.org/abs/2309.14478)</code></li>
<li>Summary: <p>Image colorization is the process of colorizing grayscale images or
recoloring an already-color image. This image manipulation can be used for
grayscale satellite, medical and historical images making them more expressive.
With the help of the increasing computation power of deep learning techniques,
the colorization algorithms results are becoming more realistic in such a way
that human eyes cannot differentiate between natural and colorized images.
However, this poses a potential security concern, as forged or illegally
manipulated images can be used illegally. There is a growing need for effective
detection methods to distinguish between natural color and computer-colorized
images. This paper presents a novel approach that combines the advantages of
transfer and ensemble learning approaches to help reduce training time and
resource requirements while proposing a model to classify natural color and
computer-colorized images. The proposed model uses pre-trained branches VGG16
and Resnet50, along with Mobile Net v2 or Efficientnet feature vectors. The
proposed model showed promising results, with accuracy ranging from 94.55% to
99.13% and very low Half Total Error Rate values. The proposed model
outperformed existing state-of-the-art models regarding classification
performance and generalization capabilities.
</p></li>
</ul>

<h3>Title: Structure Invariant Transformation for better Adversarial Transferability. (arXiv:2309.14700v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14700">http://arxiv.org/abs/2309.14700</a></li>
<li>Code URL: https://github.com/xiaosen-wang/sit</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14700]] Structure Invariant Transformation for better Adversarial Transferability(http://arxiv.org/abs/2309.14700)</code></li>
<li>Summary: <p>Given the severe vulnerability of Deep Neural Networks (DNNs) against
adversarial examples, there is an urgent need for an effective adversarial
attack to identify the deficiencies of DNNs in security-sensitive applications.
As one of the prevalent black-box adversarial attacks, the existing
transfer-based attacks still cannot achieve comparable performance with the
white-box attacks. Among these, input transformation based attacks have shown
remarkable effectiveness in boosting transferability. In this work, we find
that the existing input transformation based attacks transform the input image
globally, resulting in limited diversity of the transformed images. We
postulate that the more diverse transformed images result in better
transferability. Thus, we investigate how to locally apply various
transformations onto the input image to improve such diversity while preserving
the structure of image. To this end, we propose a novel input transformation
based attack, called Structure Invariant Attack (SIA), which applies a random
image transformation onto each image block to craft a set of diverse images for
gradient calculation. Extensive experiments on the standard ImageNet dataset
demonstrate that SIA exhibits much better transferability than the existing
SOTA input transformation based attacks on CNN-based and transformer-based
models, showing its generality and superiority in boosting transferability.
Code is available at https://github.com/xiaosen-wang/SIT.
</p></li>
</ul>

<h3>Title: Logic Locking based Trojans: A Friend Turns Foe. (arXiv:2309.15067v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15067">http://arxiv.org/abs/2309.15067</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15067]] Logic Locking based Trojans: A Friend Turns Foe(http://arxiv.org/abs/2309.15067)</code></li>
<li>Summary: <p>Logic locking and hardware Trojans are two fields in hardware security that
have been mostly developed independently from each other. In this paper, we
identify the relationship between these two fields. We find that a common
structure that exists in many logic locking techniques has desirable properties
of hardware Trojans (HWT). We then construct a novel type of HWT, called
Trojans based on Logic Locking (TroLL), in a way that can evade
state-of-the-art ATPG-based HWT detection techniques. In an effort to detect
TroLL, we propose customization of existing state-of-the-art ATPG-based HWT
detection approaches as well as adapting the SAT-based attacks on logic locking
to HWT detection. In our experiments, we use random sampling as reference. It
is shown that the customized ATPG-based approaches are the best performing but
only offer limited improvement over random sampling. Moreover, their efficacy
also diminishes as TroLL's triggers become longer, i.e., have more bits
specified). We thereby highlight the need to find a scalable HWT detection
approach for TroLL.
</p></li>
</ul>

<h3>Title: LogGPT: Log Anomaly Detection via GPT. (arXiv:2309.14482v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14482">http://arxiv.org/abs/2309.14482</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14482]] LogGPT: Log Anomaly Detection via GPT(http://arxiv.org/abs/2309.14482)</code></li>
<li>Summary: <p>Detecting system anomalies based on log data is important for ensuring the
security and reliability of computer systems. Recently, deep learning models
have been widely used for log anomaly detection. The core idea is to model the
log sequences as natural language and adopt deep sequential models, such as
LSTM or Transformer, to encode the normal patterns in log sequences via
language modeling. However, there is a gap between language modeling and
anomaly detection as the objective of training a sequential model via a
language modeling loss is not directly related to anomaly detection. To fill up
the gap, we propose LogGPT, a novel framework that employs GPT for log anomaly
detection. LogGPT is first trained to predict the next log entry based on the
preceding sequence. To further enhance the performance of LogGPT, we propose a
novel reinforcement learning strategy to finetune the model specifically for
the log anomaly detection task. The experimental results on three datasets show
that LogGPT significantly outperforms existing state-of-the-art approaches.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: PLMM: Personal Large Models on Mobile Devices. (arXiv:2309.14726v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14726">http://arxiv.org/abs/2309.14726</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14726]] PLMM: Personal Large Models on Mobile Devices(http://arxiv.org/abs/2309.14726)</code></li>
<li>Summary: <p>Inspired by Federated Learning, in this paper, we propose personal large
models that are distilled from traditional large language models but more
adaptive to local users' personal information such as education background and
hobbies. We classify the large language models into three levels: the personal
level, expert level and traditional level. The personal level models are
adaptive to users' personal information. They encrypt the users' input and
protect their privacy. The expert level models focus on merging specific
knowledge such as finance, IT and art. The traditional models focus on the
universal knowledge discovery and upgrading the expert models. In such
classifications, the personal models directly interact with the user. For the
whole system, the personal models have users' (encrypted) personal information.
Moreover, such models must be small enough to be performed on personal
computers or mobile devices. Finally, they also have to response in real-time
for better user experience and produce high quality results. The proposed
personal large models can be applied in a wide range of applications such as
language and vision tasks.
</p></li>
</ul>

<h3>Title: Concept and Construction of Group Signature with self-proof capacity for confirming and denying. (arXiv:2309.14635v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14635">http://arxiv.org/abs/2309.14635</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14635]] Concept and Construction of Group Signature with self-proof capacity for confirming and denying(http://arxiv.org/abs/2309.14635)</code></li>
<li>Summary: <p>With privacy-preserving and traceability properties, group signature is a
cryptosystem with central role in cryptography. And there are lots of
application scenarios. A new extension concept of group signature is presented,
namely group signature with self-proof capacity. For a legitimate group
signature, the real signer can prove that the signature is indeed signed by
him/her. While for the other members of the group, they can prove that the
signature is not signed by him/her. The former can be used for claiming money
reward from the police, while the latter can be used for proving one's innocent
in a criminal investigation.
</p></li>
</ul>

<h3>Title: A Quantitative Information Flow Analysis of the Topics API. (arXiv:2309.14746v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14746">http://arxiv.org/abs/2309.14746</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14746]] A Quantitative Information Flow Analysis of the Topics API(http://arxiv.org/abs/2309.14746)</code></li>
<li>Summary: <p>Third-party cookies have been a privacy concern since cookies were first
developed in the mid 1990s, but more strict cookie policies were only
introduced by Internet browser vendors in the early 2010s. More recently, due
to regulatory changes, browser vendors have started to completely block
third-party cookies, with both Firefox and Safari already compliant.
</p>
<p>The Topics API is being proposed by Google as an additional and less
intrusive source of information for interest-based advertising (IBA), following
the upcoming deprecation of third-party cookies. Initial results published by
Google estimate the probability of a correct re-identification of a random
individual would be below 3% while still supporting IBA.
</p>
<p>In this paper, we analyze the re-identification risk for individual Internet
users introduced by the Topics API from the perspective of Quantitative
Information Flow (QIF), an information- and decision-theoretic framework. Our
model allows a theoretical analysis of both privacy and utility aspects of the
API and their trade-off, and we show that the Topics API does have better
privacy than third-party cookies. We leave the utility analyses for future
work.
</p></li>
</ul>

<h3>Title: Privacy-preserving and Privacy-attacking Approaches for Speech and Audio -- A Survey. (arXiv:2309.15087v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15087">http://arxiv.org/abs/2309.15087</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15087]] Privacy-preserving and Privacy-attacking Approaches for Speech and Audio -- A Survey(http://arxiv.org/abs/2309.15087)</code></li>
<li>Summary: <p>In contemporary society, voice-controlled devices, such as smartphones and
home assistants, have become pervasive due to their advanced capabilities and
functionality. The always-on nature of their microphones offers users the
convenience of readily accessing these devices. However, recent research and
events have revealed that such voice-controlled devices are prone to various
forms of malicious attacks, hence making it a growing concern for both users
and researchers to safeguard against such attacks. Despite the numerous studies
that have investigated adversarial attacks and privacy preservation for images,
a conclusive study of this nature has not been conducted for the audio domain.
Therefore, this paper aims to examine existing approaches for
privacy-preserving and privacy-attacking strategies for audio and speech. To
achieve this goal, we classify the attack and defense scenarios into several
categories and provide detailed analysis of each approach. We also interpret
the dissimilarities between the various approaches, highlight their
contributions, and examine their limitations. Our investigation reveals that
voice-controlled devices based on neural networks are inherently susceptible to
specific types of attacks. Although it is possible to enhance the robustness of
such models to certain forms of attack, more sophisticated approaches are
required to comprehensively safeguard user privacy.
</p></li>
</ul>

<h3>Title: Parallel Multi-Objective Hyperparameter Optimization with Uniform Normalization and Bounded Objectives. (arXiv:2309.14936v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14936">http://arxiv.org/abs/2309.14936</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14936]] Parallel Multi-Objective Hyperparameter Optimization with Uniform Normalization and Bounded Objectives(http://arxiv.org/abs/2309.14936)</code></li>
<li>Summary: <p>Machine learning (ML) methods offer a wide range of configurable
hyperparameters that have a significant influence on their performance. While
accuracy is a commonly used performance objective, in many settings, it is not
sufficient. Optimizing the ML models with respect to multiple objectives such
as accuracy, confidence, fairness, calibration, privacy, latency, and memory
consumption is becoming crucial. To that end, hyperparameter optimization, the
approach to systematically optimize the hyperparameters, which is already
challenging for a single objective, is even more challenging for multiple
objectives. In addition, the differences in objective scales, the failures, and
the presence of outlier values in objectives make the problem even harder. We
propose a multi-objective Bayesian optimization (MoBO) algorithm that addresses
these problems through uniform objective normalization and randomized weights
in scalarization. We increase the efficiency of our approach by imposing
constraints on the objective to avoid exploring unnecessary configurations
(e.g., insufficient accuracy). Finally, we leverage an approach to parallelize
the MoBO which results in a 5x speed-up when using 16x more workers.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature Space. (arXiv:2309.14585v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14585">http://arxiv.org/abs/2309.14585</a></li>
<li>Code URL: https://github.com/csjunjun/difattack</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14585]] DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature Space(http://arxiv.org/abs/2309.14585)</code></li>
<li>Summary: <p>This work investigates efficient score-based black-box adversarial attacks
with a high Attack Success Rate (ASR) and good generalizability. We design a
novel attack method based on a Disentangled Feature space, called DifAttack,
which differs significantly from the existing ones operating over the entire
feature space. Specifically, DifAttack firstly disentangles an image's latent
feature into an adversarial feature and a visual feature, where the former
dominates the adversarial capability of an image, while the latter largely
determines its visual appearance. We train an autoencoder for the
disentanglement by using pairs of clean images and their Adversarial Examples
(AEs) generated from available surrogate models via white-box attack methods.
Eventually, DifAttack iteratively optimizes the adversarial feature according
to the query feedback from the victim model until a successful AE is generated,
while keeping the visual feature unaltered. In addition, due to the avoidance
of using surrogate models' gradient information when optimizing AEs for
black-box models, our proposed DifAttack inherently possesses better attack
capability in the open-set scenario, where the training dataset of the victim
model is unknown. Extensive experimental results demonstrate that our method
achieves significant improvements in ASR and query efficiency simultaneously,
especially in the targeted attack and open-set scenarios. The code will be
available at https://github.com/csjunjun/DifAttack.git soon.
</p></li>
</ul>

<h3>Title: 3D printed realistic finger vein phantoms. (arXiv:2309.14806v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14806">http://arxiv.org/abs/2309.14806</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14806]] 3D printed realistic finger vein phantoms(http://arxiv.org/abs/2309.14806)</code></li>
<li>Summary: <p>Finger vein pattern recognition is an emerging biometric with a good
resistance to presentation attacks and low error rates. One problem is that it
is hard to obtain ground truth finger vein patterns from live fingers. In this
paper we propose an advanced method to create finger vein phantoms using 3D
printing where we mimic the optical properties of the various tissues inside
the fingers, like bone, veins and soft tissues using different printing
materials and parameters. We demonstrate that we are able to create finger
phantoms that result in realistic finger vein images and precisely known vein
patterns. These phantoms can be used to develop and evaluate finger vein
extraction and recognition methods. In addition, we show that the finger vein
phantoms can be used to spoof a finger vein recognition system. This paper is
based on the Master's thesis of Rasmus van der Grift.
</p></li>
</ul>

<h3>Title: Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM. (arXiv:2309.14348v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14348">http://arxiv.org/abs/2309.14348</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14348]] Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM(http://arxiv.org/abs/2309.14348)</code></li>
<li>Summary: <p>Recently, Large Language Models (LLMs) have made significant advancements and
are now widely used across various domains. Unfortunately, there has been a
rising concern that LLMs can be misused to generate harmful or malicious
content. Though a line of research has focused on aligning LLMs with human
values and preventing them from producing inappropriate content, such
alignments are usually vulnerable and can be bypassed by alignment-breaking
attacks via adversarially optimized or handcrafted jailbreaking prompts. In
this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against
potential alignment-breaking attacks. RA-LLM can be directly constructed upon
an existing aligned LLM with a robust alignment checking function, without
requiring any expensive retraining or fine-tuning process of the original LLM.
Furthermore, we also provide a theoretical analysis for RA-LLM to verify its
effectiveness in defending against alignment-breaking attacks. Through
real-world experiments on open-source large language models, we demonstrate
that RA-LLM can successfully defend against both state-of-the-art adversarial
prompts and popular handcrafted jailbreaking prompts by reducing their attack
success rates from nearly 100\% to around 10\% or less.
</p></li>
</ul>

<h3>Title: ADESS: A Proof-of-Work Protocol to Deter Double-Spend Attacks. (arXiv:2309.14551v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14551">http://arxiv.org/abs/2309.14551</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14551]] ADESS: A Proof-of-Work Protocol to Deter Double-Spend Attacks(http://arxiv.org/abs/2309.14551)</code></li>
<li>Summary: <p>A principal vulnerability of a proof-of-work ("PoW") blockchain is that an
attacker can re-write the history of transactions by forking a previously
published block and build a new chain segment containing a different sequence
of transactions. If the attacker's chain has the most cumulative mining puzzle
difficulty, nodes will recognize it as canonical. We propose a modification to
PoW protocols, called ADESS, that contains two novel features. The first
modification enables a node to identify the attacker chain by comparing the
temporal sequence of blocks on competing chains. The second modification
penalizes the attacker by requiring it to apply exponentially increasing
hashrate in order to make its chain canonical. We demonstrate two things; (i)
the expected cost of carrying out a double-spend attack is weakly higher under
ADESS compared to the current PoW protocols and (ii) for any value of
transaction, there is a penalty setting in ADESS that renders the expected
profit of a double-spend attack negative.
</p></li>
</ul>

<h3>Title: Gray-box Adversarial Attack of Deep Reinforcement Learning-based Trading Agents. (arXiv:2309.14615v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14615">http://arxiv.org/abs/2309.14615</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14615]] Gray-box Adversarial Attack of Deep Reinforcement Learning-based Trading Agents(http://arxiv.org/abs/2309.14615)</code></li>
<li>Summary: <p>In recent years, deep reinforcement learning (Deep RL) has been successfully
implemented as a smart agent in many systems such as complex games,
self-driving cars, and chat-bots. One of the interesting use cases of Deep RL
is its application as an automated stock trading agent. In general, any
automated trading agent is prone to manipulations by adversaries in the trading
environment. Thus studying their robustness is vital for their success in
practice. However, typical mechanism to study RL robustness, which is based on
white-box gradient-based adversarial sample generation techniques (like FGSM),
is obsolete for this use case, since the models are protected behind secure
international exchange APIs, such as NASDAQ. In this research, we demonstrate
that a "gray-box" approach for attacking a Deep RL-based trading agent is
possible by trading in the same stock market, with no extra access to the
trading agent. In our proposed approach, an adversary agent uses a hybrid Deep
Neural Network as its policy consisting of Convolutional layers and
fully-connected layers. On average, over three simulated trading market
configurations, the adversary policy proposed in this research is able to
reduce the reward values by 214.17%, which results in reducing the potential
profits of the baseline by 139.4%, ensemble method by 93.7%, and an automated
trading software developed by our industrial partner by 85.5%, while consuming
significantly less budget than the victims (427.77%, 187.16%, and 66.97%,
respectively).
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs. (arXiv:2309.14356v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14356">http://arxiv.org/abs/2309.14356</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14356]] COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs(http://arxiv.org/abs/2309.14356)</code></li>
<li>Summary: <p>Counterfactual examples have proven to be valuable in the field of natural
language processing (NLP) for both evaluating and improving the robustness of
language models to spurious correlations in datasets. Despite their
demonstrated utility for NLP, multimodal counterfactual examples have been
relatively unexplored due to the difficulty of creating paired image-text data
with minimal counterfactual changes. To address this challenge, we introduce a
scalable framework for automatic generation of counterfactual examples using
text-to-image diffusion models. We use our framework to create
COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and
text captions based on the MS-COCO dataset. We validate the quality of
COCO-Counterfactuals through human evaluations and show that existing
multimodal models are challenged by our counterfactual image-text pairs.
Additionally, we demonstrate the usefulness of COCO-Counterfactuals for
improving out-of-domain generalization of multimodal vision-language models via
training data augmentation.
</p></li>
</ul>

<h3>Title: FARSEC: A Reproducible Framework for Automatic Real-Time Vehicle Speed Estimation Using Traffic Cameras. (arXiv:2309.14468v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14468">http://arxiv.org/abs/2309.14468</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14468]] FARSEC: A Reproducible Framework for Automatic Real-Time Vehicle Speed Estimation Using Traffic Cameras(http://arxiv.org/abs/2309.14468)</code></li>
<li>Summary: <p>Estimating the speed of vehicles using traffic cameras is a crucial task for
traffic surveillance and management, enabling more optimal traffic flow,
improved road safety, and lower environmental impact. Transportation-dependent
systems, such as for navigation and logistics, have great potential to benefit
from reliable speed estimation. While there is prior research in this area
reporting competitive accuracy levels, their solutions lack reproducibility and
robustness across different datasets. To address this, we provide a novel
framework for automatic real-time vehicle speed calculation, which copes with
more diverse data from publicly available traffic cameras to achieve greater
robustness. Our model employs novel techniques to estimate the length of road
segments via depth map prediction. Additionally, our framework is capable of
handling realistic conditions such as camera movements and different video
stream inputs automatically. We compare our model to three well-known models in
the field using their benchmark datasets. While our model does not set a new
state of the art regarding prediction performance, the results are competitive
on realistic CCTV videos. At the same time, our end-to-end pipeline offers more
consistent results, an easier implementation, and better compatibility. Its
modular structure facilitates reproducibility and future improvements.
</p></li>
</ul>

<h3>Title: UniBEV: Multi-modal 3D Object Detection with Uniform BEV Encoders for Robustness against Missing Sensor Modalities. (arXiv:2309.14516v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14516">http://arxiv.org/abs/2309.14516</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14516]] UniBEV: Multi-modal 3D Object Detection with Uniform BEV Encoders for Robustness against Missing Sensor Modalities(http://arxiv.org/abs/2309.14516)</code></li>
<li>Summary: <p>Multi-sensor object detection is an active research topic in automated
driving, but the robustness of such detection models against missing sensor
input (modality missing), e.g., due to a sudden sensor failure, is a critical
problem which remains under-studied. In this work, we propose UniBEV, an
end-to-end multi-modal 3D object detection framework designed for robustness
against missing modalities: UniBEV can operate on LiDAR plus camera input, but
also on LiDAR-only or camera-only input without retraining. To facilitate its
detector head to handle different input combinations, UniBEV aims to create
well-aligned Bird's Eye View (BEV) feature maps from each available modality.
Unlike prior BEV-based multi-modal detection methods, all sensor modalities
follow a uniform approach to resample features from the native sensor
coordinate systems to the BEV features. We furthermore investigate the
robustness of various fusion strategies w.r.t. missing modalities: the commonly
used feature concatenation, but also channel-wise averaging, and a
generalization to weighted averaging termed Channel Normalized Weights. To
validate its effectiveness, we compare UniBEV to state-of-the-art BEVFusion and
MetaBEV on nuScenes over all sensor input combinations. In this setting, UniBEV
achieves $52.5 \%$ mAP on average over all input combinations, significantly
improving over the baselines ($43.5 \%$ mAP on average for BEVFusion, $48.7 \%$
mAP on average for MetaBEV). An ablation study shows the robustness benefits of
fusing by weighted averaging over regular concatenation, and of sharing queries
between the BEV encoders of each modality. Our code will be released upon paper
acceptance.
</p></li>
</ul>

<h3>Title: Dynamic Scene Graph Representation for Surgical Video. (arXiv:2309.14538v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14538">http://arxiv.org/abs/2309.14538</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14538]] Dynamic Scene Graph Representation for Surgical Video(http://arxiv.org/abs/2309.14538)</code></li>
<li>Summary: <p>Surgical videos captured from microscopic or endoscopic imaging devices are
rich but complex sources of information, depicting different tools and
anatomical structures utilized during an extended amount of time. Despite
containing crucial workflow information and being commonly recorded in many
procedures, usage of surgical videos for automated surgical workflow
understanding is still limited.
</p>
<p>In this work, we exploit scene graphs as a more holistic, semantically
meaningful and human-readable way to represent surgical videos while encoding
all anatomical structures, tools, and their interactions. To properly evaluate
the impact of our solutions, we create a scene graph dataset from semantic
segmentations from the CaDIS and CATARACTS datasets. We demonstrate that scene
graphs can be leveraged through the use of graph convolutional networks (GCNs)
to tackle surgical downstream tasks such as surgical workflow recognition with
competitive performance. Moreover, we demonstrate the benefits of surgical
scene graphs regarding the explainability and robustness of model decisions,
which are crucial in the clinical setting.
</p></li>
</ul>

<h3>Title: Advanced Volleyball Stats for All Levels: Automatic Setting Tactic Detection and Classification with a Single Camera. (arXiv:2309.14753v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14753">http://arxiv.org/abs/2309.14753</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14753]] Advanced Volleyball Stats for All Levels: Automatic Setting Tactic Detection and Classification with a Single Camera(http://arxiv.org/abs/2309.14753)</code></li>
<li>Summary: <p>This paper presents PathFinder and PathFinderPlus, two novel end-to-end
computer vision frameworks designed specifically for advanced setting strategy
classification in volleyball matches from a single camera view. Our frameworks
combine setting ball trajectory recognition with a novel set trajectory
classifier to generate comprehensive and advanced statistical data. This
approach offers a fresh perspective for in-game analysis and surpasses the
current level of granularity in volleyball statistics. In comparison to
existing methods used in our baseline PathFinder framework, our proposed ball
trajectory detection methodology in PathFinderPlus exhibits superior
performance for classifying setting tactics under various game conditions. This
robustness is particularly advantageous in handling complex game situations and
accommodating different camera angles. Additionally, our study introduces an
innovative algorithm for automatic identification of the opposing team's
right-side (opposite) hitter's current row (front or back) during gameplay,
providing critical insights for tactical analysis. The successful demonstration
of our single-camera system's feasibility and benefits makes high-level
technical analysis accessible to volleyball enthusiasts of all skill levels and
resource availability. Furthermore, the computational efficiency of our system
allows for real-time deployment, enabling in-game strategy analysis and
on-the-spot gameplan adjustments.
</p></li>
</ul>

<h3>Title: A Comparative Study of Population-Graph Construction Methods and Graph Neural Networks for Brain Age Regression. (arXiv:2309.14816v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14816">http://arxiv.org/abs/2309.14816</a></li>
<li>Code URL: https://github.com/bintsi/brain-age-population-graphs</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14816]] A Comparative Study of Population-Graph Construction Methods and Graph Neural Networks for Brain Age Regression(http://arxiv.org/abs/2309.14816)</code></li>
<li>Summary: <p>The difference between the chronological and biological brain age of a
subject can be an important biomarker for neurodegenerative diseases, thus
brain age estimation can be crucial in clinical settings. One way to
incorporate multimodal information into this estimation is through population
graphs, which combine various types of imaging data and capture the
associations among individuals within a population. In medical imaging,
population graphs have demonstrated promising results, mostly for
classification tasks. In most cases, the graph structure is pre-defined and
remains static during training. However, extracting population graphs is a
non-trivial task and can significantly impact the performance of Graph Neural
Networks (GNNs), which are sensitive to the graph structure. In this work, we
highlight the importance of a meaningful graph construction and experiment with
different population-graph construction methods and their effect on GNN
performance on brain age estimation. We use the homophily metric and graph
visualizations to gain valuable quantitative and qualitative insights on the
extracted graph structures. For the experimental evaluation, we leverage the UK
Biobank dataset, which offers many imaging and non-imaging phenotypes. Our
results indicate that architectures highly sensitive to the graph structure,
such as Graph Convolutional Network (GCN) and Graph Attention Network (GAT),
struggle with low homophily graphs, while other architectures, such as
GraphSage and Chebyshev, are more robust across different homophily ratios. We
conclude that static graph construction approaches are potentially insufficient
for the task of brain age estimation and make recommendations for alternative
research directions.
</p></li>
</ul>

<h3>Title: Generalization of pixel-wise phase estimation by CNN and improvement of phase-unwrapping by MRF optimization for one-shot 3D scan. (arXiv:2309.14824v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14824">http://arxiv.org/abs/2309.14824</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14824]] Generalization of pixel-wise phase estimation by CNN and improvement of phase-unwrapping by MRF optimization for one-shot 3D scan(http://arxiv.org/abs/2309.14824)</code></li>
<li>Summary: <p>Active stereo technique using single pattern projection, a.k.a. one-shot 3D
scan, have drawn a wide attention from industry, medical purposes, etc. One
severe drawback of one-shot 3D scan is sparse reconstruction. In addition,
since spatial pattern becomes complicated for the purpose of efficient
embedding, it is easily affected by noise, which results in unstable decoding.
To solve the problems, we propose a pixel-wise interpolation technique for
one-shot scan, which is applicable to any types of static pattern if the
pattern is regular and periodic. This is achieved by U-net which is pre-trained
by CG with efficient data augmentation algorithm. In the paper, to further
overcome the decoding instability, we propose a robust correspondence finding
algorithm based on Markov random field (MRF) optimization. We also propose a
shape refinement algorithm based on b-spline and Gaussian kernel interpolation
using explicitly detected laser curves. Experiments are conducted to show the
effectiveness of the proposed method using real data with strong noises and
textures.
</p></li>
</ul>

<h3>Title: Cross-Dataset-Robust Method for Blind Real-World Image Quality Assessment. (arXiv:2309.14868v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14868">http://arxiv.org/abs/2309.14868</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14868]] Cross-Dataset-Robust Method for Blind Real-World Image Quality Assessment(http://arxiv.org/abs/2309.14868)</code></li>
<li>Summary: <p>Although many effective models and real-world datasets have been presented
for blind image quality assessment (BIQA), recent BIQA models usually tend to
fit specific training set. Hence, it is still difficult to accurately and
robustly measure the visual quality of an arbitrary real-world image. In this
paper, a robust BIQA method, is designed based on three aspects, i.e., robust
training strategy, large-scale real-world dataset, and powerful backbone.
First, many individual models based on popular and state-of-the-art (SOTA)
Swin-Transformer (SwinT) are trained on different real-world BIQA datasets
respectively. Then, these biased SwinT-based models are jointly used to
generate pseudo-labels, which adopts the probability of relative quality of two
random images instead of fixed quality score. A large-scale real-world image
dataset with 1,000,000 image pairs and pseudo-labels is then proposed for
training the final cross-dataset-robust model. Experimental results on
cross-dataset tests show that the performance of the proposed method is even
better than some SOTA methods that are directly trained on these datasets, thus
verifying the robustness and generalization of our method.
</p></li>
</ul>

<h3>Title: Locality-preserving Directions for Interpreting the Latent Space of Satellite Image GANs. (arXiv:2309.14883v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14883">http://arxiv.org/abs/2309.14883</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14883]] Locality-preserving Directions for Interpreting the Latent Space of Satellite Image GANs(http://arxiv.org/abs/2309.14883)</code></li>
<li>Summary: <p>We present a locality-aware method for interpreting the latent space of
wavelet-based Generative Adversarial Networks (GANs), that can well capture the
large spatial and spectral variability that is characteristic to satellite
imagery. By focusing on preserving locality, the proposed method is able to
decompose the weight-space of pre-trained GANs and recover interpretable
directions that correspond to high-level semantic concepts (such as
urbanization, structure density, flora presence) - that can subsequently be
used for guided synthesis of satellite imagery. In contrast to typically used
approaches that focus on capturing the variability of the weight-space in a
reduced dimensionality space (i.e., based on Principal Component Analysis,
PCA), we show that preserving locality leads to vectors with different angles,
that are more robust to artifacts and can better preserve class information.
Via a set of quantitative and qualitative examples, we further show that the
proposed approach can outperform both baseline geometric augmentations, as well
as global, PCA-based approaches for data synthesis in the context of data
augmentation for satellite scene classification.
</p></li>
</ul>

<h3>Title: FDLS: A Deep Learning Approach to Production Quality, Controllable, and Retargetable Facial Performances. (arXiv:2309.14897v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14897">http://arxiv.org/abs/2309.14897</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14897]] FDLS: A Deep Learning Approach to Production Quality, Controllable, and Retargetable Facial Performances(http://arxiv.org/abs/2309.14897)</code></li>
<li>Summary: <p>Visual effects commonly requires both the creation of realistic synthetic
humans as well as retargeting actors' performances to humanoid characters such
as aliens and monsters. Achieving the expressive performances demanded in
entertainment requires manipulating complex models with hundreds of parameters.
Full creative control requires the freedom to make edits at any stage of the
production, which prohibits the use of a fully automatic ``black box'' solution
with uninterpretable parameters. On the other hand, producing realistic
animation with these sophisticated models is difficult and laborious. This
paper describes FDLS (Facial Deep Learning Solver), which is Weta Digital's
solution to these challenges. FDLS adopts a coarse-to-fine and
human-in-the-loop strategy, allowing a solved performance to be verified and
edited at several stages in the solving process. To train FDLS, we first
transform the raw motion-captured data into robust graph features. Secondly,
based on the observation that the artists typically finalize the jaw pass
animation before proceeding to finer detail, we solve for the jaw motion first
and predict fine expressions with region-based networks conditioned on the jaw
position. Finally, artists can optionally invoke a non-linear finetuning
process on top of the FDLS solution to follow the motion-captured virtual
markers as closely as possible. FDLS supports editing if needed to improve the
results of the deep learning solution and it can handle small daily changes in
the actor's face shape. FDLS permits reliable and production-quality
performance solving with minimal training and little or no manual effort in
many cases, while also allowing the solve to be guided and edited in unusual
and difficult cases. The system has been under development for several years
and has been used in major movies.
</p></li>
</ul>

<h3>Title: Pre-training-free Image Manipulation Localization through Non-Mutually Exclusive Contrastive Learning. (arXiv:2309.14900v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14900">http://arxiv.org/abs/2309.14900</a></li>
<li>Code URL: https://github.com/knightzjz/ncl-iml</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14900]] Pre-training-free Image Manipulation Localization through Non-Mutually Exclusive Contrastive Learning(http://arxiv.org/abs/2309.14900)</code></li>
<li>Summary: <p>Deep Image Manipulation Localization (IML) models suffer from training data
insufficiency and thus heavily rely on pre-training. We argue that contrastive
learning is more suitable to tackle the data insufficiency problem for IML.
Crafting mutually exclusive positives and negatives is the prerequisite for
contrastive learning. However, when adopting contrastive learning in IML, we
encounter three categories of image patches: tampered, authentic, and contour
patches. Tampered and authentic patches are naturally mutually exclusive, but
contour patches containing both tampered and authentic pixels are non-mutually
exclusive to them. Simply abnegating these contour patches results in a drastic
performance loss since contour patches are decisive to the learning outcomes.
Hence, we propose the Non-mutually exclusive Contrastive Learning (NCL)
framework to rescue conventional contrastive learning from the above dilemma.
In NCL, to cope with the non-mutually exclusivity, we first establish a pivot
structure with dual branches to constantly switch the role of contour patches
between positives and negatives while training. Then, we devise a
pivot-consistent loss to avoid spatial corruption caused by the role-switching
process. In this manner, NCL both inherits the self-supervised merits to
address the data insufficiency and retains a high manipulation localization
accuracy. Extensive experiments verify that our NCL achieves state-of-the-art
performance on all five benchmarks without any pre-training and is more robust
on unseen real-life samples. The code is available at:
https://github.com/Knightzjz/NCL-IML.
</p></li>
</ul>

<h3>Title: Multi-Source Domain Adaptation for Object Detection with Prototype-based Mean-teacher. (arXiv:2309.14950v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14950">http://arxiv.org/abs/2309.14950</a></li>
<li>Code URL: https://github.com/imatif17/Prototype-Mean-Teacher</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14950]] Multi-Source Domain Adaptation for Object Detection with Prototype-based Mean-teacher(http://arxiv.org/abs/2309.14950)</code></li>
<li>Summary: <p>Adapting visual object detectors to operational target domains is a
challenging task, commonly achieved using unsupervised domain adaptation (UDA)
methods. When the labeled dataset is coming from multiple source domains,
treating them as separate domains and performing a multi-source domain
adaptation (MSDA) improves the accuracy and robustness over mixing these source
domains and performing a UDA, as observed by recent studies in MSDA. Existing
MSDA methods learn domain invariant and domain-specific parameters (for each
source domain) for the adaptation. However, unlike single-source UDA methods,
learning domain-specific parameters makes them grow significantly proportional
to the number of source domains used. This paper proposes a novel MSDA method
called Prototype-based Mean-Teacher (PMT), which uses class prototypes instead
of domain-specific subnets to preserve domain-specific information. These
prototypes are learned using a contrastive loss, aligning the same categories
across domains and separating different categories far apart. Because of the
use of prototypes, the parameter size of our method does not increase
significantly with the number of source domains, thus reducing memory issues
and possible overfitting. Empirical studies show PMT outperforms
state-of-the-art MSDA methods on several challenging object detection datasets.
</p></li>
</ul>

<h3>Title: Robust Sequential DeepFake Detection. (arXiv:2309.14991v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14991">http://arxiv.org/abs/2309.14991</a></li>
<li>Code URL: https://github.com/rshaojimmy/seqdeepfake</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14991]] Robust Sequential DeepFake Detection(http://arxiv.org/abs/2309.14991)</code></li>
<li>Summary: <p>Since photorealistic faces can be readily generated by facial manipulation
technologies nowadays, potential malicious abuse of these technologies has
drawn great concerns. Numerous deepfake detection methods are thus proposed.
However, existing methods only focus on detecting one-step facial manipulation.
As the emergence of easy-accessible facial editing applications, people can
easily manipulate facial components using multi-step operations in a sequential
manner. This new threat requires us to detect a sequence of facial
manipulations, which is vital for both detecting deepfake media and recovering
original faces afterwards. Motivated by this observation, we emphasize the need
and propose a novel research problem called Detecting Sequential DeepFake
Manipulation (Seq-DeepFake). Unlike the existing deepfake detection task only
demanding a binary label prediction, detecting Seq-DeepFake manipulation
requires correctly predicting a sequential vector of facial manipulation
operations. To support a large-scale investigation, we construct the first
Seq-DeepFake dataset, where face images are manipulated sequentially with
corresponding annotations of sequential facial manipulation vectors. Based on
this new dataset, we cast detecting Seq-DeepFake manipulation as a specific
image-to-sequence task and propose a concise yet effective Seq-DeepFake
Transformer (SeqFakeFormer). To better reflect real-world deepfake data
distributions, we further apply various perturbations on the original
Seq-DeepFake dataset and construct the more challenging Sequential DeepFake
dataset with perturbations (Seq-DeepFake-P). To exploit deeper correlation
between images and sequences when facing Seq-DeepFake-P, a dedicated
Seq-DeepFake Transformer with Image-Sequence Reasoning (SeqFakeFormer++) is
devised, which builds stronger correspondence between image-sequence pairs for
more robust Seq-DeepFake detection.
</p></li>
</ul>

<h3>Title: Doduo: Learning Dense Visual Correspondence from Unsupervised Semantic-Aware Flow. (arXiv:2309.15110v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15110">http://arxiv.org/abs/2309.15110</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15110]] Doduo: Learning Dense Visual Correspondence from Unsupervised Semantic-Aware Flow(http://arxiv.org/abs/2309.15110)</code></li>
<li>Summary: <p>Dense visual correspondence plays a vital role in robotic perception. This
work focuses on establishing the dense correspondence between a pair of images
that captures dynamic scenes undergoing substantial transformations. We
introduce Doduo to learn general dense visual correspondence from in-the-wild
images and videos without ground truth supervision. Given a pair of images, it
estimates the dense flow field encoding the displacement of each pixel in one
image to its corresponding pixel in the other image. Doduo uses flow-based
warping to acquire supervisory signals for the training. Incorporating semantic
priors with self-supervised flow training, Doduo produces accurate dense
correspondence robust to the dynamic changes of the scenes. Trained on an
in-the-wild video dataset, Doduo illustrates superior performance on
point-level correspondence estimation over existing self-supervised
correspondence learning baselines. We also apply Doduo to articulation
estimation and zero-shot goal-conditioned manipulation, underlining its
practical applications in robotics. Code and additional visualizations are
available at https://ut-austin-rpl.github.io/Doduo
</p></li>
</ul>

<h3>Title: Updated Corpora and Benchmarks for Long-Form Speech Recognition. (arXiv:2309.15013v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15013">http://arxiv.org/abs/2309.15013</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15013]] Updated Corpora and Benchmarks for Long-Form Speech Recognition(http://arxiv.org/abs/2309.15013)</code></li>
<li>Summary: <p>The vast majority of ASR research uses corpora in which both the training and
test data have been pre-segmented into utterances. In most real-word ASR
use-cases, however, test audio is not segmented, leading to a mismatch between
inference-time conditions and models trained on segmented utterances. In this
paper, we re-release three standard ASR corpora - TED-LIUM 3, Gigapeech, and
VoxPopuli-en - with updated transcription and alignments to enable their use
for long-form ASR research. We use these reconstituted corpora to study the
train-test mismatch problem for transducers and attention-based
encoder-decoders (AEDs), confirming that AEDs are more susceptible to this
issue. Finally, we benchmark a simple long-form training for these models,
showing its efficacy for model robustness under this domain shift.
</p></li>
</ul>

<h3>Title: Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control. (arXiv:2309.14597v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14597">http://arxiv.org/abs/2309.14597</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14597]] Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control(http://arxiv.org/abs/2309.14597)</code></li>
<li>Summary: <p>Deep reinforcement learning agents for continuous control are known to
exhibit significant instability in their performance over time. In this work,
we provide a fresh perspective on these behaviors by studying the return
landscape: the mapping between a policy and a return. We find that popular
algorithms traverse noisy neighborhoods of this landscape, in which a single
update to the policy parameters leads to a wide range of returns. By taking a
distributional view of these returns, we map the landscape, characterizing
failure-prone regions of policy space and revealing a hidden dimension of
policy quality. We show that the landscape exhibits surprising structure by
finding simple paths in parameter space which improve the stability of a
policy. To conclude, we develop a distribution-aware procedure which finds such
paths, navigating away from noisy neighborhoods in order to improve the
robustness of a policy. Taken together, our results provide new insight into
the optimization, evaluation, and design of agents.
</p></li>
</ul>

<h3>Title: ALEX: Towards Effective Graph Transfer Learning with Noisy Labels. (arXiv:2309.14673v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14673">http://arxiv.org/abs/2309.14673</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14673]] ALEX: Towards Effective Graph Transfer Learning with Noisy Labels(http://arxiv.org/abs/2309.14673)</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) have garnered considerable interest due to their
exceptional performance in a wide range of graph machine learning tasks.
Nevertheless, the majority of GNN-based approaches have been examined using
well-annotated benchmark datasets, leading to suboptimal performance in
real-world graph learning scenarios. To bridge this gap, the present paper
investigates the problem of graph transfer learning in the presence of label
noise, which transfers knowledge from a noisy source graph to an unlabeled
target graph. We introduce a novel technique termed Balance Alignment and
Information-aware Examination (ALEX) to address this challenge. ALEX first
employs singular value decomposition to generate different views with crucial
structural semantics, which help provide robust node representations using
graph contrastive learning. To mitigate both label shift and domain shift, we
estimate a prior distribution to build subgraphs with balanced label
distributions. Building on this foundation, an adversarial domain discriminator
is incorporated for the implicit domain alignment of complex multi-modal
distributions. Furthermore, we project node representations into a different
space, optimizing the mutual information between the projected features and
labels. Subsequently, the inconsistency of similarity structures is evaluated
to identify noisy samples with potential overfitting. Comprehensive experiments
on various benchmark datasets substantiate the outstanding superiority of the
proposed ALEX in different settings.
</p></li>
</ul>

<h3>Title: Age Minimization in Massive IoT via UAV Swarm: A Multi-agent Reinforcement Learning Approach. (arXiv:2309.14757v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14757">http://arxiv.org/abs/2309.14757</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14757]] Age Minimization in Massive IoT via UAV Swarm: A Multi-agent Reinforcement Learning Approach(http://arxiv.org/abs/2309.14757)</code></li>
<li>Summary: <p>In many massive IoT communication scenarios, the IoT devices require coverage
from dynamic units that can move close to the IoT devices and reduce the uplink
energy consumption. A robust solution is to deploy a large number of UAVs (UAV
swarm) to provide coverage and a better line of sight (LoS) for the IoT
network. However, the study of these massive IoT scenarios with a massive
number of serving units leads to high dimensional problems with high
complexity. In this paper, we apply multi-agent deep reinforcement learning to
address the high-dimensional problem that results from deploying a swarm of
UAVs to collect fresh information from IoT devices. The target is to minimize
the overall age of information in the IoT network. The results reveal that both
cooperative and partially cooperative multi-agent deep reinforcement learning
approaches are able to outperform the high-complexity centralized deep
reinforcement learning approach, which stands helpless in large-scale networks.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud Registration. (arXiv:2309.14660v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14660">http://arxiv.org/abs/2309.14660</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14660]] CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud Registration(http://arxiv.org/abs/2309.14660)</code></li>
<li>Summary: <p>Image-to-point cloud (I2P) registration is a fundamental task in the fields
of robot navigation and mobile mapping. Existing I2P registration works
estimate correspondences at the point-to-pixel level, neglecting the global
alignment. However, I2P matching without high-level guidance from global
constraints may converge to the local optimum easily. To solve the problem,
this paper proposes CoFiI2P, a novel I2P registration network that extracts
correspondences in a coarse-to-fine manner for the global optimal solution.
First, the image and point cloud are fed into a Siamese encoder-decoder network
for hierarchical feature extraction. Then, a coarse-to-fine matching module is
designed to exploit features and establish resilient feature correspondences.
Specifically, in the coarse matching block, a novel I2P transformer module is
employed to capture the homogeneous and heterogeneous global information from
image and point cloud. With the discriminate descriptors, coarse
super-point-to-super-pixel matching pairs are estimated. In the fine matching
module, point-to-pixel pairs are established with the
super-point-to-super-pixel correspondence supervision. Finally, based on
matching pairs, the transform matrix is estimated with the EPnP-RANSAC
algorithm. Extensive experiments conducted on the KITTI dataset have
demonstrated that CoFiI2P achieves a relative rotation error (RRE) of 2.25
degrees and a relative translation error (RTE) of 0.61 meters. These results
represent a significant improvement of 14% in RRE and 52% in RTE compared to
the current state-of-the-art (SOTA) method. The demo video for the experiments
is available at https://youtu.be/TG2GBrJTuW4. The source code will be public at
https://github.com/kang-1-2-3/CoFiI2P.
</p></li>
</ul>

<h3>Title: The Surveillance AI Pipeline. (arXiv:2309.15084v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15084">http://arxiv.org/abs/2309.15084</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15084]] The Surveillance AI Pipeline(http://arxiv.org/abs/2309.15084)</code></li>
<li>Summary: <p>A rapidly growing number of voices have argued that AI research, and computer
vision in particular, is closely tied to mass surveillance. Yet the direct path
from computer vision research to surveillance has remained obscured and
difficult to assess. This study reveals the Surveillance AI pipeline. We obtain
three decades of computer vision research papers and downstream patents (more
than 20,000 documents) and present a rich qualitative and quantitative
analysis. This analysis exposes the nature and extent of the Surveillance AI
pipeline, its institutional roots and evolution, and ongoing patterns of
obfuscation. We first perform an in-depth content analysis of computer vision
papers and downstream patents, identifying and quantifying key features and the
many, often subtly expressed, forms of surveillance that appear. On the basis
of this analysis, we present a topology of Surveillance AI that characterizes
the prevalent targeting of human data, practices of data transferal, and
institutional data use. We find stark evidence of close ties between computer
vision and surveillance. The majority (68%) of annotated computer vision papers
and patents self-report their technology enables data extraction about human
bodies and body parts and even more (90%) enable data extraction about humans
in general.
</p></li>
</ul>

<h3>Title: Fine-tuning and aligning question answering models for complex information extraction tasks. (arXiv:2309.14805v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14805">http://arxiv.org/abs/2309.14805</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14805]] Fine-tuning and aligning question answering models for complex information extraction tasks(http://arxiv.org/abs/2309.14805)</code></li>
<li>Summary: <p>The emergence of Large Language Models (LLMs) has boosted performance and
possibilities in various NLP tasks. While the usage of generative AI models
like ChatGPT opens up new opportunities for several business use cases, their
current tendency to hallucinate fake content strongly limits their
applicability to document analysis, such as information retrieval from
documents. In contrast, extractive language models like question answering (QA)
or passage retrieval models guarantee query results to be found within the
boundaries of an according context document, which makes them candidates for
more reliable information extraction in productive environments of companies.
In this work we propose an approach that uses and integrates extractive QA
models for improved feature extraction of German business documents such as
insurance reports or medical leaflets into a document analysis solution. We
further show that fine-tuning existing German QA models boosts performance for
tailored extraction tasks of complex linguistic features like damage cause
explanations or descriptions of medication appearance, even with using only a
small set of annotated data. Finally, we discuss the relevance of scoring
metrics for evaluating information extraction tasks and deduce a combined
metric from Levenshtein distance, F1-Score, Exact Match and ROUGE-L to mimic
the assessment criteria from human experts.
</p></li>
</ul>

<h3>Title: On the Computational Complexity and Formal Hierarchy of Second Order Recurrent Neural Networks. (arXiv:2309.14691v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14691">http://arxiv.org/abs/2309.14691</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14691]] On the Computational Complexity and Formal Hierarchy of Second Order Recurrent Neural Networks(http://arxiv.org/abs/2309.14691)</code></li>
<li>Summary: <p>Artificial neural networks (ANNs) with recurrence and self-attention have
been shown to be Turing-complete (TC). However, existing work has shown that
these ANNs require multiple turns or unbounded computation time, even with
unbounded precision in weights, in order to recognize TC grammars. However,
under constraints such as fixed or bounded precision neurons and time, ANNs
without memory are shown to struggle to recognize even context-free languages.
In this work, we extend the theoretical foundation for the $2^{nd}$-order
recurrent network ($2^{nd}$ RNN) and prove there exists a class of a $2^{nd}$
RNN that is Turing-complete with bounded time. This model is capable of
directly encoding a transition table into its recurrent weights, enabling
bounded time computation and is interpretable by design. We also demonstrate
that $2$nd order RNNs, without memory, under bounded weights and time
constraints, outperform modern-day models such as vanilla RNNs and gated
recurrent units in recognizing regular grammars. We provide an upper bound and
a stability analysis on the maximum number of neurons required by $2$nd order
RNNs to recognize any class of regular grammar. Extensive experiments on the
Tomita grammars support our findings, demonstrating the importance of tensor
connections in crafting computationally efficient RNNs. Finally, we show
$2^{nd}$ order RNNs are also interpretable by extraction and can extract state
machines with higher success rates as compared to first-order RNNs. Our results
extend the theoretical foundations of RNNs and offer promising avenues for
future explainable AI research.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices using a Computing Power Aware Scheduler. (arXiv:2309.14675v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14675">http://arxiv.org/abs/2309.14675</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14675]] FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices using a Computing Power Aware Scheduler(http://arxiv.org/abs/2309.14675)</code></li>
<li>Summary: <p>Cross-silo federated learning offers a promising solution to collaboratively
train robust and generalized AI models without compromising the privacy of
local datasets, e.g., healthcare, financial, as well as scientific projects
that lack a centralized data facility. Nonetheless, because of the disparity of
computing resources among different clients (i.e., device heterogeneity),
synchronous federated learning algorithms suffer from degraded efficiency when
waiting for straggler clients. Similarly, asynchronous federated learning
algorithms experience degradation in the convergence rate and final model
accuracy on non-identically and independently distributed (non-IID)
heterogeneous datasets due to stale local models and client drift. To address
these limitations in cross-silo federated learning with heterogeneous clients
and data, we propose FedCompass, an innovative semi-asynchronous federated
learning algorithm with a computing power aware scheduler on the server side,
which adaptively assigns varying amounts of training tasks to different clients
using the knowledge of the computing power of individual clients. FedCompass
ensures that multiple locally trained models from clients are received almost
simultaneously as a group for aggregation, effectively reducing the staleness
of local models. At the same time, the overall training process remains
asynchronous, eliminating prolonged waiting periods from straggler clients.
Using diverse non-IID heterogeneous distributed datasets, we demonstrate that
FedCompass achieves faster convergence and higher accuracy than other
asynchronous algorithms while remaining more efficient than synchronous
algorithms when performing federated learning on heterogeneous clients.
</p></li>
</ul>

<h3>Title: Markov Chain Mirror Descent On Data Federation. (arXiv:2309.14775v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14775">http://arxiv.org/abs/2309.14775</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14775]] Markov Chain Mirror Descent On Data Federation(http://arxiv.org/abs/2309.14775)</code></li>
<li>Summary: <p>Stochastic optimization methods such as mirror descent have wide applications
due to low computational cost. Those methods have been well studied under
assumption of the independent and identical distribution, and usually achieve
sublinear rate of convergence. However, this assumption may be too strong and
unpractical in real application scenarios. Recent researches investigate
stochastic gradient descent when instances are sampled from a Markov chain.
Unfortunately, few results are known for stochastic mirror descent. In the
paper, we propose a new version of stochastic mirror descent termed by MarchOn
in the scenario of the federated learning. Given a distributed network, the
model iteratively travels from a node to one of its neighbours randomly.
Furthermore, we propose a new framework to analyze MarchOn, which yields best
rates of convergence for convex, strongly convex, and non-convex loss. Finally,
we conduct empirical studies to evaluate the convergence of MarchOn, and
validate theoretical results.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Nuclear Morphometry using a Deep Learning-based Algorithm has Prognostic Relevance for Canine Cutaneous Mast Cell Tumors. (arXiv:2309.15031v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15031">http://arxiv.org/abs/2309.15031</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15031]] Nuclear Morphometry using a Deep Learning-based Algorithm has Prognostic Relevance for Canine Cutaneous Mast Cell Tumors(http://arxiv.org/abs/2309.15031)</code></li>
<li>Summary: <p>Variation in nuclear size and shape is an important criterion of malignancy
for many tumor types; however, categorical estimates by pathologists have poor
reproducibility. Measurements of nuclear characteristics (morphometry) can
improve reproducibility, but manual methods are time consuming. In this study,
we evaluated fully automated morphometry using a deep learning-based algorithm
in 96 canine cutaneous mast cell tumors with information on patient survival.
Algorithmic morphometry was compared with karyomegaly estimates by 11
pathologists, manual nuclear morphometry of 12 cells by 9 pathologists, and the
mitotic count as a benchmark. The prognostic value of automated morphometry was
high with an area under the ROC curve regarding the tumor-specific survival of
0.943 (95% CI: 0.889 - 0.996) for the standard deviation (SD) of nuclear area,
which was higher than manual morphometry of all pathologists combined (0.868,
95% CI: 0.737 - 0.991) and the mitotic count (0.885, 95% CI: 0.765 - 1.00). At
the proposed thresholds, the hazard ratio for algorithmic morphometry (SD of
nuclear area $\geq 9.0 \mu m^2$) was 18.3 (95% CI: 5.0 - 67.1), for manual
morphometry (SD of nuclear area $\geq 10.9 \mu m^2$) 9.0 (95% CI: 6.0 - 13.4),
for karyomegaly estimates 7.6 (95% CI: 5.7 - 10.1), and for the mitotic count
30.5 (95% CI: 7.8 - 118.0). Inter-rater reproducibility for karyomegaly
estimates was fair ($\kappa$ = 0.226) with highly variable
sensitivity/specificity values for the individual pathologists. Reproducibility
for manual morphometry (SD of nuclear area) was good (ICC = 0.654). This study
supports the use of algorithmic morphometry as a prognostic test to overcome
the limitations of estimates and manual measurements.
</p></li>
</ul>

<h3>Title: Survey of Social Bias in Vision-Language Models. (arXiv:2309.14381v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14381">http://arxiv.org/abs/2309.14381</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14381]] Survey of Social Bias in Vision-Language Models(http://arxiv.org/abs/2309.14381)</code></li>
<li>Summary: <p>In recent years, the rapid advancement of machine learning (ML) models,
particularly transformer-based pre-trained models, has revolutionized Natural
Language Processing (NLP) and Computer Vision (CV) fields. However, researchers
have discovered that these models can inadvertently capture and reinforce
social biases present in their training datasets, leading to potential social
harms, such as uneven resource allocation and unfair representation of specific
social groups. Addressing these biases and ensuring fairness in artificial
intelligence (AI) systems has become a critical concern in the ML community.
</p>
<p>The recent introduction of pre-trained vision-and-language (VL) models in the
emerging multimodal field demands attention to the potential social biases
present in these models as well. Although VL models are susceptible to social
bias, there is a limited understanding compared to the extensive discussions on
bias in NLP and CV. This survey aims to provide researchers with a high-level
insight into the similarities and differences of social bias studies in
pre-trained models across NLP, CV, and VL. By examining these perspectives, the
survey aims to offer valuable guidelines on how to approach and mitigate social
bias in both unimodal and multimodal settings. The findings and recommendations
presented here can benefit the ML community, fostering the development of
fairer and non-biased AI models in various applications and research endeavors.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Pixel-Grounded Prototypical Part Networks. (arXiv:2309.14531v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14531">http://arxiv.org/abs/2309.14531</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14531]] Pixel-Grounded Prototypical Part Networks(http://arxiv.org/abs/2309.14531)</code></li>
<li>Summary: <p>Prototypical part neural networks (ProtoPartNNs), namely PROTOPNET and its
derivatives, are an intrinsically interpretable approach to machine learning.
Their prototype learning scheme enables intuitive explanations of the form,
this (prototype) looks like that (testing image patch). But, does this actually
look like that? In this work, we delve into why object part localization and
associated heat maps in past work are misleading. Rather than localizing to
object parts, existing ProtoPartNNs localize to the entire image, contrary to
generated explanatory visualizations. We argue that detraction from these
underlying issues is due to the alluring nature of visualizations and an
over-reliance on intuition. To alleviate these issues, we devise new receptive
field-based architectural constraints for meaningful localization and a
principled pixel space mapping for ProtoPartNNs. To improve interpretability,
we propose additional architectural improvements, including a simplified
classification head. We also make additional corrections to PROTOPNET and its
derivatives, such as the use of a validation set, rather than a test set, to
evaluate generalization during training. Our approach, PIXPNET (Pixel-grounded
Prototypical part Network), is the only ProtoPartNN that truly learns and
localizes to prototypical object parts. We demonstrate that PIXPNET achieves
quantifiably improved interpretability without sacrificing accuracy.
</p></li>
</ul>

<h3>Title: Image Denoising via Style Disentanglement. (arXiv:2309.14755v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14755">http://arxiv.org/abs/2309.14755</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14755]] Image Denoising via Style Disentanglement(http://arxiv.org/abs/2309.14755)</code></li>
<li>Summary: <p>Image denoising is a fundamental task in low-level computer vision. While
recent deep learning-based image denoising methods have achieved impressive
performance, they are black-box models and the underlying denoising principle
remains unclear. In this paper, we propose a novel approach to image denoising
that offers both clear denoising mechanism and good performance. We view noise
as a type of image style and remove it by incorporating noise-free styles
derived from clean images. To achieve this, we design novel losses and network
modules to extract noisy styles from noisy images and noise-free styles from
clean images. The noise-free style induces low-response activations for noise
features and high-response activations for content features in the feature
space. This leads to the separation of clean contents from noise, effectively
denoising the image. Unlike disentanglement-based image editing tasks that edit
semantic-level attributes using styles, our main contribution lies in editing
pixel-level attributes through global noise-free styles. We conduct extensive
experiments on synthetic noise removal and real-world image denoising datasets
(SIDD and DND), demonstrating the effectiveness of our method in terms of both
PSNR and SSIM metrics. Moreover, we experimentally validate that our method
offers good interpretability.
</p></li>
</ul>

<h3>Title: InvKA: Gait Recognition via Invertible Koopman Autoencoder. (arXiv:2309.14764v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14764">http://arxiv.org/abs/2309.14764</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14764]] InvKA: Gait Recognition via Invertible Koopman Autoencoder(http://arxiv.org/abs/2309.14764)</code></li>
<li>Summary: <p>Most current gait recognition methods suffer from poor interpretability and
high computational cost. To improve interpretability, we investigate gait
features in the embedding space based on Koopman operator theory. The
transition matrix in this space captures complex kinematic features of gait
cycles, namely the Koopman operator. The diagonal elements of the operator
matrix can represent the overall motion trend, providing a physically
meaningful descriptor. To reduce the computational cost of our algorithm, we
use a reversible autoencoder to reduce the model size and eliminate
convolutional layers to compress its depth, resulting in fewer floating-point
operations. Experimental results on multiple datasets show that our method
reduces computational cost to 1% compared to state-of-the-art methods while
achieving competitive recognition accuracy 98% on non-occlusion datasets.
</p></li>
</ul>

<h3>Title: Object-Centric Open-Vocabulary Image-Retrieval with Aggregated Features. (arXiv:2309.14999v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14999">http://arxiv.org/abs/2309.14999</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14999]] Object-Centric Open-Vocabulary Image-Retrieval with Aggregated Features(http://arxiv.org/abs/2309.14999)</code></li>
<li>Summary: <p>The task of open-vocabulary object-centric image retrieval involves the
retrieval of images containing a specified object of interest, delineated by an
open-set text query. As working on large image datasets becomes standard,
solving this task efficiently has gained significant practical importance.
Applications include targeted performance analysis of retrieved images using
ad-hoc queries and hard example mining during training. Recent advancements in
contrastive-based open vocabulary systems have yielded remarkable
breakthroughs, facilitating large-scale open vocabulary image retrieval.
However, these approaches use a single global embedding per image, thereby
constraining the system's ability to retrieve images containing relatively
small object instances. Alternatively, incorporating local embeddings from
detection pipelines faces scalability challenges, making it unsuitable for
retrieval from large databases.
</p>
<p>In this work, we present a simple yet effective approach to object-centric
open-vocabulary image retrieval. Our approach aggregates dense embeddings
extracted from CLIP into a compact representation, essentially combining the
scalability of image retrieval pipelines with the object identification
capabilities of dense detection methods. We show the effectiveness of our
scheme to the task by achieving significantly better results than global
feature approaches on three datasets, increasing accuracy by up to 15 mAP
points. We further integrate our scheme into a large scale retrieval framework
and demonstrate our method's advantages in terms of scalability and
interpretability.
</p></li>
</ul>

<h3>Title: A Text Classification-Based Approach for Evaluating and Enhancing the Machine Interpretability of Building Codes. (arXiv:2309.14374v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14374">http://arxiv.org/abs/2309.14374</a></li>
<li>Code URL: https://github.com/skydustz/text-classification-based-approach-for-evaluating-and-enhancing-machine-interpretability-of-building</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14374]] A Text Classification-Based Approach for Evaluating and Enhancing the Machine Interpretability of Building Codes(http://arxiv.org/abs/2309.14374)</code></li>
<li>Summary: <p>Interpreting regulatory documents or building codes into computer-processable
formats is essential for the intelligent design and construction of buildings
and infrastructures. Although automated rule interpretation (ARI) methods have
been investigated for years, most of them highly depend on the early and manual
filtering of interpretable clauses from a building code. While few of them
considered machine interpretability, which represents the potential to be
transformed into a computer-processable format, from both clause- and
document-level. Therefore, this research aims to propose a novel approach to
automatically evaluate and enhance the machine interpretability of single
clause and building codes. First, a few categories are introduced to classify
each clause in a building code considering the requirements for rule
interpretation, and a dataset is developed for model training. Then, an
efficient text classification model is developed based on a pretrained
domain-specific language model and transfer learning techniques. Finally, a
quantitative evaluation method is proposed to assess the overall
interpretability of building codes. Experiments show that the proposed text
classification algorithm outperforms the existing CNN- or RNN-based methods,
improving the F1-score from 72.16% to 93.60%. It is also illustrated that the
proposed classification method can enhance downstream ARI methods with an
improvement of 4%. Furthermore, analyzing the results of more than 150 building
codes in China showed that their average interpretability is 34.40%, which
implies that it is still hard to fully transform the entire regulatory document
into computer-processable formats. It is also argued that the interpretability
of building codes should be further improved both from the human side and the
machine side.
</p></li>
</ul>

<h3>Title: Sampling - Variational Auto Encoder - Ensemble: In the Quest of Explainable Artificial Intelligence. (arXiv:2309.14385v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14385">http://arxiv.org/abs/2309.14385</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14385]] Sampling - Variational Auto Encoder - Ensemble: In the Quest of Explainable Artificial Intelligence(http://arxiv.org/abs/2309.14385)</code></li>
<li>Summary: <p>Explainable Artificial Intelligence (XAI) models have recently attracted a
great deal of interest from a variety of application sectors. Despite
significant developments in this area, there are still no standardized methods
or approaches for understanding AI model outputs. A systematic and cohesive
framework is also increasingly necessary to incorporate new techniques like
discriminative and generative models to close the gap. This paper contributes
to the discourse on XAI by presenting an empirical evaluation based on a novel
framework: Sampling - Variational Auto Encoder (VAE) - Ensemble Anomaly
Detection (SVEAD). It is a hybrid architecture where VAE combined with ensemble
stacking and SHapley Additive exPlanations are used for imbalanced
classification. The finding reveals that combining ensemble stacking, VAE, and
SHAP can. not only lead to better model performance but also provide an easily
explainable framework. This work has used SHAP combined with Permutation
Importance and Individual Conditional Expectations to create a powerful
interpretability of the model. The finding has an important implication in the
real world, where the need for XAI is paramount to boost confidence in AI
applications.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Explaining Deep Face Algorithms through Visualization: A Survey. (arXiv:2309.14715v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14715">http://arxiv.org/abs/2309.14715</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14715]] Explaining Deep Face Algorithms through Visualization: A Survey(http://arxiv.org/abs/2309.14715)</code></li>
<li>Summary: <p>Although current deep models for face tasks surpass human performance on some
benchmarks, we do not understand how they work. Thus, we cannot predict how it
will react to novel inputs, resulting in catastrophic failures and unwanted
biases in the algorithms. Explainable AI helps bridge the gap, but currently,
there are very few visualization algorithms designed for faces. This work
undertakes a first-of-its-kind meta-analysis of explainability algorithms in
the face domain. We explore the nuances and caveats of adapting general-purpose
visualization algorithms to the face domain, illustrated by computing
visualizations on popular face models. We review existing face explainability
works and reveal valuable insights into the structure and hierarchy of face
networks. We also determine the design considerations for practical face
visualizations accessible to AI practitioners by conducting a user study on the
utility of various explainability algorithms.
</p></li>
</ul>

<h3>Title: Explainable and Accurate Natural Language Understanding for Voice Assistants and Beyond. (arXiv:2309.14485v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14485">http://arxiv.org/abs/2309.14485</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14485]] Explainable and Accurate Natural Language Understanding for Voice Assistants and Beyond(http://arxiv.org/abs/2309.14485)</code></li>
<li>Summary: <p>Joint intent detection and slot filling, which is also termed as joint NLU
(Natural Language Understanding) is invaluable for smart voice assistants.
Recent advancements in this area have been heavily focusing on improving
accuracy using various techniques. Explainability is undoubtedly an important
aspect for deep learning-based models including joint NLU models. Without
explainability, their decisions are opaque to the outside world and hence, have
tendency to lack user trust. Therefore to bridge this gap, we transform the
full joint NLU model to be `inherently' explainable at granular levels without
compromising on accuracy. Further, as we enable the full joint NLU model
explainable, we show that our extension can be successfully used in other
general classification tasks. We demonstrate this using sentiment analysis and
named entity recognition.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Domain-Guided Conditional Diffusion Model for Unsupervised Domain Adaptation. (arXiv:2309.14360v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14360">http://arxiv.org/abs/2309.14360</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14360]] Domain-Guided Conditional Diffusion Model for Unsupervised Domain Adaptation(http://arxiv.org/abs/2309.14360)</code></li>
<li>Summary: <p>Limited transferability hinders the performance of deep learning models when
applied to new application scenarios. Recently, Unsupervised Domain Adaptation
(UDA) has achieved significant progress in addressing this issue via learning
domain-invariant features. However, the performance of existing UDA methods is
constrained by the large domain shift and limited target domain data. To
alleviate these issues, we propose DomAin-guided Conditional Diffusion Model
(DACDM) to generate high-fidelity and diversity samples for the target domain.
In the proposed DACDM, by introducing class information, the labels of
generated samples can be controlled, and a domain classifier is further
introduced in DACDM to guide the generated samples for the target domain. The
generated samples help existing UDA methods transfer from the source domain to
the target domain more easily, thus improving the transfer performance.
Extensive experiments on various benchmarks demonstrate that DACDM brings a
large improvement to the performance of existing UDA methods.
</p></li>
</ul>

<h3>Title: Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator. (arXiv:2309.14494v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14494">http://arxiv.org/abs/2309.14494</a></li>
<li>Code URL: https://github.com/soolab/free-bloom</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14494]] Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator(http://arxiv.org/abs/2309.14494)</code></li>
<li>Summary: <p>Text-to-video is a rapidly growing research area that aims to generate a
semantic, identical, and temporal coherence sequence of frames that accurately
align with the input text prompt. This study focuses on zero-shot text-to-video
generation considering the data- and cost-efficient. To generate a
semantic-coherent video, exhibiting a rich portrayal of temporal semantics such
as the whole process of flower blooming rather than a set of "moving images",
we propose a novel Free-Bloom pipeline that harnesses large language models
(LLMs) as the director to generate a semantic-coherence prompt sequence, while
pre-trained latent diffusion models (LDMs) as the animator to generate the high
fidelity frames. Furthermore, to ensure temporal and identical coherence while
maintaining semantic coherence, we propose a series of annotative modifications
to adapting LDMs in the reverse process, including joint noise sampling,
step-aware attention shift, and dual-path interpolation. Without any video data
and training requirements, Free-Bloom generates vivid and high-quality videos,
awe-inspiring in generating complex scenes with semantic meaningful frame
sequences. In addition, Free-Bloom is naturally compatible with LDMs-based
extensions.
</p></li>
</ul>

<h3>Title: Bootstrap Diffusion Model Curve Estimation for High Resolution Low-Light Image Enhancement. (arXiv:2309.14709v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14709">http://arxiv.org/abs/2309.14709</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14709]] Bootstrap Diffusion Model Curve Estimation for High Resolution Low-Light Image Enhancement(http://arxiv.org/abs/2309.14709)</code></li>
<li>Summary: <p>Learning-based methods have attracted a lot of research attention and led to
significant improvements in low-light image enhancement. However, most of them
still suffer from two main problems: expensive computational cost in high
resolution images and unsatisfactory performance in simultaneous enhancement
and denoising. To address these problems, we propose BDCE, a bootstrap
diffusion model that exploits the learning of the distribution of the curve
parameters instead of the normal-light image itself. Specifically, we adopt the
curve estimation method to handle the high-resolution images, where the curve
parameters are estimated by our bootstrap diffusion model. In addition, a
denoise module is applied in each iteration of curve adjustment to denoise the
intermediate enhanced result of each iteration. We evaluate BDCE on commonly
used benchmark datasets, and extensive experiments show that it achieves
state-of-the-art qualitative and quantitative performance.
</p></li>
</ul>

<h3>Title: Text-image guided Diffusion Model for generating Deepfake celebrity interactions. (arXiv:2309.14751v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14751">http://arxiv.org/abs/2309.14751</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14751]] Text-image guided Diffusion Model for generating Deepfake celebrity interactions(http://arxiv.org/abs/2309.14751)</code></li>
<li>Summary: <p>Deepfake images are fast becoming a serious concern due to their realism.
Diffusion models have recently demonstrated highly realistic visual content
generation, which makes them an excellent potential tool for Deepfake
generation. To curb their exploitation for Deepfakes, it is imperative to first
explore the extent to which diffusion models can be used to generate realistic
content that is controllable with convenient prompts. This paper devises and
explores a novel method in that regard. Our technique alters the popular stable
diffusion model to generate a controllable high-quality Deepfake image with
text and image prompts. In addition, the original stable model lacks severely
in generating quality images that contain multiple persons. The modified
diffusion model is able to address this problem, it add input anchor image's
latent at the beginning of inferencing rather than Gaussian random latent as
input. Hence, we focus on generating forged content for celebrity interactions,
which may be used to spread rumors. We also apply Dreambooth to enhance the
realism of our fake images. Dreambooth trains the pairing of center words and
specific features to produce more refined and personalized output images. Our
results show that with the devised scheme, it is possible to create fake visual
content with alarming realism, such that the content can serve as believable
evidence of meetings between powerful political figures.
</p></li>
</ul>

<h3>Title: On quantifying and improving realism of images generated with diffusion. (arXiv:2309.14756v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14756">http://arxiv.org/abs/2309.14756</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14756]] On quantifying and improving realism of images generated with diffusion(http://arxiv.org/abs/2309.14756)</code></li>
<li>Summary: <p>Recent advances in diffusion models have led to a quantum leap in the quality
of generative visual content. However, quantification of realism of the content
is still challenging. Existing evaluation metrics, such as Inception Score and
Fr\'echet inception distance, fall short on benchmarking diffusion models due
to the versatility of the generated images. Moreover, they are not designed to
quantify realism of an individual image. This restricts their application in
forensic image analysis, which is becoming increasingly important in the
emerging era of generative models. To address that, we first propose a metric,
called Image Realism Score (IRS), computed from five statistical measures of a
given image. This non-learning based metric not only efficiently quantifies
realism of the generated images, it is readily usable as a measure to classify
a given image as real or fake. We experimentally establish the model- and
data-agnostic nature of the proposed IRS by successfully detecting fake images
generated by Stable Diffusion Model (SDM), Dalle2, Midjourney and BigGAN.
</p>
<p>We further leverage this attribute of our metric to minimize an IRS-augmented
generative loss of SDM, and demonstrate a convenient yet considerable quality
improvement of the SDM-generated content with our modification. Our efforts
have also led to Gen-100 dataset, which provides 1,000 samples for 100 classes
generated by four high-quality models. We will release the dataset and code.
</p></li>
</ul>

<h3>Title: Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation. (arXiv:2309.14859v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14859">http://arxiv.org/abs/2309.14859</a></li>
<li>Code URL: https://github.com/kohakublueleaf/lycoris</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14859]] Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation(http://arxiv.org/abs/2309.14859)</code></li>
<li>Summary: <p>Text-to-image generative models have garnered immense attention for their
ability to produce high-fidelity images from text prompts. Among these, Stable
Diffusion distinguishes itself as a leading open-source model in this
fast-growing field. However, the intricacies of fine-tuning these models pose
multiple challenges from new methodology integration to systematic evaluation.
Addressing these issues, this paper introduces LyCORIS (Lora beYond
Conventional methods, Other Rank adaptation Implementations for Stable
diffusion) [https://github.com/KohakuBlueleaf/LyCORIS], an open-source library
that offers a wide selection of fine-tuning methodologies for Stable Diffusion.
Furthermore, we present a thorough framework for the systematic assessment of
varied fine-tuning techniques. This framework employs a diverse suite of
metrics and delves into multiple facets of fine-tuning, including
hyperparameter adjustments and the evaluation with different prompt types
across various concept categories. Through this comprehensive approach, our
work provides essential insights into the nuanced effects of fine-tuning
parameters, bridging the gap between state-of-the-art research and practical
application.
</p></li>
</ul>

<h3>Title: ITEM3D: Illumination-Aware Directional Texture Editing for 3D Models. (arXiv:2309.14872v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14872">http://arxiv.org/abs/2309.14872</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14872]] ITEM3D: Illumination-Aware Directional Texture Editing for 3D Models(http://arxiv.org/abs/2309.14872)</code></li>
<li>Summary: <p>Texture editing is a crucial task in 3D modeling that allows users to
automatically manipulate the surface materials of 3D models. However, the
inherent complexity of 3D models and the ambiguous text description lead to the
challenge in this task. To address this challenge, we propose ITEM3D, an
illumination-aware model for automatic 3D object editing according to the text
prompts. Leveraging the diffusion models and the differentiable rendering,
ITEM3D takes the rendered images as the bridge of text and 3D representation,
and further optimizes the disentangled texture and environment map. Previous
methods adopt the absolute editing direction namely score distillation sampling
(SDS) as the optimization objective, which unfortunately results in the noisy
appearance and text inconsistency. To solve the problem caused by the ambiguous
text, we introduce a relative editing direction, an optimization objective
defined by the noise difference between the source and target texts, to release
the semantic ambiguity between the texts and images. Additionally, we gradually
adjust the direction during optimization to further address the unexpected
deviation in the texture domain. Qualitative and quantitative experiments show
that our ITEM3D outperforms the state-of-the-art methods on various 3D objects.
We also perform text-guided relighting to show explicit control over lighting.
</p></li>
</ul>

<h3>Title: FEC: Three Finetuning-free Methods to Enhance Consistency for Real Image Editing. (arXiv:2309.14934v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14934">http://arxiv.org/abs/2309.14934</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14934]] FEC: Three Finetuning-free Methods to Enhance Consistency for Real Image Editing(http://arxiv.org/abs/2309.14934)</code></li>
<li>Summary: <p>Text-conditional image editing is a very useful task that has recently
emerged with immeasurable potential. Most current real image editing methods
first need to complete the reconstruction of the image, and then editing is
carried out by various methods based on the reconstruction. Most methods use
DDIM Inversion for reconstruction, however, DDIM Inversion often fails to
guarantee reconstruction performance, i.e., it fails to produce results that
preserve the original image content. To address the problem of reconstruction
failure, we propose FEC, which consists of three sampling methods, each
designed for different editing types and settings. Our three methods of FEC
achieve two important goals in image editing task: 1) ensuring successful
reconstruction, i.e., sampling to get a generated result that preserves the
texture and features of the original real image. 2) these sampling methods can
be paired with many editing methods and greatly improve the performance of
these editing methods to accomplish various editing tasks. In addition, none of
our sampling methods require fine-tuning of the diffusion model or
time-consuming training on large-scale datasets. Hence the cost of time as well
as the use of computer memory and computation can be significantly reduced.
</p></li>
</ul>

<h3>Title: LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models. (arXiv:2309.15103v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15103">http://arxiv.org/abs/2309.15103</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15103]] LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models(http://arxiv.org/abs/2309.15103)</code></li>
<li>Summary: <p>This work aims to learn a high-quality text-to-video (T2V) generative model
by leveraging a pre-trained text-to-image (T2I) model as a basis. It is a
highly desirable yet challenging task to simultaneously a) accomplish the
synthesis of visually realistic and temporally coherent videos while b)
preserving the strong creative generation nature of the pre-trained T2I model.
To this end, we propose LaVie, an integrated video generation framework that
operates on cascaded video latent diffusion models, comprising a base T2V
model, a temporal interpolation model, and a video super-resolution model. Our
key insights are two-fold: 1) We reveal that the incorporation of simple
temporal self-attentions, coupled with rotary positional encoding, adequately
captures the temporal correlations inherent in video data. 2) Additionally, we
validate that the process of joint image-video fine-tuning plays a pivotal role
in producing high-quality and creative outcomes. To enhance the performance of
LaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M,
consisting of 25 million text-video pairs that prioritize quality, diversity,
and aesthetic appeal. Extensive experiments demonstrate that LaVie achieves
state-of-the-art performance both quantitatively and qualitatively.
Furthermore, we showcase the versatility of pre-trained LaVie models in various
long video generation and personalized video synthesis applications.
</p></li>
</ul>

<h3>Title: Generating Visual Scenes from Touch. (arXiv:2309.15117v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15117">http://arxiv.org/abs/2309.15117</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15117]] Generating Visual Scenes from Touch(http://arxiv.org/abs/2309.15117)</code></li>
<li>Summary: <p>An emerging line of work has sought to generate plausible imagery from touch.
Existing approaches, however, tackle only narrow aspects of the visuo-tactile
synthesis problem, and lag significantly behind the quality of cross-modal
synthesis methods in other domains. We draw on recent advances in latent
diffusion to create a model for synthesizing images from tactile signals (and
vice versa) and apply it to a number of visuo-tactile synthesis tasks. Using
this model, we significantly outperform prior work on the tactile-driven
stylization problem, i.e., manipulating an image to match a touch signal, and
we are the first to successfully generate images from touch without additional
sources of information about the scene. We also successfully use our model to
address two novel synthesis problems: generating images that do not contain the
touch sensor or the hand holding it, and estimating an image's shading from its
reflectance and touch.
</p></li>
</ul>

<h3>Title: Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation. (arXiv:2309.14394v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14394">http://arxiv.org/abs/2309.14394</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14394]] Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation(http://arxiv.org/abs/2309.14394)</code></li>
<li>Summary: <p>Domain-to-domain translation involves generating a target domain sample given
a condition in the source domain. Most existing methods focus on fixed input
and output domains, i.e. they only work for specific configurations (i.e. for
two domains, either $D_1\rightarrow{}D_2$ or $D_2\rightarrow{}D_1$). This paper
proposes Multi-Domain Diffusion (MDD), a conditional diffusion framework for
multi-domain translation in a semi-supervised context. Unlike previous methods,
MDD does not require defining input and output domains, allowing translation
between any partition of domains within a set (such as $(D_1,
D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$,
etc. for 3 domains), without the need to train separate models for each domain
configuration. The key idea behind MDD is to leverage the noise formulation of
diffusion models by incorporating one noise level per domain, which allows
missing domains to be modeled with noise in a natural way. This transforms the
training task from a simple reconstruction task to a domain translation task,
where the model relies on less noisy domains to reconstruct more noisy domains.
We present results on a multi-domain (with more than two domains) synthetic
image translation dataset with challenging semantic domain inversion.
</p></li>
</ul>

<h3>Title: Efficient Post-training Quantization with FP8 Formats. (arXiv:2309.14592v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14592">http://arxiv.org/abs/2309.14592</a></li>
<li>Code URL: https://github.com/intel/neural-compressor</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14592]] Efficient Post-training Quantization with FP8 Formats(http://arxiv.org/abs/2309.14592)</code></li>
<li>Summary: <p>Recent advances in deep learning methods such as LLMs and Diffusion models
have created a need for improved quantization methods that can meet the
computational demands of these modern architectures while maintaining accuracy.
Towards this goal, we study the advantages of FP8 data formats for
post-training quantization across 75 unique network architectures covering a
wide range of tasks, including machine translation, language modeling, text
generation, image classification, generation, and segmentation. We examine
three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects
of varying degrees of trade-off between dynamic range and precision on model
accuracy. Based on our extensive study, we developed a quantization workflow
that generalizes across different network architectures. Our empirical results
show that FP8 formats outperform INT8 in multiple aspects, including workload
coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader
range of operations. Furthermore, our findings suggest that E4M3 is better
suited for NLP models, whereas E3M4 performs marginally better than E4M3 on
computer vision tasks. The code is publicly available on Intel Neural
Compressor: https://github.com/intel/neural-compressor.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Assessment of IBM and NASA's geospatial foundation model in flood inundation mapping. (arXiv:2309.14500v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14500">http://arxiv.org/abs/2309.14500</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14500]] Assessment of IBM and NASA's geospatial foundation model in flood inundation mapping(http://arxiv.org/abs/2309.14500)</code></li>
<li>Summary: <p>Vision foundation models are a new frontier in GeoAI research because of
their potential to enable powerful image analysis by learning and extracting
important image features from vast amounts of geospatial data. This paper
evaluates the performance of the first-of-its-kind geospatial foundation model,
IBM-NASA's Prithvi, to support a crucial geospatial analysis task: flood
inundation mapping. This model is compared with popular convolutional neural
network and vision transformer-based architectures in terms of mapping accuracy
for flooded areas. A benchmark dataset, Sen1Floods11, is used in the
experiments, and the models' predictability, generalizability, and
transferability are evaluated based on both a test dataset and a dataset that
is completely unseen by the model. Results show the impressive transferability
of the Prithvi model, highlighting its performance advantages in segmenting
flooded areas in previously unseen regions. The findings also suggest areas for
improvement for the Prithvi model in terms of adopting multi-scale
representation learning, developing more end-to-end pipelines for high-level
image analysis tasks, and offering more flexibility in terms of input data
bands.
</p></li>
</ul>

<h3>Title: Event Stream-based Visual Object Tracking: A High-Resolution Benchmark Dataset and A Novel Baseline. (arXiv:2309.14611v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14611">http://arxiv.org/abs/2309.14611</a></li>
<li>Code URL: https://github.com/event-ahu/eventvot_benchmark</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14611]] Event Stream-based Visual Object Tracking: A High-Resolution Benchmark Dataset and A Novel Baseline(http://arxiv.org/abs/2309.14611)</code></li>
<li>Summary: <p>Tracking using bio-inspired event cameras has drawn more and more attention
in recent years. Existing works either utilize aligned RGB and event data for
accurate tracking or directly learn an event-based tracker. The first category
needs more cost for inference and the second one may be easily influenced by
noisy events or sparse spatial resolution. In this paper, we propose a novel
hierarchical knowledge distillation framework that can fully utilize
multi-modal / multi-view information during training to facilitate knowledge
transfer, enabling us to achieve high-speed and low-latency visual tracking
during testing by using only event signals. Specifically, a teacher
Transformer-based multi-modal tracking framework is first trained by feeding
the RGB frame and event stream simultaneously. Then, we design a new
hierarchical knowledge distillation strategy which includes pairwise
similarity, feature representation, and response maps-based knowledge
distillation to guide the learning of the student Transformer network.
Moreover, since existing event-based tracking datasets are all low-resolution
($346 \times 260$), we propose the first large-scale high-resolution ($1280
\times 720$) dataset named EventVOT. It contains 1141 videos and covers a wide
range of categories such as pedestrians, vehicles, UAVs, ping pongs, etc.
Extensive experiments on both low-resolution (FE240hz, VisEvent, COESOT), and
our newly proposed high-resolution EventVOT dataset fully validated the
effectiveness of our proposed method. The dataset, evaluation toolkit, and
source code are available on
\url{https://github.com/Event-AHU/EventVOT_Benchmark}
</p></li>
</ul>

<h3>Title: A Simple Text to Video Model via Transformer. (arXiv:2309.14683v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14683">http://arxiv.org/abs/2309.14683</a></li>
<li>Code URL: https://github.com/vividitytech/text2videogpt</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14683]] A Simple Text to Video Model via Transformer(http://arxiv.org/abs/2309.14683)</code></li>
<li>Summary: <p>We present a general and simple text to video model based on Transformer.
Since both text and video are sequential data, we encode both texts and images
into the same hidden space, which are further fed into Transformer to capture
the temporal consistency and then decoder to generate either text or images.
Considering the image signal may become weak in the long sequence, we introduce
the U-Net to reconstruct image from its noised version. Specifically, we
increase the noise level to the original image in the long sequence, then use
the $down$ module from U-Net to encode noised images, which are further input
to transformer to predict next clear images. We also add a constraint to
promote motion between any generated image pair in the video. We use GPT2 and
test our approach on UCF101 dataset and show it can generate promising videos.
</p></li>
</ul>

<h3>Title: Tile Classification Based Viewport Prediction with Multi-modal Fusion Transformer. (arXiv:2309.14704v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14704">http://arxiv.org/abs/2309.14704</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14704]] Tile Classification Based Viewport Prediction with Multi-modal Fusion Transformer(http://arxiv.org/abs/2309.14704)</code></li>
<li>Summary: <p>Viewport prediction is a crucial aspect of tile-based 360 video streaming
system. However, existing trajectory based methods lack of robustness, also
oversimplify the process of information construction and fusion between
different modality inputs, leading to the error accumulation problem. In this
paper, we propose a tile classification based viewport prediction method with
Multi-modal Fusion Transformer, namely MFTR. Specifically, MFTR utilizes
transformer-based networks to extract the long-range dependencies within each
modality, then mine intra- and inter-modality relations to capture the combined
impact of user historical inputs and video contents on future viewport
selection. In addition, MFTR categorizes future tiles into two categories: user
interested or not, and selects future viewport as the region that contains most
user interested tiles. Comparing with predicting head trajectories, choosing
future viewport based on tile's binary classification results exhibits better
robustness and interpretability. To evaluate our proposed MFTR, we conduct
extensive experiments on two widely used PVS-HM and Xu-Gaze dataset. MFTR shows
superior performance over state-of-the-art methods in terms of average
prediction accuracy and overlap ratio, also presents competitive computation
efficiency.
</p></li>
</ul>

<h3>Title: IFT: Image Fusion Transformer for Ghost-free High Dynamic Range Imaging. (arXiv:2309.15019v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15019">http://arxiv.org/abs/2309.15019</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15019]] IFT: Image Fusion Transformer for Ghost-free High Dynamic Range Imaging(http://arxiv.org/abs/2309.15019)</code></li>
<li>Summary: <p>Multi-frame high dynamic range (HDR) imaging aims to reconstruct ghost-free
images with photo-realistic details from content-complementary but spatially
misaligned low dynamic range (LDR) images. Existing HDR algorithms are prone to
producing ghosting artifacts as their methods fail to capture long-range
dependencies between LDR frames with large motion in dynamic scenes. To address
this issue, we propose a novel image fusion transformer, referred to as IFT,
which presents a fast global patch searching (FGPS) module followed by a
self-cross fusion module (SCF) for ghost-free HDR imaging. The FGPS searches
the patches from supporting frames that have the closest dependency to each
patch of the reference frame for long-range dependency modeling, while the SCF
conducts intra-frame and inter-frame feature fusion on the patches obtained by
the FGPS with linear complexity to input resolution. By matching similar
patches between frames, objects with large motion ranges in dynamic scenes can
be aligned, which can effectively alleviate the generation of artifacts. In
addition, the proposed FGPS and SCF can be integrated into various deep HDR
methods as efficient plug-in modules. Extensive experiments on multiple
benchmarks show that our method achieves state-of-the-art performance both
quantitatively and qualitatively.
</p></li>
</ul>

<h3>Title: PopBERT. Detecting populism and its host ideologies in the German Bundestag. (arXiv:2309.14355v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14355">http://arxiv.org/abs/2309.14355</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14355]] PopBERT(http://arxiv.org/abs/2309.14355)</code></li>
<li>Summary: <p>The rise of populism concerns many political scientists and practitioners,
yet the detection of its underlying language remains fragmentary. This paper
aims to provide a reliable, valid, and scalable approach to measure populist
stances. For that purpose, we created an annotated dataset based on
parliamentary speeches of the German Bundestag (2013 to 2021). Following the
ideational definition of populism, we label moralizing references to the
virtuous people or the corrupt elite as core dimensions of populist language.
To identify, in addition, how the thin ideology of populism is thickened, we
annotate how populist statements are attached to left-wing or right-wing host
ideologies. We then train a transformer-based model (PopBERT) as a multilabel
classifier to detect and quantify each dimension. A battery of validation
checks reveals that the model has a strong predictive accuracy, provides high
qualitative face validity, matches party rankings of expert surveys, and
detects out-of-sample text snippets correctly. PopBERT enables dynamic analyses
of how German-speaking politicians and parties use populist language as a
strategic device. Furthermore, the annotator-level data may also be applied in
cross-domain applications or to develop related classifiers.
</p></li>
</ul>

<h3>Title: When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs. (arXiv:2309.14488v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14488">http://arxiv.org/abs/2309.14488</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14488]] When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs(http://arxiv.org/abs/2309.14488)</code></li>
<li>Summary: <p>The use of machine learning (ML) models to assess and score textual data has
become increasingly pervasive in an array of contexts including natural
language processing, information retrieval, search and recommendation, and
credibility assessment of online content. A significant disruption at the
intersection of ML and text are text-generating large-language models such as
generative pre-trained transformers (GPTs). We empirically assess the
differences in how ML-based scoring models trained on human content assess the
quality of content generated by humans versus GPTs. To do so, we propose an
analysis framework that encompasses essay scoring ML-models, human and
ML-generated essays, and a statistical model that parsimoniously considers the
impact of type of respondent, prompt genre, and the ML model used for
assessment model. A rich testbed is utilized that encompasses 18,460
human-generated and GPT-based essays. Results of our benchmark analysis reveal
that transformer pretrained language models (PLMs) more accurately score human
essay quality as compared to CNN/RNN and feature-based ML methods.
Interestingly, we find that the transformer PLMs tend to score GPT-generated
text 10-15\% higher on average, relative to human-authored documents.
Conversely, traditional deep learning and feature-based ML models score human
text considerably higher. Further analysis reveals that although the
transformer PLMs are exclusively fine-tuned on human text, they more
prominently attend to certain tokens appearing only in GPT-generated text,
possibly due to familiarity/overlap in pre-training. Our framework and results
have implications for text classification settings where automated scoring of
text is likely to be disrupted by generative AI.
</p></li>
</ul>

<h3>Title: DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models. (arXiv:2309.14509v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14509">http://arxiv.org/abs/2309.14509</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14509]] DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models(http://arxiv.org/abs/2309.14509)</code></li>
<li>Summary: <p>Computation in a typical Transformer-based large language model (LLM) can be
characterized by batch size, hidden dimension, number of layers, and sequence
length. Until now, system works for accelerating LLM training have focused on
the first three dimensions: data parallelism for batch size, tensor parallelism
for hidden size and pipeline parallelism for model depth or layers. These
widely studied forms of parallelism are not targeted or optimized for long
sequence Transformer models. Given practical application needs for long
sequence LLM, renewed attentions are being drawn to sequence parallelism.
However, existing works in sequence parallelism are constrained by
memory-communication inefficiency, limiting their scalability to long sequence
large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable
and effective methodology for enabling highly efficient and scalable LLM
training with extremely long sequence length. DeepSpeed-Ulysses at its core
partitions input data along the sequence dimension and employs an efficient
all-to-all collective communication for attention computation. Theoretical
communication analysis shows that whereas other methods incur communication
overhead as sequence length increases, DeepSpeed-Ulysses maintains constant
communication volume when sequence length and compute devices are increased
proportionally. Furthermore, experimental evaluations show that
DeepSpeed-Ulysses trains 2.5X faster with 4X longer sequence length than the
existing method SOTA baseline.
</p></li>
</ul>

<h3>Title: Automating question generation from educational text. (arXiv:2309.15004v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15004">http://arxiv.org/abs/2309.15004</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15004]] Automating question generation from educational text(http://arxiv.org/abs/2309.15004)</code></li>
<li>Summary: <p>The use of question-based activities (QBAs) is wide-spread in education,
traditionally forming an integral part of the learning and assessment process.
In this paper, we design and evaluate an automated question generation tool for
formative and summative assessment in schools. We present an expert survey of
one hundred and four teachers, demonstrating the need for automated generation
of QBAs, as a tool that can significantly reduce the workload of teachers and
facilitate personalized learning experiences. Leveraging the recent
advancements in generative AI, we then present a modular framework employing
transformer based language models for automatic generation of multiple-choice
questions (MCQs) from textual content. The presented solution, with distinct
modules for question generation, correct answer prediction, and distractor
formulation, enables us to evaluate different language models and generation
techniques. Finally, we perform an extensive quantitative and qualitative
evaluation, demonstrating trade-offs in the use of different techniques and
models.
</p></li>
</ul>

<h3>Title: Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models. (arXiv:2309.15098v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15098">http://arxiv.org/abs/2309.15098</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15098]] Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models(http://arxiv.org/abs/2309.15098)</code></li>
<li>Summary: <p>We investigate the internal behavior of Transformer-based Large Language
Models (LLMs) when they generate factually incorrect text. We propose modeling
factual queries as Constraint Satisfaction Problems and use this framework to
investigate how the model interacts internally with factual constraints.
Specifically, we discover a strong positive relation between the model's
attention to constraint tokens and the factual accuracy of its responses. In
our curated suite of 11 datasets with over 40,000 prompts, we study the task of
predicting factual errors with the Llama-2 family across all scales (7B, 13B,
70B). We propose SAT Probe, a method probing self-attention patterns, that can
predict constraint satisfaction and factual errors, and allows early error
identification. The approach and findings demonstrate how using the mechanistic
understanding of factuality in LLMs can enhance reliability.
</p></li>
</ul>

<h3>Title: Tranformer-based classification of user queries for medical consultancy with respect to expert specialisation. (arXiv:2309.14662v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14662">http://arxiv.org/abs/2309.14662</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14662]] Tranformer-based classification of user queries for medical consultancy with respect to expert specialisation(http://arxiv.org/abs/2309.14662)</code></li>
<li>Summary: <p>The need for skilled medical support is growing in the era of digital
healthcare. This research presents an innovative strategy, utilising the RuBERT
model, for categorising user inquiries in the field of medical consultation
with a focus on expert specialisation. By harnessing the capabilities of
transformers, we fine-tuned the pre-trained RuBERT model on a varied dataset,
which facilitates precise correspondence between queries and particular medical
specialisms. Using a comprehensive dataset, we have demonstrated our approach's
superior performance with an F1-score of over 92%, calculated through both
cross-validation and the traditional split of test and train datasets. Our
approach has shown excellent generalisation across medical domains such as
cardiology, neurology and dermatology. This methodology provides practical
benefits by directing users to appropriate specialists for prompt and targeted
medical advice. It also enhances healthcare system efficiency, reduces
practitioner burden, and improves patient care quality. In summary, our
suggested strategy facilitates the attainment of specific medical knowledge,
offering prompt and precise advice within the digital healthcare field.
</p></li>
</ul>

<h3>Title: Leveraging Herpangina Data to Enhance Hospital-level Prediction of Hand-Foot-and-Mouth Disease Admissions Using UPTST. (arXiv:2309.14674v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14674">http://arxiv.org/abs/2309.14674</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14674]] Leveraging Herpangina Data to Enhance Hospital-level Prediction of Hand-Foot-and-Mouth Disease Admissions Using UPTST(http://arxiv.org/abs/2309.14674)</code></li>
<li>Summary: <p>Outbreaks of hand-foot-and-mouth disease(HFMD) have been associated with
significant morbidity and, in severe cases, mortality. Accurate forecasting of
daily admissions of pediatric HFMD patients is therefore crucial for aiding the
hospital in preparing for potential outbreaks and mitigating nosocomial
transmissions. To address this pressing need, we propose a novel
transformer-based model with a U-net shape, utilizing the patching strategy and
the joint prediction strategy that capitalizes on insights from herpangina, a
disease closely correlated with HFMD. This model also integrates representation
learning by introducing reconstruction loss as an auxiliary loss. The results
show that our U-net Patching Time Series Transformer (UPTST) model outperforms
existing approaches in both long- and short-arm prediction accuracy of HFMD at
hospital-level. Furthermore, the exploratory extension experiments show that
the model's capabilities extend beyond prediction of infectious disease,
suggesting broader applicability in various domains.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Generative Escher Meshes. (arXiv:2309.14564v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14564">http://arxiv.org/abs/2309.14564</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14564]] Generative Escher Meshes(http://arxiv.org/abs/2309.14564)</code></li>
<li>Summary: <p>This paper proposes a fully-automatic, text-guided generative method for
producing periodic, repeating, tile-able 2D art, such as the one seen on
floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to the
standard concept of a seamless texture, i.e., square images that are seamless
when tiled, our method generates non-square tilings which comprise solely of
repeating copies of the same object. It achieves this by optimizing both
geometry and color of a 2D mesh, in order to generate a non-square tile in the
shape and appearance of the desired object, with close to no additional
background details. We enable geometric optimization of tilings by our key
technical contribution: an unconstrained, differentiable parameterization of
the space of all possible tileable shapes for a given symmetry group. Namely,
we prove that modifying the laplacian used in a 2D mesh-mapping technique -
Orbifold Tutte Embedding - can achieve all possible tiling configurations for a
chosen planar symmetry group. We thus consider both the mesh's tile-shape and
its texture as optimizable parameters, rendering the textured mesh via a
differentiable renderer. We leverage a trained image diffusion model to define
a loss on the resulting image, thereby updating the mesh's parameters based on
its appearance matching the text prompt. We show our method is able to produce
plausible, appealing results, with non-trivial tiles, for a variety of
different periodic tiling patterns.
</p></li>
</ul>

<h3>Title: Machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence. (arXiv:2309.14379v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14379">http://arxiv.org/abs/2309.14379</a></li>
<li>Code URL: https://github.com/andreskarjus/machineassistedmixedmethods</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14379]] Machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence(http://arxiv.org/abs/2309.14379)</code></li>
<li>Summary: <p>The increasing capacities of large language models (LLMs) present an
unprecedented opportunity to scale up data analytics in the humanities and
social sciences, augmenting and automating qualitative analytic tasks
previously typically allocated to human labor. This contribution proposes a
systematic mixed methods framework to harness qualitative analytic expertise,
machine scalability, and rigorous quantification, with attention to
transparency and replicability. 16 machine-assisted case studies are showcased
as proof of concept. Tasks include linguistic and discourse analysis, lexical
semantic change detection, interview analysis, historical event cause inference
and text mining, detection of political stance, text and idea reuse, genre
composition in literature and film; social network inference, automated
lexicography, missing metadata augmentation, and multimodal visual cultural
analytics. In contrast to the focus on English in the emerging LLM
applicability literature, many examples here deal with scenarios involving
smaller languages and historical texts prone to digitization distortions. In
all but the most difficult tasks requiring expert knowledge, generative LLMs
can demonstrably serve as viable research instruments. LLM (and human)
annotations may contain errors and variation, but the agreement rate can and
should be accounted for in subsequent statistical modeling; a bootstrapping
approach is discussed. The replications among the case studies illustrate how
tasks previously requiring potentially months of team effort and complex
computational pipelines, can now be accomplished by an LLM-assisted scholar in
a fraction of the time. Importantly, this approach is not intended to replace,
but to augment researcher knowledge and skills. With these opportunities in
sight, qualitative expertise and the ability to pose insightful questions have
arguably never been more critical.
</p></li>
</ul>

<h3>Title: Introducing DictaLM -- A Large Generative Language Model for Modern Hebrew. (arXiv:2309.14568v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14568">http://arxiv.org/abs/2309.14568</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14568]] Introducing DictaLM -- A Large Generative Language Model for Modern Hebrew(http://arxiv.org/abs/2309.14568)</code></li>
<li>Summary: <p>We present DictaLM, a large-scale language model tailored for Modern Hebrew.
Boasting 7B parameters, this model is predominantly trained on Hebrew-centric
data. As a commitment to promoting research and development in the Hebrew
language, we release both the foundation model and the instruct-tuned model
under a Creative Commons license. Concurrently, we introduce DictaLM-Rab,
another foundation model geared towards Rabbinic/Historical Hebrew. These
foundation models serve as ideal starting points for fine-tuning various
Hebrew-specific tasks, such as instruction, Q&amp;A, sentiment analysis, and more.
This release represents a preliminary step, offering an initial Hebrew LLM
model for the Hebrew NLP community to experiment with.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Analyzing the Efficacy of an LLM-Only Approach for Image-based Document Question Answering. (arXiv:2309.14389v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14389">http://arxiv.org/abs/2309.14389</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14389]] Analyzing the Efficacy of an LLM-Only Approach for Image-based Document Question Answering(http://arxiv.org/abs/2309.14389)</code></li>
<li>Summary: <p>Recent document question answering models consist of two key components: the
vision encoder, which captures layout and visual elements in images, and a
Large Language Model (LLM) that helps contextualize questions to the image and
supplements them with external world knowledge to generate accurate answers.
However, the relative contributions of the vision encoder and the language
model in these tasks remain unclear. This is especially interesting given the
effectiveness of instruction-tuned LLMs, which exhibit remarkable adaptability
to new tasks. To this end, we explore the following aspects in this work: (1)
The efficacy of an LLM-only approach on document question answering tasks (2)
strategies for serializing textual information within document images and
feeding it directly to an instruction-tuned LLM, thus bypassing the need for an
explicit vision encoder (3) thorough quantitative analysis on the feasibility
of such an approach. Our comprehensive analysis encompasses six diverse
benchmark datasets, utilizing LLMs of varying scales. Our findings reveal that
a strategy exclusively reliant on the LLM yields results that are on par with
or closely approach state-of-the-art performance across a range of datasets. We
posit that this evaluation framework will serve as a guiding resource for
selecting appropriate datasets for future research endeavors that emphasize the
fundamental importance of layout and image content information.
</p></li>
</ul>

<h3>Title: VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning. (arXiv:2309.15091v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15091">http://arxiv.org/abs/2309.15091</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15091]] VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning(http://arxiv.org/abs/2309.15091)</code></li>
<li>Summary: <p>Although recent text-to-video (T2V) generation methods have seen significant
advancements, most of these works focus on producing short video clips of a
single event with a single background (i.e., single-scene videos). Meanwhile,
recent large language models (LLMs) have demonstrated their capability in
generating layouts and programs to control downstream visual modules such as
image generation models. This raises an important question: can we leverage the
knowledge embedded in these LLMs for temporally consistent long video
generation? In this paper, we propose VideoDirectorGPT, a novel framework for
consistent multi-scene video generation that uses the knowledge of LLMs for
video content planning and grounded video generation. Specifically, given a
single text prompt, we first ask our video planner LLM (GPT-4) to expand it
into a 'video plan', which involves generating the scene descriptions, the
entities with their respective layouts, the background for each scene, and
consistency groupings of the entities and backgrounds. Next, guided by this
output from the video planner, our video generator, Layout2Vid, has explicit
control over spatial layouts and can maintain temporal consistency of
entities/backgrounds across scenes, while only trained with image-level
annotations. Our experiments demonstrate that VideoDirectorGPT framework
substantially improves layout and movement control in both single- and
multi-scene video generation and can generate multi-scene videos with visual
consistency across scenes, while achieving competitive performance with SOTAs
in open-domain single-scene T2V generation. We also demonstrate that our
framework can dynamically control the strength for layout guidance and can also
generate videos with user-provided images. We hope our framework can inspire
future work on better integrating the planning ability of LLMs into consistent
long video generation.
</p></li>
</ul>

<h3>Title: An In-depth Survey of Large Language Model-based Artificial Intelligence Agents. (arXiv:2309.14365v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14365">http://arxiv.org/abs/2309.14365</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14365]] An In-depth Survey of Large Language Model-based Artificial Intelligence Agents(http://arxiv.org/abs/2309.14365)</code></li>
<li>Summary: <p>Due to the powerful capabilities demonstrated by large language model (LLM),
there has been a recent surge in efforts to integrate them with AI agents to
enhance their performance. In this paper, we have explored the core differences
and characteristics between LLM-based AI agents and traditional AI agents.
Specifically, we first compare the fundamental characteristics of these two
types of agents, clarifying the significant advantages of LLM-based agents in
handling natural language, knowledge storage, and reasoning capabilities.
Subsequently, we conducted an in-depth analysis of the key components of AI
agents, including planning, memory, and tool use. Particularly, for the crucial
component of memory, this paper introduced an innovative classification scheme,
not only departing from traditional classification methods but also providing a
fresh perspective on the design of an AI agent's memory system. We firmly
believe that in-depth research and understanding of these core components will
lay a solid foundation for the future advancement of AI agent technology. At
the end of the paper, we provide directional suggestions for further research
in this field, with the hope of offering valuable insights to scholars and
researchers in the field.
</p></li>
</ul>

<h3>Title: LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models. (arXiv:2309.14393v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14393">http://arxiv.org/abs/2309.14393</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14393]] LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models(http://arxiv.org/abs/2309.14393)</code></li>
<li>Summary: <p>The carbon footprint associated with large language models (LLMs) is a
significant concern, encompassing emissions from their training, inference,
experimentation, and storage processes, including operational and embodied
carbon emissions. An essential aspect is accurately estimating the carbon
impact of emerging LLMs even before their training, which heavily relies on GPU
usage. Existing studies have reported the carbon footprint of LLM training, but
only one tool, mlco2, can predict the carbon footprint of new neural networks
prior to physical training. However, mlco2 has several serious limitations. It
cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs,
disregards critical architectural parameters, focuses solely on GPUs, and
cannot model embodied carbon footprints. Addressing these gaps, we introduce
\textit{LLMCarbon}, an end-to-end carbon footprint projection model designed
for both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly
enhances the accuracy of carbon footprint estimations for various LLMs.
</p></li>
</ul>

<h3>Title: Art or Artifice? Large Language Models and the False Promise of Creativity. (arXiv:2309.14556v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14556">http://arxiv.org/abs/2309.14556</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14556]] Art or Artifice? Large Language Models and the False Promise of Creativity(http://arxiv.org/abs/2309.14556)</code></li>
<li>Summary: <p>Researchers have argued that large language models (LLMs) exhibit
high-quality writing capabilities from blogs to stories. However, evaluating
objectively the creativity of a piece of writing is challenging. Inspired by
the Torrance Test of Creative Thinking (TTCT), which measures creativity as a
process, we use the Consensual Assessment Technique [3] and propose the
Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product.
TTCW consists of 14 binary tests organized into the original dimensions of
Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative
writers and implement a human assessment of 48 stories written either by
professional authors or LLMs using TTCW. Our analysis shows that LLM-generated
stories pass 3-10X less TTCW tests than stories written by professionals. In
addition, we explore the use of LLMs as assessors to automate the TTCW
evaluation, revealing that none of the LLMs positively correlate with the
expert assessments.
</p></li>
</ul>

<h3>Title: ConPET: Continual Parameter-Efficient Tuning for Large Language Models. (arXiv:2309.14763v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14763">http://arxiv.org/abs/2309.14763</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14763]] ConPET: Continual Parameter-Efficient Tuning for Large Language Models(http://arxiv.org/abs/2309.14763)</code></li>
<li>Summary: <p>Continual learning necessitates the continual adaptation of models to newly
emerging tasks while minimizing the catastrophic forgetting of old ones. This
is extremely challenging for large language models (LLMs) with vanilla
full-parameter tuning due to high computation costs, memory consumption, and
forgetting issue. Inspired by the success of parameter-efficient tuning (PET),
we propose Continual Parameter-Efficient Tuning (ConPET), a generalizable
paradigm for continual task adaptation of LLMs with task-number-independent
training complexity. ConPET includes two versions with different application
scenarios. First, Static ConPET can adapt former continual learning methods
originally designed for relatively smaller models to LLMs through PET and a
dynamic replay strategy, which largely reduces the tuning costs and alleviates
the over-fitting and forgetting issue. Furthermore, to maintain scalability,
Dynamic ConPET adopts separate PET modules for different tasks and a PET module
selector for dynamic optimal selection. In our extensive experiments, the
adaptation of Static ConPET helps multiple former methods reduce the scale of
tunable parameters by over 3,000 times and surpass the PET-only baseline by at
least 5 points on five smaller benchmarks, while Dynamic ConPET gains its
advantage on the largest dataset. The codes and datasets are available at
https://github.com/Raincleared-Song/ConPET.
</p></li>
</ul>

<h3>Title: Boosting In-Context Learning with Factual Knowledge. (arXiv:2309.14771v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14771">http://arxiv.org/abs/2309.14771</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14771]] Boosting In-Context Learning with Factual Knowledge(http://arxiv.org/abs/2309.14771)</code></li>
<li>Summary: <p>In-Context Learning (ICL) over Large language models (LLMs) aims at solving
previously unseen tasks by conditioning on a few training examples, eliminating
the need for parameter updates and achieving competitive performance. In this
paper, we demonstrate that factual knowledge is imperative for the performance
of ICL in three core facets, i.e., the inherent knowledge learned in LLMs, the
factual knowledge derived from the selected in-context examples, and the
knowledge biases in LLMs for output generation. To unleash the power of LLMs in
few-shot learning scenarios, we introduce a novel Knowledgeable In-Context
Tuning (KICT) framework to further improve the performance of ICL: 1) injecting
factual knowledge to LLMs during continual self-supervised pre-training, 2)
judiciously selecting the examples with high knowledge relevance, and 3)
calibrating the prediction results based on prior knowledge. We evaluate the
proposed approaches on auto-regressive LLMs (e.g., GPT-style models) over
multiple text classification and question answering tasks. Experimental results
demonstrate that KICT substantially outperforms strong baselines, and improves
by more than 13% and 7% of accuracy on text classification and question
answering tasks, respectively.
</p></li>
</ul>

<h3>Title: Exploring Small Language Models with Prompt-Learning Paradigm for Efficient Domain-Specific Text Classification. (arXiv:2309.14779v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14779">http://arxiv.org/abs/2309.14779</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14779]] Exploring Small Language Models with Prompt-Learning Paradigm for Efficient Domain-Specific Text Classification(http://arxiv.org/abs/2309.14779)</code></li>
<li>Summary: <p>Domain-specific text classification faces the challenge of scarce labeled
data due to the high cost of manual labeling. Prompt-learning, known for its
efficiency in few-shot scenarios, is proposed as an alternative to traditional
fine-tuning methods. And besides, although large language models (LLMs) have
gained prominence, small language models (SLMs, with under 1B parameters) offer
significant customizability, adaptability, and cost-effectiveness for
domain-specific tasks, given industry constraints. In this study, we
investigate the potential of SLMs combined with prompt-learning paradigm for
domain-specific text classification, specifically within customer-agent
interactions in retail. Our evaluations show that, in few-shot settings when
prompt-based model fine-tuning is possible, T5-base, a typical SLM with 220M
parameters, achieve approximately 75% accuracy with limited labeled data (up to
15% of full data), which shows great potentials of SLMs with prompt-learning.
Based on this, We further validate the effectiveness of active few-shot
sampling and the ensemble strategy in the prompt-learning pipeline that
contribute to a remarkable performance gain. Besides, in zero-shot settings
with a fixed model, we underscore a pivotal observation that, although the
GPT-3.5-turbo equipped with around 154B parameters garners an accuracy of
55.16%, the power of well designed prompts becomes evident when the
FLAN-T5-large, a model with a mere 0.5% of GPT-3.5-turbo's parameters, achieves
an accuracy exceeding 31% with the optimized prompt, a leap from its sub-18%
performance with an unoptimized one. Our findings underscore the promise of
prompt-learning in classification tasks with SLMs, emphasizing the benefits of
active few-shot sampling, and ensemble strategies in few-shot settings, and the
importance of prompt engineering in zero-shot settings.
</p></li>
</ul>

<h3>Title: Large Language Model Alignment: A Survey. (arXiv:2309.15025v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15025">http://arxiv.org/abs/2309.15025</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15025]] Large Language Model Alignment: A Survey(http://arxiv.org/abs/2309.15025)</code></li>
<li>Summary: <p>Recent years have witnessed remarkable progress made in large language models
(LLMs). Such advancements, while garnering significant attention, have
concurrently elicited various concerns. The potential of these models is
undeniably vast; however, they may yield texts that are imprecise, misleading,
or even detrimental. Consequently, it becomes paramount to employ alignment
techniques to ensure these models to exhibit behaviors consistent with human
values.
</p>
<p>This survey endeavors to furnish an extensive exploration of alignment
methodologies designed for LLMs, in conjunction with the extant capability
research in this domain. Adopting the lens of AI alignment, we categorize the
prevailing methods and emergent proposals for the alignment of LLMs into outer
and inner alignment. We also probe into salient issues including the models'
interpretability, and potential vulnerabilities to adversarial attacks. To
assess LLM alignment, we present a wide variety of benchmarks and evaluation
methodologies. After discussing the state of alignment research for LLMs, we
finally cast a vision toward the future, contemplating the promising avenues of
research that lie ahead.
</p>
<p>Our aspiration for this survey extends beyond merely spurring research
interests in this realm. We also envision bridging the gap between the AI
alignment research community and the researchers engrossed in the capability
exploration of LLMs for both capable and safe LLMs.
</p></li>
</ul>

<h3>Title: Natural Language based Context Modeling and Reasoning with LLMs: A Tutorial. (arXiv:2309.15074v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15074">http://arxiv.org/abs/2309.15074</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15074]] Natural Language based Context Modeling and Reasoning with LLMs: A Tutorial(http://arxiv.org/abs/2309.15074)</code></li>
<li>Summary: <p>Large language models (LLMs) have become phenomenally surging, since
2018--two decades after introducing context-awareness into computing systems.
Through taking into account the situations of ubiquitous devices, users and the
societies, context-aware computing has enabled a wide spectrum of innovative
applications, such as assisted living, location-based social network services
and so on. To recognize contexts and make decisions for actions accordingly,
various artificial intelligence technologies, such as Ontology and OWL, have
been adopted as representations for context modeling and reasoning. Recently,
with the rise of LLMs and their improved natural language understanding and
reasoning capabilities, it has become feasible to model contexts using natural
language and perform context reasoning by interacting with LLMs such as ChatGPT
and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and
autonomous agents (AutoAgents) that enable LLMs to perform context modeling and
reasoning without requiring fine-tuning of the model. We organize and introduce
works in the related field, and name this computing paradigm as the LLM-driven
Context-aware Computing (LCaC). In the LCaC paradigm, users' requests, sensors
reading data, and the command to actuators are supposed to be represented as
texts. Given the text of users' request and sensor data, the AutoAgent models
the context by prompting and sends to the LLM for context reasoning. LLM
generates a plan of actions and responds to the AutoAgent, which later follows
the action plan to foster context-awareness. To prove the concepts, we use two
showcases--(1) operating a mobile z-arm in an apartment for assisted living,
and (2) planning a trip and scheduling the itinerary in a context-aware and
personalized manner.
</p></li>
</ul>

<h3>Title: Are Human-generated Demonstrations Necessary for In-context Learning?. (arXiv:2309.14681v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14681">http://arxiv.org/abs/2309.14681</a></li>
<li>Code URL: https://github.com/ruili33/sec</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14681]] Are Human-generated Demonstrations Necessary for In-context Learning?(http://arxiv.org/abs/2309.14681)</code></li>
<li>Summary: <p>Despite the promising few-shot ability of large language models (LLMs), the
standard paradigm of In-context Learning (ICL) suffers the disadvantages of
susceptibility to selected demonstrations and the intricacy to generate these
demonstrations. In this paper, we raise the fundamental question that whether
human-generated demonstrations are necessary for ICL. To answer this question,
we propose self-contemplation prompting strategy (SEC), a paradigm free from
human-crafted demonstrations. The key point of SEC is that, instead of using
hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create
demonstrations on their own, based on which the final output is generated. SEC
is a flexible framework and can be adapted to both the vanilla ICL and the
chain-of-thought (CoT), but with greater ease: as the manual-generation process
of both examples and rationale can be saved. Extensive experiments in
arithmetic reasoning, commonsense reasoning, multi-task language understanding,
and code generation benchmarks, show that SEC, which does not require
hand-crafted demonstrations, significantly outperforms the zero-shot learning
strategy, and achieves comparable results to ICL with hand-crafted
demonstrations. This demonstrates that, for many tasks, contemporary LLMs
possess a sufficient level of competence to exclusively depend on their own
capacity for decision making, removing the need for external training data.
Code is available at https://github.com/ruili33/SEC.
</p></li>
</ul>

<h3>Title: QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models. (arXiv:2309.14717v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14717">http://arxiv.org/abs/2309.14717</a></li>
<li>Code URL: https://github.com/yuhuixu1993/qa-lora</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14717]] QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models(http://arxiv.org/abs/2309.14717)</code></li>
<li>Summary: <p>Recently years have witnessed a rapid development of large language models
(LLMs). Despite the strong ability in many language-understanding tasks, the
heavy computational burden largely restricts the application of LLMs especially
when one needs to deploy them onto edge devices. In this paper, we propose a
quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies
in the imbalanced degrees of freedom of quantization and adaptation, and the
solution is to use group-wise operators which increase the degree of freedom of
quantization meanwhile decreasing that of adaptation. QA-LoRA is easily
implemented with a few lines of code, and it equips the original LoRA with
two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized
(e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the
LLM and auxiliary weights are naturally integrated into a quantized model
without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model
families and validate its effectiveness in different fine-tuning datasets and
downstream scenarios. Code will be made available at
https://github.com/yuhuixu1993/qa-lora.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: ZiCo-BC: A Bias Corrected Zero-Shot NAS for Vision Tasks. (arXiv:2309.14666v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14666">http://arxiv.org/abs/2309.14666</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14666]] ZiCo-BC: A Bias Corrected Zero-Shot NAS for Vision Tasks(http://arxiv.org/abs/2309.14666)</code></li>
<li>Summary: <p>Zero-Shot Neural Architecture Search (NAS) approaches propose novel
training-free metrics called zero-shot proxies to substantially reduce the
search time compared to the traditional training-based NAS. Despite the success
on image classification, the effectiveness of zero-shot proxies is rarely
evaluated on complex vision tasks such as semantic segmentation and object
detection. Moreover, existing zero-shot proxies are shown to be biased towards
certain model characteristics which restricts their broad applicability. In
this paper, we empirically study the bias of state-of-the-art (SOTA) zero-shot
proxy ZiCo across multiple vision tasks and observe that ZiCo is biased towards
thinner and deeper networks, leading to sub-optimal architectures. To solve the
problem, we propose a novel bias correction on ZiCo, called ZiCo-BC. Our
extensive experiments across various vision tasks (image classification, object
detection and semantic segmentation) show that our approach can successfully
search for architectures with higher accuracy and significantly lower latency
on Samsung Galaxy S10 devices.
</p></li>
</ul>

<h3>Title: Treating Motion as Option with Output Selection for Unsupervised Video Object Segmentation. (arXiv:2309.14786v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14786">http://arxiv.org/abs/2309.14786</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14786]] Treating Motion as Option with Output Selection for Unsupervised Video Object Segmentation(http://arxiv.org/abs/2309.14786)</code></li>
<li>Summary: <p>Unsupervised video object segmentation (VOS) is a task that aims to detect
the most salient object in a video without external guidance about the object.
To leverage the property that salient objects usually have distinctive
movements compared to the background, recent methods collaboratively use motion
cues extracted from optical flow maps with appearance cues extracted from RGB
images. However, as optical flow maps are usually very relevant to segmentation
masks, the network is easy to be learned overly dependent on the motion cues
during network training. As a result, such two-stream approaches are vulnerable
to confusing motion cues, making their prediction unstable. To relieve this
issue, we design a novel motion-as-option network by treating motion cues as
optional. During network training, RGB images are randomly provided to the
motion encoder instead of optical flow maps, to implicitly reduce motion
dependency of the network. As the learned motion encoder can deal with both RGB
images and optical flow maps, two different predictions can be generated
depending on which source information is used as motion input. In order to
fully exploit this property, we also propose an adaptive output selection
algorithm to adopt optimal prediction result at test time. Our proposed
approach affords state-of-the-art performance on all public benchmark datasets,
even maintaining real-time inference speed.
</p></li>
</ul>

<h3>Title: Discrepancy Matters: Learning from Inconsistent Decoder Features for Consistent Semi-supervised Medical Image Segmentation. (arXiv:2309.14819v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14819">http://arxiv.org/abs/2309.14819</a></li>
<li>Code URL: https://github.com/maxwell0027/lefed</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14819]] Discrepancy Matters: Learning from Inconsistent Decoder Features for Consistent Semi-supervised Medical Image Segmentation(http://arxiv.org/abs/2309.14819)</code></li>
<li>Summary: <p>Semi-supervised learning (SSL) has been proven beneficial for mitigating the
issue of limited labeled data especially on the task of volumetric medical
image segmentation. Unlike previous SSL methods which focus on exploring highly
confident pseudo-labels or developing consistency regularization schemes, our
empirical findings suggest that inconsistent decoder features emerge naturally
when two decoders strive to generate consistent predictions. Based on the
observation, we first analyze the treasure of discrepancy in learning towards
consistency, under both pseudo-labeling and consistency regularization
settings, and subsequently propose a novel SSL method called LeFeD, which
learns the feature-level discrepancy obtained from two decoders, by feeding the
discrepancy as a feedback signal to the encoder. The core design of LeFeD is to
enlarge the difference by training differentiated decoders, and then learn from
the inconsistent information iteratively. We evaluate LeFeD against eight
state-of-the-art (SOTA) methods on three public datasets. Experiments show
LeFeD surpasses competitors without any bells and whistles such as uncertainty
estimation and strong constraints, as well as setting a new state-of-the-art
for semi-supervised medical image segmentation. Code is available at
\textcolor{cyan}{https://github.com/maxwell0027/LeFeD}
</p></li>
</ul>

<h3>Title: Addressing Data Misalignment in Image-LiDAR Fusion on Point Cloud Segmentation. (arXiv:2309.14932v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14932">http://arxiv.org/abs/2309.14932</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14932]] Addressing Data Misalignment in Image-LiDAR Fusion on Point Cloud Segmentation(http://arxiv.org/abs/2309.14932)</code></li>
<li>Summary: <p>With the advent of advanced multi-sensor fusion models, there has been a
notable enhancement in the performance of perception tasks within in terms of
autonomous driving. Despite these advancements, the challenges persist,
particularly in the fusion of data from cameras and LiDAR sensors. A critial
concern is the accurate alignment of data from these disparate sensors. Our
observations indicate that the projected positions of LiDAR points often
misalign on the corresponding image. Furthermore, fusion models appear to
struggle in accurately segmenting these misaligned points. In this paper, we
would like to address this problem carefully, with a specific focus on the
nuScenes dataset and the SOTA of fusion models 2DPASS, and providing the
possible solutions or potential improvements.
</p></li>
</ul>

<h3>Title: MoCaE: Mixture of Calibrated Experts Significantly Improves Object Detection. (arXiv:2309.14976v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14976">http://arxiv.org/abs/2309.14976</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14976]] MoCaE: Mixture of Calibrated Experts Significantly Improves Object Detection(http://arxiv.org/abs/2309.14976)</code></li>
<li>Summary: <p>We propose an extremely simple and highly effective approach to faithfully
combine different object detectors to obtain a Mixture of Experts (MoE) that
has a superior accuracy to the individual experts in the mixture. We find that
naively combining these experts in a similar way to the well-known Deep
Ensembles (DEs), does not result in an effective MoE. We identify the
incompatibility between the confidence score distribution of different
detectors to be the primary reason for such failure cases. Therefore, to
construct the MoE, our proposal is to first calibrate each individual detector
against a target calibration function. Then, filter and refine all the
predictions from different detectors in the mixture. We term this approach as
MoCaE and demonstrate its effectiveness through extensive experiments on object
detection, instance segmentation and rotated object detection tasks.
Specifically, MoCaE improves (i) three strong object detectors on COCO test-dev
by $2.4$ $\mathrm{AP}$ by reaching $59.0$ $\mathrm{AP}$; (ii) instance
segmentation methods on the challenging long-tailed LVIS dataset by $2.3$
$\mathrm{AP}$; and (iii) all existing rotated object detectors by reaching
$82.62$ $\mathrm{AP_{50}}$ on DOTA dataset, establishing a new state-of-the-art
(SOTA). Code will be made public.
</p></li>
</ul>

<h3>Title: Segmentation-Free Streaming Machine Translation. (arXiv:2309.14823v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14823">http://arxiv.org/abs/2309.14823</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14823]] Segmentation-Free Streaming Machine Translation(http://arxiv.org/abs/2309.14823)</code></li>
<li>Summary: <p>Streaming Machine Translation (MT) is the task of translating an unbounded
input text stream in real-time. The traditional cascade approach, which
combines an Automatic Speech Recognition (ASR) and an MT system, relies on an
intermediate segmentation step which splits the transcription stream into
sentence-like units. However, the incorporation of a hard segmentation
constrains the MT system and is a source of errors. This paper proposes a
Segmentation-Free framework that enables the model to translate an unsegmented
source stream by delaying the segmentation decision until the translation has
been generated. Extensive experiments show how the proposed Segmentation-Free
framework has better quality-latency trade-off than competing approaches that
use an independent segmentation model. Software, data and models will be
released upon paper acceptance.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
