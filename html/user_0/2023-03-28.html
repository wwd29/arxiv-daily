<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Indian Language Summarization using Pretrained Sequence-to-Sequence Models. (arXiv:2303.14461v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14461">http://arxiv.org/abs/2303.14461</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14461] Indian Language Summarization using Pretrained Sequence-to-Sequence Models](http://arxiv.org/abs/2303.14461) #secure</code></li>
<li>Summary: <p>The ILSUM shared task focuses on text summarization for two major Indian
languages- Hindi and Gujarati, along with English. In this task, we experiment
with various pretrained sequence-to-sequence models to find out the best model
for each of the languages. We present a detailed overview of the models and our
approaches in this paper. We secure the first rank across all three sub-tasks
(English, Hindi and Gujarati). This paper also extensively analyzes the impact
of k-fold cross-validation while experimenting with limited data size, and we
also perform various experiments with a combination of the original and a
filtered version of the data to determine the efficacy of the pretrained
models.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: A User-Based Authentication and DoS Mitigation Scheme for Wearable Wireless Body Sensor Networks. (arXiv:2303.14441v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14441">http://arxiv.org/abs/2303.14441</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14441] A User-Based Authentication and DoS Mitigation Scheme for Wearable Wireless Body Sensor Networks](http://arxiv.org/abs/2303.14441) #security</code></li>
<li>Summary: <p>Wireless Body Sensor Networks (WBSNs) is one of the greatest growing
technology for sensing and performing various tasks. The information
transmitted in the WBSNs is vulnerable to cyber-attacks, therefore security is
very important. Denial of Service (DoS) attacks are considered one of the major
threats against WBSNs security. In DoS attacks, an adversary targets to degrade
and shut down the efficient use of the network and disrupt the services in the
network causing them inaccessible to its intended users. If sensitive
information of patients in WBSNs, such as the medical history is accessed by
unauthorized users, the patient may suffer much more than the disease itself,
it may result in loss of life. This paper proposes a User-Based authentication
scheme to mitigate DoS attacks in WBSNs. A five-phase User-Based authentication
DoS mitigation scheme for WBSNs is designed by integrating Elliptic Curve
Cryptography (ECC) with Rivest Cipher 4 (RC4) to ensure a strong authentication
process that will only allow authorized users to access nodes on WBSNs.
</p></li>
</ul>

<h3>Title: No more Reviewer #2: Subverting Automatic Paper-Reviewer Assignment using Adversarial Learning. (arXiv:2303.14443v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14443">http://arxiv.org/abs/2303.14443</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14443] No more Reviewer #2: Subverting Automatic Paper-Reviewer Assignment using Adversarial Learning](http://arxiv.org/abs/2303.14443) #security</code></li>
<li>Summary: <p>The number of papers submitted to academic conferences is steadily rising in
many scientific disciplines. To handle this growth, systems for automatic
paper-reviewer assignments are increasingly used during the reviewing process.
These systems use statistical topic models to characterize the content of
submissions and automate the assignment to reviewers. In this paper, we show
that this automation can be manipulated using adversarial learning. We propose
an attack that adapts a given paper so that it misleads the assignment and
selects its own reviewers. Our attack is based on a novel optimization strategy
that alternates between the feature space and problem space to realize
unobtrusive changes to the paper. To evaluate the feasibility of our attack, we
simulate the paper-reviewer assignment of an actual security conference (IEEE
S&amp;P) with 165 reviewers on the program committee. Our results show that we can
successfully select and remove reviewers without access to the assignment
system. Moreover, we demonstrate that the manipulated papers remain plausible
and are often indistinguishable from benign submissions.
</p></li>
</ul>

<h3>Title: A Hybrid Algorithm to Enhance Wireless Sensor Networks security on the IoT. (arXiv:2303.14445v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14445">http://arxiv.org/abs/2303.14445</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14445] A Hybrid Algorithm to Enhance Wireless Sensor Networks security on the IoT](http://arxiv.org/abs/2303.14445) #security</code></li>
<li>Summary: <p>The Internet of Things (IoT) is a futuristic technology that promises to
connect tons of devices via the internet. As more individuals connect to the
internet, it is believed that communication will generate mountains of data.
IoT is currently leveraging Wireless Sensor Networks (WSNs) to collect,
monitor, and transmit data and sensitive data across wireless networks using
sensor nodes. WSNs encounter a variety of threats posed by attackers, including
unauthorized access and data security. Especially in the context of the
Internet of Things, where small embedded devices with limited computational
capabilities, such as sensor nodes, are expected to connect to a larger
network. As a result, WSNs are vulnerable to a variety of attacks. Furthermore,
implementing security is time-consuming and selective, as traditional security
algorithms degrade network performance due to their computational complexity
and inherent delays. This paper describes an encryption algorithm that combines
the Secure IoT (SIT) algorithm with the Security Protocols for Sensor Networks
(SPINS) security protocol to create the Lightweight Security Algorithm (LSA),
which addresses data security concerns while reducing power consumption in WSNs
without sacrificing performance.
</p></li>
</ul>

<h3>Title: Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban Computing: A Survey. (arXiv:2303.14483v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14483">http://arxiv.org/abs/2303.14483</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14483] Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban Computing: A Survey](http://arxiv.org/abs/2303.14483) #security</code></li>
<li>Summary: <p>With the development of sophisticated sensors and large database
technologies, more and more spatio-temporal data in urban systems are recorded
and stored. Predictive learning for the evolution patterns of these
spatio-temporal data is a basic but important loop in urban computing, which
can better support urban intelligent management decisions, especially in the
fields of transportation, environment, security, public health, etc. Since
traditional statistical learning and deep learning methods can hardly capture
the complex correlations in the urban spatio-temporal data, the framework of
spatio-temporal graph neural network (STGNN) has been proposed in recent years.
STGNNs enable the extraction of complex spatio-temporal dependencies by
integrating graph neural networks (GNNs) and various temporal learning methods.
However, for different predictive learning tasks, it is a challenging problem
to effectively design the spatial dependencies learning modules, temporal
dependencies learning modules and spatio-temporal dependencies fusion methods
in STGNN framework. In this paper, we provide a comprehensive survey on recent
progress on STGNN technologies for predictive learning in urban computing. We
first briefly introduce the construction methods of spatio-temporal graph data
and popular deep learning models that are employed in STGNNs. Then we sort out
the main application domains and specific predictive learning tasks from the
existing literature. Next we analyze the design approaches of STGNN framework
and the combination with some advanced technologies in recent years. Finally,
we conclude the limitations of the existing research and propose some potential
directions.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Blockchain Technology for Preventing Counterfeit in Health Insurance. (arXiv:2303.14416v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14416">http://arxiv.org/abs/2303.14416</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14416] Blockchain Technology for Preventing Counterfeit in Health Insurance](http://arxiv.org/abs/2303.14416) #privacy</code></li>
<li>Summary: <p>The paper proposes a Blockchain (BC) system to prevent counterfeiting in
health insurance sector. The results show the system strength in terms of
achieving data integrity and privacy of data. Moreover, the results show that
the consensus algorithm can effectively reduce the total validation time for
the proposed system.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Adaptive Bi-Recommendation and Self-Improving Network for Heterogeneous Domain Adaptation-Assisted IoT Intrusion Detection. (arXiv:2303.14317v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14317">http://arxiv.org/abs/2303.14317</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14317] Adaptive Bi-Recommendation and Self-Improving Network for Heterogeneous Domain Adaptation-Assisted IoT Intrusion Detection](http://arxiv.org/abs/2303.14317) #protect</code></li>
<li>Summary: <p>As Internet of Things devices become prevalent, using intrusion detection to
protect IoT from malicious intrusions is of vital importance. However, the data
scarcity of IoT hinders the effectiveness of traditional intrusion detection
methods. To tackle this issue, in this paper, we propose the Adaptive
Bi-Recommendation and Self-Improving Network (ABRSI) based on unsupervised
heterogeneous domain adaptation (HDA). The ABRSI transfers enrich intrusion
knowledge from a data-rich network intrusion source domain to facilitate
effective intrusion detection for data-scarce IoT target domains. The ABRSI
achieves fine-grained intrusion knowledge transfer via adaptive
bi-recommendation matching. Matching the bi-recommendation interests of two
recommender systems and the alignment of intrusion categories in the shared
feature space form a mutual-benefit loop. Besides, the ABRSI uses a
self-improving mechanism, autonomously improving the intrusion knowledge
transfer from four ways. A hard pseudo label voting mechanism jointly considers
recommender system decision and label relationship information to promote more
accurate hard pseudo label assignment. To promote diversity and target data
participation during intrusion knowledge transfer, target instances failing to
be assigned with a hard pseudo label will be assigned with a probabilistic soft
pseudo label, forming a hybrid pseudo-labelling strategy. Meanwhile, the ABRSI
also makes soft pseudo-labels globally diverse and individually certain.
Finally, an error knowledge learning mechanism is utilised to adversarially
exploit factors that causes detection ambiguity and learns through both current
and previous error knowledge, preventing error knowledge forgetfulness.
Holistically, these mechanisms form the ABRSI model that boosts IoT intrusion
detection accuracy via HDA-assisted intrusion knowledge transfer.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Optimal Smoothing Distribution Exploration for Backdoor Neutralization in Deep Learning-based Traffic Systems. (arXiv:2303.14197v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14197">http://arxiv.org/abs/2303.14197</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14197] Optimal Smoothing Distribution Exploration for Backdoor Neutralization in Deep Learning-based Traffic Systems](http://arxiv.org/abs/2303.14197) #defense</code></li>
<li>Summary: <p>Deep Reinforcement Learning (DRL) enhances the efficiency of Autonomous
Vehicles (AV), but also makes them susceptible to backdoor attacks that can
result in traffic congestion or collisions. Backdoor functionality is typically
incorporated by contaminating training datasets with covert malicious data to
maintain high precision on genuine inputs while inducing the desired
(malicious) outputs for specific inputs chosen by adversaries. Current defenses
against backdoors mainly focus on image classification using image-based
features, which cannot be readily transferred to the regression task of
DRL-based AV controllers since the inputs are continuous sensor data, i.e., the
combinations of velocity and distance of AV and its surrounding vehicles. Our
proposed method adds well-designed noise to the input to neutralize backdoors.
The approach involves learning an optimal smoothing (noise) distribution to
preserve the normal functionality of genuine inputs while neutralizing
backdoors. By doing so, the resulting model is expected to be more resilient
against backdoor attacks while maintaining high accuracy on genuine inputs. The
effectiveness of the proposed method is verified on a simulated traffic system
based on a microscopic traffic simulator, where experimental results showcase
that the smoothed traffic controller can neutralize all trigger samples and
maintain the performance of relieving traffic congestion
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Ensemble-based Blackbox Attacks on Dense Prediction. (arXiv:2303.14304v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14304">http://arxiv.org/abs/2303.14304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14304] Ensemble-based Blackbox Attacks on Dense Prediction](http://arxiv.org/abs/2303.14304) #attack</code></li>
<li>Summary: <p>We propose an approach for adversarial attacks on dense prediction models
(such as object detectors and segmentation). It is well known that the attacks
generated by a single surrogate model do not transfer to arbitrary (blackbox)
victim models. Furthermore, targeted attacks are often more challenging than
the untargeted attacks. In this paper, we show that a carefully designed
ensemble can create effective attacks for a number of victim models. In
particular, we show that normalization of the weights for individual models
plays a critical role in the success of the attacks. We then demonstrate that
by adjusting the weights of the ensemble according to the victim model can
further improve the performance of the attacks. We performed a number of
experiments for object detectors and segmentation to highlight the significance
of the our proposed methods. Our proposed ensemble-based method outperforms
existing blackbox attack methods for object detection and segmentation. Finally
we show that our proposed method can also generate a single perturbation that
can fool multiple blackbox detection and segmentation models simultaneously.
Code is available at https://github.com/CSIPlab/EBAD.
</p></li>
</ul>

<h3>Title: Backdoor Attacks with Input-unique Triggers in NLP. (arXiv:2303.14325v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14325">http://arxiv.org/abs/2303.14325</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14325] Backdoor Attacks with Input-unique Triggers in NLP](http://arxiv.org/abs/2303.14325) #attack</code></li>
<li>Summary: <p>Backdoor attack aims at inducing neural models to make incorrect predictions
for poison data while keeping predictions on the clean dataset unchanged, which
creates a considerable threat to current natural language processing (NLP)
systems. Existing backdoor attacking systems face two severe issues:firstly,
most backdoor triggers follow a uniform and usually input-independent pattern,
e.g., insertion of specific trigger words, synonym replacement. This
significantly hinders the stealthiness of the attacking model, leading the
trained backdoor model being easily identified as malicious by model probes.
Secondly, trigger-inserted poisoned sentences are usually disfluent,
ungrammatical, or even change the semantic meaning from the original sentence,
making them being easily filtered in the pre-processing stage. To resolve these
two issues, in this paper, we propose an input-unique backdoor attack(NURA),
where we generate backdoor triggers unique to inputs. IDBA generates
context-related triggers by continuing writing the input with a language model
like GPT2. The generated sentence is used as the backdoor trigger. This
strategy not only creates input-unique backdoor triggers, but also preserves
the semantics of the original input, simultaneously resolving the two issues
above. Experimental results show that the IDBA attack is effective for attack
and difficult to defend: it achieves high attack success rate across all the
widely applied benchmarks, while is immune to existing defending methods. In
addition, it is able to generate fluent, grammatical, and diverse backdoor
inputs, which can hardly be recognized through human inspection.
</p></li>
</ul>

<h3>Title: Sem4SAP: Synonymous Expression Mining From Open Knowledge Graph For Language Model Synonym-Aware Pretraining. (arXiv:2303.14425v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14425">http://arxiv.org/abs/2303.14425</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14425] Sem4SAP: Synonymous Expression Mining From Open Knowledge Graph For Language Model Synonym-Aware Pretraining](http://arxiv.org/abs/2303.14425) #attack</code></li>
<li>Summary: <p>The model's ability to understand synonymous expression is crucial in many
kinds of downstream tasks. It will make the model to better understand the
similarity between context, and more robust to the synonym substitution attack.
However, many Pretrained Language Model (PLM) lack synonym knowledge due to
limitation of small-scale synsets and PLM's pretraining objectives. In this
paper, we propose a framework called Sem4SAP to mine synsets from Open
Knowledge Graph (Open-KG) and using the mined synsets to do synonym-aware
pretraining for language models. We propose to coarsly filter the content in
Open-KG and use the frequency information to better help the clustering process
under low-resource unsupervised conditions. We expand the mined synsets by
migrating core semantics between synonymous expressions.We also propose two
novel and effective synonym-aware pre-training methods for injecting synonym
knowledge into PLMs.Extensive experiments demonstrate that Sem4SAP can
dramatically outperform the original PLMs and other baselines on ten different
tasks.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels. (arXiv:2303.14307v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14307">http://arxiv.org/abs/2303.14307</a></li>
<li>Code URL: <a href="https://github.com/mpc001/Visual_Speech_Recognition_for_Multiple_Languages">https://github.com/mpc001/Visual_Speech_Recognition_for_Multiple_Languages</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14307] Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels](http://arxiv.org/abs/2303.14307) #robust</code></li>
<li>Summary: <p>Audio-visual speech recognition has received a lot of attention due to its
robustness against acoustic noise. Recently, the performance of automatic,
visual, and audio-visual speech recognition (ASR, VSR, and AV-ASR,
respectively) has been substantially improved, mainly due to the use of larger
models and training sets. However, accurate labelling of datasets is
time-consuming and expensive. Hence, in this work, we investigate the use of
automatically-generated transcriptions of unlabelled datasets to increase the
training set size. For this purpose, we use publicly-available pre-trained ASR
models to automatically transcribe unlabelled datasets such as AVSpeech and
VoxCeleb2. Then, we train ASR, VSR and AV-ASR models on the augmented training
set, which consists of the LRS2 and LRS3 datasets as well as the additional
automatically-transcribed data. We demonstrate that increasing the size of the
training set, a recent trend in the literature, leads to reduced WER despite
using noisy transcriptions. The proposed model achieves new state-of-the-art
performance on AV-ASR on LRS2 and LRS3. In particular, it achieves a WER of
0.9% on LRS3, a relative improvement of 30% over the current state-of-the-art
approach, and outperforms methods that have been trained on non-publicly
available datasets with 26 times more training data.
</p></li>
</ul>

<h3>Title: Train/Test-Time Adaptation with Retrieval. (arXiv:2303.14333v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14333">http://arxiv.org/abs/2303.14333</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14333] Train/Test-Time Adaptation with Retrieval](http://arxiv.org/abs/2303.14333) #robust</code></li>
<li>Summary: <p>We introduce Train/Test-Time Adaptation with Retrieval (${\rm T^3AR}$), a
method to adapt models both at train and test time by means of a retrieval
module and a searchable pool of external samples. Before inference, ${\rm
T^3AR}$ adapts a given model to the downstream task using refined pseudo-labels
and a self-supervised contrastive objective function whose noise distribution
leverages retrieved real samples to improve feature adaptation on the target
data manifold. The retrieval of real images is key to ${\rm T^3AR}$ since it
does not rely solely on synthetic data augmentations to compensate for the lack
of adaptation data, as typically done by other adaptation algorithms.
Furthermore, thanks to the retrieval module, our method gives the user or
service provider the possibility to improve model adaptation on the downstream
task by incorporating further relevant data or to fully remove samples that may
no longer be available due to changes in user preference after deployment.
First, we show that ${\rm T^3AR}$ can be used at training time to improve
downstream fine-grained classification over standard fine-tuning baselines, and
the fewer the adaptation data the higher the relative improvement (up to 13%).
Second, we apply ${\rm T^3AR}$ for test-time adaptation and show that
exploiting a pool of external images at test-time leads to more robust
representations over existing methods on DomainNet-126 and VISDA-C, especially
when few adaptation data are available (up to 8%).
</p></li>
</ul>

<h3>Title: Collaborative Multi-Object Tracking with Conformal Uncertainty Propagation. (arXiv:2303.14346v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14346">http://arxiv.org/abs/2303.14346</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14346] Collaborative Multi-Object Tracking with Conformal Uncertainty Propagation](http://arxiv.org/abs/2303.14346) #robust</code></li>
<li>Summary: <p>Object detection and multiple object tracking (MOT) are essential components
of self-driving systems. Accurate detection and uncertainty quantification are
both critical for onboard modules, such as perception, prediction, and
planning, to improve the safety and robustness of autonomous vehicles.
Collaborative object detection (COD) has been proposed to improve detection
accuracy and reduce uncertainty by leveraging the viewpoints of multiple
agents. However, little attention has been paid on how to leverage the
uncertainty quantification from COD to enhance MOT performance. In this paper,
as the first attempt, we design the uncertainty propagation framework to
address this challenge, called MOT-CUP. Our framework first quantifies the
uncertainty of COD through direct modeling and conformal prediction, and
propogates this uncertainty information during the motion prediction and
association steps. MOT-CUP is designed to work with different collaborative
object detectors and baseline MOT algorithms. We evaluate MOT-CUP on V2X-Sim, a
comprehensive collaborative perception dataset, and demonstrate a 2%
improvement in accuracy and a 2.67X reduction in uncertainty compared to the
baselines, e.g., SORT and ByteTrack. MOT-CUP demonstrates the importance of
uncertainty quantification in both COD and MOT, and provides the first attempt
to improve the accuracy and reduce the uncertainty in MOT based on COD through
uncertainty propogation.
</p></li>
</ul>

<h3>Title: A Self-supervised Framework for Improved Data-Driven Monitoring of Stress via Multi-modal Passive Sensing. (arXiv:2303.14267v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14267">http://arxiv.org/abs/2303.14267</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14267] A Self-supervised Framework for Improved Data-Driven Monitoring of Stress via Multi-modal Passive Sensing](http://arxiv.org/abs/2303.14267) #robust</code></li>
<li>Summary: <p>Recent advances in remote health monitoring systems have significantly
benefited patients and played a crucial role in improving their quality of
life. However, while physiological health-focused solutions have demonstrated
increasing success and maturity, mental health-focused applications have seen
comparatively limited success in spite of the fact that stress and anxiety
disorders are among the most common issues people deal with in their daily
lives. In the hopes of furthering progress in this domain through the
development of a more robust analytic framework for the measurement of
indicators of mental health, we propose a multi-modal semi-supervised framework
for tracking physiological precursors of the stress response. Our methodology
enables utilizing multi-modal data of differing domains and resolutions from
wearable devices and leveraging them to map short-term episodes to semantically
efficient embeddings for a given task. Additionally, we leverage an
inter-modality contrastive objective, with the advantages of rendering our
framework both modular and scalable. The focus on optimizing both local and
global aspects of our embeddings via a hierarchical structure renders
transferring knowledge and compatibility with other devices easier to achieve.
In our pipeline, a task-specific pooling based on an attention mechanism, which
estimates the contribution of each modality on an instance level, computes the
final embeddings for observations. This additionally provides a thorough
diagnostic insight into the data characteristics and highlights the importance
of signals in the broader view of predicting episodes annotated per mental
health status. We perform training experiments using a corpus of real-world
data on perceived stress, and our results demonstrate the efficacy of the
proposed approach in performance improvements.
</p></li>
</ul>

<h3>Title: Verifying Properties of Tsetlin Machines. (arXiv:2303.14464v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14464">http://arxiv.org/abs/2303.14464</a></li>
<li>Code URL: <a href="https://github.com/bimalb58/logical-tsetlin-machine-robustness">https://github.com/bimalb58/logical-tsetlin-machine-robustness</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14464] Verifying Properties of Tsetlin Machines](http://arxiv.org/abs/2303.14464) #robust</code></li>
<li>Summary: <p>Tsetlin Machines (TsMs) are a promising and interpretable machine learning
method which can be applied for various classification tasks. We present an
exact encoding of TsMs into propositional logic and formally verify properties
of TsMs using a SAT solver. In particular, we introduce in this work a notion
of similarity of machine learning models and apply our notion to check for
similarity of TsMs. We also consider notions of robustness and equivalence from
the literature and adapt them for TsMs. Then, we show the correctness of our
encoding and provide results for the properties: adversarial robustness,
equivalence, and similarity of TsMs. In our experiments, we employ the MNIST
and IMDB datasets for (respectively) image and sentiment classification. We
discuss the results for verifying robustness obtained with TsMs with those in
the literature obtained with Binarized Neural Networks on MNIST.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields. (arXiv:2303.14478v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14478">http://arxiv.org/abs/2303.14478</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14478] DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields](http://arxiv.org/abs/2303.14478) #extraction</code></li>
<li>Summary: <p>Recent works such as BARF and GARF can bundle adjust camera poses with neural
radiance fields (NeRF) which is based on coordinate-MLPs. Despite the
impressive results, these methods cannot be applied to Generalizable NeRFs
(GeNeRFs) which require image feature extractions that are often based on more
complicated 3D CNN or transformer architectures. In this work, we first analyze
the difficulties of jointly optimizing camera poses with GeNeRFs, and then
further propose our DBARF to tackle these issues. Our DBARF which bundle
adjusts camera poses by taking a cost feature map as an implicit cost function
can be jointly trained with GeNeRFs in a self-supervised manner. Unlike BARF
and its follow-up works, which can only be applied to per-scene optimized NeRFs
and need accurate initial camera poses with the exception of forward-facing
scenes, our method can generalize across scenes and does not require any good
initialization. Experiments show the effectiveness and generalization ability
of our DBARF when evaluated on real-world datasets. Our code is available at
\url{https://aibluefisher.github.io/dbarf}.
</p></li>
</ul>

<h3>Title: Knowledge-augmented Frame Semantic Parsing with Hybrid Prompt-tuning. (arXiv:2303.14375v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14375">http://arxiv.org/abs/2303.14375</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14375] Knowledge-augmented Frame Semantic Parsing with Hybrid Prompt-tuning](http://arxiv.org/abs/2303.14375) #extraction</code></li>
<li>Summary: <p>Frame semantics-based approaches have been widely used in semantic parsing
tasks and have become mainstream. It remains challenging to disambiguate frame
representations evoked by target lexical units under different contexts.
Pre-trained Language Models (PLMs) have been used in semantic parsing and
significantly improve the accuracy of neural parsers. However, the PLMs-based
approaches tend to favor collocated patterns presented in the training data,
leading to inaccurate outcomes. The intuition here is to design a mechanism to
optimally use knowledge captured in semantic frames in conjunction with PLMs to
disambiguate frames. We propose a novel Knowledge-Augmented Frame Semantic
Parsing Architecture (KAF-SPA) to enhance semantic representation by
incorporating accurate frame knowledge into PLMs during frame semantic parsing.
Specifically, a Memory-based Knowledge Extraction Module (MKEM) is devised to
select accurate frame knowledge and construct the continuous templates in the
high dimensional vector space. Moreover, we design a Task-oriented Knowledge
Probing Module (TKPM) using hybrid prompts (in terms of continuous and discrete
prompts) to incorporate the selected knowledge into the PLMs and adapt PLMs to
the tasks of frame and argument identification. Experimental results on two
public FrameNet datasets demonstrate that our method significantly outperforms
strong baselines (by more than +3$\%$ in F1), achieving state-of-art results on
the current benchmark. Ablation studies verify the effectiveness of KAF-SPA.
</p></li>
</ul>

<h3>Title: COFFEE: A Contrastive Oracle-Free Framework for Event Extraction. (arXiv:2303.14452v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14452">http://arxiv.org/abs/2303.14452</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14452] COFFEE: A Contrastive Oracle-Free Framework for Event Extraction](http://arxiv.org/abs/2303.14452) #extraction</code></li>
<li>Summary: <p>Event extraction is a complex information extraction task that involves
extracting events from unstructured text. Prior classification-based methods
require comprehensive entity annotations for joint training, while newer
generation-based methods rely on heuristic templates containing oracle
information such as event type, which is often unavailable in real-world
scenarios. In this study, we consider a more realistic setting of this task,
namely the Oracle-Free Event Extraction (OFEE) task, where only the input
context is given without any oracle information, including event type, event
ontology and trigger word. To solve this task, we propose a new framework,
called COFFEE, which extracts the events solely based on the document context
without referring to any oracle information. In particular, a contrastive
selection model is introduced in COFFEE to rectify the generated triggers and
handle multi-event instances. The proposed COFFEE outperforms state-of-the-art
approaches under the oracle-free setting of the event extraction task, as
evaluated on a public event extraction benchmark ACE05.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Learning without Full Labels: A Survey. (arXiv:2303.14453v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14453">http://arxiv.org/abs/2303.14453</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14453] Federated Learning without Full Labels: A Survey](http://arxiv.org/abs/2303.14453) #federate</code></li>
<li>Summary: <p>Data privacy has become an increasingly important concern in real-world big
data applications such as machine learning. To address the problem, federated
learning (FL) has been a promising solution to building effective machine
learning models from decentralized and private data. Existing federated
learning algorithms mainly tackle the supervised learning problem, where data
are assumed to be fully labeled. However, in practice, fully labeled data is
often hard to obtain, as the participants may not have sufficient domain
expertise, or they lack the motivation and tools to label data. Therefore, the
problem of federated learning without full labels is important in real-world FL
applications. In this paper, we discuss how the problem can be solved with
machine learning techniques that leverage unlabeled data. We present a survey
of methods that combine FL with semi-supervised learning, self-supervised
learning, and transfer learning methods. We also summarize the datasets used to
evaluate FL methods without full labels. Finally, we highlight future
directions in the context of FL without full labels.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: 3D Facial Imperfection Regeneration: Deep learning approach and 3D printing prototypes. (arXiv:2303.14381v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14381">http://arxiv.org/abs/2303.14381</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14381] 3D Facial Imperfection Regeneration: Deep learning approach and 3D printing prototypes](http://arxiv.org/abs/2303.14381) #fair</code></li>
<li>Summary: <p>This study explores the potential of a fully convolutional mesh autoencoder
model for regenerating 3D nature faces with the presence of imperfect areas. We
utilize deep learning approaches in graph processing and analysis to
investigate the capabilities model in recreating a filling part for facial
scars. Our approach in dataset creation is able to generate a facial scar
rationally in a virtual space that corresponds to the unique circumstances.
Especially, we propose a new method which is named 3D Facial Imperfection
Regeneration(3D-FaIR) for reproducing a complete face reconstruction based on
the remaining features of the patient face. To further enhance the applicable
capacity of the present research, we develop an improved outlier technique to
separate the wounds of patients and provide appropriate wound cover models.
Also, a Cir3D-FaIR dataset of imperfect faces and open codes was released at
https://github.com/SIMOGroup/3DFaIR. Our findings demonstrate the potential of
the proposed approach to help patients recover more quickly and safely through
convenient techniques. We hope that this research can contribute to the
development of new products and innovative solutions for facial scar
regeneration.
</p></li>
</ul>

<h3>Title: Fairness meets Cross-Domain Learning: a new perspective on Models and Metrics. (arXiv:2303.14411v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14411">http://arxiv.org/abs/2303.14411</a></li>
<li>Code URL: <a href="https://github.com/iurada/fairness_crossdomain">https://github.com/iurada/fairness_crossdomain</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14411] Fairness meets Cross-Domain Learning: a new perspective on Models and Metrics](http://arxiv.org/abs/2303.14411) #fair</code></li>
<li>Summary: <p>Deep learning-based recognition systems are deployed at scale for several
real-world applications that inevitably involve our social life. Although being
of great support when making complex decisions, they might capture spurious
data correlations and leverage sensitive attributes (e.g. age, gender,
ethnicity). How to factor out this information while keeping a high prediction
performance is a task with still several open questions, many of which are
shared with those of the domain adaptation and generalization literature which
focuses on avoiding visual domain biases. In this work, we propose an in-depth
study of the relationship between cross-domain learning (CD) and model fairness
by introducing a benchmark on face and medical images spanning several
demographic groups as well as classification and localization tasks. After
having highlighted the limits of the current evaluation metrics, we introduce a
new Harmonic Fairness (HF) score to assess jointly how fair and accurate every
model is with respect to a reference baseline. Our study covers 14 CD
approaches alongside three state-of-the-art fairness algorithms and shows how
the former can outperform the latter. Overall, our work paves the way for a
more systematic analysis of fairness problems in computer vision. Code
available at: https://github.com/iurada/fairness_crossdomain
</p></li>
</ul>

<h3>Title: CFA: Class-wise Calibrated Fair Adversarial Training. (arXiv:2303.14460v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14460">http://arxiv.org/abs/2303.14460</a></li>
<li>Code URL: <a href="https://github.com/pku-ml/cfa">https://github.com/pku-ml/cfa</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14460] CFA: Class-wise Calibrated Fair Adversarial Training](http://arxiv.org/abs/2303.14460) #fair</code></li>
<li>Summary: <p>Adversarial training has been widely acknowledged as the most effective
method to improve the adversarial robustness against adversarial examples for
Deep Neural Networks (DNNs). So far, most existing works focus on enhancing the
overall model robustness, treating each class equally in both the training and
testing phases. Although revealing the disparity in robustness among classes,
few works try to make adversarial training fair at the class level without
sacrificing overall robustness. In this paper, we are the first to
theoretically and empirically investigate the preference of different classes
for adversarial configurations, including perturbation margin, regularization,
and weight averaging. Motivated by this, we further propose a
\textbf{C}lass-wise calibrated \textbf{F}air \textbf{A}dversarial training
framework, named CFA, which customizes specific training configurations for
each class automatically. Experiments on benchmark datasets demonstrate that
our proposed CFA can improve both overall robustness and fairness notably over
other state-of-the-art methods. Code is available at
\url{https://github.com/PKU-ML/CFA}.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Curricular Contrastive Regularization for Physics-aware Single Image Dehazing. (arXiv:2303.14218v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14218">http://arxiv.org/abs/2303.14218</a></li>
<li>Code URL: <a href="https://github.com/yuzheng9/c2pnet">https://github.com/yuzheng9/c2pnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14218] Curricular Contrastive Regularization for Physics-aware Single Image Dehazing](http://arxiv.org/abs/2303.14218) #interpretability</code></li>
<li>Summary: <p>Considering the ill-posed nature, contrastive regularization has been
developed for single image dehazing, introducing the information from negative
images as a lower bound. However, the contrastive samples are nonconsensual, as
the negatives are usually represented distantly from the clear (i.e., positive)
image, leaving the solution space still under-constricted. Moreover, the
interpretability of deep dehazing models is underexplored towards the physics
of the hazing process. In this paper, we propose a novel curricular contrastive
regularization targeted at a consensual contrastive space as opposed to a
non-consensual one. Our negatives, which provide better lower-bound
constraints, can be assembled from 1) the hazy image, and 2) corresponding
restorations by other existing methods. Further, due to the different
similarities between the embeddings of the clear image and negatives, the
learning difficulty of the multiple components is intrinsically imbalanced. To
tackle this issue, we customize a curriculum learning strategy to reweight the
importance of different negatives. In addition, to improve the interpretability
in the feature space, we build a physics-aware dual-branch unit according to
the atmospheric scattering model. With the unit, as well as curricular
contrastive regularization, we establish our dehazing network, named C2PNet.
Extensive experiments demonstrate that our C2PNet significantly outperforms
state-of-the-art methods, with extreme PSNR boosts of 3.94dB and 1.50dB,
respectively, on SOTS-indoor and SOTS-outdoor datasets.
</p></li>
</ul>

<h3>Title: IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients. (arXiv:2303.14242v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14242">http://arxiv.org/abs/2303.14242</a></li>
<li>Code URL: <a href="https://github.com/yangruo1226/idgi">https://github.com/yangruo1226/idgi</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14242] IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients](http://arxiv.org/abs/2303.14242) #interpretability</code></li>
<li>Summary: <p>Integrated Gradients (IG) as well as its variants are well-known techniques
for interpreting the decisions of deep neural networks. While IG-based
approaches attain state-of-the-art performance, they often integrate noise into
their explanation saliency maps, which reduce their interpretability. To
minimize the noise, we examine the source of the noise analytically and propose
a new approach to reduce the explanation noise based on our analytical
findings. We propose the Important Direction Gradient Integration (IDGI)
framework, which can be easily incorporated into any IG-based method that uses
the Reimann Integration for integrated gradient computation. Extensive
experiments with three IG-based methods show that IDGI improves them
drastically on numerous interpretability metrics.
</p></li>
</ul>

<h3>Title: Spatially-Aware Car-Sharing Demand Prediction. (arXiv:2303.14421v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14421">http://arxiv.org/abs/2303.14421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14421] Spatially-Aware Car-Sharing Demand Prediction](http://arxiv.org/abs/2303.14421) #interpretability</code></li>
<li>Summary: <p>In recent years, car-sharing services have emerged as viable alternatives to
private individual mobility, promising more sustainable and resource-efficient,
but still comfortable transportation. Research on short-term prediction and
optimization methods has improved operations and fleet control of car-sharing
services; however, long-term projections and spatial analysis are sparse in the
literature. We propose to analyze the average monthly demand in a station-based
car-sharing service with spatially-aware learning algorithms that offer high
predictive performance as well as interpretability. In particular, we compare
the spatially-implicit Random Forest model with spatially-aware methods for
predicting average monthly per-station demand. The study utilizes a rich set of
socio-demographic, location-based (e.g., POIs), and car-sharing-specific
features as input, extracted from a large proprietary car-sharing dataset and
publicly available datasets. We show that the global Random Forest model with
geo-coordinates as an input feature achieves the highest predictive performance
with an R-squared score of 0.87, while local methods such as Geographically
Weighted Regression perform almost on par and additionally yield exciting
insights into the heterogeneous spatial distributions of factors influencing
car-sharing behaviour. Additionally, our study offers effective as well as
highly interpretable methods for diagnosing and planning the placement of
car-sharing stations.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: DiffuScene: Scene Graph Denoising Diffusion Probabilistic Model for Generative Indoor Scene Synthesis. (arXiv:2303.14207v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14207">http://arxiv.org/abs/2303.14207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14207] DiffuScene: Scene Graph Denoising Diffusion Probabilistic Model for Generative Indoor Scene Synthesis](http://arxiv.org/abs/2303.14207) #diffusion</code></li>
<li>Summary: <p>We present DiffuScene for indoor 3D scene synthesis based on a novel scene
graph denoising diffusion probabilistic model, which generates 3D instance
properties stored in a fully-connected scene graph and then retrieves the most
similar object geometry for each graph node i.e. object instance which is
characterized as a concatenation of different attributes, including location,
size, orientation, semantic, and geometry features. Based on this scene graph,
we designed a diffusion model to determine the placements and types of 3D
instances. Our method can facilitate many downstream applications, including
scene completion, scene arrangement, and text-conditioned scene synthesis.
Experiments on the 3D-FRONT dataset show that our method can synthesize more
physically plausible and diverse indoor scenes than state-of-the-art methods.
Extensive ablation studies verify the effectiveness of our design choice in
scene diffusion models.
</p></li>
</ul>

<h3>Title: Masked Diffusion Transformer is a Strong Image Synthesizer. (arXiv:2303.14389v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14389">http://arxiv.org/abs/2303.14389</a></li>
<li>Code URL: <a href="https://github.com/sail-sg/mdt">https://github.com/sail-sg/mdt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14389] Masked Diffusion Transformer is a Strong Image Synthesizer](http://arxiv.org/abs/2303.14389) #diffusion</code></li>
<li>Summary: <p>Despite its success in image synthesis, we observe that diffusion
probabilistic models (DPMs) often lack contextual reasoning ability to learn
the relations among object parts in an image, leading to a slow learning
process. To solve this issue, we propose a Masked Diffusion Transformer (MDT)
that introduces a mask latent modeling scheme to explicitly enhance the DPMs'
ability of contextual relation learning among object semantic parts in an
image. During training, MDT operates on the latent space to mask certain
tokens. Then, an asymmetric masking diffusion transformer is designed to
predict masked tokens from unmasked ones while maintaining the diffusion
generation process. Our MDT can reconstruct the full information of an image
from its incomplete contextual input, thus enabling it to learn the associated
relations among image tokens. Experimental results show that MDT achieves
superior image synthesis performance, e.g. a new SoTA FID score on the ImageNet
dataset, and has about 3x faster learning speed than the previous SoTA DiT. The
source code is released at https://github.com/sail-sg/MDT.
</p></li>
</ul>

<h3>Title: Freestyle Layout-to-Image Synthesis. (arXiv:2303.14412v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14412">http://arxiv.org/abs/2303.14412</a></li>
<li>Code URL: <a href="https://github.com/essunny310/freestylenet">https://github.com/essunny310/freestylenet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14412] Freestyle Layout-to-Image Synthesis](http://arxiv.org/abs/2303.14412) #diffusion</code></li>
<li>Summary: <p>Typical layout-to-image synthesis (LIS) models generate images for a closed
set of semantic classes, e.g., 182 common objects in COCO-Stuff. In this work,
we explore the freestyle capability of the model, i.e., how far can it generate
unseen semantics (e.g., classes, attributes, and styles) onto a given layout,
and call the task Freestyle LIS (FLIS). Thanks to the development of
large-scale pre-trained language-image models, a number of discriminative
models (e.g., image classification and object detection) trained on limited
base classes are empowered with the ability of unseen class prediction.
Inspired by this, we opt to leverage large-scale pre-trained text-to-image
diffusion models to achieve the generation of unseen semantics. The key
challenge of FLIS is how to enable the diffusion model to synthesize images
from a specific layout which very likely violates its pre-learned knowledge,
e.g., the model never sees "a unicorn sitting on a bench" during its
pre-training. To this end, we introduce a new module called Rectified
Cross-Attention (RCA) that can be conveniently plugged in the diffusion model
to integrate semantic masks. This "plug-in" is applied in each cross-attention
layer of the model to rectify the attention maps between image and text tokens.
The key idea of RCA is to enforce each text token to act on the pixels in a
specified region, allowing us to freely put a wide variety of semantics from
pre-trained knowledge (which is general) onto the given layout (which is
specific). Extensive experiments show that the proposed diffusion network
produces realistic and freestyle layout-to-image generation results with
diverse text inputs, which has a high potential to spawn a bunch of interesting
applications. Code is available at https://github.com/essunny310/FreestyleNet.
</p></li>
</ul>

<h3>Title: Better Aligning Text-to-Image Models with Human Preference. (arXiv:2303.14420v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.14420">http://arxiv.org/abs/2303.14420</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.14420] Better Aligning Text-to-Image Models with Human Preference](http://arxiv.org/abs/2303.14420) #diffusion</code></li>
<li>Summary: <p>Recent years have witnessed a rapid growth of deep generative models, with
text-to-image models gaining significant attention from the public. However,
existing models often generate images that do not align well with human
aesthetic preferences, such as awkward combinations of limbs and facial
expressions. To address this issue, we collect a dataset of human choices on
generated images from the Stable Foundation Discord channel. Our experiments
demonstrate that current evaluation metrics for generative models do not
correlate well with human choices. Thus, we train a human preference classifier
with the collected dataset and derive a Human Preference Score (HPS) based on
the classifier. Using the HPS, we propose a simple yet effective method to
adapt Stable Diffusion to better align with human aesthetic preferences. Our
experiments show that the HPS outperforms CLIP in predicting human choices and
has good generalization capability towards images generated from other models.
By tuning Stable Diffusion with the guidance of the HPS, the adapted model is
able to generate images that are more preferred by human users.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
