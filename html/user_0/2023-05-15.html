<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Unconditionally Secure Access Control Encryption. (arXiv:2305.07593v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07593">http://arxiv.org/abs/2305.07593</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07593] Unconditionally Secure Access Control Encryption](http://arxiv.org/abs/2305.07593) #secure</code></li>
<li>Summary: <p>Access control encryption (ACE) enforces, through a sanitizer as the
mediator, that only legitimate sender-receiver pairs can communicate, without
the sanitizer knowing the communication metadata, including its sender and
recipient identity, the policy over them, and the underlying plaintext. Any
illegitimate transmission is indistinguishable from pure noise. Existing works
focused on computational security and require trapdoor functions and possibly
other heavyweight primitives. We present the first ACE scheme with
information-theoretic security (unconditionally against unbounded adversaries).
Our novel randomization techniques over matrices realize sanitization
(traditionally via homomorphism over a fixed randomness space) such that the
secret message in the hidden message subspace remains intact if and only if
there is no illegitimate transmission.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition. (arXiv:2305.07214v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07214">http://arxiv.org/abs/2305.07214</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07214] MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition](http://arxiv.org/abs/2305.07214) #security</code></li>
<li>Summary: <p>In this paper, we study a novel problem in egocentric action recognition,
which we term as "Multimodal Generalization" (MMG). MMG aims to study how
systems can generalize when data from certain modalities is limited or even
completely missing. We thoroughly investigate MMG in the context of standard
supervised action recognition and the more challenging few-shot setting for
learning new action categories. MMG consists of two novel scenarios, designed
to support security, and efficiency considerations in real-world applications:
(1) missing modality generalization where some modalities that were present
during the train time are missing during the inference time, and (2)
cross-modal zero-shot generalization, where the modalities present during the
inference time and the training time are disjoint. To enable this
investigation, we construct a new dataset MMG-Ego4D containing data points with
video, audio, and inertial motion sensor (IMU) modalities. Our dataset is
derived from Ego4D dataset, but processed and thoroughly re-annotated by human
experts to facilitate research in the MMG problem. We evaluate a diverse array
of models on MMG-Ego4D and propose new methods with improved generalization
ability. In particular, we introduce a new fusion module with modality dropout
training, contrastive-based alignment training, and a novel cross-modal
prototypical loss for better few-shot performance. We hope this study will
serve as a benchmark and guide future research in multimodal generalization
problems. The benchmark and code will be available at
https://github.com/facebookresearch/MMG_Ego4D.
</p></li>
</ul>

<h3>Title: Configurable Spatial-Temporal Hierarchical Analysis for Flexible Video Anomaly Detection. (arXiv:2305.07328v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07328">http://arxiv.org/abs/2305.07328</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07328] Configurable Spatial-Temporal Hierarchical Analysis for Flexible Video Anomaly Detection](http://arxiv.org/abs/2305.07328) #security</code></li>
<li>Summary: <p>Video anomaly detection (VAD) is a vital task with great practical
applications in industrial surveillance, security system, and traffic control.
Unlike previous unsupervised VAD methods that adopt a fixed structure to learn
normality without considering different detection demands, we design a
spatial-temporal hierarchical architecture (STHA) as a configurable
architecture to flexibly detect different degrees of anomaly. The comprehensive
structure of the STHA is delineated into a tripartite hierarchy, encompassing
the following tiers: the stream level, the stack level, and the block level.
Specifically, we design several auto-encoder-based blocks that possess varying
capacities for extracting normal patterns. Then, we stack blocks according to
the complexity degrees with both intra-stack and inter-stack residual links to
learn hierarchical normality gradually. Considering the multisource knowledge
of videos, we also model the spatial normality of video frames and temporal
normality of RGB difference by designing two parallel streams consisting of
stacks. Thus, STHA can provide various representation learning abilities by
expanding or contracting hierarchically to detect anomalies of different
degrees. Since the anomaly set is complicated and unbounded, our STHA can
adjust its detection ability to adapt to the human detection demands and the
complexity degree of anomaly that happened in the history of a scene. We
conduct experiments on three benchmarks and perform extensive analysis, and the
results demonstrate that our method performs comparablely to the
state-of-the-art methods. In addition, we design a toy dataset to prove that
our model can better balance the learning ability to adapt to different
detection demands.
</p></li>
</ul>

<h3>Title: Gotcha! I Know What You are Doing on the FPGA Cloud: Fingerprinting Co-Located Cloud FPGA Accelerators via Measuring Communication Links. (arXiv:2305.07209v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07209">http://arxiv.org/abs/2305.07209</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07209] Gotcha! I Know What You are Doing on the FPGA Cloud: Fingerprinting Co-Located Cloud FPGA Accelerators via Measuring Communication Links](http://arxiv.org/abs/2305.07209) #security</code></li>
<li>Summary: <p>In recent decades, due to the emerging requirements of computation
acceleration, cloud FPGAs have become popular in public clouds. Major cloud
service providers, e.g. AWS and Microsoft Azure have provided FPGA computing
resources in their infrastructure and have enabled users to design and deploy
their own accelerators on these FPGAs. Multi-tenancy FPGAs, where multiple
users can share the same FPGA fabric with certain types of isolation to improve
resource efficiency, have already been proved feasible. However, this also
raises security concerns. Various types of side-channel attacks targeting
multi-tenancy FPGAs have been proposed and validated. The awareness of security
vulnerabilities in the cloud has motivated cloud providers to take action to
enhance the security of their cloud environments.
</p></li>
</ul>

<p>In FPGA security research papers, researchers always perform attacks under
the assumption that attackers successfully co-locate with victims and are aware
of the existence of victims on the same FPGA board. However, the way to reach
this point, i.e., how attackers secretly obtain information regarding
accelerators on the same fabric, is constantly ignored despite the fact that it
is non-trivial and important for attackers. In this paper, we present a novel
fingerprinting attack to gain the types of co-located FPGA accelerators. We
utilize a seemingly non-malicious benchmark accelerator to sniff the
communication link and collect performance traces of the FPGA-host
communication link. By analyzing these traces, we are able to achieve high
classification accuracy for fingerprinting co-located accelerators, which
proves that attackers can use our method to perform cloud FPGA accelerator
fingerprinting with a high success rate. As far as we know, this is the first
paper targeting multi-tenant FPGA accelerator fingerprinting with the
communication side-channel.
</p>

<h2>privacy</h2>
<h3>Title: Differentially Private Set-Based Estimation Using Zonotopes. (arXiv:2305.07407v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07407">http://arxiv.org/abs/2305.07407</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07407] Differentially Private Set-Based Estimation Using Zonotopes](http://arxiv.org/abs/2305.07407) #privacy</code></li>
<li>Summary: <p>For large-scale cyber-physical systems, the collaboration of spatially
distributed sensors is often needed to perform the state estimation process.
Privacy concerns naturally arise from disclosing sensitive measurement signals
to a cloud estimator that predicts the system state. To solve this issue, we
propose a differentially private set-based estimation protocol that preserves
the privacy of the measurement signals. Compared to existing research, our
approach achieves less privacy loss and utility loss using a numerically
optimized truncated noise distribution. The proposed estimator is perturbed by
weaker noise than the analytical approaches in the literature to guarantee the
same level of privacy, therefore improving the estimation utility. Numerical
and comparison experiments with truncated Laplace noise are presented to
support our approach. Zonotopes, a less conservative form of set
representation, are used to represent estimation sets, giving set operations a
computational advantage. The privacy-preserving noise anonymizes the centers of
these estimated zonotopes, concealing the precise positions of the estimated
zonotopes.
</p></li>
</ul>

<h3>Title: Comparison of machine learning models applied on anonymized data with different techniques. (arXiv:2305.07415v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07415">http://arxiv.org/abs/2305.07415</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07415] Comparison of machine learning models applied on anonymized data with different techniques](http://arxiv.org/abs/2305.07415) #privacy</code></li>
<li>Summary: <p>Anonymization techniques based on obfuscating the quasi-identifiers by means
of value generalization hierarchies are widely used to achieve preset levels of
privacy. To prevent different types of attacks against database privacy it is
necessary to apply several anonymization techniques beyond the classical
k-anonymity or $\ell$-diversity. However, the application of these methods is
directly connected to a reduction of their utility in prediction and decision
making tasks. In this work we study four classical machine learning methods
currently used for classification purposes in order to analyze the results as a
function of the anonymization techniques applied and the parameters selected
for each of them. The performance of these models is studied when varying the
value of k for k-anonymity and additional tools such as $\ell$-diversity,
t-closeness and $\delta$-disclosure privacy are also deployed on the well-known
adult dataset.
</p></li>
</ul>

<h3>Title: Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques. (arXiv:2305.07116v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07116">http://arxiv.org/abs/2305.07116</a></li>
<li>Code URL: <a href="https://github.com/pepijndereus/privacy-enhancing-ml">https://github.com/pepijndereus/privacy-enhancing-ml</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07116] Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques](http://arxiv.org/abs/2305.07116) #privacy</code></li>
<li>Summary: <p>To address increasing societal concerns regarding privacy and climate, the EU
adopted the General Data Protection Regulation (GDPR) and committed to the
Green Deal. Considerable research studied the energy efficiency of software and
the accuracy of machine learning models trained on anonymised data sets. Recent
work began exploring the impact of privacy-enhancing techniques (PET) on both
the energy consumption and accuracy of the machine learning models, focusing on
k-anonymity. As synthetic data is becoming an increasingly popular PET, this
paper analyses the energy consumption and accuracy of two phases: a) applying
privacy-enhancing techniques to the concerned data set, b) training the models
on the concerned privacy-enhanced data set. We use two privacy-enhancing
techniques: k-anonymisation (using generalisation and suppression) and
synthetic data, and three machine-learning models. Each model is trained on
each privacy-enhanced data set. Our results show that models trained on
k-anonymised data consume less energy than models trained on the original data,
with a similar performance regarding accuracy. Models trained on synthetic data
have a similar energy consumption and a similar to lower accuracy compared to
models trained on the original data.
</p></li>
</ul>

<h3>Title: Learn to Unlearn: A Survey on Machine Unlearning. (arXiv:2305.07512v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07512">http://arxiv.org/abs/2305.07512</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07512] Learn to Unlearn: A Survey on Machine Unlearning](http://arxiv.org/abs/2305.07512) #privacy</code></li>
<li>Summary: <p>Machine Learning (ML) models contain private information, and implementing
the right to be forgotten is a challenging privacy issue in many data
applications. Machine unlearning has emerged as an alternative to remove
sensitive data from a trained model, but completely retraining ML models is
often not feasible. This survey provides a concise appraisal of Machine
Unlearning techniques, encompassing both exact and approximate methods,
probable attacks, and verification approaches. The survey compares the merits
and limitations each method and evaluates their performance using the Deltagrad
exact machine unlearning method. The survey also highlights challenges like the
pressing need for a robust model for non-IID deletion to mitigate fairness
issues. Overall, the survey provides a thorough synopsis of machine unlearning
techniques and applications, noting future research directions in this evolving
field. The survey aims to be a valuable resource for researchers and
practitioners seeking to provide privacy and equity in ML systems.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: ViT Unified: Joint Fingerprint Recognition and Presentation Attack Detection. (arXiv:2305.07602v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07602">http://arxiv.org/abs/2305.07602</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07602] ViT Unified: Joint Fingerprint Recognition and Presentation Attack Detection](http://arxiv.org/abs/2305.07602) #attack</code></li>
<li>Summary: <p>A secure fingerprint recognition system must contain both a presentation
attack (i.e., spoof) detection and recognition module in order to protect users
against unwanted access by malicious users. Traditionally, these tasks would be
carried out by two independent systems; however, recent studies have
demonstrated the potential to have one unified system architecture in order to
reduce the computational burdens on the system, while maintaining high
accuracy. In this work, we leverage a vision transformer architecture for joint
spoof detection and matching and report competitive results with
state-of-the-art (SOTA) models for both a sequential system (two ViT models
operating independently) and a unified architecture (a single ViT model for
both tasks). ViT models are particularly well suited for this task as the ViT's
global embedding encodes features useful for recognition, whereas the
individual, local embeddings are useful for spoof detection. We demonstrate the
capability of our unified model to achieve an average integrated matching (IM)
accuracy of 98.87% across LivDet 2013 and 2015 CrossMatch sensors. This is
comparable to IM accuracy of 98.95% of our sequential dual-ViT system, but with
~50% of the parameters and ~58% of the latency.
</p></li>
</ul>

<h3>Title: Enhancing Contrastive Learning with Noise-Guided Attack: Towards Continual Relation Extraction in the Wild. (arXiv:2305.07085v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07085">http://arxiv.org/abs/2305.07085</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07085] Enhancing Contrastive Learning with Noise-Guided Attack: Towards Continual Relation Extraction in the Wild](http://arxiv.org/abs/2305.07085) #attack</code></li>
<li>Summary: <p>The principle of continual relation extraction~(CRE) involves adapting to
emerging novel relations while preserving od knowledge. While current endeavors
in CRE succeed in preserving old knowledge, they tend to fail when exposed to
contaminated data streams. We assume this is attributed to their reliance on an
artificial hypothesis that the data stream has no annotation errors, which
hinders real-world applications for CRE. Considering the ubiquity of noisy
labels in real-world datasets, in this paper, we formalize a more practical
learning scenario, termed as \textit{noisy-CRE}. Building upon this challenging
setting, we develop a noise-resistant contrastive framework named as
\textbf{N}oise-guided \textbf{a}ttack in \textbf{C}ontrative
\textbf{L}earning~(NaCL) to learn incremental corrupted relations. Compared to
direct noise discarding or inaccessible noise relabeling, we present modifying
the feature space to match the given noisy labels via attacking can better
enrich contrastive representations. Extensive empirical validations highlight
that NaCL can achieve consistent performance improvements with increasing noise
rates, outperforming state-of-the-art baselines.
</p></li>
</ul>

<h3>Title: Two-in-One: A Model Hijacking Attack Against Text Generation Models. (arXiv:2305.07406v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07406">http://arxiv.org/abs/2305.07406</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07406] Two-in-One: A Model Hijacking Attack Against Text Generation Models](http://arxiv.org/abs/2305.07406) #attack</code></li>
<li>Summary: <p>Machine learning has progressed significantly in various applications ranging
from face recognition to text generation. However, its success has been
accompanied by different attacks. Recently a new attack has been proposed which
raises both accountability and parasitic computing risks, namely the model
hijacking attack. Nevertheless, this attack has only focused on image
classification tasks. In this work, we broaden the scope of this attack to
include text generation and classification models, hence showing its broader
applicability. More concretely, we propose a new model hijacking attack, Ditto,
that can hijack different text classification tasks into multiple generation
ones, e.g., language translation, text summarization, and language modeling. We
use a range of text benchmark datasets such as SST-2, TweetEval, AGnews, QNLI,
and IMDB to evaluate the performance of our attacks. Our results show that by
using Ditto, an adversary can successfully hijack text generation models
without jeopardizing their utility.
</p></li>
</ul>

<h3>Title: SigRec: Automatic Recovery of Function Signatures in Smart Contracts. (arXiv:2305.07067v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07067">http://arxiv.org/abs/2305.07067</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07067] SigRec: Automatic Recovery of Function Signatures in Smart Contracts](http://arxiv.org/abs/2305.07067) #attack</code></li>
<li>Summary: <p>Millions of smart contracts have been deployed onto Ethereum for providing
various services, whose functions can be invoked. For this purpose, the caller
needs to know the function signature of a callee, which includes its function
id and parameter types. Such signatures are critical to many applications
focusing on smart contracts, e.g., reverse engineering, fuzzing, attack
detection, and profiling. Unfortunately, it is challenging to recover the
function signatures from contract bytecode, since neither debug information nor
type information is present in the bytecode. To address this issue, prior
approaches rely on source code, or a collection of known signatures from
incomplete databases or incomplete heuristic rules, which, however, are far
from adequate and cannot cope with the rapid growth of new contracts. In this
paper, we propose a novel solution that leverages how functions are handled by
Ethereum virtual machine (EVM) to automatically recover function signatures. In
particular, we exploit how smart contracts determine the functions to be
invoked to locate and extract function ids, and propose a new approach named
type-aware symbolic execution (TASE) that utilizes the semantics of EVM
operations on parameters to identify the number and the types of parameters.
Moreover, we develop SigRec, a new tool for recovering function signatures from
contract bytecode without the need of source code and function signature
databases. The extensive experimental results show that SigRec outperforms all
existing tools, achieving an unprecedented 98.7 percent accuracy within 0.074
seconds. We further demonstrate that the recovered function signatures are
useful in attack detection, fuzzing and reverse engineering of EVM bytecode.
</p></li>
</ul>

<h3>Title: A Lightweight Authentication Protocol against Modeling Attacks based on a Novel LFSR-APUF. (arXiv:2305.07254v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07254">http://arxiv.org/abs/2305.07254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07254] A Lightweight Authentication Protocol against Modeling Attacks based on a Novel LFSR-APUF](http://arxiv.org/abs/2305.07254) #attack</code></li>
<li>Summary: <p>Simple authentication protocols based on conventional physical unclonable
function (PUF) are vulnerable to modeling attacks and other security threats.
This paper proposes an arbiter PUF based on a linear feedback shift register
(LFSR-APUF). Different from the previously reported linear feedback shift
register for challenge extension, the proposed scheme feeds the external random
challenges into the LFSR module to obfuscate the linear mapping relationship
between the challenge and response. It can prevent attackers from obtaining
valid challenge-response pairs (CRPs), increasing its resistance to modeling
attacks significantly. A 64-stage LFSR-APUF has been implemented on a field
programmable gate array (FPGA) board. The experimental results reveal that the
proposed design can effectively resist various modeling attacks such as
logistic regression (LR), evolutionary strategy (ES), Artificial Neuro Network
(ANN), and support vector machine (SVM) with a prediction rate of 51.79% and a
slight effect on the randomness, reliability, and uniqueness. Further, a
lightweight authentication protocol is established based on the proposed
LFSR-APUF. The protocol incorporates a low-overhead, ultra-lightweight, novel
private bit conversion Cover function that is uniquely bound to each device in
the authentication network. A dynamic and timevariant obfuscation scheme in
combination with the proposed LFSR-APUF is implemented in the protocol. The
proposed authentication protocol not only resists spoofing attacks, physical
attacks, and modeling attacks effectively, but also ensures the security of the
entire authentication network by transferring important information in
encrypted form from the server to the database even when the attacker
completely controls the server.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Surgical tool classification and localization: results and methods from the MICCAI 2022 SurgToolLoc challenge. (arXiv:2305.07152v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07152">http://arxiv.org/abs/2305.07152</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07152] Surgical tool classification and localization: results and methods from the MICCAI 2022 SurgToolLoc challenge](http://arxiv.org/abs/2305.07152) #robust</code></li>
<li>Summary: <p>The ability to automatically detect and track surgical instruments in
endoscopic videos can enable transformational interventions. Assessing surgical
performance and efficiency, identifying skilled tool use and choreography, and
planning operational and logistical aspects of OR resources are just a few of
the applications that could benefit. Unfortunately, obtaining the annotations
needed to train machine learning models to identify and localize surgical tools
is a difficult task. Annotating bounding boxes frame-by-frame is tedious and
time-consuming, yet large amounts of data with a wide variety of surgical tools
and surgeries must be captured for robust training. Moreover, ongoing annotator
training is needed to stay up to date with surgical instrument innovation. In
robotic-assisted surgery, however, potentially informative data like timestamps
of instrument installation and removal can be programmatically harvested. The
ability to rely on tool installation data alone would significantly reduce the
workload to train robust tool-tracking models. With this motivation in mind we
invited the surgical data science community to participate in the challenge,
SurgToolLoc 2022. The goal was to leverage tool presence data as weak labels
for machine learning models trained to detect tools and localize them in video
frames with bounding boxes. We present the results of this challenge along with
many of the team's efforts. We conclude by discussing these results in the
broader context of machine learning and surgical data science. The training
data used for this challenge consisting of 24,695 video clips with tool
presence labels is also being released publicly and can be accessed at
https://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022.
</p></li>
</ul>

<h3>Title: Meta-Optimization for Higher Model Generalizability in Single-Image Depth Prediction. (arXiv:2305.07269v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07269">http://arxiv.org/abs/2305.07269</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07269] Meta-Optimization for Higher Model Generalizability in Single-Image Depth Prediction](http://arxiv.org/abs/2305.07269) #robust</code></li>
<li>Summary: <p>Model generalizability to unseen datasets, concerned with in-the-wild
robustness, is less studied for indoor single-image depth prediction. We
leverage gradient-based meta-learning for higher generalizability on zero-shot
cross-dataset inference. Unlike the most-studied image classification in
meta-learning, depth is pixel-level continuous range values, and mappings from
each image to depth vary widely across environments. Thus no explicit task
boundaries exist. We instead propose fine-grained task that treats each RGB-D
pair as a task in our meta-optimization. We first show meta-learning on limited
data induces much better prior (max +29.4\%). Using meta-learned weights as
initialization for following supervised learning, without involving extra data
or information, it consistently outperforms baselines without the method.
Compared to most indoor-depth methods that only train/ test on a single
dataset, we propose zero-shot cross-dataset protocols, closely evaluate
robustness, and show consistently higher generalizability and accuracy by our
meta-initialization. The work at the intersection of depth and meta-learning
potentially drives both research streams to step closer to practical use.
</p></li>
</ul>

<h3>Title: Efficient Search of Comprehensively Robust Neural Architectures via Multi-fidelity Evaluation. (arXiv:2305.07308v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07308">http://arxiv.org/abs/2305.07308</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07308] Efficient Search of Comprehensively Robust Neural Architectures via Multi-fidelity Evaluation](http://arxiv.org/abs/2305.07308) #robust</code></li>
<li>Summary: <p>Neural architecture search (NAS) has emerged as one successful technique to
find robust deep neural network (DNN) architectures. However, most existing
robustness evaluations in NAS only consider $l_{\infty}$ norm-based adversarial
noises. In order to improve the robustness of DNN models against multiple types
of noises, it is necessary to consider a comprehensive evaluation in NAS for
robust architectures. But with the increasing number of types of robustness
evaluations, it also becomes more time-consuming to find comprehensively robust
architectures. To alleviate this problem, we propose a novel efficient search
of comprehensively robust neural architectures via multi-fidelity evaluation
(ES-CRNA-ME). Specifically, we first search for comprehensively robust
architectures under multiple types of evaluations using the
weight-sharing-based NAS method, including different $l_{p}$ norm attacks,
semantic adversarial attacks, and composite adversarial attacks. In addition,
we reduce the number of robustness evaluations by the correlation analysis,
which can incorporate similar evaluations and decrease the evaluation cost.
Finally, we propose a multi-fidelity online surrogate during optimization to
further decrease the search cost. On the basis of the surrogate constructed by
low-fidelity data, the online high-fidelity data is utilized to finetune the
surrogate. Experiments on CIFAR10 and CIFAR100 datasets show the effectiveness
of our proposed method.
</p></li>
</ul>

<h3>Title: Gallery Sampling for Robust and Fast Face Identification. (arXiv:2305.07495v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07495">http://arxiv.org/abs/2305.07495</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07495] Gallery Sampling for Robust and Fast Face Identification](http://arxiv.org/abs/2305.07495) #robust</code></li>
<li>Summary: <p>Deep learning methods have been achieved brilliant results in face
recognition. One of the important tasks to improve the performance is to
collect and label images as many as possible. However, labeling identities and
checking qualities of large image data are difficult task and mistakes cannot
be avoided in processing large data. Previous works have been trying to deal
with the problem only in training domain, however it can cause much serious
problem if the mistakes are in gallery data of face identification. We proposed
gallery data sampling methods which are robust to outliers including wrong
labeled, low quality, and less-informative images and reduce searching time.
The proposed sampling-by-pruning and sampling-by-generating methods
significantly improved face identification performance on our 5.4M web image
dataset of celebrities. The proposed method achieved 0.0975 in terms of FNIR at
FPIR=0.01, while conventional method showed 0.3891. The average number of
feature vectors for each individual gallery was reduced to 17.1 from 115.9 and
it can provide much faster search. We also made experiments on public datasets
and our method achieved 0.1314 and 0.0668 FNIRs at FPIR=0.01 on the
CASIA-WebFace and MS1MV2, while the convectional method did 0.5446, and 0.1327,
respectively.
</p></li>
</ul>

<h3>Title: Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training. (arXiv:2305.07613v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07613">http://arxiv.org/abs/2305.07613</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07613] Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training](http://arxiv.org/abs/2305.07613) #robust</code></li>
<li>Summary: <p>Training Generative adversarial networks (GANs) stably is a challenging task.
The generator in GANs transform noise vectors, typically Gaussian distributed,
into realistic data such as images. In this paper, we propose a novel approach
for training GANs with images as inputs, but without enforcing any pairwise
constraints. The intuition is that images are more structured than noise, which
the generator can leverage to learn a more robust transformation. The process
can be made efficient by identifying closely related datasets, or a ``friendly
neighborhood'' of the target distribution, inspiring the moniker, Spider GAN.
To define friendly neighborhoods leveraging proximity between datasets, we
propose a new measure called the signed inception distance (SID), inspired by
the polyharmonic kernel. We show that the Spider GAN formulation results in
faster convergence, as the generator can discover correspondence even between
seemingly unrelated datasets, for instance, between Tiny-ImageNet and CelebA
faces. Further, we demonstrate cascading Spider GAN, where the output
distribution from a pre-trained GAN generator is used as the input to the
subsequent network. Effectively, transporting one distribution to another in a
cascaded fashion until the target is learnt -- a new flavor of transfer
learning. We demonstrate the efficacy of the Spider approach on DCGAN,
conditional GAN, PGGAN, StyleGAN2 and StyleGAN3. The proposed approach achieves
state-of-the-art Frechet inception distance (FID) values, with one-fifth of the
training iterations, in comparison to their baseline counterparts on
high-resolution small datasets such as MetFaces, Ukiyo-E Faces and AFHQ-Cats.
</p></li>
</ul>

<h3>Title: Model-based Programming: Redefining the Atomic Unit of Programming for the Deep Learning Era. (arXiv:2305.07341v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07341">http://arxiv.org/abs/2305.07341</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07341] Model-based Programming: Redefining the Atomic Unit of Programming for the Deep Learning Era](http://arxiv.org/abs/2305.07341) #robust</code></li>
<li>Summary: <p>This paper introduces and explores a new programming paradigm, Model-based
Programming, designed to address the challenges inherent in applying deep
learning models to real-world applications. Despite recent significant
successes of deep learning models across a range of tasks, their deployment in
real business scenarios remains fraught with difficulties, such as complex
model training, large computational resource requirements, and integration
issues with existing programming languages. To ameliorate these challenges, we
propose the concept of 'Model-based Programming' and present a novel
programming language - M Language, tailored to a prospective model-centered
programming paradigm. M Language treats models as basic computational units,
enabling developers to concentrate more on crucial tasks such as model loading,
fine-tuning, evaluation, and deployment, thereby enhancing the efficiency of
creating deep learning applications. We posit that this innovative programming
paradigm will stimulate the extensive application and advancement of deep
learning technology and provide a robust foundation for a model-driven future.
</p></li>
</ul>

<h3>Title: Investigating the Sensitivity of Automatic Speech Recognition Systems to Phonetic Variation in L2 Englishes. (arXiv:2305.07389v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07389">http://arxiv.org/abs/2305.07389</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07389] Investigating the Sensitivity of Automatic Speech Recognition Systems to Phonetic Variation in L2 Englishes](http://arxiv.org/abs/2305.07389) #robust</code></li>
<li>Summary: <p>Automatic Speech Recognition (ASR) systems exhibit the best performance on
speech that is similar to that on which it was trained. As such,
underrepresented varieties including regional dialects, minority-speakers, and
low-resource languages, see much higher word error rates (WERs) than those
varieties seen as 'prestigious', 'mainstream', or 'standard'. This can act as a
barrier to incorporating ASR technology into the annotation process for
large-scale linguistic research since the manual correction of the erroneous
automated transcripts can be just as time and resource consuming as manual
transcriptions. A deeper understanding of the behaviour of an ASR system is
thus beneficial from a speech technology standpoint, in terms of improving ASR
accuracy, and from an annotation standpoint, where knowing the likely errors
made by an ASR system can aid in this manual correction. This work demonstrates
a method of probing an ASR system to discover how it handles phonetic variation
across a number of L2 Englishes. Specifically, how particular phonetic
realisations which were rare or absent in the system's training data can lead
to phoneme level misrecognitions and contribute to higher WERs. It is
demonstrated that the behaviour of the ASR is systematic and consistent across
speakers with similar spoken varieties (in this case the same L1) and phoneme
substitution errors are typically in agreement with human annotators. By
identifying problematic productions specific weaknesses can be addressed by
sourcing such realisations for training and fine-tuning thus making the system
more robust to pronunciation variation.
</p></li>
</ul>

<h3>Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation. (arXiv:2305.07455v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07455">http://arxiv.org/abs/2305.07455</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07455] Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation](http://arxiv.org/abs/2305.07455) #robust</code></li>
<li>Summary: <p>Most of the speech translation models heavily rely on parallel data, which is
hard to collect especially for low-resource languages. To tackle this issue, we
propose to build a cascaded speech translation system without leveraging any
kind of paired data. We use fully unpaired data to train our unsupervised
systems and evaluate our results on CoVoST 2 and CVSS. The results show that
our work is comparable with some other early supervised methods in some
language pairs. While cascaded systems always suffer from severe error
propagation problems, we proposed denoising back-translation (DBT), a novel
approach to building robust unsupervised neural machine translation (UNMT). DBT
successfully increases the BLEU score by 0.7--0.9 in all three translation
directions. Moreover, we simplified the pipeline of our cascaded system to
reduce inference latency and conducted a comprehensive analysis of every part
of our work. We also demonstrate our unsupervised speech translation results on
the established website.
</p></li>
</ul>

<h3>Title: Perturbation-based QE: An Explainable, Unsupervised Word-level Quality Estimation Method for Blackbox Machine Translation. (arXiv:2305.07457v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07457">http://arxiv.org/abs/2305.07457</a></li>
<li>Code URL: <a href="https://github.com/tuanh23/perturbation-basedqe">https://github.com/tuanh23/perturbation-basedqe</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07457] Perturbation-based QE: An Explainable, Unsupervised Word-level Quality Estimation Method for Blackbox Machine Translation](http://arxiv.org/abs/2305.07457) #robust</code></li>
<li>Summary: <p>Quality Estimation (QE) is the task of predicting the quality of Machine
Translation (MT) system output, without using any gold-standard translation
references. State-of-the-art QE models are supervised: they require
human-labeled quality of some MT system output on some datasets for training,
making them domain-dependent and MT-system-dependent. There has been research
on unsupervised QE, which requires glass-box access to the MT systems, or
parallel MT data to generate synthetic errors for training QE models. In this
paper, we present Perturbation-based QE - a word-level Quality Estimation
approach that works simply by analyzing MT system output on perturbed input
source sentences. Our approach is unsupervised, explainable, and can evaluate
any type of blackbox MT systems, including the currently prominent large
language models (LLMs) with opaque internal processes. For language directions
with no labeled QE data, our approach has similar or better performance than
the zero-shot supervised approach on the WMT21 shared task. Our approach is
better at detecting gender bias and word-sense-disambiguation errors in
translation than supervised QE, indicating its robustness to out-of-domain
usage. The performance gap is larger when detecting errors on a nontraditional
translation-prompting LLM, indicating that our approach is more generalizable
to different MT systems. We give examples demonstrating our approach's
explainability power, where it shows which input source words have influence on
a certain MT output word.
</p></li>
</ul>

<h3>Title: Versatile Audio-Visual Learning for Handling Single and Multi Modalities in Emotion Regression and Classification Tasks. (arXiv:2305.07216v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07216">http://arxiv.org/abs/2305.07216</a></li>
<li>Code URL: <a href="https://github.com/ilucasgoncalves/vavl">https://github.com/ilucasgoncalves/vavl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07216] Versatile Audio-Visual Learning for Handling Single and Multi Modalities in Emotion Regression and Classification Tasks](http://arxiv.org/abs/2305.07216) #robust</code></li>
<li>Summary: <p>Most current audio-visual emotion recognition models lack the flexibility
needed for deployment in practical applications. We envision a multimodal
system that works even when only one modality is available and can be
implemented interchangeably for either predicting emotional attributes or
recognizing categorical emotions. Achieving such flexibility in a multimodal
emotion recognition system is difficult due to the inherent challenges in
accurately interpreting and integrating varied data sources. It is also a
challenge to robustly handle missing or partial information while allowing
direct switch between regression and classification tasks. This study proposes
a \emph{versatile audio-visual learning} (VAVL) framework for handling unimodal
and multimodal systems for emotion regression and emotion classification tasks.
We implement an audio-visual framework that can be trained even when audio and
visual paired data is not available for part of the training set (i.e., audio
only or only video is present). We achieve this effective representation
learning with audio-visual shared layers, residual connections over shared
layers, and a unimodal reconstruction task. Our experimental results reveal
that our architecture significantly outperforms strong baselines on both the
CREMA-D and MSP-IMPROV corpora. Notably, VAVL attains a new state-of-the-art
performance in the emotional attribute prediction task on the MSP-IMPROV
corpus. Code available at: https://github.com/ilucasgoncalves/VAVL
</p></li>
</ul>

<h3>Title: Online Learning Under A Separable Stochastic Approximation Framework. (arXiv:2305.07484v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07484">http://arxiv.org/abs/2305.07484</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07484] Online Learning Under A Separable Stochastic Approximation Framework](http://arxiv.org/abs/2305.07484) #robust</code></li>
<li>Summary: <p>We propose an online learning algorithm for a class of machine learning
models under a separable stochastic approximation framework. The essence of our
idea lies in the observation that certain parameters in the models are easier
to optimize than others. In this paper, we focus on models where some
parameters have a linear nature, which is common in machine learning. In one
routine of the proposed algorithm, the linear parameters are updated by the
recursive least squares (RLS) algorithm, which is equivalent to a stochastic
Newton method; then, based on the updated linear parameters, the nonlinear
parameters are updated by the stochastic gradient method (SGD). The proposed
algorithm can be understood as a stochastic approximation version of block
coordinate gradient descent approach in which one part of the parameters is
updated by a second-order SGD method while the other part is updated by a
first-order SGD. Global convergence of the proposed online algorithm for
non-convex cases is established in terms of the expected violation of a
first-order optimality condition. Numerical experiments have shown that the
proposed method accelerates convergence significantly and produces more robust
training and test performance when compared to other popular learning
algorithms. Moreover, our algorithm is less sensitive to the learning rate and
outperforms the recently proposed slimTrain algorithm. The code has been
uploaded to GitHub for validation.
</p></li>
</ul>

<h3>Title: MoMo: Momentum Models for Adaptive Learning Rates. (arXiv:2305.07583v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07583">http://arxiv.org/abs/2305.07583</a></li>
<li>Code URL: <a href="https://github.com/fabian-sp/MoMo">https://github.com/fabian-sp/MoMo</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07583] MoMo: Momentum Models for Adaptive Learning Rates](http://arxiv.org/abs/2305.07583) #robust</code></li>
<li>Summary: <p>We present new adaptive learning rates that can be used with any momentum
method. To showcase our new learning rates we develop MoMo and MoMo-Adam, which
are SGD with momentum (SGDM) and Adam together with our new adaptive learning
rates. Our MoMo methods are motivated through model-based stochastic
optimization, wherein we use momentum estimates of the batch losses and
gradients sampled at each iteration to build a model of the loss function. Our
model also makes use of any known lower bound of the loss function by using
truncation. Indeed most losses are bounded below by zero. We then approximately
minimize this model at each iteration to compute the next step. For losses with
unknown lower bounds, we develop new on-the-fly estimates of the lower bound
that we use in our model. Numerical experiments show that our MoMo methods
improve over SGDM and Adam in terms of accuracy and robustness to
hyperparameter tuning for training image classifiers on MNIST, CIFAR10,
CIFAR100, Imagenet32, DLRM on the Criteo dataset, and a transformer model on
the translation task IWSLT14.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Visual Information Extraction in the Wild: Practical Dataset and End-to-end Solution. (arXiv:2305.07498v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07498">http://arxiv.org/abs/2305.07498</a></li>
<li>Code URL: <a href="https://github.com/jfkuang/cfam">https://github.com/jfkuang/cfam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07498] Visual Information Extraction in the Wild: Practical Dataset and End-to-end Solution](http://arxiv.org/abs/2305.07498) #extraction</code></li>
<li>Summary: <p>Visual information extraction (VIE), which aims to simultaneously perform OCR
and information extraction in a unified framework, has drawn increasing
attention due to its essential role in various applications like understanding
receipts, goods, and traffic signs. However, as existing benchmark datasets for
VIE mainly consist of document images without the adequate diversity of layout
structures, background disturbs, and entity categories, they cannot fully
reveal the challenges of real-world applications. In this paper, we propose a
large-scale dataset consisting of camera images for VIE, which contains not
only the larger variance of layout, backgrounds, and fonts but also much more
types of entities. Besides, we propose a novel framework for end-to-end VIE
that combines the stages of OCR and information extraction in an end-to-end
learning fashion. Different from the previous end-to-end approaches that
directly adopt OCR features as the input of an information extraction module,
we propose to use contrastive learning to narrow the semantic gap caused by the
difference between the tasks of OCR and information extraction. We evaluate the
existing end-to-end methods for VIE on the proposed dataset and observe that
the performance of these methods has a distinguishable drop from SROIE (a
widely used English dataset) to our proposed dataset due to the larger variance
of layout and entities. These results demonstrate our dataset is more practical
for promoting advanced VIE algorithms. In addition, experiments demonstrate
that the proposed VIE method consistently achieves the obvious performance
gains on the proposed and SROIE datasets.
</p></li>
</ul>

<h3>Title: Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery. (arXiv:2305.07637v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07637">http://arxiv.org/abs/2305.07637</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07637] Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery](http://arxiv.org/abs/2305.07637) #extraction</code></li>
<li>Summary: <p>The Imaging Data Commons (IDC) is a cloud-based database that provides
researchers with open access to cancer imaging data and tools for analysis,
with the goal of facilitating collaboration in medical imaging research.
However, querying the IDC database for cohort discovery and access to imaging
data has a significant learning curve for researchers due to its complex and
technical nature. We developed Text2Cohort, a large language model (LLM) based
toolkit to facilitate natural language cohort discovery by translating user
input into IDC database queries through prompt engineering and returning the
query's response to the user. Furthermore, autocorrection is implemented to
resolve syntax and semantic errors in queries by passing the errors back to the
model for interpretation and correction. We evaluate Text2Cohort on 50 natural
language user inputs ranging from information extraction to cohort discovery.
The resulting queries and outputs were verified by two computer scientists to
measure Text2Cohort's accuracy and F1 score. Text2Cohort successfully generated
queries and their responses with an 88% accuracy and F1 score of 0.94. However,
it failed to generate queries for six user inputs due to syntax and semantic
errors. Our results indicate that Text2Cohort succeeded at generating queries
with correct responses, but occasionally failed due to a poor understanding of
the data schema. Despite these shortcomings, Text2Cohort demonstrates the
utility of LLMs to enable researchers to discover and curate cohorts using data
hosted on IDC with incredible accuracy using natural language in a more
intuitive and user-friendly way, thus democratizing access to the IDC.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Divide-and-Conquer the NAS puzzle in Resource Constrained Federated Learning Systems. (arXiv:2305.07135v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07135">http://arxiv.org/abs/2305.07135</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07135] Divide-and-Conquer the NAS puzzle in Resource Constrained Federated Learning Systems](http://arxiv.org/abs/2305.07135) #federate</code></li>
<li>Summary: <p>Federated Learning (FL) is a privacy-preserving distributed machine learning
approach geared towards applications in edge devices. However, the problem of
designing custom neural architectures in federated environments is not tackled
from the perspective of overall system efficiency. In this paper, we propose
DC-NAS -- a divide-and-conquer approach that performs supernet-based Neural
Architecture Search (NAS) in a federated system by systematically sampling the
search space. We propose a novel diversified sampling strategy that balances
exploration and exploitation of the search space by initially maximizing the
distance between the samples and progressively shrinking this distance as the
training progresses. We then perform channel pruning to reduce the training
complexity at the devices further. We show that our approach outperforms
several sampling strategies including Hadamard sampling, where the samples are
maximally separated. We evaluate our method on the CIFAR10, CIFAR100, EMNIST,
and TinyImagenet benchmarks and show a comprehensive analysis of different
aspects of federated learning such as scalability, and non-IID data. DC-NAS
achieves near iso-accuracy as compared to full-scale federated NAS with 50%
fewer resources.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Fairness in Machine Learning meets with Equity in Healthcare. (arXiv:2305.07041v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07041">http://arxiv.org/abs/2305.07041</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07041] Fairness in Machine Learning meets with Equity in Healthcare](http://arxiv.org/abs/2305.07041) #fair</code></li>
<li>Summary: <p>With the growing utilization of machine learning in healthcare, there is
increasing potential to enhance healthcare outcomes and efficiency. However,
this also brings the risk of perpetuating biases in data and model design that
can harm certain protected groups based on factors such as age, gender, and
race. This study proposes an artificial intelligence framework, grounded in
software engineering principles, for identifying and mitigating biases in data
and models while ensuring fairness in healthcare settings. A case study is
presented to demonstrate how systematic biases in data can lead to amplified
biases in model predictions, and machine learning methods are suggested to
prevent such biases. Future research aims to test and validate the proposed ML
framework in real-world clinical settings to evaluate its impact on promoting
health equity.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Asymmetric feature interaction for interpreting model predictions. (arXiv:2305.07224v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07224">http://arxiv.org/abs/2305.07224</a></li>
<li>Code URL: <a href="https://github.com/stilllu/asiv">https://github.com/stilllu/asiv</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07224] Asymmetric feature interaction for interpreting model predictions](http://arxiv.org/abs/2305.07224) #interpretability</code></li>
<li>Summary: <p>In natural language processing (NLP), deep neural networks (DNNs) could model
complex interactions between context and have achieved impressive results on a
range of NLP tasks. Prior works on feature interaction attribution mainly focus
on studying symmetric interaction that only explains the additional influence
of a set of words in combination, which fails to capture asymmetric influence
that contributes to model prediction. In this work, we propose an asymmetric
feature interaction attribution explanation model that aims to explore
asymmetric higher-order feature interactions in the inference of deep neural
NLP models. By representing our explanation with an directed interaction graph,
we experimentally demonstrate interpretability of the graph to discover
asymmetric feature interactions. Experimental results on two sentiment
classification datasets show the superiority of our model against the
state-of-the-art feature interaction attribution methods in identifying
influential features for model predictions. Our code is available at
https://github.com/StillLu/ASIV.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: eXplainable Artificial Intelligence on Medical Images: A Survey. (arXiv:2305.07511v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07511">http://arxiv.org/abs/2305.07511</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07511] eXplainable Artificial Intelligence on Medical Images: A Survey](http://arxiv.org/abs/2305.07511) #explainability</code></li>
<li>Summary: <p>Over the last few years, the number of works about deep learning applied to
the medical field has increased enormously. The necessity of a rigorous
assessment of these models is required to explain these results to all people
involved in medical exams. A recent field in the machine learning area is
explainable artificial intelligence, also known as XAI, which targets to
explain the results of such black box models to permit the desired assessment.
This survey analyses several recent studies in the XAI field applied to medical
diagnosis research, allowing some explainability of the machine learning
results in several different diseases, such as cancers and COVID-19.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Hawkes Process based on Controlled Differential Equations. (arXiv:2305.07031v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07031">http://arxiv.org/abs/2305.07031</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07031] Hawkes Process based on Controlled Differential Equations](http://arxiv.org/abs/2305.07031) #diffusion</code></li>
<li>Summary: <p>Hawkes processes are a popular framework to model the occurrence of
sequential events, i.e., occurrence dynamics, in several fields such as social
diffusion. In real-world scenarios, the inter-arrival time among events is
irregular. However, existing neural network-based Hawkes process models not
only i) fail to capture such complicated irregular dynamics, but also ii)
resort to heuristics to calculate the log-likelihood of events since they are
mostly based on neural networks designed for regular discrete inputs. To this
end, we present the concept of Hawkes process based on controlled differential
equations (HP-CDE), by adopting the neural controlled differential equation
(neural CDE) technology which is an analogue to continuous RNNs. Since HP-CDE
continuously reads data, i) irregular time-series datasets can be properly
treated preserving their uneven temporal spaces, and ii) the log-likelihood can
be exactly computed. Moreover, as both Hawkes processes and neural CDEs are
first developed to model complicated human behavioral dynamics, neural
CDE-based Hawkes processes are successful in modeling such occurrence dynamics.
In our experiments with 4 real-world datasets, our method outperforms existing
methods by non-trivial margins.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Salient Mask-Guided Vision Transformer for Fine-Grained Classification. (arXiv:2305.07102v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07102">http://arxiv.org/abs/2305.07102</a></li>
<li>Code URL: <a href="https://github.com/demidovd98/sm-vit">https://github.com/demidovd98/sm-vit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07102] Salient Mask-Guided Vision Transformer for Fine-Grained Classification](http://arxiv.org/abs/2305.07102) #transformer</code></li>
<li>Summary: <p>Fine-grained visual classification (FGVC) is a challenging computer vision
problem, where the task is to automatically recognise objects from subordinate
categories. One of its main difficulties is capturing the most discriminative
inter-class variances among visually similar classes. Recently, methods with
Vision Transformer (ViT) have demonstrated noticeable achievements in FGVC,
generally by employing the self-attention mechanism with additional
resource-consuming techniques to distinguish potentially discriminative regions
while disregarding the rest. However, such approaches may struggle to
effectively focus on truly discriminative regions due to only relying on the
inherent self-attention mechanism, resulting in the classification token likely
aggregating global information from less-important background patches.
Moreover, due to the immense lack of the datapoints, classifiers may fail to
find the most helpful inter-class distinguishing features, since other
unrelated but distinctive background regions may be falsely recognised as being
valuable. To this end, we introduce a simple yet effective Salient Mask-Guided
Vision Transformer (SM-ViT), where the discriminability of the standard ViT`s
attention maps is boosted through salient masking of potentially discriminative
foreground regions. Extensive experiments demonstrate that with the standard
training procedure our SM-ViT achieves state-of-the-art performance on popular
FGVC benchmarks among existing ViT-based approaches while requiring fewer
resources and lower input image resolution.
</p></li>
</ul>

<h3>Title: OneCAD: One Classifier for All image Datasets using multimodal learning. (arXiv:2305.07167v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07167">http://arxiv.org/abs/2305.07167</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07167] OneCAD: One Classifier for All image Datasets using multimodal learning](http://arxiv.org/abs/2305.07167) #transformer</code></li>
<li>Summary: <p>Vision-Transformers (ViTs) and Convolutional neural networks (CNNs) are
widely used Deep Neural Networks (DNNs) for classification task. These model
architectures are dependent on the number of classes in the dataset it was
trained on. Any change in number of classes leads to change (partial or full)
in the model's architecture. This work addresses the question: Is it possible
to create a number-of-class-agnostic model architecture?. This allows model's
architecture to be independent of the dataset it is trained on. This work
highlights the issues with the current architectures (ViTs and CNNs). Also,
proposes a training and inference framework OneCAD (One Classifier for All
image Datasets) to achieve close-to number-of-class-agnostic transformer model.
To best of our knowledge this is the first work to use Mask-Image-Modeling
(MIM) with multimodal learning for classification task to create a DNN model
architecture agnostic to the number of classes. Preliminary results are shown
on natural and medical image datasets. Datasets: MNIST, CIFAR10, CIFAR100 and
COVIDx. Code will soon be publicly available on github.
</p></li>
</ul>

<h3>Title: T-former: An Efficient Transformer for Image Inpainting. (arXiv:2305.07239v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07239">http://arxiv.org/abs/2305.07239</a></li>
<li>Code URL: <a href="https://github.com/dengyecode/t-former_image_inpainting">https://github.com/dengyecode/t-former_image_inpainting</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07239] T-former: An Efficient Transformer for Image Inpainting](http://arxiv.org/abs/2305.07239) #transformer</code></li>
<li>Summary: <p>Benefiting from powerful convolutional neural networks (CNNs), learning-based
image inpainting methods have made significant breakthroughs over the years.
However, some nature of CNNs (e.g. local prior, spatially shared parameters)
limit the performance in the face of broken images with diverse and complex
forms. Recently, a class of attention-based network architectures, called
transformer, has shown significant performance on natural language processing
fields and high-level vision tasks. Compared with CNNs, attention operators are
better at long-range modeling and have dynamic weights, but their computational
complexity is quadratic in spatial resolution, and thus less suitable for
applications involving higher resolution images, such as image inpainting. In
this paper, we design a novel attention linearly related to the resolution
according to Taylor expansion. And based on this attention, a network called
$T$-former is designed for image inpainting. Experiments on several benchmark
datasets demonstrate that our proposed method achieves state-of-the-art
accuracy while maintaining a relatively low number of parameters and
computational complexity. The code can be found at
\href{https://github.com/dengyecode/T-former_image_inpainting}{github.com/dengyecode/T-former_image_inpainting}
</p></li>
</ul>

<h3>Title: SSD-MonoDTR: Supervised Scale-constrained Deformable Transformer for Monocular 3D Object Detection. (arXiv:2305.07270v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07270">http://arxiv.org/abs/2305.07270</a></li>
<li>Code URL: <a href="https://github.com/mikasa3lili/ssd-monodetr">https://github.com/mikasa3lili/ssd-monodetr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07270] SSD-MonoDTR: Supervised Scale-constrained Deformable Transformer for Monocular 3D Object Detection](http://arxiv.org/abs/2305.07270) #transformer</code></li>
<li>Summary: <p>Transformer-based methods have demonstrated superior performance for
monocular 3D object detection recently, which predicts 3D attributes from a
single 2D image. Most existing transformer-based methods leverage visual and
depth representations to explore valuable query points on objects, and the
quality of the learned queries has a great impact on detection accuracy.
Unfortunately, existing unsupervised attention mechanisms in transformer are
prone to generate low-quality query features due to inaccurate receptive
fields, especially on hard objects. To tackle this problem, this paper proposes
a novel <code>Supervised Scale-constrained Deformable Attention'' (SSDA) for
monocular 3D object detection. Specifically, SSDA presets several masks with
different scales and utilizes depth and visual features to predict the local
feature for each query. Imposing the scale constraint, SSDA could well predict
the accurate receptive field of a query to support robust query feature
generation. What is more, SSDA is assigned with a Weighted Scale Matching (WSM)
loss to supervise scale prediction, which presents more confident results as
compared to the unsupervised attention mechanisms. Extensive experiments on</code>KITTI'' demonstrate that SSDA significantly improves the detection accuracy
especially on moderate and hard objects, yielding SOTA performance as compared
to the existing approaches. Code will be publicly available at
https://github.com/mikasa3lili/SSD-MonoDETR.
</p></li>
</ul>

<h3>Title: RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection. (arXiv:2305.07598v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07598">http://arxiv.org/abs/2305.07598</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07598] RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection](http://arxiv.org/abs/2305.07598) #transformer</code></li>
<li>Summary: <p>With the publication of DINO, a variant of the Detection Transformer (DETR),
Detection Transformers are breaking the record in the object detection
benchmark with the merits of their end-to-end design and scalability. However,
the extension of DETR to oriented object detection has not been thoroughly
studied although more benefits from its end-to-end architecture are expected
such as removing NMS and anchor-related costs. In this paper, we propose a
first strong DINO-based baseline for oriented object detection. We found that
straightforward employment of DETRs for oriented object detection does not
guarantee non-duplicate prediction, and propose a simple cost to mitigate this.
Furthermore, we introduce a novel denoising strategy that uses Hungarian
matching to filter redundant noised queries and query alignment to preserve
matching consistency between Transformer decoder layers. Our proposed model
outperforms previous rotated DETRs and other counterparts, achieving
state-of-the-art performance in DOTA-v1.0/v1.5/v2.0, and DIOR-R benchmarks.
</p></li>
</ul>

<h3>Title: Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07303">http://arxiv.org/abs/2305.07303</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07303] Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions](http://arxiv.org/abs/2305.07303) #transformer</code></li>
<li>Summary: <p>Neural-based word embeddings using solely distributional information have
consistently produced useful meaning representations for downstream tasks.
However, existing approaches often result in representations that are hard to
interpret and control. Natural language definitions, on the other side, possess
a recursive, self-explanatory semantic structure that can support novel
representation learning paradigms able to preserve explicit conceptual
relations and constraints in the vector space.
</p></li>
</ul>

<p>This paper proposes a neuro-symbolic, multi-relational framework to learn
word embeddings exclusively from natural language definitions by jointly
mapping defined and defining terms along with their corresponding semantic
relations. By automatically extracting the relations from definitions corpora
and formalising the learning problem via a translational objective, we
specialise the framework in hyperbolic space to capture the hierarchical and
multi-resolution structure induced by the definitions. An extensive empirical
analysis demonstrates that the framework can help impose the desired structural
constraints while preserving the mapping required for controllable and
interpretable semantic navigation. Moreover, the experiments reveal the
superiority of the hyperbolic word embeddings over the euclidean counterparts
and demonstrate that the multi-relational framework can obtain competitive
results when compared to state-of-the-art neural approaches (including
Transformers), with the advantage of being significantly more efficient and
intrinsically interpretable.
</p>

<h3>Title: Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning. (arXiv:2305.07475v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07475">http://arxiv.org/abs/2305.07475</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07475] Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning](http://arxiv.org/abs/2305.07475) #transformer</code></li>
<li>Summary: <p>Numerical reasoning over table-and-text hybrid passages, such as financial
reports, poses significant challenges and has numerous potential applications.
Noise and irrelevant variables in the model input have been a hindrance to its
performance. Additionally, coarse-grained supervision of the whole solution
program has impeded the model's ability to learn the underlying numerical
reasoning process. In this paper, we propose three pretraining tasks that
operate at both the whole program and sub-program level: Variable Integrity
Ranking, which guides the model to focus on useful variables; Variable Operator
Prediction, which decomposes the supervision into fine-grained single operator
prediction; and Variable Keyphrase Masking, which encourages the model to
identify key evidence that sub-programs are derived from. Experimental results
demonstrate the effectiveness of our proposed methods, surpassing
transformer-based model baselines.
</p></li>
</ul>

<h3>Title: MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers. (arXiv:2305.07185v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07185">http://arxiv.org/abs/2305.07185</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07185] MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](http://arxiv.org/abs/2305.07185) #transformer</code></li>
<li>Summary: <p>Autoregressive transformers are spectacular models for short sequences but
scale poorly to long sequences such as high-resolution images, podcasts, code,
or books. We proposed Megabyte, a multi-scale decoder architecture that enables
end-to-end differentiable modeling of sequences of over one million bytes.
Megabyte segments sequences into patches and uses a local submodel within
patches and a global model between patches. This enables sub-quadratic
self-attention, much larger feedforward layers for the same compute, and
improved parallelism during decoding -- unlocking better performance at reduced
cost for both training and generation. Extensive experiments show that Megabyte
allows byte-level models to perform competitively with subword models on long
context language modeling, achieve state-of-the-art density estimation on
ImageNet, and model audio from raw files. Together, these results establish the
viability of tokenization-free autoregressive sequence modeling at scale.
</p></li>
</ul>

<h3>Title: AGFormer: Efficient Graph Representation with Anchor-Graph Transformer. (arXiv:2305.07521v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07521">http://arxiv.org/abs/2305.07521</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07521] AGFormer: Efficient Graph Representation with Anchor-Graph Transformer](http://arxiv.org/abs/2305.07521) #transformer</code></li>
<li>Summary: <p>To alleviate the local receptive issue of GCN, Transformers have been
exploited to capture the long range dependences of nodes for graph data
representation and learning. However, existing graph Transformers generally
employ regular self-attention module for all node-to-node message passing which
needs to learn the affinities/relationships between all node's pairs, leading
to high computational cost issue. Also, they are usually sensitive to graph
noises. To overcome this issue, we propose a novel graph Transformer
architecture, termed Anchor Graph Transformer (AGFormer), by leveraging an
anchor graph model. To be specific, AGFormer first obtains some representative
anchors and then converts node-to-node message passing into anchor-to-anchor
and anchor-to-node message passing process. Thus, AGFormer performs much more
efficiently and also robustly than regular node-to-node Transformers. Extensive
experiments on several benchmark datasets demonstrate the effectiveness and
benefits of proposed AGFormer.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models. (arXiv:2305.07528v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07528">http://arxiv.org/abs/2305.07528</a></li>
<li>Code URL: <a href="https://github.com/Infernolia/WEDGE">https://github.com/Infernolia/WEDGE</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07528] WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models](http://arxiv.org/abs/2305.07528) #generative</code></li>
<li>Summary: <p>The open road poses many challenges to autonomous perception, including poor
visibility from extreme weather conditions. Models trained on good-weather
datasets frequently fail at detection in these out-of-distribution settings. To
aid adversarial robustness in perception, we introduce WEDGE (WEather images by
DALL-E GEneration): a synthetic dataset generated with a vision-language
generative model via prompting. WEDGE consists of 3360 images in 16 extreme
weather conditions manually annotated with 16513 bounding boxes, supporting
research in the tasks of weather classification and 2D object detection. We
have analyzed WEDGE from research standpoints, verifying its effectiveness for
extreme-weather autonomous perception. We establish baseline performance for
classification and detection with 53.87% test accuracy and 45.41 mAP. Most
importantly, WEDGE can be used to fine-tune state-of-the-art detectors,
improving SOTA performance on real-world weather benchmarks (such as DAWN) by
4.48 AP for well-generated classes like trucks. WEDGE has been collected under
OpenAI's terms of use and is released for public use under the CC BY-NC-SA 4.0
license. The repository for this work and dataset is available at
https://infernolia.github.io/WEDGE.
</p></li>
</ul>

<h3>Title: RepCL: Exploring Effective Representation for Continual Text Classification. (arXiv:2305.07289v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07289">http://arxiv.org/abs/2305.07289</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07289] RepCL: Exploring Effective Representation for Continual Text Classification](http://arxiv.org/abs/2305.07289) #generative</code></li>
<li>Summary: <p>Continual learning (CL) aims to constantly learn new knowledge over time
while avoiding catastrophic forgetting on old tasks. In this work, we focus on
continual text classification under the class-incremental setting. Recent CL
studies find that the representations learned in one task may not be effective
for other tasks, namely representation bias problem. For the first time we
formally analyze representation bias from an information bottleneck perspective
and suggest that exploiting representations with more class-relevant
information could alleviate the bias. To this end, we propose a novel
replay-based continual text classification method, RepCL. Our approach utilizes
contrastive and generative representation learning objectives to capture more
class-relevant features. In addition, RepCL introduces an adversarial replay
strategy to alleviate the overfitting problem of replay. Experiments
demonstrate that RepCL effectively alleviates forgetting and achieves
state-of-the-art performance on three text classification tasks.
</p></li>
</ul>

<h3>Title: Towards Understanding and Improving GFlowNet Training. (arXiv:2305.07170v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07170">http://arxiv.org/abs/2305.07170</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07170] Towards Understanding and Improving GFlowNet Training](http://arxiv.org/abs/2305.07170) #generative</code></li>
<li>Summary: <p>Generative flow networks (GFlowNets) are a family of algorithms that learn a
generative policy to sample discrete objects $x$ with non-negative reward
$R(x)$. Learning objectives guarantee the GFlowNet samples $x$ from the target
distribution $p^<em>(x) \propto R(x)$ when loss is globally minimized over all
states or trajectories, but it is unclear how well they perform with practical
limits on training resources. We introduce an efficient evaluation strategy to
compare the learned sampling distribution to the target reward distribution. As
flows can be underdetermined given training data, we clarify the importance of
learned flows to generalization and matching $p^</em>(x)$ in practice. We
investigate how to learn better flows, and propose (i) prioritized replay
training of high-reward $x$, (ii) relative edge flow policy parametrization,
and (iii) a novel guided trajectory balance objective, and show how it can
solve a substructure credit assignment problem. We substantially improve sample
efficiency on biochemical design tasks.
</p></li>
</ul>

<h3>Title: Provably Convergent Schr\"odinger Bridge with Applications to Probabilistic Time Series Imputation. (arXiv:2305.07247v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07247">http://arxiv.org/abs/2305.07247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07247] Provably Convergent Schr\"odinger Bridge with Applications to Probabilistic Time Series Imputation](http://arxiv.org/abs/2305.07247) #generative</code></li>
<li>Summary: <p>The Schr\"odinger bridge problem (SBP) is gaining increasing attention in
generative modeling and showing promising potential even in comparison with the
score-based generative models (SGMs). SBP can be interpreted as an
entropy-regularized optimal transport problem, which conducts projections onto
every other marginal alternatingly. However, in practice, only approximated
projections are accessible and their convergence is not well understood. To
fill this gap, we present a first convergence analysis of the Schr\"odinger
bridge algorithm based on approximated projections. As for its practical
applications, we apply SBP to probabilistic time series imputation by
generating missing values conditioned on observed data. We show that optimizing
the transport cost improves the performance and the proposed algorithm achieves
the state-of-the-art result in healthcare and environmental data while
exhibiting the advantage of exploring both temporal and feature patterns in
probabilistic time series imputation.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4. (arXiv:2305.07490v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07490">http://arxiv.org/abs/2305.07490</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07490] ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4](http://arxiv.org/abs/2305.07490) #large language model</code></li>
<li>Summary: <p>In recent years, large language models (LLMs) have made significant progress
in natural language processing (NLP), with models like ChatGPT and GPT-4
achieving impressive capabilities in various linguistic tasks. However,
training models on such a large scale is challenging, and finding datasets that
match the model's scale is often difficult. Fine-tuning and training models
with fewer parameters using novel methods have emerged as promising approaches
to overcome these challenges. One such model is MiniGPT-4, which achieves
comparable vision-language understanding to GPT-4 by leveraging novel
pre-training models and innovative training strategies. However, the model
still faces some challenges in image understanding, particularly in artistic
pictures. A novel multimodal model called ArtGPT-4 has been proposed to address
these limitations. ArtGPT-4 was trained on image-text pairs using a Tesla A100
device in just 2 hours, using only about 200 GB of data. The model can depict
images with an artistic flair and generate visual code, including aesthetically
pleasing HTML/CSS web pages. Furthermore, the article proposes novel benchmarks
for evaluating the performance of vision-language models. In the subsequent
evaluation methods, ArtGPT-4 scored more than 1 point higher than the current
\textbf{state-of-the-art} model and was only 0.25 points lower than artists on
a 6-point scale. Our code and pre-trained model are available at
\url{https://huggingface.co/Tyrannosaurus/ArtGPT-4}.
</p></li>
</ul>

<h3>Title: Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales. (arXiv:2305.07095v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07095">http://arxiv.org/abs/2305.07095</a></li>
<li>Code URL: <a href="https://github.com/ink-usc/rationalehumanutility">https://github.com/ink-usc/rationalehumanutility</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07095] Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales](http://arxiv.org/abs/2305.07095) #large language model</code></li>
<li>Summary: <p>Among the remarkable emergent capabilities of large language models (LMs) is
free-text rationalization; beyond a certain scale, large LMs are capable of
generating seemingly useful rationalizations, which in turn, can dramatically
enhance their performances on leaderboards. This phenomenon raises a question:
can machine generated rationales also be useful for humans, especially when lay
humans try to answer questions based on those machine rationales? We observe
that human utility of existing rationales is far from satisfactory, and
expensive to estimate with human studies. Existing metrics like task
performance of the LM generating the rationales, or similarity between
generated and gold rationales are not good indicators of their human utility.
While we observe that certain properties of rationales like conciseness and
novelty are correlated with their human utility, estimating them without human
involvement is challenging. We show that, by estimating a rationale's
helpfulness in answering similar unseen instances, we can measure its human
utility to a better extent. We also translate this finding into an automated
score, GEN-U, that we propose, which can help improve LMs' ability to generate
rationales with better human utility, while maintaining most of its task
performance. Lastly, we release all code and collected data with this project.
</p></li>
</ul>

<h3>Title: Exploring Zero and Few-shot Techniques for Intent Classification. (arXiv:2305.07157v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07157">http://arxiv.org/abs/2305.07157</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07157] Exploring Zero and Few-shot Techniques for Intent Classification](http://arxiv.org/abs/2305.07157) #large language model</code></li>
<li>Summary: <p>Conversational NLU providers often need to scale to thousands of
intent-classification models where new customers often face the cold-start
problem. Scaling to so many customers puts a constraint on storage space as
well. In this paper, we explore four different zero and few-shot intent
classification approaches with this low-resource constraint: 1) domain
adaptation, 2) data augmentation, 3) zero-shot intent classification using
descriptions large language models (LLMs), and 4) parameter-efficient
fine-tuning of instruction-finetuned language models. Our results show that all
these approaches are effective to different degrees in low-resource settings.
Parameter-efficient fine-tuning using T-few recipe (Liu et al., 2022) on
Flan-T5 (Chang et al., 2022) yields the best performance even with just one
sample per intent. We also show that the zero-shot method of prompting LLMs
using intent descriptions
</p></li>
</ul>

<h3>Title: When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust. (arXiv:2305.07230v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07230">http://arxiv.org/abs/2305.07230</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07230] When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust](http://arxiv.org/abs/2305.07230) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have significantly advanced the field of natural
language processing, with GPT models at the forefront. While their remarkable
performance spans a range of tasks, adapting LLMs for real-world business
scenarios still poses challenges warranting further investigation. This paper
presents an empirical analysis aimed at bridging the gap in adapting LLMs to
practical use cases. To do that, we select the question answering (QA) task of
insurance as a case study due to its challenge of reasoning. Based on the task
we design a new model relied on LLMs which are empowered by domain-specific
knowledge extracted from insurance policy rulebooks. The domain-specific
knowledge helps LLMs to understand new concepts of insurance for domain
adaptation. Preliminary results on real QA pairs show that knowledge
enhancement from policy rulebooks significantly improves the reasoning ability
of GPT-3.5 of 50.4% in terms of accuracy. The analysis also indicates that
existing public knowledge bases, e.g., DBPedia is beneficial for knowledge
enhancement. Our findings reveal that the inherent complexity of business
scenarios often necessitates the incorporation of domain-specific knowledge and
external resources for effective problem-solving.
</p></li>
</ul>

<h3>Title: Harvesting Event Schemas from Large Language Models. (arXiv:2305.07280v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07280">http://arxiv.org/abs/2305.07280</a></li>
<li>Code URL: <a href="https://github.com/tangjialong/event-schema-harvester">https://github.com/tangjialong/event-schema-harvester</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07280] Harvesting Event Schemas from Large Language Models](http://arxiv.org/abs/2305.07280) #large language model</code></li>
<li>Summary: <p>Event schema provides a conceptual, structural and formal language to
represent events and model the world event knowledge. Unfortunately, it is
challenging to automatically induce high-quality and high-coverage event
schemas due to the open nature of real-world events, the diversity of event
expressions, and the sparsity of event knowledge. In this paper, we propose a
new paradigm for event schema induction -- knowledge harvesting from
large-scale pre-trained language models, which can effectively resolve the
above challenges by discovering, conceptualizing and structuralizing event
schemas from PLMs. And an Event Schema Harvester (ESHer) is designed to
automatically induce high-quality event schemas via in-context generation-based
conceptualization, confidence-aware schema structuralization and graph-based
schema aggregation. Empirical results show that ESHer can induce high-quality
and high-coverage event schemas on varying domains.
</p></li>
</ul>

<h3>Title: MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large Language Models in Medicine. (arXiv:2305.07340v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07340">http://arxiv.org/abs/2305.07340</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07340] MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large Language Models in Medicine](http://arxiv.org/abs/2305.07340) #large language model</code></li>
<li>Summary: <p>METHODS: First, a set of evaluation criteria is designed based on a
comprehensive literature review. Second, existing candidate criteria are
optimized for using a Delphi method by five experts in medicine and
engineering. Third, three clinical experts design a set of medical datasets to
interact with LLMs. Finally, benchmarking experiments are conducted on the
datasets. The responses generated by chatbots based on LLMs are recorded for
blind evaluations by five licensed medical experts. RESULTS: The obtained
evaluation criteria cover medical professional capabilities, social
comprehensive capabilities, contextual capabilities, and computational
robustness, with sixteen detailed indicators. The medical datasets include
twenty-seven medical dialogues and seven case reports in Chinese. Three
chatbots are evaluated, ChatGPT by OpenAI, ERNIE Bot by Baidu Inc., and Doctor
PuJiang (Dr. PJ) by Shanghai Artificial Intelligence Laboratory. Experimental
results show that Dr. PJ outperforms ChatGPT and ERNIE Bot in both
multiple-turn medical dialogue and case report scenarios.
</p></li>
</ul>

<h3>Title: Surfacing Biases in Large Language Models using Contrastive Input Decoding. (arXiv:2305.07378v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07378">http://arxiv.org/abs/2305.07378</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07378] Surfacing Biases in Large Language Models using Contrastive Input Decoding](http://arxiv.org/abs/2305.07378) #large language model</code></li>
<li>Summary: <p>Ensuring that large language models (LMs) are fair, robust and useful
requires an understanding of how different modifications to their inputs impact
the model's behaviour. In the context of open-text generation tasks, however,
such an evaluation is not trivial. For example, when introducing a model with
an input text and a perturbed, "contrastive" version of it, meaningful
differences in the next-token predictions may not be revealed with standard
decoding strategies. With this motivation in mind, we propose Contrastive Input
Decoding (CID): a decoding algorithm to generate text given two inputs, where
the generated text is likely given one input but unlikely given the other. In
this way, the contrastive generations can highlight potentially subtle
differences in how the LM output differs for the two inputs in a simple and
interpretable manner. We use CID to highlight context-specific biases that are
hard to detect with standard decoding strategies and quantify the effect of
different input perturbations.
</p></li>
</ul>

<h3>Title: Knowledge Refinement via Interaction Between Search Engines and Large Language Models. (arXiv:2305.07402v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07402">http://arxiv.org/abs/2305.07402</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07402] Knowledge Refinement via Interaction Between Search Engines and Large Language Models](http://arxiv.org/abs/2305.07402) #large language model</code></li>
<li>Summary: <p>Information retrieval (IR) plays a crucial role in locating relevant
resources from vast amounts of data, and its applications have evolved from
traditional knowledge bases to modern search engines (SEs). The emergence of
large language models (LLMs) has further revolutionized the field by enabling
users to interact with search systems in natural language. In this paper, we
explore the advantages and disadvantages of LLMs and SEs, highlighting their
respective strengths in understanding user-issued queries and retrieving
up-to-date information. To leverage the benefits of both paradigms while
circumventing their limitations, we propose InteR, a novel framework that
facilitates knowledge refinement through interaction between SEs and LLMs.
InteR allows SEs to refine knowledge in query using LLM-generated summaries and
enables LLMs to enhance prompts using SE-retrieved documents. This iterative
refinement process augments the inputs of SEs and LLMs, leading to more
accurate retrieval. Experimental evaluations on two large-scale retrieval
benchmarks demonstrate that InteR achieves superior zero-shot document
retrieval performance compared to state-of-the-art methods, regardless of the
use of relevance judgement.
</p></li>
</ul>

<h3>Title: Calibration-Aware Bayesian Learning. (arXiv:2305.07504v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07504">http://arxiv.org/abs/2305.07504</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07504] Calibration-Aware Bayesian Learning](http://arxiv.org/abs/2305.07504) #large language model</code></li>
<li>Summary: <p>Deep learning models, including modern systems like large language models,
are well known to offer unreliable estimates of the uncertainty of their
decisions. In order to improve the quality of the confidence levels, also known
as calibration, of a model, common approaches entail the addition of either
data-dependent or data-independent regularization terms to the training loss.
Data-dependent regularizers have been recently introduced in the context of
conventional frequentist learning to penalize deviations between confidence and
accuracy. In contrast, data-independent regularizers are at the core of
Bayesian learning, enforcing adherence of the variational distribution in the
model parameter space to a prior density. The former approach is unable to
quantify epistemic uncertainty, while the latter is severely affected by model
misspecification. In light of the limitations of both methods, this paper
proposes an integrated framework, referred to as calibration-aware Bayesian
neural networks (CA-BNNs), that applies both regularizers while optimizing over
a variational distribution as in Bayesian learning. Numerical results validate
the advantages of the proposed approach in terms of expected calibration error
(ECE) and reliability diagrams.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Quaternion-valued Correlation Learning for Few-Shot Semantic Segmentation. (arXiv:2305.07283v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07283">http://arxiv.org/abs/2305.07283</a></li>
<li>Code URL: <a href="https://github.com/zwzheng98/qclnet">https://github.com/zwzheng98/qclnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07283] Quaternion-valued Correlation Learning for Few-Shot Semantic Segmentation](http://arxiv.org/abs/2305.07283) #segmentation</code></li>
<li>Summary: <p>Few-shot segmentation (FSS) aims to segment unseen classes given only a few
annotated samples. Encouraging progress has been made for FSS by leveraging
semantic features learned from base classes with sufficient training samples to
represent novel classes. The correlation-based methods lack the ability to
consider interaction of the two subspace matching scores due to the inherent
nature of the real-valued 2D convolutions. In this paper, we introduce a
quaternion perspective on correlation learning and propose a novel
Quaternion-valued Correlation Learning Network (QCLNet), with the aim to
alleviate the computational burden of high-dimensional correlation tensor and
explore internal latent interaction between query and support images by
leveraging operations defined by the established quaternion algebra.
Specifically, our QCLNet is formulated as a hyper-complex valued network and
represents correlation tensors in the quaternion domain, which uses
quaternion-valued convolution to explore the external relations of query
subspace when considering the hidden relationship of the support sub-dimension
in the quaternion space. Extensive experiments on the PASCAL-5i and COCO-20i
datasets demonstrate that our method outperforms the existing state-of-the-art
methods effectively. Our code is available at
https://github.com/zwzheng98/QCLNet
</p></li>
</ul>

<h3>Title: CLIP-Count: Towards Text-Guided Zero-Shot Object Counting. (arXiv:2305.07304v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07304">http://arxiv.org/abs/2305.07304</a></li>
<li>Code URL: <a href="https://github.com/songrise/clip-count">https://github.com/songrise/clip-count</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07304] CLIP-Count: Towards Text-Guided Zero-Shot Object Counting](http://arxiv.org/abs/2305.07304) #segmentation</code></li>
<li>Summary: <p>Recent advances in visual-language models have shown remarkable zero-shot
text-image matching ability that is transferable to down-stream tasks such as
object detection and segmentation. However, adapting these models for object
counting, which involves estimating the number of objects in an image, remains
a formidable challenge. In this study, we conduct the first exploration of
transferring visual-language models for class-agnostic object counting.
Specifically, we propose CLIP-Count, a novel pipeline that estimates density
maps for open-vocabulary objects with text guidance in a zero-shot manner,
without requiring any finetuning on specific object classes. To align the text
embedding with dense image features, we introduce a patch-text contrastive loss
that guides the model to learn informative patch-level image representations
for dense prediction. Moreover, we design a hierarchical patch-text interaction
module that propagates semantic information across different resolution levels
of image features. Benefiting from the full exploitation of the rich image-text
alignment knowledge of pretrained visual-language models, our method
effectively generates high-quality density maps for objects-of-interest.
Extensive experiments on FSC-147, CARPK, and ShanghaiTech crowd counting
datasets demonstrate that our proposed method achieves state-of-the-art
accuracy and generalizability for zero-shot object counting. Project page at
https://github.com/songrise/CLIP-Count
</p></li>
</ul>

<h3>Title: MotionBEV: Attention-Aware Online LiDAR Moving Object Segmentation with Bird's Eye View based Appearance and Motion Features. (arXiv:2305.07336v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07336">http://arxiv.org/abs/2305.07336</a></li>
<li>Code URL: <a href="https://github.com/xiekkki/motionbev">https://github.com/xiekkki/motionbev</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07336] MotionBEV: Attention-Aware Online LiDAR Moving Object Segmentation with Bird's Eye View based Appearance and Motion Features](http://arxiv.org/abs/2305.07336) #segmentation</code></li>
<li>Summary: <p>Identifying moving objects is an essential capability for autonomous systems,
as it provides critical information for pose estimation, navigation, collision
avoidance and static map construction. In this paper, we present MotionBEV, a
fast and accurate framework for LiDAR moving object segmentation, which
segments moving objects with appearance and motion features in bird's eye view
(BEV) domain. Our approach converts 3D LiDAR scans into 2D polar BEV
representation to achieve real-time performance. Specifically, we learn
appearance features with a simplified PointNet, and compute motion features
through the height differences of consecutive frames of point clouds projected
onto vertical columns in the polar BEV coordinate system. We employ a
dual-branch network bridged by the Appearance-Motion Co-attention Module (AMCM)
to adaptively fuse the spatio-temporal information from appearance and motion
features. Our approach achieves state-of-the-art performance on the
SemanticKITTI-MOS benchmark, with an average inference time of 23ms on an RTX
3090 GPU. Furthermore, to demonstrate the practical effectiveness of our
method, we provide a LiDAR-MOS dataset recorded by a solid-state LiDAR, which
features non-repetitive scanning patterns and small field of view.
</p></li>
</ul>

<h3>Title: Knowledge distillation with Segment Anything (SAM) model for Planetary Geological Mapping. (arXiv:2305.07586v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07586">http://arxiv.org/abs/2305.07586</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07586] Knowledge distillation with Segment Anything (SAM) model for Planetary Geological Mapping](http://arxiv.org/abs/2305.07586) #segmentation</code></li>
<li>Summary: <p>Planetary science research involves analysing vast amounts of remote sensing
data, which are often costly and time-consuming to annotate and process. One of
the essential tasks in this field is geological mapping, which requires
identifying and outlining regions of interest in planetary images, including
geological features and landforms. However, manually labelling these images is
a complex and challenging task that requires significant domain expertise and
effort. To expedite this endeavour, we propose the use of knowledge
distillation using the recently introduced cutting-edge Segment Anything (SAM)
model. We demonstrate the effectiveness of this prompt-based foundation model
for rapid annotation and quick adaptability to a prime use case of mapping
planetary skylights. Our work reveals that with a small set of annotations
obtained with the right prompts from the model and subsequently training a
specialised domain decoder, we can achieve satisfactory semantic segmentation
on this task. Key results indicate that the use of knowledge distillation can
significantly reduce the effort required by domain experts for manual
annotation and improve the efficiency of image segmentation tasks. This
approach has the potential to accelerate extra-terrestrial discovery by
automatically detecting and segmenting Martian landforms.
</p></li>
</ul>

<h3>Title: Meta Omnium: A Benchmark for General-Purpose Learning-to-Learn. (arXiv:2305.07625v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07625">http://arxiv.org/abs/2305.07625</a></li>
<li>Code URL: <a href="https://github.com/edi-meta-learning/meta-omnium">https://github.com/edi-meta-learning/meta-omnium</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07625] Meta Omnium: A Benchmark for General-Purpose Learning-to-Learn](http://arxiv.org/abs/2305.07625) #segmentation</code></li>
<li>Summary: <p>Meta-learning and other approaches to few-shot learning are widely studied
for image recognition, and are increasingly applied to other vision tasks such
as pose estimation and dense prediction. This naturally raises the question of
whether there is any few-shot meta-learning algorithm capable of generalizing
across these diverse task types? To support the community in answering this
question, we introduce Meta Omnium, a dataset-of-datasets spanning multiple
vision tasks including recognition, keypoint localization, semantic
segmentation and regression. We experiment with popular few-shot meta-learning
baselines and analyze their ability to generalize across tasks and to transfer
knowledge between them. Meta Omnium enables meta-learning researchers to
evaluate model generalization to a much wider array of tasks than previously
possible, and provides a single framework for evaluating meta-learners across a
wide suite of vision applications in a consistent manner.
</p></li>
</ul>

<h3>Title: The ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge 2023: Intracranial Meningioma. (arXiv:2305.07642v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07642">http://arxiv.org/abs/2305.07642</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07642] The ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge 2023: Intracranial Meningioma](http://arxiv.org/abs/2305.07642) #segmentation</code></li>
<li>Summary: <p>Meningiomas are the most common primary intracranial tumor in adults and can
be associated with significant morbidity and mortality. Radiologists,
neurosurgeons, neuro-oncologists, and radiation oncologists rely on
multiparametric MRI (mpMRI) for diagnosis, treatment planning, and longitudinal
treatment monitoring; yet automated, objective, and quantitative tools for
non-invasive assessment of meningiomas on mpMRI are lacking. The BraTS
meningioma 2023 challenge will provide a community standard and benchmark for
state-of-the-art automated intracranial meningioma segmentation models based on
the largest expert annotated multilabel meningioma mpMRI dataset to date.
Challenge competitors will develop automated segmentation models to predict
three distinct meningioma sub-regions on MRI including enhancing tumor,
non-enhancing tumor core, and surrounding nonenhancing T2/FLAIR hyperintensity.
Models will be evaluated on separate validation and held-out test datasets
using standardized metrics utilized across the BraTS 2023 series of challenges
including the Dice similarity coefficient and Hausdorff distance. The models
developed during the course of this challenge will aid in incorporation of
automated meningioma MRI segmentation into clinical practice, which will
ultimately improve care of patients with meningioma.
</p></li>
</ul>

<h3>Title: A Critical View Of Vision-Based Long-Term Dynamics Prediction Under Environment Misalignment. (arXiv:2305.07648v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07648">http://arxiv.org/abs/2305.07648</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07648] A Critical View Of Vision-Based Long-Term Dynamics Prediction Under Environment Misalignment](http://arxiv.org/abs/2305.07648) #segmentation</code></li>
<li>Summary: <p>Dynamics prediction, which is the problem of predicting future states of
scene objects based on current and prior states, is drawing increasing
attention as an instance of learning physics. To solve this problem, Region
Proposal Convolutional Interaction Network (RPCIN), a vision-based model, was
proposed and achieved state-of-the-art performance in long-term prediction.
RPCIN only takes raw images and simple object descriptions, such as the
bounding box and segmentation mask of each object, as input. However, despite
its success, the model's capability can be compromised under conditions of
environment misalignment. In this paper, we investigate two challenging
conditions for environment misalignment: Cross-Domain and Cross-Context by
proposing four datasets that are designed for these challenges: SimB-Border,
SimB-Split, BlenB-Border, and BlenB-Split. The datasets cover two domains and
two contexts. Using RPCIN as a probe, experiments conducted on the combinations
of the proposed datasets reveal potential weaknesses of the vision-based
long-term dynamics prediction model. Furthermore, we propose a promising
direction to mitigate the Cross-Domain challenge and provide concrete evidence
supporting such a direction, which provides dramatic alleviation of the
challenge on the proposed datasets.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
