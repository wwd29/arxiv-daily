<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-17</h1>
<h3>Title: Semantic Matters: Multimodal Features for Affective Analysis</h3>
<ul>
<li><strong>Authors: </strong>Tobias Hallmen, Robin-Nico Kampa, Fabian Deuser, Norbert Oswald, Elisabeth André</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11460">https://arxiv.org/abs/2504.11460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11460">https://arxiv.org/pdf/2504.11460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11460]] Semantic Matters: Multimodal Features for Affective Analysis(https://arxiv.org/abs/2504.11460)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this study, we present our methodology for two tasks: the Behavioural Ambivalence/Hesitancy (BAH) Recognition Challenge and the Emotional Mimicry Intensity (EMI) Estimation Challenge, both conducted as part of the 8th Workshop and Competition on Affective & Behavior Analysis in-the-wild. Building on previous work, we utilize a Wav2Vec 2.0 model pre-trained on a large podcast dataset to extract various audio features, capturing both linguistic and paralinguistic information. Our approach incorporates a valence-arousal-dominance (VAD) module derived from Wav2Vec 2.0, a BERT-like encoder, and a vision transformer (ViT) with predictions subsequently processed through a long short-term memory (LSTM) architecture for temporal modeling. In this iteration, we integrate the textual and visual modality into our analysis, recognizing that semantic content provides valuable contextual cues and underscoring that the meaning of speech often conveys more critical insights than its acoustic counterpart alone. Fusing in the vision modality helps in some cases to interpret the textual modality more precisely. This combined approach yields significant performance improvements over baseline methods.</li>
</ul>

<h3>Title: MultiCore+TPU Accelerated Multi-Modal TinyML for Livestock Behaviour Recognition</h3>
<ul>
<li><strong>Authors: </strong>Qianxue Zhang, Eiman Kanjo</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11467">https://arxiv.org/abs/2504.11467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11467">https://arxiv.org/pdf/2504.11467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11467]] MultiCore+TPU Accelerated Multi-Modal TinyML for Livestock Behaviour Recognition(https://arxiv.org/abs/2504.11467)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The advancement of technology has revolutionised the agricultural industry, transitioning it from labour-intensive farming practices to automated, AI-powered management systems. In recent years, more intelligent livestock monitoring solutions have been proposed to enhance farming efficiency and productivity. This work presents a novel approach to animal activity recognition and movement tracking, leveraging tiny machine learning (TinyML) techniques, wireless communication framework, and microcontroller platforms to develop an efficient, cost-effective livestock sensing system. It collects and fuses accelerometer data and vision inputs to build a multi-modal network for three tasks: image classification, object detection, and behaviour recognition. The system is deployed and evaluated on commercial microcontrollers for real-time inference using embedded applications, demonstrating up to 270$\times$ model size reduction, less than 80ms response latency, and on-par performance comparable to existing methods. The incorporation of the TinyML technique allows for seamless data transmission between devices, benefiting use cases in remote locations with poor Internet connectivity. This work delivers a robust, scalable IoT-edge livestock monitoring solution adaptable to diverse farming needs, offering flexibility for future extensions.</li>
</ul>

<h3>Title: SO-DETR: Leveraging Dual-Domain Features and Knowledge Distillation for Small Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Huaxiang Zhang, Hao Zhang, Aoran Mei, Zhongxue Gan, Guo-Niu Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11470">https://arxiv.org/abs/2504.11470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11470">https://arxiv.org/pdf/2504.11470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11470]] SO-DETR: Leveraging Dual-Domain Features and Knowledge Distillation for Small Object Detection(https://arxiv.org/abs/2504.11470)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Detection Transformer-based methods have achieved significant advancements in general object detection. However, challenges remain in effectively detecting small objects. One key difficulty is that existing encoders struggle to efficiently fuse low-level features. Additionally, the query selection strategies are not effectively tailored for small objects. To address these challenges, this paper proposes an efficient model, Small Object Detection Transformer (SO-DETR). The model comprises three key components: a dual-domain hybrid encoder, an enhanced query selection mechanism, and a knowledge distillation strategy. The dual-domain hybrid encoder integrates spatial and frequency domains to fuse multi-scale features effectively. This approach enhances the representation of high-resolution features while maintaining relatively low computational overhead. The enhanced query selection mechanism optimizes query initialization by dynamically selecting high-scoring anchor boxes using expanded IoU, thereby improving the allocation of query resources. Furthermore, by incorporating a lightweight backbone network and implementing a knowledge distillation strategy, we develop an efficient detector for small objects. Experimental results on the VisDrone-2019-DET and UAVVaste datasets demonstrate that SO-DETR outperforms existing methods with similar computational demands. The project page is available at this https URL.</li>
</ul>

<h3>Title: High Dynamic Range Modulo Imaging for Robust Object Detection in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Kebin Contreras, Brayan Monroy, Jorge Bacca</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11472">https://arxiv.org/abs/2504.11472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11472">https://arxiv.org/pdf/2504.11472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11472]] High Dynamic Range Modulo Imaging for Robust Object Detection in Autonomous Driving(https://arxiv.org/abs/2504.11472)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Object detection precision is crucial for ensuring the safety and efficacy of autonomous driving systems. The quality of acquired images directly influences the ability of autonomous driving systems to correctly recognize and respond to other vehicles, pedestrians, and obstacles in real-time. However, real environments present extreme variations in lighting, causing saturation problems and resulting in the loss of crucial details for detection. Traditionally, High Dynamic Range (HDR) images have been preferred for their ability to capture a broad spectrum of light intensities, but the need for multiple captures to construct HDR images is inefficient for real-time applications in autonomous vehicles. To address these issues, this work introduces the use of modulo sensors for robust object detection. The modulo sensor allows pixels to `reset/wrap' upon reaching saturation level by acquiring an irradiance encoding image which can then be recovered using unwrapping algorithms. The applied reconstruction techniques enable HDR recovery of color intensity and image details, ensuring better visual quality even under extreme lighting conditions at the cost of extra time. Experiments with the YOLOv10 model demonstrate that images processed using modulo images achieve performance comparable to HDR images and significantly surpass saturated images in terms of object detection accuracy. Moreover, the proposed modulo imaging step combined with HDR image reconstruction is shorter than the time required for conventional HDR image acquisition.</li>
</ul>

<h3>Title: CI-RKM: A Class-Informed Approach to Robust Restricted Kernel Machines</h3>
<ul>
<li><strong>Authors: </strong>Ritik Mishra, Mushir Akhtar, M. Tanveer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11476">https://arxiv.org/abs/2504.11476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11476">https://arxiv.org/pdf/2504.11476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11476]] CI-RKM: A Class-Informed Approach to Robust Restricted Kernel Machines(https://arxiv.org/abs/2504.11476)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Restricted kernel machines (RKMs) represent a versatile and powerful framework within the kernel machine family, leveraging conjugate feature duality to address a wide range of machine learning tasks, including classification, regression, and feature learning. However, their performance can degrade significantly in the presence of noise and outliers, which compromises robustness and predictive accuracy. In this paper, we propose a novel enhancement to the RKM framework by integrating a class-informed weighted function. This weighting mechanism dynamically adjusts the contribution of individual training points based on their proximity to class centers and class-specific characteristics, thereby mitigating the adverse effects of noisy and outlier data. By incorporating weighted conjugate feature duality and leveraging the Schur complement theorem, we introduce the class-informed restricted kernel machine (CI-RKM), a robust extension of the RKM designed to improve generalization and resilience to data imperfections. Experimental evaluations on benchmark datasets demonstrate that the proposed CI-RKM consistently outperforms existing baselines, achieving superior classification accuracy and enhanced robustness against noise and outliers. Our proposed method establishes a significant advancement in the development of kernel-based learning models, addressing a core challenge in the field.</li>
</ul>

<h3>Title: SDIGLM: Leveraging Large Language Models and Multi-Modal Chain of Thought for Structural Damage Identification</h3>
<ul>
<li><strong>Authors: </strong>Yunkai Zhang, Shiyin Wei, Yong Huang, Yawu Su, Shanshan Lu, Hui Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11477">https://arxiv.org/abs/2504.11477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11477">https://arxiv.org/pdf/2504.11477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11477]] SDIGLM: Leveraging Large Language Models and Multi-Modal Chain of Thought for Structural Damage Identification(https://arxiv.org/abs/2504.11477)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Existing computer vision(CV)-based structural damage identification models demonstrate notable accuracy in categorizing and localizing damage. However, these models present several critical limitations that hinder their practical application in civil engineering(CE). Primarily, their ability to recognize damage types remains constrained, preventing comprehensive analysis of the highly varied and complex conditions encountered in real-world CE structures. Second, these models lack linguistic capabilities, rendering them unable to articulate structural damage characteristics through natural language descriptions. With the continuous advancement of artificial intelligence(AI), large multi-modal models(LMMs) have emerged as a transformative solution, enabling the unified encoding and alignment of textual and visual data. These models can autonomously generate detailed descriptive narratives of structural damage while demonstrating robust generalization across diverse scenarios and tasks. This study introduces SDIGLM, an innovative LMM for structural damage identification, developed based on the open-source VisualGLM-6B architecture. To address the challenge of adapting LMMs to the intricate and varied operating conditions in CE, this work integrates a U-Net-based semantic segmentation module to generate defect segmentation maps as visual Chain of Thought(CoT). Additionally, a multi-round dialogue fine-tuning dataset is constructed to enhance logical reasoning, complemented by a language CoT formed through prompt engineering. By leveraging this multi-modal CoT, SDIGLM surpasses general-purpose LMMs in structural damage identification, achieving an accuracy of 95.24% across various infrastructure types. Moreover, the model effectively describes damage characteristics such as hole size, crack direction, and corrosion severity.</li>
</ul>

<h3>Title: LLM-based AI Agent for Sizing of Analog and Mixed Signal Circuit</h3>
<ul>
<li><strong>Authors: </strong>Chang Liu, Emmanuel A. Olowe, Danial Chitnis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11497">https://arxiv.org/abs/2504.11497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11497">https://arxiv.org/pdf/2504.11497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11497]] LLM-based AI Agent for Sizing of Analog and Mixed Signal Circuit(https://arxiv.org/abs/2504.11497)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often involves significant manual effort, especially during the transistor sizing process. While Machine Learning techniques in Electronic Design Automation (EDA) have shown promise in reducing complexity and minimizing human intervention, they still face challenges such as numerous iterations and a lack of knowledge about AMS circuit design. Recently, Large Language Models (LLMs) have demonstrated significant potential across various fields, showing a certain level of knowledge in circuit design and indicating their potential to automate the transistor sizing process. In this work, we propose an LLM-based AI agent for AMS circuit design to assist in the sizing process. By integrating LLMs with external circuit simulation tools and data analysis functions and employing prompt engineering strategies, the agent successfully optimized multiple circuits to achieve target performance metrics. We evaluated the performance of different LLMs to assess their applicability and optimization effectiveness across seven basic circuits, and selected the best-performing model Claude 3.5 Sonnet for further exploration on an operational amplifier, with complementary input stage and class AB output stage. This circuit was evaluated against nine performance metrics, and we conducted experiments under three distinct performance requirement groups. A success rate of up to 60% was achieved for reaching the target requirements. Overall, this work demonstrates the potential of LLMs to improve AMS circuit design.</li>
</ul>

<h3>Title: TransitReID: Transit OD Data Collection with Occlusion-Resistant Dynamic Passenger Re-Identification</h3>
<ul>
<li><strong>Authors: </strong>Kaicong Huang, Talha Azfar, Jack Reilly, Ruimin Ke</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11500">https://arxiv.org/abs/2504.11500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11500">https://arxiv.org/pdf/2504.11500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11500]] TransitReID: Transit OD Data Collection with Occlusion-Resistant Dynamic Passenger Re-Identification(https://arxiv.org/abs/2504.11500)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>Transit Origin-Destination (OD) data are essential for transit planning, particularly in route optimization and demand-responsive paratransit systems. Traditional methods, such as manual surveys, are costly and inefficient, while Bluetooth and WiFi-based approaches require passengers to carry specific devices, limiting data coverage. On the other hand, most transit vehicles are equipped with onboard cameras for surveillance, offering an opportunity to repurpose them for edge-based OD data collection through visual person re-identification (ReID). However, such approaches face significant challenges, including severe occlusion and viewpoint variations in transit environments, which greatly reduce matching accuracy and hinder their adoption. Moreover, designing effective algorithms that can operate efficiently on edge devices remains an open challenge. To address these challenges, we propose TransitReID, a novel framework for individual-level transit OD data collection. TransitReID consists of two key components: (1) An occlusion-robust ReID algorithm featuring a variational autoencoder guided region-attention mechanism that adaptively focuses on visible body regions through reconstruction loss-optimized weight allocation; and (2) a Hierarchical Storage and Dynamic Matching (HSDM) mechanism specifically designed for efficient and robust transit OD matching which balances storage, speed, and accuracy. Additionally, a multi-threaded design supports near real-time operation on edge devices, which also ensuring privacy protection. We also introduce a ReID dataset tailored for complex bus environments to address the lack of relevant training data. Experimental results demonstrate that TransitReID achieves state-of-the-art performance in ReID tasks, with an accuracy of approximately 90\% in bus route simulations.</li>
</ul>

<h3>Title: Cross-cultural Deployment of Autonomous Vehicles Using Data-light Inverse Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Hongliang Lu, Shuqi Shen, Junjie Yang, Chao Lu, Xinhu Zheng, Hai Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11506">https://arxiv.org/abs/2504.11506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11506">https://arxiv.org/pdf/2504.11506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11506]] Cross-cultural Deployment of Autonomous Vehicles Using Data-light Inverse Reinforcement Learning(https://arxiv.org/abs/2504.11506)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>More than the adherence to specific traffic regulations, driving culture touches upon a more implicit part - an informal, conventional, collective behavioral pattern followed by drivers - that varies across countries, regions, and even cities. Such cultural divergence has become one of the biggest challenges in deploying autonomous vehicles (AVs) across diverse regions today. The current emergence of data-driven methods has shown a potential solution to enable culture-compatible driving through learning from data, but what if some underdeveloped regions cannot provide sufficient local data to inform driving culture? This issue is particularly significant for a broader global AV market. Here, we propose a cross-cultural deployment scheme for AVs, called data-light inverse reinforcement learning, designed to re-calibrate culture-specific AVs and assimilate them into other cultures. First, we report the divergence in driving cultures through a comprehensive comparative analysis of naturalistic driving datasets on highways from three countries: Germany, China, and the USA. Then, we demonstrate the effectiveness of our scheme by testing the expeditious cross-cultural deployment across these three countries, with cumulative testing mileage of over 56084 km. The performance is particularly advantageous when cross-cultural deployment is carried out without affluent local data. Results show that we can reduce the dependence on local data by a margin of 98.67% at best. This study is expected to bring a broader, fairer AV global market, particularly in those regions that lack enough local data to develop culture-compatible AVs.</li>
</ul>

<h3>Title: Reward Distance Comparisons Under Transition Sparsity</h3>
<ul>
<li><strong>Authors: </strong>Clement Nyanhongo, Bruno Miranda Henrique, Eugene Santos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11508">https://arxiv.org/abs/2504.11508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11508">https://arxiv.org/pdf/2504.11508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11508]] Reward Distance Comparisons Under Transition Sparsity(https://arxiv.org/abs/2504.11508)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reward comparisons are vital for evaluating differences in agent behaviors induced by a set of reward functions. Most conventional techniques utilize the input reward functions to learn optimized policies, which are then used to compare agent behaviors. However, learning these policies can be computationally expensive and can also raise safety concerns. Direct reward comparison techniques obviate policy learning but suffer from transition sparsity, where only a small subset of transitions are sampled due to data collection challenges and feasibility constraints. Existing state-of-the-art direct reward comparison methods are ill-suited for these sparse conditions since they require high transition coverage, where the majority of transitions from a given coverage distribution are sampled. When this requirement is not satisfied, a distribution mismatch between sampled and expected transitions can occur, leading to significant errors. This paper introduces the Sparsity Resilient Reward Distance (SRRD) pseudometric, designed to eliminate the need for high transition coverage by accommodating diverse sample distributions, which are common under transition sparsity. We provide theoretical justification for SRRD's robustness and conduct experiments to demonstrate its practical efficacy across multiple domains.</li>
</ul>

<h3>Title: Position Paper: Rethinking Privacy in RL for Sequential Decision-making in the Age of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Flint Xiaofeng Fan, Cheston Tan, Roger Wattenhofer, Yew-Soon Ong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11511">https://arxiv.org/abs/2504.11511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11511">https://arxiv.org/pdf/2504.11511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11511]] Position Paper: Rethinking Privacy in RL for Sequential Decision-making in the Age of LLMs(https://arxiv.org/abs/2504.11511)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The rise of reinforcement learning (RL) in critical real-world applications demands a fundamental rethinking of privacy in AI systems. Traditional privacy frameworks, designed to protect isolated data points, fall short for sequential decision-making systems where sensitive information emerges from temporal patterns, behavioral strategies, and collaborative dynamics. Modern RL paradigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in large language models (LLMs), exacerbate these challenges by introducing complex, interactive, and context-dependent learning environments that traditional methods do not address. In this position paper, we argue for a new privacy paradigm built on four core principles: multi-scale protection, behavioral pattern protection, collaborative privacy preservation, and context-aware adaptation. These principles expose inherent tensions between privacy, utility, and interpretability that must be navigated as RL systems become more pervasive in high-stakes domains like healthcare, autonomous vehicles, and decision support systems powered by LLMs. To tackle these challenges, we call for the development of new theoretical frameworks, practical mechanisms, and rigorous evaluation methodologies that collectively enable effective privacy protection in sequential decision-making systems.</li>
</ul>

<h3>Title: Multi-output Classification Framework and Frequency Layer Normalization for Compound Fault Diagnosis in Motor</h3>
<ul>
<li><strong>Authors: </strong>Wonjun Yi, Yong-Hwa Park</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11513">https://arxiv.org/abs/2504.11513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11513">https://arxiv.org/pdf/2504.11513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11513]] Multi-output Classification Framework and Frequency Layer Normalization for Compound Fault Diagnosis in Motor(https://arxiv.org/abs/2504.11513)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This work introduces a multi-output classification (MOC) framework designed for domain adaptation in fault diagnosis, particularly under partially labeled (PL) target domain scenarios and compound fault conditions in rotating machinery. Unlike traditional multi-class classification (MCC) methods that treat each fault combination as a distinct class, the proposed approach independently estimates the severity of each fault type, improving both interpretability and diagnostic accuracy. The model incorporates multi-kernel maximum mean discrepancy (MK-MMD) and entropy minimization (EM) losses to facilitate feature transfer from the source to the target domain. In addition, frequency layer normalization (FLN) is applied to preserve structural properties in the frequency domain, which are strongly influenced by system dynamics and are often stationary with respect to changes in rpm. Evaluations across six domain adaptation cases with PL data demonstrate that MOC outperforms baseline models in macro F1 score. Moreover, MOC consistently achieves better classification performance for individual fault types, and FLN shows superior adaptability compared to other normalization techniques.</li>
</ul>

<h3>Title: Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Kangsheng Wang, Chengwei Ye, Huanzhen Zhang, Linuo Xu, Shuyan Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11515">https://arxiv.org/abs/2504.11515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11515">https://arxiv.org/pdf/2504.11515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11515]] Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment(https://arxiv.org/abs/2504.11515)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Predicting personality traits automatically has become a challenging problem in computer vision. This paper introduces an innovative multimodal feature learning framework for personality analysis in short video clips. For visual processing, we construct a facial graph and design a Geo-based two-stream network incorporating an attention mechanism, leveraging both Graph Convolutional Networks (GCN) and Convolutional Neural Networks (CNN) to capture static facial expressions. Additionally, ResNet18 and VGGFace networks are employed to extract global scene and facial appearance features at the frame level. To capture dynamic temporal information, we integrate a BiGRU with a temporal attention module for extracting salient frame representations. To enhance the model's robustness, we incorporate the VGGish CNN for audio-based features and XLM-Roberta for text-based features. Finally, a multimodal channel attention mechanism is introduced to integrate different modalities, and a Multi-Layer Perceptron (MLP) regression model is used to predict personality traits. Experimental results confirm that our proposed framework surpasses existing state-of-the-art approaches in performance.</li>
</ul>

<h3>Title: ConvShareViT: Enhancing Vision Transformers with Convolutional Attention Mechanisms for Free-Space Optical Accelerators</h3>
<ul>
<li><strong>Authors: </strong>Riad Ibadulla, Thomas M. Chen, Constantino Carlos Reyes-Aldasoro</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11517">https://arxiv.org/abs/2504.11517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11517">https://arxiv.org/pdf/2504.11517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11517]] ConvShareViT: Enhancing Vision Transformers with Convolutional Attention Mechanisms for Free-Space Optical Accelerators(https://arxiv.org/abs/2504.11517)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper introduces ConvShareViT, a novel deep learning architecture that adapts Vision Transformers (ViTs) to the 4f free-space optical system. ConvShareViT replaces linear layers in multi-head self-attention (MHSA) and Multilayer Perceptrons (MLPs) with a depthwise convolutional layer with shared weights across input channels. Through the development of ConvShareViT, the behaviour of convolutions within MHSA and their effectiveness in learning the attention mechanism were analysed systematically. Experimental results demonstrate that certain configurations, particularly those using valid-padded shared convolutions, can successfully learn attention, achieving comparable attention scores to those obtained with standard ViTs. However, other configurations, such as those using same-padded convolutions, show limitations in attention learning and operate like regular CNNs rather than transformer models. ConvShareViT architectures are specifically optimised for the 4f optical system, which takes advantage of the parallelism and high-resolution capabilities of optical systems. Results demonstrate that ConvShareViT can theoretically achieve up to 3.04 times faster inference than GPU-based systems. This potential acceleration makes ConvShareViT an attractive candidate for future optical deep learning applications and proves that our ViT (ConvShareViT) can be employed using only the convolution operation, via the necessary optimisation of the ViT to balance performance and complexity.</li>
</ul>

<h3>Title: LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation</h3>
<ul>
<li><strong>Authors: </strong>Wei-Jer Chang, Wei Zhan, Masayoshi Tomizuka, Manmohan Chandraker, Francesco Pittaluga</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11521">https://arxiv.org/abs/2504.11521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11521">https://arxiv.org/pdf/2504.11521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11521]] LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation(https://arxiv.org/abs/2504.11521)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Evaluating autonomous vehicles with controllability enables scalable testing in counterfactual or structured settings, enhancing both efficiency and safety. We introduce LangTraj, a language-conditioned scene-diffusion model that simulates the joint behavior of all agents in traffic scenarios. By conditioning on natural language inputs, LangTraj provides flexible and intuitive control over interactive behaviors, generating nuanced and realistic scenarios. Unlike prior approaches that depend on domain-specific guidance functions, LangTraj incorporates language conditioning during training, facilitating more intuitive traffic simulation control. We propose a novel closed-loop training strategy for diffusion models, explicitly tailored to enhance stability and realism during closed-loop simulation. To support language-conditioned simulation, we develop Inter-Drive, a large-scale dataset with diverse and interactive labels for training language-conditioned diffusion models. Our dataset is built upon a scalable pipeline for annotating agent-agent interactions and single-agent behaviors, ensuring rich and varied supervision. Validated on the Waymo Motion Dataset, LangTraj demonstrates strong performance in realism, language controllability, and language-conditioned safety-critical simulation, establishing a new paradigm for flexible and scalable autonomous vehicle testing.</li>
</ul>

<h3>Title: MULTI-LF: A Unified Continuous Learning Framework for Real-Time DDoS Detection in Multi-Environment Networks</h3>
<ul>
<li><strong>Authors: </strong>Furqan Rustam, Islam Obaidat, Anca Delia Jurcut</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11575">https://arxiv.org/abs/2504.11575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11575">https://arxiv.org/pdf/2504.11575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11575]] MULTI-LF: A Unified Continuous Learning Framework for Real-Time DDoS Detection in Multi-Environment Networks(https://arxiv.org/abs/2504.11575)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Detecting Distributed Denial of Service (DDoS) attacks in Multi-Environment (M-En) networks presents significant challenges due to diverse malicious traffic patterns and the evolving nature of cyber threats. Existing AI-based detection systems struggle to adapt to new attack strategies and lack real-time attack detection capabilities with high accuracy and efficiency. This study proposes an online, continuous learning methodology for DDoS detection in M-En networks, enabling continuous model updates and real-time adaptation to emerging threats, including zero-day attacks. First, we develop a unique M-En network dataset by setting up a realistic, real-time simulation using the NS-3 tool, incorporating both victim and bot devices. DDoS attacks with varying packet sizes are simulated using the DDoSim application across IoT and traditional IP-based environments under M-En network criteria. Our approach employs a multi-level framework (MULTI-LF) featuring two machine learning models: a lightweight Model 1 (M1) trained on a selective, critical packet dataset for fast and efficient initial detection, and a more complex, highly accurate Model 2 (M2) trained on extensive data. When M1 exhibits low confidence in its predictions, the decision is escalated to M2 for verification and potential fine-tuning of M1 using insights from M2. If both models demonstrate low confidence, the system flags the incident for human intervention, facilitating model updates with human-verified categories to enhance adaptability to unseen attack patterns. We validate the MULTI-LF through real-world simulations, demonstrating superior classification accuracy of 0.999 and low prediction latency of 0.866 seconds compared to established baselines. Furthermore, we evaluate performance in terms of memory usage (3.632 MB) and CPU utilization (10.05%) in real-time scenarios.</li>
</ul>

<h3>Title: Towards a Universal Vibration Analysis Dataset: A Framework for Transfer Learning in Predictive Maintenance and Structural Health Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Mert Sehri, Igor Varejão, Zehui Hua, Vitor Bonella, Adriano Santos, Francisco de Assis Boldt, Patrick Dumond, Flavio Miguel Varejão</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11581">https://arxiv.org/abs/2504.11581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11581">https://arxiv.org/pdf/2504.11581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11581]] Towards a Universal Vibration Analysis Dataset: A Framework for Transfer Learning in Predictive Maintenance and Structural Health Monitoring(https://arxiv.org/abs/2504.11581)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>ImageNet has become a reputable resource for transfer learning, allowing the development of efficient ML models with reduced training time and data requirements. However, vibration analysis in predictive maintenance, structural health monitoring, and fault diagnosis, lacks a comparable large-scale, annotated dataset to facilitate similar advancements. To address this, a dataset framework is proposed that begins with bearing vibration data as an initial step towards creating a universal dataset for vibration-based spectrogram analysis for all machinery. The initial framework includes a collection of bearing vibration signals from various publicly available datasets. To demonstrate the advantages of this framework, experiments were conducted using a deep learning architecture, showing improvements in model performance when pre-trained on bearing vibration data and fine-tuned on a smaller, domain-specific dataset. These findings highlight the potential to parallel the success of ImageNet in visual computing but for vibration analysis. For future work, this research will include a broader range of vibration signals from multiple types of machinery, emphasizing spectrogram-based representations of the data. Each sample will be labeled according to machinery type, operational status, and the presence or type of faults, ensuring its utility for supervised and unsupervised learning tasks. Additionally, a framework for data preprocessing, feature extraction, and model training specific to vibration data will be developed. This framework will standardize methodologies across the research community, allowing for collaboration and accelerating progress in predictive maintenance, structural health monitoring, and related fields. By mirroring the success of ImageNet in visual computing, this dataset has the potential to improve the development of intelligent systems in industrial applications.</li>
</ul>

<h3>Title: Deep Learning Approaches for Medical Imaging Under Varying Degrees of Label Availability: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Siteng Ma, Honghui Du, Yu An, Jing Wang, Qinqin Wang, Haochang Wu, Aonghus Lawlor, Ruihai Dong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11588">https://arxiv.org/abs/2504.11588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11588">https://arxiv.org/pdf/2504.11588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11588]] Deep Learning Approaches for Medical Imaging Under Varying Degrees of Label Availability: A Comprehensive Survey(https://arxiv.org/abs/2504.11588)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Deep learning has achieved significant breakthroughs in medical imaging, but these advancements are often dependent on large, well-annotated datasets. However, obtaining such datasets poses a significant challenge, as it requires time-consuming and labor-intensive annotations from medical experts. Consequently, there is growing interest in learning paradigms such as incomplete, inexact, and absent supervision, which are designed to operate under limited, inexact, or missing labels. This survey categorizes and reviews the evolving research in these areas, analyzing around 600 notable contributions since 2018. It covers tasks such as image classification, segmentation, and detection across various medical application areas, including but not limited to brain, chest, and cardiac imaging. We attempt to establish the relationships among existing research studies in related areas. We provide formal definitions of different learning paradigms and offer a comprehensive summary and interpretation of various learning mechanisms and strategies, aiding readers in better understanding the current research landscape and ideas. We also discuss potential future research challenges.</li>
</ul>

<h3>Title: Measuring Computational Universality of Fully Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Xue, Xin Xin, Wei Zhang, Mengxin Zheng, Qianqian Song, Minxuan Zhou, Yushun Dong, Dongjie Wang, Xun Chen, Jiafeng Xie, Liqiang Wang, David Mohaisen, Hongyi Wu, Qian Lou</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11604">https://arxiv.org/abs/2504.11604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11604">https://arxiv.org/pdf/2504.11604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11604]] Measuring Computational Universality of Fully Homomorphic Encryption(https://arxiv.org/abs/2504.11604)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Many real-world applications, such as machine learning and graph analytics, involve combinations of linear and non-linear operations. As these applications increasingly handle sensitive data, there is a significant demand for privacy-preserving computation techniques capable of efficiently supporting both types of operations-a property we define as "computational universality." Fully Homomorphic Encryption (FHE) has emerged as a powerful approach to perform computations directly on encrypted data. In this paper, we systematically evaluate and measure whether existing FHE methods achieve computational universality or primarily favor either linear or non-linear operations, especially in non-interactive settings. We evaluate FHE universality in three stages. First, we categorize existing FHE methods into ten distinct approaches and analyze their theoretical complexities, selecting the three most promising universal candidates. Next, we perform measurements on representative workloads that combine linear and non-linear operations in various sequences, assessing performance across different bit lengths and with SIMD parallelization enabled or disabled. Finally, we empirically evaluate these candidates on five real-world, privacy-sensitive applications, where each involving arithmetic (linear) and comparison-like (non-linear) operations. Our findings indicate significant overheads in current universal FHE solutions, with efficiency strongly influenced by SIMD parallelism, word-wise versus bit-wise operations, and the trade-off between approximate and exact computations. Additionally, our analysis provides practical guidance to help practitioners select the most suitable universal FHE schemes and algorithms based on specific application requirements.</li>
</ul>

<h3>Title: Making Acoustic Side-Channel Attacks on Noisy Keyboards Viable with LLM-Assisted Spectrograms' "Typo" Correction</h3>
<ul>
<li><strong>Authors: </strong>Seyyed Ali Ayati, Jin Hyun Park, Yichen Cai, Marcus Botacin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11622">https://arxiv.org/abs/2504.11622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11622">https://arxiv.org/pdf/2504.11622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11622]] Making Acoustic Side-Channel Attacks on Noisy Keyboards Viable with LLM-Assisted Spectrograms' "Typo" Correction(https://arxiv.org/abs/2504.11622)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>The large integration of microphones into devices increases the opportunities for Acoustic Side-Channel Attacks (ASCAs), as these can be used to capture keystrokes' audio signals that might reveal sensitive information. However, the current State-Of-The-Art (SOTA) models for ASCAs, including Convolutional Neural Networks (CNNs) and hybrid models, such as CoAtNet, still exhibit limited robustness under realistic noisy conditions. Solving this problem requires either: (i) an increased model's capacity to infer contextual information from longer sequences, allowing the model to learn that an initially noisily typed word is the same as a futurely collected non-noisy word, or (ii) an approach to fix misidentified information from the contexts, as one does not type random words, but the ones that best fit the conversation context. In this paper, we demonstrate that both strategies are viable and complementary solutions for making ASCAs practical. We observed that no existing solution leverages advanced transformer architectures' power for these tasks and propose that: (i) Visual Transformers (VTs) are the candidate solutions for capturing long-term contextual information and (ii) transformer-powered Large Language Models (LLMs) are the candidate solutions to fix the ``typos'' (mispredictions) the model might make. Thus, we here present the first-of-its-kind approach that integrates VTs and LLMs for ASCAs. We first show that VTs achieve SOTA performance in classifying keystrokes when compared to the previous CNN benchmark. Second, we demonstrate that LLMs can mitigate the impact of real-world noise. Evaluations on the natural sentences revealed that: (i) incorporating LLMs (e.g., GPT-4o) in our ASCA pipeline boosts the performance of error-correction tasks; and (ii) the comparable performance can be attained by a lightweight, fine-tuned smaller LLM (67 times smaller than GPT-4o), using...</li>
</ul>

<h3>Title: Chypnosis: Stealthy Secret Extraction using Undervolting-based Static Side-channel Attacks</h3>
<ul>
<li><strong>Authors: </strong>Kyle Mitard, Saleh Khalaj Monfared, Fatemeh Khojasteh Dana, Shahin Tajik</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11633">https://arxiv.org/abs/2504.11633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11633">https://arxiv.org/pdf/2504.11633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11633]] Chypnosis: Stealthy Secret Extraction using Undervolting-based Static Side-channel Attacks(https://arxiv.org/abs/2504.11633)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack, steal, extraction</a></li>
<li><strong>Abstract: </strong>There is a growing class of static physical side-channel attacks that allow adversaries to extract secrets by probing the persistent state of a circuit. Techniques such as laser logic state imaging (LLSI), impedance analysis (IA), and static power analysis fall into this category. These attacks require that the targeted data remain constant for a specific duration, which often necessitates halting the circuit's clock. Some methods additionally rely on modulating the chip's supply voltage to probe the circuit. However, tampering with the clock or voltage is typically assumed to be detectable, as secure chips often deploy sensors that erase sensitive data upon detecting such anomalies. Furthermore, many secure devices use internal clock sources, making external clock control infeasible. In this work, we introduce a novel class of static side-channel attacks, called Chypnosis, that enables adversaries to freeze a chip's internal clock by inducing a hibernation state via rapid undervolting, and then extracting secrets using static side-channels. We demonstrate that, by rapidly dropping a chip's voltage below the standard nominal levels, the attacker can bypass the clock and voltage sensors and put the chip in a so-called brownout condition, in which the chip's transistors stop switching, but volatile memories (e.g., Flip-flops and SRAMs) still retain their data. We test our attack on AMD FPGAs by putting them into hibernation. We show that not only are all clock sources deactivated, but various clock and voltage sensors also fail to detect the tamper event. Afterward, we present the successful recovery of secret bits from a hibernated chip using two static attacks, namely, LLSI and IA. Finally, we discuss potential countermeasures which could be integrated into future designs.</li>
</ul>

<h3>Title: DamageCAT: A Deep Learning Transformer Framework for Typology-Based Post-Disaster Building Damage Categorization</h3>
<ul>
<li><strong>Authors: </strong>Yiming Xiao, Ali Mostafavi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11637">https://arxiv.org/abs/2504.11637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11637">https://arxiv.org/pdf/2504.11637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11637]] DamageCAT: A Deep Learning Transformer Framework for Typology-Based Post-Disaster Building Damage Categorization(https://arxiv.org/abs/2504.11637)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Natural disasters increasingly threaten communities worldwide, creating an urgent need for rapid, reliable building damage assessment to guide emergency response and recovery efforts. Current methods typically classify damage in binary (damaged/undamaged) or ordinal severity terms, limiting their practical utility. In fact, the determination of damage typology is crucial for response and recovery efforts. To address this important gap, this paper introduces DamageCAT, a novel framework that provides typology-based categorical damage descriptions rather than simple severity ratings. Accordingly, this study presents two key contributions: (1) the BD-TypoSAT dataset containing satellite image triplets (pre-disaster, post-disaster, and damage masks) from Hurricane Ida with four damage categories (partial roof damage, total roof damage, partial structural collapse, and total structural collapse), and (2) a hierarchical U-Net-based transformer architecture that effectively processes pre-post disaster image pairs to identify and categorize building damage. Despite significant class imbalances in the training data, our model achieved robust performance with overall metrics of 0.7921 Intersection over Union (IoU) and 0.8835 F1 scores across all categories. The model's capability to recognize intricate damage typology in less common categories is especially remarkable. The DamageCAT framework advances automated damage assessment by providing actionable, typological information that better supports disaster response decision-making and resource allocation compared to traditional severity-based approaches.</li>
</ul>

<h3>Title: Achieving Tighter Finite-Time Rates for Heterogeneous Federated Stochastic Approximation under Markovian Sampling</h3>
<ul>
<li><strong>Authors: </strong>Feng Zhu, Aritra Mitra, Robert W. Heath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11645">https://arxiv.org/abs/2504.11645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11645">https://arxiv.org/pdf/2504.11645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11645]] Achieving Tighter Finite-Time Rates for Heterogeneous Federated Stochastic Approximation under Markovian Sampling(https://arxiv.org/abs/2504.11645)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, fair</a></li>
<li><strong>Abstract: </strong>Motivated by collaborative reinforcement learning (RL) and optimization with time-correlated data, we study a generic federated stochastic approximation problem involving $M$ agents, where each agent is characterized by an agent-specific (potentially nonlinear) local operator. The goal is for the agents to communicate intermittently via a server to find the root of the average of the agents' local operators. The generality of our setting stems from allowing for (i) Markovian data at each agent and (ii) heterogeneity in the roots of the agents' local operators. The limited recent work that has accounted for both these features in a federated setting fails to guarantee convergence to the desired point or to show any benefit of collaboration; furthermore, they rely on projection steps in their algorithms to guarantee bounded iterates. Our work overcomes each of these limitations. We develop a novel algorithm titled \texttt{FedHSA}, and prove that it guarantees convergence to the correct point, while enjoying an $M$-fold linear speedup in sample-complexity due to collaboration. To our knowledge, \emph{this is the first finite-time result of its kind}, and establishing it (without relying on a projection step) entails a fairly intricate argument that accounts for the interplay between complex temporal correlations due to Markovian sampling, multiple local steps to save communication, and the drift-effects induced by heterogeneous local operators. Our results have implications for a broad class of heterogeneous federated RL problems (e.g., policy evaluation and control) with function approximation, where the agents' Markov decision processes can differ in their probability transition kernels and reward functions.</li>
</ul>

<h3>Title: 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Zhang, Yang Sui, Shaochen Zhong, Vipin Chaudhary, Xia Hu, Anshumali Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11651">https://arxiv.org/abs/2504.11651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11651">https://arxiv.org/pdf/2504.11651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11651]] 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float(https://arxiv.org/abs/2504.11651)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression framework that reduces LLM size by 30% while preserving outputs that are bit-for-bit identical to the original model. DFloat11 is motivated by the low entropy in the BFloat16 weight representation of LLMs, which reveals significant inefficiency in existing storage format. By applying entropy coding, DFloat11 assigns dynamic-length encodings to weights based on frequency, achieving near information-optimal compression without any loss of precision. To facilitate efficient inference with dynamic-length encodings, we develop a custom GPU kernel for fast online decompression. Our design incorporates the following: (i) decomposition of memory-intensive lookup tables (LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for coordinating thread read/write positions using lightweight auxiliary variables, and (iii) transformer-block-level decompression to minimize latency. Experiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3, validates our hypothesis that DFloat11 achieves around 30% model size reduction while preserving bit-for-bit exact outputs. Compared to a potential alternative of offloading parts of an uncompressed model to the CPU to meet memory constraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation. With a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context lengths than uncompressed models. Notably, our method enables lossless inference of Llama-3.1-405B, an 810GB model, on a single node equipped with 8x80GB GPUs. Our code and models are available at this https URL.</li>
</ul>

<h3>Title: Cybersecurity through Entropy Injection: A Paradigm Shift from Reactive Defense to Proactive Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Kush Janani</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11661">https://arxiv.org/abs/2504.11661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11661">https://arxiv.org/pdf/2504.11661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11661]] Cybersecurity through Entropy Injection: A Paradigm Shift from Reactive Defense to Proactive Uncertainty(https://arxiv.org/abs/2504.11661)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Cybersecurity often hinges on unpredictability, with a system's defenses being strongest when sensitive values and behaviors cannot be anticipated by attackers. This paper explores the concept of entropy injection-deliberately infusing randomness into security mechanisms to increase unpredictability and enhance system security. We examine the theoretical foundations of entropy-based security, analyze real-world implementations including Address Space Layout Randomization (ASLR) and Moving Target Defense (MTD) frameworks, evaluate practical challenges in implementation, and compare entropy-based approaches with traditional security methods. Our methodology includes a systematic analysis of entropy's role across various security domains, from cryptographic operations to system-level defenses. Results demonstrate that entropy injection can significantly reduce attack probability, with some implementations showing more than 90% reduction with minimal performance impact. The discussion highlights the trade-offs between security benefits and operational complexity, while identifying future directions for entropy-enhanced security, including integration with artificial intelligence and quantum randomness sources. We conclude that entropy injection represents a paradigm shift from reactive defense to proactive uncertainty management, offering a strategic approach that can fundamentally alter the balance between attackers and defenders in cybersecurity.</li>
</ul>

<h3>Title: Higher-Order Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions</h3>
<ul>
<li><strong>Authors: </strong>Minwoo Kang, Suhong Moon, Seung Hyeong Lee, Ayush Raj, Joseph Suh, David M. Chan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11673">https://arxiv.org/abs/2504.11673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11673">https://arxiv.org/pdf/2504.11673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11673]] Higher-Order Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions(https://arxiv.org/abs/2504.11673)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly capable of simulating human behavior, offering cost-effective ways to estimate user responses during the early phases of survey design. While previous studies have examined whether models can reflect individual opinions or attitudes, we argue that a \emph{higher-order} binding of virtual personas requires successfully approximating not only the opinions of a user as an identified member of a group, but also the nuanced ways in which that user perceives and evaluates those outside the group. In particular, faithfully simulating how humans perceive different social groups is critical for applying LLMs to various political science studies, including timely topics on polarization dynamics, inter-group conflict, and democratic backsliding. To this end, we propose a novel methodology for constructing virtual personas with synthetic user ``backstories" generated as extended, multi-turn interview transcripts. Our generated backstories are longer, rich in detail, and consistent in authentically describing a singular individual, compared to previous methods. We show that virtual personas conditioned on our backstories closely replicate human response distributions (up to an 87\% improvement as measured by Wasserstein Distance) and produce effect sizes that closely match those observed in the original studies. Altogether, our work extends the applicability of LLMs beyond estimating individual self-opinions, enabling their use in a broader range of human studies.</li>
</ul>

<h3>Title: Can GPT tell us why these images are synthesized? Empowering Multimodal Large Language Models for Forensics</h3>
<ul>
<li><strong>Authors: </strong>Yiran He, Yun Cao, Bowen Yang, Zeyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11686">https://arxiv.org/abs/2504.11686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11686">https://arxiv.org/pdf/2504.11686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11686]] Can GPT tell us why these images are synthesized? Empowering Multimodal Large Language Models for Forensics(https://arxiv.org/abs/2504.11686)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The rapid development of generative AI facilitates content creation and makes image manipulation easier and more difficult to detect. While multimodal Large Language Models (LLMs) have encoded rich world knowledge, they are not inherently tailored for combating AI-generated Content (AIGC) and struggle to comprehend local forgery details. In this work, we investigate the application of multimodal LLMs in forgery detection. We propose a framework capable of evaluating image authenticity, localizing tampered regions, providing evidence, and tracing generation methods based on semantic tampering clues. Our method demonstrates that the potential of LLMs in forgery analysis can be effectively unlocked through meticulous prompt engineering and the application of few-shot learning techniques. We conduct qualitative and quantitative experiments and show that GPT4V can achieve an accuracy of 92.1% in Autosplice and 86.3% in LaMa, which is competitive with state-of-the-art AIGC detection methods. We further discuss the limitations of multimodal LLMs in such tasks and propose potential improvements.</li>
</ul>

<h3>Title: Clustering and analysis of user behaviour in blockchain: A case study of Planet IX</h3>
<ul>
<li><strong>Authors: </strong>Dorottya Zelenyanszki, Zhe Hou, Kamanashis Biswas, Vallipuram Muthukkumarasamy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11702">https://arxiv.org/abs/2504.11702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11702">https://arxiv.org/pdf/2504.11702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11702]] Clustering and analysis of user behaviour in blockchain: A case study of Planet IX(https://arxiv.org/abs/2504.11702)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Decentralised applications (dApps) that run on public blockchains have the benefit of trustworthiness and transparency as every activity that happens on the blockchain can be publicly traced through the transaction data. However, this introduces a potential privacy problem as this data can be tracked and analysed, which can reveal user-behaviour information. A user behaviour analysis pipeline was proposed to present how this type of information can be extracted and analysed to identify separate behavioural clusters that can describe how users behave in the game. The pipeline starts with the collection of transaction data, involving smart contracts, that is collected from a blockchain-based game called Planet IX. Both the raw transaction information and the transaction events are considered in the data collection. From this data, separate game actions can be formed and those are leveraged to present how and when the users conducted their in-game activities in the form of user flows. An extended version of these user flows also presents how the Non-Fungible Tokens (NFTs) are being leveraged in the user actions. The latter is given as input for a Graph Neural Network (GNN) model to provide graph embeddings for these flows which then can be leveraged by clustering algorithms to cluster user behaviours into separate behavioural clusters. We benchmark and compare well-known clustering algorithms as a part of the proposed method. The user behaviour clusters were analysed and visualised in a graph format. It was found that behavioural information can be extracted regarding the users that belong to these clusters. Such information can be exploited by malicious users to their advantage. To demonstrate this, a privacy threat model was also presented based on the results that correspond to multiple potentially affected areas.</li>
</ul>

<h3>Title: Progent: Programmable Privilege Control for LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Tianneng Shi, Jingxuan He, Zhun Wang, Linyu Wu, Hongwei Li, Wenbo Guo, Dawn Song</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11703">https://arxiv.org/abs/2504.11703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11703">https://arxiv.org/pdf/2504.11703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11703]] Progent: Programmable Privilege Control for LLM Agents(https://arxiv.org/abs/2504.11703)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>LLM agents are an emerging form of AI systems where large language models (LLMs) serve as the central component, utilizing a diverse set of tools to complete user-assigned tasks. Despite their great potential, LLM agents pose significant security risks. When interacting with the external world, they may encounter malicious commands from attackers, leading to the execution of dangerous actions. A promising way to address this is by enforcing the principle of least privilege: allowing only essential actions for task completion while blocking unnecessary ones. However, achieving this is challenging, as it requires covering diverse agent scenarios while preserving both security and utility. We introduce Progent, the first privilege control mechanism for LLM agents. At its core is a domain-specific language for flexibly expressing privilege control policies applied during agent execution. These policies provide fine-grained constraints over tool calls, deciding when tool calls are permissible and specifying fallbacks if they are not. This enables agent developers and users to craft suitable policies for their specific use cases and enforce them deterministically to guarantee security. Thanks to its modular design, integrating Progent does not alter agent internals and requires only minimal changes to agent implementation, enhancing its practicality and potential for widespread adoption. To automate policy writing, we leverage LLMs to generate policies based on user queries, which are then updated dynamically for improved security and utility. Our extensive evaluation shows that it enables strong security while preserving high utility across three distinct scenarios or benchmarks: AgentDojo, ASB, and AgentPoison. Furthermore, we perform an in-depth analysis, showcasing the effectiveness of its core components and the resilience of its automated policy generation against adaptive attacks.</li>
</ul>

<h3>Title: Learning What NOT to Count</h3>
<ul>
<li><strong>Authors: </strong>Adriano D'Alessandro, Ali Mahdavi-Amiri, Ghassan Hamarneh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11705">https://arxiv.org/abs/2504.11705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11705">https://arxiv.org/pdf/2504.11705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11705]] Learning What NOT to Count(https://arxiv.org/abs/2504.11705)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Few/zero-shot object counting methods reduce the need for extensive annotations but often struggle to distinguish between fine-grained categories, especially when multiple similar objects appear in the same scene. To address this limitation, we propose an annotation-free approach that enables the seamless integration of new fine-grained categories into existing few/zero-shot counting models. By leveraging latent generative models, we synthesize high-quality, category-specific crowded scenes, providing a rich training source for adapting to new categories without manual labeling. Our approach introduces an attention prediction network that identifies fine-grained category boundaries trained using only synthetic pseudo-annotated data. At inference, these fine-grained attention estimates refine the output of existing few/zero-shot counting networks. To benchmark our method, we further introduce the FGTC dataset, a taxonomy-specific fine-grained object counting dataset for natural images. Our method substantially enhances pre-trained state-of-the-art models on fine-grained taxon counting tasks, while using only synthetic data. Code and data to be released upon acceptance.</li>
</ul>

<h3>Title: Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Shahid Muneer, Simon S. Woo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11707">https://arxiv.org/abs/2504.11707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11707">https://arxiv.org/pdf/2504.11707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11707]] Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset(https://arxiv.org/abs/2504.11707)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>In the past years, we have witnessed the remarkable success of Text-to-Image (T2I) models and their widespread use on the web. Extensive research in making T2I models produce hyper-realistic images has led to new concerns, such as generating Not-Safe-For-Work (NSFW) web content and polluting the web society. To help prevent misuse of T2I models and create a safer web environment for users features like NSFW filters and post-hoc security checks are used in these models. However, recent work unveiled how these methods can easily fail to prevent misuse. In particular, adversarial attacks on text and image modalities can easily outplay defensive measures. %Exploiting such leads to the growing concern of preventing adversarial attacks on text and image modalities. Moreover, there is currently no robust multimodal NSFW dataset that includes both prompt and image pairs and adversarial examples. This work proposes a million-scale prompt and image dataset generated using open-source diffusion models. Second, we develop a multimodal defense to distinguish safe and NSFW text and images, which is robust against adversarial attacks and directly alleviates current challenges. Our extensive experiments show that our model performs well against existing SOTA NSFW detection methods in terms of accuracy and recall, drastically reducing the Attack Success Rate (ASR) in multimodal adversarial attack scenarios. Code: this https URL.</li>
</ul>

<h3>Title: Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching</h3>
<ul>
<li><strong>Authors: </strong>Aaron Havens, Benjamin Kurt Miller, Bing Yan, Carles Domingo-Enrich, Anuroop Sriram, Brandon Wood, Daniel Levine, Bin Hu, Brandon Amos, Brian Karrer, Xiang Fu, Guan-Horng Liu, Ricky T. Q. Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11713">https://arxiv.org/abs/2504.11713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11713">https://arxiv.org/pdf/2504.11713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11713]] Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching(https://arxiv.org/abs/2504.11713)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Adjoint Sampling, a highly scalable and efficient algorithm for learning diffusion processes that sample from unnormalized densities, or energy functions. It is the first on-policy approach that allows significantly more gradient updates than the number of energy evaluations and model samples, allowing us to scale to much larger problem settings than previously explored by similar methods. Our framework is theoretically grounded in stochastic optimal control and shares the same theoretical guarantees as Adjoint Matching, being able to train without the need for corrective measures that push samples towards the target distribution. We show how to incorporate key symmetries, as well as periodic boundary conditions, for modeling molecules in both cartesian and torsional coordinates. We demonstrate the effectiveness of our approach through extensive experiments on classical energy functions, and further scale up to neural network-based energy models where we perform amortized conformer generation across many molecular systems. To encourage further research in developing highly scalable sampling methods, we plan to open source these challenging benchmarks, where successful methods can directly impact progress in computational chemistry.</li>
</ul>

<h3>Title: Saga: Capturing Multi-granularity Semantics from Massive Unlabelled IMU Data for User Perception</h3>
<ul>
<li><strong>Authors: </strong>Yunzhe Li, Facheng Hu, Hongzi Zhu, Shifan Zhang, Liang Zhang, Shan Chang, Minyi Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11726">https://arxiv.org/abs/2504.11726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11726">https://arxiv.org/pdf/2504.11726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11726]] Saga: Capturing Multi-granularity Semantics from Massive Unlabelled IMU Data for User Perception(https://arxiv.org/abs/2504.11726)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Inertial measurement units (IMUs), have been prevalently used in a wide range of mobile perception applications such as activity recognition and user authentication, where a large amount of labelled data are normally required to train a satisfactory model. However, it is difficult to label micro-activities in massive IMU data due to the hardness of understanding raw IMU data and the lack of ground truth. In this paper, we propose a novel fine-grained user perception approach, called Saga, which only needs a small amount of labelled IMU data to achieve stunning user perception accuracy. The core idea of Saga is to first pre-train a backbone feature extraction model, utilizing the rich semantic information of different levels embedded in the massive unlabelled IMU data. Meanwhile, for a specific downstream user perception application, Bayesian Optimization is employed to determine the optimal weights for pre-training tasks involving different semantic levels. We implement Saga on five typical mobile phones and evaluate Saga on three typical tasks on three IMU datasets. Results show that when only using about 100 training samples per class, Saga can achieve over 90% accuracy of the full-fledged model trained on over ten thousands training samples with no additional system overhead.</li>
</ul>

<h3>Title: Blockchain Application in Metaverse: A Review</h3>
<ul>
<li><strong>Authors: </strong>Bingquan Jin, Hailu Kuang, Xiaoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11730">https://arxiv.org/abs/2504.11730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11730">https://arxiv.org/pdf/2504.11730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11730]] Blockchain Application in Metaverse: A Review(https://arxiv.org/abs/2504.11730)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>In recent years, the term Metaverse emerged as one of the most compelling concepts, captivating the interest of international companies such as Tencent, ByteDance, Microsoft, and Facebook. These company recognized the Metaverse as a pivotal element for future success and have since made significant investments in this area. The Metaverse is still in its developmental stages, requiring the integration and advancement of various technologies to bring its vision to life. One of the key technologies associated with the Metaverse is blockchain, known for its decentralization, security, trustworthiness, and ability to manage time-series data. These characteristics align perfectly with the ecosystem of the Metaverse, making blockchain foundational for its security and infrastructure. This paper introduces both blockchain and the Metaverse ecosystem while exploring the application of the blockchain within the Metaverse, including decentralization, consensus mechanisms, hash algorithms, timestamping, smart contracts, distributed storage, distributed ledgers, and non-fungible tokens (NFTs) to provide insights for researchers investigating these topics.</li>
</ul>

<h3>Title: EgoExo-Gen: Ego-centric Video Prediction by Watching Exo-centric Videos</h3>
<ul>
<li><strong>Authors: </strong>Jilan Xu, Yifei Huang, Baoqi Pei, Junlin Hou, Qingqiu Li, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11732">https://arxiv.org/abs/2504.11732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11732">https://arxiv.org/pdf/2504.11732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11732]] EgoExo-Gen: Ego-centric Video Prediction by Watching Exo-centric Videos(https://arxiv.org/abs/2504.11732)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating videos in the first-person perspective has broad application prospects in the field of augmented reality and embodied intelligence. In this work, we explore the cross-view video prediction task, where given an exo-centric video, the first frame of the corresponding ego-centric video, and textual instructions, the goal is to generate futur frames of the ego-centric video. Inspired by the notion that hand-object interactions (HOI) in ego-centric videos represent the primary intentions and actions of the current actor, we present EgoExo-Gen that explicitly models the hand-object dynamics for cross-view video prediction. EgoExo-Gen consists of two stages. First, we design a cross-view HOI mask prediction model that anticipates the HOI masks in future ego-frames by modeling the spatio-temporal ego-exo correspondence. Next, we employ a video diffusion model to predict future ego-frames using the first ego-frame and textual instructions, while incorporating the HOI masks as structural guidance to enhance prediction quality. To facilitate training, we develop an automated pipeline to generate pseudo HOI masks for both ego- and exo-videos by exploiting vision foundation models. Extensive experiments demonstrate that our proposed EgoExo-Gen achieves better prediction performance compared to previous video prediction models on the Ego-Exo4D and H2O benchmark datasets, with the HOI masks significantly improving the generation of hands and interactive objects in the ego-centric videos.</li>
</ul>

<h3>Title: WalletProbe: A Testing Framework for Browser-based Cryptocurrency Wallet Extensions</h3>
<ul>
<li><strong>Authors: </strong>Xiaohui Hu, Ningyu He, Haoyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11735">https://arxiv.org/abs/2504.11735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11735">https://arxiv.org/pdf/2504.11735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11735]] WalletProbe: A Testing Framework for Browser-based Cryptocurrency Wallet Extensions(https://arxiv.org/abs/2504.11735)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>Serving as the first touch point for users to the cryptocurrency world, cryptocurrency wallets allow users to manage, receive, and transmit digital assets on blockchain networks and interact with emerging decentralized finance (DeFi) applications. Unfortunately, cryptocurrency wallets have always been the prime targets for attackers, and incidents of wallet breaches have been reported from time to time. Although some recent studies have characterized the vulnerabilities and scams related to wallets, they have generally been characterized in coarse granularity, overlooking potential risks inherent in detailed designs of cryptocurrency wallets, especially from perspectives including user interaction and advanced features. To fill the void, in this paper, we present a fine-grained security analysis on browser-based cryptocurrency wallets. To pinpoint security issues of components in wallets, we design WalletProbe, a mutation-based testing framework based on visual-level oracles. We have identified 13 attack vectors that can be abused by attackers to exploit cryptocurrency wallets and exposed 21 concrete attack strategies. By applying WalletProbe on 39 widely-adopted browser-based wallet extensions, we astonishingly figure out all of them can be abused to steal crypto assets from innocent users. Identified potential attack vectors were reported to wallet developers timely and 26 issues have been patched already. It is, hence, urgent for our community to take action to mitigate threats related to cryptocurrency wallets. We promise to release all code and data to promote the development of the community.</li>
</ul>

<h3>Title: The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Bingjie Gao, Xinyu Gao, Xiaoxue Wu, Yujie Zhou, Yu Qiao, Li Niu, Xinyuan Chen, Yaohui Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11739">https://arxiv.org/abs/2504.11739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11739">https://arxiv.org/pdf/2504.11739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11739]] The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation(https://arxiv.org/abs/2504.11739)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The evolution of Text-to-video (T2V) generative models, trained on large-scale datasets, has been marked by significant progress. However, the sensitivity of T2V generative models to input prompts highlights the critical role of prompt design in influencing generative outcomes. Prior research has predominantly relied on Large Language Models (LLMs) to align user-provided prompts with the distribution of training prompts, albeit without tailored guidance encompassing prompt vocabulary and sentence structure nuances. To this end, we introduce \textbf{RAPO}, a novel \textbf{R}etrieval-\textbf{A}ugmented \textbf{P}rompt \textbf{O}ptimization framework. In order to address potential inaccuracies and ambiguous details generated by LLM-generated prompts. RAPO refines the naive prompts through dual optimization branches, selecting the superior prompt for T2V generation. The first branch augments user prompts with diverse modifiers extracted from a learned relational graph, refining them to align with the format of training prompts via a fine-tuned LLM. Conversely, the second branch rewrites the naive prompt using a pre-trained LLM following a well-defined instruction set. Extensive experiments demonstrate that RAPO can effectively enhance both the static and dynamic dimensions of generated videos, demonstrating the significance of prompt optimization for user-provided prompts. Project website: \href{this https URL}{GitHub}.</li>
</ul>

<h3>Title: From Cyber Threat to Data Shield: Constructing Provably Secure File Erasure with Repurposed Ransomware Cryptography</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Shang, Luning Zhang, Zhongxiang Zheng</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11744">https://arxiv.org/abs/2504.11744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11744">https://arxiv.org/pdf/2504.11744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11744]] From Cyber Threat to Data Shield: Constructing Provably Secure File Erasure with Repurposed Ransomware Cryptography(https://arxiv.org/abs/2504.11744)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>Ransomware has emerged as a persistent cybersecurity threat,leveraging robust encryption schemes that often remain unbroken even after public disclosure of source code. Motivated by the technical resilience of such mechanisms, this paper presents SEER (Secure and Efficient Encryption-based Erasure via Ransomware), a provably secure file destruction system that repurposes ransomware encryption for legitimate data erasure tasks. SEER integrates the triple-encryption design of the Babuk ransomware family, including Curve25519-based key exchange,SHA-256-based key derivation, and the Sosemanuk stream cipher, to construct a layered key management architecture. It tightly couples encryption and key destruction by securely erasing session keys immediately after use. Experimental results on an ESXI platform demonstrate that SEER achieves four orders of magnitude performance improvement over the DoD 5220.22 standard. The proposed system further ensures provable security through both theoretical foundations and practical validation, offering an efficient and resilient solution for the secure destruction of sensitive data.</li>
</ul>

<h3>Title: GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision</h3>
<ul>
<li><strong>Authors: </strong>Zihui Zhang, Yafei Yang, Hongtao Wen, Bo Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11754">https://arxiv.org/abs/2504.11754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11754">https://arxiv.org/pdf/2504.11754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11754]] GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision(https://arxiv.org/abs/2504.11754)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>We study the hard problem of 3D object segmentation in complex point clouds without requiring human labels of 3D scenes for supervision. By relying on the similarity of pretrained 2D features or external signals such as motion to group 3D points as objects, existing unsupervised methods are usually limited to identifying simple objects like cars or their segmented objects are often inferior due to the lack of objectness in pretrained features. In this paper, we propose a new two-stage pipeline called GrabS. The core concept of our method is to learn generative and discriminative object-centric priors as a foundation from object datasets in the first stage, and then design an embodied agent to learn to discover multiple objects by querying against the pretrained generative priors in the second stage. We extensively evaluate our method on two real-world datasets and a newly created synthetic dataset, demonstrating remarkable segmentation performance, clearly surpassing all existing unsupervised methods.</li>
</ul>

<h3>Title: Dynamics and Computational Principles of Echo State Networks: A Mathematical Perspective</h3>
<ul>
<li><strong>Authors: </strong>Pradeep Singh, Ashutosh Kumar, Sutirtha Ghosh, Hrishit B P, Balasubramanian Raman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11757">https://arxiv.org/abs/2504.11757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11757">https://arxiv.org/pdf/2504.11757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11757]] Dynamics and Computational Principles of Echo State Networks: A Mathematical Perspective(https://arxiv.org/abs/2504.11757)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reservoir computing (RC) represents a class of state-space models (SSMs) characterized by a fixed state transition mechanism (the reservoir) and a flexible readout layer that maps from the state space. It is a paradigm of computational dynamical systems that harnesses the transient dynamics of high-dimensional state spaces for efficient processing of temporal data. Rooted in concepts from recurrent neural networks, RC achieves exceptional computational power by decoupling the training of the dynamic reservoir from the linear readout layer, thereby circumventing the complexities of gradient-based optimization. This work presents a systematic exploration of RC, addressing its foundational properties such as the echo state property, fading memory, and reservoir capacity through the lens of dynamical systems theory. We formalize the interplay between input signals and reservoir states, demonstrating the conditions under which reservoirs exhibit stability and expressive power. Further, we delve into the computational trade-offs and robustness characteristics of RC architectures, extending the discussion to their applications in signal processing, time-series prediction, and control systems. The analysis is complemented by theoretical insights into optimization, training methodologies, and scalability, highlighting open challenges and potential directions for advancing the theoretical underpinnings of RC.</li>
</ul>

<h3>Title: TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion</h3>
<ul>
<li><strong>Authors: </strong>Yiran Wang, Jiaqi Li, Chaoyi Hong, Ruibo Li, Liusheng Sun, Xiao Song, Zhe Wang, Zhiguo Cao, Guosheng Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11773">https://arxiv.org/abs/2504.11773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11773">https://arxiv.org/pdf/2504.11773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11773]] TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion(https://arxiv.org/abs/2504.11773)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Radar-Camera depth estimation aims to predict dense and accurate metric depth by fusing input images and Radar data. Model efficiency is crucial for this task in pursuit of real-time processing on autonomous vehicles and robotic platforms. However, due to the sparsity of Radar returns, the prevailing methods adopt multi-stage frameworks with intermediate quasi-dense depth, which are time-consuming and not robust. To address these challenges, we propose TacoDepth, an efficient and accurate Radar-Camera depth estimation model with one-stage fusion. Specifically, the graph-based Radar structure extractor and the pyramid-based Radar fusion module are designed to capture and integrate the graph structures of Radar point clouds, delivering superior model efficiency and robustness without relying on the intermediate depth results. Moreover, TacoDepth can be flexible for different inference modes, providing a better balance of speed and accuracy. Extensive experiments are conducted to demonstrate the efficacy of our method. Compared with the previous state-of-the-art approach, TacoDepth improves depth accuracy and processing speed by 12.8% and 91.8%. Our work provides a new perspective on efficient Radar-Camera depth estimation.</li>
</ul>

<h3>Title: PCDiff: Proactive Control for Ownership Protection in Diffusion Models with Watermark Compatibility</h3>
<ul>
<li><strong>Authors: </strong>Keke Gai, Ziyue Shen, Jing Yu, Liehuang Zhu, Qi Wu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11774">https://arxiv.org/abs/2504.11774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11774">https://arxiv.org/pdf/2504.11774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11774]] PCDiff: Proactive Control for Ownership Protection in Diffusion Models with Watermark Compatibility(https://arxiv.org/abs/2504.11774)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>With the growing demand for protecting the intellectual property (IP) of text-to-image diffusion models, we propose PCDiff -- a proactive access control framework that redefines model authorization by regulating generation quality. At its core, PCDIFF integrates a trainable fuser module and hierarchical authentication layers into the decoder architecture, ensuring that only users with valid encrypted credentials can generate high-fidelity images. In the absence of valid keys, the system deliberately degrades output quality, effectively preventing unauthorized this http URL, while the primary mechanism enforces active access control through architectural intervention, its decoupled design retains compatibility with existing watermarking techniques. This satisfies the need of model owners to actively control model ownership while preserving the traceability capabilities provided by traditional watermarking this http URL experimental evaluations confirm a strong dependency between credential verification and image quality across various attack scenarios. Moreover, when combined with typical post-processing operations, PCDIFF demonstrates powerful performance alongside conventional watermarking methods. This work shifts the paradigm from passive detection to proactive enforcement of authorization, laying the groundwork for IP management of diffusion models.</li>
</ul>

<h3>Title: Bridging the Semantic Gaps: Improving Medical VQA Consistency with LLM-Augmented Question Sets</h3>
<ul>
<li><strong>Authors: </strong>Yongpei Ma, Pengyu Wang, Adam Dunn, Usman Naseem, Jinman Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11777">https://arxiv.org/abs/2504.11777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11777">https://arxiv.org/pdf/2504.11777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11777]] Bridging the Semantic Gaps: Improving Medical VQA Consistency with LLM-Augmented Question Sets(https://arxiv.org/abs/2504.11777)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Medical Visual Question Answering (MVQA) systems can interpret medical images in response to natural language queries. However, linguistic variability in question phrasing often undermines the consistency of these systems. To address this challenge, we propose a Semantically Equivalent Question Augmentation (SEQA) framework, which leverages large language models (LLMs) to generate diverse yet semantically equivalent rephrasings of questions. Specifically, this approach enriches linguistic diversity while preserving semantic meaning. We further introduce an evaluation metric, Total Agreement Rate with Semantically Equivalent Input and Correct Answer (TAR-SC), which assesses a model's capability to generate consistent and correct responses to semantically equivalent linguistic variations. In addition, we also propose three other diversity metrics - average number of QA items per image (ANQI), average number of questions per image with the same answer (ANQA), and average number of open-ended questions per image with the same semantics (ANQS). Using the SEQA framework, we augmented the benchmarked MVQA public datasets of SLAKE, VQA-RAD, and PathVQA. As a result, all three datasets achieved significant improvements by incorporating more semantically equivalent questions: ANQI increased by an average of 86.1, ANQA by 85.1, and ANQS by 46. Subsequent experiments evaluate three MVQA models (M2I2, MUMC, and BiomedGPT) under both zero-shot and fine-tuning settings on the enhanced datasets. Experimental results in MVQA datasets show that fine-tuned models achieve an average accuracy improvement of 19.35%, while our proposed TAR-SC metric shows an average improvement of 11. 61%, indicating a substantial enhancement in model consistency.</li>
</ul>

<h3>Title: Multimodal Spatio-temporal Graph Learning for Alignment-free RGBT Video Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Qishun Wang, Zhengzheng Tu, Chenglong Li, Bo Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11779">https://arxiv.org/abs/2504.11779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11779">https://arxiv.org/pdf/2504.11779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11779]] Multimodal Spatio-temporal Graph Learning for Alignment-free RGBT Video Object Detection(https://arxiv.org/abs/2504.11779)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>RGB-Thermal Video Object Detection (RGBT VOD) can address the limitation of traditional RGB-based VOD in challenging lighting conditions, making it more practical and effective in many applications. However, similar to most RGBT fusion tasks, it still mainly relies on manually aligned multimodal image pairs. In this paper, we propose a novel Multimodal Spatio-temporal Graph learning Network (MSGNet) for alignment-free RGBT VOD problem by leveraging the robust graph representation learning model. Specifically, we first design an Adaptive Partitioning Layer (APL) to estimate the corresponding regions of the Thermal image within the RGB image (high-resolution), achieving a preliminary inexact alignment. Then, we introduce the Spatial Sparse Graph Learning Module (S-SGLM) which employs a sparse information passing mechanism on the estimated inexact alignment to achieve reliable information interaction between different modalities. Moreover, to fully exploit the temporal cues for RGBT VOD problem, we introduce Hybrid Structured Temporal Modeling (HSTM), which involves a Temporal Sparse Graph Learning Module (T-SGLM) and Temporal Star Block (TSB). T-SGLM aims to filter out some redundant information between adjacent frames by employing the sparse aggregation mechanism on the temporal graph. Meanwhile, TSB is dedicated to achieving the complementary learning of local spatial relationships. Extensive comparative experiments conducted on both the aligned dataset VT-VOD50 and the unaligned dataset UVT-VOD2024 demonstrate the effectiveness and superiority of our proposed method. Our project will be made available on our website for free public access.</li>
</ul>

<h3>Title: The Digital Cybersecurity Expert: How Far Have We Come?</h3>
<ul>
<li><strong>Authors: </strong>Dawei Wang, Geng Zhou, Xianglong Li, Yu Bai, Li Chen, Ting Qin, Jian Sun, Dan Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11783">https://arxiv.org/abs/2504.11783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11783">https://arxiv.org/pdf/2504.11783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11783]] The Digital Cybersecurity Expert: How Far Have We Come?(https://arxiv.org/abs/2504.11783)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, large language model</a></li>
<li><strong>Abstract: </strong>The increasing deployment of large language models (LLMs) in the cybersecurity domain underscores the need for effective model selection and evaluation. However, traditional evaluation methods often overlook specific cybersecurity knowledge gaps that contribute to performance limitations. To address this, we develop CSEBenchmark, a fine-grained cybersecurity evaluation framework based on 345 knowledge points expected of cybersecurity experts. Drawing from cognitive science, these points are categorized into factual, conceptual, and procedural types, enabling the design of 11,050 tailored multiple-choice questions. We evaluate 12 popular LLMs on CSEBenchmark and find that even the best-performing model achieves only 85.42% overall accuracy, with particular knowledge gaps in the use of specialized tools and uncommon commands. Different LLMs have unique knowledge gaps. Even large models from the same family may perform poorly on knowledge points where smaller models excel. By identifying and addressing specific knowledge gaps in each LLM, we achieve up to an 84% improvement in correcting previously incorrect predictions across three existing benchmarks for two cybersecurity tasks. Furthermore, our assessment of each LLM's knowledge alignment with specific cybersecurity roles reveals that different models align better with different roles, such as GPT-4o for the Google Senior Intelligence Analyst and Deepseek-V3 for the Amazon Privacy Engineer. These findings underscore the importance of aligning LLM selection with the specific knowledge requirements of different cybersecurity roles for optimal performance.</li>
</ul>

<h3>Title: Enhancing Web Agents with Explicit Rollback Mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Zhisong Zhang, Tianqing Fang, Kaixin Ma, Wenhao Yu, Hongming Zhang, Haitao Mi, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11788">https://arxiv.org/abs/2504.11788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11788">https://arxiv.org/pdf/2504.11788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11788]] Enhancing Web Agents with Explicit Rollback Mechanisms(https://arxiv.org/abs/2504.11788)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With recent advancements in large language models, web agents have been greatly improved. However, dealing with complex and dynamic web environments requires more advanced planning and search abilities. Previous studies usually adopt a greedy one-way search strategy, which may struggle to recover from erroneous states. In this work, we enhance web agents with an explicit rollback mechanism, enabling the agent to revert back to a previous state in its navigation trajectory. This mechanism gives the model the flexibility to directly control the search process, leading to an effective and efficient web navigation method. We conduct experiments on two live web navigation benchmarks with zero-shot and fine-tuning settings. The results demonstrate the effectiveness of our proposed approach.</li>
</ul>

<h3>Title: Selective Attention Federated Learning: Improving Privacy and Efficiency for Clinical Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Yue Li, Lihong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11793">https://arxiv.org/abs/2504.11793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11793">https://arxiv.org/pdf/2504.11793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11793]] Selective Attention Federated Learning: Improving Privacy and Efficiency for Clinical Text Classification(https://arxiv.org/abs/2504.11793)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction, federate, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) faces major challenges regarding communication overhead and model privacy when training large language models (LLMs), especially in healthcare applications. To address these, we introduce Selective Attention Federated Learning (SAFL), a novel approach that dynamically fine-tunes only those transformer layers identified as attention-critical. By employing attention patterns to determine layer importance, SAFL significantly reduces communication bandwidth and enhances differential privacy resilience. Evaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and MIMIC-III discharge summaries) demonstrate that SAFL achieves competitive performance with centralized models while substantially improving communication efficiency and privacy preservation.</li>
</ul>

<h3>Title: Neighbor-Based Feature and Index Enhancement for Person Re-Identification</h3>
<ul>
<li><strong>Authors: </strong>Chao Yuan, Tianyi Zhang, Guanglin Niu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11798">https://arxiv.org/abs/2504.11798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11798">https://arxiv.org/pdf/2504.11798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11798]] Neighbor-Based Feature and Index Enhancement for Person Re-Identification(https://arxiv.org/abs/2504.11798)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Person re-identification (Re-ID) aims to match the same pedestrian in a large gallery with different cameras and views. Enhancing the robustness of the extracted feature representations is a main challenge in Re-ID. Existing methods usually improve feature representation by improving model architecture, but most methods ignore the potential contextual information, which limits the effectiveness of feature representation and retrieval performance. Neighborhood information, especially the potential information of multi-order neighborhoods, can effectively enrich feature expression and improve retrieval accuracy, but this has not been fully explored in existing research. Therefore, we propose a novel model DMON-ARO that leverages latent neighborhood information to enhance both feature representation and index performance. Our approach is built on two complementary modules: Dynamic Multi-Order Neighbor Modeling (DMON) and Asymmetric Relationship Optimization (ARO). The DMON module dynamically aggregates multi-order neighbor relationships, allowing it to capture richer contextual information and enhance feature representation through adaptive neighborhood modeling. Meanwhile, ARO refines the distance matrix by optimizing query-to-gallery relationships, improving the index accuracy. Extensive experiments on three benchmark datasets demonstrate that our approach achieves performance improvements against baseline models, which illustrate the effectiveness of our model. Specifically, our model demonstrates improvements in Rank-1 accuracy and mAP. Moreover, this method can also be directly extended to other re-identification tasks.</li>
</ul>

<h3>Title: Advanced MST3 Encryption scheme based on generalized Suzuki 2-groups</h3>
<ul>
<li><strong>Authors: </strong>Gennady Khalimov, Yevgen Kotukh</a></li>
<li><strong>Subjects: </strong>cs.CR, math.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11804">https://arxiv.org/abs/2504.11804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11804">https://arxiv.org/pdf/2504.11804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11804]] Advanced MST3 Encryption scheme based on generalized Suzuki 2-groups(https://arxiv.org/abs/2504.11804)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>This article presents a method for enhancing the encryption algorithm in the MST3 cryptosystem for generalized Suzuki 2-groups. The conventional MST cryptosystem based on Suzuki groups utilizes logarithmic signatures (LS) restricted to the center of the group, resulting in an expansive array of logarithmic signatures. We propose an encryption scheme based on multi-parameter non-commutative groups, specifically selecting multi-parameter generalized Suzuki 2-groups as the group construction framework. In our approach, the logarithmic signature extends across the entire group, with cipher security dependent on the group order. This design enables the development of encryption optimized for implementation efficiency determined by logarithmic signature size while maintaining robust security through appropriate key sizes and the finite field of group representation. The primary innovation in our encryption implementation lies in the sequential de-encapsulation of keys from ciphertext using logarithmic signatures and associated keys. The security evaluation of the cipher relies on attack complexity analysis, which is quantified through comprehensive key enumeration methodologies.</li>
</ul>

<h3>Title: Federated Spectral Graph Transformers Meet Neural Ordinary Differential Equations for Non-IID Graphs</h3>
<ul>
<li><strong>Authors: </strong>Kishan Gurumurthy, Himanshu Pal, Charu Sharma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11808">https://arxiv.org/abs/2504.11808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11808">https://arxiv.org/pdf/2504.11808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11808]] Federated Spectral Graph Transformers Meet Neural Ordinary Differential Equations for Non-IID Graphs(https://arxiv.org/abs/2504.11808)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, transformer</a></li>
<li><strong>Abstract: </strong>Graph Neural Network (GNN) research is rapidly advancing due to GNNs' capacity to learn distributed representations from graph-structured data. However, centralizing large volumes of real-world graph data for GNN training is often impractical due to privacy concerns, regulatory restrictions, and commercial competition. Federated learning (FL), a distributed learning paradigm, offers a solution by preserving data privacy with collaborative model training. Despite progress in training huge vision and language models, federated learning for GNNs remains underexplored. To address this challenge, we present a novel method for federated learning on GNNs based on spectral GNNs equipped with neural ordinary differential equations (ODE) for better information capture, showing promising results across both homophilic and heterophilic graphs. Our approach effectively handles non-Independent and Identically Distributed (non-IID) data, while also achieving performance comparable to existing methods that only operate on IID data. It is designed to be privacy-preserving and bandwidth-optimized, making it suitable for real-world applications such as social network analysis, recommendation systems, and fraud detection, which often involve complex, non-IID, and heterophilic graph structures. Our results in the area of federated learning on non-IID heterophilic graphs demonstrate significant improvements, while also achieving better performance on homophilic graphs. This work highlights the potential of federated learning in diverse and challenging graph settings. Open-source code available on GitHub (this https URL).</li>
</ul>

<h3>Title: Efficient and Adaptive Simultaneous Speech Translation with Fully Unidirectional Architecture</h3>
<ul>
<li><strong>Authors: </strong>Biao Fu, Donglei Yu, Minpeng Liao, Chengxi Li, Yidong Chen, Kai Fan, Xiaodong Shi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11809">https://arxiv.org/abs/2504.11809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11809">https://arxiv.org/pdf/2504.11809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11809]] Efficient and Adaptive Simultaneous Speech Translation with Fully Unidirectional Architecture(https://arxiv.org/abs/2504.11809)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Simultaneous speech translation (SimulST) produces translations incrementally while processing partial speech input. Although large language models (LLMs) have showcased strong capabilities in offline translation tasks, applying them to SimulST poses notable challenges. Existing LLM-based SimulST approaches either incur significant computational overhead due to repeated encoding of bidirectional speech encoder, or they depend on a fixed read/write policy, limiting the efficiency and performance. In this work, we introduce Efficient and Adaptive Simultaneous Speech Translation (EASiST) with fully unidirectional architecture, including both speech encoder and LLM. EASiST includes a multi-latency data curation strategy to generate semantically aligned SimulST training samples and redefines SimulST as an interleaved generation task with explicit read/write tokens. To facilitate adaptive inference, we incorporate a lightweight policy head that dynamically predicts read/write actions. Additionally, we employ a multi-stage training strategy to align speech-text modalities and optimize both translation and policy behavior. Experiments on the MuST-C En$\rightarrow$De and En$\rightarrow$Es datasets demonstrate that EASiST offers superior latency-quality trade-offs compared to several strong baselines.</li>
</ul>

<h3>Title: Real-World Depth Recovery via Structure Uncertainty Modeling and Inaccurate GT Depth Fitting</h3>
<ul>
<li><strong>Authors: </strong>Delong Suzhang, Meng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11820">https://arxiv.org/abs/2504.11820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11820">https://arxiv.org/pdf/2504.11820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11820]] Real-World Depth Recovery via Structure Uncertainty Modeling and Inaccurate GT Depth Fitting(https://arxiv.org/abs/2504.11820)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The low-quality structure in raw depth maps is prevalent in real-world RGB-D datasets, which makes real-world depth recovery a critical task in recent years. However, the lack of paired raw-ground truth (raw-GT) data in the real world poses challenges for generalized depth recovery. Existing methods insufficiently consider the diversity of structure misalignment in raw depth maps, which leads to poor generalization in real-world depth recovery. Notably, random structure misalignments are not limited to raw depth data but also affect GT depth in real-world datasets. In the proposed method, we tackle the generalization problem from both input and output perspectives. For input, we enrich the diversity of structure misalignment in raw depth maps by designing a new raw depth generation pipeline, which helps the network avoid overfitting to a specific condition. Furthermore, a structure uncertainty module is designed to explicitly identify the misaligned structure for input raw depth maps to better generalize in unseen scenarios. Notably the well-trained depth foundation model (DFM) can help the structure uncertainty module estimate the structure uncertainty better. For output, a robust feature alignment module is designed to precisely align with the accurate structure of RGB images avoiding the interference of inaccurate GT depth. Extensive experiments on multiple datasets demonstrate the proposed method achieves competitive accuracy and generalization capabilities across various challenging raw depth maps.</li>
</ul>

<h3>Title: Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Julia Kreutzer, Eleftheria Briakou, Sweta Agrawal, Marzieh Fadaee, Kocmi Tom</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11829">https://arxiv.org/abs/2504.11829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11829">https://arxiv.org/pdf/2504.11829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11829]] Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation(https://arxiv.org/abs/2504.11829)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Generation capabilities and language coverage of multilingual large language models (mLLMs) are advancing rapidly. However, evaluation practices for generative abilities of mLLMs are still lacking comprehensiveness, scientific rigor, and consistent adoption across research labs, which undermines their potential to meaningfully guide mLLM development. We draw parallels with machine translation (MT) evaluation, a field that faced similar challenges and has, over decades, developed transparent reporting standards and reliable evaluations for multilingual generative models. Through targeted experiments across key stages of the generative evaluation pipeline, we demonstrate how best practices from MT evaluation can deepen the understanding of quality differences between models. Additionally, we identify essential components for robust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are rigorously assessed. We distill these insights into a checklist of actionable recommendations for mLLM research and development.</li>
</ul>

<h3>Title: Emergence of Computational Structure in a Neural Network Physics Simulator</h3>
<ul>
<li><strong>Authors: </strong>Rohan Hitchcock, Gary W. Delaney, Jonathan H. Manton, Richard Scalzo, Jingge Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11830">https://arxiv.org/abs/2504.11830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11830">https://arxiv.org/pdf/2504.11830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11830]] Emergence of Computational Structure in a Neural Network Physics Simulator(https://arxiv.org/abs/2504.11830)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Neural networks often have identifiable computational structures - components of the network which perform an interpretable algorithm or task - but the mechanisms by which these emerge and the best methods for detecting these structures are not well understood. In this paper we investigate the emergence of computational structure in a transformer-like model trained to simulate the physics of a particle system, where the transformer's attention mechanism is used to transfer information between particles. We show that (a) structures emerge in the attention heads of the transformer which learn to detect particle collisions, (b) the emergence of these structures is associated to degenerate geometry in the loss landscape, and (c) the dynamics of this emergence follows a power law. This suggests that these components are governed by a degenerate "effective potential". These results have implications for the convergence time of computational structure within neural networks and suggest that the emergence of computational structure can be detected by studying the dynamics of network components.</li>
</ul>

<h3>Title: Support is All You Need for Certified VAE Training</h3>
<ul>
<li><strong>Authors: </strong>Changming Xu, Debangshu Banerjee, Deepak Vasisht, Gagandeep Singh</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11831">https://arxiv.org/abs/2504.11831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11831">https://arxiv.org/pdf/2504.11831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11831]] Support is All You Need for Certified VAE Training(https://arxiv.org/abs/2504.11831)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Variational Autoencoders (VAEs) have become increasingly popular and deployed in safety-critical applications. In such applications, we want to give certified probabilistic guarantees on performance under adversarial attacks. We propose a novel method, CIVET, for certified training of VAEs. CIVET depends on the key insight that we can bound worst-case VAE error by bounding the error on carefully chosen support sets at the latent layer. We show this point mathematically and present a novel training algorithm utilizing this insight. We show in an extensive evaluation across different datasets (in both the wireless and vision application areas), architectures, and perturbation magnitudes that our method outperforms SOTA methods achieving good standard performance with strong robustness guarantees.</li>
</ul>

<h3>Title: Could Thinking Multilingually Empower LLM Reasoning?</h3>
<ul>
<li><strong>Authors: </strong>Changjiang Gao, Xu Huang, Wenhao Zhu, Shujian Huang, Lei Li, Fei Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11833">https://arxiv.org/abs/2504.11833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11833">https://arxiv.org/pdf/2504.11833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11833]] Could Thinking Multilingually Empower LLM Reasoning?(https://arxiv.org/abs/2504.11833)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Previous work indicates that large language models exhibit a significant "English bias", i.e. they often perform better when tasks are presented in English. Interestingly, we have observed that using certain other languages in reasoning tasks can yield better performance than English. However, this phenomenon remains under-explored. In this paper, we explore the upper bound of harnessing multilingualism in reasoning tasks, suggesting that multilingual reasoning promises significantly (by nearly 10 Acc@$k$ points) and robustly (tolerance for variations in translation quality and language choice) higher upper bounds than English-only reasoning. Besides analyzing the reason behind the upper bound and challenges in reaching it, we also find that common answer selection methods cannot achieve this upper bound, due to their limitations and biases. These insights could pave the way for future research aimed at fully harnessing the potential of multilingual reasoning in LLMs.</li>
</ul>

<h3>Title: FiSMiness: A Finite State Machine Based Paradigm for Emotional Support Conversations</h3>
<ul>
<li><strong>Authors: </strong>Yue Zhao, Qingqing Gu, Xiaoyu Wang, Teng Chen, Zhonglin Jiang, Yong Chen, Luo Ji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11837">https://arxiv.org/abs/2504.11837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11837">https://arxiv.org/pdf/2504.11837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11837]] FiSMiness: A Finite State Machine Based Paradigm for Emotional Support Conversations(https://arxiv.org/abs/2504.11837)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Finite State Machine (FSM) on LLMs, and propose a framework called FiSMiness. Our framework allows a single LLM to bootstrap the planning during ESC, and self-reason the seeker's emotion, support strategy and the final response upon each conversational turn. Substantial experiments on ESC datasets suggest that FiSMiness outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and external-assisted methods, even those with many more parameters.</li>
</ul>

<h3>Title: ACE: Attentional Concept Erasure in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Finn Carter</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11850">https://arxiv.org/abs/2504.11850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11850">https://arxiv.org/pdf/2504.11850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11850]] ACE: Attentional Concept Erasure in Diffusion Models(https://arxiv.org/abs/2504.11850)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Large text-to-image diffusion models have demonstrated remarkable image synthesis capabilities, but their indiscriminate training on Internet-scale data has led to learned concepts that enable harmful, copyrighted, or otherwise undesirable content generation. We address the task of concept erasure in diffusion models, i.e., removing a specified concept from a pre-trained model such that prompting the concept (or related synonyms) no longer yields its depiction, while preserving the model's ability to generate other content. We propose a novel method, Attentional Concept Erasure (ACE), that integrates a closed-form attention manipulation with lightweight fine-tuning. Theoretically, we formulate concept erasure as aligning the model's conditional distribution on the target concept with a neutral distribution. Our approach identifies and nullifies concept-specific latent directions in the cross-attention modules via a gated low-rank adaptation, followed by adversarially augmented fine-tuning to ensure thorough erasure of the concept and its synonyms. Empirically, we demonstrate on multiple benchmarks, including object classes, celebrity faces, explicit content, and artistic styles, that ACE achieves state-of-the-art concept removal efficacy and robustness. Compared to prior methods, ACE better balances generality (erasing concept and related terms) and specificity (preserving unrelated content), scales to dozens of concepts, and is efficient, requiring only a few seconds of adaptation per concept. We will release our code to facilitate safer deployment of diffusion models.</li>
</ul>

<h3>Title: Cross-Frequency Collaborative Training Network and Dataset for Semi-supervised First Molar Root Canal Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhenhuan Zhou, Yuchen Zhang, Along He, Peng Wang, Xueshuo Xie, Tao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11856">https://arxiv.org/abs/2504.11856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11856">https://arxiv.org/pdf/2504.11856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11856]] Cross-Frequency Collaborative Training Network and Dataset for Semi-supervised First Molar Root Canal Segmentation(https://arxiv.org/abs/2504.11856)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Root canal (RC) treatment is a highly delicate and technically complex procedure in clinical practice, heavily influenced by the clinicians' experience and subjective judgment. Deep learning has made significant advancements in the field of computer-aided diagnosis (CAD) because it can provide more objective and accurate diagnostic results. However, its application in RC treatment is still relatively rare, mainly due to the lack of public datasets in this field. To address this issue, in this paper, we established a First Molar Root Canal segmentation dataset called FMRC-2025. Additionally, to alleviate the workload of manual annotation for dentists and fully leverage the unlabeled data, we designed a Cross-Frequency Collaborative training semi-supervised learning (SSL) Network called CFC-Net. It consists of two components: (1) Cross-Frequency Collaborative Mean Teacher (CFC-MT), which introduces two specialized students (SS) and one comprehensive teacher (CT) for collaborative multi-frequency training. The CT and SS are trained on different frequency components while fully integrating multi-frequency knowledge through cross and full frequency consistency supervisions. (2) Uncertainty-guided Cross-Frequency Mix (UCF-Mix) mechanism enables the network to generate high-confidence pseudo-labels while learning to integrate multi-frequency information and maintaining the structural integrity of the targets. Extensive experiments on FMRC-2025 and three public dental datasets demonstrate that CFC-MT is effective for RC segmentation and can also exhibit strong generalizability on other dental segmentation tasks, outperforming state-of-the-art SSL medical image segmentation methods. Codes and dataset will be released.</li>
</ul>

<h3>Title: Synthetic Data for Blood Vessel Network Extraction</h3>
<ul>
<li><strong>Authors: </strong>Joël Mathys, Andreas Plesner, Jorel Elmiger, Roger Wattenhofer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11858">https://arxiv.org/abs/2504.11858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11858">https://arxiv.org/pdf/2504.11858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11858]] Synthetic Data for Blood Vessel Network Extraction(https://arxiv.org/abs/2504.11858)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Blood vessel networks in the brain play a crucial role in stroke research, where understanding their topology is essential for analyzing blood flow dynamics. However, extracting detailed topological vessel network information from microscopy data remains a significant challenge, mainly due to the scarcity of labeled training data and the need for high topological accuracy. This work combines synthetic data generation with deep learning to automatically extract vessel networks as graphs from volumetric microscopy data. To combat data scarcity, we introduce a comprehensive pipeline for generating large-scale synthetic datasets that mirror the characteristics of real vessel networks. Our three-stage approach progresses from abstract graph generation through vessel mask creation to realistic medical image synthesis, incorporating biological constraints and imaging artifacts at each stage. Using this synthetic data, we develop a two-stage deep learning pipeline of 3D U-Net-based models for node detection and edge prediction. Fine-tuning on real microscopy data shows promising adaptation, improving edge prediction F1 scores from 0.496 to 0.626 by training on merely 5 manually labeled samples. These results suggest that automated vessel network extraction is becoming practically feasible, opening new possibilities for large-scale vascular analysis in stroke research.</li>
</ul>

<h3>Title: From Data Behavior to Code Analysis: A Multimodal Study on Security and Privacy Challenges in Blockchain-Based DApp</h3>
<ul>
<li><strong>Authors: </strong>Haoyang Sun, Yishun Wang, Xiaoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11860">https://arxiv.org/abs/2504.11860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11860">https://arxiv.org/pdf/2504.11860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11860]] From Data Behavior to Code Analysis: A Multimodal Study on Security and Privacy Challenges in Blockchain-Based DApp(https://arxiv.org/abs/2504.11860)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>The recent proliferation of blockchain-based decentralized applications (DApp) has catalyzed transformative advancements in distributed systems, with extensive deployments observed across financial, entertainment, media, and cybersecurity domains. These trustless architectures, characterized by their decentralized nature and elimination of third-party intermediaries, have garnered substantial institutional attention. Consequently, the escalating security challenges confronting DApp demand rigorous scholarly investigation. This study initiates with a systematic analysis of behavioral patterns derived from empirical DApp datasets, establishing foundational insights for subsequent methodological developments. The principal security vulnerabilities in Ethereum-based smart contracts developed via Solidity are then critically examined. Specifically, reentrancy vulnerability attacks are addressed by formally representing contract logic using highly expressive code fragments. This enables precise source code-level detection via bidirectional long short-term memory networks with attention mechanisms (BLSTM-ATT). Regarding privacy preservation challenges, contemporary solutions are evaluated through dual analytical lenses: identity privacy preservation and transaction anonymity enhancement, while proposing future research trajectories in cryptographic obfuscation techniques.</li>
</ul>

<h3>Title: MDHP-Net: Detecting an Emerging Time-exciting Threat in IVN</h3>
<ul>
<li><strong>Authors: </strong>Qi Liu, Yanchen Liu, Ruifeng Li, Chenhong Cao, Yufeng Li, Xingyu Li, Peng Wang, Runhan Feng, Shiyang Bu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11867">https://arxiv.org/abs/2504.11867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11867">https://arxiv.org/pdf/2504.11867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11867]] MDHP-Net: Detecting an Emerging Time-exciting Threat in IVN(https://arxiv.org/abs/2504.11867)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>The integration of intelligent and connected technologies in modern vehicles, while offering enhanced functionalities through Electronic Control Unit (ECU) and interfaces like OBD-II and telematics, also exposes the vehicle's in-vehicle network (IVN) to potential cyberattacks. Unlike prior work, we identify a new time-exciting threat model against IVN. These attacks inject malicious messages that exhibit a time-exciting effect, gradually manipulating network traffic to disrupt vehicle operations and compromise safety-critical functions. We systematically analyze the characteristics of the threat: dynamism, time-exciting impact, and low prior knowledge dependency. To validate its practicality, we replicate the attack on a real Advanced Driver Assistance System via Controller Area Network (CAN), exploiting Unified Diagnostic Service vulnerabilities and proposing four attack strategies. While CAN's integrity checks mitigate attacks, Ethernet migration (e.g., DoIP/SOME/IP) introduces new surfaces. We further investigate the feasibility of time-exciting threat under SOME/IP. To detect time-exciting threat, we introduce MDHP-Net, leveraging Multi-Dimentional Hawkes Process (MDHP) and temporal and message-wise feature extracting structures. Meanwhile, to estimate MDHP parameters, we developed the first GPU-optimized gradient descent solver for MDHP (MDHP-GDS). These modules significantly improves the detection rate under time-exciting attacks in multi-ECU IVN system. To address data scarcity, we release STEIA9, the first open-source dataset for time-exciting attacks, covering 9 Ethernet-based attack scenarios. Extensive experiments on STEIA9 (9 attack scenarios) show MDHP-Net outperforms 3 baselines, confirming attack feasibility and detection efficacy.</li>
</ul>

<h3>Title: A Category-Fragment Segmentation Framework for Pelvic Fracture Segmentation in X-ray Images</h3>
<ul>
<li><strong>Authors: </strong>Daiqi Liu, Fuxin Fan, Andreas Maier</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11872">https://arxiv.org/abs/2504.11872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11872">https://arxiv.org/pdf/2504.11872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11872]] A Category-Fragment Segmentation Framework for Pelvic Fracture Segmentation in X-ray Images(https://arxiv.org/abs/2504.11872)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Pelvic fractures, often caused by high-impact trauma, frequently require surgical intervention. Imaging techniques such as CT and 2D X-ray imaging are used to transfer the surgical plan to the operating room through image registration, enabling quick intraoperative adjustments. Specifically, segmenting pelvic fractures from 2D X-ray imaging can assist in accurately positioning bone fragments and guiding the placement of screws or metal plates. In this study, we propose a novel deep learning-based category and fragment segmentation (CFS) framework for the automatic segmentation of pelvic bone fragments in 2D X-ray images. The framework consists of three consecutive steps: category segmentation, fragment segmentation, and post-processing. Our best model achieves an IoU of 0.91 for anatomical structures and 0.78 for fracture segmentation. Results demonstrate that the CFS framework is effective and accurate.</li>
</ul>

<h3>Title: Benchmarking Mutual Information-based Loss Functions in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Sarang S, Harsh D. Chothani, Qilei Li, Ahmed M. Abdelmoniem, Arnab K. Paul</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11877">https://arxiv.org/abs/2504.11877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11877">https://arxiv.org/pdf/2504.11877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11877]] Benchmarking Mutual Information-based Loss Functions in Federated Learning(https://arxiv.org/abs/2504.11877)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate, fair</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has attracted considerable interest due to growing privacy concerns and regulations like the General Data Protection Regulation (GDPR), which stresses the importance of privacy-preserving and fair machine learning approaches. In FL, model training takes place on decentralized data, so as to allow clients to upload a locally trained model and receive a globally aggregated model without exposing sensitive information. However, challenges related to fairness-such as biases, uneven performance among clients, and the "free rider" issue complicates its adoption. In this paper, we examine the use of Mutual Information (MI)-based loss functions to address these concerns. MI has proven to be a powerful method for measuring dependencies between variables and optimizing deep learning models. By leveraging MI to extract essential features and minimize biases, we aim to improve both the fairness and effectiveness of FL systems. Through extensive benchmarking, we assess the impact of MI-based losses in reducing disparities among clients while enhancing the overall performance of FL.</li>
</ul>

<h3>Title: CAGS: Open-Vocabulary 3D Scene Understanding with Context-Aware Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Wei Sun, Yanzhao Zhou, Jianbin Jiao, Yuan Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11893">https://arxiv.org/abs/2504.11893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11893">https://arxiv.org/pdf/2504.11893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11893]] CAGS: Open-Vocabulary 3D Scene Understanding with Context-Aware Gaussian Splatting(https://arxiv.org/abs/2504.11893)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Open-vocabulary 3D scene understanding is crucial for applications requiring natural language-driven spatial interpretation, such as robotics and augmented reality. While 3D Gaussian Splatting (3DGS) offers a powerful representation for scene reconstruction, integrating it with open-vocabulary frameworks reveals a key challenge: cross-view granularity inconsistency. This issue, stemming from 2D segmentation methods like SAM, results in inconsistent object segmentations across views (e.g., a "coffee set" segmented as a single entity in one view but as "cup + coffee + spoon" in another). Existing 3DGS-based methods often rely on isolated per-Gaussian feature learning, neglecting the spatial context needed for cohesive object reasoning, leading to fragmented representations. We propose Context-Aware Gaussian Splatting (CAGS), a novel framework that addresses this challenge by incorporating spatial context into 3DGS. CAGS constructs local graphs to propagate contextual features across Gaussians, reducing noise from inconsistent granularity, employs mask-centric contrastive learning to smooth SAM-derived features across views, and leverages a precomputation strategy to reduce computational cost by precomputing neighborhood relationships, enabling efficient training in large-scale scenes. By integrating spatial context, CAGS significantly improves 3D instance segmentation and reduces fragmentation errors on datasets like LERF-OVS and ScanNet, enabling robust language-guided 3D scene understanding.</li>
</ul>

<h3>Title: Learning Physics-Informed Color-Aware Transforms for Low-Light Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Xingxing Yang, Jie Chen, Zaifeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11896">https://arxiv.org/abs/2504.11896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11896">https://arxiv.org/pdf/2504.11896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11896]] Learning Physics-Informed Color-Aware Transforms for Low-Light Image Enhancement(https://arxiv.org/abs/2504.11896)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Image decomposition offers deep insights into the imaging factors of visual data and significantly enhances various advanced computer vision tasks. In this work, we introduce a novel approach to low-light image enhancement based on decomposed physics-informed priors. Existing methods that directly map low-light to normal-light images in the sRGB color space suffer from inconsistent color predictions and high sensitivity to spectral power distribution (SPD) variations, resulting in unstable performance under diverse lighting conditions. To address these challenges, we introduce a Physics-informed Color-aware Transform (PiCat), a learning-based framework that converts low-light images from the sRGB color space into deep illumination-invariant descriptors via our proposed Color-aware Transform (CAT). This transformation enables robust handling of complex lighting and SPD variations. Complementing this, we propose the Content-Noise Decomposition Network (CNDN), which refines the descriptor distributions to better align with well-lit conditions by mitigating noise and other distortions, thereby effectively restoring content representations to low-light images. The CAT and the CNDN collectively act as a physical prior, guiding the transformation process from low-light to normal-light domains. Our proposed PiCat framework demonstrates superior performance compared to state-of-the-art methods across five benchmark datasets.</li>
</ul>

<h3>Title: Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection</h3>
<ul>
<li><strong>Authors: </strong>Kabir Ahuja, Melanie Sclar, Yulia Tsvetkov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11900">https://arxiv.org/abs/2504.11900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11900">https://arxiv.org/pdf/2504.11900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11900]] Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection(https://arxiv.org/abs/2504.11900)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Stories are a fundamental aspect of human experience. Engaging deeply with stories and spotting plot holes -- inconsistencies in a storyline that break the internal logic or rules of a story's world -- requires nuanced reasoning skills, including tracking entities and events and their interplay, abstract thinking, pragmatic narrative understanding, commonsense and social reasoning, and theory of mind. As Large Language Models (LLMs) increasingly generate, interpret, and modify text, rigorously assessing their narrative consistency and deeper language understanding becomes critical. However, existing benchmarks focus mainly on surface-level comprehension. In this work, we propose plot hole detection in stories as a proxy to evaluate language understanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel algorithm to controllably and carefully synthesize plot holes in human-written stories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot hole detection abilities in stories -- FlawedFictions -- , which is robust to contamination, with human filtering ensuring high quality. We find that state-of-the-art LLMs struggle in accurately solving FlawedFictions regardless of the reasoning effort allowed, with performance significantly degrading as story length increases. Finally, we show that LLM-based story summarization and story generation are prone to introducing plot holes, with more than 50% and 100% increases in plot hole detection rates with respect to human-written originals.</li>
</ul>

<h3>Title: FedCanon: Non-Convex Composite Federated Learning with Efficient Proximal Operation on Heterogeneous Data</h3>
<ul>
<li><strong>Authors: </strong>Yuan Zhou, Jiachen Zhong, Xinli Shi, Guanghui Wen, Xinghuo Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11903">https://arxiv.org/abs/2504.11903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11903">https://arxiv.org/pdf/2504.11903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11903]] FedCanon: Non-Convex Composite Federated Learning with Efficient Proximal Operation on Heterogeneous Data(https://arxiv.org/abs/2504.11903)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Composite federated learning offers a general framework for solving machine learning problems with additional regularization terms. However, many existing methods require clients to perform multiple proximal operations to handle non-smooth terms and their performance are often susceptible to data heterogeneity. To overcome these limitations, we propose a novel composite federated learning algorithm called \textbf{FedCanon}, designed to solve the optimization problems comprising a possibly non-convex loss function and a weakly convex, potentially non-smooth regularization term. By decoupling proximal mappings from local updates, FedCanon requires only a single proximal evaluation on the server per iteration, thereby reducing the overall proximal computation cost. It also introduces control variables that incorporate global gradient information into client updates, which helps mitigate the effects of data heterogeneity. Theoretical analysis demonstrates that FedCanon achieves sublinear convergence rates under general non-convex settings and linear convergence under the Polyak-Łojasiewicz condition, without relying on bounded heterogeneity assumptions. Experiments demonstrate that FedCanon outperforms the state-of-the-art methods in terms of both accuracy and computational efficiency, particularly under heterogeneous data distributions.</li>
</ul>

<h3>Title: AnomalyR1: A GRPO-based End-to-end MLLM for Industrial Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Chao, Jie Liu, Jie Tang, Gangshan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11914">https://arxiv.org/abs/2504.11914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11914">https://arxiv.org/pdf/2504.11914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11914]] AnomalyR1: A GRPO-based End-to-end MLLM for Industrial Anomaly Detection(https://arxiv.org/abs/2504.11914)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Industrial Anomaly Detection (IAD) poses a formidable challenge due to the scarcity of defective samples, making it imperative to deploy models capable of robust generalization to detect unseen anomalies effectively. Traditional approaches, often constrained by hand-crafted features or domain-specific expert models, struggle to address this limitation, underscoring the need for a paradigm shift. We introduce AnomalyR1, a pioneering framework that leverages VLM-R1, a Multimodal Large Language Model (MLLM) renowned for its exceptional generalization and interpretability, to revolutionize IAD. By integrating MLLM with Group Relative Policy Optimization (GRPO), enhanced by our novel Reasoned Outcome Alignment Metric (ROAM), AnomalyR1 achieves a fully end-to-end solution that autonomously processes inputs of image and domain knowledge, reasons through analysis, and generates precise anomaly localizations and masks. Based on the latest multimodal IAD benchmark, our compact 3-billion-parameter model outperforms existing methods, establishing state-of-the-art results. As MLLM capabilities continue to advance, this study is the first to deliver an end-to-end VLM-based IAD solution that demonstrates the transformative potential of ROAM-enhanced GRPO, positioning our framework as a forward-looking cornerstone for next-generation intelligent anomaly detection systems in industrial applications with limited defective data.</li>
</ul>

<h3>Title: Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image Detection with Forgery Amplification Approach</h3>
<ul>
<li><strong>Authors: </strong>Lvpan Cai, Haowei Wang, Jiayi Ji, YanShu ZhouMen, Yiwei Ma, Xiaoshuai Sun, Liujuan Cao, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11922">https://arxiv.org/abs/2504.11922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11922">https://arxiv.org/pdf/2504.11922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11922]] Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image Detection with Forgery Amplification Approach(https://arxiv.org/abs/2504.11922)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>The rise of AI-generated image editing tools has made localized forgeries increasingly realistic, posing challenges for visual content integrity. Although recent efforts have explored localized AIGC detection, existing datasets predominantly focus on object-level forgeries while overlooking broader scene edits in regions such as sky or ground. To address these limitations, we introduce \textbf{BR-Gen}, a large-scale dataset of 150,000 locally forged images with diverse scene-aware annotations, which are based on semantic calibration to ensure high-quality samples. BR-Gen is constructed through a fully automated Perception-Creation-Evaluation pipeline to ensure semantic coherence and visual realism. In addition, we further propose \textbf{NFA-ViT}, a Noise-guided Forgery Amplification Vision Transformer that enhances the detection of localized forgeries by amplifying forgery-related features across the entire image. NFA-ViT mines heterogeneous regions in images, \emph{i.e.}, potential edited areas, by noise fingerprints. Subsequently, attention mechanism is introduced to compel the interaction between normal and abnormal features, thereby propagating the generalization traces throughout the entire image, allowing subtle forgeries to influence a broader context and improving overall detection robustness. Extensive experiments demonstrate that BR-Gen constructs entirely new scenarios that are not covered by existing methods. Take a step further, NFA-ViT outperforms existing methods on BR-Gen and generalizes well across current benchmarks. All data and codes are available at this https URL.</li>
</ul>

<h3>Title: SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Dai, Shengcai Liu, Rui He, Jiahao Wu, Ning Lu, Wenqi Fan, Qing Li, Ke Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11923">https://arxiv.org/abs/2504.11923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11923">https://arxiv.org/pdf/2504.11923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11923]] SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models(https://arxiv.org/abs/2504.11923)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, diffusion</a></li>
<li><strong>Abstract: </strong>Unrestricted adversarial examples (UAEs), allow the attacker to create non-constrained adversarial examples without given clean samples, posing a severe threat to the safety of deep learning models. Recent works utilize diffusion models to generate UAEs. However, these UAEs often lack naturalness and imperceptibility due to simply optimizing in intermediate latent noises. In light of this, we propose SemDiff, a novel unrestricted adversarial attack that explores the semantic latent space of diffusion models for meaningful attributes, and devises a multi-attributes optimization approach to ensure attack success while maintaining the naturalness and imperceptibility of generated UAEs. We perform extensive experiments on four tasks on three high-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results demonstrate that SemDiff outperforms state-of-the-art methods in terms of attack success rate and imperceptibility. The generated UAEs are natural and exhibit semantically meaningful changes, in accord with the attributes' weights. In addition, SemDiff is found capable of evading different defenses, which further validates its effectiveness and threatening.</li>
</ul>

<h3>Title: Topological Analysis of Mixer Activities in the Bitcoin Network</h3>
<ul>
<li><strong>Authors: </strong>Francesco Zola, Jon Ander Medina, Andrea Venturi, Raul Orduna</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CE, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11924">https://arxiv.org/abs/2504.11924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11924">https://arxiv.org/pdf/2504.11924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11924]] Topological Analysis of Mixer Activities in the Bitcoin Network(https://arxiv.org/abs/2504.11924)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Cryptocurrency users increasingly rely on obfuscation techniques such as mixers, swappers, and decentralised or no-KYC exchanges to protect their anonymity. However, at the same time, these services are exploited by criminals to conceal and launder illicit funds. Among obfuscation services, mixers remain one of the most challenging entities to tackle. This is because their owners are often unwilling to cooperate with Law Enforcement Agencies, and technically, they operate as 'black boxes'. To better understand their functionalities, this paper proposes an approach to analyse the operations of mixers by examining their address-transaction graphs and identifying topological similarities to uncover common patterns that can define the mixer's modus operandi. The approach utilises community detection algorithms to extract dense topological structures and clustering algorithms to group similar communities. The analysis is further enriched by incorporating data from external sources related to known Exchanges, in order to understand their role in mixer operations. The approach is applied to dissect the this http URL mixer activities within the Bitcoin blockchain, revealing: i) consistent structural patterns across address-transaction graphs; ii) that Exchanges play a key role, following a well-established pattern, which raises several concerns about their AML/KYC policies. This paper represents an initial step toward dissecting and understanding the complex nature of mixer operations in cryptocurrency networks and extracting their modus operandi.</li>
</ul>

<h3>Title: Beyond Words: Augmenting Discriminative Richness via Diffusions in Unsupervised Prompt Learning</h3>
<ul>
<li><strong>Authors: </strong>Hairui Ren, Fan Tang, He Zhao, Zixuan Wang, Dandan Guo, Yi Chang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11930">https://arxiv.org/abs/2504.11930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11930">https://arxiv.org/pdf/2504.11930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11930]] Beyond Words: Augmenting Discriminative Richness via Diffusions in Unsupervised Prompt Learning(https://arxiv.org/abs/2504.11930)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Fine-tuning vision-language models (VLMs) with large amounts of unlabeled data has recently garnered significant interest. However, a key challenge remains the lack of high-quality pseudo-labeled data. Current pseudo-labeling strategies often struggle with mismatches between semantic and visual information, leading to sub-optimal performance of unsupervised prompt learning (UPL) methods. In this paper, we introduce a simple yet effective approach called \textbf{A}ugmenting D\textbf{i}scriminative \textbf{R}ichness via Diffusions (AiR), toward learning a richer discriminating way to represent the class comprehensively and thus facilitate classification. Specifically, our approach includes a pseudo-label generation module that leverages high-fidelity synthetic samples to create an auxiliary classifier, which captures richer visual variation, bridging text-image-pair classification to a more robust image-image-pair classification. Additionally, we exploit the diversity of diffusion-based synthetic samples to enhance prompt learning, providing greater information for semantic-visual alignment. Extensive experiments on five public benchmarks, including RESISC45 and Flowers102, and across three learning paradigms-UL, SSL, and TRZSL-demonstrate that AiR achieves substantial and consistent performance improvements over state-of-the-art unsupervised prompt learning methods.</li>
</ul>

<h3>Title: An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Andrea Piergentili, Beatrice Savoldi, Matteo Negri, Luisa Bentivogli</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11934">https://arxiv.org/abs/2504.11934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11934">https://arxiv.org/pdf/2504.11934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11934]] An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation(https://arxiv.org/abs/2504.11934)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Gender-neutral translation (GNT) aims to avoid expressing the gender of human referents when the source text lacks explicit cues about the gender of those referents. Evaluating GNT automatically is particularly challenging, with current solutions being limited to monolingual classifiers. Such solutions are not ideal because they do not factor in the source sentence and require dedicated data and fine-tuning to scale to new languages. In this work, we address such limitations by investigating the use of large language models (LLMs) as evaluators of GNT. Specifically, we explore two prompting approaches: one in which LLMs generate sentence-level assessments only, and another, akin to a chain-of-thought approach, where they first produce detailed phrase-level annotations before a sentence-level judgment. Through extensive experiments on multiple languages with five models, both open and proprietary, we show that LLMs can serve as evaluators of GNT. Moreover, we find that prompting for phrase-level annotations before sentence-level assessments consistently improves the accuracy of all models, providing a better and more scalable alternative to current solutions.</li>
</ul>

<h3>Title: R-Meshfusion: Reinforcement Learning Powered Sparse-View Mesh Reconstruction with Diffusion Priors</h3>
<ul>
<li><strong>Authors: </strong>Haoyang Wang, Liming Liu, Peiheng Wang, Junlin Hao, Jiangkai Wu, Xinggong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11946">https://arxiv.org/abs/2504.11946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11946">https://arxiv.org/pdf/2504.11946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11946]] R-Meshfusion: Reinforcement Learning Powered Sparse-View Mesh Reconstruction with Diffusion Priors(https://arxiv.org/abs/2504.11946)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Mesh reconstruction from multi-view images is a fundamental problem in computer vision, but its performance degrades significantly under sparse-view conditions, especially in unseen regions where no ground-truth observations are available. While recent advances in diffusion models have demonstrated strong capabilities in synthesizing novel views from limited inputs, their outputs often suffer from visual artifacts and lack 3D consistency, posing challenges for reliable mesh optimization. In this paper, we propose a novel framework that leverages diffusion models to enhance sparse-view mesh reconstruction in a principled and reliable manner. To address the instability of diffusion outputs, we propose a Consensus Diffusion Module that filters unreliable generations via interquartile range (IQR) analysis and performs variance-aware image fusion to produce robust pseudo-supervision. Building on this, we design an online reinforcement learning strategy based on the Upper Confidence Bound (UCB) to adaptively select the most informative viewpoints for enhancement, guided by diffusion loss. Finally, the fused images are used to jointly supervise a NeRF-based model alongside sparse-view ground truth, ensuring consistency across both geometry and appearance. Extensive experiments demonstrate that our method achieves significant improvements in both geometric quality and rendering quality.</li>
</ul>

<h3>Title: Flow Intelligence: Robust Feature Matching via Temporal Signature Correlation</h3>
<ul>
<li><strong>Authors: </strong>Jie Wang, Chen Ye Gan, Caoqi Wei, Jiangtao Wen, Yuxing Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11949">https://arxiv.org/abs/2504.11949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11949">https://arxiv.org/pdf/2504.11949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11949]] Flow Intelligence: Robust Feature Matching via Temporal Signature Correlation(https://arxiv.org/abs/2504.11949)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Feature matching across video streams remains a cornerstone challenge in computer vision. Increasingly, robust multimodal matching has garnered interest in robotics, surveillance, remote sensing, and medical imaging. While traditional rely on detecting and matching spatial features, they break down when faced with noisy, misaligned, or cross-modal data. Recent deep learning methods have improved robustness through learned representations, but remain constrained by their dependence on extensive training data and computational demands. We present Flow Intelligence, a paradigm-shifting approach that moves beyond spatial features by focusing on temporal motion patterns exclusively. Instead of detecting traditional keypoints, our method extracts motion signatures from pixel blocks across consecutive frames and extract temporal motion signatures between videos. These motion-based descriptors achieve natural invariance to translation, rotation, and scale variations while remaining robust across different imaging modalities. This novel approach also requires no pretraining data, eliminates the need for spatial feature detection, enables cross-modal matching using only temporal motion, and it outperforms existing methods in challenging scenarios where traditional approaches fail. By leveraging motion rather than appearance, Flow Intelligence enables robust, real-time video feature matching in diverse environments.</li>
</ul>

<h3>Title: Robust and Fine-Grained Detection of AI Generated Texts</h3>
<ul>
<li><strong>Authors: </strong>Ram Mohan Rao Kadiyala, Siddartha Pullakhandam, Kanwal Mehreen, Drishti Sharma, Siddhant Gupta, Jebish Purbey, Ashay Srivastava, Subhasya TippaReddy, Arvind Reddy Bobbili, Suraj Telugara Chandrashekhar, Modabbir Adeeb, Srinadh Vura, Hamza Farooq</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11952">https://arxiv.org/abs/2504.11952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11952">https://arxiv.org/pdf/2504.11952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11952]] Robust and Fine-Grained Detection of AI Generated Texts(https://arxiv.org/abs/2504.11952)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.</li>
</ul>

<h3>Title: zkFuzz: Foundation and Framework for Effective Fuzzing of Zero-Knowledge Circuits</h3>
<ul>
<li><strong>Authors: </strong>Hideaki Takahashi, Jihwan Kim, Suman Jana, Junfeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11961">https://arxiv.org/abs/2504.11961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11961">https://arxiv.org/pdf/2504.11961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11961]] zkFuzz: Foundation and Framework for Effective Fuzzing of Zero-Knowledge Circuits(https://arxiv.org/abs/2504.11961)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Zero-knowledge (ZK) circuits enable privacy-preserving computations and are central to many cryptographic protocols. Systems like Circom simplify ZK development by combining witness computation and circuit constraints in one program. However, even small errors can compromise security of ZK programs --under-constrained circuits may accept invalid witnesses, while over-constrained ones may reject valid ones. Static analyzers are often imprecise with high false positives, and formal tools struggle with real-world circuit scale. Additionally, existing tools overlook several critical behaviors, such as intermediate computations and program aborts, and thus miss many vulnerabilities. Our theoretical contribution is the Trace-Constraint Consistency Test (TCCT), a foundational language-independent formulation of ZK circuit bugs that defines bugs as discrepancies between the execution traces of the computation and the circuit constraints. TCCT captures both intermediate computations and program aborts, detecting bugs that elude prior tools. Our systems contribution is zkFuzz, a novel program mutation-based fuzzing framework for detecting TCCT violations. zkFuzz systematically mutates the computational logic of Zk programs guided by a novel fitness function, and injects carefully crafted inputs using tailored heuristics to expose bugs. We evaluated zkFuzz on 354 real-world ZK circuits written in Circom, a leading programming system for ZK development. zkFuzz successfully identified 66 bugs, including 38 zero-days --18 of which were confirmed by developers and 6 fixed, earning bug bounties.</li>
</ul>

<h3>Title: Exploring Video-Based Driver Activity Recognition under Noisy Labels</h3>
<ul>
<li><strong>Authors: </strong>Linjuan Fan, Di Wen, Kunyu Peng, Kailun Yang, Jiaming Zhang, Ruiping Liu, Yufan Chen, Junwei Zheng, Jiamin Wu, Xudong Han, Rainer Stiefelhagen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11966">https://arxiv.org/abs/2504.11966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11966">https://arxiv.org/pdf/2504.11966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11966]] Exploring Video-Based Driver Activity Recognition under Noisy Labels(https://arxiv.org/abs/2504.11966)</code><input type="text"></li>
<li><strong>Keywords: </strong>noise learning</a></li>
<li><strong>Abstract: </strong>As an open research topic in the field of deep learning, learning with noisy labels has attracted much attention and grown rapidly over the past ten years. Learning with label noise is crucial for driver distraction behavior recognition, as real-world video data often contains mislabeled samples, impacting model reliability and performance. However, label noise learning is barely explored in the driver activity recognition field. In this paper, we propose the first label noise learning approach for the driver activity recognition task. Based on the cluster assumption, we initially enable the model to learn clustering-friendly low-dimensional representations from given videos and assign the resultant embeddings into clusters. We subsequently perform co-refinement within each cluster to smooth the classifier outputs. Furthermore, we propose a flexible sample selection strategy that combines two selection criteria without relying on any hyperparameters to filter clean samples from the training dataset. We also incorporate a self-adaptive parameter into the sample selection process to enforce balancing across classes. A comprehensive variety of experiments on the public Drive&Act dataset for all granularity levels demonstrates the superior performance of our method in comparison with other label-denoising methods derived from the image classification field. The source code is available at this https URL.</li>
</ul>

<h3>Title: Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Yifei Dong, Fengyi Wu, Sanjian Zhang, Guangyu Chen, Yuzhi Hu, Masumi Yano, Jingdong Sun, Siyu Huang, Feng Liu, Qi Dai, Zhi-Qi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11967">https://arxiv.org/abs/2504.11967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11967">https://arxiv.org/pdf/2504.11967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11967]] Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions(https://arxiv.org/abs/2504.11967)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, robust, steal, diffusion</a></li>
<li><strong>Abstract: </strong>Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure inspection, surveillance, and related tasks, yet they also introduce critical security challenges. This survey provides a wide-ranging examination of the anti-UAV domain, centering on three core objectives-classification, detection, and tracking-while detailing emerging methodologies such as diffusion-based data synthesis, multi-modal fusion, vision-language modeling, self-supervised learning, and reinforcement learning. We systematically evaluate state-of-the-art solutions across both single-modality and multi-sensor pipelines (spanning RGB, infrared, audio, radar, and RF) and discuss large-scale as well as adversarially oriented benchmarks. Our analysis reveals persistent gaps in real-time performance, stealth detection, and swarm-based scenarios, underscoring pressing needs for robust, adaptive anti-UAV systems. By highlighting open research directions, we aim to foster innovation and guide the development of next-generation defense strategies in an era marked by the extensive use of UAVs.</li>
</ul>

<h3>Title: LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA</h3>
<ul>
<li><strong>Authors: </strong>Xanh Ho, Jiahao Huang, Florian Boudin, Akiko Aizawa</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11972">https://arxiv.org/abs/2504.11972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11972">https://arxiv.org/pdf/2504.11972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11972]] LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA(https://arxiv.org/abs/2504.11972)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Extractive reading comprehension question answering (QA) datasets are typically evaluated using Exact Match (EM) and F1-score, but these metrics often fail to fully capture model performance. With the success of large language models (LLMs), they have been employed in various tasks, including serving as judges (LLM-as-a-judge). In this paper, we reassess the performance of QA models using LLM-as-a-judge across four reading comprehension QA datasets. We examine different families of LLMs and various answer types to evaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show that LLM-as-a-judge is highly correlated with human judgments and can replace traditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human judgments improves significantly, from 0.17 (EM) and 0.36 (F1-score) to 0.85. These findings confirm that EM and F1 metrics underestimate the true performance of the QA models. While LLM-as-a-judge is not perfect for more difficult answer types (e.g., job), it still outperforms EM/F1, and we observe no bias issues, such as self-preference, when the same model is used for both the QA and judgment tasks.</li>
</ul>

<h3>Title: SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes</h3>
<ul>
<li><strong>Authors: </strong>Raúl Vázquez, Timothee Mickus, Elaine Zosa, Teemu Vahtola, Jörg Tiedemann, Aman Sinha, Vincent Segonne, Fernando Sánchez-Vega, Alessandro Raganato, Jindřich Libovický, Jussi Karlgren, Shaoxiong Ji, Jindřich Helcl, Liane Guillou, Ona de Gibert, Jaione Bengoetxea, Joseph Attieh, Marianna Apidianaki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11975">https://arxiv.org/abs/2504.11975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11975">https://arxiv.org/pdf/2504.11975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11975]] SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes(https://arxiv.org/abs/2504.11975)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present the Mu-SHROOM shared task which is focused on detecting hallucinations and other overgeneration mistakes in the output of instruction-tuned large language models (LLMs). Mu-SHROOM addresses general-purpose LLMs in 14 languages, and frames the hallucination detection problem as a span-labeling task. We received 2,618 submissions from 43 participating teams employing diverse methodologies. The large number of submissions underscores the interest of the community in hallucination detection. We present the results of the participating systems and conduct an empirical analysis to identify key factors contributing to strong performance in this task. We also emphasize relevant current challenges, notably the varying degree of hallucinations across languages and the high annotator disagreement when labeling hallucination spans.</li>
</ul>

<h3>Title: The Evolution of Zero Trust Architecture (ZTA) from Concept to Implementation</h3>
<ul>
<li><strong>Authors: </strong>Md Nasiruzzaman, Maaruf Ali, Iftekhar Salam, Mahdi H. Miraz</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11984">https://arxiv.org/abs/2504.11984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11984">https://arxiv.org/pdf/2504.11984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11984]] The Evolution of Zero Trust Architecture (ZTA) from Concept to Implementation(https://arxiv.org/abs/2504.11984)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Zero Trust Architecture (ZTA) is one of the paradigm changes in cybersecurity, from the traditional perimeter-based model to perimeterless. This article studies the core concepts of ZTA, its beginning, a few use cases and future trends. Emphasising the always verify and least privilege access, some key tenets of ZTA have grown to be integration technologies like Identity Management, Multi-Factor Authentication (MFA) and real-time analytics. ZTA is expected to strengthen cloud environments, education, work environments (including from home) while controlling other risks like lateral movement and insider threats. Despite ZTA's benefits, it comes with challenges in the form of complexity, performance overhead and vulnerabilities in the control plane. These require phased implementation and continuous refinement to keep up with evolving organisational needs and threat landscapes. Emerging technologies, such as Artificial Intelligence (AI) and Machine Learning (ML) will further automate policy enforcement and threat detection in keeping up with dynamic cyber threats.</li>
</ul>

<h3>Title: Language Models as Quasi-Crystalline Thought: Structure, Constraint, and Emergence in Generative Systems</h3>
<ul>
<li><strong>Authors: </strong>Jose Manuel Guevara-Vela</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11986">https://arxiv.org/abs/2504.11986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11986">https://arxiv.org/pdf/2504.11986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11986]] Language Models as Quasi-Crystalline Thought: Structure, Constraint, and Emergence in Generative Systems(https://arxiv.org/abs/2504.11986)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>This essay proposes an analogy between large language models (LLMs) and quasicrystals: systems that exhibit global coherence without periodic repetition and that are generated through local constraints. While LLMs are often evaluated in terms of predictive accuracy, factuality, or alignment, this structural perspective suggests that their most characteristic behavior is the production of internally resonant linguistic patterns. Just as quasicrystals forced a redefinition of order in physical systems, viewing LLMs as generators of quasi-structured language opens new paths for evaluation and design: privileging propagation of constraint over token-level accuracy, and coherence of form over fixed meaning. LLM outputs should be read not only for what they say, but for the patterns of constraint and coherence that organize them. This shift reframes generative language as a space of emergent patterning: LLMs are neither fully random nor strictly rule-based, but defined by a logic of constraint, resonance, and structural depth.</li>
</ul>

<h3>Title: Secure Transfer Learning: Training Clean Models Against Backdoor in (Both) Pre-trained Encoders and Downstream Datasets</h3>
<ul>
<li><strong>Authors: </strong>Yechao Zhang, Yuxuan Zhou, Tianyu Li, Minghui Li, Shengshan Hu, Wei Luo, Leo Yu Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11990">https://arxiv.org/abs/2504.11990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11990">https://arxiv.org/pdf/2504.11990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11990]] Secure Transfer Learning: Training Clean Models Against Backdoor in (Both) Pre-trained Encoders and Downstream Datasets(https://arxiv.org/abs/2504.11990)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>Transfer learning from pre-trained encoders has become essential in modern machine learning, enabling efficient model adaptation across diverse tasks. However, this combination of pre-training and downstream adaptation creates an expanded attack surface, exposing models to sophisticated backdoor embeddings at both the encoder and dataset levels--an area often overlooked in prior research. Additionally, the limited computational resources typically available to users of pre-trained encoders constrain the effectiveness of generic backdoor defenses compared to end-to-end training from scratch. In this work, we investigate how to mitigate potential backdoor risks in resource-constrained transfer learning scenarios. Specifically, we conduct an exhaustive analysis of existing defense strategies, revealing that many follow a reactive workflow based on assumptions that do not scale to unknown threats, novel attack types, or different training paradigms. In response, we introduce a proactive mindset focused on identifying clean elements and propose the Trusted Core (T-Core) Bootstrapping framework, which emphasizes the importance of pinpointing trustworthy data and neurons to enhance model security. Our empirical evaluations demonstrate the effectiveness and superiority of T-Core, specifically assessing 5 encoder poisoning attacks, 7 dataset poisoning attacks, and 14 baseline defenses across five benchmark datasets, addressing four scenarios of 3 potential backdoor threats.</li>
</ul>

<h3>Title: Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Pascal Schlachter, Jonathan Fuss, Bin Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11992">https://arxiv.org/abs/2504.11992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11992">https://arxiv.org/pdf/2504.11992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11992]] Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation(https://arxiv.org/abs/2504.11992)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A domain (distribution) shift between training and test data often hinders the real-world performance of deep neural networks, necessitating unsupervised domain adaptation (UDA) to bridge this gap. Online source-free UDA has emerged as a solution for practical scenarios where access to source data is restricted and target data is received as a continuous stream. However, the open-world nature of many real-world applications additionally introduces category shifts meaning that the source and target label spaces may differ. Online source-free universal domain adaptation (SF-UniDA) addresses this challenge. Existing methods mainly rely on self-training with pseudo-labels, yet the relationship between pseudo-labeling and adaptation outcomes has not been studied yet. To bridge this gap, we conduct a systematic analysis through controlled experiments with simulated pseudo-labeling, offering valuable insights into pseudo-labeling for online SF-UniDA. Our findings reveal a substantial gap between the current state-of-the-art and the upper bound of adaptation achieved with perfect pseudo-labeling. Moreover, we show that a contrastive loss enables effective adaptation even with moderate pseudo-label accuracy, while a cross-entropy loss, though less robust to pseudo-label errors, achieves superior results when pseudo-labeling approaches perfection. Lastly, our findings indicate that pseudo-label accuracy is in general more crucial than quantity, suggesting that prioritizing fewer but high-confidence pseudo-labels is beneficial. Overall, our study highlights the critical role of pseudo-labeling in (online) SF-UniDA and provides actionable insights to drive future advancements in the field. Our code is available at this https URL.</li>
</ul>

<h3>Title: A Complex-valued SAR Foundation Model Based on Physically Inspired Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Mengyu Wang, Hanbo Bi, Yingchao Feng, Linlin Xin, Shuo Gong, Tianqi Wang, Zhiyuan Yan, Peijin Wang, Wenhui Diao, Xian Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.11999">https://arxiv.org/abs/2504.11999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.11999">https://arxiv.org/pdf/2504.11999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.11999]] A Complex-valued SAR Foundation Model Based on Physically Inspired Representation Learning(https://arxiv.org/abs/2504.11999)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Vision foundation models in remote sensing have been extensively studied due to their superior generalization on various downstream tasks. Synthetic Aperture Radar (SAR) offers all-day, all-weather imaging capabilities, providing significant advantages for Earth observation. However, establishing a foundation model for SAR image interpretation inevitably encounters the challenges of insufficient information utilization and poor interpretability. In this paper, we propose a remote sensing foundation model based on complex-valued SAR data, which simulates the polarimetric decomposition process for pre-training, i.e., characterizing pixel scattering intensity as a weighted combination of scattering bases and scattering coefficients, thereby endowing the foundation model with physical interpretability. Specifically, we construct a series of scattering queries, each representing an independent and meaningful scattering basis, which interact with SAR features in the scattering query decoder and output the corresponding scattering coefficient. To guide the pre-training process, polarimetric decomposition loss and power self-supervision loss are constructed. The former aligns the predicted coefficients with Yamaguchi coefficients, while the latter reconstructs power from the predicted coefficients and compares it to the input image's power. The performance of our foundation model is validated on six typical downstream tasks, achieving state-of-the-art results. Notably, the foundation model can extract stable feature representations and exhibits strong generalization, even in data-scarce conditions.</li>
</ul>

<h3>Title: Instruction-augmented Multimodal Alignment for Image-Text and Element Matching</h3>
<ul>
<li><strong>Authors: </strong>Xinli Yue, JianHui Sun, Junda Lu, Liangchao Yao, Fan Xia, Tianyi Wang, Fengyun Rao, Jing Lyu, Yuetang Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12018">https://arxiv.org/abs/2504.12018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12018">https://arxiv.org/pdf/2504.12018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12018]] Instruction-augmented Multimodal Alignment for Image-Text and Element Matching(https://arxiv.org/abs/2504.12018)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of text-to-image (T2I) generation models, assessing the semantic alignment between generated images and text descriptions has become a significant research challenge. Current methods, including those based on Visual Question Answering (VQA), still struggle with fine-grained assessments and precise quantification of image-text alignment. This paper presents an improved evaluation method named Instruction-augmented Multimodal Alignment for Image-Text and Element Matching (iMatch), which evaluates image-text semantic alignment by fine-tuning multimodal large language models. We introduce four innovative augmentation strategies: First, the QAlign strategy creates a precise probabilistic mapping to convert discrete scores from multimodal large language models into continuous matching scores. Second, a validation set augmentation strategy uses pseudo-labels from model predictions to expand training data, boosting the model's generalization performance. Third, an element augmentation strategy integrates element category labels to refine the model's understanding of image-text matching. Fourth, an image augmentation strategy employs techniques like random lighting to increase the model's robustness. Additionally, we propose prompt type augmentation and score perturbation strategies to further enhance the accuracy of element assessments. Our experimental results show that the iMatch method significantly surpasses existing methods, confirming its effectiveness and practical value. Furthermore, our iMatch won first place in the CVPR NTIRE 2025 Text to Image Generation Model Quality Assessment - Track 1 Image-Text Alignment.</li>
</ul>

<h3>Title: MixSignGraph: A Sign Sequence is Worth Mixed Graphs of Nodes</h3>
<ul>
<li><strong>Authors: </strong>Shiwei Gan, Yafeng Yin, Zhiwei Jiang, Hongkai Wen, Lei Xie, Sanglu Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12020">https://arxiv.org/abs/2504.12020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12020">https://arxiv.org/pdf/2504.12020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12020]] MixSignGraph: A Sign Sequence is Worth Mixed Graphs of Nodes(https://arxiv.org/abs/2504.12020)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Recent advances in sign language research have benefited from CNN-based backbones, which are primarily transferred from traditional computer vision tasks (\eg object identification, image recognition). However, these CNN-based backbones usually excel at extracting features like contours and texture, but may struggle with capturing sign-related features. In fact, sign language tasks require focusing on sign-related regions, including the collaboration between different regions (\eg left hand region and right hand region) and the effective content in a single region. To capture such region-related features, we introduce MixSignGraph, which represents sign sequences as a group of mixed graphs and designs the following three graph modules for feature extraction, \ie Local Sign Graph (LSG) module, Temporal Sign Graph (TSG) module and Hierarchical Sign Graph (HSG) module. Specifically, the LSG module learns the correlation of intra-frame cross-region features within one frame, \ie focusing on spatial features. The TSG module tracks the interaction of inter-frame cross-region features among adjacent frames, \ie focusing on temporal features. The HSG module aggregates the same-region features from different-granularity feature maps of a frame, \ie focusing on hierarchical features. In addition, to further improve the performance of sign language tasks without gloss annotations, we propose a simple yet counter-intuitive Text-driven CTC Pre-training (TCP) method, which generates pseudo gloss labels from text labels for model pre-training. Extensive experiments conducted on current five public sign language datasets demonstrate the superior performance of the proposed model. Notably, our model surpasses the SOTA models on multiple sign language tasks across several datasets, without relying on any additional cues.</li>
</ul>

<h3>Title: Action Anticipation from SoccerNet Football Video Broadcasts</h3>
<ul>
<li><strong>Authors: </strong>Mohamad Dalal, Artur Xarles, Anthony Cioppa, Silvio Giancola, Marc Van Droogenbroeck, Bernard Ghanem, Albert Clapés, Sergio Escalera, Thomas B. Moeslund</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12021">https://arxiv.org/abs/2504.12021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12021">https://arxiv.org/pdf/2504.12021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12021]] Action Anticipation from SoccerNet Football Video Broadcasts(https://arxiv.org/abs/2504.12021)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Artificial intelligence has revolutionized the way we analyze sports videos, whether to understand the actions of games in long untrimmed videos or to anticipate the player's motion in future frames. Despite these efforts, little attention has been given to anticipating game actions before they occur. In this work, we introduce the task of action anticipation for football broadcast videos, which consists in predicting future actions in unobserved future frames, within a five- or ten-second anticipation window. To benchmark this task, we release a new dataset, namely the SoccerNet Ball Action Anticipation dataset, based on SoccerNet Ball Action Spotting. Additionally, we propose a Football Action ANticipation TRAnsformer (FAANTRA), a baseline method that adapts FUTR, a state-of-the-art action anticipation model, to predict ball-related actions. To evaluate action anticipation, we introduce new metrics, including mAP@$\delta$, which evaluates the temporal precision of predicted future actions, as well as mAP@$\infty$, which evaluates their occurrence within the anticipation window. We also conduct extensive ablation studies to examine the impact of various task settings, input configurations, and model architectures. Experimental results highlight both the feasibility and challenges of action anticipation in football videos, providing valuable insights into the design of predictive models for sports analytics. By forecasting actions before they unfold, our work will enable applications in automated broadcasting, tactical analysis, and player decision-making. Our dataset and code are publicly available at this https URL.</li>
</ul>

<h3>Title: FedEPA: Enhancing Personalization and Modality Alignment in Multimodal Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhang, Qingfeng Du, Jiaqi Lv</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12025">https://arxiv.org/abs/2504.12025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12025">https://arxiv.org/pdf/2504.12025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12025]] FedEPA: Enhancing Personalization and Modality Alignment in Multimodal Federated Learning(https://arxiv.org/abs/2504.12025)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables decentralized model training across multiple parties while preserving privacy. However, most FL systems assume clients hold only unimodal data, limiting their real-world applicability, as institutions often possess multimodal data. Moreover, the lack of labeled data further constrains the performance of most FL methods. In this work, we propose FedEPA, a novel FL framework for multimodal learning. FedEPA employs a personalized local model aggregation strategy that leverages labeled data on clients to learn personalized aggregation weights, thereby alleviating the impact of data heterogeneity. We also propose an unsupervised modality alignment strategy that works effectively with limited labeled data. Specifically, we decompose multimodal features into aligned features and context features. We then employ contrastive learning to align the aligned features across modalities, ensure the independence between aligned features and context features within each modality, and promote the diversity of context features. A multimodal feature fusion strategy is introduced to obtain a joint embedding. The experimental results show that FedEPA significantly outperforms existing FL methods in multimodal classification tasks under limited labeled data conditions.</li>
</ul>

<h3>Title: Understanding Attention Mechanism in Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Bingyan Liu, Chengyu Wang, Tongtong Su, Huan Ten, Jun Huang, Kailing Guo, Kui Jia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12027">https://arxiv.org/abs/2504.12027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12027">https://arxiv.org/pdf/2504.12027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12027]] Understanding Attention Mechanism in Video Diffusion Models(https://arxiv.org/abs/2504.12027)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-video (T2V) synthesis models, such as OpenAI's Sora, have garnered significant attention due to their ability to generate high-quality videos from a text prompt. In diffusion-based T2V models, the attention mechanism is a critical component. However, it remains unclear what intermediate features are learned and how attention blocks in T2V models affect various aspects of video synthesis, such as image quality and temporal consistency. In this paper, we conduct an in-depth perturbation analysis of the spatial and temporal attention blocks of T2V models using an information-theoretic approach. Our results indicate that temporal and spatial attention maps affect not only the timing and layout of the videos but also the complexity of spatiotemporal elements and the aesthetic quality of the synthesized videos. Notably, high-entropy attention maps are often key elements linked to superior video quality, whereas low-entropy attention maps are associated with the video's intra-frame structure. Based on our findings, we propose two novel methods to enhance video quality and enable text-guided video editing. These methods rely entirely on lightweight manipulation of the attention matrices in T2V models. The efficacy and effectiveness of our methods are further validated through experimental evaluation across multiple datasets.</li>
</ul>

<h3>Title: RadMamba: Efficient Human Activity Recognition through Radar-based Micro-Doppler-Oriented Mamba State-Space Model</h3>
<ul>
<li><strong>Authors: </strong>Yizhuo Wu, Francesco Fioranelli, Chang Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12039">https://arxiv.org/abs/2504.12039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12039">https://arxiv.org/pdf/2504.12039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12039]] RadMamba: Efficient Human Activity Recognition through Radar-based Micro-Doppler-Oriented Mamba State-Space Model(https://arxiv.org/abs/2504.12039)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, transformer</a></li>
<li><strong>Abstract: </strong>Radar-based HAR has emerged as a promising alternative to conventional monitoring approaches, such as wearable devices and camera-based systems, due to its unique privacy preservation and robustness advantages. However, existing solutions based on convolutional and recurrent neural networks, although effective, are computationally demanding during deployment. This limits their applicability in scenarios with constrained resources or those requiring multiple sensors. Advanced architectures, such as ViT and SSM architectures, offer improved modeling capabilities and have made efforts toward lightweight designs. However, their computational complexity remains relatively high. To leverage the strengths of transformer architectures while simultaneously enhancing accuracy and reducing computational complexity, this paper introduces RadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM specifically tailored for radar-based HAR. Across three diverse datasets, RadMamba matches the top-performing previous model's 99.8% classification accuracy on Dataset DIAT with only 1/400 of its parameters and equals the leading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their parameters. In scenarios with continuous sequences of actions evaluated on Dataset UoG2020, RadMamba surpasses other models with significantly higher parameter counts by at least 3%, achieving this with only 6.7k parameters. Our code is available at: this https URL.</li>
</ul>

<h3>Title: pix2pockets: Shot Suggestions in 8-Ball Pool from a Single Image in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Jonas Myhre Schiøtt, Viktor Sebastian Petersen, Dimitrios P. Papadopoulos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12045">https://arxiv.org/abs/2504.12045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12045">https://arxiv.org/pdf/2504.12045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12045]] pix2pockets: Shot Suggestions in 8-Ball Pool from a Single Image in the Wild(https://arxiv.org/abs/2504.12045)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Computer vision models have seen increased usage in sports, and reinforcement learning (RL) is famous for beating humans in strategic games such as Chess and Go. In this paper, we are interested in building upon these advances and examining the game of classic 8-ball pool. We introduce pix2pockets, a foundation for an RL-assisted pool coach. Given a single image of a pool table, we first aim to detect the table and the balls and then propose the optimal shot suggestion. For the first task, we build a dataset with 195 diverse images where we manually annotate all balls and table dots, leading to 5748 object segmentation masks. For the second task, we build a standardized RL environment that allows easy development and benchmarking of any RL algorithm. Our object detection model yields an AP50 of 91.2 while our ball location pipeline obtains an error of only 0.4 cm. Furthermore, we compare standard RL algorithms to set a baseline for the shot suggestion task and we show that all of them fail to pocket all balls without making a foul move. We also present a simple baseline that achieves a per-shot success rate of 94.7% and clears a full game in a single turn 30% of the time.</li>
</ul>

<h3>Title: Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM</h3>
<ul>
<li><strong>Authors: </strong>Zirui Pan, Xin Wang, Yipeng Zhang, Hong Chen, Kwan Man Cheng, Yaofei Wu, Wenwu Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12048">https://arxiv.org/abs/2504.12048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12048">https://arxiv.org/pdf/2504.12048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12048]] Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM(https://arxiv.org/abs/2504.12048)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Text-to-Video generation, which utilizes the provided text prompt to generate high-quality videos, has drawn increasing attention and achieved great success due to the development of diffusion models recently. Existing methods mainly rely on a pre-trained text encoder to capture the semantic information and perform cross attention with the encoded text prompt to guide the generation of video. However, when it comes to complex prompts that contain dynamic scenes and multiple camera-view transformations, these methods can not decompose the overall information into separate scenes, as well as fail to smoothly change scenes based on the corresponding camera-views. To solve these problems, we propose a novel method, i.e., Modular-Cam. Specifically, to better understand a given complex prompt, we utilize a large language model to analyze user instructions and decouple them into multiple scenes together with transition actions. To generate a video containing dynamic scenes that match the given camera-views, we incorporate the widely-used temporal transformer into the diffusion model to ensure continuity within a single scene and propose CamOperator, a modular network based module that well controls the camera movements. Moreover, we propose AdaControlNet, which utilizes ControlNet to ensure consistency across scenes and adaptively adjusts the color tone of the generated video. Extensive qualitative and quantitative experiments prove our proposed Modular-Cam's strong capability of generating multi-scene videos together with its ability to achieve fine-grained control of camera movements. Generated results are available at this https URL.</li>
</ul>

<h3>Title: Bayesian dynamic borrowing considering semantic similarity between outcomes for disproportionality analysis in FAERS</h3>
<ul>
<li><strong>Authors: </strong>François Haguinet, Jeffery L Painter, Gregory E Powell, Andrea Callegaro, Andrew Bate</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12052">https://arxiv.org/abs/2504.12052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12052">https://arxiv.org/pdf/2504.12052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12052]] Bayesian dynamic borrowing considering semantic similarity between outcomes for disproportionality analysis in FAERS(https://arxiv.org/abs/2504.12052)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a Bayesian dynamic borrowing (BDB) approach to enhance the quantitative identification of adverse events (AEs) in spontaneous reporting systems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior within a Bayesian hierarchical model and incorporates semantic similarity measures (SSMs) to enable weighted information sharing from MedDRA Preferred Terms (PTs) that are clinical similar to the target PT. This continuous similarity-based borrowing addresses limitation of rigid hierarchical grouping in current disproportionality analysis (DPA). Using data from the FDA Adverse Event Reporting System (FAERS) between 2015 and 2019, we evalute this approach - termed IC SSM - against standard Information Component (IC) analysis and IC with borrowing at the MedDRA high-level group term (HLGT) level. A novel references set (PVLens), derived from FDA product label updates, enabled prospective evaluation of method performance in identifying AEs prior to official labeling. The IC SSM approach demonstrated improved sensitivity compared to both traditional IC and HLGT-based borrowing, with minor trade-offs in F1 scores and Youden's index. IC SSM consistently identified more true positives and detected signals over 5 months sooner than traditional IC. Despite a marginally lower aggregate Youden's index, IC SSM showed higher performance in the early post-marketing period, providing more stable and relevant estimates than HLGT-based borrowing and traditional IC. These findings support the use of SSM-informed Bayesian borrowing as a scalable and context-aware enhancement to traditional DPA methods. Future research should validate this approach across other datasets and explore additional similarity metrics and Bayesian inference strategies using case-level data.</li>
</ul>

<h3>Title: A Scalable Framework for Post-Quantum Authentication in Public Key Infrastructures</h3>
<ul>
<li><strong>Authors: </strong>Antonia Tsili, Konstantinos Kordolaimis, Konstantinos Krilakis, Dimitris Syvridis</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12062">https://arxiv.org/abs/2504.12062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12062">https://arxiv.org/pdf/2504.12062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12062]] A Scalable Framework for Post-Quantum Authentication in Public Key Infrastructures(https://arxiv.org/abs/2504.12062)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, robust</a></li>
<li><strong>Abstract: </strong>This work explores the performance and scalability of a hierarchical certificate authority framework with automated certificate issuance employing post-quantum cryptographic (PQC) signature algorithms. The system is designed for compatibility with both classical and PQC algorithms, promoting crypto-agility while ensuring robust security against quantum-based threats. The proposed framework design expects minimal cryptographic requirements from potential clients, protects certificates of high importance against cross-dependent chains-of-trust and allows for prompt switching between classical and PQC algorithms. Finally, we evaluate SPHINCS$^+$, Falcon, and Dilithium variants in various configurations of certificate issuance and verification accommodating a large client base, underlining the trade-offs in balancing performance, scalability, and security.</li>
</ul>

<h3>Title: Generative Deep Learning Framework for Inverse Design of Fuels</h3>
<ul>
<li><strong>Authors: </strong>Kiran K. Yalamanchi, Pinaki Pal, Balaji Mohan, Abdullah S. AlRamadan, Jihad A. Badra, Yuanjiang Pei</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12075">https://arxiv.org/abs/2504.12075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12075">https://arxiv.org/pdf/2504.12075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12075]] Generative Deep Learning Framework for Inverse Design of Fuels(https://arxiv.org/abs/2504.12075)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the present work, a generative deep learning framework combining a Co-optimized Variational Autoencoder (Co-VAE) architecture with quantitative structure-property relationship (QSPR) techniques is developed to enable accelerated inverse design of fuels. The Co-VAE integrates a property prediction component coupled with the VAE latent space, enhancing molecular reconstruction and accurate estimation of Research Octane Number (RON) (chosen as the fuel property of interest). A subset of the GDB-13 database, enriched with a curated RON database, is used for model training. Hyperparameter tuning is further utilized to optimize the balance among reconstruction fidelity, chemical validity, and RON prediction. An independent regression model is then used to refine RON prediction, while a differential evolution algorithm is employed to efficiently navigate the VAE latent space and identify promising fuel molecule candidates with high RON. This methodology addresses the limitations of traditional fuel screening approaches by capturing complex structure-property relationships within a comprehensive latent representation. The generative model provides a flexible tool for systematically exploring vast chemical spaces, paving the way for discovering fuels with superior anti-knock properties. The demonstrated approach can be readily extended to incorporate additional fuel properties and synthesizability criteria to enhance applicability and reliability for de novo design of new fuels.</li>
</ul>

<h3>Title: Single-shot Star-convex Polygon-based Instance Segmentation for Spatially-correlated Biomedical Objects</h3>
<ul>
<li><strong>Authors: </strong>Trina De, Adrian Urbanski, Artur Yakimovich</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12078">https://arxiv.org/abs/2504.12078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12078">https://arxiv.org/pdf/2504.12078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12078]] Single-shot Star-convex Polygon-based Instance Segmentation for Spatially-correlated Biomedical Objects(https://arxiv.org/abs/2504.12078)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Biomedical images often contain objects known to be spatially correlated or nested due to their inherent properties, leading to semantic relations. Examples include cell nuclei being nested within eukaryotic cells and colonies growing exclusively within their culture dishes. While these semantic relations bear key importance, detection tasks are often formulated independently, requiring multi-shot analysis pipelines. Importantly, spatial correlation could constitute a fundamental prior facilitating learning of more meaningful representations for tasks like instance segmentation. This knowledge has, thus far, not been utilised by the biomedical computer vision community. We argue that the instance segmentation of two or more categories of objects can be achieved in parallel. We achieve this via two architectures HydraStarDist (HSD) and the novel (HSD-WBR) based on the widely-used StarDist (SD), to take advantage of the star-convexity of our target objects. HSD and HSD-WBR are constructed to be capable of incorporating their interactions as constraints into account. HSD implicitly incorporates spatial correlation priors based on object interaction through a joint encoder. HSD-WBR further enforces the prior in a regularisation layer with the penalty we proposed named Within Boundary Regularisation Penalty (WBR). Both architectures achieve nested instance segmentation in a single shot. We demonstrate their competitiveness based on $IoU_R$ and AP and superiority in a new, task-relevant criteria, Joint TP rate (JTPR) compared to their baseline SD and Cellpose. Our approach can be further modified to capture partial-inclusion/-exclusion in multi-object interactions in fluorescent or brightfield microscopy or digital imaging. Finally, our strategy suggests gains by making this learning single-shot and computationally efficient.</li>
</ul>

<h3>Title: DC-SAM: In-Context Segment Anything in Images and Videos via Dual Consistency</h3>
<ul>
<li><strong>Authors: </strong>Mengshi Qi, Pengfei Zhu, Xiangtai Li, Xiaoyang Bi, Lu Qi, Huadong Ma, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12080">https://arxiv.org/abs/2504.12080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12080">https://arxiv.org/pdf/2504.12080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12080]] DC-SAM: In-Context Segment Anything in Images and Videos via Dual Consistency(https://arxiv.org/abs/2504.12080)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Given a single labeled example, in-context segmentation aims to segment corresponding objects. This setting, known as one-shot segmentation in few-shot learning, explores the segmentation model's generalization ability and has been applied to various vision tasks, including scene understanding and image/video editing. While recent Segment Anything Models have achieved state-of-the-art results in interactive segmentation, these approaches are not directly applicable to in-context segmentation. In this work, we propose the Dual Consistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2 for in-context segmentation of both images and videos. Our key insights are to enhance the features of the SAM's prompt encoder in segmentation by providing high-quality visual prompts. When generating a mask prior, we fuse the SAM features to better align the prompt encoder. Then, we design a cycle-consistent cross-attention on fused features and initial visual prompts. Next, a dual-branch design is provided by using the discriminative positive and negative prompts in the prompt encoder. Furthermore, we design a simple mask-tube training strategy to adopt our proposed dual consistency method into the mask tube. Although the proposed DC-SAM is primarily designed for images, it can be seamlessly extended to the video domain with the support of SAM2. Given the absence of in-context segmentation in the video domain, we manually curate and construct the first benchmark from existing video segmentation datasets, named In-Context Video Object Segmentation (IC-VOS), to better assess the in-context capability of the model. Extensive experiments demonstrate that our method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on PASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our source code and benchmark are available at this https URL.</li>
</ul>

<h3>Title: Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection</h3>
<ul>
<li><strong>Authors: </strong>Yumin Kim, Hwanhee Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12082">https://arxiv.org/abs/2504.12082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12082">https://arxiv.org/pdf/2504.12082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12082]] Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection(https://arxiv.org/abs/2504.12082)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Hate speech detection is a crucial area of research in natural language processing, essential for ensuring online community safety. However, detecting implicit hate speech, where harmful intent is conveyed in subtle or indirect ways, remains a major challenge. Unlike explicit hate speech, implicit expressions often depend on context, cultural subtleties, and hidden biases, making them more challenging to identify consistently. Additionally, the interpretation of such speech is influenced by external knowledge and demographic biases, resulting in varied detection results across different language models. Furthermore, Large Language Models often show heightened sensitivity to toxic language and references to vulnerable groups, which can lead to misclassifications. This over-sensitivity results in false positives (incorrectly identifying harmless statements as hateful) and false negatives (failing to detect genuinely harmful content). Addressing these issues requires methods that not only improve detection precision but also reduce model biases and enhance robustness. To address these challenges, we propose a novel method, which utilizes in-context learning without requiring model fine-tuning. By adaptively retrieving demonstrations that focus on similar groups or those with the highest similarity scores, our approach enhances contextual comprehension. Experimental results show that our method outperforms current state-of-the-art techniques. Implementation details and code are available at TBD.</li>
</ul>

<h3>Title: AttentionDrop: A Novel Regularization Method for Transformer Models</h3>
<ul>
<li><strong>Authors: </strong>Mirza Samad Ahmed Baig, Syeda Anshrah Gillani, Abdul Akbar Khan, Shahid Munir Shah</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12088">https://arxiv.org/abs/2504.12088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12088">https://arxiv.org/pdf/2504.12088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12088]] AttentionDrop: A Novel Regularization Method for Transformer Models(https://arxiv.org/abs/2504.12088)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based architectures achieve state-of-the-art performance across a wide range of tasks in natural language processing, computer vision, and speech. However, their immense capacity often leads to overfitting, especially when training data is limited or noisy. We propose AttentionDrop, a unified family of stochastic regularization techniques that operate directly on the self-attention distributions. We introduces three variants: 1. Hard Attention Masking: randomly zeroes out top-k attention logits per query to encourage diverse context utilization. 2. Blurred Attention Smoothing: applies a dynamic Gaussian convolution over attention logits to diffuse overly peaked distributions. 3. Consistency-Regularized AttentionDrop: enforces output stability under multiple independent AttentionDrop perturbations via a KL-based consistency loss.</li>
</ul>

<h3>Title: Gauging Overprecision in LLMs: An Empirical Study</h3>
<ul>
<li><strong>Authors: </strong>Adil Bahaj, Hamed Rahimi, Mohamed Chetouani, Mounir Ghogho</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12098">https://arxiv.org/abs/2504.12098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12098">https://arxiv.org/pdf/2504.12098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12098]] Gauging Overprecision in LLMs: An Empirical Study(https://arxiv.org/abs/2504.12098)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, overconfidence in large language models (LLMs) has garnered considerable attention due to its fundamental importance in quantifying the trustworthiness of LLM generation. However, existing approaches prompt the \textit{black box LLMs} to produce their confidence (\textit{verbalized confidence}), which can be subject to many biases and hallucinations. Inspired by a different aspect of overconfidence in cognitive science called \textit{overprecision}, we designed a framework for its study in black box LLMs. This framework contains three main phases: 1) generation, 2) refinement and 3) evaluation. In the generation phase we prompt the LLM to generate answers to numerical questions in the form of intervals with a certain level of confidence. This confidence level is imposed in the prompt and not required for the LLM to generate as in previous approaches. We use various prompting techniques and use the same prompt multiple times to gauge the effects of randomness in the generation process. In the refinement phase, answers from the previous phase are refined to generate better answers. The LLM answers are evaluated and studied in the evaluation phase to understand its internal workings. This study allowed us to gain various insights into LLM overprecision: 1) LLMs are highly uncalibrated for numerical tasks 2) {\color{blue}there is no correlation between the length of the interval and the imposed confidence level, which can be symptomatic of a a) lack of understanding of the concept of confidence or b) inability to adjust self-confidence by following instructions}, {\color{blue}3)} LLM numerical precision differs depending on the task, scale of answer and prompting technique {\color{blue}4) Refinement of answers doesn't improve precision in most cases}. We believe this study offers new perspectives on LLM overconfidence and serves as a strong baseline for overprecision in LLMs.</li>
</ul>

<h3>Title: Generalized Visual Relation Detection with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kaifeng Gao, Siqi Chen, Hanwang Zhang, Jun Xiao, Yueting Zhuang, Qianru Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12100">https://arxiv.org/abs/2504.12100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12100">https://arxiv.org/pdf/2504.12100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12100]] Generalized Visual Relation Detection with Diffusion Models(https://arxiv.org/abs/2504.12100)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Visual relation detection (VRD) aims to identify relationships (or interactions) between object pairs in an image. Although recent VRD models have achieved impressive performance, they are all restricted to pre-defined relation categories, while failing to consider the semantic ambiguity characteristic of visual relations. Unlike objects, the appearance of visual relations is always subtle and can be described by multiple predicate words from different perspectives, e.g., ``ride'' can be depicted as ``race'' and ``sit on'', from the sports and spatial position views, respectively. To this end, we propose to model visual relations as continuous embeddings, and design diffusion models to achieve generalized VRD in a conditional generative manner, termed Diff-VRD. We model the diffusion process in a latent space and generate all possible relations in the image as an embedding sequence. During the generation, the visual and text embeddings of subject-object pairs serve as conditional signals and are injected via cross-attention. After the generation, we design a subsequent matching stage to assign the relation words to subject-object pairs by considering their semantic similarities. Benefiting from the diffusion-based generative process, our Diff-VRD is able to generate visual relations beyond the pre-defined category labels of datasets. To properly evaluate this generalized VRD task, we introduce two evaluation metrics, i.e., text-to-image retrieval and SPICE PR Curve inspired by image captioning. Extensive experiments in both human-object interaction (HOI) detection and scene graph generation (SGG) benchmarks attest to the superiority and effectiveness of Diff-VRD.</li>
</ul>

<h3>Title: Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Shizhan Cai, Liang Ding, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12108">https://arxiv.org/abs/2504.12108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12108">https://arxiv.org/pdf/2504.12108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12108]] Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation(https://arxiv.org/abs/2504.12108)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>The rapid development of Large Language Models (LLMs) has intensified concerns about content traceability and potential misuse. Existing watermarking schemes for sampled text often face trade-offs between maintaining text quality and ensuring robust detection against various attacks. To address these issues, we propose a novel watermarking scheme that improves both detectability and text quality by introducing a cumulative watermark entropy threshold. Our approach is compatible with and generalizes existing sampling functions, enhancing adaptability. Experimental results across multiple LLMs show that our scheme significantly outperforms existing methods, achieving over 80\% improvements on widely-used datasets, e.g., MATH and GSM8K, while maintaining high detection accuracy.</li>
</ul>

<h3>Title: A Diffusion-Based Framework for Terrain-Aware Remote Sensing Image Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Yu, Mohd Yamani Inda Idris, Pei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12112">https://arxiv.org/abs/2504.12112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12112">https://arxiv.org/pdf/2504.12112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12112]] A Diffusion-Based Framework for Terrain-Aware Remote Sensing Image Reconstruction(https://arxiv.org/abs/2504.12112)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Remote sensing imagery is essential for environmental monitoring, agricultural management, and disaster response. However, data loss due to cloud cover, sensor failures, or incomplete acquisition-especially in high-resolution and high-frequency tasks-severely limits satellite imagery's effectiveness. Traditional interpolation methods struggle with large missing areas and complex structures. Remote sensing imagery consists of multiple bands, each with distinct meanings, and ensuring consistency across bands is critical to avoid anomalies in the combined images. This paper proposes SatelliteMaker, a diffusion-based method that reconstructs missing data across varying levels of data loss while maintaining spatial, spectral, and temporal consistency. We also propose Digital Elevation Model (DEM) as a conditioning input and use tailored prompts to generate realistic images, making diffusion models applicable to quantitative remote sensing tasks. Additionally, we propose a VGG-Adapter module based on Distribution Loss, which reduces distribution discrepancy and ensures style consistency. Extensive experiments show that SatelliteMaker achieves state-of-the-art performance across multiple tasks.</li>
</ul>

<h3>Title: Remote sensing colour image semantic segmentation of trails created by large herbivorous Mammals</h3>
<ul>
<li><strong>Authors: </strong>Jose Francisco Diez-Pastor, Francisco Javier Gonzalez-Moya, Pedro Latorre-Carmona, Francisco Javier Perez-Barbería, Ludmila I.Kuncheva, Antonio Canepa-Oneto, Alvar Arnaiz-González, Cesar Garcia-Osorio</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12121">https://arxiv.org/abs/2504.12121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12121">https://arxiv.org/pdf/2504.12121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12121]] Remote sensing colour image semantic segmentation of trails created by large herbivorous Mammals(https://arxiv.org/abs/2504.12121)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Detection of spatial areas where biodiversity is at risk is of paramount importance for the conservation and monitoring of ecosystems. Large terrestrial mammalian herbivores are keystone species as their activity not only has deep effects on soils, plants, and animals, but also shapes landscapes, as large herbivores act as allogenic ecosystem engineers. One key landscape feature that indicates intense herbivore activity and potentially impacts biodiversity is the formation of grazing trails. Grazing trails are formed by the continuous trampling activity of large herbivores that can produce complex networks of tracks of bare soil. Here, we evaluated different algorithms based on machine learning techniques to identify grazing trails. Our goal is to automatically detect potential areas with intense herbivory activity, which might be beneficial for conservation and management plans. We have applied five semantic segmentation methods combined with fourteen encoders aimed at mapping grazing trails on aerial images. Our results indicate that in most cases the chosen methodology successfully mapped the trails, although there were a few instances where the actual trail structure was underestimated. The UNet architecture with the MambaOut encoder was the best architecture for mapping trails. The proposed approach could be applied to develop tools for mapping and monitoring temporal changes in these landscape structures to support habitat conservation and land management programs. This is the first time, to the best of our knowledge, that competitive image segmentation results are obtained for the detection and delineation of trails of large herbivorous mammals.</li>
</ul>

<h3>Title: Anti-Aesthetics: Protecting Facial Privacy against Customized Text-to-Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Songping Wang, Yueming Lyu, Shiqi Liu, Ning Li, Tong Tong, Hao Sun, Caifeng Shan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12129">https://arxiv.org/abs/2504.12129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12129">https://arxiv.org/pdf/2504.12129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12129]] Anti-Aesthetics: Protecting Facial Privacy against Customized Text-to-Image Synthesis(https://arxiv.org/abs/2504.12129)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, diffusion</a></li>
<li><strong>Abstract: </strong>The rise of customized diffusion models has spurred a boom in personalized visual content creation, but also poses risks of malicious misuse, severely threatening personal privacy and copyright protection. Some studies show that the aesthetic properties of images are highly positively correlated with human perception of image quality. Inspired by this, we approach the problem from a novel and intriguing aesthetic perspective to degrade the generation quality of maliciously customized models, thereby achieving better protection of facial identity. Specifically, we propose a Hierarchical Anti-Aesthetic (HAA) framework to fully explore aesthetic cues, which consists of two key branches: 1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward mechanism and a global anti-aesthetic loss, it can degrade the overall aesthetics of the generated content; 2) Local Anti-Aesthetics: A local anti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to guide adversarial perturbations to disrupt local facial identity. By seamlessly integrating both branches, our HAA effectively achieves the goal of anti-aesthetics from a global to a local level during customized generation. Extensive experiments show that HAA outperforms existing SOTA methods largely in identity removal, providing a powerful tool for protecting facial privacy and copyright.</li>
</ul>

<h3>Title: Multilingual Contextualization of Large Language Models for Document-Level Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Miguel Moura Ramos, Patrick Fernandes, Sweta Agrawal, André F. T. Martins</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12140">https://arxiv.org/abs/2504.12140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12140">https://arxiv.org/pdf/2504.12140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12140]] Multilingual Contextualization of Large Language Models for Document-Level Machine Translation(https://arxiv.org/abs/2504.12140)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated strong performance in sentence-level machine translation, but scaling to document-level translation remains challenging, particularly in modeling long-range dependencies and discourse phenomena across sentences and paragraphs. In this work, we propose a method to improve LLM-based long-document translation through targeted fine-tuning on high-quality document-level data, which we curate and introduce as DocBlocks. Our approach supports multiple translation paradigms, including direct document-to-document and chunk-level translation, by integrating instructions both with and without surrounding context. This enables models to better capture cross-sentence dependencies while maintaining strong sentence-level translation performance. Experimental results show that incorporating multiple translation paradigms improves document-level translation quality and inference speed compared to prompting and agent-based methods.</li>
</ul>

<h3>Title: Overlapping Error Correction Codes on Two-Dimensional Structures</h3>
<ul>
<li><strong>Authors: </strong>Andrew Rafael Fritsch, César Augusto Missio Marcon</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12142">https://arxiv.org/abs/2504.12142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12142">https://arxiv.org/pdf/2504.12142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12142]] Overlapping Error Correction Codes on Two-Dimensional Structures(https://arxiv.org/abs/2504.12142)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>The growing demand for highly reliable communication systems drives the research and development of algorithms that identify and correct errors during data transmission and storage. This need becomes even more critical in hard-to-access or sensitive systems, such as those used in space applications, passenger transportation, and the financial sector. In this context, Error Correction Codes (ECCs) are essential tools for ensuring a certain level of reliability. This work proposes a technique to enhance ECC error correction capability by overlapping data regions. The approach consists of protecting the same data area with multiple ECCs organized in a two-dimensional structure, enabling logical inferences that correlate the codes and improve their error detection and correction capabilities. More specifically, the overlapping is characterized by the organization of multiple ECCs, whose intersection exclusively covers the entire data region. Different configurations of overlapping ECCs were analyzed to evaluate the proposal regarding error detection and correction capability, scalability, and reliability. Experimental results confirm the technique's effectiveness and demonstrate its high scalability potential, reducing the need for redundancy bits relative to the number of data bits. Furthermore, comparisons with state-of-the-art ECC approaches indicate the technique's applicability in critical systems that require high reliability.</li>
</ul>

<h3>Title: ARCeR: an Agentic RAG for the Automated Definition of Cyber Ranges</h3>
<ul>
<li><strong>Authors: </strong>Matteo Lupinacci, Francesco Blefari, Francesco Romeo, Francesco Aurelio Pironti, Angelo Furfaro</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12143">https://arxiv.org/abs/2504.12143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12143">https://arxiv.org/pdf/2504.12143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12143]] ARCeR: an Agentic RAG for the Automated Definition of Cyber Ranges(https://arxiv.org/abs/2504.12143)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The growing and evolving landscape of cybersecurity threats necessitates the development of supporting tools and platforms that allow for the creation of realistic IT environments operating within virtual, controlled settings as Cyber Ranges (CRs). CRs can be exploited for analyzing vulnerabilities and experimenting with the effectiveness of devised countermeasures, as well as serving as training environments for building cyber security skills and abilities for IT operators. This paper proposes ARCeR as an innovative solution for the automatic generation and deployment of CRs, starting from user-provided descriptions in a natural language. ARCeR relies on the Agentic RAG paradigm, which allows it to fully exploit state-of-art AI technologies. Experimental results show that ARCeR is able to successfully process prompts even in cases that LLMs or basic RAG systems are not able to cope with. Furthermore, ARCeR is able to target any CR framework provided that specific knowledge is made available to it.</li>
</ul>

<h3>Title: Towards Explainable Fusion and Balanced Learning in Multimodal Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Miaosen Luo, Yuncheng Jiang, Sijie Mai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12151">https://arxiv.org/abs/2504.12151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12151">https://arxiv.org/pdf/2504.12151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12151]] Towards Explainable Fusion and Balanced Learning in Multimodal Sentiment Analysis(https://arxiv.org/abs/2504.12151)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Multimodal Sentiment Analysis (MSA) faces two critical challenges: the lack of interpretability in the decision logic of multimodal fusion and modality imbalance caused by disparities in inter-modal information density. To address these issues, we propose KAN-MCP, a novel framework that integrates the interpretability of Kolmogorov-Arnold Networks (KAN) with the robustness of the Multimodal Clean Pareto (MCPareto) framework. First, KAN leverages its univariate function decomposition to achieve transparent analysis of cross-modal interactions. This structural design allows direct inspection of feature transformations without relying on external interpretation tools, thereby ensuring both high expressiveness and interpretability. Second, the proposed MCPareto enhances robustness by addressing modality imbalance and noise interference. Specifically, we introduce the Dimensionality Reduction and Denoising Modal Information Bottleneck (DRD-MIB) method, which jointly denoises and reduces feature dimensionality. This approach provides KAN with discriminative low-dimensional inputs to reduce the modeling complexity of KAN while preserving critical sentiment-related information. Furthermore, MCPareto dynamically balances gradient contributions across modalities using the purified features output by DRD-MIB, ensuring lossless transmission of auxiliary signals and effectively alleviating modality imbalance. This synergy of interpretability and robustness not only achieves superior performance on benchmark datasets such as CMU-MOSI, CMU-MOSEI, and CH-SIMS v2 but also offers an intuitive visualization interface through KAN's interpretable architecture.</li>
</ul>

<h3>Title: CodingHomo: Bootstrapping Deep Homography With Video Coding</h3>
<ul>
<li><strong>Authors: </strong>Yike Liu, Haipeng Li, Shuaicheng Liu, Bing Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12165">https://arxiv.org/abs/2504.12165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12165">https://arxiv.org/pdf/2504.12165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12165]] CodingHomo: Bootstrapping Deep Homography With Video Coding(https://arxiv.org/abs/2504.12165)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Homography estimation is a fundamental task in computer vision with applications in diverse fields. Recent advances in deep learning have improved homography estimation, particularly with unsupervised learning approaches, offering increased robustness and generalizability. However, accurately predicting homography, especially in complex motions, remains a challenge. In response, this work introduces a novel method leveraging video coding, particularly by harnessing inherent motion vectors (MVs) present in videos. We present CodingHomo, an unsupervised framework for homography estimation. Our framework features a Mask-Guided Fusion (MGF) module that identifies and utilizes beneficial features among the MVs, thereby enhancing the accuracy of homography prediction. Additionally, the Mask-Guided Homography Estimation (MGHE) module is presented for eliminating undesired features in the coarse-to-fine homography refinement process. CodingHomo outperforms existing state-of-the-art unsupervised methods, delivering good robustness and generalizability. The code and dataset are available at: \href{github}{this https URL</li>
</ul>

<h3>Title: RADLER: Radar Object Detection Leveraging Semantic 3D City Models and Self-Supervised Radar-Image Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuan Luo, Rudolf Hoffmann, Yan Xia, Olaf Wysocki, Benedikt Schwab, Thomas H. Kolbe, Daniel Cremers</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12167">https://arxiv.org/abs/2504.12167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12167">https://arxiv.org/pdf/2504.12167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12167]] RADLER: Radar Object Detection Leveraging Semantic 3D City Models and Self-Supervised Radar-Image Learning(https://arxiv.org/abs/2504.12167)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Semantic 3D city models are worldwide easy-accessible, providing accurate, object-oriented, and semantic-rich 3D priors. To date, their potential to mitigate the noise impact on radar object detection remains under-explored. In this paper, we first introduce a unique dataset, RadarCity, comprising 54K synchronized radar-image pairs and semantic 3D city models. Moreover, we propose a novel neural network, RADLER, leveraging the effectiveness of contrastive self-supervised learning (SSL) and semantic 3D city models to enhance radar object detection of pedestrians, cyclists, and cars. Specifically, we first obtain the robust radar features via a SSL network in the radar-image pretext task. We then use a simple yet effective feature fusion strategy to incorporate semantic-depth features from semantic 3D city models. Having prior 3D information as guidance, RADLER obtains more fine-grained details to enhance radar object detection. We extensively evaluate RADLER on the collected RadarCity dataset and demonstrate average improvements of 5.46% in mean avarage precision (mAP) and 3.51% in mean avarage recall (mAR) over previous radar object detection methods. We believe this work will foster further research on semantic-guided and map-supported radar object detection. Our project page is publicly available athttps://gppthis http URL .</li>
</ul>

<h3>Title: Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube</h3>
<ul>
<li><strong>Authors: </strong>Victor Manuel Hernandez Lopez, Jaime E. Cuellar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12177">https://arxiv.org/abs/2504.12177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12177">https://arxiv.org/pdf/2504.12177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12177]] Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube(https://arxiv.org/abs/2504.12177)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, transformer</a></li>
<li><strong>Abstract: </strong>This article analyzes the Hamas-Israel controversy through 253,925 Spanish-language YouTube comments posted between October 2023 and January 2024, following the October 7 attack that escalated the conflict. Adopting an interdisciplinary approach, the study combines the analysis of controversies from Science and Technology Studies (STS) with advanced computational methodologies, specifically Natural Language Processing (NLP) using the BERT (Bidirectional Encoder Representations from Transformers) model. Using this approach, the comments were automatically classified into seven categories, reflecting pro-Palestinian, pro-Israeli, anti- Palestinian, anti-Israeli positions, among others. The results show a predominance of pro- Palestinian comments, although pro-Israeli and anti-Palestinian comments received more "likes." This study also applies the agenda-setting theory to demonstrate how media coverage significantly influences public perception, observing a notable shift in public opinion, transitioning from a pro- Palestinian stance to a more critical position towards Israel. This work highlights the importance of combining social science perspectives with technological tools in the analysis of controversies, presenting a methodological innovation by integrating computational analysis with critical social theories to address complex public opinion phenomena and media narratives.</li>
</ul>

<h3>Title: Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification</h3>
<ul>
<li><strong>Authors: </strong>Jaime E. Cuellar, Oscar Moreno-Martinez, Paula Sofia Torres-Rodriguez, Jaime Andres Pavlich-Mariscal, Andres Felipe Mican-Castiblanco, Juan Guillermo Torres-Hurtado</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12180">https://arxiv.org/abs/2504.12180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12180">https://arxiv.org/pdf/2504.12180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12180]] Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification(https://arxiv.org/abs/2504.12180)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>One fundamental question for the social sciences today is: how much can we trust highly complex predictive models like ChatGPT? This study tests the hypothesis that subtle changes in the structure of prompts do not produce significant variations in the classification results of sentiment polarity analysis generated by the Large Language Model GPT-4o mini. Using a dataset of 100.000 comments in Spanish on four Latin American presidents, the model classified the comments as positive, negative, or neutral on 10 occasions, varying the prompts slightly each time. The experimental methodology included exploratory and confirmatory analyses to identify significant discrepancies among classifications. The results reveal that even minor modifications to prompts such as lexical, syntactic, or modal changes, or even their lack of structure impact the classifications. In certain cases, the model produced inconsistent responses, such as mixing categories, providing unsolicited explanations, or using languages other than Spanish. Statistical analysis using Chi-square tests confirmed significant differences in most comparisons between prompts, except in one case where linguistic structures were highly similar. These findings challenge the robustness and trust of Large Language Models for classification tasks, highlighting their vulnerability to variations in instructions. Moreover, it was evident that the lack of structured grammar in prompts increases the frequency of hallucinations. The discussion underscores that trust in Large Language Models is based not only on technical performance but also on the social and institutional relationships underpinning their use.</li>
</ul>

<h3>Title: Battery-aware Cyclic Scheduling in Energy-harvesting Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Eunjeong Jeong, Nikolaos Pappas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12181">https://arxiv.org/abs/2504.12181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12181">https://arxiv.org/pdf/2504.12181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12181]] Battery-aware Cyclic Scheduling in Energy-harvesting Federated Learning(https://arxiv.org/abs/2504.12181)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has emerged as a promising framework for distributed learning, but its growing complexity has led to significant energy consumption, particularly from computations on the client side. This challenge is especially critical in energy-harvesting FL (EHFL) systems, where device availability fluctuates due to limited and time-varying energy resources. We propose FedBacys, a battery-aware FL framework that introduces cyclic client participation based on users' battery levels to cope with these issues. FedBacys enables clients to save energy and strategically perform local training just before their designated transmission time by clustering clients and scheduling their involvement sequentially. This design minimizes redundant computation, reduces system-wide energy usage, and improves learning stability. Our experiments demonstrate that FedBacys outperforms existing approaches in terms of energy efficiency and performance consistency, exhibiting robustness even under non-i.i.d. training data distributions and with very infrequent battery charging. This work presents the first comprehensive evaluation of cyclic client participation in EHFL, incorporating both communication and computation costs into a unified, resource-aware scheduling strategy.</li>
</ul>

<h3>Title: SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data</h3>
<ul>
<li><strong>Authors: </strong>Suyoung Bae, Hyojun Kim, YunSeok Choi, Jee-Hyong Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12185">https://arxiv.org/abs/2504.12185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12185">https://arxiv.org/pdf/2504.12185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12185]] SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data(https://arxiv.org/abs/2504.12185)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In various natural language processing (NLP) tasks, fine-tuning Pre-trained Language Models (PLMs) often leads to the issue of spurious correlations, which negatively impacts performance, particularly when dealing with out-of-distribution data. To address this problem, we propose SALAD}(Structure Aware and LLM-driven Augmented Data), a novel approach designed to enhance model robustness and generalization by generating structure-aware and counterfactually augmented data for contrastive learning. Our method leverages a tagging-based approach to generate structure-aware positive samples and utilizes large language models (LLMs) to generate counterfactual negative samples with diverse sentence patterns. By applying contrastive learning, SALAD enables the model to focus on learning the structural relationships between key sentence components while minimizing reliance on spurious correlations. We validate our approach through experiments on three tasks: Sentiment Classification, Sexism Detection, and Natural Language Inference. The results demonstrate that SALAD not only improves model robustness and performance across different environments but also enhances generalization to out-of-distribution datasets and cross-domain scenarios.</li>
</ul>

<h3>Title: What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure</h3>
<ul>
<li><strong>Authors: </strong>Céline Budding</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12187">https://arxiv.org/abs/2504.12187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12187">https://arxiv.org/pdf/2504.12187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12187]] What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure(https://arxiv.org/abs/2504.12187)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>It is sometimes assumed that Large Language Models (LLMs) know language, or for example that they know that Paris is the capital of France. But what -- if anything -- do LLMs actually know? In this paper, I argue that LLMs can acquire tacit knowledge as defined by Martin Davies (1990). Whereas Davies himself denies that neural networks can acquire tacit knowledge, I demonstrate that certain architectural features of LLMs satisfy the constraints of semantic description, syntactic structure, and causal systematicity. Thus, tacit knowledge may serve as a conceptual framework for describing, explaining, and intervening on LLMs and their behavior.</li>
</ul>

<h3>Title: Beyond Patches: Mining Interpretable Part-Prototypes for Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Alehdaghi, Rajarshi Bhattacharya, Pourya Shamsolmoali, Rafael M.O. Cruz, Maguelonne Heritier, Eric Granger</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12197">https://arxiv.org/abs/2504.12197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12197">https://arxiv.org/pdf/2504.12197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12197]] Beyond Patches: Mining Interpretable Part-Prototypes for Explainable AI(https://arxiv.org/abs/2504.12197)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Deep learning has provided considerable advancements for multimedia systems, yet the interpretability of deep models remains a challenge. State-of-the-art post-hoc explainability methods, such as GradCAM, provide visual interpretation based on heatmaps but lack conceptual clarity. Prototype-based approaches, like ProtoPNet and PIPNet, offer a more structured explanation but rely on fixed patches, limiting their robustness and semantic consistency. To address these limitations, a part-prototypical concept mining network (PCMNet) is proposed that dynamically learns interpretable prototypes from meaningful regions. PCMNet clusters prototypes into concept groups, creating semantically grounded explanations without requiring additional annotations. Through a joint process of unsupervised part discovery and concept activation vector extraction, PCMNet effectively captures discriminative concepts and makes interpretable classification decisions. Our extensive experiments comparing PCMNet against state-of-the-art methods on multiple datasets show that it can provide a high level of interpretability, stability, and robustness under clean and occluded scenarios.</li>
</ul>

<h3>Title: Uncertainty-Guided Coarse-to-Fine Tumor Segmentation with Anatomy-Aware Post-Processing</h3>
<ul>
<li><strong>Authors: </strong>Ilkin Sevgi Isler, David Mohaisen, Curtis Lisle, Damla Turgut, Ulas Bagci</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12215">https://arxiv.org/abs/2504.12215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12215">https://arxiv.org/pdf/2504.12215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12215]] Uncertainty-Guided Coarse-to-Fine Tumor Segmentation with Anatomy-Aware Post-Processing(https://arxiv.org/abs/2504.12215)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Reliable tumor segmentation in thoracic computed tomography (CT) remains challenging due to boundary ambiguity, class imbalance, and anatomical variability. We propose an uncertainty-guided, coarse-to-fine segmentation framework that combines full-volume tumor localization with refined region-of-interest (ROI) segmentation, enhanced by anatomically aware post-processing. The first-stage model generates a coarse prediction, followed by anatomically informed filtering based on lung overlap, proximity to lung surfaces, and component size. The resulting ROIs are segmented by a second-stage model trained with uncertainty-aware loss functions to improve accuracy and boundary calibration in ambiguous regions. Experiments on private and public datasets demonstrate improvements in Dice and Hausdorff scores, with fewer false positives and enhanced spatial interpretability. These results highlight the value of combining uncertainty modeling and anatomical priors in cascaded segmentation pipelines for robust and clinically meaningful tumor delineation. On the Orlando dataset, our framework improved Swin UNETR Dice from 0.4690 to 0.6447. Reduction in spurious components was strongly correlated with segmentation gains, underscoring the value of anatomically informed post-processing.</li>
</ul>

<h3>Title: d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Siyan Zhao, Devaansh Gupta, Qinqing Zheng, Aditya Grover</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12216">https://arxiv.org/abs/2504.12216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12216">https://arxiv.org/pdf/2504.12216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12216]] d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning(https://arxiv.org/abs/2504.12216)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and logical reasoning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM.</li>
</ul>

<h3>Title: zkVC: Fast Zero-Knowledge Proof for Private and Verifiable Computing</h3>
<ul>
<li><strong>Authors: </strong>Yancheng Zhang, Mengxin Zheng, Xun Chen, Jingtong Hu, Weidong Shi, Lei Ju, Yan Solihin, Qian Lou</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12217">https://arxiv.org/abs/2504.12217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12217">https://arxiv.org/pdf/2504.12217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12217]] zkVC: Fast Zero-Knowledge Proof for Private and Verifiable Computing(https://arxiv.org/abs/2504.12217)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>In the context of cloud computing, services are held on cloud servers, where the clients send their data to the server and obtain the results returned by server. However, the computation, data and results are prone to tampering due to the vulnerabilities on the server side. Thus, verifying the integrity of computation is important in the client-server setting. The cryptographic method known as Zero-Knowledge Proof (ZKP) is renowned for facilitating private and verifiable computing. ZKP allows the client to validate that the results from the server are computed correctly without violating the privacy of the server's intellectual property. Zero-Knowledge Succinct Non-Interactive Argument of Knowledge (zkSNARKs), in particular, has been widely applied in various applications like blockchain and verifiable machine learning. Despite their popularity, existing zkSNARKs approaches remain highly computationally intensive. For instance, even basic operations like matrix multiplication require an extensive number of constraints, resulting in significant overhead. In addressing this challenge, we introduce \textit{zkVC}, which optimizes the ZKP computation for matrix multiplication, enabling rapid proof generation on the server side and efficient verification on the client side. zkVC integrates optimized ZKP modules, such as Constraint-reduced Polynomial Circuit (CRPC) and Prefix-Sum Query (PSQ), collectively yielding a more than 12-fold increase in proof speed over prior methods. The code is available at this https URL</li>
</ul>

<h3>Title: Accountable Liveness</h3>
<ul>
<li><strong>Authors: </strong>Andrew Lewis-Pye, Joachim Neu, Tim Roughgarden, Luca Zanolini</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12218">https://arxiv.org/abs/2504.12218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12218">https://arxiv.org/pdf/2504.12218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12218]] Accountable Liveness(https://arxiv.org/abs/2504.12218)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Safety and liveness are the two classical security properties of consensus protocols. Recent works have strengthened safety with accountability: should any safety violation occur, a sizable fraction of adversary nodes can be proven to be protocol violators. This paper studies to what extent analogous accountability guarantees are achievable for liveness. To reveal the full complexity of this question, we introduce an interpolation between the classical synchronous and partially-synchronous models that we call the $x$-partially-synchronous network model in which, intuitively, at most an $x$ fraction of the time steps in any sufficiently long interval are asynchronous (and, as with a partially-synchronous network, all time steps are synchronous following the passage of an unknown "global stablization time"). We prove a precise characterization of the parameter regime in which accountable liveness is achievable: if and only if $x < 1/2$ and $f < n/2$, where $n$ denotes the number of nodes and $f$ the number of nodes controlled by an adversary. We further refine the problem statement and our analysis by parameterizing by the number of violating nodes identified following a liveness violation, and provide evidence that the guarantees achieved by our protocol are near-optimal (as a function of $x$ and $f$). Our results provide rigorous foundations for liveness-accountability heuristics such as the "inactivity leaks" employed in Ethereum.</li>
</ul>

<h3>Title: Coding-Prior Guided Diffusion Network for Video Deblurring</h3>
<ul>
<li><strong>Authors: </strong>Yike Liu, Jianhui Zhang, Haipeng Li, Shuaicheng Liu, Bing Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12222">https://arxiv.org/abs/2504.12222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12222">https://arxiv.org/pdf/2504.12222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12222]] Coding-Prior Guided Diffusion Network for Video Deblurring(https://arxiv.org/abs/2504.12222)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>While recent video deblurring methods have advanced significantly, they often overlook two valuable prior information: (1) motion vectors (MVs) and coding residuals (CRs) from video codecs, which provide efficient inter-frame alignment cues, and (2) the rich real-world knowledge embedded in pre-trained diffusion generative models. We present CPGDNet, a novel two-stage framework that effectively leverages both coding priors and generative diffusion priors for high-quality deblurring. First, our coding-prior feature propagation (CPFP) module utilizes MVs for efficient frame alignment and CRs to generate attention masks, addressing motion inaccuracies and texture variations. Second, a coding-prior controlled generation (CPC) module network integrates coding priors into a pretrained diffusion model, guiding it to enhance critical regions and synthesize realistic details. Experiments demonstrate our method achieves state-of-the-art perceptual quality with up to 30% improvement in IQA metrics. Both the code and the codingprior-augmented dataset will be open-sourced.</li>
</ul>

<h3>Title: Watermarking Needs Input Repetition Masking</h3>
<ul>
<li><strong>Authors: </strong>David Khachaturov, Robert Mullins, Ilia Shumailov, Sumanth Dathathri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12229">https://arxiv.org/abs/2504.12229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12229">https://arxiv.org/pdf/2504.12229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12229]] Watermarking Needs Input Repetition Masking(https://arxiv.org/abs/2504.12229)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) raised concerns over potential misuse, such as for spreading misinformation. In response two counter measures emerged: machine learning-based detectors that predict if text is synthetic, and LLM watermarking, which subtly marks generated text for identification and attribution. Meanwhile, humans are known to adjust language to their conversational partners both syntactically and lexically. By implication, it is possible that humans or unwatermarked LLMs could unintentionally mimic properties of LLM generated text, making counter measures unreliable. In this work we investigate the extent to which such conversational adaptation happens. We call the concept $\textit{mimicry}$ and demonstrate that both humans and LLMs end up mimicking, including the watermarking signal even in seemingly improbable settings. This challenges current academic assumptions and suggests that for long-term watermarking to be reliable, the likelihood of false positives needs to be significantly lower, while longer word sequences should be used for seeding watermarking mechanisms.</li>
</ul>

<h3>Title: Cobra: Efficient Line Art COlorization with BRoAder References</h3>
<ul>
<li><strong>Authors: </strong>Junhao Zhuang, Lingen Li, Xuan Ju, Zhaoyang Zhang, Chun Yuan, Ying Shan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12240">https://arxiv.org/abs/2504.12240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12240">https://arxiv.org/pdf/2504.12240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12240]] Cobra: Efficient Line Art COlorization with BRoAder References(https://arxiv.org/abs/2504.12240)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: this https URL.</li>
</ul>

<h3>Title: SIDME: Self-supervised Image Demoiréing via Masked Encoder-Decoder Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Xia Wang, Haiyang Sun, Tiantian Cao, Yueying Sun, Min Feng</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12245">https://arxiv.org/abs/2504.12245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12245">https://arxiv.org/pdf/2504.12245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12245]] SIDME: Self-supervised Image Demoiréing via Masked Encoder-Decoder Reconstruction(https://arxiv.org/abs/2504.12245)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Moiré patterns, resulting from aliasing between object light signals and camera sampling frequencies, often degrade image quality during capture. Traditional demoiréing methods have generally treated images as a whole for processing and training, neglecting the unique signal characteristics of different color channels. Moreover, the randomness and variability of moiré pattern generation pose challenges to the robustness of existing methods when applied to real-world data. To address these issues, this paper presents SIDME (Self-supervised Image Demoiréing via Masked Encoder-Decoder Reconstruction), a novel model designed to generate high-quality visual images by effectively processing moiré patterns. SIDME combines a masked encoder-decoder architecture with self-supervised learning, allowing the model to reconstruct images using the inherent properties of camera sampling frequencies. A key innovation is the random masked image reconstructor, which utilizes an encoder-decoder structure to handle the reconstruction task. Furthermore, since the green channel in camera sampling has a higher sampling frequency compared to red and blue channels, a specialized self-supervised loss function is designed to improve the training efficiency and effectiveness. To ensure the generalization ability of the model, a self-supervised moiré image generation method has been developed to produce a dataset that closely mimics real-world conditions. Extensive experiments demonstrate that SIDME outperforms existing methods in processing real moiré pattern data, showing its superior generalization performance and robustness.</li>
</ul>

<h3>Title: Human Aligned Compression for Robust Models</h3>
<ul>
<li><strong>Authors: </strong>Samuel Räber, Andreas Plesner, Till Aczel, Roger Wattenhofer</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12255">https://arxiv.org/abs/2504.12255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12255">https://arxiv.org/pdf/2504.12255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12255]] Human Aligned Compression for Robust Models(https://arxiv.org/abs/2504.12255)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Adversarial attacks on image models threaten system robustness by introducing imperceptible perturbations that cause incorrect predictions. We investigate human-aligned learned lossy compression as a defense mechanism, comparing two learned models (HiFiC and ELIC) against traditional JPEG across various quality levels. Our experiments on ImageNet subsets demonstrate that learned compression methods outperform JPEG, particularly for Vision Transformer architectures, by preserving semantically meaningful content while removing adversarial noise. Even in white-box settings where attackers can access the defense, these methods maintain substantial effectiveness. We also show that sequential compression--applying rounds of compression/decompression--significantly enhances defense efficacy while maintaining classification performance. Our findings reveal that human-aligned compression provides an effective, computationally efficient defense that protects the image features most relevant to human and machine understanding. It offers a practical approach to improving model robustness against adversarial threats.</li>
</ul>

<h3>Title: FLIP Reasoning Challenge</h3>
<ul>
<li><strong>Authors: </strong>Andreas Plesner, Turlan Kuzhagaliyev, Roger Wattenhofer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12256">https://arxiv.org/abs/2504.12256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12256">https://arxiv.org/pdf/2504.12256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12256]] FLIP Reasoning Challenge(https://arxiv.org/abs/2504.12256)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Over the past years, advances in artificial intelligence (AI) have demonstrated how AI can solve many perception and generation tasks, such as image classification and text writing, yet reasoning remains a challenge. This paper introduces the FLIP dataset, a benchmark for evaluating AI reasoning capabilities based on human verification tasks on the Idena blockchain. FLIP challenges present users with two orderings of 4 images, requiring them to identify the logically coherent one. By emphasizing sequential reasoning, visual storytelling, and common sense, FLIP provides a unique testbed for multimodal AI systems. Our experiments evaluate state-of-the-art models, leveraging both vision-language models (VLMs) and large language models (LLMs). Results reveal that even the best open-sourced and closed-sourced models achieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot settings, compared to human performance of 95.3%. Captioning models aid reasoning models by providing text descriptions of images, yielding better results than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5 Pro. Combining the predictions from 15 models in an ensemble increases the accuracy to 85.2%. These findings highlight the limitations of existing reasoning models and the need for robust multimodal benchmarks like FLIP. The full codebase and dataset will be available at this https URL.</li>
</ul>

<h3>Title: VGDFR: Diffusion-based Video Generation with Dynamic Latent Frame Rate</h3>
<ul>
<li><strong>Authors: </strong>Zhihang Yuan, Rui Xie, Yuzhang Shang, Hanling Zhang, Siyuan Wang, Shengen Yan, Guohao Dai, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12259">https://arxiv.org/abs/2504.12259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12259">https://arxiv.org/pdf/2504.12259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12259]] VGDFR: Diffusion-based Video Generation with Dynamic Latent Frame Rate(https://arxiv.org/abs/2504.12259)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion Transformer(DiT)-based generation models have achieved remarkable success in video generation. However, their inherent computational demands pose significant efficiency challenges. In this paper, we exploit the inherent temporal non-uniformity of real-world videos and observe that videos exhibit dynamic information density, with high-motion segments demanding greater detail preservation than static scenes. Inspired by this temporal non-uniformity, we propose VGDFR, a training-free approach for Diffusion-based Video Generation with Dynamic Latent Frame Rate. VGDFR adaptively adjusts the number of elements in latent space based on the motion frequency of the latent space content, using fewer tokens for low-frequency segments while preserving detail in high-frequency segments. Specifically, our key contributions are: (1) A dynamic frame rate scheduler for DiT video generation that adaptively assigns frame rates for video segments. (2) A novel latent-space frame merging method to align latent representations with their denoised counterparts before merging those redundant in low-resolution space. (3) A preference analysis of Rotary Positional Embeddings (RoPE) across DiT layers, informing a tailored RoPE strategy optimized for semantic and local information capture. Experiments show that VGDFR can achieve a speedup up to 3x for video generation with minimal quality degradation.</li>
</ul>

<h3>Title: SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields</h3>
<ul>
<li><strong>Authors: </strong>David Keetae Park, Xihaier Luo, Guang Zhao, Seungjun Lee, Miruna Oprescu, Shinjae Yoo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12262">https://arxiv.org/abs/2504.12262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12262">https://arxiv.org/pdf/2504.12262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12262]] SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields(https://arxiv.org/abs/2504.12262)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Spatiotemporal learning is challenging due to the intricate interplay between spatial and temporal dependencies, the high dimensionality of the data, and scalability constraints. These challenges are further amplified in scientific domains, where data is often irregularly distributed (e.g., missing values from sensor failures) and high-volume (e.g., high-fidelity simulations), posing additional computational and modeling difficulties. In this paper, we present SCENT, a novel framework for scalable and continuity-informed spatiotemporal representation learning. SCENT unifies interpolation, reconstruction, and forecasting within a single architecture. Built on a transformer-based encoder-processor-decoder backbone, SCENT introduces learnable queries to enhance generalization and a query-wise cross-attention mechanism to effectively capture multi-scale dependencies. To ensure scalability in both data size and model complexity, we incorporate a sparse attention mechanism, enabling flexible output representations and efficient evaluation at arbitrary resolutions. We validate SCENT through extensive simulations and real-world experiments, demonstrating state-of-the-art performance across multiple challenging tasks while achieving superior scalability.</li>
</ul>

<h3>Title: How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions</h3>
<ul>
<li><strong>Authors: </strong>Aditya Prakash, Benjamin Lundell, Dmitry Andreychuk, David Forsyth, Saurabh Gupta, Harpreet Sawhney</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12284">https://arxiv.org/abs/2504.12284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12284">https://arxiv.org/pdf/2504.12284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12284]] How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions(https://arxiv.org/abs/2504.12284)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We tackle the novel problem of predicting 3D hand motion and contact maps (or Interaction Trajectories) given a single RGB view, action text, and a 3D contact point on the object as input. Our approach consists of (1) Interaction Codebook: a VQVAE model to learn a latent codebook of hand poses and contact points, effectively tokenizing interaction trajectories, (2) Interaction Predictor: a transformer-decoder module to predict the interaction trajectory from test time inputs by using an indexer module to retrieve a latent affordance from the learned codebook. To train our model, we develop a data engine that extracts 3D hand poses and contact trajectories from the diverse HoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger than existing works, in terms of diversity of objects and interactions observed, and test for generalization of the model across object categories, action categories, tasks, and scenes. Experimental results show the effectiveness of our approach over transformer & diffusion baselines across all settings.</li>
</ul>

<h3>Title: BitNet b1.58 2B4T Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Shuming Ma, Hongyu Wang, Shaohan Huang, Xingxing Zhang, Ying Hu, Ting Song, Yan Xia, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.12285">https://arxiv.org/abs/2504.12285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.12285">https://arxiv.org/pdf/2504.12285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.12285]] BitNet b1.58 2B4T Technical Report(https://arxiv.org/abs/2504.12285)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
