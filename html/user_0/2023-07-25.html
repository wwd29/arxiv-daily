<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Towards Vertical Privacy-Preserving Symbolic Regression via Secure Multiparty Computation. (arXiv:2307.11756v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11756">http://arxiv.org/abs/2307.11756</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11756] Towards Vertical Privacy-Preserving Symbolic Regression via Secure Multiparty Computation](http://arxiv.org/abs/2307.11756) #secure</code></li>
<li>Summary: <p>Symbolic Regression is a powerful data-driven technique that searches for
mathematical expressions that explain the relationship between input variables
and a target of interest. Due to its efficiency and flexibility, Genetic
Programming can be seen as the standard search technique for Symbolic
Regression. However, the conventional Genetic Programming algorithm requires
storing all data in a central location, which is not always feasible due to
growing concerns about data privacy and security. While privacy-preserving
research has advanced recently and might offer a solution to this problem,
their application to Symbolic Regression remains largely unexplored.
Furthermore, the existing work only focuses on the horizontally partitioned
setting, whereas the vertically partitioned setting, another popular scenario,
has yet to be investigated. Herein, we propose an approach that employs a
privacy-preserving technique called Secure Multiparty Computation to enable
parties to jointly build Symbolic Regression models in the vertical scenario
without revealing private data. Preliminary experimental results indicate that
our proposed method delivers comparable performance to the centralized solution
while safeguarding data privacy.
</p></li>
</ul>

<h3>Title: Verifiable Sustainability in Data Centers. (arXiv:2307.11993v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11993">http://arxiv.org/abs/2307.11993</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11993] Verifiable Sustainability in Data Centers](http://arxiv.org/abs/2307.11993) #secure</code></li>
<li>Summary: <p>Sustainability is crucial for combating climate change and protecting our
planet. While there are various systems that can pose a threat to
sustainability, data centers are particularly significant due to their
substantial energy consumption and environmental impact. Although data centers
are becoming increasingly accountable to be sustainable, the current practice
of reporting sustainability data is often mired with simple green-washing. To
improve this status quo, users as well as regulators need to verify the data on
the sustainability impact reported by data center operators. To do so, data
centers must have appropriate infrastructures in place that provide the
guarantee that the data on sustainability is collected, stored, aggregated, and
converted to metrics in a secure, unforgeable, and privacy-preserving manner.
Therefore, this paper first introduces the new security challenges related to
such infrastructure, how it affects operators and users, and potential
solutions and research directions for addressing the challenges for data
centers and other industry segments.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Exploring Security Commits in Python. (arXiv:2307.11853v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11853">http://arxiv.org/abs/2307.11853</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11853] Exploring Security Commits in Python](http://arxiv.org/abs/2307.11853) #security</code></li>
<li>Summary: <p>Python has become the most popular programming language as it is friendly to
work with for beginners. However, a recent study has found that most security
issues in Python have not been indexed by CVE and may only be fixed by 'silent'
security commits, which pose a threat to software security and hinder the
security fixes to downstream software. It is critical to identify the hidden
security commits; however, the existing datasets and methods are insufficient
for security commit detection in Python, due to the limited data variety,
non-comprehensive code semantics, and uninterpretable learned features. In this
paper, we construct the first security commit dataset in Python, namely
PySecDB, which consists of three subsets including a base dataset, a pilot
dataset, and an augmented dataset. The base dataset contains the security
commits associated with CVE records provided by MITRE. To increase the variety
of security commits, we build the pilot dataset from GitHub by filtering
keywords within the commit messages. Since not all commits provide commit
messages, we further construct the augmented dataset by understanding the
semantics of code changes. To build the augmented dataset, we propose a new
graph representation named CommitCPG and a multi-attributed graph learning
model named SCOPY to identify the security commit candidates through both
sequential and structural code semantics. The evaluation shows our proposed
algorithms can improve the data collection efficiency by up to 40 percentage
points. After manual verification by three security experts, PySecDB consists
of 1,258 security commits and 2,791 non-security commits. Furthermore, we
conduct an extensive case study on PySecDB and discover four common security
fix patterns that cover over 85% of security commits in Python, providing
insight into secure software maintenance, vulnerability detection, and
automated program repair.
</p></li>
</ul>

<h3>Title: Augmented Symbolic Execution for Information Flow in Hardware Designs. (arXiv:2307.11884v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11884">http://arxiv.org/abs/2307.11884</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11884] Augmented Symbolic Execution for Information Flow in Hardware Designs](http://arxiv.org/abs/2307.11884) #security</code></li>
<li>Summary: <p>We present SEIF, a methodology that combines static analysis with symbolic
execution to verify and explicate information flow paths in a hardware design.
SEIF begins with a statically built model of the information flow through a
design and uses guided symbolic execution to recognize and eliminate non-flows
with high precision or to find corresponding paths through the design state for
true flows. We evaluate SEIF on two open-source CPUs, an AES core, and the AKER
access control module. SEIF can exhaustively explore 10-12 clock cycles deep in
4-6 seconds on average, and can automatically account for 86-90% of the paths
in the statically built model. Additionally, SEIF can be used to find multiple
violating paths for security properties, providing a new angle for security
verification.
</p></li>
</ul>

<h3>Title: Security and Privacy Issues of Federated Learning. (arXiv:2307.12181v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12181">http://arxiv.org/abs/2307.12181</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12181] Security and Privacy Issues of Federated Learning](http://arxiv.org/abs/2307.12181) #security</code></li>
<li>Summary: <p>Federated Learning (FL) has emerged as a promising approach to address data
privacy and confidentiality concerns by allowing multiple participants to
construct a shared model without centralizing sensitive data. However, this
decentralized paradigm introduces new security challenges, necessitating a
comprehensive identification and classification of potential risks to ensure
FL's security guarantees. This paper presents a comprehensive taxonomy of
security and privacy challenges in Federated Learning (FL) across various
machine learning models, including large language models. We specifically
categorize attacks performed by the aggregator and participants, focusing on
poisoning attacks, backdoor attacks, membership inference attacks, generative
adversarial network (GAN) based attacks, and differential privacy attacks.
Additionally, we propose new directions for future research, seeking innovative
solutions to fortify FL systems against emerging security risks and uphold
sensitive data confidentiality in distributed learning environments.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery. (arXiv:2307.12045v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12045">http://arxiv.org/abs/2307.12045</a></li>
<li>Code URL: <a href="https://github.com/longbai1006/cs-vqla">https://github.com/longbai1006/cs-vqla</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12045] Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery](http://arxiv.org/abs/2307.12045) #privacy</code></li>
<li>Summary: <p>The visual-question localized-answering (VQLA) system can serve as a
knowledgeable assistant in surgical education. Except for providing text-based
answers, the VQLA system can highlight the interested region for better
surgical scene understanding. However, deep neural networks (DNNs) suffer from
catastrophic forgetting when learning new knowledge. Specifically, when DNNs
learn on incremental classes or tasks, their performance on old tasks drops
dramatically. Furthermore, due to medical data privacy and licensing issues, it
is often difficult to access old data when updating continual learning (CL)
models. Therefore, we develop a non-exemplar continual surgical VQLA framework,
to explore and balance the rigidity-plasticity trade-off of DNNs in a
sequential learning paradigm. We revisit the distillation loss in CL tasks, and
propose rigidity-plasticity-aware distillation (RP-Dist) and self-calibrated
heterogeneous distillation (SH-Dist) to preserve the old knowledge. The weight
aligning (WA) technique is also integrated to adjust the weight bias between
old and new tasks. We further establish a CL framework on three public surgical
datasets in the context of surgical settings that consist of overlapping
classes between old and new surgical VQLA tasks. With extensive experiments, we
demonstrate that our proposed method excellently reconciles learning and
forgetting on the continual surgical VQLA over conventional CL methods. Our
code is publicly accessible.
</p></li>
</ul>

<h3>Title: CryptoMask : Privacy-preserving Face Recognition. (arXiv:2307.12010v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12010">http://arxiv.org/abs/2307.12010</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12010] CryptoMask : Privacy-preserving Face Recognition](http://arxiv.org/abs/2307.12010) #privacy</code></li>
<li>Summary: <p>Face recognition is a widely-used technique for identification or
verification, where a verifier checks whether a face image matches anyone
stored in a database. However, in scenarios where the database is held by a
third party, such as a cloud server, both parties are concerned about data
privacy. To address this concern, we propose CryptoMask, a privacy-preserving
face recognition system that employs homomorphic encryption (HE) and secure
multi-party computation (MPC). We design a new encoding strategy that leverages
HE properties to reduce communication costs and enable efficient similarity
checks between face images, without expensive homomorphic rotation.
Additionally, CryptoMask leaks less information than existing state-of-the-art
approaches. CryptoMask only reveals whether there is an image matching the
query or not, whereas existing approaches additionally leak sensitive
intermediate distance information. We conduct extensive experiments that
demonstrate CryptoMask's superior performance in terms of computation and
communication. For a database with 100 million 512-dimensional face vectors,
CryptoMask offers ${\thicksim}5 \times$ and ${\thicksim}144 \times$ speed-ups
in terms of computation and communication, respectively.
</p></li>
</ul>

<h3>Title: Identifying contributors to supply chain outcomes in a multi-echelon setting: a decentralised approach. (arXiv:2307.12157v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12157">http://arxiv.org/abs/2307.12157</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12157] Identifying contributors to supply chain outcomes in a multi-echelon setting: a decentralised approach](http://arxiv.org/abs/2307.12157) #privacy</code></li>
<li>Summary: <p>Organisations often struggle to identify the causes of change in metrics such
as product quality and delivery duration. This task becomes increasingly
challenging when the cause lies outside of company borders in multi-echelon
supply chains that are only partially observable. Although traditional supply
chain management has advocated for data sharing to gain better insights, this
does not take place in practice due to data privacy concerns. We propose the
use of explainable artificial intelligence for decentralised computing of
estimated contributions to a metric of interest in a multi-stage production
process. This approach mitigates the need to convince supply chain actors to
share data, as all computations occur in a decentralised manner. Our method is
empirically validated using data collected from a real multi-stage
manufacturing process. The results demonstrate the effectiveness of our
approach in detecting the source of quality variations compared to a
centralised approach using Shapley additive explanations.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: An Empirical Study &amp; Evaluation of Modern CAPTCHAs. (arXiv:2307.12108v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12108">http://arxiv.org/abs/2307.12108</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12108] An Empirical Study &amp; Evaluation of Modern CAPTCHAs](http://arxiv.org/abs/2307.12108) #protect</code></li>
<li>Summary: <p>For nearly two decades, CAPTCHAs have been widely used as a means of
protection against bots. Throughout the years, as their use grew, techniques to
defeat or bypass CAPTCHAs have continued to improve. Meanwhile, CAPTCHAs have
also evolved in terms of sophistication and diversity, becoming increasingly
difficult to solve for both bots (machines) and humans. Given this
long-standing and still-ongoing arms race, it is critical to investigate how
long it takes legitimate users to solve modern CAPTCHAs, and how they are
perceived by those users.
</p></li>
</ul>

<p>In this work, we explore CAPTCHAs in the wild by evaluating users' solving
performance and perceptions of unmodified currently-deployed CAPTCHAs. We
obtain this data through manual inspection of popular websites and user studies
in which 1,400 participants collectively solved 14,000 CAPTCHAs. Results show
significant differences between the most popular types of CAPTCHAs:
surprisingly, solving time and user perception are not always correlated. We
performed a comparative study to investigate the effect of experimental context
-- specifically the difference between solving CAPTCHAs directly versus solving
them as part of a more natural task, such as account creation. Whilst there
were several potential confounding factors, our results show that experimental
context could have an impact on this task, and must be taken into account in
future CAPTCHA studies. Finally, we investigate CAPTCHA-induced user task
abandonment by analyzing participants who start and do not complete the task.
</p>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks. (arXiv:2307.11906v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11906">http://arxiv.org/abs/2307.11906</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11906] Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks](http://arxiv.org/abs/2307.11906) #attack</code></li>
<li>Summary: <p>Deep learning has been rapidly employed in many applications revolutionizing
many industries, but it is known to be vulnerable to adversarial attacks. Such
attacks pose a serious threat to deep learning-based systems compromising their
integrity, reliability, and trust. Interpretable Deep Learning Systems (IDLSes)
are designed to make the system more transparent and explainable, but they are
also shown to be susceptible to attacks. In this work, we propose a novel
microbial genetic algorithm-based black-box attack against IDLSes that requires
no prior knowledge of the target model and its interpretation model. The
proposed attack is a query-efficient approach that combines transfer-based and
score-based methods, making it a powerful tool to unveil IDLS vulnerabilities.
Our experiments of the attack show high attack success rates using adversarial
examples with attribution maps that are highly similar to those of benign
samples which makes it difficult to detect even by human analysts. Our results
highlight the need for improved IDLS security to ensure their practical
reliability.
</p></li>
</ul>

<h3>Title: Content Censorship in the InterPlanetary File System. (arXiv:2307.12212v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12212">http://arxiv.org/abs/2307.12212</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12212] Content Censorship in the InterPlanetary File System](http://arxiv.org/abs/2307.12212) #attack</code></li>
<li>Summary: <p>The InterPlanetary File System (IPFS) is currently the largest decentralized
storage solution in operation, with thousands of active participants and
millions of daily content transfers. IPFS is used as remote data storage for
numerous blockchain-based smart contracts, Non-Fungible Tokens (NFT), and
decentralized applications.
</p></li>
</ul>

<p>We present a content censorship attack that can be executed with minimal
effort and cost, and that prevents the retrieval of any chosen content in the
IPFS network. The attack exploits a conceptual issue in a core component of
IPFS, the Kademlia Distributed Hash Table (DHT), which is used to resolve
content IDs to peer addresses. We provide efficient detection and mitigation
mechanisms for this vulnerability. Our mechanisms achieve a 99.6\% detection
rate and mitigate 100\% of the detected attacks with minimal signaling and
computational overhead. We followed responsible disclosure procedures, and our
countermeasures are scheduled for deployment in the future versions of IPFS.
</p>

<h3>Title: Adversarial Agents For Attacking Inaudible Voice Activated Devices. (arXiv:2307.12204v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12204">http://arxiv.org/abs/2307.12204</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12204] Adversarial Agents For Attacking Inaudible Voice Activated Devices](http://arxiv.org/abs/2307.12204) #attack</code></li>
<li>Summary: <p>Our analysis of inaudible attacks on voice-activated devices confirms the
alarming risk factor of 7.6 out of 10, underlining significant security
vulnerabilities scored independently by NIST National Vulnerability Database
(NVD). Our baseline network model showcases a scenario in which an attacker
uses inaudible voice commands to gain unauthorized access to confidential
information on a secured laptop. We simulated many attack scenarios on this
baseline network model, revealing the potential for mass exploitation of
interconnected devices to discover and own privileged information through
physical access without adding new hardware or amplifying device skills. Using
Microsoft's CyberBattleSim framework, we evaluated six reinforcement learning
algorithms and found that Deep-Q learning with exploitation proved optimal,
leading to rapid ownership of all nodes in fewer steps. Our findings underscore
the critical need for understanding non-conventional networks and new
cybersecurity measures in an ever-expanding digital landscape, particularly
those characterized by mobile devices, voice activation, and non-linear
microphones susceptible to malicious actors operating stealth attacks in the
near-ultrasound or inaudible ranges. By 2024, this new attack surface might
encompass more digital voice assistants than people on the planet yet offer
fewer remedies than conventional patching or firmware fixes since the inaudible
attacks arise inherently from the microphone design and digital signal
processing.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness. (arXiv:2307.11823v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11823">http://arxiv.org/abs/2307.11823</a></li>
<li>Code URL: <a href="https://github.com/mkyucel/hybrid_augment">https://github.com/mkyucel/hybrid_augment</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11823] HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness](http://arxiv.org/abs/2307.11823) #robust</code></li>
<li>Summary: <p>Convolutional Neural Networks (CNN) are known to exhibit poor generalization
performance under distribution shifts. Their generalization have been studied
extensively, and one line of work approaches the problem from a
frequency-centric perspective. These studies highlight the fact that humans and
CNNs might focus on different frequency components of an image. First, inspired
by these observations, we propose a simple yet effective data augmentation
method HybridAugment that reduces the reliance of CNNs on high-frequency
components, and thus improves their robustness while keeping their clean
accuracy high. Second, we propose HybridAugment++, which is a hierarchical
augmentation method that attempts to unify various frequency-spectrum
augmentations. HybridAugment++ builds on HybridAugment, and also reduces the
reliance of CNNs on the amplitude component of images, and promotes phase
information instead. This unification results in competitive to or better than
state-of-the-art results on clean accuracy (CIFAR-10/100 and ImageNet),
corruption benchmarks (ImageNet-C, CIFAR-10-C and CIFAR-100-C), adversarial
robustness on CIFAR-10 and out-of-distribution detection on various datasets.
HybridAugment and HybridAugment++ are implemented in a few lines of code, does
not require extra data, ensemble models or additional networks.
</p></li>
</ul>

<h3>Title: RICo: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction. (arXiv:2307.11932v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11932">http://arxiv.org/abs/2307.11932</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11932] RICo: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction](http://arxiv.org/abs/2307.11932) #robust</code></li>
<li>Summary: <p>General scene reconstruction refers to the task of estimating the full 3D
geometry and texture of a scene containing previously unseen objects. In many
practical applications such as AR/VR, autonomous navigation, and robotics, only
a single view of the scene may be available, making the scene reconstruction a
very challenging task. In this paper, we present a method for scene
reconstruction by structurally breaking the problem into two steps: rendering
novel views via inpainting and 2D to 3D scene lifting. Specifically, we
leverage the generalization capability of large language models to inpaint the
missing areas of scene color images rendered from different views. Next, we
lift these inpainted images to 3D by predicting normals of the inpainted image
and solving for the missing depth values. By predicting for normals instead of
depth directly, our method allows for robustness to changes in depth
distributions and scale. With rigorous quantitative evaluation, we show that
our method outperforms multiple baselines while providing generalization to
novel objects and scenes.
</p></li>
</ul>

<h3>Title: LAMP: Leveraging Language Prompts for Multi-person Pose Estimation. (arXiv:2307.11934v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11934">http://arxiv.org/abs/2307.11934</a></li>
<li>Code URL: <a href="https://github.com/shengnanh20/lamp">https://github.com/shengnanh20/lamp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11934] LAMP: Leveraging Language Prompts for Multi-person Pose Estimation](http://arxiv.org/abs/2307.11934) #robust</code></li>
<li>Summary: <p>Human-centric visual understanding is an important desideratum for effective
human-robot interaction. In order to navigate crowded public places, social
robots must be able to interpret the activity of the surrounding humans. This
paper addresses one key aspect of human-centric visual understanding,
multi-person pose estimation. Achieving good performance on multi-person pose
estimation in crowded scenes is difficult due to the challenges of occluded
joints and instance separation. In order to tackle these challenges and
overcome the limitations of image features in representing invisible body
parts, we propose a novel prompt-based pose inference strategy called LAMP
(Language Assisted Multi-person Pose estimation). By utilizing the text
representations generated by a well-trained language model (CLIP), LAMP can
facilitate the understanding of poses on the instance and joint levels, and
learn more robust visual representations that are less susceptible to
occlusion. This paper demonstrates that language-supervised training boosts the
performance of single-stage multi-person pose estimation, and both
instance-level and joint-level prompts are valuable for training. The code is
available at https://github.com/shengnanh20/LAMP.
</p></li>
</ul>

<h3>Title: Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?. (arXiv:2307.11978v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11978">http://arxiv.org/abs/2307.11978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11978] Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?](http://arxiv.org/abs/2307.11978) #robust</code></li>
<li>Summary: <p>Vision-language models such as CLIP learn a generic text-image embedding from
large-scale training data. A vision-language model can be adapted to a new
classification task through few-shot prompt tuning. We find that such a prompt
tuning process is highly robust to label noises. This intrigues us to study the
key reasons contributing to the robustness of the prompt tuning paradigm. We
conducted extensive experiments to explore this property and find the key
factors are: 1) the fixed classname tokens provide a strong regularization to
the optimization of the model, reducing gradients induced by the noisy samples;
2) the powerful pre-trained image-text embedding that is learned from diverse
and generic web data provides strong prior knowledge for image classification.
Further, we demonstrate that noisy zero-shot predictions from CLIP can be used
to tune its own prompt, significantly enhancing prediction accuracy in the
unsupervised setting. The code is available at https://github.com/CEWu/PTNL.
</p></li>
</ul>

<h3>Title: DeepCL: Deep Change Feature Learning on Remote Sensing Images in the Metric Space. (arXiv:2307.12208v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12208">http://arxiv.org/abs/2307.12208</a></li>
<li>Code URL: <a href="https://github.com/haonanguo/deepcl">https://github.com/haonanguo/deepcl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12208] DeepCL: Deep Change Feature Learning on Remote Sensing Images in the Metric Space](http://arxiv.org/abs/2307.12208) #robust</code></li>
<li>Summary: <p>Change detection (CD) is an important yet challenging task in the Earth
observation field for monitoring Earth surface dynamics. The advent of deep
learning techniques has recently propelled automatic CD into a technological
revolution. Nevertheless, deep learning-based CD methods are still plagued by
two primary issues: 1) insufficient temporal relationship modeling and 2)
pseudo-change misclassification. To address these issues, we complement the
strong temporal modeling ability of metric learning with the prominent fitting
ability of segmentation and propose a deep change feature learning (DeepCL)
framework for robust and explainable CD. Firstly, we designed a hard
sample-aware contrastive loss, which reweights the importance of hard and
simple samples. This loss allows for explicit modeling of the temporal
correlation between bi-temporal remote sensing images. Furthermore, the modeled
temporal relations are utilized as knowledge prior to guide the segmentation
process for detecting change regions. The DeepCL framework is thoroughly
evaluated both theoretically and experimentally, demonstrating its superior
feature discriminability, resilience against pseudo changes, and adaptability
to a variety of CD algorithms. Extensive comparative experiments substantiate
the quantitative and qualitative superiority of DeepCL over state-of-the-art CD
approaches.
</p></li>
</ul>

<h3>Title: Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding. (arXiv:2307.12134v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12134">http://arxiv.org/abs/2307.12134</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12134] Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding](http://arxiv.org/abs/2307.12134) #robust</code></li>
<li>Summary: <p>End-to-end (E2E) spoken language understanding (SLU) systems that generate a
semantic parse from speech have become more promising recently. This approach
uses a single model that utilizes audio and text representations from
pre-trained speech recognition models (ASR), and outperforms traditional
pipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems
still show weakness when text representation quality is low due to ASR
transcription errors. To overcome this issue, we propose a novel E2E SLU system
that enhances robustness to ASR errors by fusing audio and text representations
based on the estimated modality confidence of ASR hypotheses. We introduce two
novel techniques: 1) an effective method to encode the quality of ASR
hypotheses and 2) an effective approach to integrate them into E2E SLU models.
We show accuracy improvements on STOP dataset and share the analysis to
demonstrate the effectiveness of our approach.
</p></li>
</ul>

<h3>Title: FATRER: Full-Attention Topic Regularizer for Accurate and Robust Conversational Emotion Recognition. (arXiv:2307.12221v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12221">http://arxiv.org/abs/2307.12221</a></li>
<li>Code URL: <a href="https://github.com/ludybupt/FATRER">https://github.com/ludybupt/FATRER</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12221] FATRER: Full-Attention Topic Regularizer for Accurate and Robust Conversational Emotion Recognition](http://arxiv.org/abs/2307.12221) #robust</code></li>
<li>Summary: <p>This paper concentrates on the understanding of interlocutors' emotions
evoked in conversational utterances. Previous studies in this literature mainly
focus on more accurate emotional predictions, while ignoring model robustness
when the local context is corrupted by adversarial attacks. To maintain
robustness while ensuring accuracy, we propose an emotion recognizer augmented
by a full-attention topic regularizer, which enables an emotion-related global
view when modeling the local context in a conversation. A joint topic modeling
strategy is introduced to implement regularization from both representation and
loss perspectives. To avoid over-regularization, we drop the constraints on
prior distributions that exist in traditional topic modeling and perform
probabilistic approximations based entirely on attention alignment. Experiments
show that our models obtain more favorable results than state-of-the-art
models, and gain convincing robustness under three types of adversarial
attacks.
</p></li>
</ul>

<h3>Title: HIQL: Offline Goal-Conditioned RL with Latent States as Actions. (arXiv:2307.11949v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11949">http://arxiv.org/abs/2307.11949</a></li>
<li>Code URL: <a href="https://github.com/seohongpark/hiql">https://github.com/seohongpark/hiql</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11949] HIQL: Offline Goal-Conditioned RL with Latent States as Actions](http://arxiv.org/abs/2307.11949) #robust</code></li>
<li>Summary: <p>Unsupervised pre-training has recently become the bedrock for computer vision
and natural language processing. In reinforcement learning (RL),
goal-conditioned RL can potentially provide an analogous self-supervised
approach for making use of large quantities of unlabeled (reward-free) data.
However, building effective algorithms for goal-conditioned RL that can learn
directly from diverse offline data is challenging, because it is hard to
accurately estimate the exact value function for faraway goals. Nonetheless,
goal-reaching problems exhibit structure, such that reaching distant goals
entails first passing through closer subgoals. This structure can be very
useful, as assessing the quality of actions for nearby goals is typically
easier than for more distant goals. Based on this idea, we propose a
hierarchical algorithm for goal-conditioned RL from offline data. Using one
action-free value function, we learn two policies that allow us to exploit this
structure: a high-level policy that treats states as actions and predicts (a
latent representation of) a subgoal and a low-level policy that predicts the
action for reaching this subgoal. Through analysis and didactic examples, we
show how this hierarchical decomposition makes our method robust to noise in
the estimated value function. We then apply our method to offline goal-reaching
benchmarks, showing that our method can solve long-horizon tasks that stymie
prior methods, can scale to high-dimensional image observations, and can
readily make use of action-free data. Our code is available at
https://seohong.me/projects/hiql/
</p></li>
</ul>

<h3>Title: Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations. (arXiv:2307.12062v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12062">http://arxiv.org/abs/2307.12062</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12062] Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations](http://arxiv.org/abs/2307.12062) #robust</code></li>
<li>Summary: <p>Robust reinforcement learning (RL) seeks to train policies that can perform
well under environment perturbations or adversarial attacks. Existing
approaches typically assume that the space of possible perturbations remains
the same across timesteps. However, in many settings, the space of possible
perturbations at a given timestep depends on past perturbations. We formally
introduce temporally-coupled perturbations, presenting a novel challenge for
existing robust RL methods. To tackle this challenge, we propose GRAD, a novel
game-theoretic approach that treats the temporally-coupled robust RL problem as
a partially-observable two-player zero-sum game. By finding an approximate
equilibrium in this game, GRAD ensures the agent's robustness against
temporally-coupled perturbations. Empirical experiments on a variety of
continuous control tasks demonstrate that our proposed approach exhibits
significant robustness advantages compared to baselines against both standard
and temporally-coupled attacks, in both state and action spaces.
</p></li>
</ul>

<h3>Title: Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation. (arXiv:2307.12219v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12219">http://arxiv.org/abs/2307.12219</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12219] Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation](http://arxiv.org/abs/2307.12219) #robust</code></li>
<li>Summary: <p>Deep neural networks achieve superior performance for learning from
independent and identically distributed (i.i.d.) data. However, their
performance deteriorates significantly when handling out-of-distribution (OoD)
data, where the training and test are drawn from different distributions. In
this paper, we explore utilizing the generative models as a data augmentation
source for improving out-of-distribution robustness of neural classifiers.
Specifically, we develop a simple yet effective method called Generative
Interpolation to fuse generative models trained from multiple domains for
synthesizing diverse OoD samples. Training a generative model directly on the
source domains tends to suffer from mode collapse and sometimes amplifies the
data bias. Instead, we first train a StyleGAN model on one source domain and
then fine-tune it on the other domains, resulting in many correlated generators
where their model parameters have the same initialization thus are aligned. We
then linearly interpolate the model parameters of the generators to spawn new
sets of generators. Such interpolated generators are used as an extra data
augmentation source to train the classifiers. The interpolation coefficients
can flexibly control the augmentation direction and strength. In addition, a
style-mixing mechanism is applied to further improve the diversity of the
generated OoD samples. Our experiments show that the proposed method explicitly
increases the diversity of training domains and achieves consistent
improvements over baselines across datasets and multiple different distribution
shifts.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Similarity-based Memory Enhanced Joint Entity and Relation Extraction. (arXiv:2307.11762v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11762">http://arxiv.org/abs/2307.11762</a></li>
<li>Code URL: <a href="https://github.com/kosciukiewicz/similarity_based_memory_re">https://github.com/kosciukiewicz/similarity_based_memory_re</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11762] Similarity-based Memory Enhanced Joint Entity and Relation Extraction](http://arxiv.org/abs/2307.11762) #extraction</code></li>
<li>Summary: <p>Document-level joint entity and relation extraction is a challenging
information extraction problem that requires a unified approach where a single
neural network performs four sub-tasks: mention detection, coreference
resolution, entity classification, and relation extraction. Existing methods
often utilize a sequential multi-task learning approach, in which the arbitral
decomposition causes the current task to depend only on the previous one,
missing the possible existence of the more complex relationships between them.
In this paper, we present a multi-task learning framework with bidirectional
memory-like dependency between tasks to address those drawbacks and perform the
joint problem more accurately. Our empirical studies show that the proposed
approach outperforms the existing methods and achieves state-of-the-art results
on the BioCreative V CDR corpus.
</p></li>
</ul>

<h3>Title: A Topical Approach to Capturing Customer Insight In Social Media. (arXiv:2307.11775v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11775">http://arxiv.org/abs/2307.11775</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11775] A Topical Approach to Capturing Customer Insight In Social Media](http://arxiv.org/abs/2307.11775) #extraction</code></li>
<li>Summary: <p>The age of social media has opened new opportunities for businesses. This
flourishing wealth of information is outside traditional channels and
frameworks of classical marketing research, including that of Marketing Mix
Modeling (MMM). Textual data, in particular, poses many challenges that data
analysis practitioners must tackle. Social media constitute massive,
heterogeneous, and noisy document sources. Industrial data acquisition
processes include some amount of ETL. However, the variability of noise in the
data and the heterogeneity induced by different sources create the need for
ad-hoc tools. Put otherwise, customer insight extraction in fully unsupervised,
noisy contexts is an arduous task. This research addresses the challenge of
fully unsupervised topic extraction in noisy, Big Data contexts. We present
three approaches we built on the Variational Autoencoder framework: the
Embedded Dirichlet Process, the Embedded Hierarchical Dirichlet Process, and
the time-aware Dynamic Embedded Dirichlet Process. These nonparametric
approaches concerning topics present the particularity of determining word
embeddings and topic embeddings. These embeddings do not require transfer
learning, but knowledge transfer remains possible. We test these approaches on
benchmark and automotive industry-related datasets from a real-world use case.
We show that our models achieve equal to better performance than
state-of-the-art methods and that the field of topic modeling would benefit
from improved evaluation metrics.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Project Florida: Federated Learning Made Easy. (arXiv:2307.11899v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11899">http://arxiv.org/abs/2307.11899</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11899] Project Florida: Federated Learning Made Easy](http://arxiv.org/abs/2307.11899) #federate</code></li>
<li>Summary: <p>We present Project Florida, a system architecture and software development
kit (SDK) enabling deployment of large-scale Federated Learning (FL) solutions
across a heterogeneous device ecosystem. Federated learning is an approach to
machine learning based on a strong data sovereignty principle, i.e., that
privacy and security of data is best enabled by storing it at its origin,
whether on end-user devices or in segregated cloud storage silos. Federated
learning enables model training across devices and silos while the training
data remains within its security boundary, by distributing a model snapshot to
a client running inside the boundary, running client code to update the model,
and then aggregating updated snapshots across many clients in a central
orchestrator. Deploying a FL solution requires implementation of complex
privacy and security mechanisms as well as scalable orchestration
infrastructure. Scale and performance is a paramount concern, as the model
training process benefits from full participation of many client devices, which
may have a wide variety of performance characteristics. Project Florida aims to
simplify the task of deploying cross-device FL solutions by providing
cloud-hosted infrastructure and accompanying task management interfaces, as
well as a multi-platform SDK supporting most major programming languages
including C++, Java, and Python, enabling FL training across a wide range of
operating system (OS) and hardware specifications. The architecture decouples
service management from the FL workflow, enabling a cloud service provider to
deliver FL-as-a-service (FLaaS) to ML engineers and application developers. We
present an overview of Florida, including a description of the architecture,
sample code, and illustrative experiments demonstrating system capabilities.
</p></li>
</ul>

<h3>Title: CorrFL: Correlation-Based Neural Network Architecture for Unavailability Concerns in a Heterogeneous IoT Environment. (arXiv:2307.12149v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12149">http://arxiv.org/abs/2307.12149</a></li>
<li>Code URL: <a href="https://github.com/Western-OC2-Lab/CorrFL">https://github.com/Western-OC2-Lab/CorrFL</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12149] CorrFL: Correlation-Based Neural Network Architecture for Unavailability Concerns in a Heterogeneous IoT Environment](http://arxiv.org/abs/2307.12149) #federate</code></li>
<li>Summary: <p>The Federated Learning (FL) paradigm faces several challenges that limit its
application in real-world environments. These challenges include the local
models' architecture heterogeneity and the unavailability of distributed
Internet of Things (IoT) nodes due to connectivity problems. These factors
posit the question of "how can the available models fill the training gap of
the unavailable models?". This question is referred to as the "Oblique
Federated Learning" problem. This problem is encountered in the studied
environment that includes distributed IoT nodes responsible for predicting CO2
concentrations. This paper proposes the Correlation-based FL (CorrFL) approach
influenced by the representational learning field to address this problem.
CorrFL projects the various model weights to a common latent space to address
the model heterogeneity. Its loss function minimizes the reconstruction loss
when models are absent and maximizes the correlation between the generated
models. The latter factor is critical because of the intersection of the
feature spaces of the IoT devices. CorrFL is evaluated on a realistic use case,
involving the unavailability of one IoT device and heightened activity levels
that reflect occupancy. The generated CorrFL models for the unavailable IoT
device from the available ones trained on the new environment are compared
against models trained on different use cases, referred to as the benchmark
model. The evaluation criteria combine the mean absolute error (MAE) of
predictions and the impact of the amount of exchanged data on the prediction
performance improvement. Through a comprehensive experimental procedure, the
CorrFL model outperformed the benchmark model in every criterion.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Fairness of ChatGPT and the Role Of Explainable-Guided Prompts. (arXiv:2307.11761v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11761">http://arxiv.org/abs/2307.11761</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11761] Fairness of ChatGPT and the Role Of Explainable-Guided Prompts](http://arxiv.org/abs/2307.11761) #fair</code></li>
<li>Summary: <p>Our research investigates the potential of Large-scale Language Models
(LLMs), specifically OpenAI's GPT, in credit risk assessment-a binary
classification task. Our findings suggest that LLMs, when directed by
judiciously designed prompts and supplemented with domain-specific knowledge,
can parallel the performance of traditional Machine Learning (ML) models.
Intriguingly, they achieve this with significantly less data-40 times less,
utilizing merely 20 data points compared to the ML's 800. LLMs particularly
excel in minimizing false positives and enhancing fairness, both being vital
aspects of risk analysis. While our results did not surpass those of classical
ML models, they underscore the potential of LLMs in analogous tasks, laying a
groundwork for future explorations into harnessing the capabilities of LLMs in
diverse ML tasks.
</p></li>
</ul>

<h3>Title: On the Vulnerability of Fairness Constrained Learning to Malicious Noise. (arXiv:2307.11892v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11892">http://arxiv.org/abs/2307.11892</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11892] On the Vulnerability of Fairness Constrained Learning to Malicious Noise](http://arxiv.org/abs/2307.11892) #fair</code></li>
<li>Summary: <p>We consider the vulnerability of fairness-constrained learning to small
amounts of malicious noise in the training data. Konstantinov and Lampert
(2021) initiated the study of this question and presented negative results
showing there exist data distributions where for several fairness constraints,
any proper learner will exhibit high vulnerability when group sizes are
imbalanced. Here, we present a more optimistic view, showing that if we allow
randomized classifiers, then the landscape is much more nuanced. For example,
for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in
accuracy, where $\alpha$ is the malicious noise rate, matching the best
possible even without fairness constraints. For Equal Opportunity, we show we
can incur an $O(\sqrt{\alpha})$ loss, and give a matching
$\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert
(2021) showed for proper learners the loss in accuracy for both notions is
$\Omega(1)$. The key technical novelty of our work is how randomization can
bypass simple "tricks" an adversary can use to amplify his power. We also
consider additional fairness notions including Equalized Odds and Calibration.
For these fairness notions, the excess accuracy clusters into three natural
regimes $O(\alpha)$,$O(\sqrt{\alpha})$ and $O(1)$. These results provide a more
fine-grained view of the sensitivity of fairness-constrained learning to
adversarial noise in training data.
</p></li>
</ul>

<h3>Title: Blockchain-based Cloud Data Deduplication Scheme with Fair Incentives. (arXiv:2307.12052v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12052">http://arxiv.org/abs/2307.12052</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12052] Blockchain-based Cloud Data Deduplication Scheme with Fair Incentives](http://arxiv.org/abs/2307.12052) #fair</code></li>
<li>Summary: <p>With the rapid development of cloud computing, vast amounts of duplicated
data are being uploaded to the cloud, wasting storage resources. Deduplication
(dedup) is an efficient solution to save storage costs of cloud storage
providers (CSPs) by storing only one copy of the uploaded data. However, cloud
users do not benefit directly from dedup and may be reluctant to dedup their
data. To motivate the cloud users towards dedup, CSPs offer incentives on
storage fees. The problems with the existing dedup schemes are that they do not
consider: (1) correctness - the incentive offered to a cloud user should be
computed correctly without any prejudice. (2) fairness - the cloud user
receives the file link and access rights of the uploaded data if and only if
the CSP receives the storage fee. Meeting these requirements without a trusted
party is non-trivial, and most of the existing dedup schemes do not apply.
Another drawback is that most of the existing schemes emphasize incentives to
cloud users but failed to provide a reliable incentive mechanism.
</p></li>
</ul>

<p>As public Blockchain networks emulate the properties of trusted parties, in
this paper, we propose a new Blockchain-based dedup scheme to meet the above
requirements. In our scheme, a smart contract computes the incentives on
storage fee, and the fairness rules are encoded into the smart contract for
facilitating fair payments between the CSPs and cloud users. We prove the
correctness and fairness of the proposed scheme. We also design a new incentive
mechanism and show that the scheme is individually rational and incentive
compatible. Furthermore, we conduct experiments by implementing the designed
smart contract on Ethereum local Blockchain network and list the transactional
and financial costs of interacting with the designed smart contract.
</p>

<h3>Title: Spectral Normalized-Cut Graph Partitioning with Fairness Constraints. (arXiv:2307.12065v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12065">http://arxiv.org/abs/2307.12065</a></li>
<li>Code URL: <a href="https://github.com/jiali2000/fnm">https://github.com/jiali2000/fnm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12065] Spectral Normalized-Cut Graph Partitioning with Fairness Constraints](http://arxiv.org/abs/2307.12065) #fair</code></li>
<li>Summary: <p>Normalized-cut graph partitioning aims to divide the set of nodes in a graph
into $k$ disjoint clusters to minimize the fraction of the total edges between
any cluster and all other clusters. In this paper, we consider a fair variant
of the partitioning problem wherein nodes are characterized by a categorical
sensitive attribute (e.g., gender or race) indicating membership to different
demographic groups. Our goal is to ensure that each group is approximately
proportionally represented in each cluster while minimizing the normalized cut
value. To resolve this problem, we propose a two-phase spectral algorithm
called FNM. In the first phase, we add an augmented Lagrangian term based on
our fairness criteria to the objective function for obtaining a fairer spectral
node embedding. Then, in the second phase, we design a rounding scheme to
produce $k$ clusters from the fair embedding that effectively trades off
fairness and partition quality. Through comprehensive experiments on nine
benchmark datasets, we demonstrate the superior performance of FNM compared
with three baseline methods.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: NCART: Neural Classification and Regression Tree for Tabular Data. (arXiv:2307.12198v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12198">http://arxiv.org/abs/2307.12198</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12198] NCART: Neural Classification and Regression Tree for Tabular Data](http://arxiv.org/abs/2307.12198) #interpretability</code></li>
<li>Summary: <p>Deep learning models have become popular in the analysis of tabular data, as
they address the limitations of decision trees and enable valuable applications
like semi-supervised learning, online learning, and transfer learning. However,
these deep-learning approaches often encounter a trade-off. On one hand, they
can be computationally expensive when dealing with large-scale or
high-dimensional datasets. On the other hand, they may lack interpretability
and may not be suitable for small-scale datasets. In this study, we propose a
novel interpretable neural network called Neural Classification and Regression
Tree (NCART) to overcome these challenges. NCART is a modified version of
Residual Networks that replaces fully-connected layers with multiple
differentiable oblivious decision trees. By integrating decision trees into the
architecture, NCART maintains its interpretability while benefiting from the
end-to-end capabilities of neural networks. The simplicity of the NCART
architecture makes it well-suited for datasets of varying sizes and reduces
computational costs compared to state-of-the-art deep learning models.
Extensive numerical experiments demonstrate the superior performance of NCART
compared to existing deep learning models, establishing it as a strong
competitor to tree-based models.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Prediction of Handball Matches with Statistically Enhanced Learning via Estimated Team Strengths. (arXiv:2307.11777v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11777">http://arxiv.org/abs/2307.11777</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11777] Prediction of Handball Matches with Statistically Enhanced Learning via Estimated Team Strengths](http://arxiv.org/abs/2307.11777) #explainability</code></li>
<li>Summary: <p>We propose a Statistically Enhanced Learning (aka. SEL) model to predict
handball games. Our Machine Learning model augmented with SEL features
outperforms state-of-the-art models with an accuracy beyond 80%. In this work,
we show how we construct the data set to train Machine Learning models on past
female club matches. We then compare different models and evaluate them to
assess their performance capabilities. Finally, explainability methods allow us
to change the scope of our tool from a purely predictive solution to a highly
insightful analytical tool. This can become a valuable asset for handball
teams' coaches providing valuable statistical and predictive insights to
prepare future competitions.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: FSDiffReg: Feature-wise and Score-wise Diffusion-guided Unsupervised Deformable Image Registration for Cardiac Images. (arXiv:2307.12035v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12035">http://arxiv.org/abs/2307.12035</a></li>
<li>Code URL: <a href="https://github.com/xmed-lab/fsdiffreg">https://github.com/xmed-lab/fsdiffreg</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12035] FSDiffReg: Feature-wise and Score-wise Diffusion-guided Unsupervised Deformable Image Registration for Cardiac Images](http://arxiv.org/abs/2307.12035) #diffusion</code></li>
<li>Summary: <p>Unsupervised deformable image registration is one of the challenging tasks in
medical imaging. Obtaining a high-quality deformation field while preserving
deformation topology remains demanding amid a series of deep-learning-based
solutions. Meanwhile, the diffusion model's latent feature space shows
potential in modeling the deformation semantics. To fully exploit the diffusion
model's ability to guide the registration task, we present two modules:
Feature-wise Diffusion-Guided Module (FDG) and Score-wise Diffusion-Guided
Module (SDG). Specifically, FDG uses the diffusion model's multi-scale semantic
features to guide the generation of the deformation field. SDG uses the
diffusion score to guide the optimization process for preserving deformation
topology with barely any additional computation. Experiment results on the 3D
medical cardiac image registration task validate our model's ability to provide
refined deformation fields with preserved topology effectively. Code is
available at: https://github.com/xmed-lab/FSDiffReg.git.
</p></li>
</ul>

<h3>Title: Iterative Reconstruction Based on Latent Diffusion Model for Sparse Data Reconstruction. (arXiv:2307.12070v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12070">http://arxiv.org/abs/2307.12070</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12070] Iterative Reconstruction Based on Latent Diffusion Model for Sparse Data Reconstruction](http://arxiv.org/abs/2307.12070) #diffusion</code></li>
<li>Summary: <p>Reconstructing Computed tomography (CT) images from sparse measurement is a
well-known ill-posed inverse problem. The Iterative Reconstruction (IR)
algorithm is a solution to inverse problems. However, recent IR methods require
paired data and the approximation of the inverse projection matrix. To address
those problems, we present Latent Diffusion Iterative Reconstruction (LDIR), a
pioneering zero-shot method that extends IR with a pre-trained Latent Diffusion
Model (LDM) as a accurate and efficient data prior. By approximating the prior
distribution with an unconditional latent diffusion model, LDIR is the first
method to successfully integrate iterative reconstruction and LDM in an
unsupervised manner. LDIR makes the reconstruction of high-resolution images
more efficient. Moreover, LDIR utilizes the gradient from the data-fidelity
term to guide the sampling process of the LDM, therefore, LDIR does not need
the approximation of the inverse projection matrix and can solve various CT
reconstruction tasks with a single model. Additionally, for enhancing the
sample consistency of the reconstruction, we introduce a novel approach that
uses historical gradient information to guide the gradient. Our experiments on
extremely sparse CT data reconstruction tasks show that LDIR outperforms other
state-of-the-art unsupervised and even exceeds supervised methods, establishing
it as a leading technique in terms of both quantity and quality. Furthermore,
LDIR also achieves competitive performance on nature image tasks. It is worth
noting that LDIR also exhibits significantly faster execution times and lower
memory consumption compared to methods with similar network settings. Our code
will be publicly available.
</p></li>
</ul>

<h3>Title: Synthesis of Batik Motifs using a Diffusion -- Generative Adversarial Network. (arXiv:2307.12122v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12122">http://arxiv.org/abs/2307.12122</a></li>
<li>Code URL: <a href="https://github.com/octadion/diffusion-stylegan2-ada-pytorch">https://github.com/octadion/diffusion-stylegan2-ada-pytorch</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12122] Synthesis of Batik Motifs using a Diffusion -- Generative Adversarial Network](http://arxiv.org/abs/2307.12122) #diffusion</code></li>
<li>Summary: <p>Batik, a unique blend of art and craftsmanship, is a distinct artistic and
technological creation for Indonesian society. Research on batik motifs is
primarily focused on classification. However, further studies may extend to the
synthesis of batik patterns. Generative Adversarial Networks (GANs) have been
an important deep learning model for generating synthetic data, but often face
challenges in the stability and consistency of results. This research focuses
on the use of StyleGAN2-Ada and Diffusion techniques to produce realistic and
high-quality synthetic batik patterns. StyleGAN2-Ada is a variation of the GAN
model that separates the style and content aspects in an image, whereas
diffusion techniques introduce random noise into the data. In the context of
batik, StyleGAN2-Ada and Diffusion are used to produce realistic synthetic
batik patterns. This study also made adjustments to the model architecture and
used a well-curated batik dataset. The main goal is to assist batik designers
or craftsmen in producing unique and quality batik motifs with efficient
production time and costs. Based on qualitative and quantitative evaluations,
the results show that the model tested is capable of producing authentic and
quality batik patterns, with finer details and rich artistic variations. The
dataset and code can be accessed
here:https://github.com/octadion/diffusion-stylegan2-ada-pytorch
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction. (arXiv:2307.11952v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11952">http://arxiv.org/abs/2307.11952</a></li>
<li>Code URL: <a href="https://github.com/cassie07/pathomics">https://github.com/cassie07/pathomics</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11952] Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction](http://arxiv.org/abs/2307.11952) #transformer</code></li>
<li>Summary: <p>Survival outcome assessment is challenging and inherently associated with
multiple clinical factors (e.g., imaging and genomics biomarkers) in cancer.
Enabling multimodal analytics promises to reveal novel predictive patterns of
patient outcomes. In this study, we propose a multimodal transformer
(PathOmics) integrating pathology and genomics insights into colon-related
cancer survival prediction. We emphasize the unsupervised pretraining to
capture the intrinsic interaction between tissue microenvironments in gigapixel
whole slide images (WSIs) and a wide range of genomics data (e.g.,
mRNA-sequence, copy number variant, and methylation). After the multimodal
knowledge aggregation in pretraining, our task-specific model finetuning could
expand the scope of data utility applicable to both multi- and single-modal
data (e.g., image- or genomics-only). We evaluate our approach on both TCGA
colon and rectum cancer cohorts, showing that the proposed approach is
competitive and outperforms state-of-the-art studies. Finally, our approach is
desirable to utilize the limited number of finetuned samples towards
data-efficient analytics for survival outcome prediction. The code is available
at https://github.com/Cassie07/PathOmics.
</p></li>
</ul>

<h3>Title: Two-stream Multi-level Dynamic Point Transformer for Two-person Interaction Recognition. (arXiv:2307.11973v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11973">http://arxiv.org/abs/2307.11973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11973] Two-stream Multi-level Dynamic Point Transformer for Two-person Interaction Recognition](http://arxiv.org/abs/2307.11973) #transformer</code></li>
<li>Summary: <p>As a fundamental aspect of human life, two-person interactions contain
meaningful information about people's activities, relationships, and social
settings. Human action recognition serves as the foundation for many smart
applications, with a strong focus on personal privacy. However, recognizing
two-person interactions poses more challenges due to increased body occlusion
and overlap compared to single-person actions. In this paper, we propose a
point cloud-based network named Two-stream Multi-level Dynamic Point
Transformer for two-person interaction recognition. Our model addresses the
challenge of recognizing two-person interactions by incorporating local-region
spatial information, appearance information, and motion information. To achieve
this, we introduce a designed frame selection method named Interval Frame
Sampling (IFS), which efficiently samples frames from videos, capturing more
discriminative information in a relatively short processing time. Subsequently,
a frame features learning module and a two-stream multi-level feature
aggregation module extract global and partial features from the sampled frames,
effectively representing the local-region spatial information, appearance
information, and motion information related to the interactions. Finally, we
apply a transformer to perform self-attention on the learned features for the
final classification. Extensive experiments are conducted on two large-scale
datasets, the interaction subsets of NTU RGB+D 60 and NTU RGB+D 120. The
results show that our network outperforms state-of-the-art approaches across
all standard evaluation settings.
</p></li>
</ul>

<h3>Title: Sparse then Prune: Toward Efficient Vision Transformers. (arXiv:2307.11988v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11988">http://arxiv.org/abs/2307.11988</a></li>
<li>Code URL: <a href="https://github.com/yogiprsty/sparse-vit">https://github.com/yogiprsty/sparse-vit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11988] Sparse then Prune: Toward Efficient Vision Transformers](http://arxiv.org/abs/2307.11988) #transformer</code></li>
<li>Summary: <p>The Vision Transformer architecture is a deep learning model inspired by the
success of the Transformer model in Natural Language Processing. However, the
self-attention mechanism, large number of parameters, and the requirement for a
substantial amount of training data still make Vision Transformers
computationally burdensome. In this research, we investigate the possibility of
applying Sparse Regularization to Vision Transformers and the impact of
Pruning, either after Sparse Regularization or without it, on the trade-off
between performance and efficiency. To accomplish this, we apply Sparse
Regularization and Pruning methods to the Vision Transformer architecture for
image classification tasks on the CIFAR-10, CIFAR-100, and ImageNet-100
datasets. The training process for the Vision Transformer model consists of two
parts: pre-training and fine-tuning. Pre-training utilizes ImageNet21K data,
followed by fine-tuning for 20 epochs. The results show that when testing with
CIFAR-100 and ImageNet-100 data, models with Sparse Regularization can increase
accuracy by 0.12%. Furthermore, applying pruning to models with Sparse
Regularization yields even better results. Specifically, it increases the
average accuracy by 0.568% on CIFAR-10 data, 1.764% on CIFAR-100, and 0.256% on
ImageNet-100 data compared to pruning models without Sparse Regularization.
Code can be accesed here: https://github.com/yogiprsty/Sparse-ViT
</p></li>
</ul>

<h3>Title: On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement. (arXiv:2307.12027v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12027">http://arxiv.org/abs/2307.12027</a></li>
<li>Code URL: <a href="https://github.com/luciennnnnnn/dualformer">https://github.com/luciennnnnnn/dualformer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12027] On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement](http://arxiv.org/abs/2307.12027) #transformer</code></li>
<li>Summary: <p>Several recent studies advocate the use of spectral discriminators, which
evaluate the Fourier spectra of images for generative modeling. However, the
effectiveness of the spectral discriminators is not well interpreted yet. We
tackle this issue by examining the spectral discriminators in the context of
perceptual image super-resolution (i.e., GAN-based SR), as SR image quality is
susceptible to spectral changes. Our analyses reveal that the spectral
discriminator indeed performs better than the ordinary (a.k.a. spatial)
discriminator in identifying the differences in the high-frequency range;
however, the spatial discriminator holds an advantage in the low-frequency
range. Thus, we suggest that the spectral and spatial discriminators shall be
used simultaneously. Moreover, we improve the spectral discriminators by first
calculating the patch-wise Fourier spectrum and then aggregating the spectra by
Transformer. We verify the effectiveness of the proposed method twofold. On the
one hand, thanks to the additional spectral discriminator, our obtained SR
images have their spectra better aligned to those of the real images, which
leads to a better PD tradeoff. On the other hand, our ensembled discriminator
predicts the perceptual quality more accurately, as evidenced in the
no-reference image quality assessment task.
</p></li>
</ul>

<h3>Title: Patch-Wise Point Cloud Generation: A Divide-and-Conquer Approach. (arXiv:2307.12049v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12049">http://arxiv.org/abs/2307.12049</a></li>
<li>Code URL: <a href="https://github.com/wenc13/patchgeneration">https://github.com/wenc13/patchgeneration</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12049] Patch-Wise Point Cloud Generation: A Divide-and-Conquer Approach](http://arxiv.org/abs/2307.12049) #transformer</code></li>
<li>Summary: <p>A generative model for high-fidelity point clouds is of great importance in
synthesizing 3d environments for applications such as autonomous driving and
robotics. Despite the recent success of deep generative models for 2d images,
it is non-trivial to generate 3d point clouds without a comprehensive
understanding of both local and global geometric structures. In this paper, we
devise a new 3d point cloud generation framework using a divide-and-conquer
approach, where the whole generation process can be divided into a set of
patch-wise generation tasks. Specifically, all patch generators are based on
learnable priors, which aim to capture the information of geometry primitives.
We introduce point- and patch-wise transformers to enable the interactions
between points and patches. Therefore, the proposed divide-and-conquer approach
contributes to a new understanding of point cloud generation from the geometry
constitution of 3d shapes. Experimental results on a variety of object
categories from the most popular point cloud dataset, ShapeNet, show the
effectiveness of the proposed patch-wise point cloud generation, where it
clearly outperforms recent state-of-the-art methods for high-fidelity point
cloud generation.
</p></li>
</ul>

<h3>Title: Discovering Spatio-Temporal Rationales for Video Question Answering. (arXiv:2307.12058v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12058">http://arxiv.org/abs/2307.12058</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12058] Discovering Spatio-Temporal Rationales for Video Question Answering](http://arxiv.org/abs/2307.12058) #transformer</code></li>
<li>Summary: <p>This paper strives to solve complex video question answering (VideoQA) which
features long video containing multiple objects and events at different time.
To tackle the challenge, we highlight the importance of identifying
question-critical temporal moments and spatial objects from the vast amount of
video content. Towards this, we propose a Spatio-Temporal Rationalization
(STR), a differentiable selection module that adaptively collects
question-critical moments and objects using cross-modal interaction. The
discovered video moments and objects are then served as grounded rationales to
support answer reasoning. Based on STR, we further propose TranSTR, a
Transformer-style neural network architecture that takes STR as the core and
additionally underscores a novel answer interaction mechanism to coordinate STR
for answer decoding. Experiments on four datasets show that TranSTR achieves
new state-of-the-art (SoTA). Especially, on NExT-QA and Causal-VidQA which
feature complex VideoQA, it significantly surpasses the previous SoTA by 5.8\%
and 6.8\%, respectively. We then conduct extensive studies to verify the
importance of STR as well as the proposed answer interaction mechanism. With
the success of TranSTR and our comprehensive analysis, we hope this work can
spark more future efforts in complex VideoQA. Code will be released at
https://github.com/yl3800/TranSTR.
</p></li>
</ul>

<h3>Title: LIST: Learning Implicitly from Spatial Transformers for Single-View 3D Reconstruction. (arXiv:2307.12194v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12194">http://arxiv.org/abs/2307.12194</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12194] LIST: Learning Implicitly from Spatial Transformers for Single-View 3D Reconstruction](http://arxiv.org/abs/2307.12194) #transformer</code></li>
<li>Summary: <p>Accurate reconstruction of both the geometric and topological details of a 3D
object from a single 2D image embodies a fundamental challenge in computer
vision. Existing explicit/implicit solutions to this problem struggle to
recover self-occluded geometry and/or faithfully reconstruct topological shape
structures. To resolve this dilemma, we introduce LIST, a novel neural
architecture that leverages local and global image features to accurately
reconstruct the geometric and topological structure of a 3D object from a
single image. We utilize global 2D features to predict a coarse shape of the
target object and then use it as a base for higher-resolution reconstruction.
By leveraging both local 2D features from the image and 3D features from the
coarse prediction, we can predict the signed distance between an arbitrary
point and the target surface via an implicit predictor with great accuracy.
Furthermore, our model does not require camera estimation or pixel alignment.
It provides an uninfluenced reconstruction from the input-view direction.
Through qualitative and quantitative analysis, we show the superiority of our
model in reconstructing 3D objects from both synthetic and real-world images
against the state of the art.
</p></li>
</ul>

<h3>Title: Transsion TSUP's speech recognition system for ASRU 2023 MADASR Challenge. (arXiv:2307.11778v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11778">http://arxiv.org/abs/2307.11778</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11778] Transsion TSUP's speech recognition system for ASRU 2023 MADASR Challenge](http://arxiv.org/abs/2307.11778) #transformer</code></li>
<li>Summary: <p>This paper presents a speech recognition system developed by the Transsion
Speech Understanding Processing Team (TSUP) for the ASRU 2023 MADASR Challenge.
The system focuses on adapting ASR models for low-resource Indian languages and
covers all four tracks of the challenge. For tracks 1 and 2, the acoustic model
utilized a squeezeformer encoder and bidirectional transformer decoder with
joint CTC-Attention training loss. Additionally, an external KenLM language
model was used during TLG beam search decoding. For tracks 3 and 4, pretrained
IndicWhisper models were employed and finetuned on both the challenge dataset
and publicly available datasets. The whisper beam search decoding was also
modified to support an external KenLM language model, which enabled better
utilization of the additional text provided by the challenge. The proposed
method achieved word error rates (WER) of 24.17%, 24.43%, 15.97%, and 15.97%
for Bengali language in the four tracks, and WER of 19.61%, 19.54%, 15.48%, and
15.48% for Bhojpuri language in the four tracks. These results demonstrate the
effectiveness of the proposed method.
</p></li>
</ul>

<h3>Title: Adversarial Conversational Shaping for Intelligent Agents. (arXiv:2307.11785v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11785">http://arxiv.org/abs/2307.11785</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11785] Adversarial Conversational Shaping for Intelligent Agents](http://arxiv.org/abs/2307.11785) #transformer</code></li>
<li>Summary: <p>The recent emergence of deep learning methods has enabled the research
community to achieve state-of-the art results in several domains including
natural language processing. However, the current robocall system remains
unstable and inaccurate: text generator and chat-bots can be tedious and
misunderstand human-like dialogue. In this work, we study the performance of
two models able to enhance an intelligent conversational agent through
adversarial conversational shaping: a generative adversarial network with
policy gradient (GANPG) and a generative adversarial network with reward for
every generation step (REGS) based on the REGS model presented in Li et al.
[18] . This model is able to assign rewards to both partially and fully
generated text sequences. We discuss performance with different training
details : seq2seq [ 36] and transformers [37 ] in a reinforcement learning
framework.
</p></li>
</ul>

<h3>Title: Identifying Misinformation on YouTube through Transcript Contextual Analysis with Transformer Models. (arXiv:2307.12155v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12155">http://arxiv.org/abs/2307.12155</a></li>
<li>Code URL: <a href="https://github.com/christoschr97/misinf-detection-llms">https://github.com/christoschr97/misinf-detection-llms</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12155] Identifying Misinformation on YouTube through Transcript Contextual Analysis with Transformer Models](http://arxiv.org/abs/2307.12155) #transformer</code></li>
<li>Summary: <p>Misinformation on YouTube is a significant concern, necessitating robust
detection strategies. In this paper, we introduce a novel methodology for video
classification, focusing on the veracity of the content. We convert the
conventional video classification task into a text classification task by
leveraging the textual content derived from the video transcripts. We employ
advanced machine learning techniques like transfer learning to solve the
classification challenge. Our approach incorporates two forms of transfer
learning: (a) fine-tuning base transformer models such as BERT, RoBERTa, and
ELECTRA, and (b) few-shot learning using sentence-transformers MPNet and
RoBERTa-large. We apply the trained models to three datasets: (a) YouTube
Vaccine-misinformation related videos, (b) YouTube Pseudoscience videos, and
(c) Fake-News dataset (a collection of articles). Including the Fake-News
dataset extended the evaluation of our approach beyond YouTube videos. Using
these datasets, we evaluated the models distinguishing valid information from
misinformation. The fine-tuned models yielded Matthews Correlation
Coefficient>0.81, accuracy>0.90, and F1 score>0.90 in two of three datasets.
Interestingly, the few-shot models outperformed the fine-tuned ones by 20% in
both Accuracy and F1 score for the YouTube Pseudoscience dataset, highlighting
the potential utility of this approach -- especially in the context of limited
training data.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Replay: Multi-modal Multi-view Acted Videos for Casual Holography. (arXiv:2307.12067v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12067">http://arxiv.org/abs/2307.12067</a></li>
<li>Code URL: <a href="https://github.com/facebookresearch/replay_dataset">https://github.com/facebookresearch/replay_dataset</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12067] Replay: Multi-modal Multi-view Acted Videos for Casual Holography](http://arxiv.org/abs/2307.12067) #generative</code></li>
<li>Summary: <p>We introduce Replay, a collection of multi-view, multi-modal videos of humans
interacting socially. Each scene is filmed in high production quality, from
different viewpoints with several static cameras, as well as wearable action
cameras, and recorded with a large array of microphones at different positions
in the room. Overall, the dataset contains over 4000 minutes of footage and
over 7 million timestamped high-resolution frames annotated with camera poses
and partially with foreground masks. The Replay dataset has many potential
applications, such as novel-view synthesis, 3D reconstruction, novel-view
acoustic synthesis, human body and face analysis, and training generative
models. We provide a benchmark for training and evaluating novel-view
synthesis, with two scenarios of different difficulty. Finally, we evaluate
several baseline state-of-the-art methods on the new benchmark.
</p></li>
</ul>

<h3>Title: The Extractive-Abstractive Axis: Measuring Content "Borrowing" in Generative Language Models. (arXiv:2307.11779v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11779">http://arxiv.org/abs/2307.11779</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11779] The Extractive-Abstractive Axis: Measuring Content "Borrowing" in Generative Language Models](http://arxiv.org/abs/2307.11779) #generative</code></li>
<li>Summary: <p>Generative language models produce highly abstractive outputs by design, in
contrast to extractive responses in search engines. Given this characteristic
of LLMs and the resulting implications for content Licensing &amp; Attribution, we
propose the the so-called Extractive-Abstractive axis for benchmarking
generative models and highlight the need for developing corresponding metrics,
datasets and annotation guidelines. We limit our discussion to the text
modality.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11760">http://arxiv.org/abs/2307.11760</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11760] EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus](http://arxiv.org/abs/2307.11760) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have achieved significant performance in many
fields such as reasoning, language understanding, and math problem-solving, and
are regarded as a crucial step to artificial general intelligence (AGI).
However, the sensitivity of LLMs to prompts remains a major bottleneck for
their daily adoption. In this paper, we take inspiration from psychology and
propose EmotionPrompt to explore emotional intelligence to enhance the
performance of LLMs. EmotionPrompt operates on a remarkably straightforward
principle: the incorporation of emotional stimulus into prompts. Experimental
results demonstrate that our \method, using the same single prompt templates,
significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8
tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further,
EmotionPrompt was observed to improve both truthfulness and informativeness. We
believe that EmotionPrompt heralds a novel avenue for exploring
interdisciplinary knowledge for humans-LLMs interaction.
</p></li>
</ul>

<h3>Title: Question Decomposition Improves the Faithfulness of Model-Generated Reasoning. (arXiv:2307.11768v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11768">http://arxiv.org/abs/2307.11768</a></li>
<li>Code URL: <a href="https://github.com/anthropics/decompositionfaithfulnesspaper">https://github.com/anthropics/decompositionfaithfulnesspaper</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11768] Question Decomposition Improves the Faithfulness of Model-Generated Reasoning](http://arxiv.org/abs/2307.11768) #large language model</code></li>
<li>Summary: <p>As large language models (LLMs) perform more difficult tasks, it becomes
harder to verify the correctness and safety of their behavior. One approach to
help with this issue is to prompt LLMs to externalize their reasoning, e.g., by
having them generate step-by-step reasoning as they answer a question
(Chain-of-Thought; CoT). The reasoning may enable us to check the process that
models use to perform tasks. However, this approach relies on the stated
reasoning faithfully reflecting the model's actual reasoning, which is not
always the case. To improve over the faithfulness of CoT reasoning, we have
models generate reasoning by decomposing questions into subquestions.
Decomposition-based methods achieve strong performance on question-answering
tasks, sometimes approaching that of CoT while improving the faithfulness of
the model's stated reasoning on several recently-proposed metrics. By forcing
the model to answer simpler subquestions in separate contexts, we greatly
increase the faithfulness of model-generated reasoning over CoT, while still
achieving some of the performance gains of CoT. Our results show it is possible
to improve the faithfulness of model-generated reasoning; continued
improvements may lead to reasoning that enables us to verify the correctness
and safety of LLM behavior.
</p></li>
</ul>

<h3>Title: Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain. (arXiv:2307.11769v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11769">http://arxiv.org/abs/2307.11769</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11769] Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain](http://arxiv.org/abs/2307.11769) #large language model</code></li>
<li>Summary: <p>Engineering knowledge-based (or expert) systems require extensive manual
effort and domain knowledge. As Large Language Models (LLMs) are trained using
an enormous amount of cross-domain knowledge, it becomes possible to automate
such engineering processes. This paper presents an empirical automation and
semi-automation framework for domain knowledge distillation using prompt
engineering and the LLM ChatGPT. We assess the framework empirically in the
autonomous driving domain and present our key observations. In our
implementation, we construct the domain knowledge ontology by "chatting" with
ChatGPT. The key finding is that while fully automated domain ontology
construction is possible, human supervision and early intervention typically
improve efficiency and output quality as they lessen the effects of response
randomness and the butterfly effect. We, therefore, also develop a web-based
distillation assistant enabling supervision and flexible intervention at
runtime. We hope our findings and tools could inspire future research toward
revolutionizing the engineering of knowledge-based systems across application
domains.
</p></li>
</ul>

<h3>Title: LLM Cognitive Judgements Differ From Human. (arXiv:2307.11787v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11787">http://arxiv.org/abs/2307.11787</a></li>
<li>Code URL: <a href="https://github.com/sotlampr/llm-cognitive-judgements">https://github.com/sotlampr/llm-cognitive-judgements</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11787] LLM Cognitive Judgements Differ From Human](http://arxiv.org/abs/2307.11787) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs) have lately been on the spotlight of
researchers, businesses, and consumers alike. While the linguistic capabilities
of such models have been studied extensively, there is growing interest in
investigating them as cognitive subjects. In the present work I examine GPT-3
and ChatGPT capabilities on an limited-data inductive reasoning task from the
cognitive science literature. The results suggest that these models' cognitive
judgements are not human-like.
</p></li>
</ul>

<h3>Title: Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors. (arXiv:2307.11922v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11922">http://arxiv.org/abs/2307.11922</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11922] Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors](http://arxiv.org/abs/2307.11922) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) are being applied as actors for sequential
decision making tasks in domains such as robotics and games, utilizing their
general world knowledge and planning abilities. However, previous work does
little to explore what environment state information is provided to LLM actors
via language. Exhaustively describing high-dimensional states can impair
performance and raise inference costs for LLM actors. Previous LLM actors avoid
the issue by relying on hand-engineered, task-specific protocols to determine
which features to communicate about a state and which to leave out. In this
work, we propose Brief Language INputs for DEcision-making Responses (BLINDER),
a method for automatically selecting concise state descriptions by learning a
value function for task-conditioned state descriptions. We evaluate BLINDER on
the challenging video game NetHack and a robotic manipulation task. Our method
improves task success rate, reduces input size and compute costs, and
generalizes between LLM actors.
</p></li>
</ul>

<h3>Title: Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models. (arXiv:2307.11991v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11991">http://arxiv.org/abs/2307.11991</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11991] Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models](http://arxiv.org/abs/2307.11991) #large language model</code></li>
<li>Summary: <p>The demand for psychological counseling has grown significantly in recent
years, particularly with the global outbreak of COVID-19, which has heightened
the need for timely and professional mental health support. Online
psychological counseling has emerged as the predominant mode of providing
services in response to this demand. In this study, we propose the Psy-LLM
framework, an AI-based system leveraging Large Language Models (LLMs) for
question-answering in online psychological consultation. Our framework combines
pre-trained LLMs with real-world professional Q&amp;A from psychologists and
extensively crawled psychological articles. The Psy-LLM framework serves as a
front-end tool for healthcare professionals, allowing them to provide immediate
responses and mindfulness activities to alleviate patient stress. Additionally,
it functions as a screening tool to identify urgent cases requiring further
assistance. We evaluated the framework using intrinsic metrics, such as
perplexity, and extrinsic evaluation metrics, with human participant
assessments of response helpfulness, fluency, relevance, and logic. The results
demonstrate the effectiveness of the Psy-LLM framework in generating coherent
and relevant answers to psychological questions. This article concludes by
discussing the potential of large language models to enhance mental health
support through AI technologies in online psychological consultation.
</p></li>
</ul>

<h3>Title: External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback. (arXiv:2307.12057v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12057">http://arxiv.org/abs/2307.12057</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12057] External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback](http://arxiv.org/abs/2307.12057) #large language model</code></li>
<li>Summary: <p>Memory is identified as a crucial human faculty that allows for the retention
of visual and linguistic information within the hippocampus and neurons in the
brain, which can subsequently be retrieved to address real-world challenges
that arise through a lifetime of learning. The resolution of complex AI tasks
through the application of acquired knowledge represents a stride toward the
realization of artificial general intelligence. However, despite the prevalence
of Large Language Models (LLMs) like GPT-3.5 and GPT-4 , which have displayed
remarkable capabilities in language comprehension, generation, interaction, and
reasoning, they are inhibited by constraints on context length that preclude
the processing of extensive, continually evolving knowledge bases. This paper
proposes that LLMs could be augmented through the selective integration of
knowledge from external repositories, and in doing so, introduces a novel
methodology for External Reasoning, exemplified by ChatPDF. Central to this
approach is the establishment of a tiered policy for \textbf{External Reasoning
based on Multiple LLM Interchange Assistance}, where the level of support
rendered is modulated across entry, intermediate, and advanced tiers based on
the complexity of the query, with adjustments made in response to human
feedback. A comprehensive evaluation of this methodology is conducted using
multiple LLMs and the results indicate state-of-the-art performance, surpassing
existing solutions including ChatPDF.com. Moreover, the paper emphasizes that
this approach is more efficient compared to the direct processing of full text
by LLMs.
</p></li>
</ul>

<h3>Title: A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks. (arXiv:2307.12114v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12114">http://arxiv.org/abs/2307.12114</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12114] A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks](http://arxiv.org/abs/2307.12114) #large language model</code></li>
<li>Summary: <p>We evaluate four state-of-the-art instruction-tuned large language models
(LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13
real-world clinical and biomedical natural language processing (NLP) tasks in
English, such as named-entity recognition (NER), question-answering (QA),
relation extraction (RE), etc. Our overall results demonstrate that the
evaluated LLMs begin to approach performance of state-of-the-art models in
zero- and few-shot scenarios for most tasks, and particularly well for the QA
task, even though they have never seen examples from these tasks before.
However, we observed that the classification and RE tasks perform below what
can be achieved with a specifically trained model for the medical field, such
as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all
the studied tasks, with some models being better suited for certain tasks than
others.
</p></li>
</ul>

<h3>Title: The Imitation Game: Detecting Human and AI-Generated Texts in the Era of Large Language Models. (arXiv:2307.12166v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12166">http://arxiv.org/abs/2307.12166</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12166] The Imitation Game: Detecting Human and AI-Generated Texts in the Era of Large Language Models](http://arxiv.org/abs/2307.12166) #large language model</code></li>
<li>Summary: <p>The potential of artificial intelligence (AI)-based large language models
(LLMs) holds considerable promise in revolutionizing education, research, and
practice. However, distinguishing between human-written and AI-generated text
has become a significant task. This paper presents a comparative study,
introducing a novel dataset of human-written and LLM-generated texts in
different genres: essays, stories, poetry, and Python code. We employ several
machine learning models to classify the texts. Results demonstrate the efficacy
of these models in discerning between human and AI-generated text, despite the
dataset's limited sample size. However, the task becomes more challenging when
classifying GPT-generated text, particularly in story writing. The results
indicate that the models exhibit superior performance in binary classification
tasks, such as distinguishing human-generated text from a specific LLM,
compared to the more complex multiclass tasks that involve discerning among
human-generated and multiple LLMs. Our findings provide insightful implications
for AI text detection while our dataset paves the way for future research in
this evolving area.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds. (arXiv:2307.11914v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11914">http://arxiv.org/abs/2307.11914</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11914] Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds](http://arxiv.org/abs/2307.11914) #segmentation</code></li>
<li>Summary: <p>Urban modeling from LiDAR point clouds is an important topic in computer
vision, computer graphics, photogrammetry and remote sensing. 3D city models
have found a wide range of applications in smart cities, autonomous navigation,
urban planning and mapping etc. However, existing datasets for 3D modeling
mainly focus on common objects such as furniture or cars. Lack of building
datasets has become a major obstacle for applying deep learning technology to
specific domains such as urban modeling. In this paper, we present a
urban-scale dataset consisting of more than 160 thousands buildings along with
corresponding point clouds, mesh and wire-frame models, covering 16 cities in
Estonia about 998 Km2. We extensively evaluate performance of state-of-the-art
algorithms including handcrafted and deep feature based methods. Experimental
results indicate that Building3D has challenges of high intra-class variance,
data imbalance and large-scale noises. The Building3D is the first and largest
urban-scale building modeling benchmark, allowing a comparison of supervised
and self-supervised learning methods. We believe that our Building3D will
facilitate future research on urban modeling, aerial path planning, mesh
simplification, and semantic/part segmentation etc.
</p></li>
</ul>

<h3>Title: Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation. (arXiv:2307.11958v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11958">http://arxiv.org/abs/2307.11958</a></li>
<li>Code URL: <a href="https://github.com/endoluminalsurgicalvision-imr/ccfv">https://github.com/endoluminalsurgicalvision-imr/ccfv</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11958] Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation](http://arxiv.org/abs/2307.11958) #segmentation</code></li>
<li>Summary: <p>Transfer learning is a critical technique in training deep neural networks
for the challenging medical image segmentation task that requires enormous
resources. With the abundance of medical image data, many research institutions
release models trained on various datasets that can form a huge pool of
candidate source models to choose from. Hence, it's vital to estimate the
source models' transferability (i.e., the ability to generalize across
different downstream tasks) for proper and efficient model reuse. To make up
for its deficiency when applying transfer learning to medical image
segmentation, in this paper, we therefore propose a new Transferability
Estimation (TE) method. We first analyze the drawbacks of using the existing TE
algorithms for medical image segmentation and then design a source-free TE
framework that considers both class consistency and feature variety for better
estimation. Extensive experiments show that our method surpasses all current
algorithms for transferability estimation in medical image segmentation. Code
is available at https://github.com/EndoluminalSurgicalVision-IMR/CCFV
</p></li>
</ul>

<h3>Title: Morphology-inspired Unsupervised Gland Segmentation via Selective Semantic Grouping. (arXiv:2307.11989v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11989">http://arxiv.org/abs/2307.11989</a></li>
<li>Code URL: <a href="https://github.com/xmed-lab/mssg">https://github.com/xmed-lab/mssg</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11989] Morphology-inspired Unsupervised Gland Segmentation via Selective Semantic Grouping](http://arxiv.org/abs/2307.11989) #segmentation</code></li>
<li>Summary: <p>Designing deep learning algorithms for gland segmentation is crucial for
automatic cancer diagnosis and prognosis, yet the expensive annotation cost
hinders the development and application of this technology. In this paper, we
make a first attempt to explore a deep learning method for unsupervised gland
segmentation, where no manual annotations are required. Existing unsupervised
semantic segmentation methods encounter a huge challenge on gland images: They
either over-segment a gland into many fractions or under-segment the gland
regions by confusing many of them with the background. To overcome this
challenge, our key insight is to introduce an empirical cue about gland
morphology as extra knowledge to guide the segmentation process. To this end,
we propose a novel Morphology-inspired method via Selective Semantic Grouping.
We first leverage the empirical cue to selectively mine out proposals for gland
sub-regions with variant appearances. Then, a Morphology-aware Semantic
Grouping module is employed to summarize the overall information about the
gland by explicitly grouping the semantics of its sub-region proposals. In this
way, the final segmentation network could learn comprehensive knowledge about
glands and produce well-delineated, complete predictions. We conduct
experiments on GlaS dataset and CRAG dataset. Our method exceeds the
second-best counterpart over 10.56% at mIOU.
</p></li>
</ul>

<h3>Title: COLosSAL: A Benchmark for Cold-start Active Learning for 3D Medical Image Segmentation. (arXiv:2307.12004v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12004">http://arxiv.org/abs/2307.12004</a></li>
<li>Code URL: <a href="https://github.com/medicl-vu/colossal">https://github.com/medicl-vu/colossal</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12004] COLosSAL: A Benchmark for Cold-start Active Learning for 3D Medical Image Segmentation](http://arxiv.org/abs/2307.12004) #segmentation</code></li>
<li>Summary: <p>Medical image segmentation is a critical task in medical image analysis. In
recent years, deep learning based approaches have shown exceptional performance
when trained on a fully-annotated dataset. However, data annotation is often a
significant bottleneck, especially for 3D medical images. Active learning (AL)
is a promising solution for efficient annotation but requires an initial set of
labeled samples to start active selection. When the entire data pool is
unlabeled, how do we select the samples to annotate as our initial set? This is
also known as the cold-start AL, which permits only one chance to request
annotations from experts without access to previously annotated data.
Cold-start AL is highly relevant in many practical scenarios but has been
under-explored, especially for 3D medical segmentation tasks requiring
substantial annotation effort. In this paper, we present a benchmark named
COLosSAL by evaluating six cold-start AL strategies on five 3D medical image
segmentation tasks from the public Medical Segmentation Decathlon collection.
We perform a thorough performance analysis and explore important open questions
for cold-start AL, such as the impact of budget on different strategies. Our
results show that cold-start AL is still an unsolved problem for 3D
segmentation tasks but some important trends have been observed. The code
repository, data partitions, and baseline results for the complete benchmark
are publicly available at https://github.com/MedICL-VU/COLosSAL.
</p></li>
</ul>

<h3>Title: Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space. (arXiv:2307.12032v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12032">http://arxiv.org/abs/2307.12032</a></li>
<li>Code URL: <a href="https://github.com/junzis/contrail-net">https://github.com/junzis/contrail-net</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12032] Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space](http://arxiv.org/abs/2307.12032) #segmentation</code></li>
<li>Summary: <p>Air transport poses significant environmental challenges, particularly the
contribution of flight contrails to climate change due to their potential
global warming impact. Detecting contrails from satellite images has been a
long-standing challenge. Traditional computer vision techniques have
limitations under varying image conditions, and machine learning approaches
using typical convolutional neural networks are hindered by the scarcity of
hand-labeled contrail datasets and contrail-tailored learning processes. In
this paper, we introduce an innovative model based on augmented transfer
learning that accurately detects contrails with minimal data. We also propose a
novel loss function, SR Loss, which improves contrail line detection by
transforming the image space into Hough space. Our research opens new avenues
for machine learning-based contrail detection in aviation research, offering
solutions to the lack of large hand-labeled datasets, and significantly
enhancing contrail detection models.
</p></li>
</ul>

<h3>Title: Self-Supervised and Semi-Supervised Polyp Segmentation using Synthetic Data. (arXiv:2307.12033v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12033">http://arxiv.org/abs/2307.12033</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12033] Self-Supervised and Semi-Supervised Polyp Segmentation using Synthetic Data](http://arxiv.org/abs/2307.12033) #segmentation</code></li>
<li>Summary: <p>Early detection of colorectal polyps is of utmost importance for their
treatment and for colorectal cancer prevention. Computer vision techniques have
the potential to aid professionals in the diagnosis stage, where colonoscopies
are manually carried out to examine the entirety of the patient's colon. The
main challenge in medical imaging is the lack of data, and a further challenge
specific to polyp segmentation approaches is the difficulty of manually
labeling the available data: the annotation process for segmentation tasks is
very time-consuming. While most recent approaches address the data availability
challenge with sophisticated techniques to better exploit the available labeled
data, few of them explore the self-supervised or semi-supervised paradigm,
where the amount of labeling required is greatly reduced. To address both
challenges, we leverage synthetic data and propose an end-to-end model for
polyp segmentation that integrates real and synthetic data to artificially
increase the size of the datasets and aid the training when unlabeled samples
are available. Concretely, our model, Pl-CUT-Seg, transforms synthetic images
with an image-to-image translation module and combines the resulting images
with real images to train a segmentation model, where we use model predictions
as pseudo-labels to better leverage unlabeled samples. Additionally, we propose
PL-CUT-Seg+, an improved version of the model that incorporates targeted
regularization to address the domain gap between real and synthetic images. The
models are evaluated on standard benchmarks for polyp segmentation and reach
state-of-the-art results in the self- and semi-supervised setups.
</p></li>
</ul>

<h3>Title: Hallucination Improves the Performance of Unsupervised Visual Representation Learning. (arXiv:2307.12168v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12168">http://arxiv.org/abs/2307.12168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12168] Hallucination Improves the Performance of Unsupervised Visual Representation Learning](http://arxiv.org/abs/2307.12168) #segmentation</code></li>
<li>Summary: <p>Contrastive learning models based on Siamese structure have demonstrated
remarkable performance in self-supervised learning. Such a success of
contrastive learning relies on two conditions, a sufficient number of positive
pairs and adequate variations between them. If the conditions are not met,
these frameworks will lack semantic contrast and be fragile on overfitting. To
address these two issues, we propose Hallucinator that could efficiently
generate additional positive samples for further contrast. The Hallucinator is
differentiable and creates new data in the feature space. Thus, it is optimized
directly with the pre-training task and introduces nearly negligible
computation. Moreover, we reduce the mutual information of hallucinated pairs
and smooth them through non-linear operations. This process helps avoid
over-confident contrastive learning models during the training and achieves
more transformation-invariant feature embeddings. Remarkably, we empirically
prove that the proposed Hallucinator generalizes well to various contrastive
learning models, including MoCoV1&amp;V2, SimCLR and SimSiam. Under the linear
classification protocol, a stable accuracy gain is achieved, ranging from 0.3%
to 3.0% on CIFAR10&amp;100, Tiny ImageNet, STL-10 and ImageNet. The improvement is
also observed in transferring pre-train encoders to the downstream tasks,
including object detection and segmentation.
</p></li>
</ul>

<h3>Title: Expediting Building Footprint Segmentation from High-resolution Remote Sensing Images via progressive lenient supervision. (arXiv:2307.12220v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.12220">http://arxiv.org/abs/2307.12220</a></li>
<li>Code URL: <a href="https://github.com/haonanguo/bfseg-efficient-building-footprint-segmentation-framework">https://github.com/haonanguo/bfseg-efficient-building-footprint-segmentation-framework</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.12220] Expediting Building Footprint Segmentation from High-resolution Remote Sensing Images via progressive lenient supervision](http://arxiv.org/abs/2307.12220) #segmentation</code></li>
<li>Summary: <p>The efficacy of building footprint segmentation from remotely sensed images
has been hindered by model transfer effectiveness. Many existing building
segmentation methods were developed upon the encoder-decoder architecture of
U-Net, in which the encoder is finetuned from the newly developed backbone
networks that are pre-trained on ImageNet. However, the heavy computational
burden of the existing decoder designs hampers the successful transfer of these
modern encoder networks to remote sensing tasks. Even the widely-adopted deep
supervision strategy fails to mitigate these challenges due to its invalid loss
in hybrid regions where foreground and background pixels are intermixed. In
this paper, we conduct a comprehensive evaluation of existing decoder network
designs for building footprint segmentation and propose an efficient framework
denoted as BFSeg to enhance learning efficiency and effectiveness.
Specifically, a densely-connected coarse-to-fine feature fusion decoder network
that facilitates easy and fast feature fusion across scales is proposed.
Moreover, considering the invalidity of hybrid regions in the down-sampled
ground truth during the deep supervision process, we present a lenient deep
supervision and distillation strategy that enables the network to learn proper
knowledge from deep supervision. Building upon these advancements, we have
developed a new family of building segmentation networks, which consistently
surpass prior works with outstanding performance and efficiency across a wide
range of newly developed encoder networks. The code will be released on
https://github.com/HaonanGuo/BFSeg-Efficient-Building-Footprint-Segmentation-Framework.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
