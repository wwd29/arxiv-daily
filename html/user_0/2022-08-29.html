<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Image augmentation improves few-shot classification performance in plant disease recognition. (arXiv:2208.12613v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12613">http://arxiv.org/abs/2208.12613</a></li>
<li>Code URL: <a href="https://github.com/frankyaoxiao/dataaug">https://github.com/frankyaoxiao/dataaug</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12613] Image augmentation improves few-shot classification performance in plant disease recognition](http://arxiv.org/abs/2208.12613)</code></li>
<li>Summary: <p>With the world population projected to near 10 billion by 2050, minimizing
crop damage and guaranteeing food security has never been more important.
Machine learning has been proposed as a solution to quickly and efficiently
identify diseases in crops. Convolutional Neural Networks typically require
large datasets of annotated data which are not available on demand. Collecting
this data is a long and arduous process which involves manually picking,
imaging, and annotating each individual leaf. I tackle the problem of plant
image data scarcity by exploring the efficacy of various data augmentation
techniques when used in conjunction with transfer learning. I evaluate the
impact of various data augmentation techniques both individually and combined
on the performance of a ResNet. I propose an augmentation scheme utilizing a
sequence of different augmentations which consistently improves accuracy
through many trials. Using only 10 total seed images, I demonstrate that my
augmentation framework can increase model accuracy by upwards of 25\%.
</p></li>
</ul>

<h3>Title: Unraveling Threat Intelligence Through the Lens of Malicious URL Campaigns. (arXiv:2208.12449v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12449">http://arxiv.org/abs/2208.12449</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12449] Unraveling Threat Intelligence Through the Lens of Malicious URL Campaigns](http://arxiv.org/abs/2208.12449)</code></li>
<li>Summary: <p>The daily deluge of alerts is a sombre reality for Security Operations Centre
(SOC) personnel worldwide. They are at the forefront of an organisation's
cybersecurity infrastructure, and face the unenviable task of prioritising
threats amongst a flood of abstruse alerts triggered by their Security
Information and Event Management (SIEM) systems. URLs found within malicious
communications form the bulk of such alerts, and pinpointing pertinent patterns
within them allows teams to rapidly deescalate potential or extant threats.
This need for vigilance has been traditionally filled with machine-learning
based log analysis tools and anomaly detection concepts. To sidestep machine
learning approaches, we instead propose to analyse suspicious URLs from SIEM
alerts via the perspective of malicious URL campaigns. By first grouping URLs
within 311M records gathered from VirusTotal into 2.6M suspicious clusters, we
thereafter discovered 77.8K malicious campaigns. Corroborating our suspicions,
we found 9.9M unique URLs attributable to 18.3K multi-URL campaigns, and that
worryingly, only 2.97% of campaigns were found by security vendors. We also
confer insights on evasive tactics such as ever lengthier URLs and more diverse
domain names, with selected case studies exposing other adversarial techniques.
By characterising the concerted campaigns driving these URL alerts, we hope to
inform SOC teams of current threat trends, and thus arm them with better threat
intelligence.
</p></li>
</ul>

<h3>Title: Automated False Positive Filtering for esNetwork Alerts. (arXiv:2208.12729v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12729">http://arxiv.org/abs/2208.12729</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12729] Automated False Positive Filtering for esNetwork Alerts](http://arxiv.org/abs/2208.12729)</code></li>
<li>Summary: <p>An Intrusion Detection System (IDS) is one of the security tools that can
automatically analyze network traffic and detect suspicious activities. They
are widely implemented as security guarantee tools in various business
networks. However, the high rate of false-positive alerts creates an
overwhelming number of unnecessary alerts for security analysts to sift
through. The esNetwork is an IDS product by eSentire Inc. This project focuses
on reducing the false-positive alerts generated by esNetwork with the help of a
Random Forest (RF) classifier. The RF model was built to classify the alerts as
high and low and only pass high likelihood alerts to the analysts. As a result
of evaluation experiments, this model can achieve an accuracy of 97% for
training validation, 88% for testing with the recent data, and 58% with
Security Operation Centre (SOC) reviewed events. The evaluation result of the
proposed model is intermediate because of the deficiency of clearly labeled
data for training as well as the SOC-reviewed events for evaluation. The model
still needs time to be fine-tuned to meet the industry deployment requirement.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Selective manipulation of disentangled representations for privacy-aware facial image processing. (arXiv:2208.12632v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12632">http://arxiv.org/abs/2208.12632</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12632] Selective manipulation of disentangled representations for privacy-aware facial image processing](http://arxiv.org/abs/2208.12632)</code></li>
<li>Summary: <p>Camera sensors are increasingly being combined with machine learning to
perform various tasks such as intelligent surveillance. Due to its
computational complexity, most of these machine learning algorithms are
offloaded to the cloud for processing. However, users are increasingly
concerned about privacy issues such as function creep and malicious usage by
third-party cloud providers. To alleviate this, we propose an edge-based
filtering stage that removes privacy-sensitive attributes before the sensor
data are transmitted to the cloud. We use state-of-the-art image manipulation
techniques that leverage disentangled representations to achieve privacy
filtering. We define opt-in and opt-out filter operations and evaluate their
effectiveness for filtering private attributes from face images. Additionally,
we examine the effect of naturally occurring correlations and residual
information on filtering. We find the results promising and believe this
elicits further research on how image manipulation can be used for privacy
preservation.
</p></li>
</ul>

<h3>Title: Enabling Weakly-Supervised Temporal Action Localization from On-Device Learning of the Video Stream. (arXiv:2208.12673v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12673">http://arxiv.org/abs/2208.12673</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12673] Enabling Weakly-Supervised Temporal Action Localization from On-Device Learning of the Video Stream](http://arxiv.org/abs/2208.12673)</code></li>
<li>Summary: <p>Detecting actions in videos have been widely applied in on-device
applications. Practical on-device videos are always untrimmed with both action
and background. It is desirable for a model to both recognize the class of
action and localize the temporal position where the action happens. Such a task
is called temporal action location (TAL), which is always trained on the cloud
where multiple untrimmed videos are collected and labeled. It is desirable for
a TAL model to continuously and locally learn from new data, which can directly
improve the action detection precision while protecting customers' privacy.
However, it is non-trivial to train a TAL model, since tremendous video samples
with temporal annotations are required. However, annotating videos frame by
frame is exorbitantly time-consuming and expensive. Although weakly-supervised
TAL (W-TAL) has been proposed to learn from untrimmed videos with only
video-level labels, such an approach is also not suitable for on-device
learning scenarios. In practical on-device learning applications, data are
collected in streaming. Dividing such a long video stream into multiple video
segments requires lots of human effort, which hinders the exploration of
applying the TAL tasks to realistic on-device learning applications. To enable
W-TAL models to learn from a long, untrimmed streaming video, we propose an
efficient video learning approach that can directly adapt to new environments.
We first propose a self-adaptive video dividing approach with a contrast
score-based segment merging approach to convert the video stream into multiple
segments. Then, we explore different sampling strategies on the TAL tasks to
request as few labels as possible. To the best of our knowledge, we are the
first attempt to directly learn from the on-device, long video stream.
</p></li>
</ul>

<h3>Title: Reduce Communication Costs and Preserve Privacy: Prompt Tuning Method in Federated Learning. (arXiv:2208.12268v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12268">http://arxiv.org/abs/2208.12268</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12268] Reduce Communication Costs and Preserve Privacy: Prompt Tuning Method in Federated Learning](http://arxiv.org/abs/2208.12268)</code></li>
<li>Summary: <p>Federated learning (FL) has enabled global model training on decentralized
data in a privacy-preserving way by aggregating model updates. However, for
many natural language processing (NLP) tasks that utilize pre-trained language
models (PLMs) with large numbers of parameters, there are considerable
communication costs associated with FL. Recently, prompt tuning, which tunes
some soft prompts without modifying PLMs, has achieved excellent performance as
a new learning paradigm. Therefore we want to combine the two methods and
explore the effect of prompt tuning under FL. In this paper, we propose
"FedPrompt" as the first work study prompt tuning in a model split learning way
using FL, and prove that split learning greatly reduces the communication cost,
only 0.01% of the PLMs' parameters, with little decrease on accuracy both on
IID and Non-IID data distribution. This improves the efficiency of FL method
while also protecting the data privacy in prompt tuning.In addition, like PLMs,
prompts are uploaded and downloaded between public platforms and personal
users, so we try to figure out whether there is still a backdoor threat using
only soft prompt in FL scenarios. We further conduct backdoor attacks by data
poisoning on FedPrompt. Our experiments show that normal backdoor attack can
not achieve a high attack success rate, proving the robustness of FedPrompt.We
hope this work can promote the application of prompt in FL and raise the
awareness of the possible security threats.
</p></li>
</ul>

<h3>Title: COOKIEGRAPH: Measuring and Countering First-Party Tracking Cookies. (arXiv:2208.12370v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12370">http://arxiv.org/abs/2208.12370</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12370] COOKIEGRAPH: Measuring and Countering First-Party Tracking Cookies](http://arxiv.org/abs/2208.12370)</code></li>
<li>Summary: <p>Recent privacy protections by browser vendors aim to limit the abuse of
third-party cookies for cross-site tracking. While these countermeasures
against third-party cookies are widely welcome, there are concerns that they
will result in advertisers and trackers abusing first-party cookies instead. We
provide the first empirical evidence of how first-party cookies are abused by
advertisers and trackers by conducting a differential measurement study on 10K
websites with third-party cookies allowed and blocked. We find that advertisers
and trackers implement cross-site tracking despite third-party cookie blocking
by storing identifiers, based on probabilistic and deterministic attributes, in
first-party cookies. As opposed to third-party cookies, outright first-party
cookie blocking is not practical because it would result in major breakage of
legitimate website functionality.
</p></li>
</ul>

<p>We propose CookieGraph, a machine learning approach that can accurately and
robustly detect first-party tracking cookies. CookieGraph detects first-party
tracking cookies with 91.06% accuracy, outperforming the state-of-the-art
CookieBlock approach by 10.28%. We show that CookieGraph is fully robust
against cookie name manipulation while CookieBlock's accuracy drops by 15.68%.
We also show that CookieGraph does not cause any major breakage while
CookieBlock causes major breakage on 8% of the websites with SSO logins. Our
deployment of CookieGraph shows that first-party tracking cookies are used on
93.43% of the 10K websites. We also find that the most prevalent first-party
tracking cookies are set by major advertising entities such as Google as well
as many specialized entities such as Criteo.
</p>

<h3>Title: Privacy with Good Taste: A Case Study in Quantifying Privacy Risks in Genetic Scores. (arXiv:2208.12497v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12497">http://arxiv.org/abs/2208.12497</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12497] Privacy with Good Taste: A Case Study in Quantifying Privacy Risks in Genetic Scores](http://arxiv.org/abs/2208.12497)</code></li>
<li>Summary: <p>Analysis of genetic data opens up many opportunities for medical and
scientific advances. The use of phenotypic information and polygenic risk
scores to analyze genetic data is widespread. Most work on genetic privacy
focuses on basic genetic data such as SNP values and specific genotypes. In
this paper, we introduce a novel methodology to quantify and prevent privacy
risks by focusing on polygenic scores and phenotypic information. Our
methodology is based on the tool-supported privacy risk analysis method Privug.
We demonstrate the use of Privug to assess privacy risks posed by disclosing a
polygenic trait score for bitter taste receptors, encoded by TAS2R38 and
TAS2R16, to a person's privacy in regards to their ethnicity. We provide an
extensive privacy risks analysis of different programs for genetic data
disclosure: taster phenotype, tasting polygenic score, and a polygenic score
distorted with noise. Finally, we discuss the privacy/utility trade-offs of the
polygenic score.
</p></li>
</ul>

<h3>Title: I still know it's you! On Challenges in Anonymizing Source Code. (arXiv:2208.12553v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12553">http://arxiv.org/abs/2208.12553</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12553] I still know it's you! On Challenges in Anonymizing Source Code](http://arxiv.org/abs/2208.12553)</code></li>
<li>Summary: <p>The source code of a program not only defines its semantics but also contains
subtle clues that can identify its author. Several studies have shown that
these clues can be automatically extracted using machine learning and allow for
determining a program's author among hundreds of programmers. This attribution
poses a significant threat to developers of anti-censorship and
privacy-enhancing technologies, as they become identifiable and may be
prosecuted. An ideal protection from this threat would be the anonymization of
source code. However, neither theoretical nor practical principles of such an
anonymization have been explored so far.
</p></li>
</ul>

<p>In this paper, we tackle this problem and develop a framework for reasoning
about code anonymization. We prove that the task of generating a $k$-anonymous
program -- a program that cannot be attributed to one of $k$ authors -- is not
computable and thus a dead end for research. As a remedy, we introduce a
relaxed concept called $k$-uncertainty, which enables us to measure the
protection of developers. Based on this concept, we empirically study candidate
techniques for anonymization, such as code normalization, coding style
imitation, and code obfuscation. We find that none of the techniques provides
sufficient protection when the attacker is aware of the anonymization. While we
introduce an approach for removing remaining clues from the code, the main
result of our work is negative: Anonymization of source code is a hard and open
problem.
</p>

<h3>Title: Epistemic Parity: Reproducibility as an Evaluation Metric for Differential Privacy. (arXiv:2208.12700v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12700">http://arxiv.org/abs/2208.12700</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12700] Epistemic Parity: Reproducibility as an Evaluation Metric for Differential Privacy](http://arxiv.org/abs/2208.12700)</code></li>
<li>Summary: <p>Differential privacy mechanisms are increasingly used to enable public
release of sensitive datasets, relying on strong theoretical guarantees for
privacy coupled with empirical evidence of utility. Utility is typically
measured as the error on representative proxy tasks, such as descriptive
statistics, multivariate correlations, or classification accuracy. In this
paper, we propose an alternative evaluation methodology for measuring the
utility of differentially private synthetic data in scientific research, a
measure we term "epistemic parity." Our methodology consists of reproducing
empirical conclusions of peer-reviewed papers that use publicly available
datasets, and comparing these conclusions to those based on differentially
private versions of the datasets.
</p></li>
</ul>

<p>We instantiate our methodology over a benchmark of recent peer-reviewed
papers that analyze public datasets in the ICPSR social science repository. We
reproduce visualizations (qualitative results) and statistical measures
(quantitative results) from each paper. We then generate differentially private
synthetic datasets using state-of-the-art mechanisms and assess whether the
conclusions stated in the paper hold. We find that, across reasonable epsilon
values, epistemic parity only partially holds for each synthesizer we
evaluated. Therefore, we advocate for both improving existing synthesizers and
creating new data release mechanisms that offer strong guarantees for epistemic
parity while achieving risk-aware, best effort protection from privacy attacks.
</p>

<h3>Title: Federated and Privacy-Preserving Learning of Accounting Data in Financial Statement Audits. (arXiv:2208.12708v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12708">http://arxiv.org/abs/2208.12708</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12708] Federated and Privacy-Preserving Learning of Accounting Data in Financial Statement Audits](http://arxiv.org/abs/2208.12708)</code></li>
<li>Summary: <p>The ongoing 'digital transformation' fundamentally changes audit evidence's
nature, recording, and volume. Nowadays, the International Standards on
Auditing (ISA) requires auditors to examine vast volumes of a financial
statement's underlying digital accounting records. As a result, audit firms
also 'digitize' their analytical capabilities and invest in Deep Learning (DL),
a successful sub-discipline of Machine Learning. The application of DL offers
the ability to learn specialized audit models from data of multiple clients,
e.g., organizations operating in the same industry or jurisdiction. In general,
regulations require auditors to adhere to strict data confidentiality measures.
At the same time, recent intriguing discoveries showed that large-scale DL
models are vulnerable to leaking sensitive training data information. Today, it
often remains unclear how audit firms can apply DL models while complying with
data protection regulations. In this work, we propose a Federated Learning
framework to train DL models on auditing relevant accounting data of multiple
clients. The framework encompasses Differential Privacy and Split Learning
capabilities to mitigate data confidentiality risks at model inference. We
evaluate our approach to detect accounting anomalies in three real-world
datasets of city payments. Our results provide empirical evidence that auditors
can benefit from DL models that accumulate knowledge from multiple sources of
proprietary client data.
</p></li>
</ul>

<h3>Title: Another Use of SMOTE for Interpretable Data Collaboration Analysis. (arXiv:2208.12458v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12458">http://arxiv.org/abs/2208.12458</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12458] Another Use of SMOTE for Interpretable Data Collaboration Analysis](http://arxiv.org/abs/2208.12458)</code></li>
<li>Summary: <p>Recently, data collaboration (DC) analysis has been developed for
privacy-preserving integrated analysis across multiple institutions. DC
analysis centralizes individually constructed dimensionality-reduced
intermediate representations and realizes integrated analysis via collaboration
representations without sharing the original data. To construct the
collaboration representations, each institution generates and shares a
shareable anchor dataset and centralizes its intermediate representation.
Although, random anchor dataset functions well for DC analysis in general,
using an anchor dataset whose distribution is close to that of the raw dataset
is expected to improve the recognition performance, particularly for the
interpretable DC analysis. Based on an extension of the synthetic minority
over-sampling technique (SMOTE), this study proposes an anchor data
construction technique to improve the recognition performance without
increasing the risk of data leakage. Numerical results demonstrate the
efficiency of the proposed SMOTE-based method over the existing anchor data
constructions for artificial and real-world datasets. Specifically, the
proposed method achieves 9 percentage point and 38 percentage point performance
improvements regarding accuracy and essential feature selection, respectively,
over existing methods for an income dataset. The proposed method provides
another use of SMOTE not for imbalanced data classifications but for a key
technology of privacy-preserving integrated analysis.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h2>robust</h2>
<h3>Title: Learning Continuous Implicit Representation for Near-Periodic Patterns. (arXiv:2208.12278v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12278">http://arxiv.org/abs/2208.12278</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12278] Learning Continuous Implicit Representation for Near-Periodic Patterns](http://arxiv.org/abs/2208.12278)</code></li>
<li>Summary: <p>Near-Periodic Patterns (NPP) are ubiquitous in man-made scenes and are
composed of tiled motifs with appearance differences caused by lighting,
defects, or design elements. A good NPP representation is useful for many
applications including image completion, segmentation, and geometric remapping.
But representing NPP is challenging because it needs to maintain global
consistency (tiled motifs layout) while preserving local variations (appearance
differences). Methods trained on general scenes using a large dataset or
single-image optimization struggle to satisfy these constraints, while methods
that explicitly model periodicity are not robust to periodicity detection
errors. To address these challenges, we learn a neural implicit representation
using a coordinate-based MLP with single image optimization. We design an input
feature warping module and a periodicity-guided patch loss to handle both
global consistency and local variations. To further improve the robustness, we
introduce a periodicity proposal module to search and use multiple candidate
periodicities in our pipeline. We demonstrate the effectiveness of our method
on more than 500 images of building facades, friezes, wallpapers, ground, and
Mondrian patterns on single and multi-planar scenes.
</p></li>
</ul>

<h3>Title: Arbitrary Shape Text Detection via Segmentation with Probability Maps. (arXiv:2208.12419v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12419">http://arxiv.org/abs/2208.12419</a></li>
<li>Code URL: <a href="https://github.com/gxym/textpms">https://github.com/gxym/textpms</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12419] Arbitrary Shape Text Detection via Segmentation with Probability Maps](http://arxiv.org/abs/2208.12419)</code></li>
<li>Summary: <p>Arbitrary shape text detection is a challenging task due to the significantly
varied sizes and aspect ratios, arbitrary orientations or shapes, inaccurate
annotations, etc. Due to the scalability of pixel-level prediction,
segmentation-based methods can adapt to various shape texts and hence attracted
considerable attention recently. However, accurate pixel-level annotations of
texts are formidable, and the existing datasets for scene text detection only
provide coarse-grained boundary annotations. Consequently, numerous
misclassified text pixels or background pixels inside annotations always exist,
degrading the performance of segmentation-based text detection methods.
Generally speaking, whether a pixel belongs to text or not is highly related to
the distance with the adjacent annotation boundary. With this observation, in
this paper, we propose an innovative and robust segmentation-based detection
method via probability maps for accurately detecting text instances. To be
concrete, we adopt a Sigmoid Alpha Function (SAF) to transfer the distances
between boundaries and their inside pixels to a probability map. However, one
probability map can not cover complex probability distributions well because of
the uncertainty of coarse-grained text boundary annotations. Therefore, we
adopt a group of probability maps computed by a series of Sigmoid Alpha
Functions to describe the possible probability distributions. In addition, we
propose an iterative model to learn to predict and assimilate probability maps
for providing enough information to reconstruct text instances. Finally, simple
region growth algorithms are adopted to aggregate probability maps to complete
text instances. Experimental results demonstrate that our method achieves
state-of-the-art performance in terms of detection accuracy on several
benchmarks.
</p></li>
</ul>

<h3>Title: Robust Prototypical Few-Shot Organ Segmentation with Regularized Neural-ODEs. (arXiv:2208.12428v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12428">http://arxiv.org/abs/2208.12428</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12428] Robust Prototypical Few-Shot Organ Segmentation with Regularized Neural-ODEs](http://arxiv.org/abs/2208.12428)</code></li>
<li>Summary: <p>Despite the tremendous progress made by deep learning models in image
semantic segmentation, they typically require large annotated examples, and
increasing attention is being diverted to problem settings like Few-Shot
Learning (FSL) where only a small amount of annotation is needed for
generalisation to novel classes. This is especially seen in medical domains
where dense pixel-level annotations are expensive to obtain. In this paper, we
propose Regularized Prototypical Neural Ordinary Differential Equation
(R-PNODE), a method that leverages intrinsic properties of Neural-ODEs,
assisted and enhanced by additional cluster and consistency losses to perform
Few-Shot Segmentation (FSS) of organs. R-PNODE constrains support and query
features from the same classes to lie closer in the representation space
thereby improving the performance over the existing Convolutional Neural
Network (CNN) based FSS methods. We further demonstrate that while many
existing Deep CNN based methods tend to be extremely vulnerable to adversarial
attacks, R-PNODE exhibits increased adversarial robustness for a wide array of
these attacks. We experiment with three publicly available multi-organ
segmentation datasets in both in-domain and cross-domain FSS settings to
demonstrate the efficacy of our method. In addition, we perform experiments
with seven commonly used adversarial attacks in various settings to demonstrate
R-PNODE's robustness. R-PNODE outperforms the baselines for FSS by significant
margins and also shows superior performance for a wide array of attacks varying
in intensity and design.
</p></li>
</ul>

<h3>Title: GHN-Q: Parameter Prediction for Unseen Quantized Convolutional Architectures via Graph Hypernetworks. (arXiv:2208.12489v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12489">http://arxiv.org/abs/2208.12489</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12489] GHN-Q: Parameter Prediction for Unseen Quantized Convolutional Architectures via Graph Hypernetworks](http://arxiv.org/abs/2208.12489)</code></li>
<li>Summary: <p>Deep convolutional neural network (CNN) training via iterative optimization
has had incredible success in finding optimal parameters. However, modern CNN
architectures often contain millions of parameters. Thus, any given model for a
single architecture resides in a massive parameter space. Models with similar
loss could have drastically different characteristics such as adversarial
robustness, generalizability, and quantization robustness. For deep learning on
the edge, quantization robustness is often crucial. Finding a model that is
quantization-robust can sometimes require significant efforts. Recent works
using Graph Hypernetworks (GHN) have shown remarkable performance predicting
high-performant parameters of varying CNN architectures. Inspired by these
successes, we wonder if the graph representations of GHN-2 can be leveraged to
predict quantization-robust parameters as well, which we call GHN-Q. We conduct
the first-ever study exploring the use of graph hypernetworks for predicting
parameters of unseen quantized CNN architectures. We focus on a reduced CNN
search space and find that GHN-Q can in fact predict quantization-robust
parameters for various 8-bit quantized CNNs. Decent quantized accuracies are
observed even with 4-bit quantization despite GHN-Q not being trained on it.
Quantized finetuning of GHN-Q at lower bitwidths may bring further improvements
and is currently being explored.
</p></li>
</ul>

<h3>Title: Deformation equivariant cross-modality image synthesis with paired non-aligned training data. (arXiv:2208.12491v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12491">http://arxiv.org/abs/2208.12491</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12491] Deformation equivariant cross-modality image synthesis with paired non-aligned training data](http://arxiv.org/abs/2208.12491)</code></li>
<li>Summary: <p>Cross-modality image synthesis is an active research topic with multiple
medical clinically relevant applications. Recently, methods allowing training
with paired but misaligned data have started to emerge. However, no robust and
well-performing methods applicable to a wide range of real world data sets
exist. In this work, we propose a generic solution to the problem of
cross-modality image synthesis with paired but non-aligned data by introducing
new deformation equivariance encouraging loss functions. The method consists of
joint training of an image synthesis network together with separate
registration networks and allows adversarial training conditioned on the input
even with misaligned data. The work lowers the bar for new clinical
applications by allowing effortless training of cross-modality image synthesis
networks for more difficult data sets and opens up opportunities for the
development of new generic learning based cross-modality registration
algorithms.
</p></li>
</ul>

<h3>Title: Cross-Lingual Cross-Modal Retrieval with Noise-Robust Learning. (arXiv:2208.12526v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12526">http://arxiv.org/abs/2208.12526</a></li>
<li>Code URL: <a href="https://github.com/huiguanlab/nrccr">https://github.com/huiguanlab/nrccr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12526] Cross-Lingual Cross-Modal Retrieval with Noise-Robust Learning](http://arxiv.org/abs/2208.12526)</code></li>
<li>Summary: <p>Despite the recent developments in the field of cross-modal retrieval, there
has been less research focusing on low-resource languages due to the lack of
manually annotated datasets. In this paper, we propose a noise-robust
cross-lingual cross-modal retrieval method for low-resource languages. To this
end, we use Machine Translation (MT) to construct pseudo-parallel sentence
pairs for low-resource languages. However, as MT is not perfect, it tends to
introduce noise during translation, rendering textual embeddings corrupted and
thereby compromising the retrieval performance. To alleviate this, we introduce
a multi-view self-distillation method to learn noise-robust target-language
representations, which employs a cross-attention module to generate soft
pseudo-targets to provide direct supervision from the similarity-based view and
feature-based view. Besides, inspired by the back-translation in unsupervised
MT, we minimize the semantic discrepancies between origin sentences and
back-translated sentences to further improve the noise robustness of the
textual encoder. Extensive experiments are conducted on three video-text and
image-text cross-modal retrieval benchmarks across different languages, and the
results demonstrate that our method significantly improves the overall
performance without using extra human-labeled data. In addition, equipped with
a pre-trained visual encoder from a recent vision-and-language pre-training
framework, i.e., CLIP, our model achieves a significant performance gain,
showing that our method is compatible with popular pre-training models. Code
and data are available at https://github.com/HuiGuanLab/nrccr.
</p></li>
</ul>

<h3>Title: MORI-RAN: Multi-view Robust Representation Learning via Hybrid Contrastive Fusion. (arXiv:2208.12545v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12545">http://arxiv.org/abs/2208.12545</a></li>
<li>Code URL: <a href="https://github.com/guanzhou-ke/mori-ran">https://github.com/guanzhou-ke/mori-ran</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12545] MORI-RAN: Multi-view Robust Representation Learning via Hybrid Contrastive Fusion](http://arxiv.org/abs/2208.12545)</code></li>
<li>Summary: <p>Multi-view representation learning is essential for many multi-view tasks,
such as clustering and classification. However, there are two challenging
problems plaguing the community: i)how to learn robust multi-view
representation from mass unlabeled data and ii) how to balance the view
consistency and the view specificity. To this end, in this paper, we proposed a
hybrid contrastive fusion algorithm to extract robust view-common
representation from unlabeled data. Specifically, we found that introducing an
additional representation space and aligning representations on this space
enables the model to learn robust view-common representations. At the same
time, we designed an asymmetric contrastive strategy to ensure that the model
does not obtain trivial solutions. Experimental results demonstrated that the
proposed method outperforms 12 competitive multi-view methods on four
real-world datasets in terms of clustering and classification. Our source code
will be available soon at \url{https://github.com/guanzhou-ke/mori-ran}.
</p></li>
</ul>

<h3>Title: Stain-Robust Mitotic Figure Detection for MIDOG 2022 Challenge. (arXiv:2208.12587v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12587">http://arxiv.org/abs/2208.12587</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12587] Stain-Robust Mitotic Figure Detection for MIDOG 2022 Challenge](http://arxiv.org/abs/2208.12587)</code></li>
<li>Summary: <p>The detection of mitotic figures from different scanners/sites remains an
important topic of research, owing to its potential in assisting clinicians
with tumour grading. The MItosis DOmain Generalization (MIDOG) 2022 challenge
aims to test the robustness of detection models on unseen data from multiple
scanners and tissue types for this task. We present a short summary of the
approach employed by the TIA Centre team to address this challenge. Our
approach is based on a hybrid detection model, where mitotic candidates are
segmented, before being refined by a deep learning classifier. Cross-validation
on the training images achieved the F1-score of 0.816 and 0.784 on the
preliminary test set, demonstrating the generalizability of our model to unseen
data from new scanners.
</p></li>
</ul>

<h3>Title: Take One Gram of Neural Features, Get Enhanced Group Robustness. (arXiv:2208.12625v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12625">http://arxiv.org/abs/2208.12625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12625] Take One Gram of Neural Features, Get Enhanced Group Robustness](http://arxiv.org/abs/2208.12625)</code></li>
<li>Summary: <p>Predictive performance of machine learning models trained with empirical risk
minimization (ERM) can degrade considerably under distribution shifts. The
presence of spurious correlations in training datasets leads ERM-trained models
to display high loss when evaluated on minority groups not presenting such
correlations. Extensive attempts have been made to develop methods improving
worst-group robustness. However, they require group information for each
training input or at least, a validation set with group labels to tune their
hyperparameters, which may be expensive to get or unknown a priori. In this
paper, we address the challenge of improving group robustness without group
annotation during training or validation. To this end, we propose to partition
the training dataset into groups based on Gram matrices of features extracted
by an ``identification'' model and to apply robust optimization based on these
pseudo-groups. In the realistic context where no group labels are available,
our experiments show that our approach not only improves group robustness over
ERM but also outperforms all recent baselines
</p></li>
</ul>

<h3>Title: Towards Robust Drone Vision in the Wild. (arXiv:2208.12655v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12655">http://arxiv.org/abs/2208.12655</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12655] Towards Robust Drone Vision in the Wild](http://arxiv.org/abs/2208.12655)</code></li>
<li>Summary: <p>The past few years have witnessed the burst of drone-based applications where
computer vision plays an essential role. However, most public drone-based
vision datasets focus on detection and tracking. On the other hand, the
performance of most existing image super-resolution methods is sensitive to the
dataset, specifically, the degradation model between high-resolution and
low-resolution images. In this thesis, we propose the first image
super-resolution dataset for drone vision. Image pairs are captured by two
cameras on the drone with different focal lengths. We collect data at different
altitudes and then propose pre-processing steps to align image pairs. Extensive
empirical studies show domain gaps exist among images captured at different
altitudes. Meanwhile, the performance of pretrained image super-resolution
networks also suffers a drop on our dataset and varies among altitudes.
Finally, we propose two methods to build a robust image super-resolution
network at different altitudes. The first feeds altitude information into the
network through altitude-aware layers. The second uses one-shot learning to
quickly adapt the super-resolution model to unknown altitudes. Our results
reveal that the proposed methods can efficiently improve the performance of
super-resolution networks at varying altitudes.
</p></li>
</ul>

<h3>Title: Multi tasks RetinaNet for mitosis detection. (arXiv:2208.12657v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12657">http://arxiv.org/abs/2208.12657</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12657] Multi tasks RetinaNet for mitosis detection](http://arxiv.org/abs/2208.12657)</code></li>
<li>Summary: <p>The account of mitotic cells is a key feature in tumor diagnosis. However,
due to the variability of mitotic cell morphology, it is a highly challenging
task to detect mitotic cells in tumor tissues. At the same time, although
advanced deep learning method have achieved great success in cell detection,
the performance is often unsatisfactory when tested data from another domain
(i.e. the different tumor types and different scanners). Therefore, it is
necessary to develop algorithms for detecting mitotic cells with robustness in
domain shifts scenarios. Our work further proposes a foreground detection and
tumor classification task based on the baseline(Retinanet), and utilizes data
augmentation to improve the domain generalization performance of our model. We
achieve the state-of-the-art performance (F1 score: 0.5809) on the challenging
premilary test dataset.
</p></li>
</ul>

<h3>Title: Multi-Scale Architectures Matter: On the Adversarial Robustness of Flow-based Lossless Compression. (arXiv:2208.12716v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12716">http://arxiv.org/abs/2208.12716</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12716] Multi-Scale Architectures Matter: On the Adversarial Robustness of Flow-based Lossless Compression](http://arxiv.org/abs/2208.12716)</code></li>
<li>Summary: <p>As a probabilistic modeling technique, the flow-based model has demonstrated
remarkable potential in the field of lossless compression
\cite{idf,idf++,lbb,ivpf,iflow},. Compared with other deep generative models
(eg. Autoregressive, VAEs) \cite{bitswap,hilloc,pixelcnn++,pixelsnail} that
explicitly model the data distribution probabilities, flow-based models perform
better due to their excellent probability density estimation and satisfactory
inference speed. In flow-based models, multi-scale architecture provides a
shortcut from the shallow layer to the output layer, which significantly
reduces the computational complexity and avoid performance degradation when
adding more layers. This is essential for constructing an advanced flow-based
learnable bijective mapping. Furthermore, the lightweight requirement of the
model design in practical compression tasks suggests that flows with
multi-scale architecture achieve the best trade-off between coding complexity
and compression efficiency.
</p></li>
</ul>

<h3>Title: T-Person-GAN: Text-to-Person Image Generation with Identity-Consistency and Manifold Mix-Up. (arXiv:2208.12752v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12752">http://arxiv.org/abs/2208.12752</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12752] T-Person-GAN: Text-to-Person Image Generation with Identity-Consistency and Manifold Mix-Up](http://arxiv.org/abs/2208.12752)</code></li>
<li>Summary: <p>In this paper, we present an end-to-end approach to generate high-resolution
person images conditioned on texts only. State-of-the-art text-to-image
generation models are mainly designed for center-object generation, e.g.,
flowers and birds. Unlike center-placed objects with similar shapes and
orientation, person image generation is a more challenging task, for which we
observe the followings: 1) the generated images for the same person exhibit
visual details with identity-consistency, e.g., identity-related
textures/clothes/shoes across the images, and 2) those images should be
discriminant for being robust against the inter-person variations caused by
visual ambiguities. To address the above challenges, we develop an effective
generative model to produce person images with two novel mechanisms. In
particular, our first mechanism (called T-Person-GAN-ID) is to integrate the
one-stream generator with an identity-preserving network such that the
representations of generated data are regularized in their feature space to
ensure the identity-consistency. The second mechanism (called
T-Person-GAN-ID-MM) is based on the manifold mix-up to produce mixed images via
the linear interpolation across generated images from different manifold
identities, and we further enforce such interpolated images to be linearly
classified in the feature space. This amounts to learning a linear
classification boundary that can perfectly separate images from two identities.
Our proposed method is empirically validated to achieve a remarkable
improvement in text-to-person image generation. Our architecture is orthogonal
to StackGAN++ , and focuses on person image generation, with all of them
together to enrich the spectrum of GANs for the image generation task. Codes
are available on
\url{https://github.com/linwu-github/Person-Image-Generation.git}.
</p></li>
</ul>

<h3>Title: Leveraging Synthetic Data to Learn Video Stabilization Under Adverse Conditions. (arXiv:2208.12763v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12763">http://arxiv.org/abs/2208.12763</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12763] Leveraging Synthetic Data to Learn Video Stabilization Under Adverse Conditions](http://arxiv.org/abs/2208.12763)</code></li>
<li>Summary: <p>Video stabilization plays a central role to improve videos quality. However,
despite the substantial progress made by these methods, they were, mainly,
tested under standard weather and lighting conditions, and may perform poorly
under adverse conditions. In this paper, we propose a synthetic-aware adverse
weather robust algorithm for video stabilization that does not require real
data and can be trained only on synthetic data. We also present Silver, a novel
rendering engine to generate the required training data with an automatic
ground-truth extraction procedure. Our approach uses our specially generated
synthetic data for training an affine transformation matrix estimator avoiding
the feature extraction issues faced by current methods. Additionally, since no
video stabilization datasets under adverse conditions are available, we propose
the novel VSAC105Real dataset for evaluation. We compare our method to five
state-of-the-art video stabilization algorithms using two benchmarks. Our
results show that current approaches perform poorly in at least one weather
condition, and that, even training in a small dataset with synthetic data only,
we achieve the best performance in terms of stability average score, distortion
score, success rate, and average cropping ratio when considering all weather
conditions. Hence, our video stabilization model generalizes well on real-world
videos and does not require large-scale synthetic training data to converge.
</p></li>
</ul>

<h3>Title: Effectiveness of Mining Audio and Text Pairs from Public Data for Improving ASR Systems for Low-Resource Languages. (arXiv:2208.12666v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12666">http://arxiv.org/abs/2208.12666</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12666] Effectiveness of Mining Audio and Text Pairs from Public Data for Improving ASR Systems for Low-Resource Languages](http://arxiv.org/abs/2208.12666)</code></li>
<li>Summary: <p>End-to-end (E2E) models have become the default choice for state-of-the-art
speech recognition systems. Such models are trained on large amounts of
labelled data, which are often not available for low-resource languages.
Techniques such as self-supervised learning and transfer learning hold promise,
but have not yet been effective in training accurate models. On the other hand,
collecting labelled datasets on a diverse set of domains and speakers is very
expensive. In this work, we demonstrate an inexpensive and effective
alternative to these approaches by ``mining'' text and audio pairs for Indian
languages from public sources, specifically from the public archives of All
India Radio. As a key component, we adapt the Needleman-Wunsch algorithm to
align sentences with corresponding audio segments given a long audio and a PDF
of its transcript, while being robust to errors due to OCR, extraneous text,
and non-transcribed speech. We thus create Shrutilipi, a dataset which contains
over 6,400 hours of labelled audio across 12 Indian languages totalling to
4.95M sentences. On average, Shrutilipi results in a 2.3x increase over
publicly available labelled data. We establish the quality of Shrutilipi with
21 human evaluators across the 12 languages. We also establish the diversity of
Shrutilipi in terms of represented regions, speakers, and mentioned named
entities. Significantly, we show that adding Shrutilipi to the training set of
Wav2Vec models leads to an average decrease in WER of 5.8\% for 7 languages on
the IndicSUPERB benchmark. For Hindi, which has the most benchmarks (7), the
average WER falls from 18.8% to 13.5%. This improvement extends to efficient
models: We show a 2.3% drop in WER for a Conformer model (10x smaller than
Wav2Vec). Finally, we demonstrate the diversity of Shrutilipi by showing that
the model trained with it is more robust to noisy input.
</p></li>
</ul>

<h3>Title: Deep Hypergraph Structure Learning. (arXiv:2208.12547v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12547">http://arxiv.org/abs/2208.12547</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12547] Deep Hypergraph Structure Learning](http://arxiv.org/abs/2208.12547)</code></li>
<li>Summary: <p>Learning on high-order correlation has shown superiority in data
representation learning, where hypergraph has been widely used in recent
decades. The performance of hypergraph-based representation learning methods,
such as hypergraph neural networks, highly depends on the quality of the
hypergraph structure. How to generate the hypergraph structure among data is
still a challenging task. Missing and noisy data may lead to "bad connections"
in the hypergraph structure and destroy the hypergraph-based representation
learning process. Therefore, revealing the high-order structure, i.e., the
hypergraph behind the observed data, becomes an urgent but important task. To
address this issue, we design a general paradigm of deep hypergraph structure
learning, namely DeepHGSL, to optimize the hypergraph structure for
hypergraph-based representation learning. Concretely, inspired by the
information bottleneck principle for the robustness issue, we first extend it
to the hypergraph case, named by the hypergraph information bottleneck (HIB)
principle. Then, we apply this principle to guide the hypergraph structure
learning, where the HIB is introduced to construct the loss function to
minimize the noisy information in the hypergraph structure. The hypergraph
structure can be optimized and this process can be regarded as enhancing the
correct connections and weakening the wrong connections in the training phase.
Therefore, the proposed method benefits to extract more robust representations
even on a heavily noisy structure. Finally, we evaluate the model on four
benchmark datasets for representation learning. The experimental results on
both graph- and hypergraph-structured data demonstrate the effectiveness and
robustness of our method compared with other state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Towards Higher-order Topological Consistency for Unsupervised Network Alignment. (arXiv:2208.12463v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12463">http://arxiv.org/abs/2208.12463</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12463] Towards Higher-order Topological Consistency for Unsupervised Network Alignment](http://arxiv.org/abs/2208.12463)</code></li>
<li>Summary: <p>Network alignment task, which aims to identify corresponding nodes in
different networks, is of great significance for many subsequent applications.
Without the need for labeled anchor links, unsupervised alignment methods have
been attracting more and more attention. However, the topological consistency
assumptions defined by existing methods are generally low-order and less
accurate because only the edge-indiscriminative topological pattern is
considered, which is especially risky in an unsupervised setting. To reposition
the focus of the alignment process from low-order to higher-order topological
consistency, in this paper, we propose a fully unsupervised network alignment
framework named HTC. The proposed higher-order topological consistency is
formulated based on edge orbits, which is merged into the information
aggregation process of a graph convolutional network so that the alignment
consistencies are transformed into the similarity of node embeddings.
Furthermore, the encoder is trained to be multi-orbit-aware and then be refined
to identify more trusted anchor links. Node correspondence is comprehensively
evaluated by integrating all different orders of consistency. {In addition to
sound theoretical analysis, the superiority of the proposed method is also
empirically demonstrated through extensive experimental evaluation. On three
pairs of real-world datasets and two pairs of synthetic datasets, our HTC
consistently outperforms a wide variety of unsupervised and supervised methods
with the least or comparable time consumption. It also exhibits robustness to
structural noise as a result of our multi-orbit-aware training mechanism.
</p></li>
</ul>

<h3>Title: Lower Difficulty and Better Robustness: A Bregman Divergence Perspective for Adversarial Training. (arXiv:2208.12511v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12511">http://arxiv.org/abs/2208.12511</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12511] Lower Difficulty and Better Robustness: A Bregman Divergence Perspective for Adversarial Training](http://arxiv.org/abs/2208.12511)</code></li>
<li>Summary: <p>In this paper, we investigate on improving the adversarial robustness
obtained in adversarial training (AT) via reducing the difficulty of
optimization. To better study this problem, we build a novel Bregman divergence
perspective for AT, in which AT can be viewed as the sliding process of the
training data points on the negative entropy curve. Based on this perspective,
we analyze the learning objectives of two typical AT methods, i.e., PGD-AT and
TRADES, and we find that the optimization process of TRADES is easier than
PGD-AT for that TRADES separates PGD-AT. In addition, we discuss the function
of entropy in TRADES, and we find that models with high entropy can be better
robustness learners. Inspired by the above findings, we propose two methods,
i.e., FAIT and MER, which can both not only reduce the difficulty of
optimization under the 10-step PGD adversaries, but also provide better
robustness. Our work suggests that reducing the difficulty of optimization
under the 10-step PGD adversaries is a promising approach for enhancing the
adversarial robustness in AT.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: GRASP: Guiding model with RelAtional Semantics using Prompt. (arXiv:2208.12494v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12494">http://arxiv.org/abs/2208.12494</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12494] GRASP: Guiding model with RelAtional Semantics using Prompt](http://arxiv.org/abs/2208.12494)</code></li>
<li>Summary: <p>The dialogue-based relation extraction (DialogRE) task aims to predict the
relations between argument pairs that appear in dialogue. Most previous studies
utilize fine-tuning pre-trained language models (PLMs) only with extensive
features to supplement the low information density of the dialogue by multiple
speakers. To effectively exploit inherent knowledge of PLMs without extra
layers and consider scattered semantic cues on the relation between the
arguments, we propose a Guiding model with RelAtional Semantics using Prompt
(GRASP). We adopt a prompt-based fine-tuning approach and capture relational
semantic clues of a given dialogue with 1) an argument-aware prompt marker
strategy and 2) the relational clue detection task. In the experiments, GRASP
achieves state-of-the-art performance in terms of both F1 and F1c scores on a
DialogRE dataset even though our method only leverages PLMs without adding any
extra layers.
</p></li>
</ul>

<h3>Title: SNAP: Efficient Extraction of Private Properties with Poisoning. (arXiv:2208.12348v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12348">http://arxiv.org/abs/2208.12348</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12348] SNAP: Efficient Extraction of Private Properties with Poisoning](http://arxiv.org/abs/2208.12348)</code></li>
<li>Summary: <p>Property inference attacks allow an adversary to extract global properties of
the training dataset from a machine learning model. Such attacks have privacy
implications for data owners who share their datasets to train machine learning
models. Several existing approaches for property inference attacks against deep
neural networks have been proposed, but they all rely on the attacker training
a large number of shadow models, which induces large computational overhead.
</p></li>
</ul>

<p>In this paper, we consider the setting of property inference attacks in which
the attacker can poison a subset of the training dataset and query the trained
target model. Motivated by our theoretical analysis of model confidences under
poisoning, we design an efficient property inference attack, SNAP, which
obtains higher attack success and requires lower amounts of poisoning than the
state-of-the-art poisoning-based property inference attack by Mahloujifar et
al. For example, on the Census dataset, SNAP achieves 34% higher success rate
than Mahloujifar et al. while being 56.5x faster. We also extend our attack to
determine if a certain property is present at all in training, and estimate the
exact proportion of a property of interest efficiently. We evaluate our attack
on several properties of varying proportions from four datasets, and
demonstrate SNAP's generality and effectiveness.
</p>

<h3>Title: Learning and Compositionality: a Unification Attempt via Connectionist Probabilistic Programming. (arXiv:2208.12789v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12789">http://arxiv.org/abs/2208.12789</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12789] Learning and Compositionality: a Unification Attempt via Connectionist Probabilistic Programming](http://arxiv.org/abs/2208.12789)</code></li>
<li>Summary: <p>We consider learning and compositionality as the key mechanisms towards
simulating human-like intelligence. While each mechanism is successfully
achieved by neural networks and symbolic AIs, respectively, it is the
combination of the two mechanisms that makes human-like intelligence possible.
Despite the numerous attempts on building hybrid neuralsymbolic systems, we
argue that our true goal should be unifying learning and compositionality, the
core mechanisms, instead of neural and symbolic methods, the surface approaches
to achieve them. In this work, we review and analyze the strengths and
weaknesses of neural and symbolic methods by separating their forms and
meanings (structures and semantics), and propose Connectionist Probabilistic
Program (CPPs), a framework that connects connectionist structures (for
learning) and probabilistic program semantics (for compositionality). Under the
framework, we design a CPP extension for small scale sequence modeling and
provide a learning algorithm based on Bayesian inference. Although challenges
exist in learning complex patterns without supervision, our early results
demonstrate CPP's successful extraction of concepts and relations from raw
sequential data, an initial step towards compositional learning.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: DPAUC: Differentially Private AUC Computation in Federated Learning. (arXiv:2208.12294v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12294">http://arxiv.org/abs/2208.12294</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12294] DPAUC: Differentially Private AUC Computation in Federated Learning](http://arxiv.org/abs/2208.12294)</code></li>
<li>Summary: <p>Federated learning (FL) has gained significant attention recently as a
privacy-enhancing tool to jointly train a machine learning model by multiple
participants. The prior work on FL has mostly studied how to protect label
privacy during model training. However, model evaluation in FL might also lead
to potential leakage of private label information. In this work, we propose an
evaluation algorithm that can accurately compute the widely used AUC (area
under the curve) metric when using the label differential privacy (DP) in FL.
Through extensive experiments, we show our algorithms can compute accurate AUCs
compared to the ground truth.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Automatic detection of faults in race walking from a smartphone camera: a comparison of an Olympic medalist and university athletes. (arXiv:2208.12646v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12646">http://arxiv.org/abs/2208.12646</a></li>
<li>Code URL: <a href="https://github.com/szucchini/racewalk-aijudge">https://github.com/szucchini/racewalk-aijudge</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12646] Automatic detection of faults in race walking from a smartphone camera: a comparison of an Olympic medalist and university athletes](http://arxiv.org/abs/2208.12646)</code></li>
<li>Summary: <p>Automatic fault detection is a major challenge in many sports. In race
walking, referees visually judge faults according to the rules. Hence, ensuring
objectivity and fairness while judging is important. To address this issue,
some studies have attempted to use sensors and machine learning to
automatically detect faults. However, there are problems associated with sensor
attachments and equipment such as a high-speed camera, which conflict with the
visual judgement of referees, and the interpretability of the fault detection
models. In this study, we proposed a fault detection system for non-contact
measurement. We used pose estimation and machine learning models trained based
on the judgements of multiple qualified referees to realize fair fault
judgement. We verified them using smartphone videos of normal race walking and
walking with intentional faults in several athletes including the medalist of
the Tokyo Olympics. The validation results show that the proposed system
detected faults with an average accuracy of over 90%. We also revealed that the
machine learning model detects faults according to the rules of race walking.
In addition, the intentional faulty walking movement of the medalist was
different from that of university walkers. This finding informs realization of
a more general fault detection model. The code and data are available at
https://github.com/SZucchini/racewalk-aijudge.
</p></li>
</ul>

<h3>Title: Comparing Apples to Oranges: Learning Similarity Functions for Data Produced by Different Distributions. (arXiv:2208.12731v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12731">http://arxiv.org/abs/2208.12731</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12731] Comparing Apples to Oranges: Learning Similarity Functions for Data Produced by Different Distributions](http://arxiv.org/abs/2208.12731)</code></li>
<li>Summary: <p>Similarity functions measure how comparable pairs of elements are, and play a
key role in a wide variety of applications, e.g., Clustering problems and
considerations of Individual Fairness. However, access to an accurate
similarity function should not always be considered guaranteed. Specifically,
when the elements to be compared are produced by different distributions, or in
other words belong to different ``demographic'' groups, knowledge of their true
similarity might be very difficult to obtain. In this work, we present a
sampling framework that learns these across-groups similarity functions, using
only a limited amount of experts' feedback. We show analytical results with
rigorous bounds, and empirically validate our algorithms via a large suite of
experiments.
</p></li>
</ul>

<h3>Title: Socially Fair Reinforcement Learning. (arXiv:2208.12584v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12584">http://arxiv.org/abs/2208.12584</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12584] Socially Fair Reinforcement Learning](http://arxiv.org/abs/2208.12584)</code></li>
<li>Summary: <p>We consider the problem of episodic reinforcement learning where there are
multiple stakeholders with different reward functions. Our goal is to output a
policy that is socially fair with respect to different reward functions. Prior
works have proposed different objectives that a fair policy must optimize
including minimum welfare, and generalized Gini welfare. We first take an
axiomatic view of the problem, and propose four axioms that any such fair
objective must satisfy. We show that the Nash social welfare is the unique
objective that uniquely satisfies all four objectives, whereas prior objectives
fail to satisfy all four axioms. We then consider the learning version of the
problem where the underlying model i.e. Markov decision process is unknown. We
consider the problem of minimizing regret with respect to the fair policies
maximizing three different fair objectives -- minimum welfare, generalized Gini
welfare, and Nash social welfare. Based on optimistic planning, we propose a
generic learning algorithm and derive its regret bound with respect to the
three different policies. For the objective of Nash social welfare, we also
derive a lower bound in regret that grows exponentially with $n$, the number of
agents. Finally, we show that for the objective of minimum welfare, one can
improve regret by a factor of $O(H)$ for a weaker notion of regret.
</p></li>
</ul>

<h3>Title: LUCID: Exposing Algorithmic Bias through Inverse Design. (arXiv:2208.12786v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.12786">http://arxiv.org/abs/2208.12786</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.12786] LUCID: Exposing Algorithmic Bias through Inverse Design](http://arxiv.org/abs/2208.12786)</code></li>
<li>Summary: <p>AI systems can create, propagate, support, and automate bias in
decision-making processes. To mitigate biased decisions, we both need to
understand the origin of the bias and define what it means for an algorithm to
make fair decisions. Most group fairness notions assess a model's equality of
outcome by computing statistical metrics on the outputs. We argue that these
output metrics encounter intrinsic obstacles and present a complementary
approach that aligns with the increasing focus on equality of treatment. By
Locating Unfairness through Canonical Inverse Design (LUCID), we generate a
canonical set that shows the desired inputs for a model given a preferred
output. The canonical set reveals the model's internal logic and exposes
potential unethical biases by repeatedly interrogating the decision-making
process. We evaluate LUCID on the UCI Adult and COMPAS data sets and find that
some biases detected by a canonical set differ from those of output metrics.
The results show that by shifting the focus towards equality of treatment and
looking into the algorithm's internal workings, the canonical sets are a
valuable addition to the toolbox of algorithmic fairness evaluation.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
