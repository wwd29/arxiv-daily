<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: TUSH-Key: Transferable User Secrets on Hardware Key. (arXiv:2307.07484v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07484">http://arxiv.org/abs/2307.07484</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07484] TUSH-Key: Transferable User Secrets on Hardware Key](http://arxiv.org/abs/2307.07484) #secure</code></li>
<li>Summary: <p>Passwordless authentication was first tested for seamless and secure merchant
payments without the use of passwords or pins. It opened a whole new world of
authentications giving up the former reliance on traditional passwords. It
relied on the W3C Web Authentication (WebAuthn) and Client to Authenticator
Protocol (CTAP) standards to use the public key cryptosystem to uniquely attest
a user's device and then their identity. These standards comprise of the FIDO
authentication standard. As the popularity of passwordless is increasing, more
and more users and service providers are adopting to it. However, the concept
of device attestation makes it device-specific for a user. It makes it
difficult for a user to switch devices. FIDO Passkeys were aimed at solving the
same, synchronizing the private cryptographic keys across multiple devices so
that the user can perform passwordless authentication even from devices not
explicitly enrolled with the service provider. However, passkeys have certain
drawbacks including that it uses proprietary end to end encryption algorithms,
all keys pass through proprietary cloud provider, and it is usually not very
seamless when dealing with cross-platform key synchronization. To deal with the
problems and drawbacks of FIDO Passkeys, the paper proposes a novel private key
management system for passwordless authentication called Transferable User
Secret on Hardware Key (TUSH-Key). TUSH-Key allows cross-platform
synchronization of devices for seamless passwordless logins with FIDO2
specifications.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: TALL: Thumbnail Layout for Deepfake Video Detection. (arXiv:2307.07494v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07494">http://arxiv.org/abs/2307.07494</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07494] TALL: Thumbnail Layout for Deepfake Video Detection](http://arxiv.org/abs/2307.07494) #security</code></li>
<li>Summary: <p>The growing threats of deepfakes to society and cybersecurity have raised
enormous public concerns, and increasing efforts have been devoted to this
critical topic of deepfake video detection. Existing video methods achieve good
performance but are computationally intensive. This paper introduces a simple
yet effective strategy named Thumbnail Layout (TALL), which transforms a video
clip into a pre-defined layout to realize the preservation of spatial and
temporal dependencies. Specifically, consecutive frames are masked in a fixed
position in each frame to improve generalization, then resized to sub-images
and rearranged into a pre-defined layout as the thumbnail. TALL is
model-agnostic and extremely simple by only modifying a few lines of code.
Inspired by the success of vision transformers, we incorporate TALL into Swin
Transformer, forming an efficient and effective method TALL-Swin. Extensive
experiments on intra-dataset and cross-dataset validate the validity and
superiority of TALL and SOTA TALL-Swin. TALL-Swin achieves 90.79$\%$ AUC on the
challenging cross-dataset task, FaceForensics++ $\to$ Celeb-DF. The code is
available at https://github.com/rainy-xu/TALL4Deepfake.
</p></li>
</ul>

<h3>Title: A Controlled Experiment on the Impact of Intrusion Detection False Alarm Rate on Analyst Performance. (arXiv:2307.07023v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07023">http://arxiv.org/abs/2307.07023</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07023] A Controlled Experiment on the Impact of Intrusion Detection False Alarm Rate on Analyst Performance](http://arxiv.org/abs/2307.07023) #security</code></li>
<li>Summary: <p>Organizations use intrusion detection systems (IDSes) to identify harmful
activity among millions of computer network events. Cybersecurity analysts
review IDS alarms to verify whether malicious activity occurred and to take
remedial action. However, IDS systems exhibit high false alarm rates. This
study examines the impact of IDS false alarm rate on human analyst sensitivity
(probability of detection), precision (positive predictive value), and time on
task when evaluating IDS alarms. A controlled experiment was conducted with
participants divided into two treatment groups, 50% IDS false alarm rate and
86% false alarm rate, who classified whether simulated IDS alarms were true or
false alarms. Results show statistically significant differences in precision
and time on task. The median values for the 86% false alarm rate group were 47%
lower precision and 40% slower time on task than the 50% false alarm rate
group. No significant difference in analyst sensitivity was observed.
</p></li>
</ul>

<h3>Title: Proof of Training (PoT): Harnessing Crypto Mining Power for Distributed AI Training. (arXiv:2307.07066v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07066">http://arxiv.org/abs/2307.07066</a></li>
<li>Code URL: <a href="https://github.com/p-how/proof-of-training">https://github.com/p-how/proof-of-training</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07066] Proof of Training (PoT): Harnessing Crypto Mining Power for Distributed AI Training](http://arxiv.org/abs/2307.07066) #security</code></li>
<li>Summary: <p>In the midst of the emerging trend of integrating artificial intelligence
(AI) with crypto mining, we identify three major challenges that create a gap
between these two fields. To bridge this gap, we introduce the
proof-of-training (PoT) protocol, an approach that combines the strengths of
both AI and blockchain technology. The PoT protocol utilizes the practical
Byzantine fault tolerance (PBFT) consensus mechanism to synchronize global
states. To evaluate the performance of the protocol design, we present an
implementation of a decentralized training network (DTN) that adopts the PoT
protocol. Our results indicate that the protocol exhibits considerable
potential in terms of task throughput, system robustness, and network security.
</p></li>
</ul>

<h2>privacy</h2>
<h2>protect</h2>
<h3>Title: DIALGEN: Collaborative Human-LM Generated Dialogues for Improved Understanding of Human-Human Conversations. (arXiv:2307.07047v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07047">http://arxiv.org/abs/2307.07047</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07047] DIALGEN: Collaborative Human-LM Generated Dialogues for Improved Understanding of Human-Human Conversations](http://arxiv.org/abs/2307.07047) #protect</code></li>
<li>Summary: <p>Applications that could benefit from automatic understanding of human-human
conversations often come with challenges associated with private information in
real-world data such as call center or clinical conversations. Working with
protected data also increases costs of annotation, which limits technology
development. To address these challenges, we propose DIALGEN, a
human-in-the-loop semi-automated dialogue generation framework. DIALGEN uses a
language model (ChatGPT) that can follow schema and style specifications to
produce fluent conversational text, generating a complex conversation through
iteratively generating subdialogues and using human feedback to correct
inconsistencies or redirect the flow. In experiments on structured
summarization of agent-client information gathering calls, framed as dialogue
state tracking, we show that DIALGEN data enables significant improvement in
model performance.
</p></li>
</ul>

<h3>Title: Evaluation Methodologies in Software Protection Research. (arXiv:2307.07300v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07300">http://arxiv.org/abs/2307.07300</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07300] Evaluation Methodologies in Software Protection Research](http://arxiv.org/abs/2307.07300) #protect</code></li>
<li>Summary: <p>Man-at-the-end (MATE) attackers have full control over the system on which
the attacked software runs, and try to break the confidentiality or integrity
of assets embedded in the software. Both companies and malware authors want to
prevent such attacks. This has driven an arms race between attackers and
defenders, resulting in a plethora of different protection and analysis
methods. However, it remains difficult to measure the strength of protections
because MATE attackers can reach their goals in many different ways and a
universally accepted evaluation methodology does not exist. This survey
systematically reviews the evaluation methodologies of papers on obfuscation, a
major class of protections against MATE attacks. For 572 papers, we collected
113 aspects of their evaluation methodologies, ranging from sample set types
and sizes, over sample treatment, to performed measurements. We provide
detailed insights into how the academic state of the art evaluates both the
protections and analyses thereon. In summary, there is a clear need for better
evaluation methodologies. We identify nine challenges for software protection
evaluations, which represent threats to the validity, reproducibility, and
interpretation of research results in the context of MATE attacks.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Erasing, Transforming, and Noising Defense Network for Occluded Person Re-Identification. (arXiv:2307.07187v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07187">http://arxiv.org/abs/2307.07187</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07187] Erasing, Transforming, and Noising Defense Network for Occluded Person Re-Identification](http://arxiv.org/abs/2307.07187) #defense</code></li>
<li>Summary: <p>Occlusion perturbation presents a significant challenge in person
re-identification (re-ID), and existing methods that rely on external visual
cues require additional computational resources and only consider the issue of
missing information caused by occlusion. In this paper, we propose a simple yet
effective framework, termed Erasing, Transforming, and Noising Defense Network
(ETNDNet), which treats occlusion as a noise disturbance and solves occluded
person re-ID from the perspective of adversarial defense. In the proposed
ETNDNet, we introduce three strategies: Firstly, we randomly erase the feature
map to create an adversarial representation with incomplete information,
enabling adversarial learning of identity loss to protect the re-ID system from
the disturbance of missing information. Secondly, we introduce random
transformations to simulate the position misalignment caused by occlusion,
training the extractor and classifier adversarially to learn robust
representations immune to misaligned information. Thirdly, we perturb the
feature map with random values to address noisy information introduced by
obstacles and non-target pedestrians, and employ adversarial gaming in the
re-ID system to enhance its resistance to occlusion noise. Without bells and
whistles, ETNDNet has three key highlights: (i) it does not require any
external modules with parameters, (ii) it effectively handles various issues
caused by occlusion from obstacles and non-target pedestrians, and (iii) it
designs the first GAN-based adversarial defense paradigm for occluded person
re-ID. Extensive experiments on five public datasets fully demonstrate the
effectiveness, superiority, and practicality of the proposed ETNDNet. The code
will be released at \url{https://github.com/nengdong96/ETNDNet}.
</p></li>
</ul>

<h3>Title: Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning. (arXiv:2307.07250v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07250">http://arxiv.org/abs/2307.07250</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07250] Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning](http://arxiv.org/abs/2307.07250) #defense</code></li>
<li>Summary: <p>Adversarial examples derived from deliberately crafted perturbations on
visual inputs can easily harm decision process of deep neural networks. To
prevent potential threats, various adversarial training-based defense methods
have grown rapidly and become a de facto standard approach for robustness.
Despite recent competitive achievements, we observe that adversarial
vulnerability varies across targets and certain vulnerabilities remain
prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with
deeper architectures and advanced defense methods. To address this issue, in
this paper, we introduce a causal approach called Adversarial Double Machine
Learning (ADML), which allows us to quantify the degree of adversarial
vulnerability for network predictions and capture the effect of treatments on
outcome of interests. ADML can directly estimate causal parameter of
adversarial perturbations per se and mitigate negative effects that can
potentially damage robustness, bridging a causal perspective into the
adversarial vulnerability. Through extensive experiments on various CNN and
Transformer architectures, we corroborate that ADML improves adversarial
robustness with large margins and relieve the empirical observation.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Vulnerability-Aware Instance Reweighting For Adversarial Training. (arXiv:2307.07167v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07167">http://arxiv.org/abs/2307.07167</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07167] Vulnerability-Aware Instance Reweighting For Adversarial Training](http://arxiv.org/abs/2307.07167) #attack</code></li>
<li>Summary: <p>Adversarial Training (AT) has been found to substantially improve the
robustness of deep learning classifiers against adversarial attacks. AT
involves obtaining robustness by including adversarial examples in training a
classifier. Most variants of AT algorithms treat every training example
equally. However, recent works have shown that better performance is achievable
by treating them unequally. In addition, it has been observed that AT exerts an
uneven influence on different classes in a training set and unfairly hurts
examples corresponding to classes that are inherently harder to classify.
Consequently, various reweighting schemes have been proposed that assign
unequal weights to robust losses of individual examples in a training set. In
this work, we propose a novel instance-wise reweighting scheme. It considers
the vulnerability of each natural example and the resulting information loss on
its adversarial counterpart occasioned by adversarial attacks. Through
extensive experiments, we show that our proposed method significantly improves
over existing reweighting schemes, especially against strong white and
black-box attacks.
</p></li>
</ul>

<h3>Title: Information Leakage from Optical Emanations. (arXiv:2307.07043v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07043">http://arxiv.org/abs/2307.07043</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07043] Information Leakage from Optical Emanations](http://arxiv.org/abs/2307.07043) #attack</code></li>
<li>Summary: <p>A previously unknown form of compromising emanations has been discovered. LED
status indicators on data communication equipment, under certain conditions,
are shown to carry a modulated optical signal that is significantly correlated
with information being processed by the device. Physical access is not
required; the attacker gains access to all data going through the device,
including plaintext in the case of data encryption systems. Experiments show
that it is possible to intercept data under realistic conditions at a
considerable distance. Many different sorts of devices, including modems and
Internet Protocol routers, were found to be vulnerable. A taxonomy of
compromising optical emanations is developed, and design changes are described
that will successfully block this kind of "Optical TEMPEST" attack.
</p></li>
</ul>

<h3>Title: Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy. (arXiv:2307.07328v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07328">http://arxiv.org/abs/2307.07328</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07328] Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy](http://arxiv.org/abs/2307.07328) #attack</code></li>
<li>Summary: <p>Data-poisoning based backdoor attacks aim to insert backdoor into models by
manipulating training datasets without controlling the training process of the
target model. Existing attack methods mainly focus on designing triggers or
fusion strategies between triggers and benign samples. However, they often
randomly select samples to be poisoned, disregarding the varying importance of
each poisoning sample in terms of backdoor injection. A recent selection
strategy filters a fixed-size poisoning sample pool by recording forgetting
events, but it fails to consider the remaining samples outside the pool from a
global perspective. Moreover, computing forgetting events requires significant
additional computing resources. Therefore, how to efficiently and effectively
select poisoning samples from the entire dataset is an urgent problem in
backdoor attacks.To address it, firstly, we introduce a poisoning mask into the
regular backdoor training loss. We suppose that a backdoored model training
with hard poisoning samples has a more backdoor effect on easy ones, which can
be implemented by hindering the normal training process (\ie, maximizing loss
\wrt mask). To further integrate it with normal training process, we then
propose a learnable poisoning sample selection strategy to learn the mask
together with the model parameters through a min-max optimization.Specifically,
the outer loop aims to achieve the backdoor attack goal by minimizing the loss
based on the selected samples, while the inner loop selects hard poisoning
samples that impede this goal by maximizing the loss. After several rounds of
adversarial training, we finally select effective poisoning samples with high
contribution. Extensive experiments on benchmark datasets demonstrate the
effectiveness and efficiency of our approach in boosting backdoor attack
performance.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Bridging the Gap: Heterogeneous Face Recognition with Conditional Adaptive Instance Modulation. (arXiv:2307.07032v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07032">http://arxiv.org/abs/2307.07032</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07032] Bridging the Gap: Heterogeneous Face Recognition with Conditional Adaptive Instance Modulation](http://arxiv.org/abs/2307.07032) #robust</code></li>
<li>Summary: <p>Heterogeneous Face Recognition (HFR) aims to match face images across
different domains, such as thermal and visible spectra, expanding the
applicability of Face Recognition (FR) systems to challenging scenarios.
However, the domain gap and limited availability of large-scale datasets in the
target domain make training robust and invariant HFR models from scratch
difficult. In this work, we treat different modalities as distinct styles and
propose a framework to adapt feature maps, bridging the domain gap. We
introduce a novel Conditional Adaptive Instance Modulation (CAIM) module that
can be integrated into pre-trained FR networks, transforming them into HFR
networks. The CAIM block modulates intermediate feature maps, to adapt the
style of the target modality effectively bridging the domain gap. Our proposed
method allows for end-to-end training with a minimal number of paired samples.
We extensively evaluate our approach on multiple challenging benchmarks,
demonstrating superior performance compared to state-of-the-art methods. The
source code and protocols for reproducing the findings will be made publicly
available.
</p></li>
</ul>

<h3>Title: Bootstrapping Vision-Language Learning with Decoupled Language Pre-training. (arXiv:2307.07063v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07063">http://arxiv.org/abs/2307.07063</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07063] Bootstrapping Vision-Language Learning with Decoupled Language Pre-training](http://arxiv.org/abs/2307.07063) #robust</code></li>
<li>Summary: <p>We present a novel methodology aimed at optimizing the application of frozen
large language models (LLMs) for resource-intensive vision-language (VL)
pre-training. The current paradigm uses visual features as prompts to guide
language models, with a focus on determining the most relevant visual features
for corresponding text. Our approach diverges by concentrating on the language
component, specifically identifying the optimal prompts to align with visual
features. We introduce the Prompt-Transformer (P-Former), a model that predicts
these ideal prompts, which is trained exclusively on linguistic data, bypassing
the need for image-text pairings. This strategy subtly bifurcates the
end-to-end VL training process into an additional, separate stage. Our
experiments reveal that our framework significantly enhances the performance of
a robust image-to-text baseline (BLIP-2), and effectively narrows the
performance gap between models trained with either 4M or 129M image-text pairs.
Importantly, our framework is modality-agnostic and flexible in terms of
architectural design, as validated by its successful application in a video
learning task using varied base modules. The code is available at
https://github.com/yiren-jian/BLIText
</p></li>
</ul>

<h3>Title: Achelous: A Fast Unified Water-surface Panoptic Perception Framework based on Fusion of Monocular Camera and 4D mmWave Radar. (arXiv:2307.07102v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07102">http://arxiv.org/abs/2307.07102</a></li>
<li>Code URL: <a href="https://github.com/GuanRunwei/Achelous">https://github.com/GuanRunwei/Achelous</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07102] Achelous: A Fast Unified Water-surface Panoptic Perception Framework based on Fusion of Monocular Camera and 4D mmWave Radar](http://arxiv.org/abs/2307.07102) #robust</code></li>
<li>Summary: <p>Current perception models for different tasks usually exist in modular forms
on Unmanned Surface Vehicles (USVs), which infer extremely slowly in parallel
on edge devices, causing the asynchrony between perception results and USV
position, and leading to error decisions of autonomous navigation. Compared
with Unmanned Ground Vehicles (UGVs), the robust perception of USVs develops
relatively slowly. Moreover, most current multi-task perception models are huge
in parameters, slow in inference and not scalable. Oriented on this, we propose
Achelous, a low-cost and fast unified panoptic perception framework for
water-surface perception based on the fusion of a monocular camera and 4D
mmWave radar. Achelous can simultaneously perform five tasks, detection and
segmentation of visual targets, drivable-area segmentation, waterline
segmentation and radar point cloud segmentation. Besides, models in Achelous
family, with less than around 5 million parameters, achieve about 18 FPS on an
NVIDIA Jetson AGX Xavier, 11 FPS faster than HybridNets, and exceed YOLOX-Tiny
and Segformer-B0 on our collected dataset about 5 mAP$_{\text{50-95}}$ and 0.7
mIoU, especially under situations of adverse weather, dark environments and
camera failure. To our knowledge, Achelous is the first comprehensive panoptic
perception framework combining vision-level and point-cloud-level tasks for
water-surface perception. To promote the development of the intelligent
transportation community, we release our codes in
\url{https://github.com/GuanRunwei/Achelous}.
</p></li>
</ul>

<h3>Title: Challenge Results Are Not Reproducible. (arXiv:2307.07226v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07226">http://arxiv.org/abs/2307.07226</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07226] Challenge Results Are Not Reproducible](http://arxiv.org/abs/2307.07226) #robust</code></li>
<li>Summary: <p>While clinical trials are the state-of-the-art methods to assess the effect
of new medication in a comparative manner, benchmarking in the field of medical
image analysis is performed by so-called challenges. Recently, comprehensive
analysis of multiple biomedical image analysis challenges revealed large
discrepancies between the impact of challenges and quality control of the
design and reporting standard. This work aims to follow up on these results and
attempts to address the specific question of the reproducibility of the
participants methods. In an effort to determine whether alternative
interpretations of the method description may change the challenge ranking, we
reproduced the algorithms submitted to the 2019 Robust Medical Image
Segmentation Challenge (ROBUST-MIS). The leaderboard differed substantially
between the original challenge and reimplementation, indicating that challenge
rankings may not be sufficiently reproducible.
</p></li>
</ul>

<h3>Title: Sampling-Priors-Augmented Deep Unfolding Network for Robust Video Compressive Sensing. (arXiv:2307.07291v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07291">http://arxiv.org/abs/2307.07291</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07291] Sampling-Priors-Augmented Deep Unfolding Network for Robust Video Compressive Sensing](http://arxiv.org/abs/2307.07291) #robust</code></li>
<li>Summary: <p>Video Compressed Sensing (VCS) aims to reconstruct multiple frames from one
single captured measurement, thus achieving high-speed scene recording with a
low-frame-rate sensor. Although there have been impressive advances in VCS
recently, those state-of-the-art (SOTA) methods also significantly increase
model complexity and suffer from poor generality and robustness, which means
that those networks need to be retrained to accommodate the new system. Such
limitations hinder the real-time imaging and practical deployment of models. In
this work, we propose a Sampling-Priors-Augmented Deep Unfolding Network
(SPA-DUN) for efficient and robust VCS reconstruction. Under the
optimization-inspired deep unfolding framework, a lightweight and efficient
U-net is exploited to downsize the model while improving overall performance.
Moreover, the prior knowledge from the sampling model is utilized to
dynamically modulate the network features to enable single SPA-DUN to handle
arbitrary sampling settings, augmenting interpretability and generality.
Extensive experiments on both simulation and real datasets demonstrate that
SPA-DUN is not only applicable for various sampling settings with one single
model but also achieves SOTA performance with incredible efficiency.
</p></li>
</ul>

<h3>Title: Combining multitemporal optical and SAR data for LAI imputation with BiLSTM network. (arXiv:2307.07434v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07434">http://arxiv.org/abs/2307.07434</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07434] Combining multitemporal optical and SAR data for LAI imputation with BiLSTM network](http://arxiv.org/abs/2307.07434) #robust</code></li>
<li>Summary: <p>The Leaf Area Index (LAI) is vital for predicting winter wheat yield.
Acquisition of crop conditions via Sentinel-2 remote sensing images can be
hindered by persistent clouds, affecting yield predictions. Synthetic Aperture
Radar (SAR) provides all-weather imagery, and the ratio between its cross- and
co-polarized channels (C-band) shows a high correlation with time series LAI
over winter wheat regions. This study evaluates the use of time series
Sentinel-1 VH/VV for LAI imputation, aiming to increase spatial-temporal
density. We utilize a bidirectional LSTM (BiLSTM) network to impute time series
LAI and use half mean squared error for each time step as the loss function. We
trained models on data from southern Germany and the North China Plain using
only LAI data generated by Sentinel-1 VH/VV and Sentinel-2. Experimental
results show BiLSTM outperforms traditional regression methods, capturing
nonlinear dynamics between multiple time series. It proves robust in various
growing conditions and is effective even with limited Sentinel-2 images.
BiLSTM's performance surpasses that of LSTM, particularly over the senescence
period. Therefore, BiLSTM can be used to impute LAI with time-series Sentinel-1
VH/VV and Sentinel-2 data, and this method could be applied to other
time-series imputation issues.
</p></li>
</ul>

<h3>Title: Certified Robustness for Large Language Models with Self-Denoising. (arXiv:2307.07171v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07171">http://arxiv.org/abs/2307.07171</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07171] Certified Robustness for Large Language Models with Self-Denoising](http://arxiv.org/abs/2307.07171) #robust</code></li>
<li>Summary: <p>Although large language models (LLMs) have achieved great success in vast
real-world applications, their vulnerabilities towards noisy inputs have
significantly limited their uses, especially in high-stake environments. In
these contexts, it is crucial to ensure that every prediction made by large
language models is stable, i.e., LLM predictions should be consistent given
minor differences in the input. This largely falls into the study of certified
robust LLMs, i.e., all predictions of LLM are certified to be correct in a
local region around the input. Randomized smoothing has demonstrated great
potential in certifying the robustness and prediction stability of LLMs.
However, randomized smoothing requires adding noise to the input before model
prediction, and its certification performance depends largely on the model's
performance on corrupted data. As a result, its direct application to LLMs
remains challenging and often results in a small certification radius. To
address this issue, we take advantage of the multitasking nature of LLMs and
propose to denoise the corrupted inputs with LLMs in a self-denoising manner.
Different from previous works like denoised smoothing, which requires training
a separate model to robustify LLM, our method enjoys far better efficiency and
flexibility. Our experiment results show that our method outperforms the
existing certification methods under both certified robustness and empirical
robustness. The codes are available at
https://github.com/UCSB-NLP-Chang/SelfDenoise.
</p></li>
</ul>

<h3>Title: Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition. (arXiv:2307.07280v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07280">http://arxiv.org/abs/2307.07280</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07280] Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition](http://arxiv.org/abs/2307.07280) #robust</code></li>
<li>Summary: <p>While Automatic Speech Recognition (ASR) models have shown significant
advances with the introduction of unsupervised or self-supervised training
techniques, these improvements are still only limited to a subsection of
languages and speakers. Transfer learning enables the adaptation of large-scale
multilingual models to not only low-resource languages but also to more
specific speaker groups. However, fine-tuning on data from new domains is
usually accompanied by a decrease in performance on the original domain.
Therefore, in our experiments, we examine how well the performance of
large-scale ASR models can be approximated for smaller domains, with our own
dataset of German Senior Voice Commands (SVC-de), and how much of the general
speech recognition performance can be preserved by selectively freezing parts
of the model during training. To further increase the robustness of the ASR
model to vocabulary and speakers outside of the fine-tuned domain, we apply
Experience Replay for continual learning. By adding only a fraction of data
from the original domain, we are able to reach Word-Error-Rates (WERs) below
5\% on the new domain, while stabilizing performance for general speech
recognition at acceptable WERs.
</p></li>
</ul>

<h3>Title: RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition. (arXiv:2307.07417v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07417">http://arxiv.org/abs/2307.07417</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07417] RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition](http://arxiv.org/abs/2307.07417) #robust</code></li>
<li>Summary: <p>Data augmentation has been widely used in low-resource NER tasks to tackle
the problem of data sparsity. However, previous data augmentation methods have
the disadvantages of disrupted syntactic structures, token-label mismatch, and
requirement for external knowledge or manual effort. To address these issues,
we propose \textbf{Ro}bust \textbf{P}rompt-based \textbf{D}ata
\textbf{A}ugmentation (RoPDA) for low-resource NER. Based on pre-trained
language models (PLMs) with continuous prompt, RoPDA performs entity
augmentation and context augmentation through five fundamental augmentation
operations to generate label-flipping and label-preserving examples. To
optimize the utilization of the augmented samples, we present two techniques:
Self-Consistency Filtering and mixup. The former effectively eliminates
low-quality samples, while the latter prevents performance degradation arising
from the direct utilization of label-flipping samples. Extensive experiments on
three benchmarks from different domains demonstrate that RoPDA significantly
improves upon strong baselines, and also outperforms state-of-the-art
semi-supervised learning methods when unlabeled data is included.
</p></li>
</ul>

<h3>Title: Towards spoken dialect identification of Irish. (arXiv:2307.07436v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07436">http://arxiv.org/abs/2307.07436</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07436] Towards spoken dialect identification of Irish](http://arxiv.org/abs/2307.07436) #robust</code></li>
<li>Summary: <p>The Irish language is rich in its diversity of dialects and accents. This
compounds the difficulty of creating a speech recognition system for the
low-resource language, as such a system must contend with a high degree of
variability with limited corpora. A recent study investigating dialect bias in
Irish ASR found that balanced training corpora gave rise to unequal dialect
performance, with performance for the Ulster dialect being consistently worse
than for the Connacht or Munster dialects. Motivated by this, the present
experiments investigate spoken dialect identification of Irish, with a view to
incorporating such a system into the speech recognition pipeline. Two acoustic
classification models are tested, XLS-R and ECAPA-TDNN, in conjunction with a
text-based classifier using a pretrained Irish-language BERT model. The
ECAPA-TDNN, particularly a model pretrained for language identification on the
VoxLingua107 dataset, performed best overall, with an accuracy of 73%. This was
further improved to 76% by fusing the model's outputs with the text-based
model. The Ulster dialect was most accurately identified, with an accuracy of
94%, however the model struggled to disambiguate between the Connacht and
Munster dialects, suggesting a more nuanced approach may be necessary to
robustly distinguish between the dialects of Irish.
</p></li>
</ul>

<h3>Title: Multiplicative update rules for accelerating deep learning training and increasing robustness. (arXiv:2307.07189v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07189">http://arxiv.org/abs/2307.07189</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07189] Multiplicative update rules for accelerating deep learning training and increasing robustness](http://arxiv.org/abs/2307.07189) #robust</code></li>
<li>Summary: <p>Even nowadays, where Deep Learning (DL) has achieved state-of-the-art
performance in a wide range of research domains, accelerating training and
building robust DL models remains a challenging task. To this end, generations
of researchers have pursued to develop robust methods for training DL
architectures that can be less sensitive to weight distributions, model
architectures and loss landscapes. However, such methods are limited to
adaptive learning rate optimizers, initialization schemes, and clipping
gradients without investigating the fundamental rule of parameters update.
Although multiplicative updates have contributed significantly to the early
development of machine learning and hold strong theoretical claims, to best of
our knowledge, this is the first work that investigate them in context of DL
training acceleration and robustness. In this work, we propose an optimization
framework that fits to a wide range of optimization algorithms and enables one
to apply alternative update rules. To this end, we propose a novel
multiplicative update rule and we extend their capabilities by combining it
with a traditional additive update term, under a novel hybrid update method. We
claim that the proposed framework accelerates training, while leading to more
robust models in contrast to traditionally used additive update rule and we
experimentally demonstrate their effectiveness in a wide range of task and
optimization methods. Such tasks ranging from convex and non-convex
optimization to difficult image classification benchmarks applying a wide range
of traditionally used optimization methods and Deep Neural Network (DNN)
architectures.
</p></li>
</ul>

<h3>Title: Implicit regularization in AI meets generalized hardness of approximation in optimization -- Sharp results for diagonal linear networks. (arXiv:2307.07410v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07410">http://arxiv.org/abs/2307.07410</a></li>
<li>Code URL: <a href="https://github.com/johanwind/which_l1_minimizer">https://github.com/johanwind/which_l1_minimizer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07410] Implicit regularization in AI meets generalized hardness of approximation in optimization -- Sharp results for diagonal linear networks](http://arxiv.org/abs/2307.07410) #robust</code></li>
<li>Summary: <p>Understanding the implicit regularization imposed by neural network
architectures and gradient based optimization methods is a key challenge in
deep learning and AI. In this work we provide sharp results for the implicit
regularization imposed by the gradient flow of Diagonal Linear Networks (DLNs)
in the over-parameterized regression setting and, potentially surprisingly,
link this to the phenomenon of phase transitions in generalized hardness of
approximation (GHA). GHA generalizes the phenomenon of hardness of
approximation from computer science to, among others, continuous and robust
optimization. It is well-known that the $\ell^1$-norm of the gradient flow of
DLNs with tiny initialization converges to the objective function of basis
pursuit. We improve upon these results by showing that the gradient flow of
DLNs with tiny initialization approximates minimizers of the basis pursuit
optimization problem (as opposed to just the objective function), and we obtain
new and sharp convergence bounds w.r.t.\ the initialization size. Non-sharpness
of our results would imply that the GHA phenomenon would not occur for the
basis pursuit optimization problem -- which is a contradiction -- thus implying
sharpness. Moreover, we characterize $\textit{which}$ $\ell_1$ minimizer of the
basis pursuit problem is chosen by the gradient flow whenever the minimizer is
not unique. Interestingly, this depends on the depth of the DLN.
</p></li>
</ul>

<h3>Title: Expressive Monotonic Neural Networks. (arXiv:2307.07512v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07512">http://arxiv.org/abs/2307.07512</a></li>
<li>Code URL: <a href="https://github.com/niklasnolte/hlt_2track">https://github.com/niklasnolte/hlt_2track</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07512] Expressive Monotonic Neural Networks](http://arxiv.org/abs/2307.07512) #robust</code></li>
<li>Summary: <p>The monotonic dependence of the outputs of a neural network on some of its
inputs is a crucial inductive bias in many scenarios where domain knowledge
dictates such behavior. This is especially important for interpretability and
fairness considerations. In a broader context, scenarios in which monotonicity
is important can be found in finance, medicine, physics, and other disciplines.
It is thus desirable to build neural network architectures that implement this
inductive bias provably. In this work, we propose a weight-constrained
architecture with a single residual connection to achieve exact monotonic
dependence in any subset of the inputs. The weight constraint scheme directly
controls the Lipschitz constant of the neural network and thus provides the
additional benefit of robustness. Compared to currently existing techniques
used for monotonicity, our method is simpler in implementation and in theory
foundations, has negligible computational overhead, is guaranteed to produce
monotonic dependence, and is highly expressive. We show how the algorithm is
used to train powerful, robust, and interpretable discriminators that achieve
competitive performance compared to current state-of-the-art methods across
various benchmarks, from social applications to the classification of the
decays of subatomic particles produced at the CERN Large Hadron Collider.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Complementary Frequency-Varying Awareness Network for Open-Set Fine-Grained Image Recognition. (arXiv:2307.07214v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07214">http://arxiv.org/abs/2307.07214</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07214] Complementary Frequency-Varying Awareness Network for Open-Set Fine-Grained Image Recognition](http://arxiv.org/abs/2307.07214) #extraction</code></li>
<li>Summary: <p>Open-set image recognition is a challenging topic in computer vision. Most of
the existing works in literature focus on learning more discriminative features
from the input images, however, they are usually insensitive to the high- or
low-frequency components in features, resulting in a decreasing performance on
fine-grained image recognition. To address this problem, we propose a
Complementary Frequency-varying Awareness Network that could better capture
both high-frequency and low-frequency information, called CFAN. The proposed
CFAN consists of three sequential modules: (i) a feature extraction module is
introduced for learning preliminary features from the input images; (ii) a
frequency-varying filtering module is designed to separate out both high- and
low-frequency components from the preliminary features in the frequency domain
via a frequency-adjustable filter; (iii) a complementary temporal aggregation
module is designed for aggregating the high- and low-frequency components via
two Long Short-Term Memory networks into discriminative features. Based on
CFAN, we further propose an open-set fine-grained image recognition method,
called CFAN-OSFGR, which learns image features via CFAN and classifies them via
a linear classifier. Experimental results on 3 fine-grained datasets and 2
coarse-grained datasets demonstrate that CFAN-OSFGR performs significantly
better than 9 state-of-the-art methods in most cases.
</p></li>
</ul>

<h3>Title: MaxSR: Image Super-Resolution Using Improved MaxViT. (arXiv:2307.07240v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07240">http://arxiv.org/abs/2307.07240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07240] MaxSR: Image Super-Resolution Using Improved MaxViT](http://arxiv.org/abs/2307.07240) #extraction</code></li>
<li>Summary: <p>While transformer models have been demonstrated to be effective for natural
language processing tasks and high-level vision tasks, only a few attempts have
been made to use powerful transformer models for single image super-resolution.
Because transformer models have powerful representation capacity and the
in-built self-attention mechanisms in transformer models help to leverage
self-similarity prior in input low-resolution image to improve performance for
single image super-resolution, we present a single image super-resolution model
based on recent hybrid vision transformer of MaxViT, named as MaxSR. MaxSR
consists of four parts, a shallow feature extraction block, multiple cascaded
adaptive MaxViT blocks to extract deep hierarchical features and model global
self-similarity from low-level features efficiently, a hierarchical feature
fusion block, and finally a reconstruction block. The key component of MaxSR,
i.e., adaptive MaxViT block, is based on MaxViT block which mixes MBConv with
squeeze-and-excitation, block attention and grid attention. In order to achieve
better global modelling of self-similarity in input low-resolution image, we
improve block attention and grid attention in MaxViT block to adaptive block
attention and adaptive grid attention which do self-attention inside each
window across all grids and each grid across all windows respectively in the
most efficient way. We instantiate proposed model for classical single image
super-resolution (MaxSR) and lightweight single image super-resolution
(MaxSR-light). Experiments show that our MaxSR and MaxSR-light establish new
state-of-the-art performance efficiently.
</p></li>
</ul>

<h3>Title: MegaWika: Millions of reports and their sources across 50 diverse languages. (arXiv:2307.07049v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07049">http://arxiv.org/abs/2307.07049</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07049] MegaWika: Millions of reports and their sources across 50 diverse languages](http://arxiv.org/abs/2307.07049) #extraction</code></li>
<li>Summary: <p>To foster the development of new models for collaborative AI-assisted report
generation, we introduce MegaWika, consisting of 13 million Wikipedia articles
in 50 diverse languages, along with their 71 million referenced source
materials. We process this dataset for a myriad of applications, going beyond
the initial Wikipedia citation extraction and web scraping of content,
including translating non-English articles for cross-lingual applications and
providing FrameNet parses for automated semantic analysis. MegaWika is the
largest resource for sentence-level report generation and the only report
generation dataset that is multilingual. We manually analyze the quality of
this resource through a semantically stratified sample. Finally, we provide
baseline results and trained models for crucial steps in automated report
generation: cross-lingual question answering and citation retrieval.
</p></li>
</ul>

<h3>Title: Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section. (arXiv:2307.07051v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07051">http://arxiv.org/abs/2307.07051</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07051] Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section](http://arxiv.org/abs/2307.07051) #extraction</code></li>
<li>Summary: <p>Recent advances in large language models have led to renewed interest in
natural language processing in healthcare using the free text of clinical
notes. One distinguishing characteristic of clinical notes is their long time
span over multiple long documents. The unique structure of clinical notes
creates a new design choice: when the context length for a language model
predictor is limited, which part of clinical notes should we choose as the
input? Existing studies either choose the inputs with domain knowledge or
simply truncate them. We propose a framework to analyze the sections with high
predictive power. Using MIMIC-III, we show that: 1) predictive power
distribution is different between nursing notes and discharge notes and 2)
combining different types of notes could improve performance when the context
length is large. Our findings suggest that a carefully selected sampling
function could enable more efficient information extraction from clinical
notes.
</p></li>
</ul>

<h3>Title: The Automation of the Extraction of Evidence masked by Steganographic Techniques in WAV and MP3 Audio Files. (arXiv:2307.07293v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07293">http://arxiv.org/abs/2307.07293</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07293] The Automation of the Extraction of Evidence masked by Steganographic Techniques in WAV and MP3 Audio Files](http://arxiv.org/abs/2307.07293) #extraction</code></li>
<li>Summary: <p>Antiforensics techniques and particularly steganography and cryptography have
become increasingly pressing issues that affect the current digital forensics
practice, both techniques are widely researched and developed as considered in
the heart of the modern digital era but remain double edged swords standing
between the privacy conscious and the criminally malicious, dependent on the
severity of the methods deployed. This paper advances the automation of hidden
evidence extraction in the context of audio files enabling the correlation
between unprocessed evidence artefacts and extreme Steganographic and
Cryptographic techniques using the Least Significant Bits extraction method
(LSB). The research generates an in-depth review of current digital forensic
toolkit and systems and formally address their capabilities in handling
steganography-related cases, we opted for experimental research methodology in
the form of quantitative analysis of the efficiency of detecting and extraction
of hidden artefacts in WAV and MP3 audio files by comparing standard industry
software. This work establishes an environment for the practical implementation
and testing of the proposed approach and the new toolkit for extracting
evidence hidden by Cryptographic and Steganographic techniques during forensics
investigations. The proposed multi-approach automation demonstrated a huge
positive impact in terms of efficiency and accuracy and notably on large audio
files (MP3 and WAV) which the forensics analysis is time-consuming and requires
significant computational resources and memory. However, the proposed
automation may occasionally produce false positives (detecting steganography
where none exists) or false negatives (failing to detect steganography that is
present) but overall achieve a balance between detecting hidden data accurately
along with minimising the false alarms.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: L-DAWA: Layer-wise Divergence Aware Weight Aggregation in Federated Self-Supervised Visual Representation Learning. (arXiv:2307.07393v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07393">http://arxiv.org/abs/2307.07393</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07393] L-DAWA: Layer-wise Divergence Aware Weight Aggregation in Federated Self-Supervised Visual Representation Learning](http://arxiv.org/abs/2307.07393) #federate</code></li>
<li>Summary: <p>The ubiquity of camera-enabled devices has led to large amounts of unlabeled
image data being produced at the edge. The integration of self-supervised
learning (SSL) and federated learning (FL) into one coherent system can
potentially offer data privacy guarantees while also advancing the quality and
robustness of the learned visual representations without needing to move data
around. However, client bias and divergence during FL aggregation caused by
data heterogeneity limits the performance of learned visual representations on
downstream tasks. In this paper, we propose a new aggregation strategy termed
Layer-wise Divergence Aware Weight Aggregation (L-DAWA) to mitigate the
influence of client bias and divergence during FL aggregation. The proposed
method aggregates weights at the layer-level according to the measure of
angular divergence between the clients' model and the global model. Extensive
experiments with cross-silo and cross-device settings on CIFAR-10/100 and Tiny
ImageNet datasets demonstrate that our methods are effective and obtain new
SOTA performance on both contrastive and non-contrastive SSL approaches.
</p></li>
</ul>

<h3>Title: Population Expansion for Training Language Models with Private Federated Learning. (arXiv:2307.07477v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07477">http://arxiv.org/abs/2307.07477</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07477] Population Expansion for Training Language Models with Private Federated Learning](http://arxiv.org/abs/2307.07477) #federate</code></li>
<li>Summary: <p>Federated learning (FL) combined with differential privacy (DP) offers
machine learning (ML) training with distributed devices and with a formal
privacy guarantee. With a large population of devices, FL with DP produces a
performant model in a timely manner. However, for applications with a smaller
population, not only does the model utility degrade as the DP noise is
inversely proportional to population, but also the training latency increases
since waiting for enough clients to become available from a smaller pool is
slower. In this work, we thus propose expanding the population based on domain
adaptation techniques to speed up the training and improves the final model
quality when training with small populations. We empirically demonstrate that
our techniques can improve the utility by 13% to 30% on real-world language
modeling datasets.
</p></li>
</ul>

<h3>Title: Layerwise Linear Mode Connectivity. (arXiv:2307.06966v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06966">http://arxiv.org/abs/2307.06966</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06966] Layerwise Linear Mode Connectivity](http://arxiv.org/abs/2307.06966) #federate</code></li>
<li>Summary: <p>In the federated setup one performs an aggregation of separate local models
multiple times during training in order to obtain a stronger global model; most
often aggregation is a simple averaging of the parameters. Understanding when
and why averaging works in a non-convex setup, such as federated deep learning,
is an open challenge that hinders obtaining highly performant global models. On
i.i.d.~datasets federated deep learning with frequent averaging is successful.
The common understanding, however, is that during the independent training
models are drifting away from each other and thus averaging may not work
anymore after many local parameter updates. The problem can be seen from the
perspective of the loss surface: for points on a non-convex surface the average
can become arbitrarily bad. The assumption of local convexity, often used to
explain the success of federated averaging, contradicts to the empirical
evidence showing that high loss barriers exist between models from the very
beginning of the learning, even when training on the same data. Based on the
observation that the learning process evolves differently in different layers,
we investigate the barrier between models in a layerwise fashion. Our
conjecture is that barriers preventing from successful federated training are
caused by a particular layer or group of layers.
</p></li>
</ul>

<h3>Title: Improved Convergence Analysis and SNR Control Strategies for Federated Learning in the Presence of Noise. (arXiv:2307.07406v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07406">http://arxiv.org/abs/2307.07406</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07406] Improved Convergence Analysis and SNR Control Strategies for Federated Learning in the Presence of Noise](http://arxiv.org/abs/2307.07406) #federate</code></li>
<li>Summary: <p>We propose an improved convergence analysis technique that characterizes the
distributed learning paradigm of federated learning (FL) with imperfect/noisy
uplink and downlink communications. Such imperfect communication scenarios
arise in the practical deployment of FL in emerging communication systems and
protocols. The analysis developed in this paper demonstrates, for the first
time, that there is an asymmetry in the detrimental effects of uplink and
downlink communications in FL. In particular, the adverse effect of the
downlink noise is more severe on the convergence of FL algorithms. Using this
insight, we propose improved Signal-to-Noise (SNR) control strategies that,
discarding the negligible higher-order terms, lead to a similar convergence
rate for FL as in the case of a perfect, noise-free communication channel while
incurring significantly less power resources compared to existing solutions. In
particular, we establish that to maintain the $O(\frac{1}{\sqrt{K}})$ rate of
convergence like in the case of noise-free FL, we need to scale down the uplink
and downlink noise by $\Omega({\sqrt{k}})$ and $\Omega({k})$ respectively,
where $k$ denotes the communication round, $k=1,\dots, K$. Our theoretical
result is further characterized by two major benefits: firstly, it does not
assume the somewhat unrealistic assumption of bounded client dissimilarity, and
secondly, it only requires smooth non-convex loss functions, a function class
better suited for modern machine learning and deep learning models. We also
perform extensive empirical analysis to verify the validity of our theoretical
findings.
</p></li>
</ul>

<h2>fair</h2>
<h2>interpretability</h2>
<h3>Title: Implicit Neural Feature Fusion Function for Multispectral and Hyperspectral Image Fusion. (arXiv:2307.07288v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07288">http://arxiv.org/abs/2307.07288</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07288] Implicit Neural Feature Fusion Function for Multispectral and Hyperspectral Image Fusion](http://arxiv.org/abs/2307.07288) #interpretability</code></li>
<li>Summary: <p>Multispectral and Hyperspectral Image Fusion (MHIF) is a practical task that
aims to fuse a high-resolution multispectral image (HR-MSI) and a
low-resolution hyperspectral image (LR-HSI) of the same scene to obtain a
high-resolution hyperspectral image (HR-HSI). Benefiting from powerful
inductive bias capability, CNN-based methods have achieved great success in the
MHIF task. However, they lack certain interpretability and require convolution
structures be stacked to enhance performance. Recently, Implicit Neural
Representation (INR) has achieved good performance and interpretability in 2D
tasks due to its ability to locally interpolate samples and utilize multimodal
content such as pixels and coordinates. Although INR-based approaches show
promise, they require extra construction of high-frequency information
(\emph{e.g.,} positional encoding). In this paper, inspired by previous work of
MHIF task, we realize that HR-MSI could serve as a high-frequency detail
auxiliary input, leading us to propose a novel INR-based hyperspectral fusion
function named Implicit Neural Feature Fusion Function (INF). As an elaborate
structure, it solves the MHIF task and addresses deficiencies in the INR-based
approaches. Specifically, our INF designs a Dual High-Frequency Fusion (DHFF)
structure that obtains high-frequency information twice from HR-MSI and LR-HSI,
then subtly fuses them with coordinate information. Moreover, the proposed INF
incorporates a parameter-free method named INR with cosine similarity (INR-CS)
that uses cosine similarity to generate local weights through feature vectors.
Based on INF, we construct an Implicit Neural Fusion Network (INFN) that
achieves state-of-the-art performance for MHIF tasks of two public datasets,
\emph{i.e.,} CAVE and Harvard. The code will soon be made available on GitHub.
</p></li>
</ul>

<h3>Title: Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07084">http://arxiv.org/abs/2307.07084</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07084] Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability](http://arxiv.org/abs/2307.07084) #interpretability</code></li>
<li>Summary: <p>Reinforcement Learning or optimal control can provide effective reasoning for
sequential decision-making problems with variable dynamics. Such reasoning in
practical implementation, however, poses a persistent challenge in interpreting
the reward function and corresponding optimal policy. Consequently, formalizing
the sequential decision-making problems as inference has a considerable value,
as probabilistic inference in principle offers diverse and powerful
mathematical tools to infer the stochastic dynamics whilst suggesting a
probabilistic interpretation of the reward design and policy convergence. In
this study, we propose a novel Adaptive Wasserstein Variational Optimization
(AWaVO) to tackle these challenges in sequential decision-making. Our approach
utilizes formal methods to provide interpretations of reward design,
transparency of training convergence, and probabilistic interpretation of
sequential decisions. To demonstrate practicality, we show convergent training
with guaranteed global convergence rates not only in simulation but also in
real robot tasks, and empirically verify a reasonable tradeoff between high
performance and conservative interpretability.
</p></li>
</ul>

<h3>Title: Inverse Evolution Layers: Physics-informed Regularizers for Deep Neural Networks. (arXiv:2307.07344v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07344">http://arxiv.org/abs/2307.07344</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07344] Inverse Evolution Layers: Physics-informed Regularizers for Deep Neural Networks](http://arxiv.org/abs/2307.07344) #interpretability</code></li>
<li>Summary: <p>This paper proposes a novel approach to integrating partial differential
equation (PDE)-based evolution models into neural networks through a new type
of regularization. Specifically, we propose inverse evolution layers (IELs)
based on evolution equations. These layers can achieve specific regularization
objectives and endow neural networks' outputs with corresponding properties of
the evolution models. Moreover, IELs are straightforward to construct and
implement, and can be easily designed for various physical evolutions and
neural networks. Additionally, the design process for these layers can provide
neural networks with intuitive and mathematical interpretability, thus
enhancing the transparency and explainability of the approach. To demonstrate
the effectiveness, efficiency, and simplicity of our approach, we present an
example of endowing semantic segmentation models with the smoothness property
based on the heat diffusion model. To achieve this goal, we design
heat-diffusion IELs and apply them to address the challenge of semantic
segmentation with noisy labels. The experimental results demonstrate that the
heat-diffusion IELs can effectively mitigate the overfitting problem caused by
noisy labels.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Improved Flood Insights: Diffusion-Based SAR to EO Image Translation. (arXiv:2307.07123v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07123">http://arxiv.org/abs/2307.07123</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07123] Improved Flood Insights: Diffusion-Based SAR to EO Image Translation](http://arxiv.org/abs/2307.07123) #diffusion</code></li>
<li>Summary: <p>Driven by rapid climate change, the frequency and intensity of flood events
are increasing. Electro-Optical (EO) satellite imagery is commonly utilized for
rapid response. However, its utilities in flood situations are hampered by
issues such as cloud cover and limitations during nighttime, making accurate
assessment of damage challenging. Several alternative flood detection
techniques utilizing Synthetic Aperture Radar (SAR) data have been proposed.
Despite the advantages of SAR over EO in the aforementioned situations, SAR
presents a distinct drawback: human analysts often struggle with data
interpretation. To tackle this issue, this paper introduces a novel framework,
Diffusion-Based SAR to EO Image Translation (DSE). The DSE framework converts
SAR images into EO images, thereby enhancing the interpretability of flood
insights for humans. Experimental results on the Sen1Floods11 and SEN12-FLOOD
datasets confirm that the DSE framework not only delivers enhanced visual
information but also improves performance across all tested flood segmentation
baselines.
</p></li>
</ul>

<h3>Title: Multimodal Motion Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection. (arXiv:2307.07205v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07205">http://arxiv.org/abs/2307.07205</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07205] Multimodal Motion Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection](http://arxiv.org/abs/2307.07205) #diffusion</code></li>
<li>Summary: <p>Anomalies are rare and anomaly detection is often therefore framed as
One-Class Classification (OCC), i.e. trained solely on normalcy. Leading OCC
techniques constrain the latent representations of normal motions to limited
volumes and detect as abnormal anything outside, which accounts satisfactorily
for the openset'ness of anomalies. But normalcy shares the same openset'ness
property, since humans can perform the same action in several ways, which the
leading techniques neglect. We propose a novel generative model for video
anomaly detection (VAD), which assumes that both normality and abnormality are
multimodal. We consider skeletal representations and leverage state-of-the-art
diffusion probabilistic models to generate multimodal future human poses. We
contribute a novel conditioning on the past motion of people, and exploit the
improved mode coverage capabilities of diffusion processes to generate
different-but-plausible future motions. Upon the statistical aggregation of
future modes, anomaly is detected when the generated set of motions is not
pertinent to the actual future. We validate our model on 4 established
benchmarks: UBnormal, HR-UBnormal, HR-STC, and HR-Avenue, with extensive
experiments surpassing state-of-the-art results.
</p></li>
</ul>

<h3>Title: NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis. (arXiv:2307.07511v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07511">http://arxiv.org/abs/2307.07511</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07511] NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis](http://arxiv.org/abs/2307.07511) #diffusion</code></li>
<li>Summary: <p>We address the problem of generating realistic 3D motions of humans
interacting with objects in a scene. Our key idea is to create a neural
interaction field attached to a specific object, which outputs the distance to
the valid interaction manifold given a human pose as input. This interaction
field guides the sampling of an object-conditioned human motion diffusion
model, so as to encourage plausible contacts and affordance semantics. To
support interactions with scarcely available data, we propose an automated
synthetic data pipeline. For this, we seed a pre-trained motion model, which
has priors for the basics of human movement, with interaction-specific anchor
poses extracted from limited motion capture data. Using our guided diffusion
model trained on generated synthetic data, we synthesize realistic motions for
sitting and lifting with several objects, outperforming alternative approaches
in terms of motion quality and successful action completion. We call our
framework NIFTY: Neural Interaction Fields for Trajectory sYnthesis.
</p></li>
</ul>

<h3>Title: Neuro-symbolic Empowered Denoising Diffusion Probabilistic Models for Real-time Anomaly Detection in Industry 4.0. (arXiv:2307.06975v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06975">http://arxiv.org/abs/2307.06975</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06975] Neuro-symbolic Empowered Denoising Diffusion Probabilistic Models for Real-time Anomaly Detection in Industry 4](http://arxiv.org/abs/2307.06975) #diffusion</code></li>
<li>Summary: <p>Industry 4.0 involves the integration of digital technologies, such as IoT,
Big Data, and AI, into manufacturing and industrial processes to increase
efficiency and productivity. As these technologies become more interconnected
and interdependent, Industry 4.0 systems become more complex, which brings the
difficulty of identifying and stopping anomalies that may cause disturbances in
the manufacturing process. This paper aims to propose a diffusion-based model
for real-time anomaly prediction in Industry 4.0 processes. Using a
neuro-symbolic approach, we integrate industrial ontologies in the model,
thereby adding formal knowledge on smart manufacturing. Finally, we propose a
simple yet effective way of distilling diffusion models through Random Fourier
Features for deployment on an embedded system for direct integration into the
manufacturing process. To the best of our knowledge, this approach has never
been explored before.
</p></li>
</ul>

<h3>Title: Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement. (arXiv:2307.07055v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07055">http://arxiv.org/abs/2307.07055</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07055] Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement](http://arxiv.org/abs/2307.07055) #diffusion</code></li>
<li>Summary: <p>We explore the methodology and theory of reward-directed generation via
conditional diffusion models. Directed generation aims to generate samples with
desired properties as measured by a reward function, which has broad
applications in generative AI, reinforcement learning, and computational
biology. We consider the common learning scenario where the data set consists
of unlabeled data along with a smaller set of data with noisy reward labels.
Our approach leverages a learned reward function on the smaller data set as a
pseudolabeler. From a theoretical standpoint, we show that this directed
generator can effectively learn and sample from the reward-conditioned data
distribution. Additionally, our model is capable of recovering the latent
subspace representation of data. Moreover, we establish that the model
generates a new population that moves closer to a user-specified target reward
value, where the optimality gap aligns with the off-policy bandit regret in the
feature subspace. The improvement in rewards obtained is influenced by the
interplay between the strength of the reward signal, the distribution shift,
and the cost of off-support extrapolation. We provide empirical results to
validate our theory and highlight the relationship between the strength of
extrapolation and the quality of generated samples.
</p></li>
</ul>

<h3>Title: Rician likelihood loss for quantitative MRI using self-supervised deep learning. (arXiv:2307.07072v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07072">http://arxiv.org/abs/2307.07072</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07072] Rician likelihood loss for quantitative MRI using self-supervised deep learning](http://arxiv.org/abs/2307.07072) #diffusion</code></li>
<li>Summary: <p>Purpose: Previous quantitative MR imaging studies using self-supervised deep
learning have reported biased parameter estimates at low SNR. Such systematic
errors arise from the choice of Mean Squared Error (MSE) loss function for
network training, which is incompatible with Rician-distributed MR magnitude
signals. To address this issue, we introduce the negative log Rician likelihood
(NLR) loss. Methods: A numerically stable and accurate implementation of the
NLR loss was developed to estimate quantitative parameters of the apparent
diffusion coefficient (ADC) model and intra-voxel incoherent motion (IVIM)
model. Parameter estimation accuracy, precision and overall error were
evaluated in terms of bias, variance and root mean squared error and compared
against the MSE loss over a range of SNRs (5 - 30). Results: Networks trained
with NLR loss show higher estimation accuracy than MSE for the ADC and IVIM
diffusion coefficients as SNR decreases, with minimal loss of precision or
total error. At high effective SNR (high SNR and small diffusion coefficients),
both losses show comparable accuracy and precision for all parameters of both
models. Conclusion: The proposed NLR loss is numerically stable and accurate
across the full range of tested SNRs and improves parameter estimation accuracy
of diffusion coefficients using self-supervised deep learning. We expect the
development to benefit quantitative MR imaging techniques broadly, enabling
more accurate parameter estimation from noisy data.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Deepfake Video Detection Using Generative Convolutional Vision Transformer. (arXiv:2307.07036v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07036">http://arxiv.org/abs/2307.07036</a></li>
<li>Code URL: <a href="https://github.com/erprogs/genconvit">https://github.com/erprogs/genconvit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07036] Deepfake Video Detection Using Generative Convolutional Vision Transformer](http://arxiv.org/abs/2307.07036) #transformer</code></li>
<li>Summary: <p>Deepfakes have raised significant concerns due to their potential to spread
false information and compromise digital media integrity. In this work, we
propose a Generative Convolutional Vision Transformer (GenConViT) for deepfake
video detection. Our model combines ConvNeXt and Swin Transformer models for
feature extraction, and it utilizes Autoencoder and Variational Autoencoder to
learn from the latent data distribution. By learning from the visual artifacts
and latent data distribution, GenConViT achieves improved performance in
detecting a wide range of deepfake videos. The model is trained and evaluated
on DFDC, FF++, DeepfakeTIMIT, and Celeb-DF v2 datasets, achieving high
classification accuracy, F1 scores, and AUC values. The proposed GenConViT
model demonstrates robust performance in deepfake video detection, with an
average accuracy of 95.8% and an AUC value of 99.3% across the tested datasets.
Our proposed model addresses the challenge of generalizability in deepfake
detection by leveraging visual and latent features and providing an effective
solution for identifying a wide range of fake videos while preserving media
integrity. The code for GenConViT is available at
https://github.com/erprogs/GenConViT.
</p></li>
</ul>

<h3>Title: Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling. (arXiv:2307.07057v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07057">http://arxiv.org/abs/2307.07057</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07057] Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling](http://arxiv.org/abs/2307.07057) #transformer</code></li>
<li>Summary: <p>We study speech intent classification and slot filling (SICSF) by proposing
to use an encoder pretrained on speech recognition (ASR) to initialize an
end-to-end (E2E) Conformer-Transformer model, which achieves the new
state-of-the-art results on the SLURP dataset, with 90.14% intent accuracy and
82.27% SLURP-F1. We compare our model with encoders pretrained on
self-supervised learning (SSL), and show that ASR pretraining is much more
effective than SSL for SICSF. To explore parameter efficiency, we freeze the
encoder and add Adapter modules, and show that parameter efficiency is only
achievable with an ASR-pretrained encoder, while the SSL encoder needs full
finetuning to achieve comparable results. In addition, we provide an in-depth
comparison on end-to-end models versus cascading models (ASR+NLU), and show
that E2E models are better than cascaded models unless an oracle ASR model is
provided. Last but not least, our model is the first E2E model that achieves
the same performance as cascading models with oracle ASR. Code, checkpoints and
configs are available.
</p></li>
</ul>

<h3>Title: CFI2P: Coarse-to-Fine Cross-Modal Correspondence Learning for Image-to-Point Cloud Registration. (arXiv:2307.07142v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07142">http://arxiv.org/abs/2307.07142</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07142] CFI2P: Coarse-to-Fine Cross-Modal Correspondence Learning for Image-to-Point Cloud Registration](http://arxiv.org/abs/2307.07142) #transformer</code></li>
<li>Summary: <p>In the context of image-to-point cloud registration, acquiring point-to-pixel
correspondences presents a challenging task since the similarity between
individual points and pixels is ambiguous due to the visual differences in data
modalities. Nevertheless, the same object present in the two data formats can
be readily identified from the local perspective of point sets and pixel
patches. Motivated by this intuition, we propose a coarse-to-fine framework
that emphasizes the establishment of correspondences between local point sets
and pixel patches, followed by the refinement of results at both the point and
pixel levels. On a coarse scale, we mimic the classic Visual Transformer to
translate both image and point cloud into two sequences of local
representations, namely point and pixel proxies, and employ attention to
capture global and cross-modal contexts. To supervise the coarse matching, we
propose a novel projected point proportion loss, which guides to match point
sets with pixel patches where more points can be projected into. On a finer
scale, point-to-pixel correspondences are then refined from a smaller search
space (i.e., the coarsely matched sets and patches) via well-designed sampling,
attentional learning and fine matching, where sampling masks are embedded in
the last two steps to mitigate the negative effect of sampling. With the
high-quality correspondences, the registration problem is then resolved by EPnP
algorithm within RANSAC. Experimental results on large-scale outdoor benchmarks
demonstrate our superiority over existing methods.
</p></li>
</ul>

<h3>Title: TriFormer: A Multi-modal Transformer Framework For Mild Cognitive Impairment Conversion Prediction. (arXiv:2307.07177v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07177">http://arxiv.org/abs/2307.07177</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07177] TriFormer: A Multi-modal Transformer Framework For Mild Cognitive Impairment Conversion Prediction](http://arxiv.org/abs/2307.07177) #transformer</code></li>
<li>Summary: <p>The prediction of mild cognitive impairment (MCI) conversion to Alzheimer's
disease (AD) is important for early treatment to prevent or slow the
progression of AD. To accurately predict the MCI conversion to stable MCI or
progressive MCI, we propose Triformer, a novel transformer-based framework with
three specialized transformers to incorporate multi-model data. Triformer uses
I) an image transformer to extract multi-view image features from medical
scans, II) a clinical transformer to embed and correlate multi-modal clinical
data, and III) a modality fusion transformer that produces an accurate
prediction based on fusing the outputs from the image and clinical
transformers. Triformer is evaluated on the Alzheimer's Disease Neuroimaging
Initiative (ANDI)1 and ADNI2 datasets and outperforms previous state-of-the-art
single and multi-modal methods.
</p></li>
</ul>

<h3>Title: HEAL-SWIN: A Vision Transformer On The Sphere. (arXiv:2307.07313v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07313">http://arxiv.org/abs/2307.07313</a></li>
<li>Code URL: <a href="https://github.com/janegerken/heal-swin">https://github.com/janegerken/heal-swin</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07313] HEAL-SWIN: A Vision Transformer On The Sphere](http://arxiv.org/abs/2307.07313) #transformer</code></li>
<li>Summary: <p>High-resolution wide-angle fisheye images are becoming more and more
important for robotics applications such as autonomous driving. However, using
ordinary convolutional neural networks or vision transformers on this data is
problematic due to projection and distortion losses introduced when projecting
to a rectangular grid on the plane. We introduce the HEAL-SWIN transformer,
which combines the highly uniform Hierarchical Equal Area iso-Latitude
Pixelation (HEALPix) grid used in astrophysics and cosmology with the
Hierarchical Shifted-Window (SWIN) transformer to yield an efficient and
flexible model capable of training on high-resolution, distortion-free
spherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is used
to perform the patching and windowing operations of the SWIN transformer,
resulting in a one-dimensional representation of the spherical data with
minimal computational overhead. We demonstrate the superior performance of our
model for semantic segmentation and depth regression tasks on both synthetic
and real automotive datasets. Our code is available at
https://github.com/JanEGerken/HEAL-SWIN.
</p></li>
</ul>

<h3>Title: Multimodal Distillation for Egocentric Action Recognition. (arXiv:2307.07483v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07483">http://arxiv.org/abs/2307.07483</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07483] Multimodal Distillation for Egocentric Action Recognition](http://arxiv.org/abs/2307.07483) #transformer</code></li>
<li>Summary: <p>The focal point of egocentric video understanding is modelling hand-object
interactions. Standard models, e.g. CNNs or Vision Transformers, which receive
RGB frames as input perform well. However, their performance improves further
by employing additional input modalities that provide complementary cues, such
as object detections, optical flow, audio, etc. The added complexity of the
modality-specific modules, on the other hand, makes these models impractical
for deployment. The goal of this work is to retain the performance of such a
multimodal approach, while using only the RGB frames as input at inference
time. We demonstrate that for egocentric action recognition on the
Epic-Kitchens and the Something-Something datasets, students which are taught
by multimodal teachers tend to be more accurate and better calibrated than
architecturally equivalent models trained on ground truth labels in a unimodal
or multimodal fashion. We further adopt a principled multimodal knowledge
distillation framework, allowing us to deal with issues which occur when
applying multimodal knowledge distillation in a naive manner. Lastly, we
demonstrate the achieved reduction in computational complexity, and show that
our approach maintains higher performance with the reduction of the number of
input views.
</p></li>
</ul>

<h3>Title: Improving BERT with Hybrid Pooling Network and Drop Mask. (arXiv:2307.07258v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07258">http://arxiv.org/abs/2307.07258</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07258] Improving BERT with Hybrid Pooling Network and Drop Mask](http://arxiv.org/abs/2307.07258) #transformer</code></li>
<li>Summary: <p>Transformer-based pre-trained language models, such as BERT, achieve great
success in various natural language understanding tasks. Prior research found
that BERT captures a rich hierarchy of linguistic information at different
layers. However, the vanilla BERT uses the same self-attention mechanism for
each layer to model the different contextual features. In this paper, we
propose a HybridBERT model which combines self-attention and pooling networks
to encode different contextual features in each layer. Additionally, we propose
a simple DropMask method to address the mismatch between pre-training and
fine-tuning caused by excessive use of special mask tokens during Masked
Language Modeling pre-training. Experiments show that HybridBERT outperforms
BERT in pre-training with lower loss, faster training speed (8% relative),
lower memory cost (13% relative), and also in transfer learning with 1.5%
relative higher accuracies on downstream tasks. Additionally, DropMask improves
accuracies of BERT on downstream tasks across various masking rates.
</p></li>
</ul>

<h3>Title: Are words equally surprising in audio and audio-visual comprehension?. (arXiv:2307.07277v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07277">http://arxiv.org/abs/2307.07277</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07277] Are words equally surprising in audio and audio-visual comprehension?](http://arxiv.org/abs/2307.07277) #transformer</code></li>
<li>Summary: <p>We report a controlled study investigating the effect of visual information
(i.e., seeing the speaker) on spoken language comprehension. We compare the ERP
signature (N400) associated with each word in audio-only and audio-visual
presentations of the same verbal stimuli. We assess the extent to which
surprisal measures (which quantify the predictability of words in their lexical
context) are generated on the basis of different types of language models
(specifically n-gram and Transformer models) that predict N400 responses for
each word. Our results indicate that cognitive effort differs significantly
between multimodal and unimodal settings. In addition, our findings suggest
that while Transformer-based models, which have access to a larger lexical
context, provide a better fit in the audio-only setting, 2-gram language models
are more effective in the multimodal setting. This highlights the significant
impact of local lexical context on cognitive processing in a multimodal
environment.
</p></li>
</ul>

<h3>Title: Rank Your Summaries: Enhancing Bengali Text Summarization via Ranking-based Approach. (arXiv:2307.07392v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07392">http://arxiv.org/abs/2307.07392</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07392] Rank Your Summaries: Enhancing Bengali Text Summarization via Ranking-based Approach](http://arxiv.org/abs/2307.07392) #transformer</code></li>
<li>Summary: <p>With the increasing need for text summarization techniques that are both
efficient and accurate, it becomes crucial to explore avenues that enhance the
quality and precision of pre-trained models specifically tailored for
summarizing Bengali texts. When it comes to text summarization tasks, there are
numerous pre-trained transformer models at one's disposal. Consequently, it
becomes quite a challenge to discern the most informative and relevant summary
for a given text among the various options generated by these pre-trained
summarization models. This paper aims to identify the most accurate and
informative summary for a given text by utilizing a simple but effective
ranking-based approach that compares the output of four different pre-trained
Bengali text summarization models. The process begins by carrying out
preprocessing of the input text that involves eliminating unnecessary elements
such as special characters and punctuation marks. Next, we utilize four
pre-trained summarization models to generate summaries, followed by applying a
text ranking algorithm to identify the most suitable summary. Ultimately, the
summary with the highest ranking score is chosen as the final one. To evaluate
the effectiveness of this approach, the generated summaries are compared
against human-annotated summaries using standard NLG metrics such as BLEU,
ROUGE, BERTScore, WIL, WER, and METEOR. Experimental results suggest that by
leveraging the strengths of each pre-trained transformer model and combining
them using a ranking-based approach, our methodology significantly improves the
accuracy and effectiveness of the Bengali text summarization.
</p></li>
</ul>

<h3>Title: Graph Positional and Structural Encoder. (arXiv:2307.07107v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07107">http://arxiv.org/abs/2307.07107</a></li>
<li>Code URL: <a href="https://github.com/g-taxonomy-workgroup/gpse">https://github.com/g-taxonomy-workgroup/gpse</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07107] Graph Positional and Structural Encoder](http://arxiv.org/abs/2307.07107) #transformer</code></li>
<li>Summary: <p>Positional and structural encodings (PSE) enable better identifiability of
nodes within a graph, as in general graphs lack a canonical node ordering. This
renders PSEs essential tools for empowering modern GNNs, and in particular
graph Transformers. However, designing PSEs that work optimally for a variety
of graph prediction tasks is a challenging and unsolved problem. Here, we
present the graph positional and structural encoder (GPSE), a first-ever
attempt to train a graph encoder that captures rich PSE representations for
augmenting any GNN. GPSE can effectively learn a common latent representation
for multiple PSEs, and is highly transferable. The encoder trained on a
particular graph dataset can be used effectively on datasets drawn from
significantly different distributions and even modalities. We show that across
a wide range of benchmarks, GPSE-enhanced models can significantly improve the
performance in certain tasks, while performing on par with those that employ
explicitly computed PSEs in other cases. Our results pave the way for the
development of large pre-trained models for extracting graph positional and
structural information and highlight their potential as a viable alternative to
explicitly computed PSEs as well as to existing self-supervised pre-training
approaches.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Improving Zero-Shot Generalization for CLIP with Synthesized Prompts. (arXiv:2307.07397v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07397">http://arxiv.org/abs/2307.07397</a></li>
<li>Code URL: <a href="https://github.com/mrflogs/SHIP">https://github.com/mrflogs/SHIP</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07397] Improving Zero-Shot Generalization for CLIP with Synthesized Prompts](http://arxiv.org/abs/2307.07397) #generative</code></li>
<li>Summary: <p>With the growing interest in pretrained vision-language models like CLIP,
recent research has focused on adapting these models to downstream tasks.
Despite achieving promising results, most existing methods require labeled data
for all classes, which may not hold in real-world applications due to the long
tail and Zipf's law. For example, some classes may lack labeled data entirely,
such as emerging concepts. To address this problem, we propose a plug-and-play
generative approach called \textbf{S}ynt\textbf{H}es\textbf{I}zed
\textbf{P}rompts~(\textbf{SHIP}) to improve existing fine-tuning methods.
Specifically, we follow variational autoencoders to introduce a generator that
reconstructs the visual features by inputting the synthesized prompts and the
corresponding class names to the textual encoder of CLIP. In this manner, we
easily obtain the synthesized features for the remaining label-only classes.
Thereafter, we fine-tune CLIP with off-the-shelf methods by combining labeled
and synthesized features. Extensive experiments on base-to-new generalization,
cross-dataset transfer learning, and generalized zero-shot learning demonstrate
the superiority of our approach. The code is available at
\url{https://github.com/mrflogs/SHIP}.
</p></li>
</ul>

<h3>Title: DreamTeacher: Pretraining Image Backbones with Deep Generative Models. (arXiv:2307.07487v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07487">http://arxiv.org/abs/2307.07487</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07487] DreamTeacher: Pretraining Image Backbones with Deep Generative Models](http://arxiv.org/abs/2307.07487) #generative</code></li>
<li>Summary: <p>In this work, we introduce a self-supervised feature representation learning
framework DreamTeacher that utilizes generative networks for pre-training
downstream image backbones. We propose to distill knowledge from a trained
generative model into standard image backbones that have been well engineered
for specific perception tasks. We investigate two types of knowledge
distillation: 1) distilling learned generative features onto target image
backbones as an alternative to pretraining these backbones on large labeled
datasets such as ImageNet, and 2) distilling labels obtained from generative
networks with task heads onto logits of target backbones. We perform extensive
analyses on multiple generative models, dense prediction benchmarks, and
several pre-training regimes. We empirically find that our DreamTeacher
significantly outperforms existing self-supervised representation learning
approaches across the board. Unsupervised ImageNet pre-training with
DreamTeacher leads to significant improvements over ImageNet classification
pre-training on downstream datasets, showcasing generative models, and
diffusion generative models specifically, as a promising approach to
representation learning on large, diverse datasets without requiring manual
annotation.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task. (arXiv:2307.06954v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06954">http://arxiv.org/abs/2307.06954</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06954] ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task](http://arxiv.org/abs/2307.06954) #large language model</code></li>
<li>Summary: <p>Conspiracy Theory Identication task is a new shared task proposed for the
first time at the Evalita 2023. The ACTI challenge, based exclusively on
comments published on conspiratorial channels of telegram, is divided into two
subtasks: (i) Conspiratorial Content Classification: identifying conspiratorial
content and (ii) Conspiratorial Category Classification about specific
conspiracy theory classification. A total of fifteen teams participated in the
task for a total of 81 submissions. We illustrate the best performing
approaches were based on the utilization of large language models. We finally
draw conclusions about the utilization of these models for counteracting the
spreading of misinformation in online platforms.
</p></li>
</ul>

<h3>Title: Generating Efficient Training Data via LLM-based Attribute Manipulation. (arXiv:2307.07099v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07099">http://arxiv.org/abs/2307.07099</a></li>
<li>Code URL: <a href="https://github.com/komeijiforce/cotam">https://github.com/komeijiforce/cotam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07099] Generating Efficient Training Data via LLM-based Attribute Manipulation](http://arxiv.org/abs/2307.07099) #large language model</code></li>
<li>Summary: <p>In this paper, we propose a novel method, Chain-of-Thoughts Attribute
Manipulation (CoTAM), to guide few-shot learning by carefully crafted data from
Large Language Models (LLMs). The main idea is to create data with changes only
in the attribute targeted by the task. Inspired by facial attribute
manipulation, our approach generates label-switched data by leveraging LLMs to
manipulate task-specific attributes and reconstruct new sentences in a
controlled manner. Instead of conventional latent representation controlling,
we implement chain-of-thoughts decomposition and reconstruction to adapt the
procedure to LLMs. Extensive results on text classification and other tasks
verify the advantage of CoTAM over other LLM-based text generation methods with
the same number of training examples. Analysis visualizes the attribute
manipulation effectiveness of CoTAM and presents the potential of LLM-guided
learning with even less supervision.
</p></li>
</ul>

<h3>Title: Learning to Retrieve In-Context Examples for Large Language Models. (arXiv:2307.07164v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07164">http://arxiv.org/abs/2307.07164</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07164] Learning to Retrieve In-Context Examples for Large Language Models](http://arxiv.org/abs/2307.07164) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated their ability to learn
in-context, allowing them to perform various tasks based on a few input-output
examples. However, the effectiveness of in-context learning is heavily reliant
on the quality of the selected examples. In this paper, we propose a novel
framework to iteratively train dense retrievers that can identify high-quality
in-context examples for LLMs. Our framework initially trains a reward model
based on LLM feedback to evaluate the quality of candidate examples, followed
by knowledge distillation to train a bi-encoder based dense retriever. Our
experiments on a suite of 30 tasks demonstrate that our framework significantly
enhances in-context learning performance. Furthermore, we show the
generalization ability of our framework to unseen tasks during training. An
in-depth analysis reveals that our model improves performance by retrieving
examples with similar patterns, and the gains are consistent across LLMs of
varying sizes.
</p></li>
</ul>

<h3>Title: MorphPiece : Moving away from Statistical Language Representation. (arXiv:2307.07262v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07262">http://arxiv.org/abs/2307.07262</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07262] MorphPiece : Moving away from Statistical Language Representation](http://arxiv.org/abs/2307.07262) #large language model</code></li>
<li>Summary: <p>Tokenization is a critical part of modern NLP pipelines. However,
contemporary tokenizers for Large Language Models are based on statistical
analysis of text corpora, without much consideration to the linguistic
features. We propose a linguistically motivated tokenization scheme,
MorphPiece, which is based partly on morphological segmentation of the
underlying text. A GPT-style causal language model trained on this tokenizer
(called MorphGPT) shows superior convergence compared to the same architecture
trained on a standard BPE tokenizer. Specifically we get Language Modeling
performance comparable to a 6 times larger model. Additionally, we evaluate
MorphGPT on a variety of NLP tasks in supervised and unsupervised settings and
find superior performance across the board, compared to GPT-2 model.
</p></li>
</ul>

<h3>Title: Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs. (arXiv:2307.07312v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07312">http://arxiv.org/abs/2307.07312</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07312] Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs](http://arxiv.org/abs/2307.07312) #large language model</code></li>
<li>Summary: <p>In any system that uses structured knowledge graph (KG) data as its
underlying knowledge representation, KG-to-text generation is a useful tool for
turning parts of the graph data into text that can be understood by humans.
Recent work has shown that models that make use of pretraining on large amounts
of text data can perform well on the KG-to-text task even with relatively small
sets of training data on the specific graph-to-text task. In this paper, we
build on this concept by using large language models to perform zero-shot
generation based on nothing but the model's understanding of the triple
structure from what it can read. We show that ChatGPT achieves near
state-of-the-art performance on some measures of the WebNLG 2020 challenge, but
falls behind on others. Additionally, we compare factual, counter-factual and
fictional statements, and show that there is a significant connection between
what the LLM already knows about the data it is parsing and the quality of the
output text.
</p></li>
</ul>

<h3>Title: Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases. (arXiv:2307.07411v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07411">http://arxiv.org/abs/2307.07411</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07411] Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases](http://arxiv.org/abs/2307.07411) #large language model</code></li>
<li>Summary: <p>Due to the recent improvements and wide availability of Large Language Models
(LLMs), they have posed a serious threat to academic integrity in education.
Modern LLM-generated text detectors attempt to combat the problem by offering
educators with services to assess whether some text is LLM-generated. In this
work, we have collected 124 submissions from computer science students before
the creation of ChatGPT. We then generated 40 ChatGPT submissions. We used this
data to evaluate eight publicly-available LLM-generated text detectors through
the measures of accuracy, false positives, and resilience. The purpose of this
work is to inform the community of what LLM-generated text detectors work and
which do not, but also to provide insights for educators to better maintain
academic integrity in their courses. Our results find that CopyLeaks is the
most accurate LLM-generated text detector, GPTKit is the best LLM-generated
text detector to reduce false positives, and GLTR is the most resilient
LLM-generated text detector. We also express concerns over 52 false positives
(of 114 human written submissions) generated by GPTZero. Finally, we note that
all LLM-generated text detectors are less accurate with code, other languages
(aside from English), and after the use of paraphrasing tools (like QuillBot).
Modern detectors are still in need of improvements so that they can offer a
full-proof solution to help maintain academic integrity. Further, their
usability can be improved by facilitating a smooth API integration, providing
clear documentation of their features and the understandability of their
model(s), and supporting more commonly used languages.
</p></li>
</ul>

<h3>Title: AutoHint: Automatic Prompt Optimization with Hint Generation. (arXiv:2307.07415v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07415">http://arxiv.org/abs/2307.07415</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07415] AutoHint: Automatic Prompt Optimization with Hint Generation](http://arxiv.org/abs/2307.07415) #large language model</code></li>
<li>Summary: <p>This paper presents AutoHint, a novel framework for automatic prompt
engineering and optimization for Large Language Models (LLM). While LLMs have
demonstrated remarkable ability in achieving high-quality annotation in various
tasks, the key to applying this ability to specific tasks lies in developing
high-quality prompts. Thus we propose a framework to inherit the merits of both
in-context learning and zero-shot learning by incorporating enriched
instructions derived from input-output demonstrations to optimize original
prompt. We refer to the enrichment as the hint and propose a framework to
automatically generate the hint from labeled data. More concretely, starting
from an initial prompt, our method first instructs a LLM to deduce new hints
for selected samples from incorrect predictions, and then summarizes from
per-sample hints and adds the results back to the initial prompt to form a new,
enriched instruction. The proposed method is evaluated on the BIG-Bench
Instruction Induction dataset for both zero-shot and few-short prompts, where
experiments demonstrate our method is able to significantly boost accuracy for
multiple tasks.
</p></li>
</ul>

<h3>Title: Named entity recognition using GPT for identifying comparable companies. (arXiv:2307.07420v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07420">http://arxiv.org/abs/2307.07420</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07420] Named entity recognition using GPT for identifying comparable companies](http://arxiv.org/abs/2307.07420) #large language model</code></li>
<li>Summary: <p>For both public and private firms, comparable companies analysis is widely
used as a method for company valuation. In particular, the method is of great
value for valuation of private equity companies. The several approaches to the
comparable companies method usually rely on a qualitative approach to
identifying similar peer companies, which tends to use established industry
classification schemes and/or analyst intuition and knowledge. However, more
quantitative methods have started being used in the literature and in the
private equity industry, in particular, machine learning clustering, and
natural language processing (NLP). For NLP methods, the process consists of
extracting product entities from e.g., the company's website or company
descriptions from some financial database system and then to perform similarity
analysis. Here, using companies descriptions/summaries from publicly available
companies' Wikipedia websites, we show that using large language models (LLMs),
such as GPT from openaAI, has a much higher precision and success rate than
using the standard named entity recognition (NER) which uses manual annotation.
We demonstrate quantitatively a higher precision rate, and show that,
qualitatively, it can be used to create appropriate comparable companies peer
groups which can then be used for equity valuation.
</p></li>
</ul>

<h3>Title: Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes. (arXiv:2307.07422v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07422">http://arxiv.org/abs/2307.07422</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07422] Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes](http://arxiv.org/abs/2307.07422) #large language model</code></li>
<li>Summary: <p>Increasingly powerful Large Language Model (LLM) based chatbots, like ChatGPT
and Bard, are becoming available to users that have the potential to
revolutionize the quality of decision-making achieved by the public. In this
context, we set out to investigate how such systems perform in the personal
finance domain, where financial inclusion has been an overarching stated aim of
banks for decades. We asked 13 questions representing banking products in
personal finance: bank account, credit card, and certificate of deposits and
their inter-product interactions, and decisions related to high-value
purchases, payment of bank dues, and investment advice, and in different
dialects and languages (English, African American Vernacular English, and
Telugu). We find that although the outputs of the chatbots are fluent and
plausible, there are still critical gaps in providing accurate and reliable
financial information using LLM-based chatbots.
</p></li>
</ul>

<h3>Title: Can Large Language Models Empower Molecular Property Prediction?. (arXiv:2307.07443v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07443">http://arxiv.org/abs/2307.07443</a></li>
<li>Code URL: <a href="https://github.com/chnq/llm4mol">https://github.com/chnq/llm4mol</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07443] Can Large Language Models Empower Molecular Property Prediction?](http://arxiv.org/abs/2307.07443) #large language model</code></li>
<li>Summary: <p>Molecular property prediction has gained significant attention due to its
transformative potential in multiple scientific disciplines. Conventionally, a
molecule graph can be represented either as a graph-structured data or a SMILES
text. Recently, the rapid development of Large Language Models (LLMs) has
revolutionized the field of NLP. Although it is natural to utilize LLMs to
assist in understanding molecules represented by SMILES, the exploration of how
LLMs will impact molecular property prediction is still in its early stage. In
this work, we advance towards this objective through two perspectives:
zero/few-shot molecular classification, and using the new explanations
generated by LLMs as representations of molecules. To be specific, we first
prompt LLMs to do in-context molecular classification and evaluate their
performance. After that, we employ LLMs to generate semantically enriched
explanations for the original SMILES and then leverage that to fine-tune a
small-scale LM model for multiple downstream tasks. The experimental results
highlight the superiority of text explanations as molecular representations
across multiple benchmark datasets, and confirm the immense potential of LLMs
in molecular property prediction tasks. Codes are available at
\url{https://github.com/ChnQ/LLM4Mol}.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: AnyStar: Domain randomized universal star-convex 3D instance segmentation. (arXiv:2307.07044v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07044">http://arxiv.org/abs/2307.07044</a></li>
<li>Code URL: <a href="https://github.com/neel-dey/anystar">https://github.com/neel-dey/anystar</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07044] AnyStar: Domain randomized universal star-convex 3D instance segmentation](http://arxiv.org/abs/2307.07044) #segmentation</code></li>
<li>Summary: <p>Star-convex shapes arise across bio-microscopy and radiology in the form of
nuclei, nodules, metastases, and other units. Existing instance segmentation
networks for such structures train on densely labeled instances for each
dataset, which requires substantial and often impractical manual annotation
effort. Further, significant reengineering or finetuning is needed when
presented with new datasets and imaging modalities due to changes in contrast,
shape, orientation, resolution, and density. We present AnyStar, a
domain-randomized generative model that simulates synthetic training data of
blob-like objects with randomized appearance, environments, and imaging physics
to train general-purpose star-convex instance segmentation networks. As a
result, networks trained using our generative model do not require annotated
images from unseen datasets. A single network trained on our synthesized data
accurately 3D segments C. elegans and P. dumerilii nuclei in fluorescence
microscopy, mouse cortical nuclei in micro-CT, zebrafish brain nuclei in EM,
and placental cotyledons in human fetal MRI, all without any retraining,
finetuning, transfer learning, or domain adaptation. Code is available at
https://github.com/neel-dey/AnyStar.
</p></li>
</ul>

<h3>Title: Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation. (arXiv:2307.07168v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07168">http://arxiv.org/abs/2307.07168</a></li>
<li>Code URL: <a href="https://github.com/deepmicroscopy/adaptiveregionselection">https://github.com/deepmicroscopy/adaptiveregionselection</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07168] Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation](http://arxiv.org/abs/2307.07168) #segmentation</code></li>
<li>Summary: <p>The process of annotating histological gigapixel-sized whole slide images
(WSIs) at the pixel level for the purpose of training a supervised segmentation
model is time-consuming. Region-based active learning (AL) involves training
the model on a limited number of annotated image regions instead of requesting
annotations of the entire images. These annotation regions are iteratively
selected, with the goal of optimizing model performance while minimizing the
annotated area. The standard method for region selection evaluates the
informativeness of all square regions of a specified size and then selects a
specific quantity of the most informative regions. We find that the efficiency
of this method highly depends on the choice of AL step size (i.e., the
combination of region size and the number of selected regions per WSI), and a
suboptimal AL step size can result in redundant annotation requests or inflated
computation costs. This paper introduces a novel technique for selecting
annotation regions adaptively, mitigating the reliance on this AL
hyperparameter. Specifically, we dynamically determine each region by first
identifying an informative area and then detecting its optimal bounding box, as
opposed to selecting regions of a uniform predefined shape and size as in the
standard method. We evaluate our method using the task of breast cancer
metastases segmentation on the public CAMELYON16 dataset and show that it
consistently achieves higher sampling efficiency than the standard method
across various AL step sizes. With only 2.6\% of tissue area annotated, we
achieve full annotation performance and thereby substantially reduce the costs
of annotating a WSI dataset. The source code is available at
https://github.com/DeepMicroscopy/AdaptiveRegionSelection.
</p></li>
</ul>

<h3>Title: FreeCOS: Self-Supervised Learning from Fractals and Unlabeled Images for Curvilinear Object Segmentation. (arXiv:2307.07245v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07245">http://arxiv.org/abs/2307.07245</a></li>
<li>Code URL: <a href="https://github.com/ty-shi/freecos">https://github.com/ty-shi/freecos</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07245] FreeCOS: Self-Supervised Learning from Fractals and Unlabeled Images for Curvilinear Object Segmentation](http://arxiv.org/abs/2307.07245) #segmentation</code></li>
<li>Summary: <p>Curvilinear object segmentation is critical for many applications. However,
manually annotating curvilinear objects is very time-consuming and error-prone,
yielding insufficiently available annotated datasets for existing supervised
methods and domain adaptation methods. This paper proposes a self-supervised
curvilinear object segmentation method that learns robust and distinctive
features from fractals and unlabeled images (FreeCOS). The key contributions
include a novel Fractal-FDA synthesis (FFS) module and a geometric information
alignment (GIA) approach. FFS generates curvilinear structures based on the
parametric Fractal L-system and integrates the generated structures into
unlabeled images to obtain synthetic training images via Fourier Domain
Adaptation. GIA reduces the intensity differences between the synthetic and
unlabeled images by comparing the intensity order of a given pixel to the
values of its nearby neighbors. Such image alignment can explicitly remove the
dependency on absolute intensity values and enhance the inherent geometric
characteristics which are common in both synthetic and real images. In
addition, GIA aligns features of synthetic and real images via the prediction
space adaptation loss (PSAL) and the curvilinear mask contrastive loss (CMCL).
Extensive experimental results on four public datasets, i.e., XCAD, DRIVE,
STARE and CrackTree demonstrate that our method outperforms the
state-of-the-art unsupervised methods, self-supervised methods and traditional
methods by a large margin. The source code of this work is available at
https://github.com/TY-Shi/FreeCOS.
</p></li>
</ul>

<h3>Title: Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training. (arXiv:2307.07246v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07246">http://arxiv.org/abs/2307.07246</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07246] Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training](http://arxiv.org/abs/2307.07246) #segmentation</code></li>
<li>Summary: <p>The foundation models based on pre-training technology have significantly
advanced artificial intelligence from theoretical to practical applications.
These models have facilitated the feasibility of computer-aided diagnosis for
widespread use. Medical contrastive vision-language pre-training, which does
not require human annotations, is an effective approach for guiding
representation learning using description information in diagnostic reports.
However, the effectiveness of pre-training is limited by the large-scale
semantic overlap and shifting problems in medical field. To address these
issues, we propose the Knowledge-Boosting Contrastive Vision-Language
Pre-training framework (KoBo), which integrates clinical knowledge into the
learning of vision-language semantic consistency. The framework uses an
unbiased, open-set sample-wise knowledge representation to measure negative
sample noise and supplement the correspondence between vision-language mutual
information and clinical knowledge. Extensive experiments validate the effect
of our framework on eight tasks including classification, segmentation,
retrieval, and semantic relatedness, achieving comparable or better performance
with the zero-shot or few-shot settings. Our code is open on
https://github.com/ChenXiaoFei-CS/KoBo.
</p></li>
</ul>

<h3>Title: SynTable: A Synthetic Data Generation Pipeline for Unseen Object Amodal Instance Segmentation of Cluttered Tabletop Scenes. (arXiv:2307.07333v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07333">http://arxiv.org/abs/2307.07333</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07333] SynTable: A Synthetic Data Generation Pipeline for Unseen Object Amodal Instance Segmentation of Cluttered Tabletop Scenes](http://arxiv.org/abs/2307.07333) #segmentation</code></li>
<li>Summary: <p>In this work, we present SynTable, a unified and flexible Python-based
dataset generator built using NVIDIA's Isaac Sim Replicator Composer for
generating high-quality synthetic datasets for unseen object amodal instance
segmentation of cluttered tabletop scenes. Our dataset generation tool can
render a complex 3D scene containing object meshes, materials, textures,
lighting, and backgrounds. Metadata, such as modal and amodal instance
segmentation masks, occlusion masks, depth maps, bounding boxes, and material
properties, can be generated to automatically annotate the scene according to
the users' requirements. Our tool eliminates the need for manual labeling in
the dataset generation process while ensuring the quality and accuracy of the
dataset. In this work, we discuss our design goals, framework architecture, and
the performance of our tool. We demonstrate the use of a sample dataset
generated using SynTable by ray tracing for training a state-of-the-art model,
UOAIS-Net. The results show significantly improved performance in Sim-to-Real
transfer when evaluated on the OSD-Amodal dataset. We offer this tool as an
open-source, easy-to-use, photorealistic dataset generator for advancing
research in deep learning and synthetic data generation.
</p></li>
</ul>

<h3>Title: A scoping review on multimodal deep learning in biomedical images and texts. (arXiv:2307.07362v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.07362">http://arxiv.org/abs/2307.07362</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.07362] A scoping review on multimodal deep learning in biomedical images and texts](http://arxiv.org/abs/2307.07362) #segmentation</code></li>
<li>Summary: <p>Computer-assisted diagnostic and prognostic systems of the future should be
capable of simultaneously processing multimodal data. Multimodal deep learning
(MDL), which involves the integration of multiple sources of data, such as
images and text, has the potential to revolutionize the analysis and
interpretation of biomedical data. However, it only caught researchers'
attention recently. To this end, there is a critical need to conduct a
systematic review on this topic, identify the limitations of current work, and
explore future directions. In this scoping review, we aim to provide a
comprehensive overview of the current state of the field and identify key
concepts, types of studies, and research gaps with a focus on biomedical images
and texts joint learning, mainly because these two were the most commonly
available data types in MDL research. This study reviewed the current uses of
multimodal deep learning on five tasks: (1) Report generation, (2) Visual
question answering, (3) Cross-modal retrieval, (4) Computer-aided diagnosis,
and (5) Semantic segmentation. Our results highlight the diverse applications
and potential of MDL and suggest directions for future research in the field.
We hope our review will facilitate the collaboration of natural language
processing (NLP) and medical imaging communities and support the next
generation of decision-making and computer-assisted diagnostic system
development.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
