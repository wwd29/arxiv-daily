<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-21</h1>
<h3>Title: Contrastive Analysis of Constituent Order Preferences Within Adverbial Roles in English and Chinese News: A Large-Language-Model-Driven Approach</h3>
<ul>
<li><strong>Authors: </strong>Yiran Rex Ma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14054">https://arxiv.org/abs/2508.14054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14054">https://arxiv.org/pdf/2508.14054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14054]] Contrastive Analysis of Constituent Order Preferences Within Adverbial Roles in English and Chinese News: A Large-Language-Model-Driven Approach(https://arxiv.org/abs/2508.14054)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Based on comparable English-Chinese news corpora annotated by Large Language Model (LLM), this paper attempts to explore the differences in constituent order of English-Chinese news from the perspective of functional chunks with adverbial roles, and analyze their typical positional preferences and distribution patterns. It is found that: (1) English news prefers linear narrative of core information first, and functional chunks are mostly post-positioned, while Chinese news prefers overall presentation mode of background first, and functional chunks are often pre-positioned; (2) In SVO structure, both English and Chinese news show differences in the distribution of functional chunks, but the tendency of Chinese pre-positioning is more significant, while that of English post-positioning is relatively mild; (3) When function blocks are co-occurring, both English and Chinese news show high flexibility, and the order adjustment is driven by information and pragmatic purposes. The study reveals that word order has both systematic preference and dynamic adaptability, providing new empirical support for contrastive study of English-Chinese information structure.</li>
</ul>

<h3>Title: T-REX: Table -- Refute or Entail eXplainer</h3>
<ul>
<li><strong>Authors: </strong>Tim Luka Horstmann, Baptiste Geisenberger, Mehwish Alam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14055">https://arxiv.org/abs/2508.14055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14055">https://arxiv.org/pdf/2508.14055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14055]] T-REX: Table -- Refute or Entail eXplainer(https://arxiv.org/abs/2508.14055)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Verifying textual claims against structured tabular data is a critical yet challenging task in Natural Language Processing with broad real-world impact. While recent advances in Large Language Models (LLMs) have enabled significant progress in table fact-checking, current solutions remain inaccessible to non-experts. We introduce T-REX (T-REX: Table -- Refute or Entail eXplainer), the first live, interactive tool for claim verification over multimodal, multilingual tables using state-of-the-art instruction-tuned reasoning LLMs. Designed for accuracy and transparency, T-REX empowers non-experts by providing access to advanced fact-checking technology. The system is openly available online.</li>
</ul>

<h3>Title: Confidence Estimation for Text-to-SQL in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sepideh Entezari Maleki, Mohammadreza Pourreza, Davood Rafiei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14056">https://arxiv.org/abs/2508.14056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14056">https://arxiv.org/pdf/2508.14056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14056]] Confidence Estimation for Text-to-SQL in Large Language Models(https://arxiv.org/abs/2508.14056)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Confidence estimation for text-to-SQL aims to assess the reliability of model-generated SQL queries without having access to gold answers. We study this problem in the context of large language models (LLMs), where access to model weights and gradients is often constrained. We explore both black-box and white-box confidence estimation strategies, evaluating their effectiveness on cross-domain text-to-SQL benchmarks. Our evaluation highlights the superior performance of consistency-based methods among black-box models and the advantage of SQL-syntax-aware approaches for interpreting LLM logits in white-box settings. Furthermore, we show that execution-based grounding of queries provides a valuable supplementary signal, improving the effectiveness of both approaches.</li>
</ul>

<h3>Title: Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Badrinath Ramakrishnan, Akshaya Balaji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14062">https://arxiv.org/abs/2508.14062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14062">https://arxiv.org/pdf/2508.14062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14062]] Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models(https://arxiv.org/abs/2508.14062)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, but their tendency to memorize training data poses significant privacy risks, particularly during fine-tuning processes. This paper presents a comprehensive empirical analysis of data memorization in fine-tuned LLMs and introduces a novel multi-layered privacy protection framework. Through controlled experiments on modern LLM architectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that fine-tuning with repeated sensitive data increases privacy leakage rates from baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across tested models. We propose and rigorously evaluate four complementary privacy protection methods: semantic data deduplication, differential privacy during generation, entropy-based filtering, and pattern-based content filtering. Our experimental results show that these techniques can reduce data leakage to 0% while maintaining 94.7% of original model utility.</li>
</ul>

<h3>Title: Punctuation and Predicates in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sonakshi Chauhan, Maheep Chaudhary, Koby Choy, Samuel Nellessen, Nandi Schoots</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14067">https://arxiv.org/abs/2508.14067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14067">https://arxiv.org/pdf/2508.14067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14067]] Punctuation and Predicates in Language Models(https://arxiv.org/abs/2508.14067)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>In this paper we explore where information is collected and how it is propagated throughout layers in large language models (LLMs). We begin by examining the surprising computational importance of punctuation tokens which previous work has identified as attention sinks and memory aids. Using intervention-based techniques, we evaluate the necessity and sufficiency (for preserving model performance) of punctuation tokens across layers in GPT-2, DeepSeek, and Gemma. Our results show stark model-specific differences: for GPT-2, punctuation is both necessary and sufficient in multiple layers, while this holds far less in DeepSeek and not at all in Gemma. Extending beyond punctuation, we ask whether LLMs process different components of input (e.g., subjects, adjectives, punctuation, full sentences) by forming early static summaries reused across the network, or if the model remains sensitive to changes in these components across layers. Extending beyond punctuation, we investigate whether different reasoning rules are processed differently by LLMs. In particular, through interchange intervention and layer-swapping experiments, we find that conditional statements (if, then), and universal quantification (for all) are processed very differently. Our findings offer new insight into the internal mechanisms of punctuation usage and reasoning in LLMs and have implications for interpretability.</li>
</ul>

<h3>Title: Special-Character Adversarial Attacks on Open-Source Language Model</h3>
<ul>
<li><strong>Authors: </strong>Ephraiem Sarabamoun</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14070">https://arxiv.org/abs/2508.14070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14070">https://arxiv.org/pdf/2508.14070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14070]] Special-Character Adversarial Attacks on Open-Source Language Model(https://arxiv.org/abs/2508.14070)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable performance across diverse natural language processing tasks, yet their vulnerability to character-level adversarial manipulations presents significant security challenges for real-world deployments.</li>
</ul>

<h3>Title: MCLPD:Multi-view Contrastive Learning for EEG-based PD Detection Across Datasets</h3>
<ul>
<li><strong>Authors: </strong>Qian Zhanga, Ruilin Zhang, Jun Xiao, Yifan Liu, Zhe Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14073">https://arxiv.org/abs/2508.14073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14073">https://arxiv.org/pdf/2508.14073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14073]] MCLPD:Multi-view Contrastive Learning for EEG-based PD Detection Across Datasets(https://arxiv.org/abs/2508.14073)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Electroencephalography has been validated as an effective technique for detecting Parkinson's disease,particularly in its early this http URL,the high cost of EEG data annotation often results in limited dataset size and considerable discrepancies across datasets,including differences in acquisition protocols and subject demographics,significantly hinder the robustness and generalizability of models in cross-dataset detection this http URL address such challenges,this paper proposes a semi-supervised learning framework named MCLPD,which integrates multi-view contrastive pre-training with lightweight supervised fine-tuning to enhance cross-dataset PD detection this http URL pre-training,MCLPD uses self-supervised learning on the unlabeled UNM this http URL build contrastive pairs,it applies dual augmentations in both time and frequency domains,which enrich the data and naturally fuse time-frequency this http URL the fine-tuning phase,only a small proportion of labeled data from another two datasets (UI and UC)is used for supervised this http URL results show that MCLPD achieves F1 scores of 0.91 on UI and 0.81 on UC using only 1%of labeled data,which further improve to 0.97 and 0.87,respectively,when 5%of labeled data is this http URL to existing methods,MCLPD substantially improves cross-dataset generalization while reducing the dependency on labeled data,demonstrating the effectiveness of the proposed framework.</li>
</ul>

<h3>Title: GEPD:GAN-Enhanced Generalizable Model for EEG-Based Detection of Parkinson's Disease</h3>
<ul>
<li><strong>Authors: </strong>Qian Zhang, Ruilin Zhang, Biaokai Zhu, Xun Han, Jun Xiao, Yifan Liu, Zhe Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14074">https://arxiv.org/abs/2508.14074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14074">https://arxiv.org/pdf/2508.14074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14074]] GEPD:GAN-Enhanced Generalizable Model for EEG-Based Detection of Parkinson's Disease(https://arxiv.org/abs/2508.14074)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Electroencephalography has been established as an effective method for detecting Parkinson's disease, typically diagnosed this http URL Parkinson's disease detection methods have shown significant success within individual datasets, however, the variability in detection methods across different EEG datasets and the small size of each dataset pose challenges for training a generalizable model for cross-dataset scenarios. To address these issues, this paper proposes a GAN-enhanced generalizable model, named GEPD, specifically for EEG-based cross-dataset classification of Parkinson's this http URL, we design a generative network that creates fusion EEG data by controlling the distribution similarity between generated data and real this http URL addition, an EEG signal quality assessment model is designed to ensure the quality of generated data this http URL, we design a classification network that utilizes a combination of multiple convolutional neural networks to effectively capture the time-frequency characteristics of EEG signals, while maintaining a generalizable structure and ensuring easy this http URL work is dedicated to utilizing intelligent methods to study pathological manifestations, aiming to facilitate the diagnosis and monitoring of neurological this http URL evaluation results demonstrate that our model performs comparably to state-of-the-art models in cross-dataset settings, achieving an accuracy of 84.3% and an F1-score of 84.0%, showcasing the generalizability of the proposed model.</li>
</ul>

<h3>Title: Explainable Graph Spectral Clustering For Text Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Mieczysław A. Kłopotek, Sławomir T. Wierzchoń, Bartłomiej Starosta, Piotr Borkowski, Dariusz Czerski, Eryk Laskowski</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14075">https://arxiv.org/abs/2508.14075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14075">https://arxiv.org/pdf/2508.14075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14075]] Explainable Graph Spectral Clustering For Text Embeddings(https://arxiv.org/abs/2508.14075)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>In a previous paper, we proposed an introduction to the explainability of Graph Spectral Clustering results for textual documents, given that document similarity is computed as cosine similarity in term vector space. In this paper, we generalize this idea by considering other embeddings of documents, in particular, based on the GloVe embedding idea.</li>
</ul>

<h3>Title: PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Mengdi Li, Guanqiao Chen, Xufeng Zhao, Haochen Wen, Shu Yang, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14076">https://arxiv.org/abs/2508.14076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14076">https://arxiv.org/pdf/2508.14076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14076]] PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning(https://arxiv.org/abs/2508.14076)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reward models (RMs), which are central to existing post-training methods, aim to align LLM outputs with human values by providing feedback signals during fine-tuning. However, existing RMs struggle to capture nuanced, user-specific preferences, especially under limited data and across diverse domains. Thus, we introduce PersRM-R1, the first reasoning-based reward modeling framework specifically designed to identify and represent personal factors from only one or a few personal exemplars. To address challenges including limited data availability and the requirement for robust generalization, our approach combines synthetic data generation with a two-stage training pipeline consisting of supervised fine-tuning followed by reinforcement fine-tuning. Experimental results demonstrate that PersRM-R1 outperforms existing models of similar size and matches the performance of much larger models in both accuracy and generalizability, paving the way for more effective personalized LLMs.</li>
</ul>

<h3>Title: Out-of-Sample Hydrocarbon Production Forecasting: Time Series Machine Learning using Productivity Index-Driven Features and Inductive Conformal Prediction</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Hassan Abdalla Idris, Jakub Marek Cebula, Jebraeel Gholinezhad, Shamsul Masum, Hongjie Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14078">https://arxiv.org/abs/2508.14078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14078">https://arxiv.org/pdf/2508.14078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14078]] Out-of-Sample Hydrocarbon Production Forecasting: Time Series Machine Learning using Productivity Index-Driven Features and Inductive Conformal Prediction(https://arxiv.org/abs/2508.14078)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This research introduces a new ML framework designed to enhance the robustness of out-of-sample hydrocarbon production forecasting, specifically addressing multivariate time series analysis. The proposed methodology integrates Productivity Index (PI)-driven feature selection, a concept derived from reservoir engineering, with Inductive Conformal Prediction (ICP) for rigorous uncertainty quantification. Utilizing historical data from the Volve (wells PF14, PF12) and Norne (well E1H) oil fields, this study investigates the efficacy of various predictive algorithms-namely Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit (GRU), and eXtreme Gradient Boosting (XGBoost) - in forecasting historical oil production rates (OPR_H). All the models achieved "out-of-sample" production forecasts for an upcoming future timeframe. Model performance was comprehensively evaluated using traditional error metrics (e.g., MAE) supplemented by Forecast Bias and Prediction Direction Accuracy (PDA) to assess bias and trend-capturing capabilities. The PI-based feature selection effectively reduced input dimensionality compared to conventional numerical simulation workflows. The uncertainty quantification was addressed using the ICP framework, a distribution-free approach that guarantees valid prediction intervals (e.g., 95% coverage) without reliance on distributional assumptions, offering a distinct advantage over traditional confidence intervals, particularly for complex, non-normal data. Results demonstrated the superior performance of the LSTM model, achieving the lowest MAE on test (19.468) and genuine out-of-sample forecast data (29.638) for well PF14, with subsequent validation on Norne well E1H. These findings highlight the significant potential of combining domain-specific knowledge with advanced ML techniques to improve the reliability of hydrocarbon production forecasts.</li>
</ul>

<h3>Title: A Guide to Robust Generalization: The Impact of Architecture, Pre-training, and Optimization Strategy</h3>
<ul>
<li><strong>Authors: </strong>Maxime Heuillet, Rishika Bhagwatkar, Jonas Ngnawé, Yann Pequignot, Alexandre Larouche, Christian Gagné, Irina Rish, Ola Ahmad, Audrey Durand</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14079">https://arxiv.org/abs/2508.14079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14079">https://arxiv.org/pdf/2508.14079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14079]] A Guide to Robust Generalization: The Impact of Architecture, Pre-training, and Optimization Strategy(https://arxiv.org/abs/2508.14079)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning models operating in the image domain are vulnerable to small input perturbations. For years, robustness to such perturbations was pursued by training models from scratch (i.e., with random initializations) using specialized loss objectives. Recently, robust fine-tuning has emerged as a more efficient alternative: instead of training from scratch, pretrained models are adapted to maximize predictive performance and robustness. To conduct robust fine-tuning, practitioners design an optimization strategy that includes the model update protocol (e.g., full or partial) and the specialized loss objective. Additional design choices include the architecture type and size, and the pretrained representation. These design choices affect robust generalization, which is the model's ability to maintain performance when exposed to new and unseen perturbations at test time. Understanding how these design choices influence generalization remains an open question with significant practical implications. In response, we present an empirical study spanning 6 datasets, 40 pretrained architectures, 2 specialized losses, and 3 adaptation protocols, yielding 1,440 training configurations and 7,200 robustness measurements across five perturbation types. To our knowledge, this is the most diverse and comprehensive benchmark of robust fine-tuning to date. While attention-based architectures and robust pretrained representations are increasingly popular, we find that convolutional neural networks pretrained in a supervised manner on large datasets often perform best. Our analysis both confirms and challenges prior design assumptions, highlighting promising research directions and offering practical guidance.</li>
</ul>

<h3>Title: KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Guanghao Jin, Jingpei Wu, Tianpei Guo, Yiyi Niu, Weidong Zhou, Guoyang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14080">https://arxiv.org/abs/2508.14080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14080">https://arxiv.org/pdf/2508.14080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14080]] KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World Knowledge(https://arxiv.org/abs/2508.14080)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Referring Expression Comprehension (REC) is a popular multimodal task that aims to accurately detect target objects within a single image based on a given textual expression. However, due to the limitations of earlier models, traditional REC benchmarks either rely solely on intra-image cues or lack sufficiently fine-grained instance annotations, making them inadequate for evaluating the reasoning capabilities of Multi-modal Large Language Models (MLLMs). To address this gap, we propose a new benchmark, KnowDR-REC, characterized by three key features: Firstly, it is built upon real-world knowledge, requiring fine-grained multimodal reasoning across text and image. Secondly, the dataset includes elaborately constructed negative samples via fine-grained expression editing, designed to evaluate a model's robustness and anti-hallucination ability. Lastly, we introduce three novel evaluation metrics to systematically explore the model's internal reasoning process. We evaluate 16 state-of-the-art multimodal models on KnowDR-REC, with experimental results showing that existing MLLMs still struggle with knowledge-driven visual grounding tasks. Furthermore, we observe a decoupling between textual understanding and visual grounding in MLLMs, where many models are significantly influenced by memorized shortcut correlations, which severely affect their behavior on our benchmark and hinder genuine multimodal reasoning. We anticipate that the proposed benchmark will inspire future research towards developing more robust, interpretable, and knowledge-intensive visual grounding frameworks, driving the development of more reliable and robust multimodal systems for complex real-world scenarios.</li>
</ul>

<h3>Title: Toward Generalist Semi-supervised Regression via Decoupled Representation Distillation</h3>
<ul>
<li><strong>Authors: </strong>Ye Su, Hezhe Qiao, Wei Huang, Lin Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14082">https://arxiv.org/abs/2508.14082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14082">https://arxiv.org/pdf/2508.14082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14082]] Toward Generalist Semi-supervised Regression via Decoupled Representation Distillation(https://arxiv.org/abs/2508.14082)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Semi-supervised regression (SSR), which aims to predict continuous scores of samples while reducing reliance on a large amount of labeled data, has recently received considerable attention across various applications, including computer vision, natural language processing, and audio and medical analysis. Existing semi-supervised methods typically apply consistency regularization on the general regression task by generating pseudo-labels. However, these methods heavily rely on the quality of pseudo-labels, and direct regression fails to learn the label distribution and can easily lead to overfitting. To address these challenges, we introduce an end-to-end Decoupled Representation distillation framework (DRILL) which is specially designed for the semi-supervised regression task where we transform the general regression task into a Discrete Distribution Estimation (DDE) task over multiple buckets to better capture the underlying label distribution and mitigate the risk of overfitting associated with direct regression. Then we employ the Decoupled Distribution Alignment (DDA) to align the target bucket and non-target bucket between teacher and student on the distribution of buckets, encouraging the student to learn more robust and generalized knowledge from the teacher. Extensive experiments conducted on datasets from diverse domains demonstrate that the proposed DRILL has strong generalization and outperforms the competing methods.</li>
</ul>

<h3>Title: Parameter-Aware Ensemble SINDy for Interpretable Symbolic SGS Closure</h3>
<ul>
<li><strong>Authors: </strong>Hanseul Kang, Shervin Karimkashi, Ville Vuorinen</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14085">https://arxiv.org/abs/2508.14085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14085">https://arxiv.org/pdf/2508.14085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14085]] Parameter-Aware Ensemble SINDy for Interpretable Symbolic SGS Closure(https://arxiv.org/abs/2508.14085)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a scalable, parameter-aware sparse regression framework for discovering interpretable partial differential equations and subgrid-scale closures from multi-parameter simulation data. Building on SINDy (Sparse Identification of Nonlinear Dynamics), our approach addresses key limitations through four innovations: symbolic parameterisation enabling physical parameters to vary within unified regression; Dimensional Similarity Filter enforcing unit-consistency whilst reducing candidate libraries; memory-efficient Gram-matrix accumulation enabling batch processing; and ensemble consensus with coefficient stability analysis for robust model identification. Validation on canonical one-dimensional benchmarks demonstrates reliable recovery of governing equations across parameter ranges. Applied to filtered Burgers datasets, the framework discovers an SGS closure $\tau_{\mathrm{SGS}} = 0.1603\cdot\Delta^2\left(\frac{\partial \bar{u}}{\partial x}\right)^2$, corresponding to a Smagorinsky constant of approximately 0.4004. This represents autonomous discovery of Smagorinsky-type closure structure from data without prior theoretical assumptions. The discovered model achieves $R^2 = 0.886$ across filter scales and demonstrates improved prediction accuracy compared to classical closures. The framework's ability to identify physically meaningful SGS forms and calibrate coefficients offers a complementary approach to existing turbulence modelling methods, contributing to the growing field of data-driven closure discovery.</li>
</ul>

<h3>Title: EEGDM: EEG Representation Learning via Generative Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Jia Hong Puah, Sim Kuan Goh, Ziwei Zhang, Zixuan Ye, Chow Khuen Chan, Kheng Seang Lim, Si Lei Fong, Kok Sin Woon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14086">https://arxiv.org/abs/2508.14086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14086">https://arxiv.org/pdf/2508.14086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14086]] EEGDM: EEG Representation Learning via Generative Diffusion Model(https://arxiv.org/abs/2508.14086)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>While electroencephalogram (EEG) has been a crucial tool for monitoring the brain and diagnosing neurological disorders (e.g., epilepsy), learning meaningful representations from raw EEG signals remains challenging due to limited annotations and high signal variability. Recently, EEG foundation models (FMs) have shown promising potential by adopting transformer architectures and self-supervised pre-training methods from large language models (e.g., masked prediction) to learn representations from diverse EEG data, followed by fine-tuning on specific EEG tasks. Nonetheless, these large models often incurred high computational costs during both training and inference, with only marginal performance improvements as model size increases. In this work, we proposed EEG representation learning framework building upon Generative Diffusion Model (EEGDM). Specifically, we developed structured state-space model for diffusion pretraining (SSMDP) to better capture the temporal dynamics of EEG signals and trained the architecture using a Denoising Diffusion Probabilistic Model. The resulting latent EEG representations were then used for downstream classification tasks via our proposed latent fusion transformer (LFT). To evaluate our method, we used the multi-event Temple University EEG Event Corpus and compared EEGDM with current state-of-the-art approaches, including EEG FMs. Empirical results showed that our method outperformed existing methods while being approximately 19x more lightweight. These findings suggested that EEGDM offered a promising alternative to current FMs. Our code is available at: this https URL.</li>
</ul>

<h3>Title: FM4NPP: A Scaling Foundation Model for Nuclear and Particle Physics</h3>
<ul>
<li><strong>Authors: </strong>David Park, Shuhang Li, Yi Huang, Xihaier Luo, Haiwang Yu, Yeonju Go, Christopher Pinkenburg, Yuewei Lin, Shinjae Yoo, Joseph Osborn, Jin Huang, Yihui Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, hep-ex</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14087">https://arxiv.org/abs/2508.14087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14087">https://arxiv.org/pdf/2508.14087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14087]] FM4NPP: A Scaling Foundation Model for Nuclear and Particle Physics(https://arxiv.org/abs/2508.14087)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models have revolutionized artificial intelligence by enabling large, generalizable models trained through self-supervision. This paradigm has inspired the development of scientific foundation models (FMs). However, applying this capability to experimental particle physics is challenging due to the sparse, spatially distributed nature of detector data, which differs dramatically from natural language. This work addresses if an FM for particle physics can scale and generalize across diverse tasks. We introduce a new dataset with more than 11 million particle collision events and a suite of downstream tasks and labeled data for evaluation. We propose a novel self-supervised training method for detector data and demonstrate its neural scalability with models that feature up to 188 million parameters. With frozen weights and task-specific adapters, this FM consistently outperforms baseline models across all downstream tasks. The performance also exhibits robust data-efficient adaptation. Further analysis reveals that the representations extracted by the FM are task-agnostic but can be specialized via a single linear mapping for different downstream tasks.</li>
</ul>

<h3>Title: DLLMQuant: Quantizing Diffusion-based Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chen Xu, Dawei Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14090">https://arxiv.org/abs/2508.14090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14090">https://arxiv.org/pdf/2508.14090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14090]] DLLMQuant: Quantizing Diffusion-based Large Language Models(https://arxiv.org/abs/2508.14090)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion-based large language models (DLLMs) have shown promise for non-autoregressive text generation, but their deployment is constrained by large model sizes and heavy computational costs. Post-training quantization (PTQ), a widely used method for compressing and accelerating Large Language Models (LLMs), suffers from severe accuracy degradation and reduced generalization performance when directly applied to DLLMs (e.g., AWQ suffers a 16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key mechanisms - dynamic masking, iterative generation, bidirectional attention - clash with quantization. We identify three core issues: 1) Iterative generation and dynamic masking ratios lead to distinct token distributions across decoding steps, which are not adequately captured by existing PTQ calibration methods; 2) Quantization errors are accumulated and amplified progressively during iteration in DLLMs, causing quantized models to perform worse as decoding steps progress; 3) Unmasked tokens stabilize while masked remain probabilistic, making overall feature distribution incompatible with existing PTQ methods. To address these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs, which incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling (TMAS), a calibration method that accounts for both time and mask factors, with the capacity to capture distributions across timesteps. 2) Interaction-Aware Activation Quantization (IA-AQ), which utilizes bidirectional attention's interaction signals to dynamically allocate quantization resources. 3) Certainty-Guided Quantization (CGQ), which integrates mask status and token scores as key weighting criteria into error compensation, making weight quantization more suitable for DLLMs. Experiments show that DLLMQuant achieves significant performance gains while enhancing efficiency.</li>
</ul>

<h3>Title: Logical Expressivity and Explanations for Monotonic GNNs with Scoring Functions</h3>
<ul>
<li><strong>Authors: </strong>Matthew Morris, David J. Tena Cucala, Bernardo Cuenca Grau</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14091">https://arxiv.org/abs/2508.14091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14091">https://arxiv.org/pdf/2508.14091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14091]] Logical Expressivity and Explanations for Monotonic GNNs with Scoring Functions(https://arxiv.org/abs/2508.14091)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs) are often used for the task of link prediction: predicting missing binary facts in knowledge graphs (KGs). To address the lack of explainability of GNNs on KGs, recent works extract Datalog rules from GNNs with provable correspondence guarantees. The extracted rules can be used to explain the GNN's predictions; furthermore, they can help characterise the expressive power of various GNN models. However, these works address only a form of link prediction based on a restricted, low-expressivity graph encoding/decoding method. In this paper, we consider a more general and popular approach for link prediction where a scoring function is used to decode the GNN output into fact predictions. We show how GNNs and scoring functions can be adapted to be monotonic, use the monotonicity to extract sound rules for explaining predictions, and leverage existing results about the kind of rules that scoring functions can capture. We also define procedures for obtaining equivalent Datalog programs for certain classes of monotonic GNNs with scoring functions. Our experiments show that, on link prediction benchmarks, monotonic GNNs and scoring functions perform well in practice and yield many sound rules.</li>
</ul>

<h3>Title: From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Wei, Yuejin Yang, Xiang Zhang, Yuhan Chen, Xiang Zhuang, Zhangyang Gao, Dongzhan Zhou, Guangshuai Wang, Zhiqiang Gao, Juntai Cao, Zijie Qiu, Xuming He, Qiang Zhang, Chenyu You, Shuangjia Zheng, Ning Ding, Wanli Ouyang, Nanqing Dong, Yu Cheng, Siqi Sun, Lei Bai, Bowen Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14111">https://arxiv.org/abs/2508.14111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14111">https://arxiv.org/pdf/2508.14111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14111]] From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery(https://arxiv.org/abs/2508.14111)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) is reshaping scientific discovery, evolving from specialized computational tools into autonomous research partners. We position Agentic Science as a pivotal stage within the broader AI for Science paradigm, where AI systems progress from partial assistance to full scientific agency. Enabled by large language models (LLMs), multimodal systems, and integrated research platforms, agentic AI shows capabilities in hypothesis generation, experimental design, execution, analysis, and iterative refinement -- behaviors once regarded as uniquely human. This survey provides a domain-oriented review of autonomous scientific discovery across life sciences, chemistry, materials science, and physics. We unify three previously fragmented perspectives -- process-oriented, autonomy-oriented, and mechanism-oriented -- through a comprehensive framework that connects foundational capabilities, core processes, and domain-specific realizations. Building on this framework, we (i) trace the evolution of AI for Science, (ii) identify five core capabilities underpinning scientific agency, (iii) model discovery as a dynamic four-stage workflow, (iv) review applications across the above domains, and (v) synthesize key challenges and future opportunities. This work establishes a domain-oriented synthesis of autonomous scientific discovery and positions Agentic Science as a structured paradigm for advancing AI-driven research.</li>
</ul>

<h3>Title: Federated Action Recognition for Smart Worker Assistance Using FastPose</h3>
<ul>
<li><strong>Authors: </strong>Vinit Hegiste, Vidit Goyal, Tatjana Legler, Martin Ruskowski</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.DC, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14113">https://arxiv.org/abs/2508.14113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14113">https://arxiv.org/pdf/2508.14113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14113]] Federated Action Recognition for Smart Worker Assistance Using FastPose(https://arxiv.org/abs/2508.14113)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate, transformer</a></li>
<li><strong>Abstract: </strong>In smart manufacturing environments, accurate and real-time recognition of worker actions is essential for productivity, safety, and human-machine collaboration. While skeleton-based human activity recognition (HAR) offers robustness to lighting, viewpoint, and background variations, most existing approaches rely on centralized datasets, which are impractical in privacy-sensitive industrial scenarios. This paper presents a federated learning (FL) framework for pose-based HAR using a custom skeletal dataset of eight industrially relevant upper-body gestures, captured from five participants and processed using a modified FastPose model. Two temporal backbones, an LSTM and a Transformer encoder, are trained and evaluated under four paradigms: centralized, local (per-client), FL with weighted federated averaging (FedAvg), and federated ensemble learning (FedEnsemble). On the global test set, the FL Transformer improves over centralized training by +12.4 percentage points, with FedEnsemble delivering a +16.3 percentage points gain. On an unseen external client, FL and FedEnsemble exceed centralized accuracy by +52.6 and +58.3 percentage points, respectively. These results demonstrate that FL not only preserves privacy but also substantially enhances cross-user generalization, establishing it as a practical solution for scalable, privacy-aware HAR in heterogeneous industrial settings.</li>
</ul>

<h3>Title: CCFC: Core & Core-Full-Core Dual-Track Defense for LLM Jailbreak Protection</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Hu, Haoyu Wang, Debarghya Mukherjee, Ioannis Ch. Paschalidis</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14128">https://arxiv.org/abs/2508.14128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14128">https://arxiv.org/pdf/2508.14128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14128]] CCFC: Core & Core-Full-Core Dual-Track Defense for LLM Jailbreak Protection(https://arxiv.org/abs/2508.14128)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Jailbreak attacks pose a serious challenge to the safe deployment of large language models (LLMs). We introduce CCFC (Core & Core-Full-Core), a dual-track, prompt-level defense framework designed to mitigate LLMs' vulnerabilities from prompt injection and structure-aware jailbreak attacks. CCFC operates by first isolating the semantic core of a user query via few-shot prompting, and then evaluating the query using two complementary tracks: a core-only track to ignore adversarial distractions (e.g., toxic suffixes or prefix injections), and a core-full-core (CFC) track to disrupt the structural patterns exploited by gradient-based or edit-based attacks. The final response is selected based on a safety consistency check across both tracks, ensuring robustness without compromising on response quality. We demonstrate that CCFC cuts attack success rates by 50-75% versus state-of-the-art defenses against strong adversaries (e.g., DeepInception, GCG), without sacrificing fidelity on benign queries. Our method consistently outperforms state-of-the-art prompt-level defenses, offering a practical and effective solution for safer LLM deployment.</li>
</ul>

<h3>Title: ERIS: An Energy-Guided Feature Disentanglement Framework for Out-of-Distribution Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Xin Wu, Fei Teng, Ji Zhang, Xingwang Li, Yuxuan Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14134">https://arxiv.org/abs/2508.14134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14134">https://arxiv.org/pdf/2508.14134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14134]] ERIS: An Energy-Guided Feature Disentanglement Framework for Out-of-Distribution Time Series Classification(https://arxiv.org/abs/2508.14134)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>An ideal time series classification (TSC) should be able to capture invariant representations, but achieving reliable performance on out-of-distribution (OOD) data remains a core obstacle. This obstacle arises from the way models inherently entangle domain-specific and label-relevant features, resulting in spurious correlations. While feature disentanglement aims to solve this, current methods are largely unguided, lacking the semantic direction required to isolate truly universal features. To address this, we propose an end-to-end Energy-Regularized Information for Shift-Robustness (\textbf{ERIS}) framework to enable guided and reliable feature disentanglement. The core idea is that effective disentanglement requires not only mathematical constraints but also semantic guidance to anchor the separation process. ERIS incorporates three key mechanisms to achieve this goal. Specifically, we first introduce an energy-guided calibration mechanism, which provides crucial semantic guidance for the separation, enabling the model to self-calibrate. Additionally, a weight-level orthogonality strategy enforces structural independence between domain-specific and label-relevant features, thereby mitigating their interference. Moreover, an auxiliary adversarial training mechanism enhances robustness by injecting structured perturbations. Experiments demonstrate that ERIS improves upon state-of-the-art baselines by an average of 4.04% accuracy across four benchmarks.</li>
</ul>

<h3>Title: Towards Agent-based Test Support Systems: An Unsupervised Environment Design Approach</h3>
<ul>
<li><strong>Authors: </strong>Collins O.Ogbodo, Timothy J. Rogers, Mattia Dal Borgo, David J. Wagg</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14135">https://arxiv.org/abs/2508.14135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14135">https://arxiv.org/pdf/2508.14135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14135]] Towards Agent-based Test Support Systems: An Unsupervised Environment Design Approach(https://arxiv.org/abs/2508.14135)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Modal testing plays a critical role in structural analysis by providing essential insights into dynamic behaviour across a wide range of engineering industries. In practice, designing an effective modal test campaign involves complex experimental planning, comprising a series of interdependent decisions that significantly influence the final test outcome. Traditional approaches to test design are typically static-focusing only on global tests without accounting for evolving test campaign parameters or the impact of such changes on previously established decisions, such as sensor configurations, which have been found to significantly influence test outcomes. These rigid methodologies often compromise test accuracy and adaptability. To address these limitations, this study introduces an agent-based decision support framework for adaptive sensor placement across dynamically changing modal test environments. The framework formulates the problem using an underspecified partially observable Markov decision process, enabling the training of a generalist reinforcement learning agent through a dual-curriculum learning strategy. A detailed case study on a steel cantilever structure demonstrates the efficacy of the proposed method in optimising sensor locations across frequency segments, validating its robustness and real-world applicability in experimental settings.</li>
</ul>

<h3>Title: Topological Data Analysis for Unsupervised Anomaly Detection and Customer Segmentation on Banking Data</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Aldo Alejandro Barberi, Linda Maria De Cave</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14136">https://arxiv.org/abs/2508.14136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14136">https://arxiv.org/pdf/2508.14136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14136]] Topological Data Analysis for Unsupervised Anomaly Detection and Customer Segmentation on Banking Data(https://arxiv.org/abs/2508.14136)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper introduces advanced techniques of Topological Data Analysis (TDA) for unsupervised anomaly detection and customer segmentation in banking data. Using the Mapper algorithm and persistent homology, we develop unsupervised procedures that uncover meaningful patterns in customers' banking data by exploiting topological information. The framework we present in this paper yields actionable insights that combine the abstract mathematical subject of topology with real-life use cases that are useful in industry.</li>
</ul>

<h3>Title: STAS: Spatio-Temporal Adaptive Computation Time for Spiking Transformers</h3>
<ul>
<li><strong>Authors: </strong>Donghwa Kang, Doohyun Kim, Sang-Ki Ko, Jinkyu Lee, Brent ByungHoon Kang, Hyeongboo Baek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14138">https://arxiv.org/abs/2508.14138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14138">https://arxiv.org/pdf/2508.14138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14138]] STAS: Spatio-Temporal Adaptive Computation Time for Spiking Transformers(https://arxiv.org/abs/2508.14138)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Spiking neural networks (SNNs) offer energy efficiency over artificial neural networks (ANNs) but suffer from high latency and computational overhead due to their multi-timestep operational nature. While various dynamic computation methods have been developed to mitigate this by targeting spatial, temporal, or architecture-specific redundancies, they remain fragmented. While the principles of adaptive computation time (ACT) offer a robust foundation for a unified approach, its application to SNN-based vision Transformers (ViTs) is hindered by two core issues: the violation of its temporal similarity prerequisite and a static architecture fundamentally unsuited for its principles. To address these challenges, we propose STAS (Spatio-Temporal Adaptive computation time for Spiking transformers), a framework that co-designs the static architecture and dynamic computation policy. STAS introduces an integrated spike patch splitting (I-SPS) module to establish temporal stability by creating a unified input representation, thereby solving the architectural problem of temporal dissimilarity. This stability, in turn, allows our adaptive spiking self-attention (A-SSA) module to perform two-dimensional token pruning across both spatial and temporal axes. Implemented on spiking Transformer architectures and validated on CIFAR-10, CIFAR-100, and ImageNet, STAS reduces energy consumption by up to 45.9%, 43.8%, and 30.1%, respectively, while simultaneously improving accuracy over SOTA models.</li>
</ul>

<h3>Title: Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs</h3>
<ul>
<li><strong>Authors: </strong>Orestis Konstantaropoulos, Stelios Manolis Smirnakis, Maria Papadopouli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14140">https://arxiv.org/abs/2508.14140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14140">https://arxiv.org/pdf/2508.14140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14140]] Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs(https://arxiv.org/abs/2508.14140)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The structure of biological neural circuits-modular, hierarchical, and sparsely interconnected-reflects an efficient trade-off between wiring cost, functional specialization, and robustness. These principles offer valuable insights for artificial neural network (ANN) design, especially as networks grow in depth and scale. Sparsity, in particular, has been widely explored for reducing memory and computation, improving speed, and enhancing generalization. Motivated by systems neuroscience findings, we explore how patterns of functional connectivity in the mouse visual cortex-specifically, ensemble-to-ensemble communication, can inform ANN design. We introduce G2GNet, a novel architecture that imposes sparse, modular connectivity across feedforward layers. Despite having significantly fewer parameters than fully connected models, G2GNet achieves superior accuracy on standard vision benchmarks. To our knowledge, this is the first architecture to incorporate biologically observed functional connectivity patterns as a structural bias in ANN design. We complement this static bias with a dynamic sparse training (DST) mechanism that prunes and regrows edges during training. We also propose a Hebbian-inspired rewiring rule based on activation correlations, drawing on principles of biological plasticity. G2GNet achieves up to 75% sparsity while improving accuracy by up to 4.3% on benchmarks, including Fashion-MNIST, CIFAR-10, and CIFAR-100, outperforming dense baselines with far fewer computations.</li>
</ul>

<h3>Title: MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation</h3>
<ul>
<li><strong>Authors: </strong>Xian Gao, Jiacheng Ruan, Zongyun Zhang, Jingsheng Gao, Ting Liu, Yuzhuo Fu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14146">https://arxiv.org/abs/2508.14146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14146">https://arxiv.org/pdf/2508.14146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14146]] MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation(https://arxiv.org/abs/2508.14146)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid growth of academic publications, peer review has become an essential yet time-consuming responsibility within the research community. Large Language Models (LLMs) have increasingly been adopted to assist in the generation of review comments; however, current LLM-based review tasks lack a unified evaluation benchmark to rigorously assess the models' ability to produce comprehensive, accurate, and human-aligned assessments, particularly in scenarios involving multimodal content such as figures and tables. To address this gap, we propose \textbf{MMReview}, a comprehensive benchmark that spans multiple disciplines and modalities. MMReview includes multimodal content and expert-written review comments for 240 papers across 17 research domains within four major academic disciplines: Artificial Intelligence, Natural Sciences, Engineering Sciences, and Social Sciences. We design a total of 13 tasks grouped into four core categories, aimed at evaluating the performance of LLMs and Multimodal LLMs (MLLMs) in step-wise review generation, outcome formulation, alignment with human preferences, and robustness to adversarial input manipulation. Extensive experiments conducted on 16 open-source models and 5 advanced closed-source models demonstrate the thoroughness of the benchmark. We envision MMReview as a critical step toward establishing a standardized foundation for the development of automated peer review systems.</li>
</ul>

<h3>Title: DPad: Efficient Diffusion Language Models with Suffix Dropout</h3>
<ul>
<li><strong>Authors: </strong>Xinhua Chen, Sitao Huang, Cong Guo, Chiyue Wei, Yintao He, Jianyi Zhang, Hai "Hellen" Li, Yiran Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14148">https://arxiv.org/abs/2508.14148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14148">https://arxiv.org/pdf/2508.14148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14148]] DPad: Efficient Diffusion Language Models with Suffix Dropout(https://arxiv.org/abs/2508.14148)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion-based Large Language Models (dLLMs) parallelize text generation by framing decoding as a denoising process, but suffer from high computational overhead since they predict all future suffix tokens at each step while retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a training-free method that restricts attention to a small set of nearby suffix tokens, preserving fidelity while eliminating redundancy. DPad integrates two strategies: (i) a sliding window, which maintains a fixed-length suffix window, and (ii) distance-decay dropout, which deterministically removes distant suffix tokens before attention computation. This simple design is compatible with existing optimizations such as prefix caching and can be implemented with only a few lines of code. Comprehensive evaluations across multiple benchmarks on LLaDA-1.5 and Dream models demonstrate that DPad delivers up to $\mathbf{61.4\times}$ speedup over vanilla dLLMs while maintaining comparable accuracy, highlighting its potential for efficient and scalable long-sequence inference. Our code is available at this https URL.</li>
</ul>

<h3>Title: LENS: Learning to Segment Anything with Unified Reinforced Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Lianghui Zhu, Bin Ouyang, Yuxuan Zhang, Tianheng Cheng, Rui Hu, Haocheng Shen, Longjin Ran, Xiaoxin Chen, Li Yu, Wenyu Liu, Xinggang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14153">https://arxiv.org/abs/2508.14153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14153">https://arxiv.org/pdf/2508.14153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14153]] LENS: Learning to Segment Anything with Unified Reinforced Reasoning(https://arxiv.org/abs/2508.14153)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Text-prompted image segmentation enables fine-grained visual understanding and is critical for applications such as human-computer interaction and robotics. However, existing supervised fine-tuning methods typically ignore explicit chain-of-thought (CoT) reasoning at test time, which limits their ability to generalize to unseen prompts and domains. To address this issue, we introduce LENS, a scalable reinforcement-learning framework that jointly optimizes the reasoning process and segmentation in an end-to-end manner. We propose unified reinforcement-learning rewards that span sentence-, box-, and segment-level cues, encouraging the model to generate informative CoT rationales while refining mask quality. Using a publicly available 3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to 5.6%. These results demonstrate that RL-driven CoT reasoning serves as a robust prior for text-prompted segmentation and offers a practical path toward more generalizable Segment Anything models. Code is available at this https URL.</li>
</ul>

<h3>Title: RynnEC: Bringing MLLMs into Embodied World</h3>
<ul>
<li><strong>Authors: </strong>Ronghao Dang, Yuqian Yuan, Yunxuan Mao, Kehan Li, Jiangpin Liu, Zhikai Wang, Xin Li, Fan Wang, Deli Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14160">https://arxiv.org/abs/2508.14160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14160">https://arxiv.org/pdf/2508.14160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14160]] RynnEC: Bringing MLLMs into Embodied World(https://arxiv.org/abs/2508.14160)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>We introduce RynnEC, a video multimodal large language model designed for embodied cognition. Built upon a general-purpose vision-language foundation model, RynnEC incorporates a region encoder and a mask decoder, enabling flexible region-level video interaction. Despite its compact architecture, RynnEC achieves state-of-the-art performance in object property understanding, object segmentation, and spatial reasoning. Conceptually, it offers a region-centric video paradigm for the brain of embodied agents, providing fine-grained perception of the physical world and enabling more precise interactions. To mitigate the scarcity of annotated 3D datasets, we propose an egocentric video based pipeline for generating embodied cognition data. Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for evaluating embodied cognitive capabilities. We anticipate that RynnEC will advance the development of general-purpose cognitive cores for embodied agents and facilitate generalization across diverse embodied tasks. The code, model checkpoints, and benchmark are available at: this https URL</li>
</ul>

<h3>Title: Comparing energy consumption and accuracy in text classification inference</h3>
<ul>
<li><strong>Authors: </strong>Johannes Zschache, Tilman Hartwig</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14170">https://arxiv.org/abs/2508.14170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14170">https://arxiv.org/pdf/2508.14170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14170]] Comparing energy consumption and accuracy in text classification inference(https://arxiv.org/abs/2508.14170)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The increasing deployment of large language models (LLMs) in natural language processing (NLP) tasks raises concerns about energy efficiency and sustainability. While prior research has largely focused on energy consumption during model training, the inference phase has received comparatively less attention. This study systematically evaluates the trade-offs between model accuracy and energy consumption in text classification inference across various model architectures and hardware configurations. Our empirical analysis shows that the best-performing model in terms of accuracy can also be energy-efficient, while larger LLMs tend to consume significantly more energy with lower classification accuracy. We observe substantial variability in inference energy consumption ($<$mWh to $>$kWh), influenced by model type, model size, and hardware specifications. Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible. These findings have implications for sustainable AI development, providing actionable insights for researchers, industry practitioners, and policymakers seeking to balance performance and resource efficiency in NLP applications.</li>
</ul>

<h3>Title: Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text</h3>
<ul>
<li><strong>Authors: </strong>Zixin Rao, Youssef Mohamed, Shang Liu, Zeyan Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14190">https://arxiv.org/abs/2508.14190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14190">https://arxiv.org/pdf/2508.14190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14190]] Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text(https://arxiv.org/abs/2508.14190)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), such as GPT-4 and Llama, have demonstrated remarkable abilities in generating natural language. However, they also pose security and integrity challenges. Existing countermeasures primarily focus on distinguishing AI-generated content from human-written text, with most solutions tailored for English. Meanwhile, authorship attribution--determining which specific LLM produced a given text--has received comparatively little attention despite its importance in forensic analysis. In this paper, we present DA-MTL, a multi-task learning framework that simultaneously addresses both text detection and authorship attribution. We evaluate DA-MTL on nine datasets and four backbone models, demonstrating its strong performance across multiple languages and LLM sources. Our framework captures each task's unique characteristics and shares insights between them, which boosts performance in both tasks. Additionally, we conduct a thorough analysis of cross-modal and cross-lingual patterns and assess the framework's robustness against adversarial obfuscation techniques. Our findings offer valuable insights into LLM behavior and the generalization of both detection and authorship attribution.</li>
</ul>

<h3>Title: Noise Robust One-Class Intrusion Detection on Dynamic Graphs</h3>
<ul>
<li><strong>Authors: </strong>Aleksei Liuliakov, Alexander Schulz, Luca Hermes, Barbara Hammer</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14192">https://arxiv.org/abs/2508.14192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14192">https://arxiv.org/pdf/2508.14192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14192]] Noise Robust One-Class Intrusion Detection on Dynamic Graphs(https://arxiv.org/abs/2508.14192)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the domain of network intrusion detection, robustness against contaminated and noisy data inputs remains a critical challenge. This study introduces a probabilistic version of the Temporal Graph Network Support Vector Data Description (TGN-SVDD) model, designed to enhance detection accuracy in the presence of input noise. By predicting parameters of a Gaussian distribution for each network event, our model is able to naturally address noisy adversarials and improve robustness compared to a baseline model. Our experiments on a modified CIC-IDS2017 data set with synthetic noise demonstrate significant improvements in detection performance compared to the baseline TGN-SVDD model, especially as noise levels increase.</li>
</ul>

<h3>Title: CLIPSym: Delving into Symmetry Detection with CLIP</h3>
<ul>
<li><strong>Authors: </strong>Tinghan Yang, Md Ashiqur Rahman, Raymond A. Yeh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14197">https://arxiv.org/abs/2508.14197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14197">https://arxiv.org/pdf/2508.14197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14197]] CLIPSym: Delving into Symmetry Detection with CLIP(https://arxiv.org/abs/2508.14197)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Symmetry is one of the most fundamental geometric cues in computer vision, and detecting it has been an ongoing challenge. With the recent advances in vision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP model can aid symmetry detection by leveraging the additional symmetry cues found in the natural image descriptions. We propose CLIPSym, which leverages CLIP's image and language encoders and a rotation-equivariant decoder based on a hybrid of Transformer and $G$-Convolution to detect rotation and reflection symmetries. To fully utilize CLIP's language encoder, we have developed a novel prompting technique called Semantic-Aware Prompt Grouping (SAPG), which aggregates a diverse set of frequent object-based prompts to better integrate the semantic cues for symmetry detection. Empirically, we show that CLIPSym outperforms the current state-of-the-art on three standard symmetry detection datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations verifying the benefits of CLIP's pre-training, the proposed equivariant decoder, and the SAPG technique. The code is available at this https URL.</li>
</ul>

<h3>Title: A Taxonomy and Methodology for Proof-of-Location Systems</h3>
<ul>
<li><strong>Authors: </strong>Eduardo Brito, Fernando Castillo, Liina Kamm, Amnir Hadachi, Ulrich Norbisrath</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14230">https://arxiv.org/abs/2508.14230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14230">https://arxiv.org/pdf/2508.14230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14230]] A Taxonomy and Methodology for Proof-of-Location Systems(https://arxiv.org/abs/2508.14230)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack</a></li>
<li><strong>Abstract: </strong>Digital societies increasingly rely on trustworthy proofs of physical presence for services such as supply-chain tracking, e-voting, ride-sharing, and location-based rewards. Yet, traditional localization methods often lack cryptographic guarantees of where and when an entity was present, leaving them vulnerable to spoofing, replay, or collusion attacks. In response, research on Proof-of-Location (PoL) has emerged, with recent approaches combining distance bounding, distributed consensus, and privacy-enhancing techniques to enable verifiable, tamper-resistant location claims. As the design space for PoL systems grows in complexity, this paper provides a unified framework to help practitioners navigate diverse application needs. We first propose a taxonomy identifying four core domains: (1) cryptographic guarantees, (2) spatio-temporal synchronization, (3) trust and witness models, and (4) interaction and overhead. Building on this, we introduce a methodology to map application-specific requirements onto appropriate PoL architectures. We illustrate this process through three use cases (retail e-coupons, supply chain auditing, and physical e-voting), each showing how different constraints shape protocol choices. Overall, this work offers a structured approach to building secure, scalable, and interoperable PoL systems.</li>
</ul>

<h3>Title: Graph Concept Bottleneck Models</h3>
<ul>
<li><strong>Authors: </strong>Haotian Xu, Tsui-Wei Weng, Lam M. Nguyen, Tengfei Ma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14255">https://arxiv.org/abs/2508.14255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14255">https://arxiv.org/pdf/2508.14255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14255]] Graph Concept Bottleneck Models(https://arxiv.org/abs/2508.14255)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Concept Bottleneck Models (CBMs) provide explicit interpretations for deep neural networks through concepts and allow intervention with concepts to adjust final predictions. Existing CBMs assume concepts are conditionally independent given labels and isolated from each other, ignoring the hidden relationships among concepts. However, the set of concepts in CBMs often has an intrinsic structure where concepts are generally correlated: changing one concept will inherently impact its related concepts. To mitigate this limitation, we propose GraphCBMs: a new variant of CBM that facilitates concept relationships by constructing latent concept graphs, which can be combined with CBMs to enhance model performance while retaining their interpretability. Our experiment results on real-world image classification tasks demonstrate Graph CBMs offer the following benefits: (1) superior in image classification tasks while providing more concept structure information for interpretability; (2) able to utilize latent concept graphs for more effective interventions; and (3) robust in performance across different training and architecture settings.</li>
</ul>

<h3>Title: SaMOSA: Sandbox for Malware Orchestration and Side-Channel Analysis</h3>
<ul>
<li><strong>Authors: </strong>Meet Udeshi, Venkata Sai Charan Putrevu, Prashanth Krishnamurthy, Ramesh Karri, Farshad Khorrami</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14261">https://arxiv.org/abs/2508.14261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14261">https://arxiv.org/pdf/2508.14261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14261]] SaMOSA: Sandbox for Malware Orchestration and Side-Channel Analysis(https://arxiv.org/abs/2508.14261)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Cyber-attacks on operational technology (OT) and cyber-physical systems (CPS) have increased tremendously in recent years with the proliferation of malware targeting Linux-based embedded devices of OT and CPS systems. Comprehensive malware detection requires dynamic analysis of execution behavior in addition to static analysis of binaries. Safe execution of malware in a manner that captures relevant behaviors via side-channels requires a sandbox environment. Existing Linux sandboxes are built for specific tasks, only capture one or two side-channels, and do not offer customization for different analysis tasks. We present the SaMOSA Linux sandbox that allows emulation of Linux malwares while capturing time-synchronized side-channels from four sources. SaMOSA additionally provides emulation of network services via FakeNet, and allows orchestration and customization of the sandbox environment via pipeline hooks. In comparison to existing Linux sandboxes, SaMOSA captures more side-channels namely system calls, network activity, disk activity, and hardware performance counters. It supports three architectures predominantly used in OT and CPS namely x86-64, ARM64, and PowerPC 64. SaMOSA fills a gap in Linux malware analysis by providing a modular and customizable sandbox framework that can be adapted for many malware analysis tasks. We present three case studies of three different malware families to demonstrate the advantages of SaMOSA.</li>
</ul>

<h3>Title: Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Thanh-Dat Truong, Huu-Thien Tran, Tran Thai Son, Bhiksha Raj, Khoa Luu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14264">https://arxiv.org/abs/2508.14264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14264">https://arxiv.org/pdf/2508.14264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14264]] Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models(https://arxiv.org/abs/2508.14264)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large multimodal models (LMMs) have gained impressive performance due to their outstanding capability in various understanding tasks. However, these models still suffer from some fundamental limitations related to robustness and generalization due to the alignment and correlation between visual and textual features. In this paper, we introduce a simple but efficient learning mechanism for improving the robust alignment between visual and textual modalities by solving shuffling problems. In particular, the proposed approach can improve reasoning capability, visual understanding, and cross-modality alignment by introducing two new tasks: reconstructing the image order and the text order into the LMM's pre-training and fine-tuning phases. In addition, we propose a new directed-token approach to capture visual and textual knowledge, enabling the capability to reconstruct the correct order of visual inputs. Then, we introduce a new Image-to-Response Guided loss to further improve the visual understanding of the LMM in its responses. The proposed approach consistently achieves state-of-the-art (SoTA) performance compared with prior LMMs on academic task-oriented and instruction-following LMM benchmarks.</li>
</ul>

<h3>Title: Effect of Data Augmentation on Conformal Prediction for Diabetic Retinopathy</h3>
<ul>
<li><strong>Authors: </strong>Rizwan Ahamed, Annahita Amireskandari, Joel Palko, Carol Laxson, Binod Bhattarai, Prashnna Gyawali</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14266">https://arxiv.org/abs/2508.14266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14266">https://arxiv.org/pdf/2508.14266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14266]] Effect of Data Augmentation on Conformal Prediction for Diabetic Retinopathy(https://arxiv.org/abs/2508.14266)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>The clinical deployment of deep learning models for high-stakes tasks such as diabetic retinopathy (DR) grading requires demonstrable reliability. While models achieve high accuracy, their clinical utility is limited by a lack of robust uncertainty quantification. Conformal prediction (CP) offers a distribution-free framework to generate prediction sets with statistical guarantees of coverage. However, the interaction between standard training practices like data augmentation and the validity of these guarantees is not well understood. In this study, we systematically investigate how different data augmentation strategies affect the performance of conformal predictors for DR grading. Using the DDR dataset, we evaluate two backbone architectures -- ResNet-50 and a Co-Scale Conv-Attentional Transformer (CoaT) -- trained under five augmentation regimes: no augmentation, standard geometric transforms, CLAHE, Mixup, and CutMix. We analyze the downstream effects on conformal metrics, including empirical coverage, average prediction set size, and correct efficiency. Our results demonstrate that sample-mixing strategies like Mixup and CutMix not only improve predictive accuracy but also yield more reliable and efficient uncertainty estimates. Conversely, methods like CLAHE can negatively impact model certainty. These findings highlight the need to co-design augmentation strategies with downstream uncertainty quantification in mind to build genuinely trustworthy AI systems for medical imaging.</li>
</ul>

<h3>Title: Tooth-Diffusion: Guided 3D CBCT Synthesis with Fine-Grained Tooth Conditioning</h3>
<ul>
<li><strong>Authors: </strong>Said Djafar Said, Torkan Gholamalizadeh, Mostafa Mehdipour Ghazi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14276">https://arxiv.org/abs/2508.14276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14276">https://arxiv.org/pdf/2508.14276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14276]] Tooth-Diffusion: Guided 3D CBCT Synthesis with Fine-Grained Tooth Conditioning(https://arxiv.org/abs/2508.14276)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Despite the growing importance of dental CBCT scans for diagnosis and treatment planning, generating anatomically realistic scans with fine-grained control remains a challenge in medical image synthesis. In this work, we propose a novel conditional diffusion framework for 3D dental volume generation, guided by tooth-level binary attributes that allow precise control over tooth presence and configuration. Our approach integrates wavelet-based denoising diffusion, FiLM conditioning, and masked loss functions to focus learning on relevant anatomical structures. We evaluate the model across diverse tasks, such as tooth addition, removal, and full dentition synthesis, using both paired and distributional similarity metrics. Results show strong fidelity and generalization with low FID scores, robust inpainting performance, and SSIM values above 0.91 even on unseen scans. By enabling realistic, localized modification of dentition without rescanning, this work opens opportunities for surgical planning, patient communication, and targeted data augmentation in dental AI workflows. The codes are available at: this https URL.</li>
</ul>

<h3>Title: GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs</h3>
<ul>
<li><strong>Authors: </strong>Adrian-Marius Dumitran, Alexandra-Mihaela Danila, Angela-Liliana Dumitran</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14279">https://arxiv.org/abs/2508.14279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14279">https://arxiv.org/pdf/2508.14279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14279]] GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs(https://arxiv.org/abs/2508.14279)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>LLMs (Large language models) have revolutionized NLP (Natural Language Processing), yet their pedagogical value for low-resource languages remains unclear. We present GRILE (Grammar Romanian Inference and Language Explanations) , the first open benchmark of 1,151 multiple-choice questions harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate, university admissions). GRILE enables us to probe two complementary abilities of seven state-of-the-art multilingual and Romanian-specific LLMs: (i) selecting the correct answer, and (ii) producing linguistically accurate explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight models stay below 65%, and 48% of their explanations contain factual or pedagogical flaws according to expert review. A detailed error analysis pinpoints systematic weaknesses in morphology and in applying the latest DOOM3 orthographic norms. All data, code and a public web demo are released to catalyze future research. Our findings expose open challenges for trustworthy educational NLP in low-resource settings and establish GRILE as a new test-bed for controllable explanation generation and evaluation.</li>
</ul>

<h3>Title: Differentially Private aggregate hints in mev-share</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Passerat-Palmbach, Sarisht Wadhwa</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14284">https://arxiv.org/abs/2508.14284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14284">https://arxiv.org/pdf/2508.14284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14284]] Differentially Private aggregate hints in mev-share(https://arxiv.org/abs/2508.14284)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, extraction</a></li>
<li><strong>Abstract: </strong>Flashbots recently released mev-share to empower users with control over the amount of information they share with searchers for extracting Maximal Extractable Value (MEV). Searchers require more information to maintain on-chain exchange efficiency and profitability, while users aim to prevent frontrunning by withholding information. After analyzing two searching strategies in mev-share to reason about searching techniques, this paper introduces Differentially-Private (DP) aggregate hints as a new type of hints to disclose information quantitatively. DP aggregate hints enable users to formally quantify their privacy loss to searchers, and thus better estimate the level of rebates to ask in return. The paper discusses the current properties and privacy loss in mev-share and lays out how DP aggregate hints could enhance the system for both users and searchers. We leverage Differential Privacy in the Trusted Curator Model to design our aggregate hints. Additionally, we explain how random sampling can defend against sybil attacks and amplify overall user privacy while providing valuable hints to searchers for improved backrunning extraction and frontrunning prevention.</li>
</ul>

<h3>Title: Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Liyi Zhang, Jake Snell, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14285">https://arxiv.org/abs/2508.14285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14285">https://arxiv.org/pdf/2508.14285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14285]] Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models(https://arxiv.org/abs/2508.14285)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) with low-rank adaptaion (LoRA) is a cost-effective way to incorporate information from a specific dataset. However, it is often unclear how well the fine-tuned LLM will generalize, i.e., how well it will perform on unseen datasets. Methods have been proposed to improve generalization by optimizing with in-context prompts, or by using meta-learning to fine-tune LLMs. However, these methods are expensive in memory and computation, requiring either long-context prompts or saving copies of parameters and using second-order gradient updates. To address these challenges, we propose Amortized Bayesian Meta-Learning for LoRA (ABMLL). This method builds on amortized Bayesian meta-learning for smaller models, adapting this approach to LLMs while maintaining its computational efficiency. We reframe task-specific and global parameters in the context of LoRA and use a set of new hyperparameters to balance reconstruction accuracy and the fidelity of task-specific parameters to the global ones. ABMLL provides effective generalization and scales to large models such as Llama3-8B. Furthermore, as a result of using a Bayesian framework, ABMLL provides improved uncertainty quantification. We test ABMLL on Unified-QA and CrossFit datasets and find that it outperforms existing methods on these benchmarks in terms of both accuracy and expected calibration error.</li>
</ul>

<h3>Title: OccluNet: Spatio-Temporal Deep Learning for Occlusion Detection on DSA</h3>
<ul>
<li><strong>Authors: </strong>Anushka A. Kore, Frank G. te Nijenhuis, Matthijs van der Sluijs, Wim van Zwam, Charles Majoie, Geert Lycklama à Nijeholt, Danny Ruijters, Frans Vos, Sandra Cornelissen, Ruisheng Su, Theo van Walsum</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14286">https://arxiv.org/abs/2508.14286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14286">https://arxiv.org/pdf/2508.14286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14286]] OccluNet: Spatio-Temporal Deep Learning for Occlusion Detection on DSA(https://arxiv.org/abs/2508.14286)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate detection of vascular occlusions during endovascular thrombectomy (EVT) is critical in acute ischemic stroke (AIS). Interpretation of digital subtraction angiography (DSA) sequences poses challenges due to anatomical complexity and time constraints. This work proposes OccluNet, a spatio-temporal deep learning model that integrates YOLOX, a single-stage object detector, with transformer-based temporal attention mechanisms to automate occlusion detection in DSA sequences. We compared OccluNet with a YOLOv11 baseline trained on either individual DSA frames or minimum intensity projections. Two spatio-temporal variants were explored for OccluNet: pure temporal attention and divided space-time attention. Evaluation on DSA images from the MR CLEAN Registry revealed the model's capability to capture temporally consistent features, achieving precision and recall of 89.02% and 74.87%, respectively. OccluNet significantly outperformed the baseline models, and both attention variants attained similar performance. Source code is available at this https URL</li>
</ul>

<h3>Title: Tokens with Meaning: A Hybrid Tokenization Approach for NLP</h3>
<ul>
<li><strong>Authors: </strong>M. Ali Bayram, Ali Arda Fincan, Ahmet Semih Gümüş, Sercan Karakaş, Banu Diri, Savaş Yıldırım, Demircan Çelik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14292">https://arxiv.org/abs/2508.14292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14292">https://arxiv.org/pdf/2508.14292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14292]] Tokens with Meaning: A Hybrid Tokenization Approach for NLP(https://arxiv.org/abs/2508.14292)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Tokenization plays a pivotal role in natural language processing (NLP), shaping how text is segmented and interpreted by language models. While subword methods such as Byte Pair Encoding (BPE) and WordPiece have been effective, they often struggle with morphologically rich and agglutinative languages because they rely on frequency rather than linguistic structure. We introduce a hybrid tokenization framework that combines rule-based morphological analysis with statistical subword segmentation. The method uses phonological normalization, root-affix dictionaries, and a novel algorithm that balances morpheme preservation with vocabulary efficiency. It assigns shared identifiers to phonologically variant affixes (e.g., -ler and -lar) and altered root forms (e.g., kitap vs. kitabı), reducing redundancy while maintaining semantic integrity. Special tokens are added for whitespace and case, including an UPPERCASE marker to avoid vocabulary inflation from capitalization. BPE is integrated for out-of-vocabulary coverage without harming morphological coherence. On the TR-MMLU benchmark, the tokenizer achieves the highest Turkish Token Percentage (90.29\%) and Pure Token Percentage (85.8\%). Comparisons with tokenizers from LLaMA, Gemma, and GPT show more linguistically meaningful and coherent tokens. Although demonstrated on Turkish, the approach is language-independent and adaptable to other languages, offering a practical path toward more interpretable and effective multilingual NLP systems.</li>
</ul>

<h3>Title: Pixels to Play: A Foundation Model for 3D Gameplay</h3>
<ul>
<li><strong>Authors: </strong>Yuguang Yue, Chris Green, Samuel Hunt, Irakli Salia, Wenzhe Shi, Jonathan J Hunt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14295">https://arxiv.org/abs/2508.14295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14295">https://arxiv.org/pdf/2508.14295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14295]] Pixels to Play: A Foundation Model for 3D Gameplay(https://arxiv.org/abs/2508.14295)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce Pixels2Play-0.1 (P2P0.1), a foundation model that learns to play a wide range of 3D video games with recognizable human-like behavior. Motivated by emerging consumer and developer use cases - AI teammates, controllable NPCs, personalized live-streamers, assistive testers - we argue that an agent must rely on the same pixel stream available to players and generalize to new titles with minimal game-specific engineering. P2P0.1 is trained end-to-end with behavior cloning: labeled demonstrations collected from instrumented human game-play are complemented by unlabeled public videos, to which we impute actions via an inverse-dynamics model. A decoder-only transformer with auto-regressive action output handles the large action space while remaining latency-friendly on a single consumer GPU. We report qualitative results showing competent play across simple Roblox and classic MS-DOS titles, ablations on unlabeled data, and outline the scaling and evaluation steps required to reach expert-level, text-conditioned control.</li>
</ul>

<h3>Title: MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing</h3>
<ul>
<li><strong>Authors: </strong>Youssef Maklad, Fares Wael, Ali Hamdi, Wael Elsersy, Khaled Shaban</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.MA, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14300">https://arxiv.org/abs/2508.14300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14300">https://arxiv.org/pdf/2508.14300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14300]] MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing(https://arxiv.org/abs/2508.14300)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Traditional protocol fuzzing techniques, such as those employed by AFL-based systems, often lack effectiveness due to a limited semantic understanding of complex protocol grammars and rigid seed mutation strategies. Recent works, such as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol fuzzing and address these limitations, pushing protocol fuzzers to wider exploration of the protocol state space. But ChatAFL still faces issues like unreliable output, LLM hallucinations, and assumptions of LLM knowledge about protocol specifications. This paper introduces MultiFuzz, a novel dense retrieval-based multi-agent system designed to overcome these limitations by integrating semantic-aware context retrieval, specialized agents, and structured tool-assisted reasoning. MultiFuzz utilizes agentic chunks of protocol documentation (RFC Documents) to build embeddings in a vector database for a retrieval-augmented generation (RAG) pipeline, enabling agents to generate more reliable and structured outputs, enhancing the fuzzer in mutating protocol messages with enhanced state coverage and adherence to syntactic constraints. The framework decomposes the fuzzing process into modular groups of agents that collaborate through chain-of-thought reasoning to dynamically adapt fuzzing strategies based on the retrieved contextual knowledge. Experimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate that MultiFuzz significantly improves branch coverage and explores deeper protocol states and transitions over state-of-the-art (SOTA) fuzzers such as NSFuzz, AFLNet, and ChatAFL. By combining dense retrieval, agentic coordination, and language model reasoning, MultiFuzz establishes a new paradigm in autonomous protocol fuzzing, offering a scalable and extensible foundation for future research in intelligent agentic-based fuzzing systems.</li>
</ul>

<h3>Title: GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Amirmohsen Sattarifard, Sepehr Lavasani, Ehsan Imani, Kunlin Zhang, Hanlin Xu, Fengyu Sun, Negar Hassanpour, Chao Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14302">https://arxiv.org/abs/2508.14302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14302">https://arxiv.org/pdf/2508.14302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14302]] GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation(https://arxiv.org/abs/2508.14302)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Deploying Large Language Models (LLMs) on edge hardware demands aggressive, prompt-aware dynamic pruning to reduce computation without degrading quality. Static or predictor-based schemes either lock in a single sparsity pattern or incur extra runtime overhead, and recent zero-shot methods that rely on statistics from a single prompt fail on short prompt and/or long generation scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, two training-free methods that dynamically select FFN units using a rank-aggregation of prompt local and model-intrinsic global neuron statistics. Empirical results across multiple LLMs and benchmarks demonstrate that GLASS significantly outperforms prior training-free methods, particularly in challenging long-form generation scenarios, without relying on auxiliary predictors or adding any inference overhead.</li>
</ul>

<h3>Title: Learning Time-Varying Convexifications of Multiple Fairness Measures</h3>
<ul>
<li><strong>Authors: </strong>Quan Zhou, Jakub Marecek, Robert Shorten</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14311">https://arxiv.org/abs/2508.14311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14311">https://arxiv.org/pdf/2508.14311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14311]] Learning Time-Varying Convexifications of Multiple Fairness Measures(https://arxiv.org/abs/2508.14311)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>There is an increasing appreciation that one may need to consider multiple measures of fairness, e.g., considering multiple group and individual fairness notions. The relative weights of the fairness regularisers are a priori unknown, may be time varying, and need to be learned on the fly. We consider the learning of time-varying convexifications of multiple fairness measures with limited graph-structured feedback.</li>
</ul>

<h3>Title: Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS</h3>
<ul>
<li><strong>Authors: </strong>Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, Dimitris N. Metaxas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14313">https://arxiv.org/abs/2508.14313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14313">https://arxiv.org/pdf/2508.14313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14313]] Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS(https://arxiv.org/abs/2508.14313)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or LLM-generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in LLMs.</li>
</ul>

<h3>Title: Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency</h3>
<ul>
<li><strong>Authors: </strong>Aman Goel, Daniel Schwartz, Yanjun Qi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14314">https://arxiv.org/abs/2508.14314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14314">https://arxiv.org/pdf/2508.14314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14314]] Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency(https://arxiv.org/abs/2508.14314)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, but they remain susceptible to hallucinations--generating content that appears plausible but contains factual inaccuracies. We present Finch-Zk, a black-box framework that leverages FINe-grained Cross-model consistency to detect and mitigate Hallucinations in LLM outputs without requiring external knowledge sources. Finch-Zk introduces two key innovations: 1) a cross-model consistency checking strategy that reveals fine-grained inaccuracies by comparing responses generated by diverse models from semantically-equivalent prompts, and 2) a targeted mitigation technique that applies precise corrections to problematic segments while preserving accurate content. Experiments on the FELM dataset show Finch-Zk improves hallucination detection F1 scores by 6-39\% compared to existing approaches. For mitigation, Finch-Zk achieves 7-8 absolute percentage points improvement in answer accuracy on the GPQA-diamond dataset when applied to state-of-the-art models like Llama 4 Maverick and Claude 4 Sonnet. Extensive evaluation across multiple models demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for enhancing factual reliability in production LLM systems.</li>
</ul>

<h3>Title: FedRAIN-Lite: Federated Reinforcement Algorithms for Improving Idealised Numerical Weather and Climate Models</h3>
<ul>
<li><strong>Authors: </strong>Pritthijit Nath, Sebastian Schemm, Henry Moss, Peter Haynes, Emily Shuckburgh, Mark Webb</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14315">https://arxiv.org/abs/2508.14315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14315">https://arxiv.org/pdf/2508.14315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14315]] FedRAIN-Lite: Federated Reinforcement Algorithms for Improving Idealised Numerical Weather and Climate Models(https://arxiv.org/abs/2508.14315)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Sub-grid parameterisations in climate models are traditionally static and tuned offline, limiting adaptability to evolving states. This work introduces FedRAIN-Lite, a federated reinforcement learning (FedRL) framework that mirrors the spatial decomposition used in general circulation models (GCMs) by assigning agents to latitude bands, enabling local parameter learning with periodic global aggregation. Using a hierarchy of simplified energy-balance climate models, from a single-agent baseline (ebm-v1) to multi-agent ensemble (ebm-v2) and GCM-like (ebm-v3) setups, we benchmark three RL algorithms under different FedRL configurations. Results show that Deep Deterministic Policy Gradient (DDPG) consistently outperforms both static and single-agent baselines, with faster convergence and lower area-weighted RMSE in tropical and mid-latitude zones across both ebm-v2 and ebm-v3 setups. DDPG's ability to transfer across hyperparameters and low computational cost make it well-suited for geographically adaptive parameter learning. This capability offers a scalable pathway towards high-complexity GCMs and provides a prototype for physically aligned, online-learning climate models that can evolve with a changing climate. Code accessible at this https URL.</li>
</ul>

<h3>Title: SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing</h3>
<ul>
<li><strong>Authors: </strong>Jing Chen, Zhiheng Yang, Yixian Shen, Jie Liu, Adam Belloum, Chrysa Papagainni, Paola Grosso</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14317">https://arxiv.org/abs/2508.14317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14317">https://arxiv.org/pdf/2508.14317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14317]] SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing(https://arxiv.org/abs/2508.14317)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Survey papers play a critical role in scientific communication by consolidating progress across a field. Recent advances in Large Language Models (LLMs) offer a promising solution by automating key steps in the survey-generation pipeline, such as retrieval, structuring, and summarization. However, existing LLM-based approaches often struggle with maintaining coherence across long, multi-section surveys and providing comprehensive citation coverage. To address these limitations, we introduce SurveyGen-I, an automatic survey generation framework that combines coarse-to-fine retrieval, adaptive planning, and memory-guided generation. SurveyGen-I first performs survey-level retrieval to construct the initial outline and writing plan, and then dynamically refines both during generation through a memory mechanism that stores previously written content and terminology, ensuring coherence across subsections. When the system detects insufficient context, it triggers fine-grained subsection-level retrieval. During generation, SurveyGen-I leverages this memory mechanism to maintain coherence across subsections. Experiments across four scientific domains demonstrate that SurveyGen-I consistently outperforms previous works in content quality, consistency, and citation coverage.</li>
</ul>

<h3>Title: Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever</h3>
<ul>
<li><strong>Authors: </strong>Yixin Chen, Ying Xiong, Shangyu Wu, Yufei Cui, Xue Liu, Nan Guan, Chun Jason Xue</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14323">https://arxiv.org/abs/2508.14323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14323">https://arxiv.org/pdf/2508.14323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14323]] Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever(https://arxiv.org/abs/2508.14323)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Tool-augmented large language models (LLMs) leverage external functions to extend their capabilities, but inaccurate function calls can lead to inefficiencies and increased this http URL methods address this challenge by fine-tuning LLMs or using demonstration-based prompting, yet they often suffer from high training overhead and fail to account for inconsistent demonstration samples, which misguide the model's invocation behavior. In this paper, we trained a behavior-aligned retriever (BAR), which provides behaviorally consistent demonstrations to help LLMs make more accurate tool-using decisions. To train the BAR, we construct a corpus including different function-calling behaviors, i.e., calling or this http URL use the contrastive learning framework to train the BAR with customized positive/negative pairs and a dual-negative contrastive loss, ensuring robust retrieval of behaviorally consistent this http URL demonstrate that our approach significantly reduces erroneous function calls while maintaining high task performance, offering a cost-effective and efficient solution for tool-augmented LLMs.</li>
</ul>

<h3>Title: MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Guile Wu, David Huang, Dongfeng Bai, Bingbing Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14327">https://arxiv.org/abs/2508.14327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14327">https://arxiv.org/pdf/2508.14327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14327]] MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation(https://arxiv.org/abs/2508.14327)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Video generation has recently shown superiority in urban scene synthesis for autonomous driving. Existing video generation approaches to autonomous driving primarily focus on RGB video generation and lack the ability to support multi-modal video generation. However, multi-modal data, such as depth maps and semantic maps, are crucial for holistic urban scene understanding in autonomous driving. Although it is feasible to use multiple models to generate different modalities, this increases the difficulty of model deployment and does not leverage complementary cues for multi-modal data generation. To address this problem, in this work, we propose a novel multi-modal multi-view video generation approach to autonomous driving. Specifically, we construct a unified diffusion transformer model composed of modal-shared components and modal-specific components. Then, we leverage diverse conditioning inputs to encode controllable scene structure and content cues into the unified diffusion model for multi-modal multi-view video generation. In this way, our approach is capable of generating multi-modal multi-view driving scene videos in a unified framework. Our experiments on the challenging real-world autonomous driving dataset, nuScenes, show that our approach can generate multi-modal multi-view urban scene videos with high fidelity and controllability, surpassing the state-of-the-art methods.</li>
</ul>

<h3>Title: Multi-view Graph Condensation via Tensor Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Nícolas Roque dos Santos, Dawon Ahn, Diego Minatel, Alneu de Andrade Lopes, Evangelos E. Papalexakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14330">https://arxiv.org/abs/2508.14330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14330">https://arxiv.org/pdf/2508.14330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14330]] Multi-view Graph Condensation via Tensor Decomposition(https://arxiv.org/abs/2508.14330)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have demonstrated remarkable results in various real-world applications, including drug discovery, object detection, social media analysis, recommender systems, and text classification. In contrast to their vast potential, training them on large-scale graphs presents significant computational challenges due to the resources required for their storage and processing. Graph Condensation has emerged as a promising solution to reduce these demands by learning a synthetic compact graph that preserves the essential information of the original one while maintaining the GNN's predictive performance. Despite their efficacy, current graph condensation approaches frequently rely on a computationally intensive bi-level optimization. Moreover, they fail to maintain a mapping between synthetic and original nodes, limiting the interpretability of the model's decisions. In this sense, a wide range of decomposition techniques have been applied to learn linear or multi-linear functions from graph data, offering a more transparent and less resource-intensive alternative. However, their applicability to graph condensation remains unexplored. This paper addresses this gap and proposes a novel method called Multi-view Graph Condensation via Tensor Decomposition (GCTD) to investigate to what extent such techniques can synthesize an informative smaller graph and achieve comparable downstream task performance. Extensive experiments on six real-world datasets demonstrate that GCTD effectively reduces graph size while preserving GNN performance, achieving up to a 4.0\ improvement in accuracy on three out of six datasets and competitive performance on large graphs compared to existing approaches. Our code is available at this https URL.</li>
</ul>

<h3>Title: NeRC: Neural Ranging Correction through Differentiable Moving Horizon Location Estimation</h3>
<ul>
<li><strong>Authors: </strong>Xu Weng, K.V. Ling, Haochen Liu, Bingheng Wang, Kun Cao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14336">https://arxiv.org/abs/2508.14336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14336">https://arxiv.org/pdf/2508.14336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14336]] NeRC: Neural Ranging Correction through Differentiable Moving Horizon Location Estimation(https://arxiv.org/abs/2508.14336)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>GNSS localization using everyday mobile devices is challenging in urban environments, as ranging errors caused by the complex propagation of satellite signals and low-quality onboard GNSS hardware are blamed for undermining positioning accuracy. Researchers have pinned their hopes on data-driven methods to regress such ranging errors from raw measurements. However, the grueling annotation of ranging errors impedes their pace. This paper presents a robust end-to-end Neural Ranging Correction (NeRC) framework, where localization-related metrics serve as the task objective for training the neural modules. Instead of seeking impractical ranging error labels, we train the neural network using ground-truth locations that are relatively easy to obtain. This functionality is supported by differentiable moving horizon location estimation (MHE) that handles a horizon of measurements for positioning and backpropagates the gradients for training. Even better, as a blessing of end-to-end learning, we propose a new training paradigm using Euclidean Distance Field (EDF) cost maps, which alleviates the demands on labeled locations. We evaluate the proposed NeRC on public benchmarks and our collected datasets, demonstrating its distinguished improvement in positioning accuracy. We also deploy NeRC on the edge to verify its real-time performance for mobile devices.</li>
</ul>

<h3>Title: A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations</h3>
<ul>
<li><strong>Authors: </strong>Konur Tholl, Mariam El Mezouar, Ranwa Al Mallah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14340">https://arxiv.org/abs/2508.14340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14340">https://arxiv.org/pdf/2508.14340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14340]] A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations(https://arxiv.org/abs/2508.14340)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Autonomous Cyber Operations (ACO) rely on Reinforcement Learning (RL) to train agents to make effective decisions in the cybersecurity domain. However, existing ACO applications require agents to learn from scratch, leading to slow convergence and poor early-stage performance. While teacher-guided techniques have demonstrated promise in other domains, they have not yet been applied to ACO. In this study, we implement four distinct teacher-guided techniques in the simulated CybORG environment and conduct a comparative evaluation. Our results demonstrate that teacher integration can significantly improve training efficiency in terms of early policy performance and convergence speed, highlighting its potential benefits for autonomous cybersecurity.</li>
</ul>

<h3>Title: Generative AI Against Poaching: Latent Composite Flow Matching for Wildlife Conservation</h3>
<ul>
<li><strong>Authors: </strong>Lingkai Kong, Haichuan Wang, Charles A. Emogor, Vincent Börsch-Supan, Lily Xu, Milind Tambe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14342">https://arxiv.org/abs/2508.14342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14342">https://arxiv.org/pdf/2508.14342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14342]] Generative AI Against Poaching: Latent Composite Flow Matching for Wildlife Conservation(https://arxiv.org/abs/2508.14342)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Poaching poses significant threats to wildlife and biodiversity. A valuable step in reducing poaching is to forecast poacher behavior, which can inform patrol planning and other conservation interventions. Existing poaching prediction methods based on linear models or decision trees lack the expressivity to capture complex, nonlinear spatiotemporal patterns. Recent advances in generative modeling, particularly flow matching, offer a more flexible alternative. However, training such models on real-world poaching data faces two central obstacles: imperfect detection of poaching events and limited data. To address imperfect detection, we integrate flow matching with an occupancy-based detection model and train the flow in latent space to infer the underlying occupancy state. To mitigate data scarcity, we adopt a composite flow initialized from a linear-model prediction rather than random noise which is the standard in diffusion models, injecting prior knowledge and improving generalization. Evaluations on datasets from two national parks in Uganda show consistent gains in predictive accuracy.</li>
</ul>

<h3>Title: ISCA: A Framework for Interview-Style Conversational Agents</h3>
<ul>
<li><strong>Authors: </strong>Charles Welch, Allison Lahnala, Vasudha Varadarajan, Lucie Flek, Rada Mihalcea, J. Lomax Boyd, João Sedoc</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14344">https://arxiv.org/abs/2508.14344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14344">https://arxiv.org/pdf/2508.14344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14344]] ISCA: A Framework for Interview-Style Conversational Agents(https://arxiv.org/abs/2508.14344)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a low-compute non-generative system for implementing interview-style conversational agents which can be used to facilitate qualitative data collection through controlled interactions and quantitative analysis. Use cases include applications to tracking attitude formation or behavior change, where control or standardization over the conversational flow is desired. We show how our system can be easily adjusted through an online administrative panel to create new interviews, making the tool accessible without coding. Two case studies are presented as example applications, one regarding the Expressive Interviewing system for COVID-19 and the other a semi-structured interview to survey public opinion on emerging neurotechnology. Our code is open-source, allowing others to build off of our work and develop extensions for additional functionality.</li>
</ul>

<h3>Title: HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Gaston Gustavo Rios</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14345">https://arxiv.org/abs/2508.14345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14345">https://arxiv.org/pdf/2508.14345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14345]] HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation(https://arxiv.org/abs/2508.14345)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Sign Language Recognition (SLR) models face significant performance limitations due to insufficient training data availability. In this article, we address the challenge of limited data in SLR by introducing a novel and lightweight sign generation model based on CMLPe. This model, coupled with a synthetic data pretraining approach, consistently improves recognition accuracy, establishing new state-of-the-art results for the LSFB and DiSPLaY datasets using our Mamba-SL and Transformer-SL classifiers. Our findings reveal that synthetic data pretraining outperforms traditional augmentation methods in some cases and yields complementary benefits when implemented alongside them. Our approach democratizes sign generation and synthetic data pretraining for SLR by providing computationally efficient methods that achieve significant performance improvements across diverse datasets.</li>
</ul>

<h3>Title: Deep Learning for Taxol Exposure Analysis: A New Cell Image Dataset and Attention-Based Baseline Model</h3>
<ul>
<li><strong>Authors: </strong>Sean Fletcher, Gabby Scott, Douglas Currie, Xin Zhang, Yuqi Song, Bruce MacLeod</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14349">https://arxiv.org/abs/2508.14349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14349">https://arxiv.org/pdf/2508.14349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14349]] Deep Learning for Taxol Exposure Analysis: A New Cell Image Dataset and Attention-Based Baseline Model(https://arxiv.org/abs/2508.14349)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Monitoring the effects of the chemotherapeutic agent Taxol at the cellular level is critical for both clinical evaluation and biomedical research. However, existing detection methods require specialized equipment, skilled personnel, and extensive sample preparation, making them expensive, labor-intensive, and unsuitable for high-throughput or real-time analysis. Deep learning approaches have shown great promise in medical and biological image analysis, enabling automated, high-throughput assessment of cellular morphology. Yet, no publicly available dataset currently exists for automated morphological analysis of cellular responses to Taxol exposure. To address this gap, we introduce a new microscopy image dataset capturing C6 glioma cells treated with varying concentrations of Taxol. To provide an effective solution for Taxol concentration classification and establish a benchmark for future studies on this dataset, we propose a baseline model named ResAttention-KNN, which combines a ResNet-50 with Convolutional Block Attention Modules and uses a k-Nearest Neighbors classifier in the learned embedding space. This model integrates attention-based refinement and non-parametric classification to enhance robustness and interpretability. Both the dataset and implementation are publicly released to support reproducibility and facilitate future research in vision-based biomedical analysis.</li>
</ul>

<h3>Title: A Non-Asymptotic Convergent Analysis for Scored-Based Graph Generative Model via a System of Stochastic Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Junwei Su, Chuan Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14351">https://arxiv.org/abs/2508.14351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14351">https://arxiv.org/pdf/2508.14351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14351]] A Non-Asymptotic Convergent Analysis for Scored-Based Graph Generative Model via a System of Stochastic Differential Equations(https://arxiv.org/abs/2508.14351)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Score-based graph generative models (SGGMs) have proven effective in critical applications such as drug discovery and protein synthesis. However, their theoretical behavior, particularly regarding convergence, remains underexplored. Unlike common score-based generative models (SGMs), which are governed by a single stochastic differential equation (SDE), SGGMs involve a system of coupled SDEs. In SGGMs, the graph structure and node features are governed by separate but interdependent SDEs. This distinction makes existing convergence analyses from SGMs inapplicable for SGGMs. In this work, we present the first non-asymptotic convergence analysis for SGGMs, focusing on the convergence bound (the risk of generative error) across three key graph generation paradigms: (1) feature generation with a fixed graph structure, (2) graph structure generation with fixed node features, and (3) joint generation of both graph structure and node features. Our analysis reveals several unique factors specific to SGGMs (e.g., the topological properties of the graph structure) which affect the convergence bound. Additionally, we offer theoretical insights into the selection of hyperparameters (e.g., sampling steps and diffusion length) and advocate for techniques like normalization to improve convergence. To validate our theoretical findings, we conduct a controlled empirical study using synthetic graph models, and the results align with our theoretical predictions. This work deepens the theoretical understanding of SGGMs, demonstrates their applicability in critical domains, and provides practical guidance for designing effective models.</li>
</ul>

<h3>Title: SBGD: Improving Graph Diffusion Generative Model via Stochastic Block Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Junwei Su, Shan Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14352">https://arxiv.org/abs/2508.14352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14352">https://arxiv.org/pdf/2508.14352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14352]] SBGD: Improving Graph Diffusion Generative Model via Stochastic Block Diffusion(https://arxiv.org/abs/2508.14352)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Graph diffusion generative models (GDGMs) have emerged as powerful tools for generating high-quality graphs. However, their broader adoption faces challenges in \emph{scalability and size generalization}. GDGMs struggle to scale to large graphs due to their high memory requirements, as they typically operate in the full graph space, requiring the entire graph to be stored in memory during training and inference. This constraint limits their feasibility for large-scale real-world graphs. GDGMs also exhibit poor size generalization, with limited ability to generate graphs of sizes different from those in the training data, restricting their adaptability across diverse applications. To address these challenges, we propose the stochastic block graph diffusion (SBGD) model, which refines graph representations into a block graph space. This space incorporates structural priors based on real-world graph patterns, significantly reducing memory complexity and enabling scalability to large graphs. The block representation also improves size generalization by capturing fundamental graph structures. Empirical results show that SBGD achieves significant memory improvements (up to 6$\times$) while maintaining comparable or even superior graph generation performance relative to state-of-the-art methods. Furthermore, experiments demonstrate that SBGD better generalizes to unseen graph sizes. The significance of SBGD extends beyond being a scalable and effective GDGM; it also exemplifies the principle of modularization in generative modeling, offering a new avenue for exploring generative models by decomposing complex tasks into more manageable components.</li>
</ul>

<h3>Title: Organ-Agents: Virtual Human Physiology Simulator via LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rihao Chang, He Jiao, Weizhi Nie, Honglin Guo, Keliang Xie, Zhenhua Wu, Lina Zhao, Yunpeng Bai, Yongtao Ma, Lanjun Wang, Yuting Su, Xi Gao, Weijie Wang, Nicu Sebe, Bruno Lepri, Bingwei Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14357">https://arxiv.org/abs/2508.14357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14357">https://arxiv.org/pdf/2508.14357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14357]] Organ-Agents: Virtual Human Physiology Simulator via LLMs(https://arxiv.org/abs/2508.14357)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have enabled new possibilities in simulating complex physiological systems. We introduce Organ-Agents, a multi-agent framework that simulates human physiology via LLM-driven agents. Each Simulator models a specific system (e.g., cardiovascular, renal, immune). Training consists of supervised fine-tuning on system-specific time-series data, followed by reinforcement-guided coordination using dynamic reference selection and error correction. We curated data from 7,134 sepsis patients and 7,895 controls, generating high-resolution trajectories across 9 systems and 125 variables. Organ-Agents achieved high simulation accuracy on 4,509 held-out patients, with per-system MSEs <0.16 and robustness across SOFA-based severity strata. External validation on 22,689 ICU patients from two hospitals showed moderate degradation under distribution shifts with stable simulation. Organ-Agents faithfully reproduces critical multi-system events (e.g., hypotension, hyperlactatemia, hypoxemia) with coherent timing and phase progression. Evaluation by 15 critical care physicians confirmed realism and physiological plausibility (mean Likert ratings 3.9 and 3.7). Organ-Agents also enables counterfactual simulations under alternative sepsis treatment strategies, generating trajectories and APACHE II scores aligned with matched real-world patients. In downstream early warning tasks, classifiers trained on synthetic data showed minimal AUROC drops (<0.04), indicating preserved decision-relevant patterns. These results position Organ-Agents as a credible, interpretable, and generalizable digital twin for precision diagnosis, treatment simulation, and hypothesis testing in critical care.</li>
</ul>

<h3>Title: Taming Transformer for Emotion-Controllable Talking Face Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Zhang, Cheng Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14359">https://arxiv.org/abs/2508.14359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14359">https://arxiv.org/pdf/2508.14359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14359]] Taming Transformer for Emotion-Controllable Talking Face Generation(https://arxiv.org/abs/2508.14359)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Talking face generation is a novel and challenging generation task, aiming at synthesizing a vivid speaking-face video given a specific audio. To fulfill emotion-controllable talking face generation, current methods need to overcome two challenges: One is how to effectively model the multimodal relationship related to the specific emotion, and the other is how to leverage this relationship to synthesize identity preserving emotional videos. In this paper, we propose a novel method to tackle the emotion-controllable talking face generation task discretely. Specifically, we employ two pre-training strategies to disentangle audio into independent components and quantize videos into combinations of visual tokens. Subsequently, we propose the emotion-anchor (EA) representation that integrates the emotional information into visual tokens. Finally, we introduce an autoregressive transformer to model the global distribution of the visual tokens under the given conditions and further predict the index sequence for synthesizing the manipulated videos. We conduct experiments on the MEAD dataset that controls the emotion of videos conditioned on multiple emotional audios. Extensive experiments demonstrate the superiorities of our method both qualitatively and quantitatively.</li>
</ul>

<h3>Title: FastTracker: Real-Time and Accurate Visual Tracking</h3>
<ul>
<li><strong>Authors: </strong>Hamidreza Hashempoor, Yu Dong Hwang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14370">https://arxiv.org/abs/2508.14370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14370">https://arxiv.org/pdf/2508.14370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14370]] FastTracker: Real-Time and Accurate Visual Tracking(https://arxiv.org/abs/2508.14370)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Conventional multi-object tracking (MOT) systems are predominantly designed for pedestrian tracking and often exhibit limited generalization to other object categories. This paper presents a generalized tracking framework capable of handling multiple object types, with a particular emphasis on vehicle tracking in complex traffic scenes. The proposed method incorporates two key components: (1) an occlusion-aware re-identification mechanism that enhances identity preservation for heavily occluded objects, and (2) a road-structure-aware tracklet refinement strategy that utilizes semantic scene priors such as lane directions, crosswalks, and road boundaries to improve trajectory continuity and accuracy. In addition, we introduce a new benchmark dataset comprising diverse vehicle classes with frame-level tracking annotations, specifically curated to support evaluation of vehicle-focused tracking methods. Extensive experimental results demonstrate that the proposed approach achieves robust performance on both the newly introduced dataset and several public benchmarks, highlighting its effectiveness in general-purpose object tracking. While our framework is designed for generalized multi-class tracking, it also achieves strong performance on conventional benchmarks, with HOTA scores of 66.4 on MOT17 and 65.7 on MOT20 test sets. Code and Benchmark are available: this http URL, this http URL.</li>
</ul>

<h3>Title: TCFNet: Bidirectional face-bone transformation via a Transformer-based coarse-to-fine point movement network</h3>
<ul>
<li><strong>Authors: </strong>Runshi Zhang, Bimeng Jie, Yang He, Junchen Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14373">https://arxiv.org/abs/2508.14373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14373">https://arxiv.org/pdf/2508.14373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14373]] TCFNet: Bidirectional face-bone transformation via a Transformer-based coarse-to-fine point movement network(https://arxiv.org/abs/2508.14373)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Computer-aided surgical simulation is a critical component of orthognathic surgical planning, where accurately simulating face-bone shape transformations is significant. The traditional biomechanical simulation methods are limited by their computational time consumption levels, labor-intensive data processing strategies and low accuracy. Recently, deep learning-based simulation methods have been proposed to view this problem as a point-to-point transformation between skeletal and facial point clouds. However, these approaches cannot process large-scale points, have limited receptive fields that lead to noisy points, and employ complex preprocessing and postprocessing operations based on registration. These shortcomings limit the performance and widespread applicability of such methods. Therefore, we propose a Transformer-based coarse-to-fine point movement network (TCFNet) to learn unique, complicated correspondences at the patch and point levels for dense face-bone point cloud transformations. This end-to-end framework adopts a Transformer-based network and a local information aggregation network (LIA-Net) in the first and second stages, respectively, which reinforce each other to generate precise point movement paths. LIA-Net can effectively compensate for the neighborhood precision loss of the Transformer-based network by modeling local geometric structures (edges, orientations and relative position features). The previous global features are employed to guide the local displacement using a gated recurrent unit. Inspired by deformable medical image registration, we propose an auxiliary loss that can utilize expert knowledge for reconstructing critical this http URL with the existing state-of-the-art (SOTA) methods on gathered datasets, TCFNet achieves outstanding evaluation metrics and visualization results. The code is available at this https URL.</li>
</ul>

<h3>Title: ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities</h3>
<ul>
<li><strong>Authors: </strong>Wenhan Dong, Zhen Sun, Yuemeng Zhao, Zifan Peng, Jun Wu, Jingyi Zheng, Yule Liu, Xinlei He, Yu Wang, Ruiming Wang, Xinyi Huang, Lei Mo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14377">https://arxiv.org/abs/2508.14377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14377">https://arxiv.org/pdf/2508.14377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14377]] ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities(https://arxiv.org/abs/2508.14377)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated potential in educational applications, yet their capacity to accurately assess the cognitive alignment of reading materials with students' developmental stages remains insufficiently explored. This gap is particularly critical given the foundational educational principle of the Zone of Proximal Development (ZPD), which emphasizes the need to match learning resources with Students' Cognitive Abilities (SCA). Despite the importance of this alignment, there is a notable absence of comprehensive studies investigating LLMs' ability to evaluate reading comprehension difficulty across different student age groups, especially in the context of Chinese language education. To fill this gap, we introduce ZPD-SCA, a novel benchmark specifically designed to assess stage-level Chinese reading comprehension difficulty. The benchmark is annotated by 60 Special Grade teachers, a group that represents the top 0.15% of all in-service teachers nationwide. Experimental results reveal that LLMs perform poorly in zero-shot learning scenarios, with Qwen-max and GLM even falling below the probability of random guessing. When provided with in-context examples, LLMs performance improves substantially, with some models achieving nearly double the accuracy of their zero-shot baselines. These results reveal that LLMs possess emerging abilities to assess reading difficulty, while also exposing limitations in their current training for educationally aligned judgment. Notably, even the best-performing models display systematic directional biases, suggesting difficulties in accurately aligning material difficulty with SCA. Furthermore, significant variations in model performance across different genres underscore the complexity of task. We envision that ZPD-SCA can provide a foundation for evaluating and improving LLMs in cognitively aligned educational applications.</li>
</ul>

<h3>Title: Online Incident Response Planning under Model Misspecification through Bayesian Learning and Belief Quantization</h3>
<ul>
<li><strong>Authors: </strong>Kim Hammar, Tao Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14385">https://arxiv.org/abs/2508.14385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14385">https://arxiv.org/pdf/2508.14385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14385]] Online Incident Response Planning under Model Misspecification through Bayesian Learning and Belief Quantization(https://arxiv.org/abs/2508.14385)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Effective responses to cyberattacks require fast decisions, even when information about the attack is incomplete or inaccurate. However, most decision-support frameworks for incident response rely on a detailed system model that describes the incident, which restricts their practical utility. In this paper, we address this limitation and present an online method for incident response planning under model misspecification, which we call MOBAL: Misspecified Online Bayesian Learning. MOBAL iteratively refines a conjecture about the model through Bayesian learning as new information becomes available, which facilitates model adaptation as the incident unfolds. To determine effective responses online, we quantize the conjectured model into a finite Markov model, which enables efficient response planning through dynamic programming. We prove that Bayesian learning is asymptotically consistent with respect to the information feedback. Additionally, we establish bounds on misspecification and quantization errors. Experiments on the CAGE-2 benchmark show that MOBAL outperforms the state of the art in terms of adaptability and robustness to model misspecification.</li>
</ul>

<h3>Title: Credence Calibration Game? Calibrating Large Language Models through Structured Play</h3>
<ul>
<li><strong>Authors: </strong>Ke Fang, Tianyi Zhao, Lu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14390">https://arxiv.org/abs/2508.14390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14390">https://arxiv.org/pdf/2508.14390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14390]] Credence Calibration Game? Calibrating Large Language Models through Structured Play(https://arxiv.org/abs/2508.14390)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are increasingly deployed in decision-critical domains, it becomes essential to ensure that their confidence estimates faithfully correspond to their actual correctness. Existing calibration methods have primarily focused on post-hoc adjustments or auxiliary model training; however, many of these approaches necessitate additional supervision or parameter updates. In this work, we propose a novel prompt-based calibration framework inspired by the Credence Calibration Game. Our method establishes a structured interaction loop wherein LLMs receive feedback based on the alignment of their predicted confidence with correctness. Through feedback-driven prompting and natural language summaries of prior performance, our framework dynamically improves model calibration. Extensive experiments across models and game configurations demonstrate consistent improvements in evaluation metrics. Our results highlight the potential of game-based prompting as an effective strategy for LLM calibration. Code and data are available at this https URL.</li>
</ul>

<h3>Title: DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement</h3>
<ul>
<li><strong>Authors: </strong>Yupei Yang, Fan Feng, Lin Yang, Wanxi Deng, Lin Qu, Biwei Huang, Shikui Tu, Lei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14391">https://arxiv.org/abs/2508.14391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14391">https://arxiv.org/pdf/2508.14391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14391]] DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement(https://arxiv.org/abs/2508.14391)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Relation extraction enables the construction of structured knowledge for many downstream applications. While large language models (LLMs) have shown great promise in this domain, most existing methods concentrate on relation classification, which predicts the semantic relation type between a related entity pair. However, we observe that LLMs often struggle to reliably determine whether a relation exists, especially in cases involving complex sentence structures or intricate semantics, which leads to spurious predictions. Such hallucinations can introduce noisy edges in knowledge graphs, compromising the integrity of structured knowledge and downstream reliability. To address these challenges, we propose DEPTH, a framework that integrates Dependency-aware sEntence simPlification and Two-tiered Hierarchical refinement into the relation extraction pipeline. Given a sentence and its candidate entity pairs, DEPTH operates in two stages: (1) the Grounding module extracts relations for each pair by leveraging their shortest dependency path, distilling the sentence into a minimal yet coherent relational context that reduces syntactic noise while preserving key semantics; (2) the Refinement module aggregates all local predictions and revises them based on a holistic understanding of the sentence, correcting omissions and inconsistencies. We further introduce a causality-driven reward model that mitigates reward hacking by disentangling spurious correlations, enabling robust fine-tuning via reinforcement learning with human feedback. Experiments on six benchmarks demonstrate that DEPTH reduces the average hallucination rate to 7.0\% while achieving a 17.2\% improvement in average F1 score over state-of-the-art baselines.</li>
</ul>

<h3>Title: Img2ST-Net: Efficient High-Resolution Spatial Omics Prediction from Whole Slide Histology Images via Fully Convolutional Image-to-Image Learning</h3>
<ul>
<li><strong>Authors: </strong>Junchao Zhu, Ruining Deng, Junlin Guo, Tianyuan Yao, Juming Xiong, Chongyu Qu, Mengmeng Yin, Yu Wang, Shilin Zhao, Haichun Yang, Daguang Xu, Yucheng Tang, Yuankai Huo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14393">https://arxiv.org/abs/2508.14393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14393">https://arxiv.org/pdf/2508.14393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14393]] Img2ST-Net: Efficient High-Resolution Spatial Omics Prediction from Whole Slide Histology Images via Fully Convolutional Image-to-Image Learning(https://arxiv.org/abs/2508.14393)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in multi-modal AI have demonstrated promising potential for generating the currently expensive spatial transcriptomics (ST) data directly from routine histology images, offering a means to reduce the high cost and time-intensive nature of ST data acquisition. However, the increasing resolution of ST, particularly with platforms such as Visium HD achieving 8um or finer, introduces significant computational and modeling challenges. Conventional spot-by-spot sequential regression frameworks become inefficient and unstable at this scale, while the inherent extreme sparsity and low expression levels of high-resolution ST further complicate both prediction and evaluation. To address these limitations, we propose Img2ST-Net, a novel histology-to-ST generation framework for efficient and parallel high-resolution ST prediction. Unlike conventional spot-by-spot inference methods, Img2ST-Net employs a fully convolutional architecture to generate dense, HD gene expression maps in a parallelized manner. By modeling HD ST data as super-pixel representations, the task is reformulated from image-to-omics inference into a super-content image generation problem with hundreds or thousands of output channels. This design not only improves computational efficiency but also better preserves the spatial organization intrinsic to spatial omics data. To enhance robustness under sparse expression patterns, we further introduce SSIM-ST, a structural-similarity-based evaluation metric tailored for high-resolution ST analysis. We present a scalable, biologically coherent framework for high-resolution ST prediction. Img2ST-Net offers a principled solution for efficient and accurate ST inference at scale. Our contributions lay the groundwork for next-generation ST modeling that is robust and resolution-aware. The source code has been made publicly available at this https URL.</li>
</ul>

<h3>Title: Precision over Noise: Tailoring S3 Public Access Detection to Reduce False Positives in Cloud Security Platforms</h3>
<ul>
<li><strong>Authors: </strong>Dikshant, Geetika Verma</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14402">https://arxiv.org/abs/2508.14402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14402">https://arxiv.org/pdf/2508.14402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14402]] Precision over Noise: Tailoring S3 Public Access Detection to Reduce False Positives in Cloud Security Platforms(https://arxiv.org/abs/2508.14402)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Excessive and spurious alert generation by cloud security solutions is a root cause of analyst fatigue and operational inefficiencies. In this study, the long-standing issue of false positives from publicly accessible alerts in Amazon S3, as generated by a licensed cloud-native security solution, is examined. In a simulated production test environment, which consisted of over 1,000 Amazon S3 buckets with diverse access configurations, it was discovered that over 80\% of the alerts generated by default rules were classified as false positives, thus demonstrating the severity of the detection issue. This severely impacted detection accuracy and generated a heavier workload for analysts due to redundant manual triage efforts. For addressing this problem, custom detection logic was created as an exercise of the native rule customization capabilities of the solution. A unified titled ``S3 Public Access Validation and Data Exposure'' was created in an effort to consolidate different forms of alerts into one, context-aware logic that systematically scans ACL configurations, bucket policies, indicators of public exposure, and the presence of sensitive data, and then marks only those S3 buckets that indeed denote security risk and are publicly exposed on the internet with no authentication. The results demonstrate a significant reduction in false positives, more precise alert fidelity, and significant time saving for security analysts, thus demonstrating an actionable and reproducible solution to enhance the accuracy of security alerting in compliance-focused cloud environments.</li>
</ul>

<h3>Title: CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities</h3>
<ul>
<li><strong>Authors: </strong>Yue Gong, Shanyuan Liu, Liuzhuozheng Li, Jian Zhu, Bo Cheng, Liebucha Wu, Xiaoyu Wu, Yuhang Ma, Dawei Leng, Yuhui Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14405">https://arxiv.org/abs/2508.14405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14405">https://arxiv.org/pdf/2508.14405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14405]] CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities(https://arxiv.org/abs/2508.14405)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>We proposed the Chinese Text Adapter-Flux (CTA-Flux). An adaptation method fits the Chinese text inputs to Flux, a powerful text-to-image (TTI) generative model initially trained on the English corpus. Despite the notable image generation ability conditioned on English text inputs, Flux performs poorly when processing non-English prompts, particularly due to linguistic and cultural biases inherent in predominantly English-centric training datasets. Existing approaches, such as translating non-English prompts into English or finetuning models for bilingual mappings, inadequately address culturally specific semantics, compromising image authenticity and quality. To address this issue, we introduce a novel method to bridge Chinese semantic understanding with compatibility in English-centric TTI model communities. Existing approaches relying on ControlNet-like architectures typically require a massive parameter scale and lack direct control over Chinese semantics. In comparison, CTA-flux leverages MultiModal Diffusion Transformer (MMDiT) to control the Flux backbone directly, significantly reducing the number of parameters while enhancing the model's understanding of Chinese semantics. This integration significantly improves the generation quality and cultural authenticity without extensive retraining of the entire model, thus maintaining compatibility with existing text-to-image plugins such as LoRA, IP-Adapter, and ControlNet. Empirical evaluations demonstrate that CTA-flux supports Chinese and English prompts and achieves superior image generation quality, visual realism, and faithful depiction of Chinese semantics.</li>
</ul>

<h3>Title: Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yinghan Zhou, Weifeng Zhu, Juan Wen, Wanli Peng, Zhengxian Wu, Yiming Xue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14408">https://arxiv.org/abs/2508.14408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14408">https://arxiv.org/pdf/2508.14408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14408]] Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs(https://arxiv.org/abs/2508.14408)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been shown to possess a degree of self-recognition capability-the ability to identify whether a given text was generated by themselves. Prior work has demonstrated that this capability is reliably expressed under the Pair Presentation Paradigm (PPP), where the model is presented with two texts and asked to choose which one it authored. However, performance deteriorates sharply under the Individual Presentation Paradigm (IPP), where the model is given a single text to judge authorship. Although this phenomenon has been observed, its underlying causes have not been systematically analyzed. In this paper, we first replicate existing findings to confirm that LLMs struggle to distinguish self- from other-generated text under IPP. We then investigate the reasons for this failure and attribute it to a phenomenon we term Implicit Territorial Awareness (ITA)-the model's latent ability to distinguish self- and other-texts in representational space, which remains unexpressed in its output behavior. To awaken the ITA of LLMs, we propose Cognitive Surgery (CoSur), a novel framework comprising four main modules: representation extraction, territory construction, authorship discrimination and cognitive editing. Experimental results demonstrate that our proposed method improves the performance of three different LLMs in the IPP scenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%, respectively.</li>
</ul>

<h3>Title: Disentanglement in T-space for Faster and Distributed Training of Diffusion Models with Fewer Latent-states</h3>
<ul>
<li><strong>Authors: </strong>Samarth Gupta, Raghudeep Gadde, Rui Chen, Aleix M. Martinez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14413">https://arxiv.org/abs/2508.14413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14413">https://arxiv.org/pdf/2508.14413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14413]] Disentanglement in T-space for Faster and Distributed Training of Diffusion Models with Fewer Latent-states(https://arxiv.org/abs/2508.14413)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We challenge a fundamental assumption of diffusion models, namely, that a large number of latent-states or time-steps is required for training so that the reverse generative process is close to a Gaussian. We first show that with careful selection of a noise schedule, diffusion models trained over a small number of latent states (i.e. $T \sim 32$) match the performance of models trained over a much large number of latent states ($T \sim 1,000$). Second, we push this limit (on the minimum number of latent states required) to a single latent-state, which we refer to as complete disentanglement in T-space. We show that high quality samples can be easily generated by the disentangled model obtained by combining several independently trained single latent-state models. We provide extensive experiments to show that the proposed disentangled model provides 4-6$\times$ faster convergence measured across a variety of metrics on two different datasets.</li>
</ul>

<h3>Title: MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing</h3>
<ul>
<li><strong>Authors: </strong>Jeahun Sung, Changhyun Roh, Chanho Eom, Jihyong Oh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14423">https://arxiv.org/abs/2508.14423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14423">https://arxiv.org/pdf/2508.14423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14423]] MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing(https://arxiv.org/abs/2508.14423)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in portable imaging have made camera-based screen capture ubiquitous. Unfortunately, frequency aliasing between the camera's color filter array (CFA) and the display's sub-pixels induces moiré patterns that severely degrade captured photos and videos. Although various demoiréing models have been proposed to remove such moiré patterns, these approaches still suffer from several limitations: (i) spatially varying artifact strength within a frame, (ii) large-scale and globally spreading structures, (iii) channel-dependent statistics and (iv) rapid temporal fluctuations across frames. We address these issues with the Moiré Conditioned Hybrid Adaptive Transformer (MoCHA-former), which comprises two key components: Decoupled Moiré Adaptive Demoiréing (DMAD) and Spatio-Temporal Adaptive Demoiréing (STAD). DMAD separates moiré and content via a Moiré Decoupling Block (MDB) and a Detail Decoupling Block (DDB), then produces moiré-adaptive features using a Moiré Conditioning Block (MCB) for targeted restoration. STAD introduces a Spatial Fusion Block (SFB) with window attention to capture large-scale structures, and a Feature Channel Attention (FCA) to model channel dependence in RAW frames. To ensure temporal consistency, MoCHA-former performs implicit frame alignment without any explicit alignment module. We analyze moiré characteristics through qualitative and quantitative studies, and evaluate on two video datasets covering RAW and sRGB domains. MoCHA-former consistently surpasses prior methods across PSNR, SSIM, and LPIPS.</li>
</ul>

<h3>Title: Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wuyang Zhang, Yexin Tian, Xiandong Meng, Mengjie Wang, Junliang Du</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14427">https://arxiv.org/abs/2508.14427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14427">https://arxiv.org/pdf/2508.14427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14427]] Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models(https://arxiv.org/abs/2508.14427)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>This paper addresses the problems of missing reasoning chains and insufficient entity-level semantic understanding in large language models when dealing with tasks that require structured knowledge. It proposes a fine-tuning algorithm framework based on knowledge graph injection. The method builds on pretrained language models and introduces structured graph information for auxiliary learning. A graph neural network is used to encode entities and their relations, constructing a graph-based semantic representation. A fusion mechanism is then designed to jointly model the knowledge graph embeddings with the contextual representations from the language model. To enhance the robustness of knowledge integration, a gating mechanism is introduced to dynamically balance the contributions of linguistic semantics and structural knowledge. This effectively mitigates conflicts between different representational spaces. During training, a joint loss function is constructed to account for both task performance and structural alignment objectives. This helps improve the accuracy of entity prediction and semantic reasoning. The study also includes a series of systematic sensitivity experiments. It evaluates the effects of learning rate, graph coverage, and structural perturbations on model performance. The results further validate the effectiveness and stability of the proposed method across tasks such as entity recognition, question answering, and language generation. Experimental findings show that the proposed structure-aware fine-tuning framework significantly enhances the model's ability to represent complex semantic units. It demonstrates better semantic consistency and contextual logic modeling in scenarios involving structural reasoning and entity extraction.</li>
</ul>

<h3>Title: HyperDiff: Hypergraph Guided Diffusion Model for 3D Human Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Bing Han, Yuhua Huang, Pan Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14431">https://arxiv.org/abs/2508.14431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14431">https://arxiv.org/pdf/2508.14431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14431]] HyperDiff: Hypergraph Guided Diffusion Model for 3D Human Pose Estimation(https://arxiv.org/abs/2508.14431)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Monocular 3D human pose estimation (HPE) often encounters challenges such as depth ambiguity and occlusion during the 2D-to-3D lifting process. Additionally, traditional methods may overlook multi-scale skeleton features when utilizing skeleton structure information, which can negatively impact the accuracy of pose estimation. To address these challenges, this paper introduces a novel 3D pose estimation method, HyperDiff, which integrates diffusion models with HyperGCN. The diffusion model effectively captures data uncertainty, alleviating depth ambiguity and occlusion. Meanwhile, HyperGCN, serving as a denoiser, employs multi-granularity structures to accurately model high-order correlations between joints. This improves the model's denoising capability especially for complex poses. Experimental results demonstrate that HyperDiff achieves state-of-the-art performance on the Human3.6M and MPI-INF-3DHP datasets and can flexibly adapt to varying computational resources to balance performance and efficiency.</li>
</ul>

<h3>Title: FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Tjio, Jie Zhang, Xulei Yang, Yun Xing, Nhat Chung, Xiaofeng Cao, Ivor W. Tsang, Chee Keong Kwoh, Qing Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14437">https://arxiv.org/abs/2508.14437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14437">https://arxiv.org/pdf/2508.14437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14437]] FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation(https://arxiv.org/abs/2508.14437)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Test-time adaptation enables models to adapt to evolving domains. However, balancing the tradeoff between preserving knowledge and adapting to domain shifts remains challenging for model adaptation methods, since adapting to domain shifts can induce forgetting of task-relevant knowledge. To address this problem, we propose FOCUS, a novel frequency-based conditioning approach within a diffusion-driven input-adaptation framework. Utilising learned, spatially adaptive frequency priors, our approach conditions the reverse steps during diffusion-driven denoising to preserve task-relevant semantic information for dense prediction. FOCUS leverages a trained, lightweight, Y-shaped Frequency Prediction Network (Y-FPN) that disentangles high and low frequency information from noisy images. This minimizes the computational costs involved in implementing our approach in a diffusion-driven framework. We train Y-FPN with FrequencyMix, a novel data augmentation method that perturbs the images across diverse frequency bands, which improves the robustness of our approach to diverse corruptions. We demonstrate the effectiveness of FOCUS for semantic segmentation and monocular depth estimation across 15 corruption types and three datasets, achieving state-of-the-art averaged performance. In addition to improving standalone performance, FOCUS complements existing model adaptation methods since we can derive pseudo labels from FOCUS-denoised images for additional supervision. Even under limited, intermittent supervision with the pseudo labels derived from the FOCUS denoised images, we show that FOCUS mitigates catastrophic forgetting for recent model adaptation methods.</li>
</ul>

<h3>Title: MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion</h3>
<ul>
<li><strong>Authors: </strong>Fei Peng, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, Huiyuan Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14440">https://arxiv.org/abs/2508.14440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14440">https://arxiv.org/pdf/2508.14440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14440]] MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion(https://arxiv.org/abs/2508.14440)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images guided by textual prompts. However, achieving multi-subject compositional synthesis with precise spatial control remains a significant challenge. In this work, we address the task of layout-controllable multi-subject synthesis (LMS), which requires both faithful reconstruction of reference subjects and their accurate placement in specified regions within a unified image. While recent advancements have separately improved layout control and subject synthesis, existing approaches struggle to simultaneously satisfy the dual requirements of spatial precision and identity preservation in this composite task. To bridge this gap, we propose MUSE, a unified synthesis framework that employs concatenated cross-attention (CCA) to seamlessly integrate layout specifications with textual guidance through explicit semantic space expansion. The proposed CCA mechanism enables bidirectional modality alignment between spatial constraints and textual descriptions without interference. Furthermore, we design a progressive two-stage training strategy that decomposes the LMS task into learnable sub-objectives for effective optimization. Extensive experiments demonstrate that MUSE achieves zero-shot end-to-end generation with superior spatial accuracy and identity consistency compared to existing solutions, advancing the frontier of controllable image synthesis. Our code and model are available at this https URL.</li>
</ul>

<h3>Title: Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Gyusam Chang, Tuan-Anh Vu, Vivek Alumootil, Harris Song, Deanna Pham, Sangpil Kim, M. Khalid Jawed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14443">https://arxiv.org/abs/2508.14443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14443">https://arxiv.org/pdf/2508.14443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14443]] Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting(https://arxiv.org/abs/2508.14443)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While 3D Gaussian Splatting (3DGS) has rapidly advanced, its application in agriculture remains underexplored. Agricultural scenes present unique challenges for 3D reconstruction methods, particularly due to uneven illumination, occlusions, and a limited field of view. To address these limitations, we introduce \textbf{NIRPlant}, a novel multimodal dataset encompassing Near-Infrared (NIR) imagery, RGB imagery, textual metadata, Depth, and LiDAR data collected under varied indoor and outdoor lighting conditions. By integrating NIR data, our approach enhances robustness and provides crucial botanical insights that extend beyond the visible spectrum. Additionally, we leverage text-based metadata derived from vegetation indices, such as NDVI, NDWI, and the chlorophyll index, which significantly enriches the contextual understanding of complex agricultural environments. To fully exploit these modalities, we propose \textbf{NIRSplat}, an effective multimodal Gaussian splatting architecture employing a cross-attention mechanism combined with 3D point-based positional encoding, providing robust geometric priors. Comprehensive experiments demonstrate that \textbf{NIRSplat} outperforms existing landmark methods, including 3DGS, CoR-GS, and InstantSplat, highlighting its effectiveness in challenging agricultural scenarios. The code and dataset are publicly available at: this https URL</li>
</ul>

<h3>Title: NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model</h3>
<ul>
<li><strong>Authors: </strong>NVIDIA: Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adi Renduchintala, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao, Ann Guan, Anna Shors, Anubhav Mandarwal, Arham Mehta, Arun Venkatesan, Ashton Sharabiani, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Banghua Zhu, Barnaby Simkin, Bilal Kartal, Bita Darvish Rouhani, Bobby Chen, Boris Ginsburg, Brandon Norick, Brian Yu, Bryan Catanzaro, Charles Wang, Charlie Truong, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christian Munley, Christopher Parisien, Dan Su, Daniel Afrimi, Daniel Korzekwa, Daniel Rohrer, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Dima Rekesh, Dina Yared, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Eileen Long, Elliott Ning, Eric Chung, Erick Galinkin, Evelina Bakhturina, Gargi Prasad, Gerald Shen, Haim Elisha, Harsh Sharma, Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Hoo Chang Shin, Hua Huang, Iain Cunningham, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jimmy Zhang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jonathan Cohen, Joseph Jennings, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kezhi Kong, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Kushan Ahmadian, Lawrence McAfee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14444">https://arxiv.org/abs/2508.14444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14444">https://arxiv.org/pdf/2508.14444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14444]] NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model(https://arxiv.org/abs/2508.14444)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face.</li>
</ul>

<h3>Title: Generalizable Engagement Estimation in Conversation via Domain Prompting and Parallel Attention</h3>
<ul>
<li><strong>Authors: </strong>Yangche Yu, Yin Chen, Jia Li, Peng Jia, Yu Zhang, Li Dai, Zhenzhen Hu, Meng Wang, Richang Hong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14448">https://arxiv.org/abs/2508.14448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14448">https://arxiv.org/pdf/2508.14448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14448]] Generalizable Engagement Estimation in Conversation via Domain Prompting and Parallel Attention(https://arxiv.org/abs/2508.14448)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate engagement estimation is essential for adaptive human-computer interaction systems, yet robust deployment is hindered by poor generalizability across diverse domains and challenges in modeling complex interaction this http URL tackle these issues, we propose DAPA (Domain-Adaptive Parallel Attention), a novel framework for generalizable conversational engagement modeling. DAPA introduces a Domain Prompting mechanism by prepending learnable domain-specific vectors to the input, explicitly conditioning the model on the data's origin to facilitate domain-aware adaptation while preserving generalizable engagement representations. To capture interactional synchrony, the framework also incorporates a Parallel Cross-Attention module that explicitly aligns reactive (forward BiLSTM) and anticipatory (backward BiLSTM) states between this http URL experiments demonstrate that DAPA establishes a new state-of-the-art performance on several cross-cultural and cross-linguistic benchmarks, notably achieving an absolute improvement of 0.45 in Concordance Correlation Coefficient (CCC) over a strong baseline on the NoXi-J test set. The superiority of our method was also confirmed by winning the first place in the Multi-Domain Engagement Estimation Challenge at MultiMediate'25.</li>
</ul>

<h3>Title: Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and Inverse Rendering</h3>
<ul>
<li><strong>Authors: </strong>Shanlin Sun, Yifan Wang, Hanwen Zhang, Yifeng Xiong, Qin Ren, Ruogu Fang, Xiaohui Xie, Chenyu You</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14461">https://arxiv.org/abs/2508.14461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14461">https://arxiv.org/pdf/2508.14461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14461]] Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and Inverse Rendering(https://arxiv.org/abs/2508.14461)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While multi-step diffusion models have advanced both forward and inverse rendering, existing approaches often treat these problems independently, leading to cycle inconsistency and slow inference speed. In this work, we present Ouroboros, a framework composed of two single-step diffusion models that handle forward and inverse rendering with mutual reinforcement. Our approach extends intrinsic decomposition to both indoor and outdoor scenes and introduces a cycle consistency mechanism that ensures coherence between forward and inverse rendering outputs. Experimental results demonstrate state-of-the-art performance across diverse scenes while achieving substantially faster inference speed compared to other diffusion-based methods. We also demonstrate that Ouroboros can transfer to video decomposition in a training-free manner, reducing temporal inconsistency in video sequences while maintaining high-quality per-frame inverse rendering.</li>
</ul>

<h3>Title: In2x at WMT25 Translation Task</h3>
<ul>
<li><strong>Authors: </strong>Lei Pang, Hanyi Mao, Quanjia Xiao, HaiXiao Liu, Xiangyi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14472">https://arxiv.org/abs/2508.14472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14472">https://arxiv.org/pdf/2508.14472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14472]] In2x at WMT25 Translation Task(https://arxiv.org/abs/2508.14472)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents the open-system submission by the In2x research team for the WMT25 General Machine Translation Shared Task. Our submission focuses on Japanese-related translation tasks, aiming to explore a generalizable paradigm for extending large language models (LLMs) to other languages. This paradigm encompasses aspects such as data construction methods and reward model design. The ultimate goal is to enable large language model systems to achieve exceptional performance in low-resource or less commonly spoken languages.</li>
</ul>

<h3>Title: On the notion of missingness for path attribution explainability methods in medical settings: Guiding the selection of medically meaningful baselines</h3>
<ul>
<li><strong>Authors: </strong>Alexander Geiger, Lars Wagner, Daniel Rueckert, Dirk Wilhelm, Alissa Jell</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14482">https://arxiv.org/abs/2508.14482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14482">https://arxiv.org/pdf/2508.14482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14482]] On the notion of missingness for path attribution explainability methods in medical settings: Guiding the selection of medically meaningful baselines(https://arxiv.org/abs/2508.14482)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, generative</a></li>
<li><strong>Abstract: </strong>The explainability of deep learning models remains a significant challenge, particularly in the medical domain where interpretable outputs are critical for clinical trust and transparency. Path attribution methods such as Integrated Gradients rely on a baseline input representing the absence of relevant features ("missingness"). Commonly used baselines, such as all-zero inputs, are often semantically meaningless, especially in medical contexts where missingness can itself be informative. While alternative baseline choices have been explored, existing methods lack a principled approach to dynamically select baselines tailored to each input. In this work, we examine the notion of missingness in the medical setting, analyze its implications for baseline selection, and introduce a counterfactual-guided approach to address the limitations of conventional baselines. We argue that a clinically normal but input-close counterfactual represents a more accurate representation of a meaningful absence of features in medical data. To implement this, we use a Variational Autoencoder to generate counterfactual baselines, though our concept is generative-model-agnostic and can be applied with any suitable counterfactual method. We evaluate the approach on three distinct medical data sets and empirically demonstrate that counterfactual baselines yield more faithful and medically relevant attributions compared to standard baseline choices.</li>
</ul>

<h3>Title: Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration</h3>
<ul>
<li><strong>Authors: </strong>Haoran Bai, Xiaoxu Chen, Canqian Yang, Zongyao He, Sibin Deng, Ying Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14483">https://arxiv.org/abs/2508.14483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14483">https://arxiv.org/pdf/2508.14483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14483]] Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration(https://arxiv.org/abs/2508.14483)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>We present Vivid-VR, a DiT-based generative video restoration method built upon an advanced T2V foundation model, where ControlNet is leveraged to control the generation process, ensuring content consistency. However, conventional fine-tuning of such controllable pipelines frequently suffers from distribution drift due to limitations in imperfect multimodal alignment, resulting in compromised texture realism and temporal coherence. To tackle this challenge, we propose a concept distillation training strategy that utilizes the pretrained T2V model to synthesize training samples with embedded textual concepts, thereby distilling its conceptual understanding to preserve texture and temporal quality. To enhance generation controllability, we redesign the control architecture with two key components: 1) a control feature projector that filters degradation artifacts from input video latents to minimize their propagation through the generation pipeline, and 2) a new ControlNet connector employing a dual-branch design. This connector synergistically combines MLP-based feature mapping with cross-attention mechanism for dynamic control feature retrieval, enabling both content preservation and adaptive control signal modulation. Extensive experiments show that Vivid-VR performs favorably against existing approaches on both synthetic and real-world benchmarks, as well as AIGC videos, achieving impressive texture realism, visual vividness, and temporal consistency. The codes and checkpoints are publicly available at this https URL.</li>
</ul>

<h3>Title: WeedSense: Multi-Task Learning for Weed Segmentation, Height Estimation, and Growth Stage Classification</h3>
<ul>
<li><strong>Authors: </strong>Toqi Tahamid Sarker, Khaled R Ahmed, Taminul Islam, Cristiana Bernardi Rankrape, Karla Gage</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14486">https://arxiv.org/abs/2508.14486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14486">https://arxiv.org/pdf/2508.14486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14486]] WeedSense: Multi-Task Learning for Weed Segmentation, Height Estimation, and Growth Stage Classification(https://arxiv.org/abs/2508.14486)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Weed management represents a critical challenge in agriculture, significantly impacting crop yields and requiring substantial resources for control. Effective weed monitoring and analysis strategies are crucial for implementing sustainable agricultural practices and site-specific management approaches. We introduce WeedSense, a novel multi-task learning architecture for comprehensive weed analysis that jointly performs semantic segmentation, height estimation, and growth stage classification. We present a unique dataset capturing 16 weed species over an 11-week growth cycle with pixel-level annotations, height measurements, and temporal labels. WeedSense leverages a dual-path encoder incorporating Universal Inverted Bottleneck blocks and a Multi-Task Bifurcated Decoder with transformer-based feature fusion to generate multi-scale features and enable simultaneous prediction across multiple tasks. WeedSense outperforms other state-of-the-art models on our comprehensive evaluation. On our multi-task dataset, WeedSense achieves mIoU of 89.78% for segmentation, 1.67cm MAE for height estimation, and 99.99% accuracy for growth stage classification while maintaining real-time inference at 160 FPS. Our multitask approach achieves 3$\times$ faster inference than sequential single-task execution and uses 32.4% fewer parameters. Please see our project page at this http URL.</li>
</ul>

<h3>Title: Reasoning is about giving reasons</h3>
<ul>
<li><strong>Authors: </strong>Krunal Shah, Dan Roth</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14488">https://arxiv.org/abs/2508.14488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14488">https://arxiv.org/pdf/2508.14488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14488]] Reasoning is about giving reasons(https://arxiv.org/abs/2508.14488)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Convincing someone of the truth value of a premise requires understanding and articulating the core logical structure of the argument which proves or disproves the premise. Understanding the logical structure of an argument refers to understanding the underlying "reasons" which make up the proof or disproof of the premise - as a function of the "logical atoms" in the argument. While it has been shown that transformers can "chain" rules to derive simple arguments, the challenge of articulating the "reasons" remains. Not only do current approaches to chaining rules suffer in terms of their interpretability, they are also quite constrained in their ability to accommodate extensions to theoretically equivalent reasoning tasks - a model trained to chain rules cannot support abduction or identify contradictions. In this work we suggest addressing these shortcomings by identifying an intermediate representation (which we call the Representation of the Logical Structure (RLS) of the argument) that possesses an understanding of the logical structure of a natural language argument - the logical atoms in the argument and the rules incorporating them. Given the logical structure, reasoning is deterministic and easy to compute. Therefore, our approach supports all forms of reasoning that depend on the logical structure of the natural language argument, including arbitrary depths of reasoning, on-the-fly mistake rectification and interactive discussion with respect to an argument. We show that we can identify and extract the logical structure of natural language arguments in three popular reasoning datasets with high accuracies, thus supporting explanation generation and extending the reasoning capabilities significantly.</li>
</ul>

<h3>Title: Semantic Energy: Detecting LLM Hallucination Beyond Entropy</h3>
<ul>
<li><strong>Authors: </strong>Huan Ma, Jiadong Pan, Jing Liu, Yan Chen, Joey Tianyi Zhou, Guangyu Wang, Qinghua Hu, Hua Wu, Changqing Zhang, Haifeng Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14496">https://arxiv.org/abs/2508.14496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14496">https://arxiv.org/pdf/2508.14496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14496]] Semantic Energy: Detecting LLM Hallucination Beyond Entropy(https://arxiv.org/abs/2508.14496)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are being increasingly deployed in real-world applications, but they remain susceptible to hallucinations, which produce fluent yet incorrect responses and lead to erroneous decision-making. Uncertainty estimation is a feasible approach to detect such hallucinations. For example, semantic entropy estimates uncertainty by considering the semantic diversity across multiple sampled responses, thus identifying hallucinations. However, semantic entropy relies on post-softmax probabilities and fails to capture the model's inherent uncertainty, causing it to be ineffective in certain scenarios. To address this issue, we introduce Semantic Energy, a novel uncertainty estimation framework that leverages the inherent confidence of LLMs by operating directly on logits of penultimate layer. By combining semantic clustering with a Boltzmann-inspired energy distribution, our method better captures uncertainty in cases where semantic entropy fails. Experiments across multiple benchmarks show that Semantic Energy significantly improves hallucination detection and uncertainty estimation, offering more reliable signals for downstream applications such as hallucination detection.</li>
</ul>

<h3>Title: SATURN: Autoregressive Image Generation Guided by Scene Graphs</h3>
<ul>
<li><strong>Authors: </strong>Thanh-Nhan Vo, Trong-Thuan Nguyen, Tam V. Nguyen, Minh-Triet Tran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14502">https://arxiv.org/abs/2508.14502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14502">https://arxiv.org/pdf/2508.14502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14502]] SATURN: Autoregressive Image Generation Guided by Scene Graphs(https://arxiv.org/abs/2508.14502)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>State-of-the-art text-to-image models excel at photorealistic rendering but often struggle to capture the layout and object relationships implied by complex prompts. Scene graphs provide a natural structural prior, yet previous graph-guided approaches have typically relied on heavy GAN or diffusion pipelines, which lag behind modern autoregressive architectures in both speed and fidelity. We introduce SATURN (Structured Arrangement of Triplets for Unified Rendering Networks), a lightweight extension to VAR-CLIP that translates a scene graph into a salience-ordered token sequence, enabling a frozen CLIP-VQ-VAE backbone to interpret graph structure while fine-tuning only the VAR transformer. On the Visual Genome dataset, SATURN reduces FID from 56.45% to 21.62% and increases the Inception Score from 16.03 to 24.78, outperforming prior methods such as SG2IM and SGDiff without requiring extra modules or multi-stage training. Qualitative results further confirm improvements in object count fidelity and spatial relation accuracy, showing that SATURN effectively combines structural awareness with state-of-the-art autoregressive fidelity.</li>
</ul>

<h3>Title: Artificial Intelligence-Based Multiscale Temporal Modeling for Anomaly Detection in Cloud Services</h3>
<ul>
<li><strong>Authors: </strong>Lian Lian, Yilin Li, Song Han, Renzi Meng, Sibo Wang, Ming Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14503">https://arxiv.org/abs/2508.14503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14503">https://arxiv.org/pdf/2508.14503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14503]] Artificial Intelligence-Based Multiscale Temporal Modeling for Anomaly Detection in Cloud Services(https://arxiv.org/abs/2508.14503)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>This study proposes an anomaly detection method based on the Transformer architecture with integrated multiscale feature perception, aiming to address the limitations of temporal modeling and scale-aware feature representation in cloud service environments. The method first employs an improved Transformer module to perform temporal modeling on high-dimensional monitoring data, using a self-attention mechanism to capture long-range dependencies and contextual semantics. Then, a multiscale feature construction path is introduced to extract temporal features at different granularities through downsampling and parallel encoding. An attention-weighted fusion module is designed to dynamically adjust the contribution of each scale to the final decision, enhancing the model's robustness in anomaly pattern modeling. In the input modeling stage, standardized multidimensional time series are constructed, covering core signals such as CPU utilization, memory usage, and task scheduling states, while positional encoding is used to strengthen the model's temporal awareness. A systematic experimental setup is designed to evaluate performance, including comparative experiments and hyperparameter sensitivity analysis, focusing on the impact of optimizers, learning rates, anomaly ratios, and noise levels. Experimental results show that the proposed method outperforms mainstream baseline models in key metrics, including precision, recall, AUC, and F1-score, and maintains strong stability and detection performance under various perturbation conditions, demonstrating its superior capability in complex cloud environments.</li>
</ul>

<h3>Title: CoFacS -- Simulating a Complete Factory to Study the Security of Interconnected Production</h3>
<ul>
<li><strong>Authors: </strong>Stefan Lenz, David Schachtschneider, Simon Jonas, Liam Tirpitz, Sandra Geisler, Martin Henze</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14526">https://arxiv.org/abs/2508.14526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14526">https://arxiv.org/pdf/2508.14526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14526]] CoFacS -- Simulating a Complete Factory to Study the Security of Interconnected Production(https://arxiv.org/abs/2508.14526)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>While the digitization of industrial factories provides tremendous improvements for the production of goods, it also renders such systems vulnerable to serious cyber-attacks. To research, test, and validate security measures protecting industrial networks against such cyber-attacks, the security community relies on testbeds to simulate industrial systems, as utilizing live systems endangers costly components or even human life. However, existing testbeds focus on individual parts of typically complex production lines in industrial factories. Consequently, the impact of cyber-attacks on industrial networks as well as the effectiveness of countermeasures cannot be evaluated in an end-to-end manner. To address this issue and facilitate research on novel security mechanisms, we present CoFacS, the first COmplete FACtory Simulation that replicates an entire production line and affords the integration of real-life industrial applications. To showcase that CoFacS accurately captures real-world behavior, we validate it against a physical model factory widely used in security research. We show that CoFacS has a maximum deviation of 0.11% to the physical reference, which enables us to study the impact of physical attacks or network-based cyber-attacks. Moreover, we highlight how CoFacS enables security research through two cases studies surrounding attack detection and the resilience of 5G-based industrial communication against jamming.</li>
</ul>

<h3>Title: Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Jiangfan Liu, Yongkang Guo, Fangzhi Zhong, Tianyuan Zhang, Zonglei Jing, Siyuan Liang, Jiakai Wang, Mingchuan Zhang, Aishan Liu, Xianglong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14527">https://arxiv.org/abs/2508.14527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14527">https://arxiv.org/pdf/2508.14527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14527]] Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles(https://arxiv.org/abs/2508.14527)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The generation of safety-critical scenarios in simulation has become increasingly crucial for safety evaluation in autonomous vehicles prior to road deployment in society. However, current approaches largely rely on predefined threat patterns or rule-based strategies, which limit their ability to expose diverse and unforeseen failure modes. To overcome these, we propose ScenGE, a framework that can generate plentiful safety-critical scenarios by reasoning novel adversarial cases and then amplifying them with complex traffic flows. Given a simple prompt of a benign scene, it first performs Meta-Scenario Generation, where a large language model, grounded in structured driving knowledge, infers an adversarial agent whose behavior poses a threat that is both plausible and deliberately challenging. This meta-scenario is then specified in executable code for precise in-simulator control. Subsequently, Complex Scenario Evolution uses background vehicles to amplify the core threat introduced by Meta-Scenario. It builds an adversarial collaborator graph to identify key agent trajectories for optimization. These perturbations are designed to simultaneously reduce the ego vehicle's maneuvering space and create critical occlusions. Extensive experiments conducted on multiple reinforcement learning based AV models show that ScenGE uncovers more severe collision cases (+31.96%) on average than SoTA baselines. Additionally, our ScenGE can be applied to large model based AV systems and deployed on different simulators; we further observe that adversarial training on our scenarios improves the model robustness. Finally, we validate our framework through real-world vehicle tests and human evaluation, confirming that the generated scenarios are both plausible and critical. We hope our paper can build up a critical step towards building public trust and ensuring their safe deployment.</li>
</ul>

<h3>Title: DOPA: Stealthy and Generalizable Backdoor Attacks from a Single Client under Challenging Federated Constraints</h3>
<ul>
<li><strong>Authors: </strong>Xuezheng Qin, Ruwei Huang, Xiaolong Tang, Feng Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14530">https://arxiv.org/abs/2508.14530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14530">https://arxiv.org/pdf/2508.14530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14530]] DOPA: Stealthy and Generalizable Backdoor Attacks from a Single Client under Challenging Federated Constraints(https://arxiv.org/abs/2508.14530)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust, steal, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is increasingly adopted for privacy-preserving collaborative training, but its decentralized nature makes it particularly susceptible to backdoor attacks. Existing attack methods, however, often rely on idealized assumptions and fail to remain effective under real-world constraints, such as limited attacker control, non-IID data distributions, and the presence of diverse defense mechanisms. To address this gap, we propose DOPA (Divergent Optimization Path Attack), a novel framework that simulates heterogeneous local training dynamics and seeks consensus across divergent optimization trajectories to craft universally effective and stealthy backdoor triggers. By leveraging consistency signals across simulated paths to guide optimization, DOPA overcomes the challenge of heterogeneity-induced instability and achieves practical attack viability under stringent federated constraints. We validate DOPA on a comprehensive suite of 12 defense strategies, two model architectures (ResNet18/VGG16), two datasets (CIFAR-10/TinyImageNet), and both mild and extreme non-IID settings. Despite operating under a single-client, black-box, and sparsely participating threat model, DOPA consistently achieves high attack success, minimal accuracy degradation, low runtime, and long-term persistence. These results demonstrate a more practical attack paradigm, offering new perspectives for designing robust defense strategies in federated learning systems</li>
</ul>

<h3>Title: WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion</h3>
<ul>
<li><strong>Authors: </strong>Yonghan Shin, SeungKyu Kim, Won-Ki Jeong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14537">https://arxiv.org/abs/2508.14537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14537">https://arxiv.org/pdf/2508.14537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14537]] WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion(https://arxiv.org/abs/2508.14537)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Whole slide images (WSIs) in computational pathology (CPath) pose a major computational challenge due to their gigapixel scale, often requiring the processing of tens to hundreds of thousands of high-resolution patches per slide. This results in prohibitive encoding costs, with preprocessing and training times extending to days or even weeks-making WSI encoding the most significant bottleneck in real-world deployment. In this work, we propose WISE-FUSE, an adaptive WSI encoding framework that leverages pathology-domain vision-language models and large language models to address this challenge by selectively processing diagnostically relevant regions. WISE-FUSE first computes similarity scores between low-resolution patches and class-specific textual descriptions using a knowledge distillation mechanism that preserves fine-grained diagnostic features. Based on these similarity scores, we select a small subset of informative regions for the target task, which quickly eliminates irrelevant patches at the coarse level. The corresponding high-resolution patches are then selectively encoded and fused with textual embeddings to reinforce diagnostic context. Extensive experiments demonstrate that WISE-FUSE reduces WSI encoding time by over threefold while achieving diagnostic performance comparable to or surpassing that of exhaustive patch processing, offering a scalable and practical solution for CPath.</li>
</ul>

<h3>Title: FedEve: On Bridging the Client Drift and Period Drift for Cross-device Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Tao Shen, Zexi Li, Didi Zhu, Ziyu Zhao, Chao Wu, Fei Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14539">https://arxiv.org/abs/2508.14539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14539">https://arxiv.org/pdf/2508.14539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14539]] FedEve: On Bridging the Client Drift and Period Drift for Cross-device Federated Learning(https://arxiv.org/abs/2508.14539)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a machine learning paradigm that allows multiple clients to collaboratively train a shared model without exposing their private data. Data heterogeneity is a fundamental challenge in FL, which can result in poor convergence and performance degradation. Client drift has been recognized as one of the factors contributing to this issue resulting from the multiple local updates in FedAvg. However, in cross-device FL, a different form of drift arises due to the partial client participation, but it has not been studied well. This drift, we referred as period drift, occurs as participating clients at each communication round may exhibit distinct data distribution that deviates from that of all clients. It could be more harmful than client drift since the optimization objective shifts with every round. In this paper, we investigate the interaction between period drift and client drift, finding that period drift can have a particularly detrimental effect on cross-device FL as the degree of data heterogeneity increases. To tackle these issues, we propose a predict-observe framework and present an instantiated method, FedEve, where these two types of drift can compensate each other to mitigate their overall impact. We provide theoretical evidence that our approach can reduce the variance of model updates. Extensive experiments demonstrate that our method outperforms alternatives on non-iid data in cross-device settings.</li>
</ul>

<h3>Title: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Zixi Chen, Yinyu Ye, Zijie Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14544">https://arxiv.org/abs/2508.14544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14544">https://arxiv.org/pdf/2508.14544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14544]] Adaptively Robust LLM Inference Optimization under Prediction Uncertainty(https://arxiv.org/abs/2508.14544)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We study the problem of optimizing Large Language Model (LLM) inference scheduling to minimize total latency. LLM inference is an online and multi-task service process and also heavily energy consuming by which a pre-trained LLM processes input requests and generates output tokens sequentially. Therefore, it is vital to improve its scheduling efficiency and reduce the power consumption while a great amount of prompt requests are arriving. A key challenge in LLM inference scheduling is that while the prompt length is known upon arrival, the output length, which critically impacts memory usage and processing time, is unknown. To address this uncertainty, we propose algorithms that leverage machine learning to predict output lengths, assuming the prediction provides an interval classification (min-max range) for each request. We first design a conservative algorithm, $\mathcal{A}_{\max}$, which schedules requests based on the upper bound of predicted output lengths to prevent memory overflow. However, this approach is overly conservative: as prediction accuracy decreases, performance degrades significantly due to potential overestimation. To overcome this limitation, we propose $\mathcal{A}_{\min}$, an adaptive algorithm that initially treats the predicted lower bound as the output length and dynamically refines this estimate during inferencing. We prove that $\mathcal{A}_{\min}$ achieves a log-scale competitive ratio. Through numerical simulations, we demonstrate that $\mathcal{A}_{\min}$ often performs nearly as well as the hindsight scheduler, highlighting both its efficiency and robustness in practical scenarios. Moreover, $\mathcal{A}_{\min}$ relies solely on the lower bound of the prediction interval--an advantageous design choice since upper bounds on output length are typically more challenging to predict accurately.</li>
</ul>

<h3>Title: A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives</h3>
<ul>
<li><strong>Authors: </strong>Juepeng Zheng, Zi Ye, Yibin Wen, Jianxi Huang, Zhiwei Zhang, Qingmei Li, Qiong Hu, Baodong Xu, Lingyuan Zhao, Haohuan Fu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14558">https://arxiv.org/abs/2508.14558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14558">https://arxiv.org/pdf/2508.14558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14558]] A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives(https://arxiv.org/abs/2508.14558)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Powered by advances in multiple remote sensing sensors, the production of high spatial resolution images provides great potential to achieve cost-efficient and high-accuracy agricultural inventory and analysis in an automated way. Lots of studies that aim at providing an inventory of the level of each agricultural parcel have generated many methods for Agricultural Parcel and Boundary Delineation (APBD). This review covers APBD methods for detecting and delineating agricultural parcels and systematically reviews the past and present of APBD-related research applied to remote sensing images. With the goal to provide a clear knowledge map of existing APBD efforts, we conduct a comprehensive review of recent APBD papers to build a meta-data analysis, including the algorithm, the study site, the crop type, the sensor type, the evaluation method, etc. We categorize the methods into three classes: (1) traditional image processing methods (including pixel-based, edge-based and region-based); (2) traditional machine learning methods (such as random forest, decision tree); and (3) deep learning-based methods. With deep learning-oriented approaches contributing to a majority, we further discuss deep learning-based methods like semantic segmentation-based, object detection-based and Transformer-based methods. In addition, we discuss five APBD-related issues to further comprehend the APBD domain using remote sensing data, such as multi-sensor data in APBD task, comparisons between single-task learning and multi-task learning in the APBD domain, comparisons among different algorithms and different APBD tasks, etc. Finally, this review proposes some APBD-related applications and a few exciting prospects and potential hot topics in future APBD research. We hope this review help researchers who involved in APBD domain to keep track of its development and tendency.</li>
</ul>

<h3>Title: Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization</h3>
<ul>
<li><strong>Authors: </strong>Sukhyun Jeong, Hong-Gi Shin, Yong-Hoon Choi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14561">https://arxiv.org/abs/2508.14561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14561">https://arxiv.org/pdf/2508.14561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14561]] Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization(https://arxiv.org/abs/2508.14561)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Recent progress in text-to-motion has advanced both 3D human motion generation and text-based motion control. Controllable motion generation (CoMo), which enables intuitive control, typically relies on pose code representations, but discrete pose codes alone cannot capture fine-grained motion details, limiting expressiveness. To overcome this, we propose a method that augments pose code-based latent representations with continuous motion features using residual vector quantization (RVQ). This design preserves the interpretability and manipulability of pose codes while effectively capturing subtle motion characteristics such as high-frequency details. Experiments on the HumanML3D dataset show that our model reduces Frechet inception distance (FID) from 0.041 to 0.015 and improves Top-1 R-Precision from 0.508 to 0.510. Qualitative analysis of pairwise direction similarity between pose codes further confirms the model's controllability for motion editing.</li>
</ul>

<h3>Title: GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels</h3>
<ul>
<li><strong>Authors: </strong>Xingyuan Yang, Min Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14563">https://arxiv.org/abs/2508.14563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14563">https://arxiv.org/pdf/2508.14563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14563]] GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels(https://arxiv.org/abs/2508.14563)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Inverse rendering of glossy objects from RGB imagery remains fundamentally limited by inherent ambiguity. Although NeRF-based methods achieve high-fidelity reconstruction via dense-ray sampling, their computational cost is prohibitive. Recent 3D Gaussian Splatting achieves high reconstruction efficiency but exhibits limitations under specular reflections. Multi-view inconsistencies introduce high-frequency surface noise and structural artifacts, while simplified rendering equations obscure material properties, leading to implausible relighting results. To address these issues, we propose GOGS, a novel two-stage framework based on 2D Gaussian surfels. First, we establish robust surface reconstruction through physics-based rendering with split-sum approximation, enhanced by geometric priors from foundation models. Second, we perform material decomposition by leveraging Monte Carlo importance sampling of the full rendering equation, modeling indirect illumination via differentiable 2D Gaussian ray tracing and refining high-frequency specular details through spherical mipmap-based directional encoding that captures anisotropic highlights. Extensive experiments demonstrate state-of-the-art performance in geometry reconstruction, material separation, and photorealistic relighting under novel illuminations, outperforming existing inverse rendering approaches.</li>
</ul>

<h3>Title: Safety-Critical Learning for Long-Tail Events: The TUM Traffic Accident Dataset</h3>
<ul>
<li><strong>Authors: </strong>Walter Zimmer, Ross Greer, Xingcheng Zhou, Rui Song, Marc Pavel, Daniel Lehmberg, Ahmed Ghita, Akshay Gopalkrishnan, Mohan Trivedi, Alois Knoll</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14567">https://arxiv.org/abs/2508.14567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14567">https://arxiv.org/pdf/2508.14567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14567]] Safety-Critical Learning for Long-Tail Events: The TUM Traffic Accident Dataset(https://arxiv.org/abs/2508.14567)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Even though a significant amount of work has been done to increase the safety of transportation networks, accidents still occur regularly. They must be understood as an unavoidable and sporadic outcome of traffic networks. We present the TUM Traffic Accident (TUMTraf-A) dataset, a collection of real-world highway accidents. It contains ten sequences of vehicle crashes at high-speed driving with 294,924 labeled 2D and 93,012 labeled 3D boxes and track IDs within 48,144 labeled frames recorded from four roadside cameras and LiDARs at 10 Hz. The dataset contains ten object classes and is provided in the OpenLABEL format. We propose Accid3nD, an accident detection model that combines a rule-based approach with a learning-based one. Experiments and ablation studies on our dataset show the robustness of our proposed method. The dataset, model, and code are available on our project website: this https URL.</li>
</ul>

<h3>Title: Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Guilhem Fauré (MULTISPEECH), Mostafa Sadeghi (MULTISPEECH), Sam Bigeard (MULTISPEECH), Slim Ouni (LORIA, MULTISPEECH)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14574">https://arxiv.org/abs/2508.14574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14574">https://arxiv.org/pdf/2508.14574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14574]] Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning(https://arxiv.org/abs/2508.14574)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>One of the main challenges in neural sign language production (SLP) lies in the high intra-class variability of signs, arising from signer morphology and stylistic variety in the training data. To improve robustness to such variations, we propose two enhancements to the standard Progressive Transformers (PT) architecture (Saunders et al., 2020). First, we encode poses using bone rotations in quaternion space and train with a geodesic loss to improve the accuracy and clarity of angular joint movements. Second, we introduce a contrastive loss to structure decoder embeddings by semantic similarity, using either gloss overlap or SBERT-based sentence similarity, aiming to filter out anatomical and stylistic features that do not convey relevant semantic information. On the Phoenix14T dataset, the contrastive loss alone yields a 16% improvement in Probability of Correct Keypoint over the PT baseline. When combined with quaternion-based pose encoding, the model achieves a 6% reduction in Mean Bone Angle Error. These results point to the benefit of incorporating skeletal structure modeling and semantically guided contrastive objectives on sign pose representations into the training of Transformer-based SLP models.</li>
</ul>

<h3>Title: A Comprehensive Evaluation of the Sensitivity of Density-Ratio Estimation Based Fairness Measurement in Regression</h3>
<ul>
<li><strong>Authors: </strong>Abdalwahab Almajed, Maryam Tabar, Peyman Najafirad</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14576">https://arxiv.org/abs/2508.14576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14576">https://arxiv.org/pdf/2508.14576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14576]] A Comprehensive Evaluation of the Sensitivity of Density-Ratio Estimation Based Fairness Measurement in Regression(https://arxiv.org/abs/2508.14576)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The prevalence of algorithmic bias in Machine Learning (ML)-driven approaches has inspired growing research on measuring and mitigating bias in the ML domain. Accordingly, prior research studied how to measure fairness in regression which is a complex problem. In particular, recent research proposed to formulate it as a density-ratio estimation problem and relied on a Logistic Regression-driven probabilistic classifier-based approach to solve it. However, there are several other methods to estimate a density ratio, and to the best of our knowledge, prior work did not study the sensitivity of such fairness measurement methods to the choice of underlying density ratio estimation algorithm. To fill this gap, this paper develops a set of fairness measurement methods with various density-ratio estimation cores and thoroughly investigates how different cores would affect the achieved level of fairness. Our experimental results show that the choice of density-ratio estimation core could significantly affect the outcome of fairness measurement method, and even, generate inconsistent results with respect to the relative fairness of various algorithms. These observations suggest major issues with density-ratio estimation based fairness measurement in regression and a need for further research to enhance their reliability.</li>
</ul>

<h3>Title: Controllable Latent Space Augmentation for Digital Pathology</h3>
<ul>
<li><strong>Authors: </strong>Sofiène Boutaj, Marin Scalbert, Pierre Marza, Florent Couzinie-Devy, Maria Vakalopoulou, Stergios Christodoulidis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14588">https://arxiv.org/abs/2508.14588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14588">https://arxiv.org/pdf/2508.14588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14588]] Controllable Latent Space Augmentation for Digital Pathology(https://arxiv.org/abs/2508.14588)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Whole slide image (WSI) analysis in digital pathology presents unique challenges due to the gigapixel resolution of WSIs and the scarcity of dense supervision signals. While Multiple Instance Learning (MIL) is a natural fit for slide-level tasks, training robust models requires large and diverse datasets. Even though image augmentation techniques could be utilized to increase data variability and reduce overfitting, implementing them effectively is not a trivial task. Traditional patch-level augmentation is prohibitively expensive due to the large number of patches extracted from each WSI, and existing feature-level augmentation methods lack control over transformation semantics. We introduce HistAug, a fast and efficient generative model for controllable augmentations in the latent space for digital pathology. By conditioning on explicit patch-level transformations (e.g., hue, erosion), HistAug generates realistic augmented embeddings while preserving initial semantic information. Our method allows the processing of a large number of patches in a single forward pass efficiently, while at the same time consistently improving MIL model performance. Experiments across multiple slide-level tasks and diverse organs show that HistAug outperforms existing methods, particularly in low-data regimes. Ablation studies confirm the benefits of learned transformations over noise-based perturbations and highlight the importance of uniform WSI-wise augmentation. Code is available at this https URL.</li>
</ul>

<h3>Title: Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling</h3>
<ul>
<li><strong>Authors: </strong>Nitish Kumar Mahala, Muzammil Khan, Pushpendra Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14597">https://arxiv.org/abs/2508.14597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14597">https://arxiv.org/pdf/2508.14597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14597]] Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling(https://arxiv.org/abs/2508.14597)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Fire outbreaks pose critical threats to human life and infrastructure, necessitating high-fidelity early-warning systems that detect combustion precursors such as smoke. However, smoke plumes exhibit complex spatiotemporal dynamics influenced by illumination variability, flow kinematics, and environmental noise, undermining the reliability of traditional detectors. To address these challenges without the logistical complexity of multi-sensor arrays, we propose an information-fusion framework by integrating smoke feature representations extracted from monocular imagery. Specifically, a Two-Phase Uncertainty-Aware Shifted Windows Transformer for robust and reliable smoke detection, leveraging a novel smoke segmentation dataset, constructed via optical flow-based motion encoding, is proposed. The optical flow estimation is performed with a four-color-theorem-inspired dual-phase level-set fractional-order variational model, which preserves motion discontinuities. The resulting color-encoded optical flow maps are fused with appearance cues via a Gaussian Mixture Model to generate binary segmentation masks of the smoke regions. These fused representations are fed into the novel Shifted-Windows Transformer, which is augmented with a multi-scale uncertainty estimation head and trained under a two-phase learning regimen. First learning phase optimizes smoke detection accuracy, while during the second phase, the model learns to estimate plausibility confidence in its predictions by jointly modeling aleatoric and epistemic uncertainties. Extensive experiments using multiple evaluation metrics and comparative analysis with state-of-the-art approaches demonstrate superior generalization and robustness, offering a reliable solution for early fire detection in surveillance, industrial safety, and autonomous monitoring applications.</li>
</ul>

<h3>Title: DualNILM: Energy Injection Identification Enabled Disaggregation with Deep Multi-Task Learning</h3>
<ul>
<li><strong>Authors: </strong>Xudong Wang, Guoming Tang, Junyu Xue, Srinivasan Keshav, Tongxin Li, Chris Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14600">https://arxiv.org/abs/2508.14600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14600">https://arxiv.org/pdf/2508.14600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14600]] DualNILM: Energy Injection Identification Enabled Disaggregation with Deep Multi-Task Learning(https://arxiv.org/abs/2508.14600)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Non-Intrusive Load Monitoring (NILM) offers a cost-effective method to obtain fine-grained appliance-level energy consumption in smart homes and building applications. However, the increasing adoption of behind-the-meter energy sources, such as solar panels and battery storage, poses new challenges for conventional NILM methods that rely solely on at-the-meter data. The injected energy from the behind-the-meter sources can obscure the power signatures of individual appliances, leading to a significant decline in NILM performance. To address this challenge, we present DualNILM, a deep multi-task learning framework designed for the dual tasks of appliance state recognition and injected energy identification in NILM. By integrating sequence-to-point and sequence-to-sequence strategies within a Transformer-based architecture, DualNILM can effectively capture multi-scale temporal dependencies in the aggregate power consumption patterns, allowing for accurate appliance state recognition and energy injection identification. We conduct validation of DualNILM using both self-collected and synthesized open NILM datasets that include both appliance-level energy consumption and energy injection. Extensive experimental results demonstrate that DualNILM maintains an excellent performance for the dual tasks in NILM, much outperforming conventional methods.</li>
</ul>

<h3>Title: SMTrack: End-to-End Trained Spiking Neural Networks for Multi-Object Tracking in RGB Videos</h3>
<ul>
<li><strong>Authors: </strong>Pengzhi Zhong, Xinzhe Wang, Dan Zeng, Qihua Zhou, Feixiang He, Shuiwang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14607">https://arxiv.org/abs/2508.14607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14607">https://arxiv.org/pdf/2508.14607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14607]] SMTrack: End-to-End Trained Spiking Neural Networks for Multi-Object Tracking in RGB Videos(https://arxiv.org/abs/2508.14607)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Brain-inspired Spiking Neural Networks (SNNs) exhibit significant potential for low-power computation, yet their application in visual tasks remains largely confined to image classification, object detection, and event-based tracking. In contrast, real-world vision systems still widely use conventional RGB video streams, where the potential of directly-trained SNNs for complex temporal tasks such as multi-object tracking (MOT) remains underexplored. To address this challenge, we propose SMTrack-the first directly trained deep SNN framework for end-to-end multi-object tracking on standard RGB videos. SMTrack introduces an adaptive and scale-aware Normalized Wasserstein Distance loss (Asa-NWDLoss) to improve detection and localization performance under varying object scales and densities. Specifically, the method computes the average object size within each training batch and dynamically adjusts the normalization factor, thereby enhancing sensitivity to small objects. For the association stage, we incorporate the TrackTrack identity module to maintain robust and consistent object trajectories. Extensive evaluations on BEE24, MOT17, MOT20, and DanceTrack show that SMTrack achieves performance on par with leading ANN-based MOT methods, advancing robust and accurate SNN-based tracking in complex scenarios.</li>
</ul>

<h3>Title: AnchorSync: Global Consistency Optimization for Long Video Editing</h3>
<ul>
<li><strong>Authors: </strong>Zichi Liu, Yinggui Wang, Tao Wei, Chao Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14609">https://arxiv.org/abs/2508.14609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14609">https://arxiv.org/pdf/2508.14609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14609]] AnchorSync: Global Consistency Optimization for Long Video Editing(https://arxiv.org/abs/2508.14609)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Editing long videos remains a challenging task due to the need for maintaining both global consistency and temporal coherence across thousands of frames. Existing methods often suffer from structural drift or temporal artifacts, particularly in minute-long sequences. We introduce AnchorSync, a novel diffusion-based framework that enables high-quality, long-term video editing by decoupling the task into sparse anchor frame editing and smooth intermediate frame interpolation. Our approach enforces structural consistency through a progressive denoising process and preserves temporal dynamics via multimodal guidance. Extensive experiments show that AnchorSync produces coherent, high-fidelity edits, surpassing prior methods in visual quality and temporal stability.</li>
</ul>

<h3>Title: A Fuzzy-Enhanced Explainable AI Framework for Flight Continuous Descent Operations Classification</h3>
<ul>
<li><strong>Authors: </strong>Amin Noroozi, Sandaruwan K. Sethunge, Elham Norouzi, Phat T. Phan, Kavinda U. Waduge, Md. Arafatur Rahman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14618">https://arxiv.org/abs/2508.14618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14618">https://arxiv.org/pdf/2508.14618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14618]] A Fuzzy-Enhanced Explainable AI Framework for Flight Continuous Descent Operations Classification(https://arxiv.org/abs/2508.14618)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, explainability</a></li>
<li><strong>Abstract: </strong>Continuous Descent Operations (CDO) involve smooth, idle-thrust descents that avoid level-offs, reducing fuel burn, emissions, and noise while improving efficiency and passenger comfort. Despite its operational and environmental benefits, limited research has systematically examined the factors influencing CDO performance. Moreover, many existing methods in related areas, such as trajectory optimization, lack the transparency required in aviation, where explainability is critical for safety and stakeholder trust. This study addresses these gaps by proposing a Fuzzy-Enhanced Explainable AI (FEXAI) framework that integrates fuzzy logic with machine learning and SHapley Additive exPlanations (SHAP) analysis. For this purpose, a comprehensive dataset of 29 features, including 11 operational and 18 weather-related features, was collected from 1,094 flights using Automatic Dependent Surveillance-Broadcast (ADS-B) data. Machine learning models and SHAP were then applied to classify flights' CDO adherence levels and rank features by importance. The three most influential features, as identified by SHAP scores, were then used to construct a fuzzy rule-based classifier, enabling the extraction of interpretable fuzzy rules. All models achieved classification accuracies above 90%, with FEXAI providing meaningful, human-readable rules for operational users. Results indicated that the average descent rate within the arrival route, the number of descent segments, and the average change in directional heading during descent were the strongest predictors of CDO performance. The FEXAI method proposed in this study presents a novel pathway for operational decision support and could be integrated into aviation tools to enable real-time advisories that maintain CDO adherence under varying operational conditions.</li>
</ul>

<h3>Title: Continuous sentiment scores for literary and multilingual contexts</h3>
<ul>
<li><strong>Authors: </strong>Laurits Lyngbaek, Pascale Feldkamp, Yuri Bizzoni, Kristoffer Nielbo, Kenneth Enevoldsen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14620">https://arxiv.org/abs/2508.14620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14620">https://arxiv.org/pdf/2508.14620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14620]] Continuous sentiment scores for literary and multilingual contexts(https://arxiv.org/abs/2508.14620)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Sentiment Analysis is widely used to quantify sentiment in text, but its application to literary texts poses unique challenges due to figurative language, stylistic ambiguity, as well as sentiment evocation strategies. Traditional dictionary-based tools often underperform, especially for low-resource languages, and transformer models, while promising, typically output coarse categorical labels that limit fine-grained analysis. We introduce a novel continuous sentiment scoring method based on concept vector projection, trained on multilingual literary data, which more effectively captures nuanced sentiment expressions across genres, languages, and historical periods. Our approach outperforms existing tools on English and Danish texts, producing sentiment scores whose distribution closely matches human ratings, enabling more accurate analysis and sentiment arc modeling in literature.</li>
</ul>

<h3>Title: Clinical semantics for lung cancer prediction</h3>
<ul>
<li><strong>Authors: </strong>Luis H. John, Jan A. Kors, Jenna M. Reps, Peter R. Rijnbeek, Egill A. Fridgeirsson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14627">https://arxiv.org/abs/2508.14627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14627">https://arxiv.org/pdf/2508.14627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14627]] Clinical semantics for lung cancer prediction(https://arxiv.org/abs/2508.14627)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Background: Existing clinical prediction models often represent patient data using features that ignore the semantic relationships between clinical concepts. This study integrates domain-specific semantic information by mapping the SNOMED medical term hierarchy into a low-dimensional hyperbolic space using Poincaré embeddings, with the aim of improving lung cancer onset prediction. Methods: Using a retrospective cohort from the Optum EHR dataset, we derived a clinical knowledge graph from the SNOMED taxonomy and generated Poincaré embeddings via Riemannian stochastic gradient descent. These embeddings were then incorporated into two deep learning architectures, a ResNet and a Transformer model. Models were evaluated for discrimination (area under the receiver operating characteristic curve) and calibration (average absolute difference between observed and predicted probabilities) performance. Results: Incorporating pre-trained Poincaré embeddings resulted in modest and consistent improvements in discrimination performance compared to baseline models using randomly initialized Euclidean embeddings. ResNet models, particularly those using a 10-dimensional Poincaré embedding, showed enhanced calibration, whereas Transformer models maintained stable calibration across configurations. Discussion: Embedding clinical knowledge graphs into hyperbolic space and integrating these representations into deep learning models can improve lung cancer onset prediction by preserving the hierarchical structure of clinical terminologies used for prediction. This approach demonstrates a feasible method for combining data-driven feature extraction with established clinical knowledge.</li>
</ul>

<h3>Title: Towards PerSense++: Advancing Training-Free Personalized Instance Segmentation in Dense Images</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Ibraheem Siddiqui, Muhammad Umer Sheikh, Hassan Abid, Kevin Henry, Muhammad Haris Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14660">https://arxiv.org/abs/2508.14660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14660">https://arxiv.org/pdf/2508.14660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14660]] Towards PerSense++: Advancing Training-Free Personalized Instance Segmentation in Dense Images(https://arxiv.org/abs/2508.14660)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Segmentation in dense visual scenes poses significant challenges due to occlusions, background clutter, and scale variations. To address this, we introduce PerSense, an end-to-end, training-free, and model-agnostic one-shot framework for Personalized instance Segmentation in dense images. PerSense employs a novel Instance Detection Module (IDM) that leverages density maps (DMs) to generate instance-level candidate point prompts, followed by a Point Prompt Selection Module (PPSM) that filters false positives via adaptive thresholding and spatial gating. A feedback mechanism further enhances segmentation by automatically selecting effective exemplars to improve DM quality. We additionally present PerSense++, an enhanced variant that incorporates three additional components to improve robustness in cluttered scenes: (i) a diversity-aware exemplar selection strategy that leverages feature and scale diversity for better DM generation; (ii) a hybrid IDM combining contour and peak-based prompt generation for improved instance separation within complex density patterns; and (iii) an Irrelevant Mask Rejection Module (IMRM) that discards spatially inconsistent masks using outlier analysis. Finally, to support this underexplored task, we introduce PerSense-D, a dedicated benchmark for personalized segmentation in dense images. Extensive experiments across multiple benchmarks demonstrate that PerSense++ outperforms existing methods in dense settings.</li>
</ul>

<h3>Title: GeMS: Efficient Gaussian Splatting for Extreme Motion Blur</h3>
<ul>
<li><strong>Authors: </strong>Gopi Raju Matta, Trisha Reddypalli, Vemunuri Divya Madhuri, Kaushik Mitra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14682">https://arxiv.org/abs/2508.14682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14682">https://arxiv.org/pdf/2508.14682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14682]] GeMS: Efficient Gaussian Splatting for Extreme Motion Blur(https://arxiv.org/abs/2508.14682)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce GeMS, a framework for 3D Gaussian Splatting (3DGS) designed to handle severely motion-blurred images. State-of-the-art deblurring methods for extreme blur, such as ExBluRF, as well as Gaussian Splatting-based approaches like Deblur-GS, typically assume access to sharp images for camera pose estimation and point cloud generation, an unrealistic assumption. Methods relying on COLMAP initialization, such as BAD-Gaussians, also fail due to unreliable feature correspondences under severe blur. To address these challenges, we propose GeMS, a 3DGS framework that reconstructs scenes directly from extremely blurred images. GeMS integrates: (1) VGGSfM, a deep learning-based Structure-from-Motion pipeline that estimates poses and generates point clouds directly from blurred inputs; (2) 3DGS-MCMC, which enables robust scene initialization by treating Gaussians as samples from a probability distribution, eliminating heuristic densification and pruning; and (3) joint optimization of camera trajectories and Gaussian parameters for stable reconstruction. While this pipeline produces strong results, inaccuracies may remain when all inputs are severely blurred. To mitigate this, we propose GeMS-E, which integrates a progressive refinement step using events: (4) Event-based Double Integral (EDI) deblurring restores sharper images that are then fed into GeMS, improving pose estimation, point cloud generation, and overall reconstruction. Both GeMS and GeMS-E achieve state-of-the-art performance on synthetic and real-world datasets. To our knowledge, this is the first framework to address extreme motion blur within 3DGS directly from severely blurred inputs.</li>
</ul>

<h3>Title: Improving Fairness in Graph Neural Networks via Counterfactual Debiasing</h3>
<ul>
<li><strong>Authors: </strong>Zengyi Wo, Chang Liu, Yumeng Wang, Minglai Shao, Wenjun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14683">https://arxiv.org/abs/2508.14683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14683">https://arxiv.org/pdf/2508.14683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14683]] Improving Fairness in Graph Neural Networks via Counterfactual Debiasing(https://arxiv.org/abs/2508.14683)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have been successful in modeling graph-structured data. However, similar to other machine learning models, GNNs can exhibit bias in predictions based on attributes like race and gender. Moreover, bias in GNNs can be exacerbated by the graph structure and message-passing mechanisms. Recent cutting-edge methods propose mitigating bias by filtering out sensitive information from input or representations, like edge dropping or feature masking. Yet, we argue that such strategies may unintentionally eliminate non-sensitive features, leading to a compromised balance between predictive accuracy and fairness. To tackle this challenge, we present a novel approach utilizing counterfactual data augmentation for bias mitigation. This method involves creating diverse neighborhoods using counterfactuals before message passing, facilitating unbiased node representations learning from the augmented graph. Subsequently, an adversarial discriminator is employed to diminish bias in predictions by conventional GNN classifiers. Our proposed technique, Fair-ICD, ensures the fairness of GNNs under moderate conditions. Experiments on standard datasets using three GNN backbones demonstrate that Fair-ICD notably enhances fairness metrics while preserving high predictive performance.</li>
</ul>

<h3>Title: Improving in-context learning with a better scoring function</h3>
<ul>
<li><strong>Authors: </strong>Omar Naim, Swarnadeep Bhar, Jérôme Bolte, Nicholas Asher</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14685">https://arxiv.org/abs/2508.14685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14685">https://arxiv.org/pdf/2508.14685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14685]] Improving in-context learning with a better scoring function(https://arxiv.org/abs/2508.14685)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit a remarkable capacity to learn by analogy, known as in-context learning (ICL). However, recent studies have revealed limitations in this ability. In this paper, we examine these limitations on tasks involving first-order quantifiers such as {\em all} and {\em some}, as well as on ICL with linear functions. We identify Softmax, the scoring function in attention mechanism, as a contributing factor to these constraints. To address this, we propose \textbf{scaled signed averaging (SSA)}, a novel alternative to Softmax. Empirical results show that SSA dramatically improves performance on our target tasks. Furthermore, we evaluate both encoder-only and decoder-only transformers models with SSA, demonstrating that they match or exceed their Softmax-based counterparts across a variety of linguistic probing tasks.</li>
</ul>

<h3>Title: Foe for Fraud: Transferable Adversarial Attacks in Credit Card Fraud Detection</h3>
<ul>
<li><strong>Authors: </strong>Jan Lum Fok, Qingwen Zeng, Shiping Chen, Oscar Fawkes, Huaming Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14699">https://arxiv.org/abs/2508.14699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14699">https://arxiv.org/pdf/2508.14699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14699]] Foe for Fraud: Transferable Adversarial Attacks in Credit Card Fraud Detection(https://arxiv.org/abs/2508.14699)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Credit card fraud detection (CCFD) is a critical application of Machine Learning (ML) in the financial sector, where accurately identifying fraudulent transactions is essential for mitigating financial losses. ML models have demonstrated their effectiveness in fraud detection task, in particular with the tabular dataset. While adversarial attacks have been extensively studied in computer vision and deep learning, their impacts on the ML models, particularly those trained on CCFD tabular datasets, remains largely unexplored. These latent vulnerabilities pose significant threats to the security and stability of the financial industry, especially in high-value transactions where losses could be substantial. To address this gap, in this paper, we present a holistic framework that investigate the robustness of CCFD ML model against adversarial perturbations under different circumstances. Specifically, the gradient-based attack methods are incorporated into the tabular credit card transaction data in both black- and white-box adversarial attacks settings. Our findings confirm that tabular data is also susceptible to subtle perturbations, highlighting the need for heightened awareness among financial technology practitioners regarding ML model security and trustworthiness. Furthermore, the experiments by transferring adversarial samples from gradient-based attack method to non-gradient-based models also verify our findings. Our results demonstrate that such attacks remain effective, emphasizing the necessity of developing robust defenses for CCFD algorithms.</li>
</ul>

<h3>Title: A Lightweight Incentive-Based Privacy-Preserving Smart Metering Protocol for Value-Added Services</h3>
<ul>
<li><strong>Authors: </strong>Farid Zaredar, Morteza Amini</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14703">https://arxiv.org/abs/2508.14703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14703">https://arxiv.org/pdf/2508.14703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14703]] A Lightweight Incentive-Based Privacy-Preserving Smart Metering Protocol for Value-Added Services(https://arxiv.org/abs/2508.14703)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>The emergence of smart grids and advanced metering infrastructure (AMI) has revolutionized energy management. Unlike traditional power grids, smart grids benefit from two-way communication through AMI, which surpasses earlier automated meter reading (AMR). AMI enables diverse demand- and supply-side utilities such as accurate billing, outage detection, real-time grid control, load forecasting, and value-added services. Smart meters play a key role by delivering consumption values at predefined intervals to the utility provider (UP). However, such reports may raise privacy concerns, as adversaries can infer lifestyle patterns, political orientations, and the types of electrical devices in a household, or even sell the data to third parties (TP) such as insurers. In this paper, we propose a lightweight, privacy-preserving smart metering protocol for incentive-based value-added services. The scheme employs local differential privacy, hash chains, blind digital signatures, pseudonyms, temporal aggregation, and anonymous overlay networks to report coarse-grained values with adjustable granularity to the UP. This protects consumers' privacy while preserving data utility. The scheme prevents identity disclosure while enabling automatic token redemption. From a performance perspective, our results show that with a 1024-bit RSA key, a 7-day duration, and four reports per day, our protocol runs in approximately 0.51s and consumes about 4.5 MB of memory. From a privacy perspective, the protocol resists semi-trusted and untrusted adversaries.</li>
</ul>

<h3>Title: ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine</h3>
<ul>
<li><strong>Authors: </strong>Junying Chen, Zhenyang Cai, Zhiheng Liu, Yunjin Yang, Rongsheng Wang, Qingying Xiao, Xiangyi Feng, Zhan Su, Jing Guo, Xiang Wan, Guangjun Yu, Haizhou Li, Benyou Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14706">https://arxiv.org/abs/2508.14706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14706">https://arxiv.org/pdf/2508.14706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14706]] ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine(https://arxiv.org/abs/2508.14706)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the success of large language models (LLMs) in various domains, their potential in Traditional Chinese Medicine (TCM) remains largely underexplored due to two critical barriers: (1) the scarcity of high-quality TCM data and (2) the inherently multimodal nature of TCM diagnostics, which involve looking, listening, smelling, and pulse-taking. These sensory-rich modalities are beyond the scope of conventional LLMs. To address these challenges, we present ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and physiological signals. ShizhenGPT is pretrained and instruction-tuned to achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect recent national TCM qualification exams and build a visual benchmark for Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that ShizhenGPT outperforms comparable-scale LLMs and competes with larger proprietary models. Moreover, it leads in TCM visual understanding among existing multimodal LLMs and demonstrates unified perception across modalities like sound, pulse, smell, and vision, paving the way toward holistic multimodal perception and diagnosis in TCM. Datasets, models, and code are publicly available. We hope this work will inspire further exploration in this field.</li>
</ul>

<h3>Title: Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Jiabo Huang, Chen Chen, Lingjuan Lyu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14707">https://arxiv.org/abs/2508.14707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14707">https://arxiv.org/pdf/2508.14707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14707]] Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models(https://arxiv.org/abs/2508.14707)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Vision foundation models (VFMs) are predominantly developed using data-centric methods. These methods require training on vast amounts of data usually with high-quality labels, which poses a bottleneck for most institutions that lack both large-scale data and high-end GPUs. On the other hand, many open-source vision models have been pretrained on domain-specific data, enabling them to distill and represent core knowledge in a form that is transferable across diverse applications. Even though these models are highly valuable assets, they remain largely under-explored in empowering the development of a general-purpose VFM. In this paper, we presents a new model-driven approach for training VFMs through joint knowledge transfer and preservation. Our method unifies multiple pre-trained teacher models in a shared latent space to mitigate the ``imbalanced transfer'' issue caused by their distributional gaps. Besides, we introduce a knowledge preservation strategy to take a general-purpose teacher as a knowledge base for integrating knowledge from the remaining purpose-specific teachers using an adapter module. By unifying and aggregating existing models, we build a powerful VFM to inherit teachers' expertise without needing to train on a large amount of labeled data. Our model not only provides generalizable visual features, but also inherently supports multiple downstream tasks. Extensive experiments demonstrate that our VFM outperforms existing data-centric models across four fundamental vision tasks, including image classification, object detection, semantic and instance segmentation.</li>
</ul>

<h3>Title: GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Wei, Stefan Leutenegger, Simon Schaefer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14717">https://arxiv.org/abs/2508.14717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14717">https://arxiv.org/pdf/2508.14717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14717]] GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting(https://arxiv.org/abs/2508.14717)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent developments in 3D Gaussian Splatting have significantly enhanced novel view synthesis, yet generating high-quality renderings from extreme novel viewpoints or partially observed regions remains challenging. Meanwhile, diffusion models exhibit strong generative capabilities, but their reliance on text prompts and lack of awareness of specific scene information hinder accurate 3D reconstruction tasks. To address these limitations, we introduce GSFix3D, a novel framework that improves the visual fidelity in under-constrained regions by distilling prior knowledge from diffusion models into 3D representations, while preserving consistency with observed scene details. At its core is GSFixer, a latent diffusion model obtained via our customized fine-tuning protocol that can leverage both mesh and 3D Gaussians to adapt pretrained generative models to a variety of environments and artifact types from different reconstruction methods, enabling robust novel view repair for unseen camera poses. Moreover, we propose a random mask augmentation strategy that empowers GSFixer to plausibly inpaint missing regions. Experiments on challenging benchmarks demonstrate that our GSFix3D and GSFixer achieve state-of-the-art performance, requiring only minimal scene-specific fine-tuning on captured data. Real-world test further confirms its resilience to potential pose errors. Our code and data will be made publicly available. Project page: this https URL.</li>
</ul>

<h3>Title: The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation</h3>
<ul>
<li><strong>Authors: </strong>Shubham Pundhir, Ganesh Bagler</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14718">https://arxiv.org/abs/2508.14718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14718">https://arxiv.org/pdf/2508.14718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14718]] The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation(https://arxiv.org/abs/2508.14718)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We established a rigorous benchmark for text-based recipe generation, a fundamental task in natural language generation. We present a comprehensive comparative study contrasting a fine-tuned GPT-2 large (774M) model against the GPT-2 small (124M) model and traditional LSTM/RNN baselines on the 5-cuisine corpus from RecipeDB. Our key contribution is a targeted tokenization strategy that augments the vocabulary with 23 common fraction tokens and custom structural markers. This approach addresses a critical limitation of generic tokenizers by preserving essential recipe structures and precise numerical quantities, thereby enhancing domain specificity. Performance is evaluated using a comprehensive suite of seven automatic metrics spanning fluency (BLEU-4, METEOR), coherence (ROUGE-L), semantic relevance (BERTScore), and diversity. Our experiments show that the large transformer-based approach yields a >20% relative improvement in BERTScore (F1) (0.92 vs 0.72) over the best recurrent baseline, while reducing perplexity by 69.8%. We conclude with a discussion of remaining challenges, particularly regarding factual accuracy, and outline how this foundational study paves the way for integrating real-world constraints and multi-modal inputs in advanced recipe generation research.</li>
</ul>

<h3>Title: Transplant Then Regenerate: A New Paradigm for Text Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Guangzhan Wang, Hongyu Zhang, Beijun Shen, Xiaodong Gu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14723">https://arxiv.org/abs/2508.14723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14723">https://arxiv.org/pdf/2508.14723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14723]] Transplant Then Regenerate: A New Paradigm for Text Data Augmentation(https://arxiv.org/abs/2508.14723)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Data augmentation is a critical technique in deep learning. Traditional methods like Back-translation typically focus on lexical-level rephrasing, which primarily produces variations with the same semantics. While large language models (LLMs) have enhanced text augmentation by their "knowledge emergence" capability, controlling the style and structure of these outputs remains challenging and requires meticulous prompt engineering. In this paper, we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs. The core idea of LMTransplant is transplant-then-regenerate: incorporating seed text into a context expanded by LLM, and asking the LLM to regenerate a variant based on the expanded context. This strategy allows the model to create more diverse and creative content-level variants by fully leveraging the knowledge embedded in LLMs, while preserving the core attributes of the original text. We evaluate LMTransplant across various text-related tasks, demonstrating its superior performance over existing text augmentation methods. Moreover, LMTransplant demonstrates exceptional scalability as the size of augmented data grows.</li>
</ul>

<h3>Title: Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Leila Cheshmi, Mennatullah Siam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14729">https://arxiv.org/abs/2508.14729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14729">https://arxiv.org/pdf/2508.14729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14729]] Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving(https://arxiv.org/abs/2508.14729)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Ensuring safety in autonomous driving is a complex challenge requiring handling unknown objects and unforeseen driving scenarios. We develop multiscale video transformers capable of detecting unknown objects using only motion cues. Video semantic and panoptic segmentation often relies on known classes seen during training, overlooking novel categories. Recent visual grounding with large language models is computationally expensive, especially for pixel-level output. We propose an efficient video transformer trained end-to-end for class-agnostic segmentation without optical flow. Our method uses multi-stage multiscale query-memory decoding and a scale-specific random drop-token to ensure efficiency and accuracy, maintaining detailed spatiotemporal features with a shared, learnable memory module. Unlike conventional decoders that compress features, our memory-centric design preserves high-resolution information at multiple scales. We evaluate on DAVIS'16, KITTI, and Cityscapes. Our method consistently outperforms multiscale baselines while being efficient in GPU memory and run-time, demonstrating a promising direction for real-time, robust dense prediction in safety-critical robotics.</li>
</ul>

<h3>Title: AFABench: A Generic Framework for Benchmarking Active Feature Acquisition</h3>
<ul>
<li><strong>Authors: </strong>Valter Schütz, Han Wu, Reza Rezvan, Linus Aronsson, Morteza Haghir Chehreghani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14734">https://arxiv.org/abs/2508.14734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14734">https://arxiv.org/pdf/2508.14734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14734]] AFABench: A Generic Framework for Benchmarking Active Feature Acquisition(https://arxiv.org/abs/2508.14734)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair</a></li>
<li><strong>Abstract: </strong>In many real-world scenarios, acquiring all features of a data instance can be expensive or impractical due to monetary cost, latency, or privacy concerns. Active Feature Acquisition (AFA) addresses this challenge by dynamically selecting a subset of informative features for each data instance, trading predictive performance against acquisition cost. While numerous methods have been proposed for AFA, ranging from greedy information-theoretic strategies to non-myopic reinforcement learning approaches, fair and systematic evaluation of these methods has been hindered by the lack of standardized benchmarks. In this paper, we introduce AFABench, the first benchmark framework for AFA. Our benchmark includes a diverse set of synthetic and real-world datasets, supports a wide range of acquisition policies, and provides a modular design that enables easy integration of new methods and tasks. We implement and evaluate representative algorithms from all major categories, including static, greedy, and reinforcement learning-based approaches. To test the lookahead capabilities of AFA policies, we introduce a novel synthetic dataset, AFAContext, designed to expose the limitations of greedy selection. Our results highlight key trade-offs between different AFA strategies and provide actionable insights for future research. The benchmark code is available at: this https URL.</li>
</ul>

<h3>Title: Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference</h3>
<ul>
<li><strong>Authors: </strong>Samir Abdaljalil, Erchin Serpedin, Khalid Qaraqe, Hasan Kurban</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14735">https://arxiv.org/abs/2508.14735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14735">https://arxiv.org/pdf/2508.14735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14735]] Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference(https://arxiv.org/abs/2508.14735)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly applied in multilingual contexts, yet their capacity for consistent, logically grounded alignment across languages remains underexplored. We present a controlled evaluation framework for multilingual natural language inference (NLI) that generates synthetic, logic-based premise-hypothesis pairs and translates them into a typologically diverse set of languages. This design enables precise control over semantic relations and allows testing in both monolingual and mixed-language (code-switched) conditions. Surprisingly, code-switching does not degrade, and can even improve, performance, suggesting that translation-induced lexical variation may serve as a regularization signal. We validate semantic preservation through embedding-based similarity analyses and cross-lingual alignment visualizations, confirming the fidelity of translated pairs. Our findings expose both the potential and the brittleness of current LLM cross-lingual reasoning, and identify code-switching as a promising lever for improving multilingual robustness. Code available at: this https URL</li>
</ul>

<h3>Title: A Collusion-Resistance Privacy-Preserving Smart Metering Protocol for Operational Utility</h3>
<ul>
<li><strong>Authors: </strong>Farid Zaredar, Morteza Amini</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14744">https://arxiv.org/abs/2508.14744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14744">https://arxiv.org/pdf/2508.14744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14744]] A Collusion-Resistance Privacy-Preserving Smart Metering Protocol for Operational Utility(https://arxiv.org/abs/2508.14744)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Modern grids have adopted advanced metering infrastructure (AMI) to facilitate bidirectional communication between smart meters and control centers. This enables smart meters to report consumption values at predefined intervals to utility providers for purposes including demand balancing, load forecasting, dynamic billing, and operational efficiency. Compared to traditional power grids, smart grids offer advantages such as enhanced reliability, improved energy efficiency, and increased security. However, utility providers can compromise user privacy by analyzing fine-grained readings and extracting individuals' daily activities from this time-series data. To address this concern, we propose a collusion-resistant, privacy-preserving aggregation protocol for smart metering in operational services. Our protocol ensures privacy by leveraging techniques such as partially additive homomorphic encryption, aggregation, data perturbation, and data minimization. The scheme aggregates perturbed readings using the additive homomorphic property of the Paillier cryptosystem to provide results for multiple operational purposes. We evaluate the protocol in terms of both performance and privacy. Computational, memory, and communication overhead were examined. The total execution time with 1024-bit key size is about 2.21 seconds. We also evaluated privacy through the normalized conditional entropy (NCE) metric. Higher NCE values, closer to 1, indicate stronger privacy. By increasing noise scale, the NCE value rises, showing perturbed values retain minimal information about the original, thereby reducing risks. Overall, evaluation demonstrates the protocol's efficiency while employing various privacy-preserving techniques.</li>
</ul>

<h3>Title: MissionHD: Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding</h3>
<ul>
<li><strong>Authors: </strong>Sanggeon Yun, Raheeb Hassan, Ryozo Masukawa, Mohsen Imani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14746">https://arxiv.org/abs/2508.14746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14746">https://arxiv.org/pdf/2508.14746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14746]] MissionHD: Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding(https://arxiv.org/abs/2508.14746)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning graphs from Large Language Models (LLMs) are often misaligned with downstream visual tasks such as video anomaly detection (VAD). Existing Graph Structure Refinement (GSR) methods are ill-suited for these novel, dataset-less graphs. We introduce Data-driven GSR (D-GSR), a new paradigm that directly optimizes graph structure using downstream task data, and propose MissionHD, a hyperdimensional computing (HDC) framework to operationalize it. MissionHD uses an efficient encode-decode process to refine the graph, guided by the downstream task signal. Experiments on challenging VAD and VAR benchmarks show significant performance improvements when using our refined graphs, validating our approach as an effective pre-processing step.</li>
</ul>

<h3>Title: Cross-Modality Controlled Molecule Generation with Diffusion Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yunzhe Zhang, Yifei Wang, Khanh Vinh Nguyen, Pengyu Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14748">https://arxiv.org/abs/2508.14748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14748">https://arxiv.org/pdf/2508.14748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14748]] Cross-Modality Controlled Molecule Generation with Diffusion Language Model(https://arxiv.org/abs/2508.14748)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Current SMILES-based diffusion models for molecule generation typically support only unimodal constraint. They inject conditioning signals at the start of the training process and require retraining a new model from scratch whenever the constraint changes. However, real-world applications often involve multiple constraints across different modalities, and additional constraints may emerge over the course of a study. This raises a challenge: how to extend a pre-trained diffusion model not only to support cross-modality constraints but also to incorporate new ones without retraining. To tackle this problem, we propose the Cross-Modality Controlled Molecule Generation with Diffusion Language Model (CMCM-DLM), demonstrated by two distinct cross modalities: molecular structure and chemical properties. Our approach builds upon a pre-trained diffusion model, incorporating two trainable modules, the Structure Control Module (SCM) and the Property Control Module (PCM), and operates in two distinct phases during the generation process. In Phase I, we employs the SCM to inject structural constraints during the early diffusion steps, effectively anchoring the molecular backbone. Phase II builds on this by further introducing PCM to guide the later stages of inference to refine the generated molecules, ensuring their chemical properties match the specified targets. Experimental results on multiple datasets demonstrate the efficiency and adaptability of our approach, highlighting CMCM-DLM's significant advancement in molecular generation for drug discovery applications.</li>
</ul>

<h3>Title: HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Thomas Carta, Clément Romac, Loris Gaven, Pierre-Yves Oudeyer, Olivier Sigaud, Sylvain Lamprier</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14751">https://arxiv.org/abs/2508.14751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14751">https://arxiv.org/pdf/2508.14751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14751]] HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents(https://arxiv.org/abs/2508.14751)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Open-ended AI agents need to be able to learn efficiently goals of increasing complexity, abstraction and heterogeneity over their lifetime. Beyond sampling efficiently their own goals, autotelic agents specifically need to be able to keep the growing complexity of goals under control, limiting the associated growth in sample and computational complexity. To adress this challenge, recent approaches have leveraged hierarchical reinforcement learning (HRL) and language, capitalizing on its compositional and combinatorial generalization capabilities to acquire temporally extended reusable behaviours. Existing approaches use expert defined spaces of subgoals over which they instantiate a hierarchy, and often assume pre-trained associated low-level policies. Such designs are inadequate in open-ended scenarios, where goal spaces naturally diversify across a broad spectrum of difficulties. We introduce HERAKLES, a framework that enables a two-level hierarchical autotelic agent to continuously compile mastered goals into the low-level policy, executed by a small, fast neural network, dynamically expanding the set of subgoals available to the high-level policy. We train a Large Language Model (LLM) to serve as the high-level controller, exploiting its strengths in goal decomposition and generalization to operate effectively over this evolving subgoal space. We evaluate HERAKLES in the open-ended Crafter environment and show that it scales effectively with goal complexity, improves sample efficiency through skill compilation, and enables the agent to adapt robustly to novel challenges over time.</li>
</ul>

<h3>Title: PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ruheng Wang, Hang Zhang, Trieu Nguyen, Shasha Feng, Hao-Wei Pang, Xiang Yu, Li Xiao, Peter Zhiping Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14765">https://arxiv.org/abs/2508.14765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14765">https://arxiv.org/pdf/2508.14765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14765]] PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning(https://arxiv.org/abs/2508.14765)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Designing therapeutic peptides with tailored properties is hindered by the vastness of sequence space, limited experimental data, and poor interpretability of current generative models. To address these challenges, we introduce PepThink-R1, a generative framework that integrates large language models (LLMs) with chain-of-thought (CoT) supervised fine-tuning and reinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly reasons about monomer-level modifications during sequence generation, enabling interpretable design choices while optimizing for multiple pharmacological properties. Guided by a tailored reward function balancing chemical validity and property improvements, the model autonomously explores diverse sequence variants. We demonstrate that PepThink-R1 generates cyclic peptides with significantly enhanced lipophilicity, stability, and exposure, outperforming existing general LLMs (e.g., GPT-5) and domain-specific baseline in both optimization success and interpretability. To our knowledge, this is the first LLM-based peptide design framework that combines explicit reasoning with RL-driven property control, marking a step toward reliable and transparent peptide optimization for therapeutic discovery.</li>
</ul>

<h3>Title: Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Mujtaba, Gleb Radchenko, Radu Prodan, Marc Masana</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14769">https://arxiv.org/abs/2508.14769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14769">https://arxiv.org/pdf/2508.14769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14769]] Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data(https://arxiv.org/abs/2508.14769)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated distillation has emerged as a promising collaborative machine learning approach, offering enhanced privacy protection and reduced communication compared to traditional federated learning by exchanging model outputs (soft logits) rather than full model parameters. However, existing methods employ complex selective knowledge-sharing strategies that require clients to identify in-distribution proxy data through computationally expensive statistical density ratio estimators. Additionally, server-side filtering of ambiguous knowledge introduces latency to the process. To address these challenges, we propose a robust, resource-efficient EdgeFD method that reduces the complexity of the client-side density ratio estimation and removes the need for server-side filtering. EdgeFD introduces an efficient KMeans-based density ratio estimator for effectively filtering both in-distribution and out-of-distribution proxy data on clients, significantly improving the quality of knowledge sharing. We evaluate EdgeFD across diverse practical scenarios, including strong non-IID, weak non-IID, and IID data distributions on clients, without requiring a pre-trained teacher model on the server for knowledge distillation. Experimental results demonstrate that EdgeFD outperforms state-of-the-art methods, consistently achieving accuracy levels close to IID scenarios even under heterogeneous and challenging conditions. The significantly reduced computational overhead of the KMeans-based estimator is suitable for deployment on resource-constrained edge devices, thereby enhancing the scalability and real-world applicability of federated distillation. The code is available online for reproducibility.</li>
</ul>

<h3>Title: Context Steering: A New Paradigm for Compression-based Embeddings by Synthesizing Relevant Information Features</h3>
<ul>
<li><strong>Authors: </strong>Guillermo Sarasa Durán, Ana Granados Fontecha, Francisco de Borja Rodríguez Ortíz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14780">https://arxiv.org/abs/2508.14780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14780">https://arxiv.org/pdf/2508.14780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14780]] Context Steering: A New Paradigm for Compression-based Embeddings by Synthesizing Relevant Information Features(https://arxiv.org/abs/2508.14780)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Compression-based distances (CD) offer a flexible and domain-agnostic means of measuring similarity by identifying implicit information through redundancies between data objects. However, as similarity features are derived from the data, rather than defined as an input, it often proves difficult to align with the task at hand, particularly in complex clustering or classification settings. To address this issue, we introduce "context steering," a novel methodology that actively guides the feature-shaping process. Instead of passively accepting the emergent data structure (typically a hierarchy derived from clustering CDs), our approach "steers" the process by systematically analyzing how each object influences the relational context within a clustering framework. This process generates a custom-tailored embedding that isolates and amplifies class-distinctive information. We validate the capabilities of this strategy using Normalized Compression Distance (NCD) and Relative Compression Distance (NRC) with common hierarchical clustering, providing an effective alternative to common transductive methods. Experimental results across heterogeneous datasets-from text to real-world audio-validate the robustness and generality of context steering, marking a fundamental shift in their application: from merely discovering inherent data structures to actively shaping a feature space tailored to a specific objective.</li>
</ul>

<h3>Title: TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Leng, Yunying Bi, Chuan Qin, Bing Yin, Yanyong Zhang, Chao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14782">https://arxiv.org/abs/2508.14782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14782">https://arxiv.org/pdf/2508.14782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14782]] TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting(https://arxiv.org/abs/2508.14782)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Urban transportation systems encounter diverse challenges across multiple tasks, such as traffic forecasting, electric vehicle (EV) charging demand prediction, and taxi dispatch. Existing approaches suffer from two key limitations: small-scale deep learning models are task-specific and data-hungry, limiting their generalizability across diverse scenarios, while large language models (LLMs), despite offering flexibility through natural language interfaces, struggle with structured spatiotemporal data and numerical reasoning in transportation domains. To address these limitations, we propose TransLLM, a unified foundation framework that integrates spatiotemporal modeling with large language models through learnable prompt composition. Our approach features a lightweight spatiotemporal encoder that captures complex dependencies via dilated temporal convolutions and dual-adjacency graph attention networks, seamlessly interfacing with LLMs through structured embeddings. A novel instance-level prompt routing mechanism, trained via reinforcement learning, dynamically personalizes prompts based on input characteristics, moving beyond fixed task-specific templates. The framework operates by encoding spatiotemporal patterns into contextual representations, dynamically composing personalized prompts to guide LLM reasoning, and projecting the resulting representations through specialized output layers to generate task-specific predictions. Experiments across seven datasets and three tasks demonstrate the exceptional effectiveness of TransLLM in both supervised and zero-shot settings. Compared to ten baseline models, it delivers competitive performance on both regression and planning problems, showing strong generalization and cross-task adaptability. Our code is available at this https URL.</li>
</ul>

<h3>Title: A Guide to Stakeholder Analysis for Cybersecurity Researchers</h3>
<ul>
<li><strong>Authors: </strong>James C Davis, Sophie Chen, Huiyun Peng, Paschal C Amusuo, Kelechi G Kalu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14796">https://arxiv.org/abs/2508.14796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14796">https://arxiv.org/pdf/2508.14796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14796]] A Guide to Stakeholder Analysis for Cybersecurity Researchers(https://arxiv.org/abs/2508.14796)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Stakeholder-based ethics analysis is now a formal requirement for submissions to top cybersecurity research venues. This requirement reflects a growing consensus that cybersecurity researchers must go beyond providing capabilities to anticipating and mitigating the potential harms thereof. However, many cybersecurity researchers may be uncertain about how to proceed in an ethics analysis. In this guide, we provide practical support for that requirement by enumerating stakeholder types and mapping them to common empirical research methods. We also offer worked examples to demonstrate how researchers can identify likely stakeholder exposures in real-world projects. Our goal is to help research teams meet new ethics mandates with confidence and clarity, not confusion.</li>
</ul>

<h3>Title: MF-LPR$^2$: Multi-Frame License Plate Image Restoration and Recognition using Optical Flow</h3>
<ul>
<li><strong>Authors: </strong>Kihyun Na, Junseok Oh, Youngkwan Cho, Bumjin Kim, Sungmin Cho, Jinyoung Choi, Injung Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14797">https://arxiv.org/abs/2508.14797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14797">https://arxiv.org/pdf/2508.14797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14797]] MF-LPR$^2$: Multi-Frame License Plate Image Restoration and Recognition using Optical Flow(https://arxiv.org/abs/2508.14797)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>License plate recognition (LPR) is important for traffic law enforcement, crime investigation, and surveillance. However, license plate areas in dash cam images often suffer from low resolution, motion blur, and glare, which make accurate recognition challenging. Existing generative models that rely on pretrained priors cannot reliably restore such poor-quality images, frequently introducing severe artifacts and distortions. To address this issue, we propose a novel multi-frame license plate restoration and recognition framework, MF-LPR$^2$, which addresses ambiguities in poor-quality images by aligning and aggregating neighboring frames instead of relying on pretrained knowledge. To achieve accurate frame alignment, we employ a state-of-the-art optical flow estimator in conjunction with carefully designed algorithms that detect and correct erroneous optical flow estimations by leveraging the spatio-temporal consistency inherent in license plate image sequences. Our approach enhances both image quality and recognition accuracy while preserving the evidential content of the input images. In addition, we constructed a novel Realistic LPR (RLPR) dataset to evaluate MF-LPR$^2$. The RLPR dataset contains 200 pairs of low-quality license plate image sequences and high-quality pseudo ground-truth images, reflecting the complexities of real-world scenarios. In experiments, MF-LPR$^2$ outperformed eight recent restoration models in terms of PSNR, SSIM, and LPIPS by significant margins. In recognition, MF-LPR$^2$ achieved an accuracy of 86.44%, outperforming both the best single-frame LPR (14.04%) and the multi-frame LPR (82.55%) among the eleven baseline models. The results of ablation studies confirm that our filtering and refinement algorithms significantly contribute to these improvements.</li>
</ul>

<h3>Title: Source-Guided Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Zifan Wang, Alice Harting, Matthieu Barreau, Michael M. Zavlanos, Karl H. Johansson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14807">https://arxiv.org/abs/2508.14807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14807">https://arxiv.org/pdf/2508.14807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14807]] Source-Guided Flow Matching(https://arxiv.org/abs/2508.14807)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Guidance of generative models is typically achieved by modifying the probability flow vector field through the addition of a guidance field. In this paper, we instead propose the Source-Guided Flow Matching (SGFM) framework, which modifies the source distribution directly while keeping the pre-trained vector field intact. This reduces the guidance problem to a well-defined problem of sampling from the source distribution. We theoretically show that SGFM recovers the desired target distribution exactly. Furthermore, we provide bounds on the Wasserstein error for the generated distribution when using an approximate sampler of the source distribution and an approximate vector field. The key benefit of our approach is that it allows the user to flexibly choose the sampling method depending on their specific problem. To illustrate this, we systematically compare different sampling methods and discuss conditions for asymptotically exact guidance. Moreover, our framework integrates well with optimal flow matching models since the straight transport map generated by the vector field is preserved. Experimental results on synthetic 2D benchmarks, image datasets, and physics-informed generative tasks demonstrate the effectiveness and flexibility of the proposed framework.</li>
</ul>

<h3>Title: Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization</h3>
<ul>
<li><strong>Authors: </strong>Canyu Zhao, Xiaoman Li, Tianjian Feng, Zhiyue Zhao, Hao Chen, Chunhua Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14811">https://arxiv.org/abs/2508.14811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14811">https://arxiv.org/pdf/2508.14811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14811]] Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization(https://arxiv.org/abs/2508.14811)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Tinker, a versatile framework for high-fidelity 3D editing that operates in both one-shot and few-shot regimes without any per-scene finetuning. Unlike prior techniques that demand extensive per-scene optimization to ensure multi-view consistency or to produce dozens of consistent edited input views, Tinker delivers robust, multi-view consistent edits from as few as one or two images. This capability stems from repurposing pretrained diffusion models, which unlocks their latent 3D awareness. To drive research in this space, we curate the first large-scale multi-view editing dataset and data pipeline, spanning diverse scenes and styles. Building on this dataset, we develop our framework capable of generating multi-view consistent edited views without per-scene training, which consists of two novel components: (1) Referring multi-view editor: Enables precise, reference-driven edits that remain coherent across all viewpoints. (2) Any-view-to-video synthesizer: Leverages spatial-temporal priors from video diffusion to perform high-quality scene completion and novel-view generation even from sparse inputs. Through extensive experiments, Tinker significantly reduces the barrier to generalizable 3D content creation, achieving state-of-the-art performance on editing, novel-view synthesis, and rendering enhancement tasks. We believe that Tinker represents a key step towards truly scalable, zero-shot 3D editing. Project webpage: this https URL</li>
</ul>

<h3>Title: TransLight: Image-Guided Customized Lighting Control with Generative Decoupling</h3>
<ul>
<li><strong>Authors: </strong>Zongming Li, Lianghui Zhu, Haocheng Shen, Longjin Ran, Wenyu Liu, Xinggang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14814">https://arxiv.org/abs/2508.14814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14814">https://arxiv.org/pdf/2508.14814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14814]] TransLight: Image-Guided Customized Lighting Control with Generative Decoupling(https://arxiv.org/abs/2508.14814)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Most existing illumination-editing approaches fail to simultaneously provide customized control of light effects and preserve content integrity. This makes them less effective for practical lighting stylization requirements, especially in the challenging task of transferring complex light effects from a reference image to a user-specified target image. To address this problem, we propose TransLight, a novel framework that enables high-fidelity and high-freedom transfer of light effects. Extracting the light effect from the reference image is the most critical and challenging step in our method. The difficulty lies in the complex geometric structure features embedded in light effects that are highly coupled with content in real-world scenarios. To achieve this, we first present Generative Decoupling, where two fine-tuned diffusion models are used to accurately separate image content and light effects, generating a newly curated, million-scale dataset of image-content-light triplets. Then, we employ IC-Light as the generative model and train our model with our triplets, injecting the reference lighting image as an additional conditioning signal. The resulting TransLight model enables customized and natural transfer of diverse light effects. Notably, by thoroughly disentangling light effects from reference images, our generative decoupling strategy endows TransLight with highly flexible illumination control. Experimental results establish TransLight as the first method to successfully transfer light effects across disparate images, delivering more customized illumination control than existing techniques and charting new directions for research in illumination harmonization and editing.</li>
</ul>

<h3>Title: A Lightweight Privacy-Preserving Smart Metering Billing Protocol with Dynamic Tariff Policy Adjustment</h3>
<ul>
<li><strong>Authors: </strong>Farid Zaredar, Morteza Amini</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14815">https://arxiv.org/abs/2508.14815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14815">https://arxiv.org/pdf/2508.14815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14815]] A Lightweight Privacy-Preserving Smart Metering Billing Protocol with Dynamic Tariff Policy Adjustment(https://arxiv.org/abs/2508.14815)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>The integration of information and communication technology (ICT) with traditional power grids has led to the emergence of smart grids. Advanced metering infrastructure (AMI) plays a crucial role in smart grids by facilitating two-way communication between smart meters and the utility provider. This bidirectional communication allows intelligent meters to report fine-grained consumption data at predefined intervals, enabling accurate billing, efficient grid monitoring and management, and rapid outage detection. However, the collection of detailed consumption data can inadvertently disclose consumers' daily activities, raising privacy concerns and potentially leading to privacy violations. To address these issues and preserve individuals' privacy, we propose a lightweight privacy-preserving smart metering protocol specifically designed to support real-time tariff billing service with dynamic policy adjustment. Our scheme employs an efficient data perturbation technique to obscure precise energy usage data from internal adversaries, including the intermediary gateways and the utility provider. Subsequently, we validate the efficiency and security of our protocol through comprehensive performance and privacy evaluations. We examined the computational, memory, and communication overhead of the proposed scheme. The execution time of our secure and privacy-aware billing system is approximately 3.94540 seconds for a complete year. Furthermore, we employed the Jensen-Shannon divergence as a privacy metric to demonstrate that our protocol can effectively safeguard users' privacy by increasing the noise scale.</li>
</ul>

<h3>Title: Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs</h3>
<ul>
<li><strong>Authors: </strong>Skatje Myers, Dmitriy Dligach, Timothy A. Miller, Samantha Barr, Yanjun Gao, Matthew Churpek, Anoop Mayampurath, Majid Afshar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14817">https://arxiv.org/abs/2508.14817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14817">https://arxiv.org/pdf/2508.14817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14817]] Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs(https://arxiv.org/abs/2508.14817)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Electronic health records (EHRs) are long, noisy, and often redundant, posing a major challenge for the clinicians who must navigate them. Large language models (LLMs) offer a promising solution for extracting and reasoning over this unstructured text, but the length of clinical notes often exceeds even state-of-the-art models' extended context windows. Retrieval-augmented generation (RAG) offers an alternative by retrieving task-relevant passages from across the entire EHR, potentially reducing the amount of required input tokens. In this work, we propose three clinical tasks designed to be replicable across health systems with minimal effort: 1) extracting imaging procedures, 2) generating timelines of antibiotic use, and 3) identifying key diagnoses. Using EHRs from actual hospitalized patients, we test three state-of-the-art LLMs with varying amounts of provided context, using either targeted text retrieval or the most recent clinical notes. We find that RAG closely matches or exceeds the performance of using recent notes, and approaches the performance of using the models' full context while requiring drastically fewer input tokens. Our results suggest that RAG remains a competitive and efficient approach even as newer models become capable of handling increasingly longer amounts of text.</li>
</ul>

<h3>Title: Long Chain-of-Thought Reasoning Across Languages</h3>
<ul>
<li><strong>Authors: </strong>Josh Barua, Seun Eisape, Kayo Yin, Alane Suhr</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14828">https://arxiv.org/abs/2508.14828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14828">https://arxiv.org/pdf/2508.14828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14828]] Long Chain-of-Thought Reasoning Across Languages(https://arxiv.org/abs/2508.14828)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scaling inference through long chains-of-thought (CoTs) has unlocked impressive reasoning capabilities in large language models (LLMs), yet the reasoning process remains almost exclusively English-centric. We construct translated versions of two popular English reasoning datasets, fine-tune Qwen 2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT generation across French, Japanese, Latvian, and Swahili. Our experiments reveal three key findings. First, the efficacy of using English as a pivot language varies by language: it provides no benefit for French, improves performance when used as the reasoning language for Japanese and Latvian, and proves insufficient for Swahili where both task comprehension and reasoning remain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but does not eliminate the cross-lingual performance gap. A lightweight fine-tune using only 1k traces still improves performance by over 30\% in Swahili. Third, data quality versus scale trade-offs are language dependent: small, carefully curated datasets suffice for English and French, whereas larger but noisier corpora prove more effective for Swahili and Latvian. Together, these results clarify when and why long CoTs transfer across languages and provide translated datasets to foster equitable multilingual reasoning research.</li>
</ul>

<h3>Title: On Defining Neural Averaging</h3>
<ul>
<li><strong>Authors: </strong>Su Hyeong Lee, Richard Ngo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14832">https://arxiv.org/abs/2508.14832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14832">https://arxiv.org/pdf/2508.14832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14832]] On Defining Neural Averaging(https://arxiv.org/abs/2508.14832)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free</a></li>
<li><strong>Abstract: </strong>What does it even mean to average neural networks? We investigate the problem of synthesizing a single neural network from a collection of pretrained models, each trained on disjoint data shards, using only their final weights and no access to training data. In forming a definition of neural averaging, we take insight from model soup, which appears to aggregate multiple models into a singular model while enhancing generalization performance. In this work, we reinterpret model souping as a special case of a broader framework: Amortized Model Ensembling (AME) for neural averaging, a data-free meta-optimization approach that treats model differences as pseudogradients to guide neural weight updates. We show that this perspective not only recovers model soup but enables more expressive and adaptive ensembling strategies. Empirically, AME produces averaged neural solutions that outperform both individual experts and model soup baselines, especially in out-of-distribution settings. Our results suggest a principled and generalizable notion of data-free model weight aggregation and defines, in one sense, how to perform neural averaging.</li>
</ul>

<h3>Title: Multimodal Quantum Vision Transformer for Enzyme Commission Classification from Biochemical Representations</h3>
<ul>
<li><strong>Authors: </strong>Murat Isik, Mandeep Kaur Saggi, Humaira Gowher, Sabre Kais</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14844">https://arxiv.org/abs/2508.14844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14844">https://arxiv.org/pdf/2508.14844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14844]] Multimodal Quantum Vision Transformer for Enzyme Commission Classification from Biochemical Representations(https://arxiv.org/abs/2508.14844)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurately predicting enzyme functionality remains one of the major challenges in computational biology, particularly for enzymes with limited structural annotations or sequence homology. We present a novel multimodal Quantum Machine Learning (QML) framework that enhances Enzyme Commission (EC) classification by integrating four complementary biochemical modalities: protein sequence embeddings, quantum-derived electronic descriptors, molecular graph structures, and 2D molecular image representations. Quantum Vision Transformer (QVT) backbone equipped with modality-specific encoders and a unified cross-attention fusion module. By integrating graph features and spatial patterns, our method captures key stereoelectronic interactions behind enzyme function. Experimental results demonstrate that our multimodal QVT model achieves a top-1 accuracy of 85.1%, outperforming sequence-only baselines by a substantial margin and achieving better performance results compared to other QML models.</li>
</ul>

<h3>Title: Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent</h3>
<ul>
<li><strong>Authors: </strong>Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, Xiuwen Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14853">https://arxiv.org/abs/2508.14853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14853">https://arxiv.org/pdf/2508.14853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14853]] Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent(https://arxiv.org/abs/2508.14853)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly deployed in critical applications, ensuring their robustness and safety alignment remains a major challenge. Despite the overall success of alignment techniques such as reinforcement learning from human feedback (RLHF) on typical prompts, LLMs remain vulnerable to jailbreak attacks enabled by crafted adversarial triggers appended to user prompts. Most existing jailbreak methods either rely on inefficient searches over discrete token spaces or direct optimization of continuous embeddings. While continuous embeddings can be given directly to selected open-source models as input, doing so is not feasible for proprietary models. On the other hand, projecting these embeddings back into valid discrete tokens introduces additional complexity and often reduces attack effectiveness. We propose an intrinsic optimization method which directly optimizes relaxed one-hot encodings of the adversarial suffix tokens using exponentiated gradient descent coupled with Bregman projection, ensuring that the optimized one-hot encoding of each token always remains within the probability simplex. We provide theoretical proof of convergence for our proposed method and implement an efficient algorithm that effectively jailbreaks several widely used LLMs. Our method achieves higher success rates and faster convergence compared to three state-of-the-art baselines, evaluated on five open-source LLMs and four adversarial behavior datasets curated for evaluating jailbreak methods. In addition to individual prompt attacks, we also generate universal adversarial suffixes effective across multiple prompts and demonstrate transferability of optimized suffixes to different LLMs.</li>
</ul>

<h3>Title: EventSSEG: Event-driven Self-Supervised Segmentation with Probabilistic Attention</h3>
<ul>
<li><strong>Authors: </strong>Lakshmi Annamalai, Chetan Singh Thakur</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14856">https://arxiv.org/abs/2508.14856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14856">https://arxiv.org/pdf/2508.14856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14856]] EventSSEG: Event-driven Self-Supervised Segmentation with Probabilistic Attention(https://arxiv.org/abs/2508.14856)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Road segmentation is pivotal for autonomous vehicles, yet achieving low latency and low compute solutions using frame based cameras remains a challenge. Event cameras offer a promising alternative. To leverage their low power sensing, we introduce EventSSEG, a method for road segmentation that uses event only computing and a probabilistic attention mechanism. Event only computing poses a challenge in transferring pretrained weights from the conventional camera domain, requiring abundant labeled data, which is scarce. To overcome this, EventSSEG employs event-based self supervised learning, eliminating the need for extensive labeled data. Experiments on DSEC-Semantic and DDD17 show that EventSSEG achieves state of the art performance with minimal labeled events. This approach maximizes event cameras capabilities and addresses the lack of labeled events.</li>
</ul>

<h3>Title: Squeezed Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jyotirmai Singh, Samar Khanna, James Burgess</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14871">https://arxiv.org/abs/2508.14871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14871">https://arxiv.org/pdf/2508.14871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14871]] Squeezed Diffusion Models(https://arxiv.org/abs/2508.14871)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models typically inject isotropic Gaussian noise, disregarding structure in the data. Motivated by the way quantum squeezed states redistribute uncertainty according to the Heisenberg uncertainty principle, we introduce Squeezed Diffusion Models (SDM), which scale noise anisotropically along the principal component of the training distribution. As squeezing enhances the signal-to-noise ratio in physics, we hypothesize that scaling noise in a data-dependent manner can better assist diffusion models in learning important data features. We study two configurations: (i) a Heisenberg diffusion model that compensates the scaling on the principal axis with inverse scaling on orthogonal directions and (ii) a standard SDM variant that scales only the principal axis. Counterintuitively, on CIFAR-10/100 and CelebA-64, mild antisqueezing - i.e. increasing variance on the principal axis - consistently improves FID by up to 15% and shifts the precision-recall frontier toward higher recall. Our results demonstrate that simple, data-aware noise shaping can deliver robust generative gains without architectural changes.</li>
</ul>

<h3>Title: MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework</h3>
<ul>
<li><strong>Authors: </strong>Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Jinjie Gu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14880">https://arxiv.org/abs/2508.14880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14880">https://arxiv.org/pdf/2508.14880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14880]] MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework(https://arxiv.org/abs/2508.14880)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent developments in Large Language Model (LLM)-based agents have shown impressive capabilities spanning multiple domains, exemplified by deep research systems that demonstrate superior performance on complex information-seeking and synthesis tasks. While general-purpose deep research agents have shown impressive capabilities, they struggle significantly with medical domain challenges, as evidenced by leading proprietary systems achieving limited accuracy on complex medical benchmarks. The key limitations are: (1) the model lacks sufficient dense medical knowledge for clinical reasoning, and (2) the framework is constrained by the absence of specialized retrieval tools tailored for medical this http URL present a medical deep research agent that addresses these challenges through two core innovations. First, we develop a novel data synthesis framework using medical knowledge graphs, extracting the longest chains from subgraphs around rare medical entities to generate complex multi-hop question-answer pairs. Second, we integrate a custom-built private medical retrieval engine alongside general-purpose tools, enabling accurate medical information synthesis. Our approach generates 2100+ diverse trajectories across 12 medical specialties, each averaging 4.2 tool this http URL a two-stage training paradigm combining supervised fine-tuning and online reinforcement learning with composite rewards, our MedResearcher-R1-32B model demonstrates exceptional performance, establishing new state-of-the-art results on medical benchmarks while maintaining competitive performance on general deep research tasks. Our work demonstrates that strategic domain-specific innovations in architecture, tool design, and training data construction can enable smaller open-source models to outperform much larger proprietary systems in specialized domains.</li>
</ul>

<h3>Title: MS-CLR: Multi-Skeleton Contrastive Learning for Human Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Mert Kiray, Alvaro Ritter, Nassir Navab, Benjamin Busam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14889">https://arxiv.org/abs/2508.14889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14889">https://arxiv.org/pdf/2508.14889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14889]] MS-CLR: Multi-Skeleton Contrastive Learning for Human Action Recognition(https://arxiv.org/abs/2508.14889)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Contrastive learning has gained significant attention in skeleton-based action recognition for its ability to learn robust representations from unlabeled data. However, existing methods rely on a single skeleton convention, which limits their ability to generalize across datasets with diverse joint structures and anatomical coverage. We propose Multi-Skeleton Contrastive Learning (MS-CLR), a general self-supervised framework that aligns pose representations across multiple skeleton conventions extracted from the same sequence. This encourages the model to learn structural invariances and capture diverse anatomical cues, resulting in more expressive and generalizable features. To support this, we adapt the ST-GCN architecture to handle skeletons with varying joint layouts and scales through a unified representation scheme. Experiments on the NTU RGB+D 60 and 120 datasets demonstrate that MS-CLR consistently improves performance over strong single-skeleton contrastive learning baselines. A multi-skeleton ensemble further boosts performance, setting new state-of-the-art results on both datasets.</li>
</ul>

<h3>Title: GaussianArt: Unified Modeling of Geometry and Motion for Articulated Objects</h3>
<ul>
<li><strong>Authors: </strong>Licheng Shen, Saining Zhang, Honghan Li, Peilin Yang, Zihao Huang, Zongzheng Zhang, Hao Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14891">https://arxiv.org/abs/2508.14891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14891">https://arxiv.org/pdf/2508.14891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14891]] GaussianArt: Unified Modeling of Geometry and Motion for Articulated Objects(https://arxiv.org/abs/2508.14891)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reconstructing articulated objects is essential for building digital twins of interactive environments. However, prior methods typically decouple geometry and motion by first reconstructing object shape in distinct states and then estimating articulation through post-hoc alignment. This separation complicates the reconstruction pipeline and restricts scalability, especially for objects with complex, multi-part articulation. We introduce a unified representation that jointly models geometry and motion using articulated 3D Gaussians. This formulation improves robustness in motion decomposition and supports articulated objects with up to 20 parts, significantly outperforming prior approaches that often struggle beyond 2--3 parts due to brittle initialization. To systematically assess scalability and generalization, we propose MPArt-90, a new benchmark consisting of 90 articulated objects across 20 categories, each with diverse part counts and motion configurations. Extensive experiments show that our method consistently achieves superior accuracy in part-level geometry reconstruction and motion estimation across a broad range of object types. We further demonstrate applicability to downstream tasks such as robotic simulation and human-scene interaction modeling, highlighting the potential of unified articulated representations in scalable physical modeling.</li>
</ul>

<h3>Title: Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs</h3>
<ul>
<li><strong>Authors: </strong>Haokun Lin, Haobo Xu, Yichen Wu, Ziyu Guo, Renrui Zhang, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14896">https://arxiv.org/abs/2508.14896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14896">https://arxiv.org/pdf/2508.14896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14896]] Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs(https://arxiv.org/abs/2508.14896)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion large language models (dLLMs) have introduced a promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantization (PTQ) has emerged as a widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide a foundation for future research in efficient dLLM deployment. All codes and experimental setups will be released to support the community.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
