<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Two Fish Encryption Based Blockchain Technology for Secured Data Storage. (arXiv:2309.11770v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11770">http://arxiv.org/abs/2309.11770</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11770]] Two Fish Encryption Based Blockchain Technology for Secured Data Storage(http://arxiv.org/abs/2309.11770)</code></li>
<li>Summary: <p>Data security and sharing remains nuisance among many applications like
business data, medical data, banking data etc. In this research, block chain
technology is built with encryption algorithm for high level data security in
cloud storage. Medical data security seems critical aspect due to sensitivity
of patient information. Unauthorized access of medical data creates major issue
to patients. This article proposed block chain with hybrid encryption technique
for securing medical data stored in block chain model at cloud storage. New Two
fish encryption model is implemented based on RSA Multiple Precision
Arithmetic. MPA works by using library concept. The objective of using this
methodology is to enhance security performance with less execution time.
Patient data is processed by encryption algorithm and stored at blockchain
infrastructure using encrypted key. Access permission allows user to read or
write the medical data attached in block chain framework. The performance of
traditional cryptographic techniques is very less in providing security
infrastructure.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Information Forensics and Security: A quarter-century-long journey. (arXiv:2309.12159v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12159">http://arxiv.org/abs/2309.12159</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12159]] Information Forensics and Security: A quarter-century-long journey(http://arxiv.org/abs/2309.12159)</code></li>
<li>Summary: <p>Information Forensics and Security (IFS) is an active R&amp;D area whose goal is
to ensure that people use devices, data, and intellectual properties for
authorized purposes and to facilitate the gathering of solid evidence to hold
perpetrators accountable. For over a quarter century since the 1990s, the IFS
research area has grown tremendously to address the societal needs of the
digital information era. The IEEE Signal Processing Society (SPS) has emerged
as an important hub and leader in this area, and the article below celebrates
some landmark technical contributions. In particular, we highlight the major
technological advances on some selected focus areas in the field developed in
the last 25 years from the research community and present future trends.
</p></li>
</ul>

<h3>Title: Towards the Comprehensive Understanding of Mempool DoS Security in Ethereum (Work in Progress). (arXiv:2309.11721v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11721">http://arxiv.org/abs/2309.11721</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11721]] Towards the Comprehensive Understanding of Mempool DoS Security in Ethereum (Work in Progress)(http://arxiv.org/abs/2309.11721)</code></li>
<li>Summary: <p>While awareness has been recently raised on Ethereum mempool security, the
current state of the art lacks a comprehensive understanding of the subject:
The only known attack, DETER (CCS'21), is manually discovered, and it remains
an open problem whether attacks other than DETER exist that disable the mempool
at an asymmetrically low cost.
</p>
<p>In this paper, we propose automatic exploit generation techniques to discover
new mempool-DoS attack. By employing model checking, we discover a new attack
pattern beyond DETER. By further leveraging attack synthesis techniques, we
generate exploits from the patterns to adaptively bypass defenses adopted in
real Ethereum clients. Our evaluation result shows that while the recent
Ethereum clients (e.g., Geth V1.10.14 and OpenEthereum V3.3.5) have mitigated
the existing DETER attacks, they are vulnerable to the newly discovered attacks
that achieve high success rates (88% - 96%) and low costs (as low as zero
Gas/Ether).
</p></li>
</ul>

<h3>Title: Full mesh networking technology with peer to peer grid topology based on variable parameter full dimensional space. (arXiv:2309.11903v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11903">http://arxiv.org/abs/2309.11903</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11903]] Full mesh networking technology with peer to peer grid topology based on variable parameter full dimensional space(http://arxiv.org/abs/2309.11903)</code></li>
<li>Summary: <p>The continuous development of computer network technology has accelerated the
pace of informatization, and at the same time, network security issues are
becoming increasingly prominent. Networking technology with different network
topologies is one of the important means to solve network security problems.
The security of VPN is based on the division of geographical boundaries, but
the granularity is relatively coarse, which is difficult to cope with the
dynamic changes of the security situation. Zero trust network solves the VPN
problem through peer to peer authorization and continuous verification, but
most of the solutions use a central proxy device, resulting in the central node
becoming the bottleneck of the network. This paper put forward the hard-Nat
traversal formula based on the birthday paradox, which solves the long-standing
problem of hard NAT traversal. A full mesh networking mechanism with variable
parameter full-dimensional spatial peer-to-peer grid topology was proposed,
which covers all types of networking schemes and achieve peer-2-peer resource
interconnection on both methodological and engineering level.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Towards Differential Privacy in Sequential Recommendation: A Noisy Graph Neural Network Approach. (arXiv:2309.11515v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11515">http://arxiv.org/abs/2309.11515</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11515]] Towards Differential Privacy in Sequential Recommendation: A Noisy Graph Neural Network Approach(http://arxiv.org/abs/2309.11515)</code></li>
<li>Summary: <p>With increasing frequency of high-profile privacy breaches in various online
platforms, users are becoming more concerned about their privacy. And
recommender system is the core component of online platforms for providing
personalized service, consequently, its privacy preservation has attracted
great attention. As the gold standard of privacy protection, differential
privacy has been widely adopted to preserve privacy in recommender systems.
However, existing differentially private recommender systems only consider
static and independent interactions, so they cannot apply to sequential
recommendation where behaviors are dynamic and dependent. Meanwhile, little
attention has been paid on the privacy risk of sensitive user features, most of
them only protect user feedbacks. In this work, we propose a novel
DIfferentially Private Sequential recommendation framework with a noisy Graph
Neural Network approach (denoted as DIPSGNN) to address these limitations. To
the best of our knowledge, we are the first to achieve differential privacy in
sequential recommendation with dependent interactions. Specifically, in
DIPSGNN, we first leverage piecewise mechanism to protect sensitive user
features. Then, we innovatively add calibrated noise into aggregation step of
graph neural network based on aggregation perturbation mechanism. And this
noisy graph neural network can protect sequentially dependent interactions and
capture user preferences simultaneously. Extensive experiments demonstrate the
superiority of our method over state-of-the-art differentially private
recommender systems in terms of better balance between privacy and accuracy.
</p></li>
</ul>

<h3>Title: CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches. (arXiv:2309.11587v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11587">http://arxiv.org/abs/2309.11587</a></li>
<li>Code URL: https://github.com/geods/cats</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11587]] CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches(http://arxiv.org/abs/2309.11587)</code></li>
<li>Summary: <p>The prevalence of ubiquitous location-aware devices and mobile Internet
enables us to collect massive individual-level trajectory dataset from users.
Such trajectory big data bring new opportunities to human mobility research but
also raise public concerns with regard to location privacy. In this work, we
present the Conditional Adversarial Trajectory Synthesis (CATS), a
deep-learning-based GeoAI methodological framework for privacy-preserving
trajectory data generation and publication. CATS applies K-anonymity to the
underlying spatiotemporal distributions of human movements, which provides a
distributional-level strong privacy guarantee. By leveraging conditional
adversarial training on K-anonymized human mobility matrices, trajectory global
context learning using the attention-based mechanism, and recurrent bipartite
graph matching of adjacent trajectory points, CATS is able to reconstruct
trajectory topology from conditionally sampled locations and generate
high-quality individual-level synthetic trajectory data, which can serve as
supplements or alternatives to raw data for privacy-preserving trajectory data
publication. The experiment results on over 90k GPS trajectories show that our
method has a better performance in privacy preservation, spatiotemporal
characteristic preservation, and downstream utility compared with baseline
methods, which brings new insights into privacy-preserving human mobility
research using generative AI techniques and explores data ethics issues in
GIScience.
</p></li>
</ul>

<h3>Title: Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation. (arXiv:2309.11765v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11765">http://arxiv.org/abs/2309.11765</a></li>
<li>Code URL: https://github.com/microsoft/dp-few-shot-generation</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11765]] Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation(http://arxiv.org/abs/2309.11765)</code></li>
<li>Summary: <p>We study the problem of in-context learning (ICL) with large language models
(LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak
or regurgitate the private examples demonstrated in the prompt. We propose a
novel algorithm that generates synthetic few-shot demonstrations from the
private dataset with formal differential privacy (DP) guarantees, and show
empirically that it can achieve effective ICL. We conduct extensive experiments
on standard benchmarks and compare our algorithm with non-private ICL and
zero-shot solutions. Our results demonstrate that our algorithm can achieve
competitive performance with strong privacy levels. These results open up new
possibilities for ICL with privacy protection for a broad range of
applications.
</p></li>
</ul>

<h3>Title: S-GBDT: Frugal Differentially Private Gradient Boosting Decision Trees. (arXiv:2309.12041v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12041">http://arxiv.org/abs/2309.12041</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12041]] S-GBDT: Frugal Differentially Private Gradient Boosting Decision Trees(http://arxiv.org/abs/2309.12041)</code></li>
<li>Summary: <p>Privacy-preserving learning of gradient boosting decision trees (GBDT) has
the potential for strong utility-privacy tradeoffs for tabular data, such as
census data or medical meta data: classical GBDT learners can extract
non-linear patterns from small sized datasets. The state-of-the-art notion for
provable privacy-properties is differential privacy, which requires that the
impact of single data points is limited and deniable. We introduce a novel
differentially private GBDT learner and utilize four main techniques to improve
the utility-privacy tradeoff. (1) We use an improved noise scaling approach
with tighter accounting of privacy leakage of a decision tree leaf compared to
prior work, resulting in noise that in expectation scales with $O(1/n)$, for
$n$ data points. (2) We integrate individual R\'enyi filters to our method to
learn from data points that have been underutilized during an iterative
training process, which -- potentially of independent interest -- results in a
natural yet effective insight to learning streams of non-i.i.d. data. (3) We
incorporate the concept of random decision tree splits to concentrate privacy
budget on learning leaves. (4) We deploy subsampling for privacy amplification.
Our evaluation shows for the Abalone dataset ($&lt;4k$ training data points) a
$R^2$-score of $0.39$ for $\varepsilon=0.15$, which the closest prior work only
achieved for $\varepsilon=10.0$. On the Adult dataset ($50k$ training data
points) we achieve test error of $18.7\,\%$ for $\varepsilon=0.07$ which the
closest prior work only achieved for $\varepsilon=1.0$. For the Abalone dataset
for $\varepsilon=0.54$ we achieve $R^2$-score of $0.47$ which is very close to
the $R^2$-score of $0.54$ for the nonprivate version of GBDT. For the Adult
dataset for $\varepsilon=0.54$ we achieve test error $17.1\,\%$ which is very
close to the test error $13.7\,\%$ of the nonprivate version of GBDT.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Understanding Pose and Appearance Disentanglement in 3D Human Pose Estimation. (arXiv:2309.11667v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11667">http://arxiv.org/abs/2309.11667</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11667]] Understanding Pose and Appearance Disentanglement in 3D Human Pose Estimation(http://arxiv.org/abs/2309.11667)</code></li>
<li>Summary: <p>As 3D human pose estimation can now be achieved with very high accuracy in
the supervised learning scenario, tackling the case where 3D pose annotations
are not available has received increasing attention. In particular, several
methods have proposed to learn image representations in a self-supervised
fashion so as to disentangle the appearance information from the pose one. The
methods then only need a small amount of supervised data to train a pose
regressor using the pose-related latent vector as input, as it should be free
of appearance information. In this paper, we carry out in-depth analysis to
understand to what degree the state-of-the-art disentangled representation
learning methods truly separate the appearance information from the pose one.
First, we study disentanglement from the perspective of the self-supervised
network, via diverse image synthesis experiments. Second, we investigate
disentanglement with respect to the 3D pose regressor following an adversarial
attack perspective. Specifically, we design an adversarial strategy focusing on
generating natural appearance changes of the subject, and against which we
could expect a disentangled network to be robust. Altogether, our analyses show
that disentanglement in the three state-of-the-art disentangled representation
learning frameworks if far from complete, and that their pose codes contain
significant appearance information. We believe that our approach provides a
valuable testbed to evaluate the degree of disentanglement of pose from
appearance in self-supervised 3D human pose estimation.
</p></li>
</ul>

<h3>Title: How Robust is Google's Bard to Adversarial Image Attacks?. (arXiv:2309.11751v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11751">http://arxiv.org/abs/2309.11751</a></li>
<li>Code URL: https://github.com/thu-ml/attack-bard</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11751]] How Robust is Google's Bard to Adversarial Image Attacks?(http://arxiv.org/abs/2309.11751)</code></li>
<li>Summary: <p>Multimodal Large Language Models (MLLMs) that integrate text and other
modalities (especially vision) have achieved unprecedented performance in
various multimodal tasks. However, due to the unsolved adversarial robustness
problem of vision models, MLLMs can have more severe safety and security risks
by introducing the vision inputs. In this work, we study the adversarial
robustness of Google's Bard, a competitive chatbot to ChatGPT that released its
multimodal capability recently, to better understand the vulnerabilities of
commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs,
the generated adversarial examples can mislead Bard to output wrong image
descriptions with a 22% success rate based solely on the transferability. We
show that the adversarial examples can also attack other MLLMs, e.g., a 26%
attack success rate against Bing Chat and a 86% attack success rate against
ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face
detection and toxicity detection of images. We design corresponding attacks to
evade these defenses, demonstrating that the current defenses of Bard are also
vulnerable. We hope this work can deepen our understanding on the robustness of
MLLMs and facilitate future research on defenses. Our code is available at
https://github.com/thu-ml/Attack-Bard.
</p></li>
</ul>

<h3>Title: Dictionary Attack on IMU-based Gait Authentication. (arXiv:2309.11766v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11766">http://arxiv.org/abs/2309.11766</a></li>
<li>Code URL: https://github.com/rajeshjnu2006/dictionaryattackonimugait</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11766]] Dictionary Attack on IMU-based Gait Authentication(http://arxiv.org/abs/2309.11766)</code></li>
<li>Summary: <p>We present a novel adversarial model for authentication systems that use gait
patterns recorded by the inertial measurement unit (IMU) built into
smartphones. The attack idea is inspired by and named after the concept of a
dictionary attack on knowledge (PIN or password) based authentication systems.
In particular, this work investigates whether it is possible to build a
dictionary of IMUGait patterns and use it to launch an attack or find an
imitator who can actively reproduce IMUGait patterns that match the target's
IMUGait pattern. Nine physically and demographically diverse individuals walked
at various levels of four predefined controllable and adaptable gait factors
(speed, step length, step width, and thigh-lift), producing 178 unique IMUGait
patterns. Each pattern attacked a wide variety of user authentication models.
The deeper analysis of error rates (before and after the attack) challenges the
belief that authentication systems based on IMUGait patterns are the most
difficult to spoof; further research is needed on adversarial models and
associated countermeasures.
</p></li>
</ul>

<h3>Title: Vulnerability of 3D Face Recognition Systems to Morphing Attacks. (arXiv:2309.12118v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12118">http://arxiv.org/abs/2309.12118</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12118]] Vulnerability of 3D Face Recognition Systems to Morphing Attacks(http://arxiv.org/abs/2309.12118)</code></li>
<li>Summary: <p>In recent years face recognition systems have been brought to the mainstream
due to development in hardware and software. Consistent efforts are being made
to make them better and more secure. This has also brought developments in 3D
face recognition systems at a rapid pace. These 3DFR systems are expected to
overcome certain vulnerabilities of 2DFR systems. One such problem that the
domain of 2DFR systems face is face image morphing. A substantial amount of
research is being done for generation of high quality face morphs along with
detection of attacks from these morphs. Comparatively the understanding of
vulnerability of 3DFR systems against 3D face morphs is less. But at the same
time an expectation is set from 3DFR systems to be more robust against such
attacks. This paper attempts to research and gain more information on this
matter. The paper describes a couple of methods that can be used to generate 3D
face morphs. The face morphs that are generated using this method are then
compared to the contributing faces to obtain similarity scores. The highest
MMPMR is obtained around 40% with RMMR of 41.76% when 3DFRS are attacked with
look-a-like morphs.
</p></li>
</ul>

<h3>Title: A Chinese Prompt Attack Dataset for LLMs with Evil Content. (arXiv:2309.11830v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11830">http://arxiv.org/abs/2309.11830</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11830]] A Chinese Prompt Attack Dataset for LLMs with Evil Content(http://arxiv.org/abs/2309.11830)</code></li>
<li>Summary: <p>Large Language Models (LLMs) present significant priority in text
understanding and generation. However, LLMs suffer from the risk of generating
harmful contents especially while being employed to applications. There are
several black-box attack methods, such as Prompt Attack, which can change the
behaviour of LLMs and induce LLMs to generate unexpected answers with harmful
contents. Researchers are interested in Prompt Attack and Defense with LLMs,
while there is no publicly available dataset to evaluate the abilities of
defending prompt attack. In this paper, we introduce a Chinese Prompt Attack
Dataset for LLMs, called CPAD. Our prompts aim to induce LLMs to generate
unexpected outputs with several carefully designed prompt attack approaches and
widely concerned attacking contents. Different from previous datasets involving
safety estimation, We construct the prompts considering three dimensions:
contents, attacking methods and goals, thus the responses can be easily
evaluated and analysed. We run several well-known Chinese LLMs on our dataset,
and the results show that our prompts are significantly harmful to LLMs, with
around 70% attack success rate. We will release CPAD to encourage further
studies on prompt attack and defense.
</p></li>
</ul>

<h3>Title: The supersingular endomorphism ring problem given one endomorphism. (arXiv:2309.11912v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11912">http://arxiv.org/abs/2309.11912</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11912]] The supersingular endomorphism ring problem given one endomorphism(http://arxiv.org/abs/2309.11912)</code></li>
<li>Summary: <p>Given a supersingular elliptic curve E and a non-scalar endomorphism $\alpha$
of E, we prove that the endomorphism ring of E can be computed in classical
time about disc(Z[$\alpha$])^1/4 , and in quantum subexponential time, assuming
the generalised Riemann hypothesis. Previous results either had higher
complexities, or relied on heuristic assumptions. Along the way, we prove that
the Primitivisation problem can be solved in polynomial time (a problem
previously believed to be hard), and we prove that the action of smooth ideals
on oriented elliptic curves can be computed in polynomial time (previous
results of this form required the ideal to be powersmooth, i.e., not divisible
by any large prime power). Following the attacks on SIDH, isogenies in high
dimension are a central ingredient of our results.
</p></li>
</ul>

<h3>Title: Generic Selfish Mining MDP for DAG Protocols. (arXiv:2309.11924v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11924">http://arxiv.org/abs/2309.11924</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11924]] Generic Selfish Mining MDP for DAG Protocols(http://arxiv.org/abs/2309.11924)</code></li>
<li>Summary: <p>Selfish Mining is strategic rule-breaking to maximize rewards in
proof-of-work protocols [3] and Markov Decision Processes (MDPs) are the
preferred tool for finding optimal strategies in Bitcoin [4, 10] and similar
linear chain protocols [12]. Protocols increasingly adopt non-sequential chain
structures [11], for which MDP analysis is more involved [2]. To date,
researchers have tailored specific attack spaces for each protocol [2, 4, 5, 7,
10, 12]. Assumptions differ, and validating and comparing results is difficult.
To overcome this, we propose a generic attack space that supports the wide
class of DAG protocols that provide a total ordering of blocks [11], e. g.,
Ethereum, Fruitchains, and Parallel Proof-of-Work. Our approach is modular: we
specify each protocol as one program, and then derive the Selfish Mining MDPs
automatically.
</p></li>
</ul>

<h3>Title: De-authentication using Ambient Light Sensor. (arXiv:2309.12220v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12220">http://arxiv.org/abs/2309.12220</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12220]] De-authentication using Ambient Light Sensor(http://arxiv.org/abs/2309.12220)</code></li>
<li>Summary: <p>While user authentication happens before initiating or resuming a login
session, de-authentication detects the absence of a previously-authenticated
user to revoke her currently active login session. The absence of proper
de-authentication can lead to well-known lunchtime attacks, where a nearby
adversary takes over a carelessly departed user's running login session. The
existing solutions for automatic de-authentication have distinct practical
limitations, e.g., extraordinary deployment requirements or high initial cost
of external equipment.
</p>
<p>In this paper, we propose "DE-authentication using Ambient Light sensor"
(DEAL), a novel, inexpensive, fast, and user-friendly de-authentication
approach. DEAL utilizes the built-in ambient light sensor of a modern computer
to determine if the user is leaving her work-desk. DEAL, by design, is
resilient to natural shifts in lighting conditions and can be configured to
handle abrupt changes in ambient illumination (e.g., due to toggling of room
lights). We collected data samples from 4800 sessions with 120 volunteers in 4
typical workplace settings and conducted a series of experiments to evaluate
the quality of our proposed approach thoroughly. Our results show that DEAL can
de-authenticate a departing user within 4 seconds with a hit rate of 89.15% and
a fall-out of 7.35%. Finally, bypassing DEAL to launch a lunchtime attack is
practically infeasible as it requires the attacker to either take the user's
position within a few seconds or manipulate the sensor readings sophisticatedly
in real-time.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Neural Image Compression Using Masked Sparse Visual Representation. (arXiv:2309.11661v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11661">http://arxiv.org/abs/2309.11661</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11661]] Neural Image Compression Using Masked Sparse Visual Representation(http://arxiv.org/abs/2309.11661)</code></li>
<li>Summary: <p>We study neural image compression based on the Sparse Visual Representation
(SVR), where images are embedded into a discrete latent space spanned by
learned visual codebooks. By sharing codebooks with the decoder, the encoder
transfers integer codeword indices that are efficient and cross-platform
robust, and the decoder retrieves the embedded latent feature using the indices
for reconstruction. Previous SVR-based compression lacks effective mechanism
for rate-distortion tradeoffs, where one can only pursue either high
reconstruction quality or low transmission bitrate. We propose a Masked
Adaptive Codebook learning (M-AdaCode) method that applies masks to the latent
feature subspace to balance bitrate and reconstruction quality. A set of
semantic-class-dependent basis codebooks are learned, which are weighted
combined to generate a rich latent feature for high-quality reconstruction. The
combining weights are adaptively derived from each input image, providing
fidelity information with additional transmission costs. By masking out
unimportant weights in the encoder and recovering them in the decoder, we can
trade off reconstruction quality for transmission bits, and the masking rate
controls the balance between bitrate and distortion. Experiments over the
standard JPEG-AI dataset demonstrate the effectiveness of our M-AdaCode
approach.
</p></li>
</ul>

<h3>Title: ContextRef: Evaluating Referenceless Metrics For Image Description Generation. (arXiv:2309.11710v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11710">http://arxiv.org/abs/2309.11710</a></li>
<li>Code URL: https://github.com/elisakreiss/contextref</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11710]] ContextRef: Evaluating Referenceless Metrics For Image Description Generation(http://arxiv.org/abs/2309.11710)</code></li>
<li>Summary: <p>Referenceless metrics (e.g., CLIPScore) use pretrained vision--language
models to assess image descriptions directly without costly ground-truth
reference texts. Such methods can facilitate rapid progress, but only if they
truly align with human preference judgments. In this paper, we introduce
ContextRef, a benchmark for assessing referenceless metrics for such alignment.
ContextRef has two components: human ratings along a variety of established
quality dimensions, and ten diverse robustness checks designed to uncover
fundamental weaknesses. A crucial aspect of ContextRef is that images and
descriptions are presented in context, reflecting prior work showing that
context is important for description quality. Using ContextRef, we assess a
variety of pretrained models, scoring functions, and techniques for
incorporating context. None of the methods is successful with ContextRef, but
we show that careful fine-tuning yields substantial improvements. ContextRef
remains a challenging benchmark though, in large part due to the challenge of
context dependence.
</p></li>
</ul>

<h3>Title: A Real-Time Multi-Task Learning System for Joint Detection of Face, Facial Landmark and Head Pose. (arXiv:2309.11773v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11773">http://arxiv.org/abs/2309.11773</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11773]] A Real-Time Multi-Task Learning System for Joint Detection of Face, Facial Landmark and Head Pose(http://arxiv.org/abs/2309.11773)</code></li>
<li>Summary: <p>Extreme head postures pose a common challenge across a spectrum of facial
analysis tasks, including face detection, facial landmark detection (FLD), and
head pose estimation (HPE). These tasks are interdependent, where accurate FLD
relies on robust face detection, and HPE is intricately associated with these
key points. This paper focuses on the integration of these tasks, particularly
when addressing the complexities posed by large-angle face poses. The primary
contribution of this study is the proposal of a real-time multi-task detection
system capable of simultaneously performing joint detection of faces, facial
landmarks, and head poses. This system builds upon the widely adopted YOLOv8
detection framework. It extends the original object detection head by
incorporating additional landmark regression head, enabling efficient
localization of crucial facial landmarks. Furthermore, we conduct optimizations
and enhancements on various modules within the original YOLOv8 framework. To
validate the effectiveness and real-time performance of our proposed model, we
conduct extensive experiments on 300W-LP and AFLW2000-3D datasets. The results
obtained verify the capability of our model to tackle large-angle face pose
challenges while delivering real-time performance across these interconnected
tasks.
</p></li>
</ul>

<h3>Title: MEFLUT: Unsupervised 1D Lookup Tables for Multi-exposure Image Fusion. (arXiv:2309.11847v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11847">http://arxiv.org/abs/2309.11847</a></li>
<li>Code URL: https://github.com/hedlen/meflut</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11847]] MEFLUT: Unsupervised 1D Lookup Tables for Multi-exposure Image Fusion(http://arxiv.org/abs/2309.11847)</code></li>
<li>Summary: <p>In this paper, we introduce a new approach for high-quality multi-exposure
image fusion (MEF). We show that the fusion weights of an exposure can be
encoded into a 1D lookup table (LUT), which takes pixel intensity value as
input and produces fusion weight as output. We learn one 1D LUT for each
exposure, then all the pixels from different exposures can query 1D LUT of that
exposure independently for high-quality and efficient fusion. Specifically, to
learn these 1D LUTs, we involve attention mechanism in various dimensions
including frame, channel and spatial ones into the MEF task so as to bring us
significant quality improvement over the state-of-the-art (SOTA). In addition,
we collect a new MEF dataset consisting of 960 samples, 155 of which are
manually tuned by professionals as ground-truth for evaluation. Our network is
trained by this dataset in an unsupervised manner. Extensive experiments are
conducted to demonstrate the effectiveness of all the newly proposed
components, and results show that our approach outperforms the SOTA in our and
another representative dataset SICE, both qualitatively and quantitatively.
Moreover, our 1D LUT approach takes less than 4ms to run a 4K image on a PC
GPU. Given its high quality, efficiency and robustness, our method has been
shipped into millions of Android mobiles across multiple brands world-wide.
Code is available at: https://github.com/Hedlen/MEFLUT.
</p></li>
</ul>

<h3>Title: On-the-Fly SfM: What you capture is What you get. (arXiv:2309.11883v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11883">http://arxiv.org/abs/2309.11883</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11883]] On-the-Fly SfM: What you capture is What you get(http://arxiv.org/abs/2309.11883)</code></li>
<li>Summary: <p>Over the last decades, ample achievements have been made on Structure from
motion (SfM). However, the vast majority of them basically work in an offline
manner, i.e., images are firstly captured and then fed together into a SfM
pipeline for obtaining poses and sparse point cloud. In this work, on the
contrary, we present an on-the-fly SfM: running online SfM while image
capturing, the newly taken On-the-Fly image is online estimated with the
corresponding pose and points, i.e., what you capture is what you get.
Specifically, our approach firstly employs a vocabulary tree that is
unsupervised trained using learning-based global features for fast image
retrieval of newly fly-in image. Then, a robust feature matching mechanism with
least squares (LSM) is presented to improve image registration performance.
Finally, via investigating the influence of newly fly-in image's connected
neighboring images, an efficient hierarchical weighted local bundle adjustment
(BA) is used for optimization. Extensive experimental results demonstrate that
on-the-fly SfM can meet the goal of robustly registering the images while
capturing in an online way.
</p></li>
</ul>

<h3>Title: Unlocking the Heart Using Adaptive Locked Agnostic Networks. (arXiv:2309.11899v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11899">http://arxiv.org/abs/2309.11899</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11899]] Unlocking the Heart Using Adaptive Locked Agnostic Networks(http://arxiv.org/abs/2309.11899)</code></li>
<li>Summary: <p>Supervised training of deep learning models for medical imaging applications
requires a significant amount of labeled data. This is posing a challenge as
the images are required to be annotated by medical professionals. To address
this limitation, we introduce the Adaptive Locked Agnostic Network (ALAN), a
concept involving self-supervised visual feature extraction using a large
backbone model to produce anatomically robust semantic self-segmentation. In
the ALAN methodology, this self-supervised training occurs only once on a large
and diverse dataset. Due to the intuitive interpretability of the segmentation,
downstream models tailored for specific tasks can be easily designed using
white-box models with few parameters. This, in turn, opens up the possibility
of communicating the inner workings of a model with domain experts and
introducing prior knowledge into it. It also means that the downstream models
become less data-hungry compared to fully supervised approaches. These
characteristics make ALAN particularly well-suited for resource-scarce
scenarios, such as costly clinical trials and rare diseases. In this paper, we
apply the ALAN approach to three publicly available echocardiography datasets:
EchoNet-Dynamic, CAMUS, and TMED-2. Our findings demonstrate that the
self-supervised backbone model robustly identifies anatomical subregions of the
heart in an apical four-chamber view. Building upon this, we design two
downstream models, one for segmenting a target anatomical region, and a second
for echocardiogram view classification.
</p></li>
</ul>

<h3>Title: Self-Calibrating, Fully Differentiable NLOS Inverse Rendering. (arXiv:2309.12047v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12047">http://arxiv.org/abs/2309.12047</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12047]] Self-Calibrating, Fully Differentiable NLOS Inverse Rendering(http://arxiv.org/abs/2309.12047)</code></li>
<li>Summary: <p>Existing time-resolved non-line-of-sight (NLOS) imaging methods reconstruct
hidden scenes by inverting the optical paths of indirect illumination measured
at visible relay surfaces. These methods are prone to reconstruction artifacts
due to inversion ambiguities and capture noise, which are typically mitigated
through the manual selection of filtering functions and parameters. We
introduce a fully-differentiable end-to-end NLOS inverse rendering pipeline
that self-calibrates the imaging parameters during the reconstruction of hidden
scenes, using as input only the measured illumination while working both in the
time and frequency domains. Our pipeline extracts a geometric representation of
the hidden scene from NLOS volumetric intensities and estimates the
time-resolved illumination at the relay wall produced by such geometric
information using differentiable transient rendering. We then use gradient
descent to optimize imaging parameters by minimizing the error between our
simulated time-resolved illumination and the measured illumination. Our
end-to-end differentiable pipeline couples diffraction-based volumetric NLOS
reconstruction with path-space light transport and a simple ray marching
technique to extract detailed, dense sets of surface points and normals of
hidden scenes. We demonstrate the robustness of our method to consistently
reconstruct geometry and albedo, even under significant noise levels.
</p></li>
</ul>

<h3>Title: Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer -- Current Trends and Research Perspectives. (arXiv:2309.12067v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12067">http://arxiv.org/abs/2309.12067</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12067]] Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer -- Current Trends and Research Perspectives(http://arxiv.org/abs/2309.12067)</code></li>
<li>Summary: <p>Action scene understanding in soccer is a challenging task due to the complex
and dynamic nature of the game, as well as the interactions between players.
This article provides a comprehensive overview of this task divided into action
recognition, spotting, and spatio-temporal action localization, with a
particular emphasis on the modalities used and multimodal methods. We explore
the publicly available data sources and metrics used to evaluate models'
performance. The article reviews recent state-of-the-art methods that leverage
deep learning techniques and traditional methods. We focus on multimodal
methods, which integrate information from multiple sources, such as video and
audio data, and also those that represent one source in various ways. The
advantages and limitations of methods are discussed, along with their potential
for improving the accuracy and robustness of models. Finally, the article
highlights some of the open research questions and future directions in the
field of soccer action recognition, including the potential for multimodal
methods to advance this field. Overall, this survey provides a valuable
resource for researchers interested in the field of action scene understanding
in soccer.
</p></li>
</ul>

<h3>Title: ORTexME: Occlusion-Robust Human Shape and Pose via Temporal Average Texture and Mesh Encoding. (arXiv:2309.12183v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12183">http://arxiv.org/abs/2309.12183</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12183]] ORTexME: Occlusion-Robust Human Shape and Pose via Temporal Average Texture and Mesh Encoding(http://arxiv.org/abs/2309.12183)</code></li>
<li>Summary: <p>In 3D human shape and pose estimation from a monocular video, models trained
with limited labeled data cannot generalize well to videos with occlusion,
which is common in the wild videos. The recent human neural rendering
approaches focusing on novel view synthesis initialized by the off-the-shelf
human shape and pose methods have the potential to correct the initial human
shape. However, the existing methods have some drawbacks such as, erroneous in
handling occlusion, sensitive to inaccurate human segmentation, and ineffective
loss computation due to the non-regularized opacity field. To address these
problems, we introduce ORTexME, an occlusion-robust temporal method that
utilizes temporal information from the input video to better regularize the
occluded body parts. While our ORTexME is based on NeRF, to determine the
reliable regions for the NeRF ray sampling, we utilize our novel average
texture learning approach to learn the average appearance of a person, and to
infer a mask based on the average texture. In addition, to guide the
opacity-field updates in NeRF to suppress blur and noise, we propose the use of
human body mesh. The quantitative evaluation demonstrates that our method
achieves significant improvement on the challenging multi-person 3DPW dataset,
where our method achieves 1.8 P-MPJPE error reduction. The SOTA rendering-based
methods fail and enlarge the error up to 5.6 on the same dataset.
</p></li>
</ul>

<h3>Title: Can We Reliably Improve the Robustness to Image Acquisition of Remote Sensing of PV Systems?. (arXiv:2309.12214v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12214">http://arxiv.org/abs/2309.12214</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12214]] Can We Reliably Improve the Robustness to Image Acquisition of Remote Sensing of PV Systems?(http://arxiv.org/abs/2309.12214)</code></li>
<li>Summary: <p>Photovoltaic (PV) energy is crucial for the decarbonization of energy
systems. Due to the lack of centralized data, remote sensing of rooftop PV
installations is the best option to monitor the evolution of the rooftop PV
installed fleet at a regional scale. However, current techniques lack
reliability and are notably sensitive to shifts in the acquisition conditions.
To overcome this, we leverage the wavelet scale attribution method (WCAM),
which decomposes a model's prediction in the space-scale domain. The WCAM
enables us to assess on which scales the representation of a PV model rests and
provides insights to derive methods that improve the robustness to acquisition
conditions, thus increasing trust in deep learning systems to encourage their
use for the safe integration of clean energy in electric systems.
</p></li>
</ul>

<h3>Title: Environment-biased Feature Ranking for Novelty Detection Robustness. (arXiv:2309.12301v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12301">http://arxiv.org/abs/2309.12301</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12301]] Environment-biased Feature Ranking for Novelty Detection Robustness(http://arxiv.org/abs/2309.12301)</code></li>
<li>Summary: <p>We tackle the problem of robust novelty detection, where we aim to detect
novelties in terms of semantic content while being invariant to changes in
other, irrelevant factors. Specifically, we operate in a setup with multiple
environments, where we determine the set of features that are associated more
with the environments, rather than to the content relevant for the task. Thus,
we propose a method that starts with a pretrained embedding and a multi-env
setup and manages to rank the features based on their environment-focus. First,
we compute a per-feature score based on the feature distribution variance
between envs. Next, we show that by dropping the highly scored ones, we manage
to remove spurious correlations and improve the overall performance by up to
6%, both in covariance and sub-population shift cases, both for a real and a
synthetic benchmark, that we introduce for this task.
</p></li>
</ul>

<h3>Title: Incorporating Singletons and Mention-based Features in Coreference Resolution via Multi-task Learning for Better Generalization. (arXiv:2309.11582v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11582">http://arxiv.org/abs/2309.11582</a></li>
<li>Code URL: https://github.com/yilunzhu/coref-mtl</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11582]] Incorporating Singletons and Mention-based Features in Coreference Resolution via Multi-task Learning for Better Generalization(http://arxiv.org/abs/2309.11582)</code></li>
<li>Summary: <p>Previous attempts to incorporate a mention detection step into end-to-end
neural coreference resolution for English have been hampered by the lack of
singleton mention span data as well as other entity information. This paper
presents a coreference model that learns singletons as well as features such as
entity type and information status via a multi-task learning-based approach.
This approach achieves new state-of-the-art scores on the OntoGUM benchmark
(+2.7 points) and increases robustness on multiple out-of-domain datasets (+2.3
points on average), likely due to greater generalizability for mention
detection and utilization of more data from singletons when compared to only
coreferent mention pair matching.
</p></li>
</ul>

<h3>Title: Word Embedding with Neural Probabilistic Prior. (arXiv:2309.11824v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11824">http://arxiv.org/abs/2309.11824</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11824]] Word Embedding with Neural Probabilistic Prior(http://arxiv.org/abs/2309.11824)</code></li>
<li>Summary: <p>To improve word representation learning, we propose a probabilistic prior
which can be seamlessly integrated with word embedding models. Different from
previous methods, word embedding is taken as a probabilistic generative model,
and it enables us to impose a prior regularizing word representation learning.
The proposed prior not only enhances the representation of embedding vectors
but also improves the model's robustness and stability. The structure of the
proposed prior is simple and effective, and it can be easily implemented and
flexibly plugged in most existing word embedding models. Extensive experiments
show the proposed method improves word representation on various tasks.
</p></li>
</ul>

<h3>Title: Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics. (arXiv:2309.11981v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11981">http://arxiv.org/abs/2309.11981</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11981]] Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics(http://arxiv.org/abs/2309.11981)</code></li>
<li>Summary: <p>In the burgeoning field of artificial intelligence (AI), the unprecedented
progress of large language models (LLMs) in natural language processing (NLP)
offers an opportunity to revisit the entire approach of traditional metrics of
machine intelligence, both in form and content. As the realm of machine
cognitive evaluation has already reached Imitation, the next step is an
efficient Language Acquisition and Understanding. Our paper proposes a paradigm
shift from the established Turing Test towards an all-embracing framework that
hinges on language acquisition, taking inspiration from the recent advancements
in LLMs. The present contribution is deeply tributary of the excellent work
from various disciplines, point out the need to keep interdisciplinary bridges
open, and delineates a more robust and sustainable approach.
</p></li>
</ul>

<h3>Title: On the Relationship between Skill Neurons and Robustness in Prompt Tuning. (arXiv:2309.12263v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12263">http://arxiv.org/abs/2309.12263</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12263]] On the Relationship between Skill Neurons and Robustness in Prompt Tuning(http://arxiv.org/abs/2309.12263)</code></li>
<li>Summary: <p>Prompt Tuning is a popular parameter-efficient finetuning method for
pre-trained large language models (PLMs). Recently, based on experiments with
RoBERTa, it has been suggested that Prompt Tuning activates specific neurons in
the transformer's feed-forward networks, that are highly predictive and
selective for the given task. In this paper, we study the robustness of Prompt
Tuning in relation to these "skill neurons", using RoBERTa and T5. We show that
prompts tuned for a specific task are transferable to tasks of the same type
but are not very robust to adversarial data, with higher robustness for T5 than
RoBERTa. At the same time, we replicate the existence of skill neurons in
RoBERTa and further show that skill neurons also seem to exist in T5.
Interestingly, the skill neurons of T5 determined on non-adversarial data are
also among the most predictive neurons on the adversarial data, which is not
the case for RoBERTa. We conclude that higher adversarial robustness may be
related to a model's ability to activate the relevant skill neurons on
adversarial data.
</p></li>
</ul>

<h3>Title: Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports. (arXiv:2309.12273v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12273">http://arxiv.org/abs/2309.12273</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12273]] Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports(http://arxiv.org/abs/2309.12273)</code></li>
<li>Summary: <p>Rapid and accurate identification of Venous thromboembolism (VTE), a severe
cardiovascular condition including deep vein thrombosis (DVT) and pulmonary
embolism (PE), is important for effective treatment. Leveraging Natural
Language Processing (NLP) on radiology reports, automated methods have shown
promising advancements in identifying VTE events from retrospective data
cohorts or aiding clinical experts in identifying VTE events from radiology
reports. However, effectively training Deep Learning (DL) and the NLP models is
challenging due to limited labeled medical text data, the complexity and
heterogeneity of radiology reports, and data imbalance. This study proposes
novel method combinations of DL methods, along with data augmentation, adaptive
pre-trained NLP model selection, and a clinical expert NLP rule-based
classifier, to improve the accuracy of VTE identification in unstructured
(free-text) radiology reports. Our experimental results demonstrate the model's
efficacy, achieving an impressive 97\% accuracy and 97\% F1 score in predicting
DVT, and an outstanding 98.3\% accuracy and 98.4\% F1 score in predicting PE.
These findings emphasize the model's robustness and its potential to
significantly contribute to VTE research.
</p></li>
</ul>

<h3>Title: The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12288">http://arxiv.org/abs/2309.12288</a></li>
<li>Code URL: https://github.com/lukasberglund/reversal_curse</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12288]] The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"(http://arxiv.org/abs/2309.12288)</code></li>
<li>Summary: <p>We expose a surprising failure of generalization in auto-regressive large
language models (LLMs). If a model is trained on a sentence of the form "A is
B", it will not automatically generalize to the reverse direction "B is A".
This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz
was the ninth Chancellor of Germany", it will not automatically be able to
answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the
likelihood of the correct answer ("Olaf Scholz") will not be higher than for a
random name. Thus, models exhibit a basic failure of logical deduction and do
not generalize a prevalent pattern in their training set (i.e. if "A is B''
occurs, "B is A" is more likely to occur). We provide evidence for the Reversal
Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah
Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to
correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Curse is
robust across model sizes and model families and is not alleviated by data
augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about
real-world celebrities, such as "Who is Tom Cruise's mother? [A: Mary Lee
Pfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". GPT-4 correctly
answers questions like the former 79% of the time, compared to 33% for the
latter. This shows a failure of logical deduction that we hypothesize is caused
by the Reversal Curse. Code is available at
https://github.com/lukasberglund/reversal_curse.
</p></li>
</ul>

<h3>Title: Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework. (arXiv:2309.11682v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11682">http://arxiv.org/abs/2309.11682</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11682]] Dr(http://arxiv.org/abs/2309.11682)</code></li>
<li>Summary: <p>While training fair machine learning models has been studied extensively in
recent years, most developed methods rely on the assumption that the training
and test data have similar distributions. In the presence of distribution
shifts, fair models may behave unfairly on test data. There have been some
developments for fair learning robust to distribution shifts to address this
shortcoming. However, most proposed solutions are based on the assumption of
having access to the causal graph describing the interaction of different
features. Moreover, existing algorithms require full access to data and cannot
be used when small batches are used (stochastic/batch implementation). This
paper proposes the first stochastic distributionally robust fairness framework
with convergence guarantees that do not require knowledge of the causal graph.
More specifically, we formulate the fair inference in the presence of the
distribution shift as a distributionally robust optimization problem under
$L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual
Information (ERMI) as the measure of fairness violation. We then discuss how
the proposed method can be implemented in a stochastic fashion. We have
evaluated the presented framework's performance and efficiency through
extensive experiments on real datasets consisting of distribution shifts.
</p></li>
</ul>

<h3>Title: Soft Merging: A Flexible and Robust Soft Model Merging Approach for Enhanced Neural Network Performance. (arXiv:2309.12259v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12259">http://arxiv.org/abs/2309.12259</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12259]] Soft Merging: A Flexible and Robust Soft Model Merging Approach for Enhanced Neural Network Performance(http://arxiv.org/abs/2309.12259)</code></li>
<li>Summary: <p>Stochastic Gradient Descent (SGD), a widely used optimization algorithm in
deep learning, is often limited to converging to local optima due to the
non-convex nature of the problem. Leveraging these local optima to improve
model performance remains a challenging task. Given the inherent complexity of
neural networks, the simple arithmetic averaging of the obtained local optima
models in undesirable results. This paper proposes a {\em soft merging} method
that facilitates rapid merging of multiple models, simplifies the merging of
specific parts of neural networks, and enhances robustness against malicious
models with extreme values. This is achieved by learning gate parameters
through a surrogate of the $l_0$ norm using hard concrete distribution without
modifying the model weights of the given local optima models. This merging
process not only enhances the model performance by converging to a better local
optimum, but also minimizes computational costs, offering an efficient and
explicit learning process integrated with stochastic gradient descent. Thorough
experiments underscore the effectiveness and superior performance of the merged
neural networks.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric Comparators. (arXiv:2309.12237v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12237">http://arxiv.org/abs/2309.12237</a></li>
<li>Code URL: https://github.com/takhemlata/t-eer</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12237]] t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric Comparators(http://arxiv.org/abs/2309.12237)</code></li>
<li>Summary: <p>Presentation attack (spoofing) detection (PAD) typically operates alongside
biometric verification to improve reliablity in the face of spoofing attacks.
Even though the two sub-systems operate in tandem to solve the single task of
reliable biometric verification, they address different detection tasks and are
hence typically evaluated separately. Evidence shows that this approach is
suboptimal. We introduce a new metric for the joint evaluation of PAD solutions
operating in situ with biometric verification. In contrast to the tandem
detection cost function proposed recently, the new tandem equal error rate
(t-EER) is parameter free. The combination of two classifiers nonetheless leads
to a \emph{set} of operating points at which false alarm and miss rates are
equal and also dependent upon the prevalence of attacks. We therefore introduce
the \emph{concurrent} t-EER, a unique operating point which is invariable to
the prevalence of attacks. Using both modality (and even application) agnostic
simulated scores, as well as real scores for a voice biometrics application, we
demonstrate application of the t-EER to a wide range of biometric system
evaluations under attack. The proposed approach is a strong candidate metric
for the tandem evaluation of PAD systems and biometric comparators.
</p></li>
</ul>

<h2>steal</h2>
<h3>Title: DeepTheft: Stealing DNN Model Architectures through Power Side Channel. (arXiv:2309.11894v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11894">http://arxiv.org/abs/2309.11894</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11894]] DeepTheft: Stealing DNN Model Architectures through Power Side Channel(http://arxiv.org/abs/2309.11894)</code></li>
<li>Summary: <p>Deep Neural Network (DNN) models are often deployed in resource-sharing
clouds as Machine Learning as a Service (MLaaS) to provide inference
services.To steal model architectures that are of valuable intellectual
properties, a class of attacks has been proposed via different side-channel
leakage, posing a serious security challenge to MLaaS.
</p>
<p>Also targeting MLaaS, we propose a new end-to-end attack, DeepTheft, to
accurately recover complex DNN model architectures on general processors via
the RAPL-based power side channel. However, an attacker can acquire only a low
sampling rate (1 KHz) of the time-series energy traces from the RAPL interface,
rendering existing techniques ineffective in stealing large and deep DNN
models. To this end, we design a novel and generic learning-based framework
consisting of a set of meta-models, based on which DeepTheft is demonstrated to
have high accuracy in recovering a large number (thousands) of models
architectures from different model families including the deepest ResNet152.
Particularly, DeepTheft has achieved a Levenshtein Distance Accuracy of 99.75%
in recovering network structures, and a weighted average F1 score of 99.60% in
recovering diverse layer-wise hyperparameters. Besides, our proposed learning
framework is general to other time-series side-channel signals. To validate its
generalization, another existing side channel is exploited, i.e., CPU
frequency. Different from RAPL, CPU frequency is accessible to unprivileged
users in bare-metal OSes. By using our generic learning framework trained
against CPU frequency traces, DeepTheft has shown similarly high attack
performance in stealing model architectures.
</p></li>
</ul>

<h2>extraction</h2>
<h3>Title: Precision in Building Extraction: Comparing Shallow and Deep Models using LiDAR Data. (arXiv:2309.12027v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12027">http://arxiv.org/abs/2309.12027</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12027]] Precision in Building Extraction: Comparing Shallow and Deep Models using LiDAR Data(http://arxiv.org/abs/2309.12027)</code></li>
<li>Summary: <p>Building segmentation is essential in infrastructure development, population
management, and geological observations. This article targets shallow models
due to their interpretable nature to assess the presence of LiDAR data for
supervised segmentation. The benchmark data used in this article are published
in NORA MapAI competition for deep learning model. Shallow models are compared
with deep learning models based on Intersection over Union (IoU) and Boundary
Intersection over Union (BIoU). In the proposed work, boundary masks from the
original mask are generated to improve the BIoU score, which relates to
building shapes' borderline. The influence of LiDAR data is tested by training
the model with only aerial images in task 1 and a combination of aerial and
LiDAR data in task 2 and then compared. shallow models outperform deep learning
models in IoU by 8% using aerial images (task 1) only and 2% in combined aerial
images and LiDAR data (task 2). In contrast, deep learning models show better
performance on BIoU scores. Boundary masks improve BIoU scores by 4% in both
tasks. Light Gradient-Boosting Machine (LightGBM) performs better than RF and
Extreme Gradient Boosting (XGBoost).
</p></li>
</ul>

<h3>Title: SlowFast Network for Continuous Sign Language Recognition. (arXiv:2309.12304v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12304">http://arxiv.org/abs/2309.12304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12304]] SlowFast Network for Continuous Sign Language Recognition(http://arxiv.org/abs/2309.12304)</code></li>
<li>Summary: <p>The objective of this work is the effective extraction of spatial and dynamic
features for Continuous Sign Language Recognition (CSLR). To accomplish this,
we utilise a two-pathway SlowFast network, where each pathway operates at
distinct temporal resolutions to separately capture spatial (hand shapes,
facial expressions) and dynamic (movements) information. In addition, we
introduce two distinct feature fusion methods, carefully designed for the
characteristics of CSLR: (1) Bi-directional Feature Fusion (BFF), which
facilitates the transfer of dynamic semantics into spatial semantics and vice
versa; and (2) Pathway Feature Enhancement (PFE), which enriches dynamic and
spatial representations through auxiliary subnetworks, while avoiding the need
for extra inference time. As a result, our model further strengthens spatial
and dynamic representations in parallel. We demonstrate that the proposed
framework outperforms the current state-of-the-art performance on popular CSLR
datasets, including PHOENIX14, PHOENIX14-T, and CSL-Daily.
</p></li>
</ul>

<h3>Title: BitCoin: Bidirectional Tagging and Supervised Contrastive Learning based Joint Relational Triple Extraction Framework. (arXiv:2309.11853v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11853">http://arxiv.org/abs/2309.11853</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11853]] BitCoin: Bidirectional Tagging and Supervised Contrastive Learning based Joint Relational Triple Extraction Framework(http://arxiv.org/abs/2309.11853)</code></li>
<li>Summary: <p>Relation triple extraction (RTE) is an essential task in information
extraction and knowledge graph construction. Despite recent advancements,
existing methods still exhibit certain limitations. They just employ
generalized pre-trained models and do not consider the specificity of RTE
tasks. Moreover, existing tagging-based approaches typically decompose the RTE
task into two subtasks, initially identifying subjects and subsequently
identifying objects and relations. They solely focus on extracting relational
triples from subject to object, neglecting that once the extraction of a
subject fails, it fails in extracting all triples associated with that subject.
To address these issues, we propose BitCoin, an innovative Bidirectional
tagging and supervised Contrastive learning based joint relational triple
extraction framework. Specifically, we design a supervised contrastive learning
method that considers multiple positives per anchor rather than restricting it
to just one positive. Furthermore, a penalty term is introduced to prevent
excessive similarity between the subject and object. Our framework implements
taggers in two directions, enabling triples extraction from subject to object
and object to subject. Experimental results show that BitCoin achieves
state-of-the-art results on the benchmark datasets and significantly improves
the F1 score on Normal, SEO, EPO, and multiple relation extraction tasks.
</p></li>
</ul>

<h3>Title: The Cambridge Law Corpus: A Corpus for Legal AI Research. (arXiv:2309.12269v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12269">http://arxiv.org/abs/2309.12269</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12269]] The Cambridge Law Corpus: A Corpus for Legal AI Research(http://arxiv.org/abs/2309.12269)</code></li>
<li>Summary: <p>We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research.
It consists of over 250 000 court cases from the UK. Most cases are from the
21st century, but the corpus includes cases as old as the 16th century. This
paper presents the first release of the corpus, containing the raw text and
meta-data. Together with the corpus, we provide annotations on case outcomes
for 638 cases, done by legal experts. Using our annotated data, we have trained
and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to
provide benchmarks. We include an extensive legal and ethical discussion to
address the potentially sensitive nature of this material. As a consequence,
the corpus will only be released for research purposes under certain
restrictions.
</p></li>
</ul>

<h3>Title: Dynamic Hypergraph Structure Learning for Traffic Flow Forecasting. (arXiv:2309.12028v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12028">http://arxiv.org/abs/2309.12028</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12028]] Dynamic Hypergraph Structure Learning for Traffic Flow Forecasting(http://arxiv.org/abs/2309.12028)</code></li>
<li>Summary: <p>This paper studies the problem of traffic flow forecasting, which aims to
predict future traffic conditions on the basis of road networks and traffic
conditions in the past. The problem is typically solved by modeling complex
spatio-temporal correlations in traffic data using spatio-temporal graph neural
networks (GNNs). However, the performance of these methods is still far from
satisfactory since GNNs usually have limited representation capacity when it
comes to complex traffic networks. Graphs, by nature, fall short in capturing
non-pairwise relations. Even worse, existing methods follow the paradigm of
message passing that aggregates neighborhood information linearly, which fails
to capture complicated spatio-temporal high-order interactions. To tackle these
issues, in this paper, we propose a novel model named Dynamic Hypergraph
Structure Learning (DyHSL) for traffic flow prediction. To learn non-pairwise
relationships, our DyHSL extracts hypergraph structural information to model
dynamics in the traffic networks, and updates each node representation by
aggregating messages from its associated hyperedges. Additionally, to capture
high-order spatio-temporal relations in the road network, we introduce an
interactive graph convolution block, which further models the neighborhood
interaction for each node. Finally, we integrate these two views into a
holistic multi-scale correlation extraction module, which conducts temporal
pooling with different scales to model different temporal patterns. Extensive
experiments on four popular traffic benchmark datasets demonstrate the
effectiveness of our proposed DyHSL compared with a broad range of competing
baselines.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Enabling Quartile-based Estimated-Mean Gradient Aggregation As Baseline for Federated Image Classifications. (arXiv:2309.12267v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12267">http://arxiv.org/abs/2309.12267</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12267]] Enabling Quartile-based Estimated-Mean Gradient Aggregation As Baseline for Federated Image Classifications(http://arxiv.org/abs/2309.12267)</code></li>
<li>Summary: <p>Federated Learning (FL) has revolutionized how we train deep neural networks
by enabling decentralized collaboration while safeguarding sensitive data and
improving model performance. However, FL faces two crucial challenges: the
diverse nature of data held by individual clients and the vulnerability of the
FL system to security breaches. This paper introduces an innovative solution
named Estimated Mean Aggregation (EMA) that not only addresses these challenges
but also provides a fundamental reference point as a $\mathsf{baseline}$ for
advanced aggregation techniques in FL systems. EMA's significance lies in its
dual role: enhancing model security by effectively handling malicious outliers
through trimmed means and uncovering data heterogeneity to ensure that trained
models are adaptable across various client datasets. Through a wealth of
experiments, EMA consistently demonstrates high accuracy and area under the
curve (AUC) compared to alternative methods, establishing itself as a robust
baseline for evaluating the effectiveness and security of FL aggregation
methods. EMA's contributions thus offer a crucial step forward in advancing the
efficiency, security, and versatility of decentralized deep learning in the
context of FL.
</p></li>
</ul>

<h3>Title: Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems. (arXiv:2309.11526v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11526">http://arxiv.org/abs/2309.11526</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11526]] Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems(http://arxiv.org/abs/2309.11526)</code></li>
<li>Summary: <p>An important task in the field of sensor technology is the efficient
implementation of adaptation procedures of measurements from one sensor to
another sensor of identical design. One idea is to use the estimation of an
affine transformation between different systems, which can be improved by the
knowledge of experts. This paper presents an improved solution from Glacier
Research that was published back in 1973. It is shown that this solution can be
adapted for software calibration of sensors, implementation of expert-based
adaptation, and federated learning methods. We evaluate our research with
simulations and also with real measured data of a multi-sensor board with 8
identical sensors. The results show an improvement for both the simulation and
the experiments with real data.
</p></li>
</ul>

<h3>Title: Federated Learning with Neural Graphical Models. (arXiv:2309.11680v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11680">http://arxiv.org/abs/2309.11680</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11680]] Federated Learning with Neural Graphical Models(http://arxiv.org/abs/2309.11680)</code></li>
<li>Summary: <p>Federated Learning (FL) addresses the need to create models based on
proprietary data in such a way that multiple clients retain exclusive control
over their data, while all benefit from improved model accuracy due to pooled
resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic
Graphical models that utilize the expressive power of neural networks to learn
complex non-linear dependencies between the input features. They learn to
capture the underlying data distribution and have efficient algorithms for
inference and sampling. We develop a FL framework which maintains a global NGM
model that learns the averaged information from the local NGM models while
keeping the training data within the client's environment. Our design, FedNGMs,
avoids the pitfalls and shortcomings of neuron matching frameworks like
Federated Matched Averaging that suffers from model parameter explosion. Our
global model size remains constant throughout the process. In the cases where
clients have local variables that are not part of the combined global
distribution, we propose a `Stitching' algorithm, which personalizes the global
NGM models by merging the additional variables using the client's data. FedNGM
is robust to data heterogeneity, large number of participants, and limited
communication bandwidth.
</p></li>
</ul>

<h3>Title: Incentivized Communication for Federated Bandits. (arXiv:2309.11702v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11702">http://arxiv.org/abs/2309.11702</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11702]] Incentivized Communication for Federated Bandits(http://arxiv.org/abs/2309.11702)</code></li>
<li>Summary: <p>Most existing works on federated bandits take it for granted that all clients
are altruistic about sharing their data with the server for the collective good
whenever needed. Despite their compelling theoretical guarantee on performance
and communication efficiency, this assumption is overly idealistic and
oftentimes violated in practice, especially when the algorithm is operated over
self-interested clients, who are reluctant to share data without explicit
benefits. Negligence of such self-interested behaviors can significantly affect
the learning efficiency and even the practical operability of federated bandit
learning. In light of this, we aim to spark new insights into this
under-explored research area by formally introducing an incentivized
communication problem for federated bandits, where the server shall motivate
clients to share data by providing incentives. Without loss of generality, we
instantiate this bandit problem with the contextual linear setting and propose
the first incentivized communication protocol, namely, Inc-FedUCB, that
achieves near-optimal regret with provable communication and incentive cost
guarantees. Extensive empirical experiments on both synthetic and real-world
datasets further validate the effectiveness of the proposed method across
various environments.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: OSN-MDAD: Machine Translation Dataset for Arabic Multi-Dialectal Conversations on Online Social Media. (arXiv:2309.12137v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12137">http://arxiv.org/abs/2309.12137</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12137]] OSN-MDAD: Machine Translation Dataset for Arabic Multi-Dialectal Conversations on Online Social Media(http://arxiv.org/abs/2309.12137)</code></li>
<li>Summary: <p>While resources for English language are fairly sufficient to understand
content on social media, similar resources in Arabic are still immature. The
main reason that the resources in Arabic are insufficient is that Arabic has
many dialects in addition to the standard version (MSA). Arabs do not use MSA
in their daily communications; rather, they use dialectal versions.
Unfortunately, social users transfer this phenomenon into their use of social
media platforms, which in turn has raised an urgent need for building suitable
AI models for language-dependent applications. Existing machine translation
(MT) systems designed for MSA fail to work well with Arabic dialects. In light
of this, it is necessary to adapt to the informal nature of communication on
social networks by developing MT systems that can effectively handle the
various dialects of Arabic. Unlike for MSA that shows advanced progress in MT
systems, little effort has been exerted to utilize Arabic dialects for MT
systems. While few attempts have been made to build translation datasets for
dialectal Arabic, they are domain dependent and are not OSN cultural-language
friendly. In this work, we attempt to alleviate these limitations by proposing
an online social network-based multidialect Arabic dataset that is crafted by
contextually translating English tweets into four Arabic dialects: Gulf,
Yemeni, Iraqi, and Levantine. To perform the translation, we followed our
proposed guideline framework for content translation, which could be
universally applicable for translation between foreign languages and local
dialects. We validated the authenticity of our proposed dataset by developing
neural MT models for four Arabic dialects. Our results have shown a superior
performance of our NMT models trained using our dataset. We believe that our
dataset can reliably serve as an Arabic multidialectal translation dataset for
informal MT tasks.
</p></li>
</ul>

<h3>Title: Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets. (arXiv:2309.12032v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12032">http://arxiv.org/abs/2309.12032</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12032]] Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets(http://arxiv.org/abs/2309.12032)</code></li>
<li>Summary: <p>Structure learning is the crux of causal inference. Notably, causal discovery
(CD) algorithms are brittle when data is scarce, possibly inferring imprecise
causal relations that contradict expert knowledge -- especially when
considering latent confounders. To aggravate the issue, most CD methods do not
provide uncertainty estimates, making it hard for users to interpret results
and improve the inference process. Surprisingly, while CD is a human-centered
affair, no works have focused on building methods that both 1) output
uncertainty estimates that can be verified by experts and 2) interact with
those experts to iteratively refine CD. To solve these issues, we start by
proposing to sample (causal) ancestral graphs proportionally to a belief
distribution based on a score function, such as the Bayesian information
criterion (BIC), using generative flow networks. Then, we leverage the
diversity in candidate graphs and introduce an optimal experimental design to
iteratively probe the expert about the relations among variables, effectively
reducing the uncertainty of our belief over ancestral graphs. Finally, we
update our samples to incorporate human feedback via importance sampling.
Importantly, our method does not require causal sufficiency (i.e., unobserved
confounders may exist). Experiments with synthetic observational data show that
our method can accurately sample from distributions over ancestral graphs and
that we can greatly improve inference quality with human aid.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: A survey on the semantics of sequential patterns with negation. (arXiv:2309.11638v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11638">http://arxiv.org/abs/2309.11638</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11638]] A survey on the semantics of sequential patterns with negation(http://arxiv.org/abs/2309.11638)</code></li>
<li>Summary: <p>A sequential pattern with negation, or negative sequential pattern, takes the
form of a sequential pattern for which the negation symbol may be used in front
of some of the pattern's itemsets. Intuitively, such a pattern occurs in a
sequence if negated itemsets are absent in the sequence. Recent work has shown
that different semantics can be attributed to these pattern forms, and that
state-of-the-art algorithms do not extract the same sets of patterns. This
raises the important question of the interpretability of sequential pattern
with negation. In this study, our focus is on exploring how potential users
perceive negation in sequential patterns. Our aim is to determine whether
specific semantics are more "intuitive" than others and whether these align
with the semantics employed by one or more state-of-the-art algorithms. To
achieve this, we designed a questionnaire to reveal the semantics' intuition of
each user. This article presents both the design of the questionnaire and an
in-depth analysis of the 124 responses obtained. The outcomes indicate that two
of the semantics are predominantly intuitive; however, neither of them aligns
with the semantics of the primary state-of-the-art algorithms. As a result, we
provide recommendations to account for this disparity in the conclusions drawn.
</p></li>
</ul>

<h3>Title: Regionally Additive Models: Explainable-by-design models minimizing feature interactions. (arXiv:2309.12215v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12215">http://arxiv.org/abs/2309.12215</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12215]] Regionally Additive Models: Explainable-by-design models minimizing feature interactions(http://arxiv.org/abs/2309.12215)</code></li>
<li>Summary: <p>Generalized Additive Models (GAMs) are widely used explainable-by-design
models in various applications. GAMs assume that the output can be represented
as a sum of univariate functions, referred to as components. However, this
assumption fails in ML problems where the output depends on multiple features
simultaneously. In these cases, GAMs fail to capture the interaction terms of
the underlying function, leading to subpar accuracy. To (partially) address
this issue, we propose Regionally Additive Models (RAMs), a novel class of
explainable-by-design models. RAMs identify subregions within the feature space
where interactions are minimized. Within these regions, it is more accurate to
express the output as a sum of univariate functions (components). Consequently,
RAMs fit one component per subregion of each feature instead of one component
per feature. This approach yields a more expressive model compared to GAMs
while retaining interpretability. The RAM framework consists of three steps.
Firstly, we train a black-box model. Secondly, using Regional Effect Plots, we
identify subregions where the black-box model exhibits near-local additivity.
Lastly, we fit a GAM component for each identified subregion. We validate the
effectiveness of RAMs through experiments on both synthetic and real-world
datasets. The results confirm that RAMs offer improved expressiveness compared
to GAMs while maintaining interpretability.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Predictability and Comprehensibility in Post-Hoc XAI Methods: A User-Centered Analysis. (arXiv:2309.11987v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11987">http://arxiv.org/abs/2309.11987</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11987]] Predictability and Comprehensibility in Post-Hoc XAI Methods: A User-Centered Analysis(http://arxiv.org/abs/2309.11987)</code></li>
<li>Summary: <p>Post-hoc explainability methods aim to clarify predictions of black-box
machine learning models. However, it is still largely unclear how well users
comprehend the provided explanations and whether these increase the users
ability to predict the model behavior. We approach this question by conducting
a user study to evaluate comprehensibility and predictability in two widely
used tools: LIME and SHAP. Moreover, we investigate the effect of
counterfactual explanations and misclassifications on users ability to
understand and predict the model behavior. We find that the comprehensibility
of SHAP is significantly reduced when explanations are provided for samples
near a model's decision boundary. Furthermore, we find that counterfactual
explanations and misclassifications can significantly increase the users
understanding of how a machine learning model is making decisions. Based on our
findings, we also derive design recommendations for future post-hoc
explainability methods with increased comprehensibility and predictability.
</p></li>
</ul>

<h2>watermark</h2>
<h3>Title: MarkNerf:Watermarking for Neural Radiance Field. (arXiv:2309.11747v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11747">http://arxiv.org/abs/2309.11747</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11747]] MarkNerf:Watermarking for Neural Radiance Field(http://arxiv.org/abs/2309.11747)</code></li>
<li>Summary: <p>A watermarking algorithm is proposed in this paper to address the copyright
protection issue of implicit 3D models. The algorithm involves embedding
watermarks into the images in the training set through an embedding network,
and subsequently utilizing the NeRF model for 3D modeling. A copyright verifier
is employed to generate a backdoor image by providing a secret perspective as
input to the neural radiation field. Subsequently, a watermark extractor is
devised using the hyperparameterization method of the neural network to extract
the embedded watermark image from that perspective. In a black box scenario, if
there is a suspicion that the 3D model has been used without authorization, the
verifier can extract watermarks from a secret perspective to verify network
copyright. Experimental results demonstrate that the proposed algorithm
effectively safeguards the copyright of 3D models. Furthermore, the extracted
watermarks exhibit favorable visual effects and demonstrate robust resistance
against various types of noise attacks.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: Light Field Diffusion for Single-View Novel View Synthesis. (arXiv:2309.11525v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11525">http://arxiv.org/abs/2309.11525</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11525]] Light Field Diffusion for Single-View Novel View Synthesis(http://arxiv.org/abs/2309.11525)</code></li>
<li>Summary: <p>Single-view novel view synthesis, the task of generating images from new
viewpoints based on a single reference image, is an important but challenging
task in computer vision. Recently, Denoising Diffusion Probabilistic Model
(DDPM) has become popular in this area due to its strong ability to generate
high-fidelity images. However, current diffusion-based methods directly rely on
camera pose matrices as viewing conditions, globally and implicitly introducing
3D constraints. These methods may suffer from inconsistency among generated
images from different perspectives, especially in regions with intricate
textures and structures. In this work, we present Light Field Diffusion (LFD),
a conditional diffusion-based model for single-view novel view synthesis.
Unlike previous methods that employ camera pose matrices, LFD transforms the
camera view information into light field encoding and combines it with the
reference image. This design introduces local pixel-wise constraints within the
diffusion models, thereby encouraging better multi-view consistency.
Experiments on several datasets show that our LFD can efficiently generate
high-fidelity images and maintain better 3D consistency even in intricate
regions. Our method can generate images with higher quality than NeRF-based
models, and we obtain sample quality similar to other diffusion-based models
but with only one-third of the model size.
</p></li>
</ul>

<h3>Title: Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow removal. (arXiv:2309.11715v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11715">http://arxiv.org/abs/2309.11715</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11715]] Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow removal(http://arxiv.org/abs/2309.11715)</code></li>
<li>Summary: <p>Segment Anything (SAM), an advanced universal image segmentation model
trained on an expansive visual dataset, has set a new benchmark in image
segmentation and computer vision. However, it faced challenges when it came to
distinguishing between shadows and their backgrounds. To address this, we
developed Deshadow-Anything, considering the generalization of large-scale
datasets, and we performed Fine-tuning on large-scale datasets to achieve image
shadow removal. The diffusion model can diffuse along the edges and textures of
an image, helping to remove shadows while preserving the details of the image.
Furthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input
perturbation (DDPM-AIP) to accelerate the iterative training speed of
diffusion. Experiments on shadow removal tasks demonstrate that these methods
can effectively improve image restoration performance.
</p></li>
</ul>

<h3>Title: Latent Diffusion Models for Structural Component Design. (arXiv:2309.11601v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11601">http://arxiv.org/abs/2309.11601</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11601]] Latent Diffusion Models for Structural Component Design(http://arxiv.org/abs/2309.11601)</code></li>
<li>Summary: <p>Recent advances in generative modeling, namely Diffusion models, have
revolutionized generative modeling, enabling high-quality image generation
tailored to user needs. This paper proposes a framework for the generative
design of structural components. Specifically, we employ a Latent Diffusion
model to generate potential designs of a component that can satisfy a set of
problem-specific loading conditions. One of the distinct advantages our
approach offers over other generative approaches, such as generative
adversarial networks (GANs), is that it permits the editing of existing
designs. We train our model using a dataset of geometries obtained from
structural topology optimization utilizing the SIMP algorithm. Consequently,
our framework generates inherently near-optimal designs. Our work presents
quantitative results that support the structural performance of the generated
designs and the variability in potential candidate designs. Furthermore, we
provide evidence of the scalability of our framework by operating over voxel
domains with resolutions varying from $32^3$ to $128^3$. Our framework can be
used as a starting point for generating novel near-optimal designs similar to
topology-optimized designs.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: RMT: Retentive Networks Meet Vision Transformers. (arXiv:2309.11523v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11523">http://arxiv.org/abs/2309.11523</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11523]] RMT: Retentive Networks Meet Vision Transformers(http://arxiv.org/abs/2309.11523)</code></li>
<li>Summary: <p>Transformer first appears in the field of natural language processing and is
later migrated to the computer vision domain, where it demonstrates excellent
performance in vision tasks. However, recently, Retentive Network (RetNet) has
emerged as an architecture with the potential to replace Transformer,
attracting widespread attention in the NLP community. Therefore, we raise the
question of whether transferring RetNet's idea to vision can also bring
outstanding performance to vision tasks. To address this, we combine RetNet and
Transformer to propose RMT. Inspired by RetNet, RMT introduces explicit decay
into the vision backbone, bringing prior knowledge related to spatial distances
to the vision model. This distance-related spatial prior allows for explicit
control of the range of tokens that each token can attend to. Additionally, to
reduce the computational cost of global modeling, we decompose this modeling
process along the two coordinate axes of the image. Abundant experiments have
demonstrated that our RMT exhibits exceptional performance across various
computer vision tasks. For example, RMT achieves 84.1% Top1-acc on ImageNet-1k
using merely 4.5G FLOPs. To the best of our knowledge, among all models, RMT
achieves the highest Top1-acc when models are of similar size and trained with
the same strategy. Moreover, RMT significantly outperforms existing vision
backbones in downstream tasks such as object detection, instance segmentation,
and semantic segmentation. Our work is still in progress.
</p></li>
</ul>

<h3>Title: EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian. (arXiv:2309.11531v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11531">http://arxiv.org/abs/2309.11531</a></li>
<li>Code URL: https://github.com/ssi-research/eptq</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11531]] EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian(http://arxiv.org/abs/2309.11531)</code></li>
<li>Summary: <p>Quantization of deep neural networks (DNN) has become a key element in the
efforts of embedding such networks on end-user devices. However, current
quantization methods usually suffer from costly accuracy degradation. In this
paper, we propose a new method for Enhanced Post Training Quantization named
EPTQ. The method is based on knowledge distillation with an adaptive weighting
of layers. In addition, we introduce a new label-free technique for
approximating the Hessian trace of the task loss, named Label-Free Hessian.
This technique removes the requirement of a labeled dataset for computing the
Hessian. The adaptive knowledge distillation uses the Label-Free Hessian
technique to give greater attention to the sensitive parts of the model while
performing the optimization. Empirically, by employing EPTQ we achieve
state-of-the-art results on a wide variety of models, tasks, and datasets,
including ImageNet classification, COCO object detection, and Pascal-VOC for
semantic segmentation. We demonstrate the performance and compatibility of EPTQ
on an extended set of architectures, including CNNs, Transformers, hybrid, and
MLP-only models.
</p></li>
</ul>

<h3>Title: OSNet & MNetO: Two Types of General Reconstruction Architectures for Linear Computed Tomography in Multi-Scenarios. (arXiv:2309.11858v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11858">http://arxiv.org/abs/2309.11858</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11858]] OSNet & MNetO: Two Types of General Reconstruction Architectures for Linear Computed Tomography in Multi-Scenarios(http://arxiv.org/abs/2309.11858)</code></li>
<li>Summary: <p>Recently, linear computed tomography (LCT) systems have actively attracted
attention. To weaken projection truncation and image the region of interest
(ROI) for LCT, the backprojection filtration (BPF) algorithm is an effective
solution. However, in BPF for LCT, it is difficult to achieve stable interior
reconstruction, and for differentiated backprojection (DBP) images of LCT,
multiple rotation-finite inversion of Hilbert transform (Hilbert
filtering)-inverse rotation operations will blur the image. To satisfy multiple
reconstruction scenarios for LCT, including interior ROI, complete object, and
exterior region beyond field-of-view (FOV), and avoid the rotation operations
of Hilbert filtering, we propose two types of reconstruction architectures. The
first overlays multiple DBP images to obtain a complete DBP image, then uses a
network to learn the overlying Hilbert filtering function, referred to as the
Overlay-Single Network (OSNet). The second uses multiple networks to train
different directional Hilbert filtering models for DBP images of multiple
linear scannings, respectively, and then overlays the reconstructed results,
i.e., Multiple Networks Overlaying (MNetO). In two architectures, we introduce
a Swin Transformer (ST) block to the generator of pix2pixGAN to extract both
local and global features from DBP images at the same time. We investigate two
architectures from different networks, FOV sizes, pixel sizes, number of
projections, geometric magnification, and processing time. Experimental results
show that two architectures can both recover images. OSNet outperforms BPF in
various scenarios. For the different networks, ST-pix2pixGAN is superior to
pix2pixGAN and CycleGAN. MNetO exhibits a few artifacts due to the differences
among the multiple models, but any one of its models is suitable for imaging
the exterior edge in a certain direction.
</p></li>
</ul>

<h3>Title: Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation. (arXiv:2309.11933v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11933">http://arxiv.org/abs/2309.11933</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11933]] Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation(http://arxiv.org/abs/2309.11933)</code></li>
<li>Summary: <p>Referring Video Object Segmentation (RVOS) requires segmenting the object in
video referred by a natural language query. Existing methods mainly rely on
sophisticated pipelines to tackle such cross-modal task, and do not explicitly
model the object-level spatial context which plays an important role in
locating the referred object. Therefore, we propose an end-to-end RVOS
framework completely built upon transformers, termed \textit{Fully
Transformer-Equipped Architecture} (FTEA), which treats the RVOS task as a mask
sequence learning problem and regards all the objects in video as candidate
objects. Given a video clip with a text query, the visual-textual features are
yielded by encoder, while the corresponding pixel-level and word-level features
are aligned in terms of semantic similarity. To capture the object-level
spatial context, we have developed the Stacked Transformer, which individually
characterizes the visual appearance of each candidate object, whose feature map
is decoded to the binary mask sequence in order directly. Finally, the model
finds the best matching between mask sequence and text query. In addition, to
diversify the generated masks for candidate objects, we impose a diversity loss
on the model for capturing more accurate mask of the referred object. Empirical
studies have shown the superiority of the proposed method on three benchmarks,
e.g., FETA achieves 45.1% and 38.7% in terms of mAP on A2D Sentences (3782
videos) and J-HMDB Sentences (928 videos), respectively; it achieves 56.6% in
terms of $\mathcal{J\&amp;F}$ on Ref-YouTube-VOS (3975 videos and 7451 objects).
Particularly, compared to the best candidate method, it has a gain of 2.1% and
3.2% in terms of P$@$0.5 on the former two, respectively, while it has a gain
of 2.9% in terms of $\mathcal{J}$ on the latter one.
</p></li>
</ul>

<h3>Title: ZS6D: Zero-shot 6D Object Pose Estimation using Vision Transformers. (arXiv:2309.11986v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11986">http://arxiv.org/abs/2309.11986</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11986]] ZS6D: Zero-shot 6D Object Pose Estimation using Vision Transformers(http://arxiv.org/abs/2309.11986)</code></li>
<li>Summary: <p>As robotic systems increasingly encounter complex and unconstrained
real-world scenarios, there is a demand to recognize diverse objects. The
state-of-the-art 6D object pose estimation methods rely on object-specific
training and therefore do not generalize to unseen objects. Recent novel object
pose estimation methods are solving this issue using task-specific fine-tuned
CNNs for deep template matching. This adaptation for pose estimation still
requires expensive data rendering and training procedures. MegaPose for example
is trained on a dataset consisting of two million images showing 20,000
different objects to reach such generalization capabilities. To overcome this
shortcoming we introduce ZS6D, for zero-shot novel object 6D pose estimation.
Visual descriptors, extracted using pre-trained Vision Transformers (ViT), are
used for matching rendered templates against query images of objects and for
establishing local correspondences. These local correspondences enable deriving
geometric correspondences and are used for estimating the object's 6D pose with
RANSAC-based PnP. This approach showcases that the image descriptors extracted
by pre-trained ViTs are well-suited to achieve a notable improvement over two
state-of-the-art novel object 6D pose estimation methods, without the need for
task-specific fine-tuning. Experiments are performed on LMO, YCBV, and TLESS.
In comparison to one of the two methods we improve the Average Recall on all
three datasets and compared to the second method we improve on two datasets.
</p></li>
</ul>

<h3>Title: PanoVOS:Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation. (arXiv:2309.12303v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12303">http://arxiv.org/abs/2309.12303</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12303]] PanoVOS:Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation(http://arxiv.org/abs/2309.12303)</code></li>
<li>Summary: <p>Panoramic videos contain richer spatial information and have attracted
tremendous amounts of attention due to their exceptional experience in some
fields such as autonomous driving and virtual reality. However, existing
datasets for video segmentation only focus on conventional planar images. To
address the challenge, in this paper, we present a panoramic video dataset,
PanoVOS. The dataset provides 150 videos with high video resolutions and
diverse motions. To quantify the domain gap between 2D planar videos and
panoramic videos, we evaluate 15 off-the-shelf video object segmentation (VOS)
models on PanoVOS. Through error analysis, we found that all of them fail to
tackle pixel-level content discontinues of panoramic videos. Thus, we present a
Panoramic Space Consistency Transformer (PSCFormer), which can effectively
utilize the semantic boundary information of the previous frame for pixel-level
matching with the current frame. Extensive experiments demonstrate that
compared with the previous SOTA models, our PSCFormer network exhibits a great
advantage in terms of segmentation results under the panoramic setting. Our
dataset poses new challenges in panoramic VOS and we hope that our PanoVOS can
advance the development of panoramic segmentation/tracking.
</p></li>
</ul>

<h3>Title: SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References. (arXiv:2309.12250v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12250">http://arxiv.org/abs/2309.12250</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12250]] SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References(http://arxiv.org/abs/2309.12250)</code></li>
<li>Summary: <p>Evaluation of QA systems is very challenging and expensive, with the most
reliable approach being human annotations of correctness of answers for
questions. Recent works (AVA, BEM) have shown that transformer LM encoder based
similarity metrics transfer well for QA evaluation, but they are limited by the
usage of a single correct reference answer. We propose a new evaluation metric:
SQuArE (Sentence-level QUestion AnsweRing Evaluation), using multiple reference
answers (combining multiple correct and incorrect references) for sentence-form
QA. We evaluate SQuArE on both sentence-level extractive (Answer Selection) and
generative (GenQA) QA systems, across multiple academic and industrial
datasets, and show that it outperforms previous baselines and obtains the
highest correlation with human annotations.
</p></li>
</ul>

<h3>Title: Large-scale Pretraining Improves Sample Efficiency of Active Learning based Molecule Virtual Screening. (arXiv:2309.11687v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11687">http://arxiv.org/abs/2309.11687</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11687]] Large-scale Pretraining Improves Sample Efficiency of Active Learning based Molecule Virtual Screening(http://arxiv.org/abs/2309.11687)</code></li>
<li>Summary: <p>Virtual screening of large compound libraries to identify potential hit
candidates is one of the earliest steps in drug discovery. As the size of
commercially available compound collections grows exponentially to the scale of
billions, brute-force virtual screening using traditional tools such as docking
becomes infeasible in terms of time and computational resources. Active
learning and Bayesian optimization has recently been proven as effective
methods of narrowing down the search space. An essential component in those
methods is a surrogate machine learning model that is trained with a small
subset of the library to predict the desired properties of compounds. Accurate
model can achieve high sample efficiency by finding the most promising
compounds with only a fraction of the whole library being virtually screened.
In this study, we examined the performance of pretrained transformer-based
language model and graph neural network in Bayesian optimization active
learning framework. The best pretrained models identifies 58.97% of the
top-50000 by docking score after screening only 0.6% of an ultra-large library
containing 99.5 million compounds, improving 8% over previous state-of-the-art
baseline. Through extensive benchmarks, we show that the superior performance
of pretrained models persists in both structure-based and ligand-based drug
discovery. Such model can serve as a boost to the accuracy and sample
efficiency of active learning based molecule virtual screening.
</p></li>
</ul>

<h3>Title: Boolformer: Symbolic Regression of Logic Functions with Transformers. (arXiv:2309.12207v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12207">http://arxiv.org/abs/2309.12207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12207]] Boolformer: Symbolic Regression of Logic Functions with Transformers(http://arxiv.org/abs/2309.12207)</code></li>
<li>Summary: <p>In this work, we introduce Boolformer, the first Transformer architecture
trained to perform end-to-end symbolic regression of Boolean functions. First,
we show that it can predict compact formulas for complex functions which were
not seen during training, when provided a clean truth table. Then, we
demonstrate its ability to find approximate expressions when provided
incomplete and noisy observations. We evaluate the Boolformer on a broad set of
real-world binary classification datasets, demonstrating its potential as an
interpretable alternative to classic machine learning methods. Finally, we
apply it to the widespread task of modelling the dynamics of gene regulatory
networks. Using a recent benchmark, we show that Boolformer is competitive with
state-of-the art genetic algorithms with a speedup of several orders of
magnitude. Our code and models are available publicly.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge. (arXiv:2309.11575v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11575">http://arxiv.org/abs/2309.11575</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11575]] Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge(http://arxiv.org/abs/2309.11575)</code></li>
<li>Summary: <p>Text-conditioned image generation models have recently achieved astonishing
image quality and alignment results. Consequently, they are employed in a
fast-growing number of applications. Since they are highly data-driven, relying
on billion-sized datasets randomly scraped from the web, they also produce
unsafe content. As a contribution to the Adversarial Nibbler challenge, we
distill a large set of over 1,000 potential adversarial inputs from existing
safety benchmarks. Our analysis of the gathered prompts and corresponding
images demonstrates the fragility of input filters and provides further
insights into systematic safety issues in current generative image models.
</p></li>
</ul>

<h3>Title: TextCLIP: Text-Guided Face Image Generation And Manipulation Without Adversarial Training. (arXiv:2309.11923v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11923">http://arxiv.org/abs/2309.11923</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11923]] TextCLIP: Text-Guided Face Image Generation And Manipulation Without Adversarial Training(http://arxiv.org/abs/2309.11923)</code></li>
<li>Summary: <p>Text-guided image generation aimed to generate desired images conditioned on
given texts, while text-guided image manipulation refers to semantically edit
parts of a given image based on specified texts. For these two similar tasks,
the key point is to ensure image fidelity as well as semantic consistency. Many
previous approaches require complex multi-stage generation and adversarial
training, while struggling to provide a unified framework for both tasks. In
this work, we propose TextCLIP, a unified framework for text-guided image
generation and manipulation without adversarial training. The proposed method
accepts input from images or random noise corresponding to these two different
tasks, and under the condition of the specific texts, a carefully designed
mapping network that exploits the powerful generative capabilities of StyleGAN
and the text image representation capabilities of Contrastive Language-Image
Pre-training (CLIP) generates images of up to $1024\times1024$ resolution that
can currently be generated. Extensive experiments on the Multi-modal CelebA-HQ
dataset have demonstrated that our proposed method outperforms existing
state-of-the-art methods, both on text-guided generation tasks and manipulation
tasks.
</p></li>
</ul>

<h3>Title: InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework. (arXiv:2309.11911v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11911">http://arxiv.org/abs/2309.11911</a></li>
<li>Code URL: https://github.com/LIN-SHANG/InstructERC</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11911]] InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework(http://arxiv.org/abs/2309.11911)</code></li>
<li>Summary: <p>The development of emotion recognition in dialogue (ERC) has been
consistently hindered by the complexity of pipeline designs, leading to ERC
models that often overfit to specific datasets and dialogue patterns. In this
study, we propose a novel approach, namely
</p>
<p>InstructERC, to reformulates the ERC task from a discriminative framework to
a generative framework based on Large Language Models (LLMs) . InstructERC has
two significant contributions: Firstly, InstructERC introduces a simple yet
effective retrieval template module, which helps the model explicitly integrate
multi-granularity dialogue supervision information by concatenating the
historical dialog content, label statement, and emotional domain demonstrations
with high semantic similarity. Furthermore, we introduce two additional emotion
alignment tasks, namely speaker identification and emotion prediction tasks, to
implicitly model the dialogue role relationships and future emotional
tendencies in conversations. Our LLM-based plug-and-play plugin framework
significantly outperforms all previous models and achieves comprehensive SOTA
on three commonly used ERC datasets. Extensive analysis of parameter-efficient
and data-scaling experiments provide empirical guidance for applying
InstructERC in practical scenarios. Our code will be released after blind
review.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent. (arXiv:2309.12311v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12311">http://arxiv.org/abs/2309.12311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12311]] LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent(http://arxiv.org/abs/2309.12311)</code></li>
<li>Summary: <p>3D visual grounding is a critical skill for household robots, enabling them
to navigate, manipulate objects, and answer questions based on their
environment. While existing approaches often rely on extensive labeled data or
exhibit limitations in handling complex language queries, we propose
LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model
(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to
decompose complex natural language queries into semantic constituents and
employs a visual grounding tool, such as OpenScene or LERF, to identify objects
in a 3D scene. The LLM then evaluates the spatial and commonsense relations
among the proposed objects to make a final grounding decision. Our method does
not require any labeled training data and can generalize to novel 3D scenes and
arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and
demonstrate state-of-the-art zero-shot grounding accuracy. Our findings
indicate that LLMs significantly improve the grounding capability, especially
for complex language queries, making LLM-Grounder an effective approach for 3D
vision-language tasks in robotics. Videos and interactive demos can be found on
the project website https://chat-with-nerf.github.io/ .
</p></li>
</ul>

<h3>Title: Towards LLM-based Autograding for Short Textual Answers. (arXiv:2309.11508v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11508">http://arxiv.org/abs/2309.11508</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11508]] Towards LLM-based Autograding for Short Textual Answers(http://arxiv.org/abs/2309.11508)</code></li>
<li>Summary: <p>Grading of exams is an important, labor intensive, subjective, repetitive and
frequently challenging task. The feasibility of autograding textual responses
has greatly increased thanks to the availability of large language models
(LLMs) such as ChatGPT and because of the substantial influx of data brought
about by digitalization. However, entrusting AI models with decision-making
roles raises ethical considerations, mainly stemming from potential biases and
issues related to generating false information. Thus, in this manuscript we
provide an evaluation of a large language model for the purpose of autograding,
while also highlighting how LLMs can support educators in validating their
grading procedures. Our evaluation is targeted towards automatic short textual
answers grading (ASAG), spanning various languages and examinations from two
distinct courses. Our findings suggest that while "out-of-the-box" LLMs provide
a valuable tool to provide a complementary perspective, their readiness for
independent automated grading remains a work in progress, necessitating human
oversight.
</p></li>
</ul>

<h3>Title: Towards Effective Disambiguation for Machine Translation with Large Language Models. (arXiv:2309.11668v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11668">http://arxiv.org/abs/2309.11668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11668]] Towards Effective Disambiguation for Machine Translation with Large Language Models(http://arxiv.org/abs/2309.11668)</code></li>
<li>Summary: <p>Resolving semantic ambiguity has long been recognised as a central challenge
in the field of machine translation. Recent work on benchmarking translation
performance on ambiguous sentences has exposed the limitations of conventional
Neural Machine Translation (NMT) systems, which fail to capture many of these
cases. Large language models (LLMs) have emerged as a promising alternative,
demonstrating comparable performance to traditional NMT models while
introducing new paradigms for controlling the target outputs. In this paper, we
study the capabilities of LLMs to translate ambiguous sentences containing
polysemous words and rare word senses. We also propose two ways to improve the
handling of such ambiguity through in-context learning and fine-tuning on
carefully curated ambiguous datasets. Experiments show that our methods can
match or outperform state-of-the-art systems such as DeepL and NLLB in four out
of five language directions. Our research provides valuable insights into
effectively adapting LLMs for disambiguation during machine translation.
</p></li>
</ul>

<h3>Title: Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation. (arXiv:2309.11669v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11669">http://arxiv.org/abs/2309.11669</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11669]] Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation(http://arxiv.org/abs/2309.11669)</code></li>
<li>Summary: <p>Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used
to train forward and reverse neural models that generate text from KG and vice
versa. However models trained on datasets where KG and text pairs are not
equivalent can suffer from more hallucination and poorer recall. In this paper,
we verify this empirically by generating datasets with different levels of
noise and find that noisier datasets do indeed lead to more hallucination. We
argue that the ability of forward and reverse models trained on a dataset to
cyclically regenerate source KG or text is a proxy for the equivalence between
the KG and the text in the dataset. Using cyclic evaluation we find that
manually created WebNLG is much better than automatically created TeKGen and
T-REx. Guided by these observations, we construct a new, improved dataset
called LAGRANGE using heuristics meant to improve equivalence between KG and
text and show the impact of each of the heuristics on cyclic evaluation. We
also construct two synthetic datasets using large language models (LLMs), and
observe that these are conducive to models that perform significantly well on
cyclic generation of text, but less so on cyclic generation of KGs, probably
because of a lack of a consistent underlying ontology.
</p></li>
</ul>

<h3>Title: A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models. (arXiv:2309.11674v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11674">http://arxiv.org/abs/2309.11674</a></li>
<li>Code URL: https://github.com/fe1ixxu/alma</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11674]] A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models(http://arxiv.org/abs/2309.11674)</code></li>
<li>Summary: <p>Generative Large Language Models (LLMs) have achieved remarkable advancements
in various NLP tasks. However, these advances have not been reflected in the
translation task, especially those with moderate model sizes (i.e., 7B or 13B
parameters), which still lag behind conventional supervised encoder-decoder
translation models. Previous studies have attempted to improve the translation
capabilities of these moderate LLMs, but their gains have been limited. In this
study, we propose a novel fine-tuning approach for LLMs that is specifically
designed for the translation task, eliminating the need for the abundant
parallel data that traditional translation models usually depend on. Our
approach consists of two fine-tuning stages: initial fine-tuning on monolingual
data followed by subsequent fine-tuning on a small set of high-quality parallel
data. We introduce the LLM developed through this strategy as Advanced Language
Model-based trAnslator (ALMA). Based on LLaMA-2 as our underlying model, our
results show that the model can achieve an average improvement of more than 12
BLEU and 12 COMET over its zero-shot performance across 10 translation
directions from the WMT'21 (2 directions) and WMT'22 (8 directions) test
datasets. The performance is significantly better than all prior work and even
superior to the NLLB-54B model and GPT-3.5-text-davinci-003, with only 7B or
13B parameters. This method establishes the foundation for a novel training
paradigm in machine translation.
</p></li>
</ul>

<h3>Title: LLM Guided Inductive Inference for Solving Compositional Problems. (arXiv:2309.11688v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11688">http://arxiv.org/abs/2309.11688</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11688]] LLM Guided Inductive Inference for Solving Compositional Problems(http://arxiv.org/abs/2309.11688)</code></li>
<li>Summary: <p>While large language models (LLMs) have demonstrated impressive performance
in question-answering tasks, their performance is limited when the questions
require knowledge that is not included in the model's training data and can
only be acquired through direct observation or interaction with the real world.
Existing methods decompose reasoning tasks through the use of modules invoked
sequentially, limiting their ability to answer deep reasoning tasks. We
introduce a method, Recursion based extensible LLM (REBEL), which handles
open-world, deep reasoning tasks by employing automated reasoning techniques
like dynamic planning and forward-chaining strategies. REBEL allows LLMs to
reason via recursive problem decomposition and utilization of external tools.
The tools that REBEL uses are specified only by natural language description.
We further demonstrate REBEL capabilities on a set of problems that require a
deeply nested use of external tools in a compositional and conversational
setting.
</p></li>
</ul>

<h3>Title: Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination. (arXiv:2309.11696v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11696">http://arxiv.org/abs/2309.11696</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11696]] Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination(http://arxiv.org/abs/2309.11696)</code></li>
<li>Summary: <p>Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable
proficiency in comprehending and generating natural language. However, their
unpersonalized generation paradigm may result in suboptimal user-specific
outcomes. Typically, users converse differently based on their knowledge and
preferences. This necessitates the task of enhancing user-oriented LLM which
remains unexplored. While one can fully train an LLM for this objective, the
resource consumption is unaffordable. Prior research has explored memory-based
methods to store and retrieve knowledge to enhance generation without
retraining for new queries. However, we contend that a mere memory module is
inadequate to comprehend a user's preference, and fully training an LLM can be
excessively costly. In this study, we propose a novel computational bionic
memory mechanism, equipped with a parameter-efficient fine-tuning schema, to
personalize LLMs. Our extensive experimental results demonstrate the
effectiveness and superiority of the proposed approach. To encourage further
research into this area, we are releasing a new conversation dataset generated
entirely by LLM based on an open-source medical corpus, as well as our
implementation code.
</p></li>
</ul>

<h3>Title: Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues. (arXiv:2309.11838v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11838">http://arxiv.org/abs/2309.11838</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11838]] Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues(http://arxiv.org/abs/2309.11838)</code></li>
<li>Summary: <p>In this paper, we investigate the use of large language models (LLMs) like
ChatGPT for document-grounded response generation in the context of
information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus
of task-oriented dialogues in four social service domains previously used in
the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded
in multiple documents providing relevant information. We generate dialogue
completion responses by prompting a ChatGPT model, using two methods:
Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT
model pretraining while LlamaIndex also extracts relevant information from
documents. Observing that document-grounded response generation via LLMs cannot
be adequately assessed by automatic evaluation metrics as they are
significantly more verbose, we perform a human evaluation where annotators rate
the output of the shared task winning system, the two Chat-GPT variants
outputs, and human responses. While both ChatGPT variants are more likely to
include information not present in the relevant segments, possibly including a
presence of hallucinations, they are rated higher than both the shared task
winning system and human responses.
</p></li>
</ul>

<h3>Title: Knowledge Sanitization of Large Language Models. (arXiv:2309.11852v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11852">http://arxiv.org/abs/2309.11852</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11852]] Knowledge Sanitization of Large Language Models(http://arxiv.org/abs/2309.11852)</code></li>
<li>Summary: <p>We explore a knowledge sanitization approach to mitigate the privacy concerns
associated with large language models (LLMs). LLMs trained on a large corpus of
Web data can memorize and potentially reveal sensitive or confidential
information, raising critical security concerns. Our technique fine-tunes these
models, prompting them to generate harmless responses such as ``I don't know''
when queried about specific information. Experimental results in a closed-book
question-answering task show that our straightforward method not only minimizes
particular knowledge leakage but also preserves the overall performance of LLM.
These two advantages strengthen the defense against extraction attacks and
reduces the emission of harmful content such as hallucinations.
</p></li>
</ul>

<h3>Title: Focal Inferential Infusion Coupled with Tractable Density Discrimination for Implicit Hate Speech Detection. (arXiv:2309.11896v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11896">http://arxiv.org/abs/2309.11896</a></li>
<li>Code URL: https://github.com/lcs2-iiitd/fiadd</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11896]] Focal Inferential Infusion Coupled with Tractable Density Discrimination for Implicit Hate Speech Detection(http://arxiv.org/abs/2309.11896)</code></li>
<li>Summary: <p>Although pre-trained large language models (PLMs) have achieved
state-of-the-art on many NLP tasks, they lack understanding of subtle
expressions of implicit hate speech. Such nuanced and implicit hate is often
misclassified as non-hate. Various attempts have been made to enhance the
detection of (implicit) hate content by augmenting external context or
enforcing label separation via distance-based metrics. We combine these two
approaches and introduce FiADD, a novel Focused Inferential Adaptive Density
Discrimination framework. FiADD enhances the PLM finetuning pipeline by
bringing the surface form of an implicit hate speech closer to its implied form
while increasing the inter-cluster distance among various class labels. We test
FiADD on three implicit hate datasets and observe significant improvement in
the two-way and three-way hate classification tasks. We further experiment on
the generalizability of FiADD on three other tasks, namely detecting sarcasm,
irony, and stance, in which surface and implied forms differ, and observe
similar performance improvement. We analyze the generated latent space to
understand its evolution under FiADD, which corroborates the advantage of
employing FiADD for implicit hate speech detection.
</p></li>
</ul>

<h3>Title: LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. (arXiv:2309.11998v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11998">http://arxiv.org/abs/2309.11998</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11998]] LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset(http://arxiv.org/abs/2309.11998)</code></li>
<li>Summary: <p>Studying how people interact with large language models (LLMs) in real-world
scenarios is increasingly important due to their widespread use in various
applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset
containing one million real-world conversations with 25 state-of-the-art LLMs.
This dataset is collected from 210K unique IP addresses in the wild on our
Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's
content, including its curation process, basic statistics, and topic
distribution, highlighting its diversity, originality, and scale. We
demonstrate its versatility through four use cases: developing content
moderation models that perform similarly to GPT-4, building a safety benchmark,
training instruction-following models that perform similarly to Vicuna, and
creating challenging benchmark questions. We believe that this dataset will
serve as a valuable resource for understanding and advancing LLM capabilities.
The dataset is publicly available at
\url{https://huggingface.co/datasets/lmsys/lmsys-chat-1m}.
</p></li>
</ul>

<h3>Title: AceGPT, Localizing Large Language Models in Arabic. (arXiv:2309.12053v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12053">http://arxiv.org/abs/2309.12053</a></li>
<li>Code URL: https://github.com/freedomintelligence/acegpt</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12053]] AceGPT, Localizing Large Language Models in Arabic(http://arxiv.org/abs/2309.12053)</code></li>
<li>Summary: <p>This paper explores the imperative need and methodology for developing a
localized Large Language Model (LLM) tailored for Arabic, a language with
unique cultural characteristics that are not adequately addressed by current
mainstream models like ChatGPT. Key concerns additionally arise when
considering cultural sensitivity and local values. To this end, the paper
outlines a packaged solution, including further pre-training with Arabic texts,
supervised fine-tuning (SFT) using native Arabic instructions and GPT-4
responses in Arabic, and reinforcement learning with AI feedback (RLAIF) using
a reward model that is sensitive to local culture and values. The objective is
to train culturally aware and value-aligned Arabic LLMs that can serve the
diverse application-specific needs of Arabic-speaking communities.
</p>
<p>Extensive evaluations demonstrated that the resulting LLM called
`\textbf{AceGPT}' is the SOTA open Arabic LLM in various benchmarks, including
instruction-following benchmark (i.e., Arabic Vicuna-80 and Arabic AlpacaEval),
knowledge benchmark (i.e., Arabic MMLU and EXAMs), as well as the
newly-proposed Arabic cultural \&amp; value alignment benchmark. Notably, AceGPT
outperforms ChatGPT in the popular Vicuna-80 benchmark when evaluated with
GPT-4, despite the benchmark's limited scale. % Natural Language Understanding
(NLU) benchmark (i.e., ALUE)
</p>
<p>Codes, data, and models are in https://github.com/FreedomIntelligence/AceGPT.
</p></li>
</ul>

<h3>Title: PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models. (arXiv:2309.12109v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12109">http://arxiv.org/abs/2309.12109</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12109]] PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models(http://arxiv.org/abs/2309.12109)</code></li>
<li>Summary: <p>In this era of large language models (LLMs), the traditional training of
models has become increasingly unimaginable for regular users and institutions.
The exploration of efficient fine-tuning for high-resource languages on these
models is an undeniable trend that is gradually gaining popularity. However,
there has been very little exploration for various low-resource languages, such
as Tibetan. Research in Tibetan NLP is inherently scarce and limited. While
there is currently no existing large language model for Tibetan due to its
low-resource nature, that day will undoubtedly arrive. Therefore, research on
efficient fine-tuning for low-resource language models like Tibetan is highly
necessary. Our research can serve as a reference to fill this crucial gap.
Efficient fine-tuning strategies for pre-trained language models (PLMs) in
Tibetan have seen minimal exploration. We conducted three types of efficient
fine-tuning experiments on the publicly available TNCC-title dataset:
"prompt-tuning," "Adapter lightweight fine-tuning," and "prompt-tuning +
Adapter fine-tuning." The experimental results demonstrate significant
improvements using these methods, providing valuable insights for advancing
Tibetan language applications in the context of pre-trained models.
</p></li>
</ul>

<h3>Title: Code Soliloquies for Accurate Calculations in Large Language Models. (arXiv:2309.12161v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12161">http://arxiv.org/abs/2309.12161</a></li>
<li>Code URL: https://github.com/luffycodes/tutorbot-spock-phys</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12161]] Code Soliloquies for Accurate Calculations in Large Language Models(http://arxiv.org/abs/2309.12161)</code></li>
<li>Summary: <p>High-quality conversational datasets are integral to the successful
development of Intelligent Tutoring Systems (ITS) that employ a Large Language
Model (LLM) backend. These datasets, when used to fine-tune the LLM backend,
significantly enhance the quality of interactions between students and ITS. A
common strategy for developing these datasets involves generating synthetic
student-teacher dialogues using advanced GPT-4 models. However, challenges
arise when these dialogues demand complex calculations, common in subjects like
physics. Despite its advanced capabilities, GPT-4's performance falls short in
reliably handling even simple multiplication tasks, marking a significant
limitation in its utility for these subjects. To address these challenges, this
paper introduces an innovative stateful prompt design. Our approach generates a
mock conversation between a student and a tutorbot, both roles simulated by
GPT-4. Each student response triggers a soliloquy (an inner monologue) in the
GPT-tutorbot, which assesses whether its response would necessitate
calculations. If so, it proceeds to script the required code in Python and then
uses the resulting output to construct its response to the student. Our
approach notably enhances the quality of synthetic conversation datasets,
especially for subjects that are calculation-intensive. Our findings show that
our Higgs model -- a LLaMA finetuned with datasets generated through our novel
stateful prompt design -- proficiently utilizes Python for computations.
Consequently, finetuning with our datasets enriched with code soliloquies
enhances not just the accuracy but also the computational reliability of Higgs'
responses.
</p></li>
</ul>

<h3>Title: Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection. (arXiv:2309.12247v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12247">http://arxiv.org/abs/2309.12247</a></li>
<li>Code URL: https://github.com/ictmcg/arg</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12247]] Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection(http://arxiv.org/abs/2309.12247)</code></li>
<li>Summary: <p>Detecting fake news requires both a delicate sense of diverse clues and a
profound understanding of the real-world background, which remains challenging
for detectors based on small language models (SLMs) due to their knowledge and
capability limitations. Recent advances in large language models (LLMs) have
shown remarkable performance in various tasks, but whether and how LLMs could
help with fake news detection remains underexplored. In this paper, we
investigate the potential of LLMs in fake news detection. First, we conduct an
empirical study and find that a sophisticated LLM such as GPT 3.5 could
generally expose fake news and provide desirable multi-perspective rationales
but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis
attributes such a gap to the LLM's inability to select and integrate rationales
properly to conclude. Based on these findings, we propose that current LLMs may
not substitute fine-tuned SLMs in fake news detection but can be a good advisor
for SLMs by providing multi-perspective instructive rationales. To instantiate
this proposal, we design an adaptive rationale guidance network for fake news
detection (ARG), in which SLMs selectively acquire insights on news analysis
from the LLMs' rationales. We further derive a rationale-free version of ARG by
distillation, namely ARG-D, which services cost-sensitive scenarios without
inquiring LLMs. Experiments on two real-world datasets demonstrate that ARG and
ARG-D outperform three types of baseline methods, including SLM-based,
LLM-based, and combinations of small and large language models.
</p></li>
</ul>

<h3>Title: Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition. (arXiv:2309.12278v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12278">http://arxiv.org/abs/2309.12278</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12278]] Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition(http://arxiv.org/abs/2309.12278)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated dominating performance in many
NLP tasks, especially on generative tasks. However, they often fall short in
some information extraction tasks, particularly those requiring domain-specific
knowledge, such as Biomedical Named Entity Recognition (NER). In this paper,
inspired by Chain-of-thought, we leverage the LLM to solve the Biomedical NER
step-by-step: break down the NER task into entity span extraction and entity
type determination. Additionally, for entity type determination, we inject
entity knowledge to address the problem that LLM's lack of domain knowledge
when predicting entity category. Experimental results show a significant
improvement in our two-step BioNER approach compared to previous few-shot LLM
baseline. Additionally, the incorporation of external knowledge significantly
enhances entity category determination performance.
</p></li>
</ul>

<h3>Title: MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12284">http://arxiv.org/abs/2309.12284</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12284]] MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models(http://arxiv.org/abs/2309.12284)</code></li>
<li>Summary: <p>Large language models (LLMs) have pushed the limits of natural language
understanding and exhibited excellent problem-solving ability. Despite the
great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away
from satisfactory for solving mathematical problem due to the complex reasoning
procedures. To bridge this gap, we propose \emph{MetaMath}, a fine-tuned
language model that specializes in mathematical reasoning. Specifically, we
start by bootstrapping mathematical questions by rewriting the question from
multiple perspectives without extra knowledge, which results in a new dataset
called {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA.
Experimental results on two popular benchmarks (\ie, GSM8K and MATH) for
mathematical reasoning demonstrate that MetaMath outperforms a suite of
open-source LLMs by a significant margin. Our MetaMath-7B model achieves
$66.4\%$ on GSM8K and $19.4\%$ on MATH, exceeding the state-of-the-art models
of the same size by $11.5\%$ and $8.7\%$. Particularly, {MetaMath-70B} achieves
an accuracy of $82.3\%$ on {GSM8K}, slightly better than {GPT-3.5-Turbo}. We
release the {MetaMathQA} dataset, the {MetaMath} models with different model
sizes and the training code for public use.
</p></li>
</ul>

<h3>Title: Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models. (arXiv:2309.12294v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12294">http://arxiv.org/abs/2309.12294</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12294]] Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models(http://arxiv.org/abs/2309.12294)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated impressive capabilities in
natural language generation. However, their output quality can be inconsistent,
posing challenges for generating natural language from logical forms (LFs).
This task requires the generated outputs to embody the exact semantics of LFs,
without missing any LF semantics or creating any hallucinations. In this work,
we tackle this issue by proposing a novel generate-and-rerank approach. Our
approach involves initially generating a set of candidate outputs by prompting
an LLM and subsequently reranking them using a task-specific reranker model. In
addition, we curate a manually collected dataset to evaluate the alignment
between different ranking metrics and human judgements. The chosen ranking
metrics are utilized to enhance the training and evaluation of the reranker
model. By conducting extensive experiments on three diverse datasets, we
demonstrate that the candidates selected by our reranker outperform those
selected by baseline methods in terms of semantic consistency and fluency, as
measured by three comprehensive metrics. Our findings provide strong evidence
for the effectiveness of our approach in improving the quality of generated
outputs.
</p></li>
</ul>

<h3>Title: LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12307">http://arxiv.org/abs/2309.12307</a></li>
<li>Code URL: https://github.com/dvlab-research/longlora</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12307]] LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models(http://arxiv.org/abs/2309.12307)</code></li>
<li>Summary: <p>We present LongLoRA, an efficient fine-tuning approach that extends the
context sizes of pre-trained large language models (LLMs), with limited
computation cost. Typically, training LLMs with long context sizes is
computationally expensive, requiring extensive training hours and GPU
resources. For example, training on the context length of 8192 needs 16x
computational costs in self-attention layers as that of 2048. In this paper, we
speed up the context extension of LLMs in two aspects. On the one hand,
although dense global attention is needed during inference, fine-tuning the
model can be effectively and efficiently done by sparse local attention. The
proposed shift short attention effectively enables context extension, leading
to non-trivial computation saving with similar performance to fine-tuning with
vanilla attention. Particularly, it can be implemented with only two lines of
code in training, while being optional in inference. On the other hand, we
revisit the parameter-efficient fine-tuning regime for context expansion.
Notably, we find that LoRA for context extension works well under the premise
of trainable embedding and normalization. LongLoRA demonstrates strong
empirical results on various tasks on LLaMA2 models from 7B/13B to 70B.
LongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on a
single 8x A100 machine. LongLoRA extends models' context while retaining their
original architectures, and is compatible with most existing techniques, like
FlashAttention-2. In addition, to make LongLoRA practical, we collect a
dataset, LongQA, for supervised fine-tuning. It contains more than 3k long
context question-answer pairs.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Revisiting Kernel Temporal Segmentation as an Adaptive Tokenizer for Long-form Video Understanding. (arXiv:2309.11569v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11569">http://arxiv.org/abs/2309.11569</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11569]] Revisiting Kernel Temporal Segmentation as an Adaptive Tokenizer for Long-form Video Understanding(http://arxiv.org/abs/2309.11569)</code></li>
<li>Summary: <p>While most modern video understanding models operate on short-range clips,
real-world videos are often several minutes long with semantically consistent
segments of variable length. A common approach to process long videos is
applying a short-form video model over uniformly sampled clips of fixed
temporal length and aggregating the outputs. This approach neglects the
underlying nature of long videos since fixed-length clips are often redundant
or uninformative. In this paper, we aim to provide a generic and adaptive
sampling approach for long-form videos in lieu of the de facto uniform
sampling. Viewing videos as semantically consistent segments, we formulate a
task-agnostic, unsupervised, and scalable approach based on Kernel Temporal
Segmentation (KTS) for sampling and tokenizing long videos. We evaluate our
method on long-form video understanding tasks such as video classification and
temporal action localization, showing consistent gains over existing approaches
and achieving state-of-the-art performance on long-form video modeling.
</p></li>
</ul>

<h3>Title: Efficient Long-Short Temporal Attention Network for Unsupervised Video Object Segmentation. (arXiv:2309.11707v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11707">http://arxiv.org/abs/2309.11707</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11707]] Efficient Long-Short Temporal Attention Network for Unsupervised Video Object Segmentation(http://arxiv.org/abs/2309.11707)</code></li>
<li>Summary: <p>Unsupervised Video Object Segmentation (VOS) aims at identifying the contours
of primary foreground objects in videos without any prior knowledge. However,
previous methods do not fully use spatial-temporal context and fail to tackle
this challenging task in real-time. This motivates us to develop an efficient
Long-Short Temporal Attention network (termed LSTA) for unsupervised VOS task
from a holistic view. Specifically, LSTA consists of two dominant modules,
i.e., Long Temporal Memory and Short Temporal Attention. The former captures
the long-term global pixel relations of the past frames and the current frame,
which models constantly present objects by encoding appearance pattern.
Meanwhile, the latter reveals the short-term local pixel relations of one
nearby frame and the current frame, which models moving objects by encoding
motion pattern. To speedup the inference, the efficient projection and the
locality-based sliding window are adopted to achieve nearly linear time
complexity for the two light modules, respectively. Extensive empirical studies
on several benchmarks have demonstrated promising performances of the proposed
method with high efficiency.
</p></li>
</ul>

<h3>Title: MoDA: Leveraging Motion Priors from Videos for Advancing Unsupervised Domain Adaptation in Semantic Segmentation. (arXiv:2309.11711v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11711">http://arxiv.org/abs/2309.11711</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11711]] MoDA: Leveraging Motion Priors from Videos for Advancing Unsupervised Domain Adaptation in Semantic Segmentation(http://arxiv.org/abs/2309.11711)</code></li>
<li>Summary: <p>Unsupervised domain adaptation (UDA) is an effective approach to handle the
lack of annotations in the target domain for the semantic segmentation task. In
this work, we consider a more practical UDA setting where the target domain
contains sequential frames of the unlabeled videos which are easy to collect in
practice. A recent study suggests self-supervised learning of the object motion
from unlabeled videos with geometric constraints. We design a motion-guided
domain adaptive semantic segmentation framework (MoDA), that utilizes
self-supervised object motion to learn effective representations in the target
domain. MoDA differs from previous methods that use temporal consistency
regularization for the target domain frames. Instead, MoDA deals separately
with the domain alignment on the foreground and background categories using
different strategies. Specifically, MoDA contains foreground object discovery
and foreground semantic mining to align the foreground domain gaps by taking
the instance-level guidance from the object motion. Additionally, MoDA includes
background adversarial training which contains a background category-specific
discriminator to handle the background domain gaps. Experimental results on
multiple benchmarks highlight the effectiveness of MoDA against existing
approaches in the domain adaptive image segmentation and domain adaptive video
segmentation. Moreover, MoDA is versatile and can be used in conjunction with
existing state-of-the-art approaches to further improve performance.
</p></li>
</ul>

<h3>Title: 2DDATA: 2D Detection Annotations Transmittable Aggregation for Semantic Segmentation on Point Cloud. (arXiv:2309.11755v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11755">http://arxiv.org/abs/2309.11755</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11755]] 2DDATA: 2D Detection Annotations Transmittable Aggregation for Semantic Segmentation on Point Cloud(http://arxiv.org/abs/2309.11755)</code></li>
<li>Summary: <p>Recently, multi-modality models have been introduced because of the
complementary information from different sensors such as LiDAR and cameras. It
requires paired data along with precise calibrations for all modalities, the
complicated calibration among modalities hugely increases the cost of
collecting such high-quality datasets, and hinder it from being applied to
practical scenarios. Inherit from the previous works, we not only fuse the
information from multi-modality without above issues, and also exhaust the
information in the RGB modality. We introduced the 2D Detection Annotations
Transmittable Aggregation(\textbf{2DDATA}), designing a data-specific branch,
called \textbf{Local Object Branch}, which aims to deal with points in a
certain bounding box, because of its easiness of acquiring 2D bounding box
annotations. We demonstrate that our simple design can transmit bounding box
prior information to the 3D encoder model, proving the feasibility of large
multi-modality models fused with modality-specific data.
</p></li>
</ul>

<h3>Title: SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks. (arXiv:2309.11758v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11758">http://arxiv.org/abs/2309.11758</a></li>
<li>Code URL: https://github.com/shellredia/sam-octa</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11758]] SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks(http://arxiv.org/abs/2309.11758)</code></li>
<li>Summary: <p>In the analysis of optical coherence tomography angiography (OCTA) images,
the operation of segmenting specific targets is necessary. Existing methods
typically train on supervised datasets with limited samples (approximately a
few hundred), which can lead to overfitting. To address this, the low-rank
adaptation technique is adopted for foundation model fine-tuning and proposed
corresponding prompt point generation strategies to process various
segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been
experimented on the publicly available OCTA-500 dataset. While achieving
state-of-the-art performance metrics, this method accomplishes local vessel
segmentation as well as effective artery-vein segmentation, which was not
well-solved in previous works. The code is available at:
https://github.com/ShellRedia/SAM-OCTA.
</p></li>
</ul>

<h3>Title: MoPA: Multi-Modal Prior Aided Domain Adaptation for 3D Semantic Segmentation. (arXiv:2309.11839v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11839">http://arxiv.org/abs/2309.11839</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11839]] MoPA: Multi-Modal Prior Aided Domain Adaptation for 3D Semantic Segmentation(http://arxiv.org/abs/2309.11839)</code></li>
<li>Summary: <p>Multi-modal unsupervised domain adaptation (MM-UDA) for 3D semantic
segmentation is a practical solution to embed semantic understanding in
autonomous systems without expensive point-wise annotations. While previous
MM-UDA methods can achieve overall improvement, they suffer from significant
class-imbalanced performance, restricting their adoption in real applications.
This imbalanced performance is mainly caused by: 1) self-training with
imbalanced data and 2) the lack of pixel-wise 2D supervision signals. In this
work, we propose Multi-modal Prior Aided (MoPA) domain adaptation to improve
the performance of rare objects. Specifically, we develop Valid Ground-based
Insertion (VGI) to rectify the imbalance supervision signals by inserting prior
rare objects collected from the wild while avoiding introducing artificial
artifacts that lead to trivial solutions. Meanwhile, our SAM consistency loss
leverages the 2D prior semantic masks from SAM as pixel-wise supervision
signals to encourage consistent predictions for each object in the semantic
mask. The knowledge learned from modal-specific prior is then shared across
modalities to achieve better rare object segmentation. Extensive experiments
show that our method achieves state-of-the-art performance on the challenging
MM-UDA benchmark. Code will be available at https://github.com/AronCao49/MoPA.
</p></li>
</ul>

<h3>Title: TCOVIS: Temporally Consistent Online Video Instance Segmentation. (arXiv:2309.11857v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11857">http://arxiv.org/abs/2309.11857</a></li>
<li>Code URL: https://github.com/jun-long-li/tcovis</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11857]] TCOVIS: Temporally Consistent Online Video Instance Segmentation(http://arxiv.org/abs/2309.11857)</code></li>
<li>Summary: <p>In recent years, significant progress has been made in video instance
segmentation (VIS), with many offline and online methods achieving
state-of-the-art performance. While offline methods have the advantage of
producing temporally consistent predictions, they are not suitable for
real-time scenarios. Conversely, online methods are more practical, but
maintaining temporal consistency remains a challenging task. In this paper, we
propose a novel online method for video instance segmentation, called TCOVIS,
which fully exploits the temporal information in a video clip. The core of our
method consists of a global instance assignment strategy and a spatio-temporal
enhancement module, which improve the temporal consistency of the features from
two aspects. Specifically, we perform global optimal matching between the
predictions and ground truth across the whole video clip, and supervise the
model with the global optimal objective. We also capture the spatial feature
and aggregate it with the semantic feature between frames, thus realizing the
spatio-temporal enhancement. We evaluate our method on four widely adopted VIS
benchmarks, namely YouTube-VIS 2019/2021/2022 and OVIS, and achieve
state-of-the-art performance on all benchmarks without bells-and-whistles. For
instance, on YouTube-VIS 2021, TCOVIS achieves 49.5 AP and 61.3 AP with
ResNet-50 and Swin-L backbones, respectively. Code is available at
https://github.com/jun-long-li/TCOVIS.
</p></li>
</ul>

<h3>Title: Multi-level Asymmetric Contrastive Learning for Medical Image Segmentation Pre-training. (arXiv:2309.11876v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11876">http://arxiv.org/abs/2309.11876</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11876]] Multi-level Asymmetric Contrastive Learning for Medical Image Segmentation Pre-training(http://arxiv.org/abs/2309.11876)</code></li>
<li>Summary: <p>Contrastive learning, which is a powerful technique for learning image-level
representations from unlabeled data, leads a promising direction to dealing
with the dilemma between large-scale pre-training and limited labeled data.
However, most existing contrastive learning strategies are designed mainly for
downstream tasks of natural images, therefore they are sub-optimal and even
worse than learning from scratch when directly applied to medical images whose
downstream tasks are usually segmentation. In this work, we propose a novel
asymmetric contrastive learning framework named JCL for medical image
segmentation with self-supervised pre-training. Specifically, (1) A novel
asymmetric contrastive learning strategy is proposed to pre-train both encoder
and decoder simultaneously in one-stage to provide better initialization for
segmentation models. (2) A multi-level contrastive loss is designed to take the
correspondence among feature-level, image-level and pixel-level projections,
respectively into account to make sure multi-level representations can be
learned by the encoder and decoder during pre-training. (3) Experiments on
multiple medical image datasets indicate our JCL framework outperforms existing
SOTA contrastive learning strategies.
</p></li>
</ul>

<h3>Title: NeuralLabeling: A versatile toolset for labeling vision datasets using Neural Radiance Fields. (arXiv:2309.11966v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11966">http://arxiv.org/abs/2309.11966</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11966]] NeuralLabeling: A versatile toolset for labeling vision datasets using Neural Radiance Fields(http://arxiv.org/abs/2309.11966)</code></li>
<li>Summary: <p>We present NeuralLabeling, a labeling approach and toolset for annotating a
scene using either bounding boxes or meshes and generating segmentation masks,
affordance maps, 2D bounding boxes, 3D bounding boxes, 6DOF object poses, depth
maps and object meshes. NeuralLabeling uses Neural Radiance Fields (NeRF) as
renderer, allowing labeling to be performed using 3D spatial tools while
incorporating geometric clues such as occlusions, relying only on images
captured from multiple viewpoints as input. To demonstrate the applicability of
NeuralLabeling to a practical problem in robotics, we added ground truth depth
maps to 30000 frames of transparent object RGB and noisy depth maps of glasses
placed in a dishwasher captured using an RGBD sensor, yielding the
Dishwasher30k dataset. We show that training a simple deep neural network with
supervision using the annotated depth maps yields a higher reconstruction
performance than training with the previously applied weakly supervised
approach.
</p></li>
</ul>

<h3>Title: FourierLoss: Shape-Aware Loss Function with Fourier Descriptors. (arXiv:2309.12106v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12106">http://arxiv.org/abs/2309.12106</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12106]] FourierLoss: Shape-Aware Loss Function with Fourier Descriptors(http://arxiv.org/abs/2309.12106)</code></li>
<li>Summary: <p>Encoder-decoder networks become a popular choice for various medical image
segmentation tasks. When they are trained with a standard loss function, these
networks are not explicitly enforced to preserve the shape integrity of an
object in an image. However, this ability of the network is important to obtain
more accurate results, especially when there is a low-contrast difference
between the object and its surroundings. In response to this issue, this work
introduces a new shape-aware loss function, which we name FourierLoss. This
loss function relies on quantifying the shape dissimilarity between the ground
truth and the predicted segmentation maps through the Fourier descriptors
calculated on their objects, and penalizing this dissimilarity in network
training. Different than the previous studies, FourierLoss offers an adaptive
loss function with trainable hyperparameters that control the importance of the
level of the shape details that the network is enforced to learn in the
training process. This control is achieved by the proposed adaptive loss update
mechanism, which end-to-end learns the hyperparameters simultaneously with the
network weights by backpropagation. As a result of using this mechanism, the
network can dynamically change its attention from learning the general outline
of an object to learning the details of its contour points, or vice versa, in
different training epochs. Working on 2879 computed tomography images of 93
subjects, our experiments revealed that the proposed adaptive shape-aware loss
function led to statistically significantly better results for liver
segmentation, compared to its counterparts.
</p></li>
</ul>

<h3>Title: SANPO: A Scene Understanding, Accessibility, Navigation, Pathfinding, Obstacle Avoidance Dataset. (arXiv:2309.12172v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.12172">http://arxiv.org/abs/2309.12172</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.12172]] SANPO: A Scene Understanding, Accessibility, Navigation, Pathfinding, Obstacle Avoidance Dataset(http://arxiv.org/abs/2309.12172)</code></li>
<li>Summary: <p>We introduce SANPO, a large-scale egocentric video dataset focused on dense
prediction in outdoor environments. It contains stereo video sessions collected
across diverse outdoor environments, as well as rendered synthetic video
sessions. (Synthetic data was provided by Parallel Domain.) All sessions have
(dense) depth and odometry labels. All synthetic sessions and a subset of real
sessions have temporally consistent dense panoptic segmentation labels. To our
knowledge, this is the first human egocentric video dataset with both large
scale dense panoptic segmentation and depth annotations. In addition to the
dataset we also provide zero-shot baselines and SANPO benchmarks for future
research. We hope that the challenging nature of SANPO will help advance the
state-of-the-art in video segmentation, depth estimation, multi-task visual
modeling, and synthetic-to-real domain adaptation, while enabling human
navigation systems.
</p>
<p>SANPO is available here:
https://google-research-datasets.github.io/sanpo_dataset/
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
