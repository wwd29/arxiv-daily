<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Strong Anonymity for Mesh Messaging. (arXiv:2207.04145v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04145">http://arxiv.org/abs/2207.04145</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Messaging systems built on mesh networks consisting of smartphones
communicating over Bluetooth have been used by protesters around the world
after governments have disrupted Internet connectivity. Unfortunately, existing
systems have been shown to be insecure; most concerningly by not adequately
hiding metadata. This is further complicated by the fact that wireless
communication such as Bluetooth is inherently a broadcasting medium. In this
paper, we present a new threat model that captures the security requirements of
protesters in this setting. We then provide a solution that satisfies the
required security properties, hides all relevant metadata, scales to moderately
sized protests, and supports group messaging. This is achieved by broadcasting
all messages in a way that limits the overhead of duplicate messages, ensuring
that ciphertexts do not leak metadata, and limiting what can be learned by
observing user behavior. We also build a model of our system and numerically
evaluate it to support our claims and analyze how many users it supports.
Finally, we discuss further extensions that remove potential bottlenecks in
scaling and support substantially more users.
</p></li>
</ul>

<h3>Title: A Decentralised Real Estate Transfer Verification Based on Self-Sovereign Identity and Smart Contracts. (arXiv:2207.04459v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04459">http://arxiv.org/abs/2207.04459</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Since its first introduction in late 90s, the use of marketplaces has
continued to grow, today virtually everything from physical assets to services
can be purchased on digital marketplaces, real estate is not an exception. Some
marketplaces allow acclaimed asset owners to advertise their products, to which
the services gets commission/percentage from proceeds of sale/lease. Despite
the success recorded in the use of the marketplaces, they are not without
limitations which include identity and property fraud, impersonation and the
use of centralised technology with trusted parties that are prone to single
point of failures (SPOF). Being one of the most valuable assets, real estate
has been a target for marketplace fraud as impersonators take pictures of
properties they do not own, upload them on marketplace with promising prices
that lures innocent or naive buyers. This paper addresses these issues by
proposing a self sovereign identity (SSI) and smart contract based framework
for identity verification and verified transaction management on secure digital
marketplaces. First, the use of SSI technology enable methods for acquiring
verified credential (VC) that are verifiable on a decentralised blockchain
registry to identify both real estate owner(s) and real estate property.
Second, the smart contracts are used to negotiate the secure transfer of real
estate property deeds on the marketplace. To assess the viability of our
proposal we define an application scenario and compare our work with other
approaches
</p></li>
</ul>

<h3>Title: Towards Decentralized Identity Management in Multi-stakeholder 6G Networks. (arXiv:2203.00300v2 [cs.NI] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2203.00300">http://arxiv.org/abs/2203.00300</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Trust-building mechanisms among network entities of different administrative
domains will gain significant importance in 6G because a future mobile network
will be operated cooperatively by a variety of different stakeholders rather
than by a single mobile network operator. The use of trusted third party issued
certificates for initial trust establishment in multi-stakeholder 6G networks
is only advisable to a limited extent, as trusted third parties not only
represent single point of failures or attacks, but they also cannot guarantee
global independence due to national legislation and regulatory or political
influence. This article proposes to decentralize identity management in 6G
networks to enable secure mutual authentication between network entities of
different trust domains without relying on a trusted third party and to empower
network entities with the ability to shape and strengthen cross-domain trust
relationships by the exchange of verifiable credentials. A reference model for
decentralized identity management in 6G is given as an initial guide for the
fundamental design of a common identity management system whose operation and
governance are distributed equally across multiple trust domains of
interconnected and multi-stakeholder 6G ecosystems.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Hiding Your Signals: A Security Analysis of PPG-based Biometric Authentication. (arXiv:2207.04434v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04434">http://arxiv.org/abs/2207.04434</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Recently, physiological signal-based biometric systems have received wide
attention. Unlike traditional biometric features, physiological signals can not
be easily compromised (usually unobservable to human eyes).
Photoplethysmography (PPG) signal is easy to measure, making it more attractive
than many other physiological signals for biometric authentication. However,
with the advent of remote PPG (rPPG), unobservability has been challenged when
the attacker can remotely steal the rPPG signals by monitoring the victim's
face, subsequently posing a threat to PPG-based biometrics. In PPG-based
biometric authentication, current attack approaches mandate the victim's PPG
signal, making rPPG-based attacks neglected. In this paper, we firstly analyze
the security of PPG-based biometrics, including user authentication and
communication protocols. We evaluate the signal waveforms, heart rate and
inter-pulse-interval information extracted by five rPPG methods, including four
traditional optical computing methods (CHROM, POS, LGI, PCA) and one deep
learning method (CL_rPPG). We conducted experiments on five datasets (PURE,
UBFC_rPPG, UBFC_Phys, LGI_PPGI, and COHFACE) to collect a comprehensive set of
results. Our empirical studies show that rPPG poses a serious threat to the
authentication system. The success rate of the rPPG signal spoofing attack in
the user authentication system reached 0.35. The bit hit rate is 0.6 in
inter-pulse-interval-based security protocols. Further, we propose an active
defence strategy to hide the physiological signals of the face to resist the
attack. It reduces the success rate of rPPG spoofing attacks in user
authentication to 0.05. The bit hit rate was reduced to 0.5, which is at the
level of a random guess. Our strategy effectively prevents the exposure of PPG
signals to protect users' sensitive physiological data.
</p></li>
</ul>

<h3>Title: BotNet Intrusion Detection System in Internet of Things with Developed Deep Learning. (arXiv:2207.04503v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04503">http://arxiv.org/abs/2207.04503</a></li>
<li>Code URL: null</li>
<li>Summary: <p>The rapid growth of technology has led to the creation of computing networks.
The applications of the Internet of Things are becoming more and more visible
with the expansion and development of sensors and the use of a series of
equipment to connect to the Internet. Of course, the growth of any network will
also provide some challenges. The main challenge of IoT like any other network
is its security. In the field of security, there are issues such as attack
detection, authentication, encryption and the so on. One of the most important
attack is cyber-attacks that disrupt the network usage. One of the most
important attacks on the IoT is BotNet attack. The most important challenges of
this topic include very high computational complexity, lack of comparison with
previous methods, lack of scalability, high execution time, lack of review of
the proposed approach in terms of accuracy to detect and classify attacks and
intrusions. Using intrusion detection systems for the IoT is an important step
in identifying and detecting various attacks. Therefore, an algorithm that can
solve these challenges has provided a near-optimal method. Using training-based
models and algorithms such as Deep Dearning-Reinforcement Learning and XGBoost
learning in combination (DRL-XGBoost) models can be an interesting approach to
overcoming previous weaknesses. The data of this research is Bot-IoT-2018.
</p></li>
</ul>

<h3>Title: Gas Gauge: A Security Analysis Tool for Smart Contract Out-of-Gas Vulnerabilities. (arXiv:2112.14771v2 [cs.CR] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2112.14771">http://arxiv.org/abs/2112.14771</a></li>
<li>Code URL: null</li>
<li>Summary: <p>In recent years we have witnessed a dramatic increase in the adoption and
application of smart contracts in a variety of contexts such as decentralized
finance, supply chain management, and identity management. However, a critical
stumbling block to the further adoption of smart contracts is their security. A
particularly widespread class of security vulnerabilities that afflicts
Ethereum smart contracts is the gas limit denial of service(DoS) on a contract
via unbounded operations. These vulnerabilities result in a failed transaction
with an out-of-gas error and are often present in contracts containing loops
whose bounds are affected by end-user input. Note that such vulnerabilities
differ from gas limit DoS on the network via block stuffing. Therefore, we
present Gas Gauge, a tool aimed at detecting Out-of-Gas DoS vulnerabilities in
Ethereum smart contracts. Gas Gauge consists of three major components: the
Detection, Identification, and Correction Phases. The Detection Phase consists
of an accurate static analysis approach that finds and summarizes all the loops
in a smart contract. The Identification Phase uses a white-box fuzzing approach
to generate a set of inputs that causes the contract to run out of gas. The
Correction Phase uses static analysis and run-time verification to predict the
maximum loop bounds consistent with allowable gas usage and suggest appropriate
repairs to the user of the tool. Each part of the tool can be used separately
for different purposes or all together to detect, identify and help repair the
contracts vulnerable to Out-of-Gas DoS vulnerabilities. Gas Gauge was tested on
1,000 real-world solidity smart contracts deployed on the Ethereum Mainnet. The
results were compared to seven state-of-the-art static and symbolic tools, and
it was empirically demonstrated that Gas Gauge is far more effective than
competing state-of-the-art tools.
</p></li>
</ul>

<h3>Title: ADVERT: An Adaptive and Data-Driven Attention Enhancement Mechanism for Phishing Prevention. (arXiv:2106.06907v3 [cs.HC] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2106.06907">http://arxiv.org/abs/2106.06907</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Attacks exploiting the innate and the acquired vulnerabilities of human users
have posed severe threats to cybersecurity. This work proposes ADVERT, a
human-technical solution that generates adaptive visual aids in real-time to
prevent users from inadvertence and reduce their susceptibility to phishing
attacks. Based on the eye-tracking data, we extract visual states and attention
states as system-level sufficient statistics to characterize the user's visual
behaviors and attention status. By adopting a data-driven approach and two
learning feedback of different time scales, this work lays out a theoretical
foundation to analyze, evaluate, and particularly modify humans' attention
processes while they vet and recognize phishing emails. We corroborate the
effectiveness, efficiency, and robustness of ADVERT through a case study based
on the data set collected from human subject experiments conducted at New York
University. The results show that the visual aids can statistically increase
the attention level and improve the accuracy of phishing recognition from 74.6%
to a minimum of 86%. The meta-adaptation can further improve the accuracy to
91.5% (resp. 93.7%) in less than 3 (resp. 50) tuning stages.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Source-free Video Domain Adaptation by Learning Temporal Consistency for Action Recognition. (arXiv:2203.04559v4 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2203.04559">http://arxiv.org/abs/2203.04559</a></li>
<li>Code URL: <a href="https://github.com/xuyu0010/atcon">https://github.com/xuyu0010/atcon</a></li>
<li>Summary: <p>Video-based Unsupervised Domain Adaptation (VUDA) methods improve the
robustness of video models, enabling them to be applied to action recognition
tasks across different environments. However, these methods require constant
access to source data during the adaptation process. Yet in many real-world
applications, subjects and scenes in the source video domain should be
irrelevant to those in the target video domain. With the increasing emphasis on
data privacy, such methods that require source data access would raise serious
privacy issues. Therefore, to cope with such concern, a more practical domain
adaptation scenario is formulated as the Source-Free Video-based Domain
Adaptation (SFVDA). Though there are a few methods for Source-Free Domain
Adaptation (SFDA) on image data, these methods yield degenerating performance
in SFVDA due to the multi-modality nature of videos, with the existence of
additional temporal features. In this paper, we propose a novel Attentive
Temporal Consistent Network (ATCoN) to address SFVDA by learning temporal
consistency, guaranteed by two novel consistency objectives, namely feature
consistency and source prediction consistency, performed across local temporal
features. ATCoN further constructs effective overall temporal features by
attending to local temporal features based on prediction confidence. Empirical
results demonstrate the state-of-the-art performance of ATCoN across various
cross-domain action recognition benchmarks.
</p></li>
</ul>

<h3>Title: Connect the Dots: Tighter Discrete Approximations of Privacy Loss Distributions. (arXiv:2207.04380v1 [cs.DS])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04380">http://arxiv.org/abs/2207.04380</a></li>
<li>Code URL: null</li>
<li>Summary: <p>The privacy loss distribution (PLD) provides a tight characterization of the
privacy loss of a mechanism in the context of differential privacy (DP). Recent
work has shown that PLD-based accounting allows for tighter $(\varepsilon,
\delta)$-DP guarantees for many popular mechanisms compared to other known
methods. A key question in PLD-based accounting is how to approximate any
(potentially continuous) PLD with a PLD over any specified discrete support.
</p></li>
</ul>

<p>We present a novel approach to this problem. Our approach supports both
pessimistic estimation, which overestimates the hockey-stick divergence (i.e.,
$\delta$) for any value of $\varepsilon$, and optimistic estimation, which
underestimates the hockey-stick divergence. Moreover, we show that our
pessimistic estimate is the best possible among all pessimistic estimates.
Experimental evaluation shows that our approach can work with much larger
discretization intervals while keeping a similar error bound compared to
previous approaches and yet give a better approximation than existing methods.
</p>

<h3>Title: Faster Privacy Accounting via Evolving Discretization. (arXiv:2207.04381v1 [cs.DS])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04381">http://arxiv.org/abs/2207.04381</a></li>
<li>Code URL: null</li>
<li>Summary: <p>We introduce a new algorithm for numerical composition of privacy random
variables, useful for computing the accurate differential privacy parameters
for composition of mechanisms. Our algorithm achieves a running time and memory
usage of $\mathrm{polylog}(k)$ for the task of self-composing a mechanism, from
a broad class of mechanisms, $k$ times; this class, e.g., includes the
sub-sampled Gaussian mechanism, that appears in the analysis of differentially
private stochastic gradient descent. By comparison, recent work by Gopi et al.
(NeurIPS 2021) has obtained a running time of $\widetilde{O}(\sqrt{k})$ for the
same task. Our approach extends to the case of composing $k$ different
mechanisms in the same class, improving upon their running time and memory
usage from $\widetilde{O}(k^{1.5})$ to $\widetilde{O}(k)$.
</p></li>
</ul>

<h3>Title: Scalable Privacy-enhanced Benchmark Graph Generative Model for Graph Convolutional Networks. (arXiv:2207.04396v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04396">http://arxiv.org/abs/2207.04396</a></li>
<li>Code URL: null</li>
<li>Summary: <p>A surge of interest in Graph Convolutional Networks (GCN) has produced
thousands of GCN variants, with hundreds introduced every year. In contrast,
many GCN models re-use only a handful of benchmark datasets as many graphs of
interest, such as social or commercial networks, are proprietary. We propose a
new graph generation problem to enable generating a diverse set of benchmark
graphs for GCNs following the distribution of a source graph -- possibly
proprietary -- with three requirements: 1) benchmark effectiveness as a
substitute for the source graph for GCN research, 2) scalability to process
large-scale real-world graphs, and 3) a privacy guarantee for end-users. With a
novel graph encoding scheme, we reframe large-scale graph generation problem
into medium-length sequence generation problem and apply the strong generation
power of the Transformer architecture to the graph domain. Extensive
experiments across a vast body of graph generative models show that our model
can successfully generate benchmark graphs with the realistic graph structure,
node attributes, and node labels required to benchmark GCNs on node
classification tasks.
</p></li>
</ul>

<h3>Title: Differential Imaging Forensics: A Feasibility Study. (arXiv:2207.04548v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04548">http://arxiv.org/abs/2207.04548</a></li>
<li>Code URL: null</li>
<li>Summary: <p>We motivate and develop a new line of digital forensics. In the meanwhile, we
propose a novel approach to photographer identification, a rarely explored
authorship attribution problem. We report a proof-of-concept study, which shows
the feasibility of our method. Our contributions include a new forensic method
for photographer de-anonymization and revealing a novel privacy threat which
had been ignored before. The success of our creation builds on top of a new
optical side-channel which we have discovered, as well as on how to exploit it
effectively. We also make the first attempt to bridge side channels and inverse
problems, two fields that appear to be completely isolated from each other but
have deep connections.
</p></li>
</ul>

<h3>Title: APPFLChain: A Privacy Protection Distributed Artificial-Intelligence Architecture Based on Federated Learning and Consortium Blockchain. (arXiv:2206.12790v2 [cs.CR] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2206.12790">http://arxiv.org/abs/2206.12790</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Recent research in Internet of things has been widely applied for industrial
practices, fostering the exponential growth of data and connected devices.
Henceforth, data-driven AI models would be accessed by different parties
through certain data-sharing policies. However, most of the current training
procedures rely on the centralized data-collection strategy and a single
computational server. However, such a centralized scheme may lead to many
issues. Customer data stored in a centralized database may be tampered with so
the provenance and authenticity of data cannot be justified. Once the
aforementioned security concerns occur, the credibility of the trained AI
models would be questionable and even unfavorable outcomes might be produced at
the test stage. Lately, blockchain and AI, the two core technologies in
Industry 4.0 and Web 3.0, have been explored to facilitate the decentralized AI
training strategy. To serve on this very purpose, we propose a new system
architecture called APPFLChain, namely an integrated architecture of a
Hyperledger Fabric-based blockchain and a federated-learning paradigm. Our
proposed new system allows different parties to jointly train AI models and
their customers or stakeholders are connected by a consortium blockchain-based
network. Our new system can maintain a high degree of security and privacy as
users do not need to share sensitive personal information to the server. For
numerical evaluation, we simulate a real-world scenario to illustrate the whole
operational process of APPFLChain. Simulation results show that taking
advantage of the characteristics of consortium blockchain and federated
learning, APPFLChain can demonstrate favorable properties including
untamperability, traceability, privacy protection, and reliable
decision-making.
</p></li>
</ul>

<h3>Title: When Does Differentially Private Learning Not Suffer in High Dimensions?. (arXiv:2207.00160v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.00160">http://arxiv.org/abs/2207.00160</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Large pretrained models can be privately fine-tuned to achieve performance
approaching that of non-private models. A common theme in these results is the
surprising observation that high-dimensional models can achieve favorable
privacy-utility trade-offs. This seemingly contradicts known results on the
model-size dependence of differentially private convex learning and raises the
following research question: When does the performance of differentially
private learning not degrade with increasing model size? We identify that the
magnitudes of gradients projected onto subspaces is a key factor that
determines performance. To precisely characterize this for private convex
learning, we introduce a condition on the objective that we term restricted
Lipschitz continuity and derive improved bounds for the excess empirical and
population risks that are dimension-independent under additional conditions. We
empirically show that in private fine-tuning of large language models,
gradients evaluated near a local optimum are mostly controlled by a few
principal components. This behavior is similar to conditions under which we
obtain dimension-independent bounds in convex settings. Our theoretical and
empirical results together provide a possible explanation for recent successes
in large-scale private fine-tuning.
</p></li>
</ul>

<h3>Title: Enabling Binary Neural Network Training on the Edge. (arXiv:2102.04270v5 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2102.04270">http://arxiv.org/abs/2102.04270</a></li>
<li>Code URL: <a href="https://github.com/awai54st/Enabling-Binary-Neural-Network-Training-on-the-Edge">https://github.com/awai54st/Enabling-Binary-Neural-Network-Training-on-the-Edge</a></li>
<li>Summary: <p>The ever-growing computational demands of increasingly complex machine
learning models frequently necessitate the use of powerful cloud-based
infrastructure for their training. Binary neural networks are known to be
promising candidates for on-device inference due to their extreme compute and
memory savings over higher-precision alternatives. However, their existing
training methods require the concurrent storage of high-precision activations
for all layers, generally making learning on memory-constrained devices
infeasible. In this article, we demonstrate that the backward propagation
operations needed for binary neural network training are strongly robust to
quantization, thereby making on-the-edge learning with modern models a
practical proposition. We introduce a low-cost binary neural network training
strategy exhibiting sizable memory footprint reductions while inducing little
to no accuracy loss vs Courbariaux &amp; Bengio's standard approach. These
decreases are primarily enabled through the retention of activations
exclusively in binary format. Against the latter algorithm, our drop-in
replacement sees memory requirement reductions of 3--5$\times$, while reaching
similar test accuracy in comparable time, across a range of small-scale models
trained to classify popular datasets. We also demonstrate from-scratch ImageNet
training of binarized ResNet-18, achieving a 3.78$\times$ memory reduction. Our
work is open-source, and includes the Raspberry Pi-targeted prototype we used
to verify our modeled memory decreases and capture the associated energy drops.
Such savings will allow for unnecessary cloud offloading to be avoided,
reducing latency, increasing energy efficiency, and safeguarding end-user
privacy.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Not all broken defenses are equal: The dead angles of adversarial accuracy. (arXiv:2207.04129v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04129">http://arxiv.org/abs/2207.04129</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Robustness to adversarial attack is typically evaluated with adversarial
accuracy. This metric is however too coarse to properly capture all robustness
properties of machine learning models. Many defenses, when evaluated against a
strong attack, do not provide accuracy improvements while still contributing
partially to adversarial robustness. Popular certification methods suffer from
the same issue, as they provide a lower bound to accuracy. To capture finer
robustness properties we propose a new metric for L2 robustness, adversarial
angular sparsity, which partially answers the question "how many adversarial
examples are there around an input". We demonstrate its usefulness by
evaluating both "strong" and "weak" defenses. We show that some
state-of-the-art defenses, delivering very similar accuracy, can have very
different sparsity on the inputs that they are not robust on. We also show that
some weak defenses actually decrease robustness, while others strengthen it in
a measure that accuracy cannot capture. These differences are predictive of how
useful such defenses can become when combined with adversarial training.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Visual explanation of black-box model: Similarity Difference and Uniqueness (SIDU) method. (arXiv:2101.10710v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2101.10710">http://arxiv.org/abs/2101.10710</a></li>
<li>Code URL: <a href="https://github.com/satyamahesh84/SIDU_XAI_CODE">https://github.com/satyamahesh84/SIDU_XAI_CODE</a></li>
<li>Summary: <p>Explainable Artificial Intelligence (XAI) has in recent years become a
well-suited framework to generate human understandable explanations of
"black-box" models. In this paper, a novel XAI visual explanation algorithm
known as the Similarity Difference and Uniqueness (SIDU) method that can
effectively localize entire object regions responsible for prediction is
presented in full detail. The SIDU algorithm robustness and effectiveness is
analyzed through various computational and human subject experiments. In
particular, the SIDU algorithm is assessed using three different types of
evaluations (Application, Human and Functionally-Grounded) to demonstrate its
superior performance. The robustness of SIDU is further studied in the presence
of adversarial attack on "black-box" models to better understand its
performance. Our code is available at:
https://github.com/satyamahesh84/SIDU_XAI_CODE.
</p></li>
</ul>

<h3>Title: Universal Adversarial Examples in Remote Sensing: Methodology and Benchmark. (arXiv:2202.07054v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2202.07054">http://arxiv.org/abs/2202.07054</a></li>
<li>Code URL: <a href="https://github.com/yonghaoxu/uae-rs">https://github.com/yonghaoxu/uae-rs</a></li>
<li>Summary: <p>Deep neural networks have achieved great success in many important remote
sensing tasks. Nevertheless, their vulnerability to adversarial examples should
not be neglected. In this study, we systematically analyze the universal
adversarial examples in remote sensing data for the first time, without any
knowledge from the victim model. Specifically, we propose a novel black-box
adversarial attack method, namely Mixup-Attack, and its simple variant
Mixcut-Attack, for remote sensing data. The key idea of the proposed methods is
to find common vulnerabilities among different networks by attacking the
features in the shallow layer of a given surrogate model. Despite their
simplicity, the proposed methods can generate transferable adversarial examples
that deceive most of the state-of-the-art deep neural networks in both scene
classification and semantic segmentation tasks with high success rates. We
further provide the generated universal adversarial examples in the dataset
named UAE-RS, which is the first dataset that provides black-box adversarial
samples in the remote sensing field. We hope UAE-RS may serve as a benchmark
that helps researchers to design deep neural networks with strong resistance
toward adversarial attacks in the remote sensing field. Codes and the UAE-RS
dataset are available online (https://github.com/YonghaoXu/UAE-RS).
</p></li>
</ul>

<h3>Title: Invisible Backdoor Attacks Using Data Poisoning in the Frequency Domain. (arXiv:2207.04209v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04209">http://arxiv.org/abs/2207.04209</a></li>
<li>Code URL: null</li>
<li>Summary: <p>With the broad application of deep neural networks (DNNs), backdoor attacks
have gradually attracted attention. Backdoor attacks are insidious, and
poisoned models perform well on benign samples and are only triggered when
given specific inputs, which cause the neural network to produce incorrect
outputs. The state-of-the-art backdoor attack work is implemented by data
poisoning, i.e., the attacker injects poisoned samples into the dataset, and
the models trained with that dataset are infected with the backdoor. However,
most of the triggers used in the current study are fixed patterns patched on a
small fraction of an image and are often clearly mislabeled, which is easily
detected by humans or defense methods such as Neural Cleanse and SentiNet.
Also, it's difficult to be learned by DNNs without mislabeling, as they may
ignore small patterns. In this paper, we propose a generalized backdoor attack
method based on the frequency domain, which can implement backdoor implantation
without mislabeling and accessing the training process. It is invisible to
human beings and able to evade the commonly used defense methods. We evaluate
our approach in the no-label and clean-label cases on three datasets (CIFAR-10,
STL-10, and GTSRB) with two popular scenarios (self-supervised learning and
supervised learning). The results show our approach can achieve a high attack
success rate (above 90%) on all the tasks without significant performance
degradation on main tasks. Also, we evaluate the bypass performance of our
approach for different kinds of defenses, including the detection of training
data (i.e., Activation Clustering), the preprocessing of inputs (i.e.,
Filtering), the detection of inputs (i.e., SentiNet), and the detection of
models (i.e., Neural Cleanse). The experimental results demonstrate that our
approach shows excellent robustness to such defenses.
</p></li>
</ul>

<h3>Title: An Overview of Cyber Threats, Attacks, and Countermeasures on the Primary Domains of Smart Cities. (arXiv:2207.04424v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04424">http://arxiv.org/abs/2207.04424</a></li>
<li>Code URL: null</li>
<li>Summary: <p>A smart city is a place where existing facilities and services are enhanced
by digital technology to benefit people and companies. The most critical
infrastructures in this city are interconnected. Increased data exchange across
municipal domains aims to manage the essential assets, leading to more
automation in city governance and optimization of the dynamic offered services.
However, no clear guideline or standard exists for modeling these data flows.
As a result, operators, municipalities, policymakers, manufac-turers, solution
providers, and vendors are forced to accept systems with limited scalability
and varying needs. Nonetheless, it is critical to raise awareness about smart
city cybersecurity and implement suitable measures to safeguard citizens'
privacy and security because the cyber threats seem to be well-organized,
diverse, and sophisticated. This study aims to present an overview of cyber
threats, attacks, and countermeasures on the primary domains of smart cities
(smart government, smart mobility, smart environment, smart living, smart
healthcare, smart economy, and smart people) to present information extracted
from state-of-the-art to policymakers to perceive the critical situation and,
at the same time, to be a valuable resource for the scientific community.
</p></li>
</ul>

<h3>Title: Phish-Defence: Phishing Detection Using Deep Recurrent Neural Networks. (arXiv:2110.13424v3 [cs.CR] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2110.13424">http://arxiv.org/abs/2110.13424</a></li>
<li>Code URL: null</li>
<li>Summary: <p>In the growing world of the internet, the number of ways to obtain crucial
data such as passwords and login credentials, as well as sensitive personal
information has expanded. Page impersonation, often known as phishing, is one
method of obtaining such valuable information. Phishing is one of the most
straightforward forms of cyberattack for hackers and one of the simplest for
victims to fall for. It can also provide hackers with everything they need to
get access to their target's personal and corporate accounts. Such websites do
not offer a service, but instead, gather personal information from users. In
this paper, we achieved state-of-the-art accuracy in detecting malicious URLs
using recurrent neural networks. Unlike previous studies, which looked at
online content, URLs, and traffic numbers, we merely look at the text in the
URL, which makes it quicker and catches zero-day assaults. The network has been
optimised to be utilised on tiny devices like Mobiles, and Raspberry Pi without
sacrificing the inference time.
</p></li>
</ul>

<h3>Title: IPAL: Breaking up Silos of Protocol-dependent and Domain-specific Industrial Intrusion Detection Systems. (arXiv:2111.03438v2 [cs.CR] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2111.03438">http://arxiv.org/abs/2111.03438</a></li>
<li>Code URL: <a href="https://github.com/fkie-cad/ipal_datasets">https://github.com/fkie-cad/ipal_datasets</a></li>
<li>Summary: <p>The increasing interconnection of industrial networks exposes them to an
ever-growing risk of cyber attacks. To reveal such attacks early and prevent
any damage, industrial intrusion detection searches for anomalies in otherwise
predictable communication or process behavior. However, current efforts mostly
focus on specific domains and protocols, leading to a research landscape broken
up into isolated silos. Thus, existing approaches cannot be applied to other
industries that would equally benefit from powerful detection. To better
understand this issue, we survey 53 detection systems and find no fundamental
reason for their narrow focus. Although they are often coupled to specific
industrial protocols in practice, many approaches could generalize to new
industrial scenarios in theory. To unlock this potential, we propose IPAL, our
industrial protocol abstraction layer, to decouple intrusion detection from
domain-specific industrial protocols. After proving IPAL's correctness in a
reproducibility study of related work, we showcase its unique benefits by
studying the generalizability of existing approaches to new datasets and
conclude that they are indeed not restricted to specific domains or protocols
and can perform outside their restricted silos.
</p></li>
</ul>

<h3>Title: Strategic Analysis of Griefing Attack in Lightning Network. (arXiv:2203.10533v2 [cs.CR] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2203.10533">http://arxiv.org/abs/2203.10533</a></li>
<li>Code URL: <a href="https://github.com/subhramazumdar/strategic_analysis_griefing">https://github.com/subhramazumdar/strategic_analysis_griefing</a></li>
<li>Summary: <p>Hashed Timelock Contract (\emph{HTLC}) in Lightning Network is susceptible to
a \emph{griefing attack}. An attacker can block several channels and stall
payments by mounting this attack. A state-of-the-art countermeasure, Hashed
Timelock Contract with Griefing-Penalty (\emph{HTLC-GP}) is found to work under
the classical assumption of participants being either honest or malicious but
fails for rational participants. To address the gap, we introduce a
game-theoretic model for analyzing griefing attacks in \emph{HTLC}. We use this
model to analyze griefing attacks in \emph{HTLC-GP} and conjecture that it is
impossible to design an efficient protocol that will penalize a malicious
participant with the current Bitcoin scripting system. We study the impact of
the penalty on the cost of mounting the attack and observe that \emph{HTLC-GP}
is \emph{weakly effective} in disincentivizing the attacker in certain
conditions. To further increase the cost of attack, we introduce the concept of
\emph{guaranteed minimum compensation}, denoted as $\zeta$, and modify
\emph{HTLC-GP} into $\textrm{HTLC-GP}^{\zeta}$. By experimenting on several
instances of Lightning Network, we observe that the capacity locked in the
network drops to $28\%$ for $\textrm{HTLC-GP}^{\zeta}$ whereas the capacity
locked does not drop below $40\%$ for \emph{HTLC-GP}. These results justify
that $\textrm{HTLC-GP}^{\zeta}$ is better than \emph{HTLC-GP} to counter
griefing attacks.
</p></li>
</ul>

<h3>Title: Fooling Partial Dependence via Data Poisoning. (arXiv:2105.12837v3 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2105.12837">http://arxiv.org/abs/2105.12837</a></li>
<li>Code URL: <a href="https://github.com/MI2DataLab/fooling-partial-dependence">https://github.com/MI2DataLab/fooling-partial-dependence</a></li>
<li>Summary: <p>Many methods have been developed to understand complex predictive models and
high expectations are placed on post-hoc model explainability. It turns out
that such explanations are not robust nor trustworthy, and they can be fooled.
This paper presents techniques for attacking Partial Dependence (plots,
profiles, PDP), which are among the most popular methods of explaining any
predictive model trained on tabular data. We showcase that PD can be
manipulated in an adversarial manner, which is alarming, especially in
financial or medical applications where auditability became a must-have trait
supporting black-box machine learning. The fooling is performed via poisoning
the data to bend and shift explanations in the desired direction using genetic
and gradient algorithms. We believe this to be the first work using a genetic
algorithm for manipulating explanations, which is transferable as it
generalizes both ways: in a model-agnostic and an explanation-agnostic manner.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: SInGE: Sparsity via Integrated Gradients Estimation of Neuron Relevance. (arXiv:2207.04089v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04089">http://arxiv.org/abs/2207.04089</a></li>
<li>Code URL: null</li>
<li>Summary: <p>The leap in performance in state-of-the-art computer vision methods is
attributed to the development of deep neural networks. However it often comes
at a computational price which may hinder their deployment. To alleviate this
limitation, structured pruning is a well known technique which consists in
removing channels, neurons or filters, and is commonly applied in order to
produce more compact models. In most cases, the computations to remove are
selected based on a relative importance criterion. At the same time, the need
for explainable predictive models has risen tremendously and motivated the
development of robust attribution methods that highlight the relative
importance of pixels of an input image or feature map. In this work, we discuss
the limitations of existing pruning heuristics, among which magnitude and
gradient-based methods. We draw inspiration from attribution methods to design
a novel integrated gradient pruning criterion, in which the relevance of each
neuron is defined as the integral of the gradient variation on a path towards
this neuron removal. Furthermore, we propose an entwined DNN pruning and
fine-tuning flowchart to better preserve DNN accuracy while removing
parameters. We show through extensive validation on several datasets,
architectures as well as pruning scenarios that the proposed method, dubbed
SInGE, significantly outperforms existing state-of-the-art DNN pruning methods.
</p></li>
</ul>

<h3>Title: FAIVConf: Face enhancement for AI-based Video Conference with Low Bit-rate. (arXiv:2207.04090v1 [eess.IV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04090">http://arxiv.org/abs/2207.04090</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Recently, high-quality video conferencing with fewer transmission bits has
become a very hot and challenging problem. We propose FAIVConf, a specially
designed video compression framework for video conferencing, based on the
effective neural human face generation techniques. FAIVConf brings together
several designs to improve the system robustness in real video conference
scenarios: face-swapping to avoid artifacts in background animation; facial
blurring to decrease transmission bit-rate and maintain the quality of
extracted facial landmarks; and dynamic source update for face view
interpolation to accommodate a large range of head poses. Our method achieves a
significant bit-rate reduction in the video conference and gives much better
visual quality under the same bit-rate compared with H.264 and H.265 coding
schemes.
</p></li>
</ul>

<h3>Title: Cross-Attention Transformer for Video Interpolation. (arXiv:2207.04132v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04132">http://arxiv.org/abs/2207.04132</a></li>
<li>Code URL: null</li>
<li>Summary: <p>We propose TAIN (Transformers and Attention for video INterpolation), a
residual neural network for video interpolation, which aims to interpolate an
intermediate frame given two consecutive image frames around it. We first
present a novel visual transformer module, named Cross-Similarity (CS), to
globally aggregate input image features with similar appearance as those of the
predicted interpolated frame. These CS features are then used to refine the
interpolated prediction. To account for occlusions in the CS features, we
propose an Image Attention (IA) module to allow the network to focus on CS
features from one frame over those of the other. Additionally, we augment our
training dataset with an occluder patch that moves across frames to improve the
network's robustness to occlusions and large motion. Because existing methods
yield smooth predictions especially near MBs, we use an additional training
loss based on image gradient to yield sharper predictions. TAIN outperforms
existing methods that do not require flow estimation and performs comparably to
flow-based methods while being computationally efficient in terms of inference
time on Vimeo90k, UCF101, and SNU-FILM benchmarks.
</p></li>
</ul>

<h3>Title: Learning Robust Representation for Joint Grading of Ophthalmic Diseases via Adaptive Curriculum and Feature Disentanglement. (arXiv:2207.04183v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04183">http://arxiv.org/abs/2207.04183</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Diabetic retinopathy (DR) and diabetic macular edema (DME) are leading causes
of permanent blindness worldwide. Designing an automatic grading system with
good generalization ability for DR and DME is vital in clinical practice.
However, prior works either grade DR or DME independently, without considering
internal correlations between them, or grade them jointly by shared feature
representation, yet ignoring potential generalization issues caused by
difficult samples and data bias. Aiming to address these problems, we propose a
framework for joint grading with the dynamic difficulty-aware weighted loss
(DAW) and the dual-stream disentangled learning architecture (DETACH). Inspired
by curriculum learning, DAW learns from simple samples to difficult samples
dynamically via measuring difficulty adaptively. DETACH separates features of
grading tasks to avoid potential emphasis on the bias. With the addition of DAW
and DETACH, the model learns robust disentangled feature representations to
explore internal correlations between DR and DME and achieve better grading
performance. Experiments on three benchmarks show the effectiveness and
robustness of our framework under both the intra-dataset and cross-dataset
tests.
</p></li>
</ul>

<h3>Title: Domain Alignment Meets Fully Test-Time Adaptation. (arXiv:2207.04185v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04185">http://arxiv.org/abs/2207.04185</a></li>
<li>Code URL: null</li>
<li>Summary: <p>A foundational requirement of a deployed ML model is to generalize to data
drawn from a testing distribution that is different from training. A popular
solution to this problem is to adapt a pre-trained model to novel domains using
only unlabeled data. In this paper, we focus on a challenging variant of this
problem, where access to the original source data is restricted. While fully
test-time adaptation (FTTA) and unsupervised domain adaptation (UDA) are
closely related, the advances in UDA are not readily applicable to TTA, since
most UDA methods require access to the source data. Hence, we propose a new
approach, CATTAn, that bridges UDA and FTTA, by relaxing the need to access
entire source data, through a novel deep subspace alignment strategy. With a
minimal overhead of storing the subspace basis set for the source data, CATTAn
enables unsupervised alignment between source and target data during
adaptation. Through extensive experimental evaluation on multiple 2D and 3D
vision benchmarks (ImageNet-C, Office-31, OfficeHome, DomainNet, PointDA-10)
and model architectures, we demonstrate significant gains in FTTA performance.
Furthermore, we make a number of crucial findings on the utility of the
alignment objective even with inherently robust models, pre-trained ViT
representations and under low sample availability in the target domain.
</p></li>
</ul>

<h3>Title: A Study on Self-Supervised Object Detection Pretraining. (arXiv:2207.04186v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04186">http://arxiv.org/abs/2207.04186</a></li>
<li>Code URL: null</li>
<li>Summary: <p>In this work, we study different approaches to self-supervised pretraining of
object detection models. We first design a general framework to learn a
spatially consistent dense representation from an image, by randomly sampling
and projecting boxes to each augmented view and maximizing the similarity
between corresponding box features. We study existing design choices in the
literature, such as box generation, feature extraction strategies, and using
multiple views inspired by its success on instance-level image representation
learning techniques. Our results suggest that the method is robust to different
choices of hyperparameters, and using multiple views is not as effective as
shown for instance-level image representation learning. We also design two
auxiliary tasks to predict boxes in one view from their features in the other
view, by (1) predicting boxes from the sampled set by using a contrastive loss,
and (2) predicting box coordinates using a transformer, which potentially
benefits downstream object detection tasks. We found that these tasks do not
lead to better object detection performance when finetuning the pretrained
model on labeled data.
</p></li>
</ul>

<h3>Title: BOSS: Bottom-up Cross-modal Semantic Composition with Hybrid Counterfactual Training for Robust Content-based Image Retrieval. (arXiv:2207.04211v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04211">http://arxiv.org/abs/2207.04211</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Content-Based Image Retrieval (CIR) aims to search for a target image by
concurrently comprehending the composition of an example image and a
complementary text, which potentially impacts a wide variety of real-world
applications, such as internet search and fashion retrieval. In this scenario,
the input image serves as an intuitive context and background for the search,
while the corresponding language expressly requests new traits on how specific
characteristics of the query image should be modified in order to get the
intended target image. This task is challenging since it necessitates learning
and understanding the composite image-text representation by incorporating
cross-granular semantic updates. In this paper, we tackle this task by a novel
\underline{\textbf{B}}ottom-up cr\underline{\textbf{O}}ss-modal
\underline{\textbf{S}}emantic compo\underline{\textbf{S}}ition (\textbf{BOSS})
with Hybrid Counterfactual Training framework, which sheds new light on the CIR
task by studying it from two previously overlooked perspectives:
\emph{implicitly bottom-up composition of visiolinguistic representation} and
\emph{explicitly fine-grained correspondence of query-target construction}. On
the one hand, we leverage the implicit interaction and composition of
cross-modal embeddings from the bottom local characteristics to the top global
semantics, preserving and transforming the visual representation conditioned on
language semantics in several continuous steps for effective target image
search. On the other hand, we devise a hybrid counterfactual training strategy
that can reduce the model's ambiguity for similar queries.
</p></li>
</ul>

<h3>Title: Explaining Chest X-ray Pathologies in Natural Language. (arXiv:2207.04343v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04343">http://arxiv.org/abs/2207.04343</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Most deep learning algorithms lack explanations for their predictions, which
limits their deployment in clinical practice. Approaches to improve
explainability, especially in medical imaging, have often been shown to convey
limited information, be overly reassuring, or lack robustness. In this work, we
introduce the task of generating natural language explanations (NLEs) to
justify predictions made on medical images. NLEs are human-friendly and
comprehensive, and enable the training of intrinsically explainable models. To
this goal, we introduce MIMIC-NLE, the first, large-scale, medical imaging
dataset with NLEs. It contains over 38,000 NLEs, which explain the presence of
various thoracic pathologies and chest X-ray findings. We propose a general
approach to solve the task and evaluate several architectures on this dataset,
including via clinician assessment.
</p></li>
</ul>

<h3>Title: Segmentation of Blood Vessels, Optic Disc Localization, Detection of Exudates and Diabetic Retinopathy Diagnosis from Digital Fundus Images. (arXiv:2207.04345v1 [eess.IV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04345">http://arxiv.org/abs/2207.04345</a></li>
<li>Code URL: <a href="https://github.com/sohambasu07/dr_2021">https://github.com/sohambasu07/dr_2021</a></li>
<li>Summary: <p>Diabetic Retinopathy (DR) is a complication of long-standing, unchecked
diabetes and one of the leading causes of blindness in the world. This paper
focuses on improved and robust methods to extract some of the features of DR,
viz. Blood Vessels and Exudates. Blood vessels are segmented using multiple
morphological and thresholding operations. For the segmentation of exudates,
k-means clustering and contour detection on the original images are used.
Extensive noise reduction is performed to remove false positives from the
vessel segmentation algorithm's results. The localization of Optic Disc using
k-means clustering and template matching is also performed. Lastly, this paper
presents a Deep Convolutional Neural Network (DCNN) model with 14 Convolutional
Layers and 2 Fully Connected Layers, for the automatic, binary diagnosis of DR.
The vessel segmentation, optic disc localization and DCNN achieve accuracies of
95.93%, 98.77% and 75.73% respectively. The source code and pre-trained model
are available https://github.com/Sohambasu07/DR_2021
</p></li>
</ul>

<h3>Title: Progressively-connected Light Field Network for Efficient View Synthesis. (arXiv:2207.04465v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04465">http://arxiv.org/abs/2207.04465</a></li>
<li>Code URL: null</li>
<li>Summary: <p>This paper presents a Progressively-connected Light Field network (ProLiF),
for the novel view synthesis of complex forward-facing scenes. ProLiF encodes a
4D light field, which allows rendering a large batch of rays in one training
step for image- or patch-level losses. Directly learning a neural light field
from images has difficulty in rendering multi-view consistent images due to its
unawareness of the underlying 3D geometry. To address this problem, we propose
a progressive training scheme and regularization losses to infer the underlying
geometry during training, both of which enforce the multi-view consistency and
thus greatly improves the rendering quality. Experiments demonstrate that our
method is able to achieve significantly better rendering quality than the
vanilla neural light fields and comparable results to NeRF-like rendering
methods on the challenging LLFF dataset and Shiny Object dataset. Moreover, we
demonstrate better compatibility with LPIPS loss to achieve robustness to
varying light conditions and CLIP loss to control the rendering style of the
scene. Project page: https://totoro97.github.io/projects/prolif.
</p></li>
</ul>

<h3>Title: DPText-DETR: Towards Better Scene Text Detection with Dynamic Points in Transformer. (arXiv:2207.04491v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04491">http://arxiv.org/abs/2207.04491</a></li>
<li>Code URL: <a href="https://github.com/ymy-k/dptext-detr">https://github.com/ymy-k/dptext-detr</a></li>
<li>Summary: <p>Recently, Transformer-based methods, which predict polygon points or Bezier
curve control points to localize texts, are quite popular in scene text
detection. However, the used point label form implies the reading order of
humans, which affects the robustness of Transformer model. As for the model
architecture, the formulation of queries used in decoder has not been fully
explored by previous methods. In this paper, we propose a concise dynamic point
scene text detection Transformer network termed DPText-DETR, which directly
uses point coordinates as queries and dynamically updates them between decoder
layers. We point out a simple yet effective positional point label form to
tackle the side effect of the original one. Moreover, an Enhanced Factorized
Self-Attention module is designed to explicitly model the circular shape of
polygon point sequences beyond non-local attention. Extensive experiments prove
the training efficiency, robustness, and state-of-the-art performance on
various arbitrary shape scene text benchmarks. Beyond detector, we observe that
existing end-to-end spotters struggle to recognize inverse-like texts. To
evaluate their performance objectively and facilitate future research, we
propose an Inverse-Text test set containing 500 manually labeled images. The
code and Inverse-Text test set will be available at
https://github.com/ymy-k/DPText-DETR.
</p></li>
</ul>

<h3>Title: An Open-Source Tool for Longitudinal Whole-Brain and White Matter Lesion Segmentation. (arXiv:2207.04534v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04534">http://arxiv.org/abs/2207.04534</a></li>
<li>Code URL: null</li>
<li>Summary: <p>In this paper we describe and validate a longitudinal method for whole-brain
segmentation of longitudinal MRI scans. It builds upon an existing whole-brain
segmentation method that can handle multi-contrast data and robustly analyze
images with white matter lesions. This method is here extended with
subject-specific latent variables that encourage temporal consistency between
its segmentation results, enabling it to better track subtle morphological
changes in dozens of neuroanatomical structures and white matter lesions. We
validate the proposed method on multiple datasets of control subjects and
patients suffering from Alzheimer's disease and multiple sclerosis, and compare
its results against those obtained with its original cross-sectional
formulation and two benchmark longitudinal methods. The results indicate that
the method attains a higher test-retest reliability, while being more sensitive
to longitudinal disease effect differences between patient groups. An
implementation is publicly available as part of the open-source neuroimaging
package FreeSurfer.
</p></li>
</ul>

<h3>Title: Inverting Adversarially Robust Networks for Image Synthesis. (arXiv:2106.06927v4 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2106.06927">http://arxiv.org/abs/2106.06927</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Despite unconditional feature inversion being the foundation of many image
synthesis applications, training an inverter demands a high computational
budget, large decoding capacity and imposing conditions such as autoregressive
priors. To address these limitations, we propose the use of adversarially
robust representations as a perceptual primitive for feature inversion. We
train an adversarially robust encoder to extract disentangled and
perceptually-aligned image representations, making them easily invertible. By
training a simple generator with the mirror architecture of the encoder, we
achieve superior reconstruction quality and generalization over standard
models. Based on this, we propose an adversarially robust autoencoder and
demonstrate its improved performance on style transfer, image denoising and
anomaly detection tasks. Compared to recent ImageNet feature inversion methods,
our model attains improved performance with significantly less complexity.
</p></li>
</ul>

<h3>Title: FINT: Field-aware INTeraction Neural Network For CTR Prediction. (arXiv:2107.01999v3 [cs.IR] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2107.01999">http://arxiv.org/abs/2107.01999</a></li>
<li>Code URL: <a href="https://github.com/zhishan01/FINT">https://github.com/zhishan01/FINT</a></li>
<li>Summary: <p>As a critical component for online advertising and marking, click-through
rate (CTR) prediction has draw lots of attentions from both industry and
academia field. Recently, the deep learning has become the mainstream
methodological choice for CTR. Despite of sustainable efforts have been made,
existing approaches still pose several challenges. On the one hand, high-order
interaction between the features is under-explored. On the other hand,
high-order interactions may neglect the semantic information from the low-order
fields. In this paper, we proposed a novel prediction method, named FINT, that
employs the Field-aware INTeraction layer which captures high-order feature
interactions while retaining the low-order field information. To empirically
investigate the effectiveness and robustness of the FINT, we perform extensive
experiments on the three realistic databases: KDD2012, Criteo and Avazu. The
obtained results demonstrate that the FINT can significantly improve the
performance compared to the existing methods, without increasing the amount of
computation required. Moreover, the proposed method brought about 2.72\%
increase to the advertising revenue of a big online video app through A/B
testing. To better promote the research in CTR field, we released our code as
well as reference implementation at: https://github.com/zhishan01/FINT.
</p></li>
</ul>

<h3>Title: Tensor Full Feature Measure and Its Nonconvex Relaxation Applications to Tensor Recovery. (arXiv:2109.12257v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2109.12257">http://arxiv.org/abs/2109.12257</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Tensor sparse modeling as a promising approach, in the whole of science and
engineering has been a huge success. As is known to all, various data in
practical application are often generated by multiple factors, so the use of
tensors to represent the data containing the internal structure of multiple
factors came into being. However, different from the matrix case, constructing
reasonable sparse measure of tensor is a relatively difficult and very
important task. Therefore, in this paper, we propose a new tensor sparsity
measure called Tensor Full Feature Measure (FFM). It can simultaneously
describe the feature information of each dimension of the tensor and the
related features between two dimensions, and connect the Tucker rank with the
tensor tube rank. This measurement method can describe the sparse features of
the tensor more comprehensively. On this basis, we establish its non-convex
relaxation, and apply FFM to low rank tensor completion (LRTC) and tensor
robust principal component analysis (TRPCA). LRTC and TRPCA models based on FFM
are proposed, and two efficient Alternating Direction Multiplier Method (ADMM)
algorithms are developed to solve the proposed model. A variety of real
numerical experiments substantiate the superiority of the proposed methods
beyond state-of-the-arts.
</p></li>
</ul>

<h3>Title: CTRN: Class-Temporal Relational Network for Action Detection. (arXiv:2110.13473v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2110.13473">http://arxiv.org/abs/2110.13473</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Action detection is an essential and challenging task, especially for densely
labelled datasets of untrimmed videos. There are many real-world challenges in
those datasets, such as composite action, co-occurring action, and high
temporal variation of instance duration. For handling these challenges, we
propose to explore both the class and temporal relations of detected actions.
In this work, we introduce an end-to-end network: Class-Temporal Relational
Network (CTRN). It contains three key components: (1) The Representation
Transform Module filters the class-specific features from the mixed
representations to build graph-structured data. (2) The Class-Temporal Module
models the class and temporal relations in a sequential manner. (3)
G-classifier leverages the privileged knowledge of the snippet-wise
co-occurring action pairs to further improve the co-occurring action detection.
We evaluate CTRN on three challenging densely labelled datasets and achieve
state-of-the-art performance, reflecting the effectiveness and robustness of
our method.
</p></li>
</ul>

<h3>Title: Robust deep learning-based semantic organ segmentation in hyperspectral images. (arXiv:2111.05408v2 [eess.IV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2111.05408">http://arxiv.org/abs/2111.05408</a></li>
<li>Code URL: <a href="https://github.com/imsy-dkfz/htc">https://github.com/imsy-dkfz/htc</a></li>
<li>Summary: <p>Semantic image segmentation is an important prerequisite for
context-awareness and autonomous robotics in surgery. The state of the art has
focused on conventional RGB video data acquired during minimally invasive
surgery, but full-scene semantic segmentation based on spectral imaging data
and obtained during open surgery has received almost no attention to date. To
address this gap in the literature, we are investigating the following research
questions based on hyperspectral imaging (HSI) data of pigs acquired in an open
surgery setting: (1) What is an adequate representation of HSI data for neural
network-based fully automated organ segmentation, especially with respect to
the spatial granularity of the data (pixels vs. superpixels vs. patches vs.
full images)? (2) Is there a benefit of using HSI data compared to other
modalities, namely RGB data and processed HSI data (e.g. tissue parameters like
oxygenation), when performing semantic organ segmentation? According to a
comprehensive validation study based on 506 HSI images from 20 pigs, annotated
with a total of 19 classes, deep learning-based segmentation performance
increases, consistently across modalities, with the spatial context of the
input data. Unprocessed HSI data offers an advantage over RGB data or processed
data from the camera provider, with the advantage increasing with decreasing
size of the input to the neural network. Maximum performance (HSI applied to
whole images) yielded a mean DSC of 0.90 ((standard deviation (SD)) 0.04),
which is in the range of the inter-rater variability (DSC of 0.89 ((standard
deviation (SD)) 0.07)). We conclude that HSI could become a powerful image
modality for fully-automatic surgical scene understanding with many advantages
over traditional imaging, including the ability to recover additional
functional tissue information. Code and pre-trained models:
https://github.com/IMSY-DKFZ/htc.
</p></li>
</ul>

<h3>Title: Multiple Hypothesis Hypergraph Tracking for Posture Identification in Embryonic Caenorhabditis elegans. (arXiv:2111.06425v2 [eess.IV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2111.06425">http://arxiv.org/abs/2111.06425</a></li>
<li>Code URL: <a href="https://github.com/lauziere/mhht">https://github.com/lauziere/mhht</a></li>
<li>Summary: <p>Current methods in multiple object tracking (MOT) rely on independent object
trajectories undergoing predictable motion to effectively track large numbers
of objects. Adversarial conditions such as volatile object motion and imperfect
detections create a challenging tracking landscape in which established methods
may yield inadequate results. Multiple hypothesis hypergraph tracking (MHHT) is
developed to perform MOT among interdependent objects amid noisy detections.
The method extends traditional multiple hypothesis tracking (MHT) via
hypergraphs to model correlated object motion, allowing for robust tracking in
challenging scenarios. MHHT is applied to perform seam cell tracking during
late-stage embryogenesis in embryonic C. elegans.
</p></li>
</ul>

<h3>Title: Free-Viewpoint RGB-D Human Performance Capture and Rendering. (arXiv:2112.13889v3 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2112.13889">http://arxiv.org/abs/2112.13889</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Capturing and faithfully rendering photo-realistic humans from novel views is
a fundamental problem for AR/VR applications. While prior work has shown
impressive performance capture results in laboratory settings, it is
non-trivial to achieve casual free-viewpoint human capture and rendering for
unseen identities with high fidelity, especially for facial expressions, hands,
and clothes. To tackle these challenges we introduce a novel view synthesis
framework that generates realistic renders from unseen views of any human
captured from a single-view and sparse RGB-D sensor, similar to a low-cost
depth camera, and without actor-specific models. We propose an architecture to
create dense feature maps in novel views obtained by sphere-based neural
rendering, and create complete renders using a global context inpainting model.
Additionally, an enhancer network leverages the overall fidelity, even in
occluded areas from the original view, producing crisp renders with fine
details. We show that our method generates high-quality novel views of
synthetic and real human actors given a single-stream, sparse RGB-D input. It
generalizes to unseen identities, and new poses and faithfully reconstructs
facial expressions. Our approach outperforms prior view synthesis methods and
is robust to different levels of depth sparsity.
</p></li>
</ul>

<h3>Title: Tensor Recovery Based on Tensor Equivalent Minimax-Concave Penalty. (arXiv:2201.12709v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2201.12709">http://arxiv.org/abs/2201.12709</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Tensor recovery is an important problem in computer vision and machine
learning. It usually uses the convex relaxation of tensor rank and $l_{0}$
norm, i.e., the nuclear norm and $l_{1}$ norm respectively, to solve such
problem. Convex approximations are known to produce biased estimators. To
overcome this problem, a corresponding non-convex regularizer is adopted and
designed. Inspired by the recently developed matrix equivalent Minimax-Concave
Penalty (EMCP) theorem, a theorem of tensor equivalent Minimax-Concave Penalty
(TEMCP) is established in this paper. Tensor equivalent MCP (TEMCP) as the
non-convex regularizer part and equivalent weighted tensor $\gamma$ norm
(EWTGN) as the low-rank part are constructed, both of which can achieve weight
adaptive. Meanwhile, we propose two corresponding adaptive models for two
classical tensor recovery problems, namely, low-rank tensor completion (LRTC)
and tensor robust principal component analysis (TRPCA), in which the
optimization algorithm is based on alternating direction multiplier (ADMM).
This novel iterative adaptive algorithm is devised, which can produce more
accurate tensor recovery effect. For the tensor completion model, multispectral
image (MSI), magnetic resonance imaging (MRI) and color video (CV) data are
considered, while for the tensor robust principal component analysis model,
hyperspectral image (HSI) denoising under gaussian noise plus salt and pepper
noise is considered. The proposed algorithm is superior to the state-of-arts
method, and the reduction and convergence of which are guaranteed through
experiments.
</p></li>
</ul>

<h3>Title: RelMobNet: End-to-end relative camera pose estimation using a robust two-stage training. (arXiv:2202.12838v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2202.12838">http://arxiv.org/abs/2202.12838</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Relative camera pose estimation, i.e. estimating the translation and rotation
vectors using a pair of images taken in different locations, is an important
part of systems in augmented reality and robotics. In this paper, we present an
end-to-end relative camera pose estimation network using a siamese architecture
that is independent of camera parameters. The network is trained using the
Cambridge Landmarks data with four individual scene datasets and a dataset
combining the four scenes. To improve generalization, we propose a novel
two-stage training that alleviates the need of a hyperparameter to balance the
translation and rotation loss scale. The proposed method is compared with
one-stage training CNN-based methods such as RPNet and RCPNet and demonstrate
that the proposed model improves translation vector estimation by 16.11%,
28.88%, and 52.27% on the Kings College, Old Hospital, and St Marys Church
scenes, respectively. For proving texture invariance, we investigate the
generalization of the proposed method augmenting the datasets to different
scene styles, as ablation studies, using generative adversarial networks. Also,
we present a qualitative assessment of epipolar lines of our network
predictions and ground truth poses.
</p></li>
</ul>

<h3>Title: Semantic-Aware Latent Space Exploration for Face Image Restoration. (arXiv:2203.03005v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2203.03005">http://arxiv.org/abs/2203.03005</a></li>
<li>Code URL: <a href="https://github.com/liamkuo/sair">https://github.com/liamkuo/sair</a></li>
<li>Summary: <p>For image restoration, the majority of existing deep learning-based
algorithms have a tendency to overfit the training data, resulting in poor
performance when confronted with unseen degradations. To achieve more robust
restoration, generative adversarial network (GAN) prior based methods have been
proposed, demonstrating a promising capacity to restore photo-realistic and
high-quality results. However, these methods are susceptible to semantic
ambiguity, particularly with semantically relevant images such as facial
images. In this paper, we propose a semantic-aware latent space exploration
method for image restoration (SAIR). By explicitly modeling referenced
semantics information, SAIR is able to reliably restore severely degraded
images not only to high-resolution highly-realistic looks but also to correct
semantics. Quantitative and qualitative experiments collectively demonstrate
the effectiveness of the proposed SAIR. Our code can be found in
https://github.com/Liamkuo/SAIR.
</p></li>
</ul>

<h3>Title: Unitail: Detecting, Reading, and Matching in Retail Scene. (arXiv:2204.00298v3 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2204.00298">http://arxiv.org/abs/2204.00298</a></li>
<li>Code URL: null</li>
<li>Summary: <p>To make full use of computer vision technology in stores, it is required to
consider the actual needs that fit the characteristics of the retail scene.
Pursuing this goal, we introduce the United Retail Datasets (Unitail), a
large-scale benchmark of basic visual tasks on products that challenges
algorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped
instances annotated, the Unitail offers a detection dataset to align product
appearance better. Furthermore, it provides a gallery-style OCR dataset
containing 1454 product categories, 30k text regions, and 21k transcriptions to
enable robust reading on products and motivate enhanced product matching.
Besides benchmarking the datasets using various state-of-the-arts, we customize
a new detector for product detection and provide a simple OCR-based matching
solution that verifies its effectiveness.
</p></li>
</ul>

<h3>Title: A Dempster-Shafer approach to trustworthy AI with application to fetal brain MRI segmentation. (arXiv:2204.02779v2 [eess.IV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2204.02779">http://arxiv.org/abs/2204.02779</a></li>
<li>Code URL: <a href="https://github.com/lucasfidon/trustworthy-ai-fetal-brain-segmentation">https://github.com/lucasfidon/trustworthy-ai-fetal-brain-segmentation</a></li>
<li>Summary: <p>Deep learning models for medical image segmentation can fail unexpectedly and
spectacularly for pathological cases and images acquired at different centers
than training images, with labeling errors that violate expert knowledge. Such
errors undermine the trustworthiness of deep learning models for medical image
segmentation. Mechanisms for detecting and correcting such failures are
essential for safely translating this technology into clinics and are likely to
be a requirement of future regulations on artificial intelligence (AI). In this
work, we propose a trustworthy AI theoretical framework and a practical system
that can augment any backbone AI system using a fallback method and a fail-safe
mechanism based on Dempster-Shafer theory. Our approach relies on an actionable
definition of trustworthy AI. Our method automatically discards the voxel-level
labeling predicted by the backbone AI that violate expert knowledge and relies
on a fallback for those voxels. We demonstrate the effectiveness of the
proposed trustworthy AI approach on the largest reported annotated dataset of
fetal MRI consisting of 540 manually annotated fetal brain 3D T2w MRIs from 13
centers. Our trustworthy AI method improves the robustness of a
state-of-the-art backbone AI for fetal brain MRIs acquired across various
centers and for fetuses with various brain abnormalities.
</p></li>
</ul>

<h3>Title: Calibrating Class Weights with Multi-Modal Information for Partial Video Domain Adaptation. (arXiv:2204.06187v3 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2204.06187">http://arxiv.org/abs/2204.06187</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Assuming the source label space subsumes the target one, Partial Video Domain
Adaptation (PVDA) is a more general and practical scenario for cross-domain
video classification problems. The key challenge of PVDA is to mitigate the
negative transfer caused by the source-only outlier classes. To tackle this
challenge, a crucial step is to aggregate target predictions to assign class
weights by up-weighing target classes and down-weighing outlier classes.
However, the incorrect predictions of class weights can mislead the network and
lead to negative transfer. Previous works improve the class weight accuracy by
utilizing temporal features and attention mechanisms, but these methods may
fall short when trying to generate accurate class weight when domain shifts are
significant, as in most real-world scenarios. To deal with these challenges, we
propose the Multi-modality Cluster-calibrated partial Adversarial Network
(MCAN). MCAN enhances video feature extraction with multi-modal features from
multiple temporal scales to form more robust overall features. It utilizes a
novel class weight calibration method to alleviate the negative transfer caused
by incorrect class weights. The calibration method tries to identify and weigh
correct and incorrect predictions using distributional information implied by
unsupervised clustering. Extensive experiments are conducted on prevailing PVDA
benchmarks, and the proposed MCAN achieves significant improvements when
compared to state-of-the-art PVDA methods.
</p></li>
</ul>

<h3>Title: Sim-To-Real Transfer of Visual Grounding for Human-Aided Ambiguity Resolution. (arXiv:2205.12089v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2205.12089">http://arxiv.org/abs/2205.12089</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Service robots should be able to interact naturally with non-expert human
users, not only to help them in various tasks but also to receive guidance in
order to resolve ambiguities that might be present in the instruction. We
consider the task of visual grounding, where the agent segments an object from
a crowded scene given a natural language description. Modern holistic
approaches to visual grounding usually ignore language structure and struggle
to cover generic domains, therefore relying heavily on large datasets.
Additionally, their transfer performance in RGB-D datasets suffers due to high
visual discrepancy between the benchmark and the target domains. Modular
approaches marry learning with domain modeling and exploit the compositional
nature of language to decouple visual representation from language parsing, but
either rely on external parsers or are trained in an end-to-end fashion due to
the lack of strong supervision. In this work, we seek to tackle these
limitations by introducing a fully decoupled modular framework for
compositional visual grounding of entities, attributes, and spatial relations.
We exploit rich scene graph annotations generated in a synthetic domain and
train each module independently. Our approach is evaluated both in simulation
and in two real RGB-D scene datasets. Experimental results show that the
decoupled nature of our framework allows for easy integration with domain
adaptation approaches for Sim-To-Real visual recognition, offering a
data-efficient, robust, and interpretable solution to visual grounding in
robotic applications.
</p></li>
</ul>

<h3>Title: Self-Supervision on Images and Text Reduces Reliance on Visual Shortcut Features. (arXiv:2206.07155v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2206.07155">http://arxiv.org/abs/2206.07155</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Deep learning models trained in a fully supervised manner have been shown to
rely on so-called "shortcut" features. Shortcut features are inputs that are
associated with the outcome of interest in the training data, but are either no
longer associated or not present in testing or deployment settings. Here we
provide experiments that show recent self-supervised models trained on images
and text provide more robust image representations and reduce the model's
reliance on visual shortcut features on a realistic medical imaging example.
Additionally, we find that these self-supervised models "forget" shortcut
features more quickly than fully supervised ones when fine-tuned on labeled
data. Though not a complete solution, our experiments provide compelling
evidence that self-supervised models trained on images and text provide some
resilience to visual shortcut features.
</p></li>
</ul>

<h3>Title: Robustness Implies Generalization via Data-Dependent Generalization Bounds. (arXiv:2206.13497v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2206.13497">http://arxiv.org/abs/2206.13497</a></li>
<li>Code URL: null</li>
<li>Summary: <p>This paper proves that robustness implies generalization via data-dependent
generalization bounds. As a result, robustness and generalization are shown to
be connected closely in a data-dependent manner. Our bounds improve previous
bounds in two directions, to solve an open problem that has seen little
development since 2010. The first is to reduce the dependence on the covering
number. The second is to remove the dependence on the hypothesis space. We
present several examples, including ones for lasso and deep learning, in which
our bounds are provably preferable. The experiments on real-world data and
theoretical models demonstrate near-exponential improvements in various
situations. To achieve these improvements, we do not require additional
assumptions on the unknown distribution; instead, we only incorporate an
observable and computable property of the training samples. A key technical
innovation is an improved concentration bound for multinomial random variables
that is of independent interest beyond robustness and generalization.
</p></li>
</ul>

<h3>Title: Multiview Detection with Cardboard Human Modeling. (arXiv:2207.02013v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.02013">http://arxiv.org/abs/2207.02013</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Multiview detection uses multiple calibrated cameras with overlapping fields
of views to locate occluded pedestrians. In this field, existing methods
typically adopt a "human modeling - aggregation" strategy. To find robust
pedestrian representations, some intuitively use locations of detected 2D
bounding boxes, while others use entire frame features projected to the ground
plane. However, the former does not consider human appearance and leads to many
ambiguities, and the latter suffers from projection errors due to the lack of
accurate height of the human torso and head. In this paper, we propose a new
pedestrian representation scheme based on human point clouds modeling.
Specifically, using ray tracing for holistic human depth estimation, we model
pedestrians as upright, thin cardboard point clouds on the ground. Then, we
aggregate the point clouds of the pedestrian cardboard across multiple views
for a final decision. Compared with existing representations, the proposed
method explicitly leverages human appearance and reduces projection errors
significantly by relatively accurate height estimation. On two standard
evaluation benchmarks, the proposed method achieves very competitive results.
</p></li>
</ul>

<h3>Title: Graph-based Multi-View Fusion and Local Adaptation: Mitigating Within-Household Confusability for Speaker Identification. (arXiv:2207.04081v1 [eess.AS])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04081">http://arxiv.org/abs/2207.04081</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Speaker identification (SID) in the household scenario (e.g., for smart
speakers) is an important but challenging problem due to limited number of
labeled (enrollment) utterances, confusable voices, and demographic imbalances.
Conventional speaker recognition systems generalize from a large random sample
of speakers, causing the recognition to underperform for households drawn from
specific cohorts or otherwise exhibiting high confusability. In this work, we
propose a graph-based semi-supervised learning approach to improve
household-level SID accuracy and robustness with locally adapted graph
normalization and multi-signal fusion with multi-view graphs. Unlike other work
on household SID, fairness, and signal fusion, this work focuses on speaker
label inference (scoring) and provides a simple solution to realize
household-specific adaptation and multi-signal fusion without tuning the
embeddings or training a fusion network. Experiments on the VoxCeleb dataset
demonstrate that our approach consistently improves the performance across
households with different customer cohorts and degrees of confusability.
</p></li>
</ul>

<h3>Title: Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval. (arXiv:2101.00436v3 [cs.CL] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2101.00436">http://arxiv.org/abs/2101.00436</a></li>
<li>Code URL: <a href="https://github.com/stanford-futuredata/ColBERT">https://github.com/stanford-futuredata/ColBERT</a></li>
<li>Summary: <p>Multi-hop reasoning (i.e., reasoning across two or more documents) is a key
ingredient for NLP models that leverage large corpora to exhibit broad
knowledge. To retrieve evidence passages, multi-hop models must contend with a
fast-growing search space across the hops, represent complex queries that
combine multiple information needs, and resolve ambiguity about the best order
in which to hop between training passages. We tackle these problems via Baleen,
a system that improves the accuracy of multi-hop retrieval while learning
robustly from weak training signals in the many-hop setting. To tame the search
space, we propose condensed retrieval, a pipeline that summarizes the retrieved
passages after each hop into a single compact context. To model complex
queries, we introduce a focused late interaction retriever that allows
different parts of the same query representation to match disparate relevant
passages. Lastly, to infer the hopping dependencies among unordered training
passages, we devise latent hop ordering, a weak-supervision strategy in which
the trained retriever itself selects the sequence of hops. We evaluate Baleen
on retrieval for two-hop question answering and many-hop claim verification,
establishing state-of-the-art performance.
</p></li>
</ul>

<h3>Title: MultiWOZ 2.4: A Multi-Domain Task-Oriented Dialogue Dataset with Essential Annotation Corrections to Improve State Tracking Evaluation. (arXiv:2104.00773v2 [cs.CL] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2104.00773">http://arxiv.org/abs/2104.00773</a></li>
<li>Code URL: <a href="https://github.com/smartyfh/MultiWOZ2.4">https://github.com/smartyfh/MultiWOZ2.4</a></li>
<li>Summary: <p>The MultiWOZ 2.0 dataset has greatly stimulated the research of task-oriented
dialogue systems. However, its state annotations contain substantial noise,
which hinders a proper evaluation of model performance. To address this issue,
massive efforts were devoted to correcting the annotations. Three improved
versions (i.e., MultiWOZ 2.1-2.3) have then been released. Nonetheless, there
are still plenty of incorrect and inconsistent annotations. This work
introduces MultiWOZ 2.4, which refines the annotations in the validation set
and test set of MultiWOZ 2.1. The annotations in the training set remain
unchanged (same as MultiWOZ 2.1) to elicit robust and noise-resilient model
training. We benchmark eight state-of-the-art dialogue state tracking models on
MultiWOZ 2.4. All of them demonstrate much higher performance than on MultiWOZ
2.1.
</p></li>
</ul>

<h3>Title: Is Attention always needed? A Case Study on Language Identification from Speech. (arXiv:2110.03427v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2110.03427">http://arxiv.org/abs/2110.03427</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Language Identification (LID), a recommended initial step to Automatic Speech
Recognition (ASR), is used to detect a spoken language from audio specimens. In
state-of-the-art systems capable of multilingual speech processing, however,
users have to explicitly set one or more languages before using them. LID,
therefore, plays a very important role in situations where ASR based systems
cannot parse the uttered language in multilingual contexts causing failure in
speech recognition. We propose an attention based convolutional recurrent
neural network (CRNN with Attention) that works on Mel-frequency Cepstral
Coefficient (MFCC) features of audio specimens. Additionally, we reproduce some
state-of-the-art approaches, namely Convolutional Neural Network (CNN) and
Convolutional Recurrent Neural Network (CRNN), and compare them to our proposed
method. We performed extensive evaluation on thirteen different Indian
languages and our model achieves classification accuracy over 98%. Our LID
model is robust to noise and provides 91.2% accuracy in a noisy scenario. The
proposed model is easily extensible to new languages.
</p></li>
</ul>

<h3>Title: Learning Discriminative Representations and Decision Boundaries for Open Intent Detection. (arXiv:2203.05823v2 [cs.CL] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2203.05823">http://arxiv.org/abs/2203.05823</a></li>
<li>Code URL: <a href="https://github.com/thuiar/textoir">https://github.com/thuiar/textoir</a></li>
<li>Summary: <p>Open intent detection is a significant problem in natural language
understanding, which aims to detect the unseen open intent with the prior
knowledge of only known intents. Current methods have two core challenges in
this task. On the one hand, they have limitations in learning friendly
representations to detect the open intent. On the other hand, there lacks an
effective approach to obtaining specific and compact decision boundaries for
known intents. To address these issues, this paper introduces an original
framework, DA-ADB, which successively learns distance-aware intent
representations and adaptive decision boundaries for open intent detection.
Specifically, we first leverage distance information to enhance the
distinguishing capability of the intent representations. Then, we design a
novel loss function to obtain appropriate decision boundaries by balancing both
empirical and open space risks. Extensive experiments show the effectiveness of
distance-aware and boundary learning strategies. Compared with the
state-of-the-art methods, our method achieves substantial improvements on three
benchmark datasets. It also yields robust performance with different
proportions of labeled data and known categories. The full data and codes are
available at https://github.com/thuiar/TEXTOIR
</p></li>
</ul>

<h3>Title: WavThruVec: Latent speech representation as intermediate features for neural speech synthesis. (arXiv:2203.16930v2 [cs.SD] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2203.16930">http://arxiv.org/abs/2203.16930</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Recent advances in neural text-to-speech research have been dominated by
two-stage pipelines utilizing low-level intermediate speech representation such
as mel-spectrograms. However, such predetermined features are fundamentally
limited, because they do not allow to exploit the full potential of a
data-driven approach through learning hidden representations. For this reason,
several end-to-end methods have been proposed. However, such models are harder
to train and require a large number of high-quality recordings with
transcriptions. Here, we propose WavThruVec - a two-stage architecture that
resolves the bottleneck by using high-dimensional Wav2Vec 2.0 embeddings as
intermediate speech representation. Since these hidden activations provide
high-level linguistic features, they are more robust to noise. That allows us
to utilize annotated speech datasets of a lower quality to train the
first-stage module. At the same time, the second-stage component can be trained
on large-scale untranscribed audio corpora, as Wav2Vec 2.0 embeddings are
already time-aligned. This results in an increased generalization capability to
out-of-vocabulary words, as well as to a better generalization to unseen
speakers. We show that the proposed model not only matches the quality of
state-of-the-art neural models, but also presents useful properties enabling
tasks like voice conversion or zero-shot synthesis.
</p></li>
</ul>

<h3>Title: SCouT: Synthetic Counterfactuals via Spatiotemporal Transformers for Actionable Healthcare. (arXiv:2207.04208v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04208">http://arxiv.org/abs/2207.04208</a></li>
<li>Code URL: null</li>
<li>Summary: <p>The Synthetic Control method has pioneered a class of powerful data-driven
techniques to estimate the counterfactual reality of a unit from donor units.
At its core, the technique involves a linear model fitted on the
pre-intervention period that combines donor outcomes to yield the
counterfactual. However, linearly combining spatial information at each time
instance using time-agnostic weights fails to capture important inter-unit and
intra-unit temporal contexts and complex nonlinear dynamics of real data. We
instead propose an approach to use local spatiotemporal information before the
onset of the intervention as a promising way to estimate the counterfactual
sequence. To this end, we suggest a Transformer model that leverages particular
positional embeddings, a modified decoder attention mask, and a novel
pre-training task to perform spatiotemporal sequence-to-sequence modeling. Our
experiments on synthetic data demonstrate the efficacy of our method in the
typical small donor pool setting and its robustness against noise. We also
generate actionable healthcare insights at the population and patient levels by
simulating a state-wide public health policy to evaluate its effectiveness, an
in silico trial for asthma medications to support randomized controlled trials,
and a medical intervention for patients with Friedreich's ataxia to improve
clinical decision-making and promote personalized therapy.
</p></li>
</ul>

<h3>Title: Objective-aware Traffic Simulation via Inverse Reinforcement Learning. (arXiv:2105.09560v3 [cs.AI] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2105.09560">http://arxiv.org/abs/2105.09560</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Traffic simulators act as an essential component in the operating and
planning of transportation systems. Conventional traffic simulators usually
employ a calibrated physical car-following model to describe vehicles'
behaviors and their interactions with traffic environment. However, there is no
universal physical model that can accurately predict the pattern of vehicle's
behaviors in different situations. A fixed physical model tends to be less
effective in a complicated environment given the non-stationary nature of
traffic dynamics. In this paper, we formulate traffic simulation as an inverse
reinforcement learning problem, and propose a parameter sharing adversarial
inverse reinforcement learning model for dynamics-robust simulation learning.
Our proposed model is able to imitate a vehicle's trajectories in the real
world while simultaneously recovering the reward function that reveals the
vehicle's true objective which is invariant to different dynamics. Extensive
experiments on synthetic and real-world datasets show the superior performance
of our approach compared to state-of-the-art methods and its robustness to
variant dynamics of traffic.
</p></li>
</ul>

<h3>Title: MEPG: A Minimalist Ensemble Policy Gradient Framework for Deep Reinforcement Learning. (arXiv:2109.10552v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2109.10552">http://arxiv.org/abs/2109.10552</a></li>
<li>Code URL: null</li>
<li>Summary: <p>During the training of a reinforcement learning (RL) agent, the distribution
of training data is non-stationary as the agent's behavior changes over time.
Therefore, there is a risk that the agent is overspecialized to a particular
distribution and its performance suffers in the larger picture. Ensemble RL can
mitigate this issue by learning a robust policy. However, it suffers from heavy
computational resource consumption due to the newly introduced value and policy
functions. In this paper, to avoid the notorious resources consumption issue,
we design a novel and simple ensemble deep RL framework that integrates
multiple models into a single model. Specifically, we propose the
\underline{M}inimalist \underline{E}nsemble \underline{P}olicy
\underline{G}radient framework (MEPG), which introduces minimalist ensemble
consistent Bellman update by utilizing a modified dropout operator. MEPG holds
ensemble property by keeping the dropout consistency of both sides of the
Bellman equation. Additionally, the dropout operator also increases MEPG's
generalization capability. Moreover, we theoretically show that the policy
evaluation phase in the MEPG maintains two synchronized deep Gaussian
Processes. To verify the MEPG framework's ability to generalize, we perform
experiments on the gym simulator, which presents that the MEPG framework
outperforms or achieves a similar level of performance as the current
state-of-the-art ensemble methods and model-free methods without increasing
additional computational resource costs.
</p></li>
</ul>

<h3>Title: Noise-aware Physics-informed Machine Learning for Robust PDE Discovery. (arXiv:2206.12901v4 [math.NA] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2206.12901">http://arxiv.org/abs/2206.12901</a></li>
<li>Code URL: null</li>
<li>Summary: <p>This work is concerned with discovering the governing partial differential
equation (PDE) of a physical system. Existing methods have demonstrated the PDE
identification from finite observations but failed to maintain satisfying
performance against noisy data, partly owing to suboptimal estimated
derivatives and found PDE coefficients. We address the issues by introducing a
noise-aware physics-informed machine learning (nPIML) framework to discover the
governing PDE from data following arbitrary distributions. Our proposals are
twofold. First, we propose a couple of neural networks, namely solver and
preselector, which yield an interpretable neural representation of the hidden
physical constraint. After they are jointly trained, the solver network
approximates potential candidates, e.g., partial derivatives, which are then
fed to the sparse regression algorithm that initially unveils the most likely
parsimonious PDE, decided according to the information criterion. Second, we
propose the denoising physics-informed neural networks (dPINNs), based on
Discrete Fourier Transform (DFT), to deliver a set of the optimal finetuned PDE
coefficients respecting the noise-reduced variables. The denoising PINNs'
structures are compartmentalized into forefront projection networks and a PINN,
by which the formerly learned solver initializes. Our extensive experiments on
five canonical PDEs affirm that the proposed framework presents a robust and
interpretable approach for PDE discovery, applicable to a wide range of
systems, possibly complicated by noise.
</p></li>
</ul>

<h3>Title: Models Out of Line: A Fourier Lens on Distribution Shift Robustness. (arXiv:2207.04075v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04075">http://arxiv.org/abs/2207.04075</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Improving the accuracy of deep neural networks (DNNs) on out-of-distribution
(OOD) data is critical to an acceptance of deep learning (DL) in real world
applications. It has been observed that accuracies on in-distribution (ID)
versus OOD data follow a linear trend and models that outperform this baseline
are exceptionally rare (and referred to as "effectively robust"). Recently,
some promising approaches have been developed to improve OOD robustness: model
pruning, data augmentation, and ensembling or zero-shot evaluating large
pretrained models. However, there still is no clear understanding of the
conditions on OOD data and model properties that are required to observe
effective robustness. We approach this issue by conducting a comprehensive
empirical study of diverse approaches that are known to impact OOD robustness
on a broad range of natural and synthetic distribution shifts of CIFAR-10 and
ImageNet. In particular, we view the "effective robustness puzzle" through a
Fourier lens and ask how spectral properties of both models and OOD data
influence the corresponding effective robustness. We find this Fourier lens
offers some insight into why certain robust models, particularly those from the
CLIP family, achieve OOD robustness. However, our analysis also makes clear
that no known metric is consistently the best explanation (or even a strong
explanation) of OOD robustness. Thus, to aid future research into the OOD
puzzle, we address the gap in publicly-available models with effective
robustness by introducing a set of pretrained models--RobustNets--with varying
levels of OOD robustness.
</p></li>
</ul>

<h3>Title: Improved Binary Forward Exploration: Learning Rate Scheduling Method for Stochastic Optimization. (arXiv:2207.04198v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04198">http://arxiv.org/abs/2207.04198</a></li>
<li>Code URL: null</li>
<li>Summary: <p>A new gradient-based optimization approach by automatically scheduling the
learning rate has been proposed recently, which is called Binary Forward
Exploration (BFE). The Adaptive version of BFE has also been discussed
thereafter. In this paper, the improved algorithms based on them will be
investigated, in order to optimize the efficiency and robustness of the new
methodology. This improved approach provides a new perspective to scheduling
the update of learning rate and will be compared with the stochastic gradient
descent (SGD) algorithm with momentum or Nesterov momentum and the most
successful adaptive learning rate algorithm e.g. Adam. The goal of this method
does not aim to beat others but provide a different viewpoint to optimize the
gradient descent process. This approach combines the advantages of the
first-order and second-order optimizations in the aspects of speed and
efficiency.
</p></li>
</ul>

<h3>Title: On the Robustness and Anomaly Detection of Sparse Neural Networks. (arXiv:2207.04227v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04227">http://arxiv.org/abs/2207.04227</a></li>
<li>Code URL: null</li>
<li>Summary: <p>The robustness and anomaly detection capability of neural networks are
crucial topics for their safe adoption in the real-world. Moreover, the
over-parameterization of recent networks comes with high computational costs
and raises questions about its influence on robustness and anomaly detection.
In this work, we show that sparsity can make networks more robust and better
anomaly detectors. To motivate this even further, we show that a pre-trained
neural network contains, within its parameter space, sparse subnetworks that
are better at these tasks without any further training. We also show that
structured sparsity greatly helps in reducing the complexity of expensive
robustness and detection methods, while maintaining or even improving their
results on these tasks. Finally, we introduce a new method, SensNorm, which
uses the sensitivity of weights derived from an appropriate pruning method to
detect anomalous samples in the input.
</p></li>
</ul>

<h3>Title: Training Robust Deep Models for Time-Series Domain: Novel Algorithms and Theoretical Analysis. (arXiv:2207.04305v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04305">http://arxiv.org/abs/2207.04305</a></li>
<li>Code URL: <a href="https://github.com/tahabelkhouja/robust-training-for-time-series">https://github.com/tahabelkhouja/robust-training-for-time-series</a></li>
<li>Summary: <p>Despite the success of deep neural networks (DNNs) for real-world
applications over time-series data such as mobile health, little is known about
how to train robust DNNs for time-series domain due to its unique
characteristics compared to images and text data. In this paper, we propose a
novel algorithmic framework referred as RObust Training for Time-Series (RO-TS)
to create robust DNNs for time-series classification tasks. Specifically, we
formulate a min-max optimization problem over the model parameters by
explicitly reasoning about the robustness criteria in terms of additive
perturbations to time-series inputs measured by the global alignment kernel
(GAK) based distance. We also show the generality and advantages of our
formulation using the summation structure over time-series alignments by
relating both GAK and dynamic time warping (DTW). This problem is an instance
of a family of compositional min-max optimization problems, which are
challenging and open with unclear theoretical guarantee. We propose a
principled stochastic compositional alternating gradient descent ascent
(SCAGDA) algorithm for this family of optimization problems. Unlike traditional
methods for time-series that require approximate computation of distance
measures, SCAGDA approximates the GAK based distance on-the-fly using a moving
average approach. We theoretically analyze the convergence rate of SCAGDA and
provide strong theoretical support for the estimation of GAK based distance.
Our experiments on real-world benchmarks demonstrate that RO-TS creates more
robust DNNs when compared to adversarial training using prior methods that rely
on data augmentation or new definitions of loss functions. We also demonstrate
the importance of GAK for time-series data over the Euclidean distance. The
source code of RO-TS algorithms is available at
https://github.com/tahabelkhouja/Robust-Training-for-Time-Series
</p></li>
</ul>

<h3>Title: Adversarial Framework with Certified Robustness for Time-Series Domain via Statistical Features. (arXiv:2207.04307v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04307">http://arxiv.org/abs/2207.04307</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Time-series data arises in many real-world applications (e.g., mobile health)
and deep neural networks (DNNs) have shown great success in solving them.
Despite their success, little is known about their robustness to adversarial
attacks. In this paper, we propose a novel adversarial framework referred to as
Time-Series Attacks via STATistical Features (TSA-STAT)}. To address the unique
challenges of time-series domain, TSA-STAT employs constraints on statistical
features of the time-series data to construct adversarial examples. Optimized
polynomial transformations are used to create attacks that are more effective
(in terms of successfully fooling DNNs) than those based on additive
perturbations. We also provide certified bounds on the norm of the statistical
features for constructing adversarial examples. Our experiments on diverse
real-world benchmark datasets show the effectiveness of TSA-STAT in fooling
DNNs for time-series domain and in improving their robustness. The source code
of TSA-STAT algorithms is available at
https://github.com/tahabelkhouja/Time-Series-Attacks-via-STATistical-Features
</p></li>
</ul>

<h3>Title: Dynamic Time Warping based Adversarial Framework for Time-Series Domain. (arXiv:2207.04308v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04308">http://arxiv.org/abs/2207.04308</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Despite the rapid progress on research in adversarial robustness of deep
neural networks (DNNs), there is little principled work for the time-series
domain. Since time-series data arises in diverse applications including mobile
health, finance, and smart grid, it is important to verify and improve the
robustness of DNNs for the time-series domain. In this paper, we propose a
novel framework for the time-series domain referred as {\em Dynamic Time
Warping for Adversarial Robustness (DTW-AR)} using the dynamic time warping
measure. Theoretical and empirical evidence is provided to demonstrate the
effectiveness of DTW over the standard Euclidean distance metric employed in
prior methods for the image domain. We develop a principled algorithm justified
by theoretical analysis to efficiently create diverse adversarial examples
using random alignment paths. Experiments on diverse real-world benchmarks show
the effectiveness of DTW-AR to fool DNNs for time-series data and to improve
their robustness using adversarial training. The source code of DTW-AR
algorithms is available at https://github.com/tahabelkhouja/DTW-AR
</p></li>
</ul>

<h3>Title: Robust Dynamic Assortment Optimization in the Presence of Outlier Customers. (arXiv:1910.04183v2 [stat.ML] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/1910.04183">http://arxiv.org/abs/1910.04183</a></li>
<li>Code URL: null</li>
<li>Summary: <p>We consider the dynamic assortment optimization problem under the multinomial
logit model (MNL) with unknown utility parameters. The main question
investigated in this paper is model mis-specification under the
$\varepsilon$-contamination model, which is a fundamental model in robust
statistics and machine learning. In particular, throughout a selling horizon of
length $T$, we assume that customers make purchases according to a well
specified underlying multinomial logit choice model in a
$(1-\varepsilon)$-fraction of the time periods, and make arbitrary purchasing
decisions instead in the remaining $\varepsilon$-fraction of the time periods.
In this model, we develop a new robust online assortment optimization policy
via an active elimination strategy. We establish both upper and lower bounds on
the regret, and show that our policy is optimal up to logarithmic factor in $T$
when the assortment capacity is constant. %% capacity of assortments has a
constant upper limit. We further develop a fully adaptive policy that does not
require any prior knowledge of the contamination parameter $\varepsilon$. In
the case of the existence a sub-optimality gap between optimal and sub-optimal
products, we also established gap-dependent logarithmic regret upper bounds and
lower bounds in both the known-$\varepsilon$ and unknown-$\varepsilon$ cases.
Our simulation study shows that our policy outperforms the existing policies
based on upper confidence bounds (UCB) and Thompson sampling.
</p></li>
</ul>

<h3>Title: Heteroscedastic Uncertainty for Robust Generative Latent Dynamics. (arXiv:2008.08157v2 [cs.RO] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2008.08157">http://arxiv.org/abs/2008.08157</a></li>
<li>Code URL: <a href="https://github.com/utiasSTARS/robust-latent-srl">https://github.com/utiasSTARS/robust-latent-srl</a></li>
<li>Summary: <p>Learning or identifying dynamics from a sequence of high-dimensional
observations is a difficult challenge in many domains, including reinforcement
learning and control. The problem has recently been studied from a generative
perspective through latent dynamics: high-dimensional observations are embedded
into a lower-dimensional space in which the dynamics can be learned. Despite
some successes, latent dynamics models have not yet been applied to real-world
robotic systems where learned representations must be robust to a variety of
perceptual confounds and noise sources not seen during training. In this paper,
we present a method to jointly learn a latent state representation and the
associated dynamics that is amenable for long-term planning and closed-loop
control under perceptually difficult conditions. As our main contribution, we
describe how our representation is able to capture a notion of heteroscedastic
or input-specific uncertainty at test time by detecting novel or
out-of-distribution (OOD) inputs. We present results from prediction and
control experiments on two image-based tasks: a simulated pendulum balancing
task and a real-world robotic manipulator reaching task. We demonstrate that
our model produces significantly more accurate predictions and exhibits
improved control performance, compared to a model that assumes homoscedastic
uncertainty only, in the presence of varying degrees of input degradation.
</p></li>
</ul>

<h3>Title: A Survey of Deep Learning Architectures for Intelligent Reflecting Surfaces. (arXiv:2009.02540v4 [eess.SP] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2009.02540">http://arxiv.org/abs/2009.02540</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Intelligent reflecting surfaces (IRSs) have recently received significant
attention for wireless communications because it reduces the hardware
complexity, physical size, weight, and cost of conventional large arrays.
However, deployment of IRS entails dealing with multiple channel links between
the base station (BS) and the users. Further, the BS and IRS beamformers
require a joint design, wherein the IRS elements must be rapidly reconfigured.
Data-driven techniques, such as deep learning (DL), are critical in addressing
these challenges. The lower computation time and model-free nature of DL makes
it robust against the data imperfections and environmental changes. At the
physical layer, DL has been shown to be effective for IRS signal detection,
channel estimation and active/passive beamforming using architectures such as
supervised, unsupervised and reinforcement learning. This article provides a
synopsis of these techniques for designing DL-based IRS-assisted wireless
systems.
</p></li>
</ul>

<h3>Title: EdiTTS: Score-based Editing for Controllable Text-to-Speech. (arXiv:2110.02584v3 [cs.SD] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2110.02584">http://arxiv.org/abs/2110.02584</a></li>
<li>Code URL: <a href="https://github.com/neosapience/editts">https://github.com/neosapience/editts</a></li>
<li>Summary: <p>We present EdiTTS, an off-the-shelf speech editing methodology based on
score-based generative modeling for text-to-speech synthesis. EdiTTS allows for
targeted, granular editing of audio, both in terms of content and pitch,
without the need for any additional training, task-specific optimization, or
architectural modifications to the score-based model backbone. Specifically, we
apply coarse yet deliberate perturbations in the Gaussian prior space to induce
desired behavior from the diffusion model while applying masks and softening
kernels to ensure that iterative edits are applied only to the target region.
Through listening tests and speech-to-text back transcription, we show that
EdiTTS outperforms existing baselines and produces robust samples that satisfy
user-imposed requirements.
</p></li>
</ul>

<h3>Title: Towards Model Reduction for Power System Transients with Physics-Informed PDE. (arXiv:2110.14066v2 [eess.SY] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2110.14066">http://arxiv.org/abs/2110.14066</a></li>
<li>Code URL: null</li>
<li>Summary: <p>This manuscript reports the first step towards building a robust and
efficient model reduction methodology to capture transient dynamics in a
transmission level electric power system. Such dynamics is normally modeled on
seconds-to-tens-of-seconds time scales by the so-called swing equations, which
are ordinary differential equations defined on a spatially discrete model of
the power grid. Following Seymlyen (1974) and Thorpe, Seyler, and Phadke
(1999), we suggest to map the swing equations onto a linear, inhomogeneous
Partial Differential Equation (PDE) of parabolic type in two space and one time
dimensions with time-independent coefficients and properly defined boundary
conditions. We illustrate our method on the synchronous transmission grid of
continental Europe. We show that, when properly coarse-grained, i.e., with the
PDE coefficients and source terms extracted from a spatial convolution
procedure of the respective discrete coefficients in the swing equations, the
resulting PDE reproduces faithfully and efficiently the original swing
dynamics. We finally discuss future extensions of this work, where the
presented PDE-based modeling will initialize a physics-informed machine
learning approach for real-time modeling, $n-1$ feasibility assessment and
transient stability analysis of power systems.
</p></li>
</ul>

<h3>Title: Self-Supervised Representation Learning via Latent Graph Prediction. (arXiv:2202.08333v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2202.08333">http://arxiv.org/abs/2202.08333</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Self-supervised learning (SSL) of graph neural networks is emerging as a
promising way of leveraging unlabeled data. Currently, most methods are based
on contrastive learning adapted from the image domain, which requires view
generation and a sufficient number of negative samples. In contrast, existing
predictive models do not require negative sampling, but lack theoretical
guidance on the design of pretext training tasks. In this work, we propose the
LaGraph, a theoretically grounded predictive SSL framework based on latent
graph prediction. Learning objectives of LaGraph are derived as self-supervised
upper bounds to objectives for predicting unobserved latent graphs. In addition
to its improved performance, LaGraph provides explanations for recent successes
of predictive models that include invariance-based objectives. We provide
theoretical analysis comparing LaGraph to related methods in different domains.
Our experimental results demonstrate the superiority of LaGraph in performance
and the robustness to decreasing of training sample size on both graph-level
and node-level tasks.
</p></li>
</ul>

<h3>Title: Robust Classification using Contractive Hamiltonian Neural ODEs. (arXiv:2203.11805v3 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2203.11805">http://arxiv.org/abs/2203.11805</a></li>
<li>Code URL: <a href="https://github.com/decodepfl/contractive-hamiltonian-neural-odes">https://github.com/decodepfl/contractive-hamiltonian-neural-odes</a></li>
<li>Summary: <p>Deep neural networks can be fragile and sensitive to small input
perturbations that might cause a significant change in the output. In this
paper, we employ contraction theory to improve the robustness of neural ODEs
(NODEs). A dynamical system is contractive if all solutions with different
initial conditions converge to each other exponentially fast. As a consequence,
perturbations in initial conditions become less and less relevant over time.
Since in NODEs the input data corresponds to the initial condition of dynamical
systems, we show contractivity can mitigate the effect of input perturbations.
More precisely, inspired by NODEs with Hamiltonian dynamics, we propose a class
of contractive Hamiltonian NODEs (CH-NODEs). By properly tuning a scalar
parameter, CH-NODEs ensure contractivity by design and can be trained using
standard backpropagation. Moreover, CH-NODEs enjoy built-in guarantees of
non-exploding gradients, which ensure a well-posed training process. Finally,
we demonstrate the robustness of CH-NODEs on the MNIST image classification
problem with noisy test data.
</p></li>
</ul>

<h3>Title: OccamNets: Mitigating Dataset Bias by Favoring Simpler Hypotheses. (arXiv:2204.02426v3 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2204.02426">http://arxiv.org/abs/2204.02426</a></li>
<li>Code URL: <a href="https://github.com/erobic/occam-nets-v1">https://github.com/erobic/occam-nets-v1</a></li>
<li>Summary: <p>Dataset bias and spurious correlations can significantly impair
generalization in deep neural networks. Many prior efforts have addressed this
problem using either alternative loss functions or sampling strategies that
focus on rare patterns. We propose a new direction: modifying the network
architecture to impose inductive biases that make the network robust to dataset
bias. Specifically, we propose OccamNets, which are biased to favor simpler
solutions by design. OccamNets have two inductive biases. First, they are
biased to use as little network depth as needed for an individual example.
Second, they are biased toward using fewer image locations for prediction.
While OccamNets are biased toward simpler hypotheses, they can learn more
complex hypotheses if necessary. In experiments, OccamNets outperform or rival
state-of-the-art methods run on architectures that do not incorporate these
inductive biases. Furthermore, we demonstrate that when the state-of-the-art
debiasing methods are combined with OccamNets results further improve.
</p></li>
</ul>

<h3>Title: Neural Moving Horizon Estimation for Robust Flight Control. (arXiv:2206.10397v9 [cs.RO] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2206.10397">http://arxiv.org/abs/2206.10397</a></li>
<li>Code URL: <a href="https://github.com/rcl-nus/neuromhe">https://github.com/rcl-nus/neuromhe</a></li>
<li>Summary: <p>Estimating and reacting to external disturbances is crucial for robust flight
control of quadrotors. Existing estimators typically require significant tuning
for a specific flight scenario or training with extensive ground-truth
disturbance data to achieve satisfactory performance. In this paper, we propose
a neural moving horizon estimator (NeuroMHE) that can automatically tune the
key parameters modeled by a neural network and adapt to different flight
scenarios. We achieve this by deriving the analytical gradients of the MHE
estimates with respect to the weighting matrices, which enables a seamless
embedding of the MHE as a learnable layer into neural networks for highly
effective learning. Interestingly, we show that the gradients can be computed
efficiently using a Kalman filter in a recursive form. Moreover, we develop a
model-based policy gradient algorithm to train NeuroMHE directly from the
quadrotor trajectory tracking error without needing the ground-truth
disturbance data. The effectiveness of NeuroMHE is verified extensively via
both simulations and physical experiments on quadrotors in various challenging
flights. Notably, NeuroMHE outperforms the state-of-the-art neural
network-based estimator with estimation error reductions of up to about 49.4%
by using only a 2.5% amount of the neural network parameters. The proposed
method is general and can be applied to robust adaptive control of other
robotic systems.
</p></li>
</ul>

<h3>Title: UniCR: Universally Approximated Certified Robustness via Randomized Smoothing. (arXiv:2207.02152v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.02152">http://arxiv.org/abs/2207.02152</a></li>
<li>Code URL: null</li>
<li>Summary: <p>We study certified robustness of machine learning classifiers against
adversarial perturbations. In particular, we propose the first universally
approximated certified robustness (UniCR) framework, which can approximate the
robustness certification of any input on any classifier against any $\ell_p$
perturbations with noise generated by any continuous probability distribution.
Compared with the state-of-the-art certified defenses, UniCR provides many
significant benefits: (1) the first universal robustness certification
framework for the above 4 'any's; (2) automatic robustness certification that
avoids case-by-case analysis, (3) tightness validation of certified robustness,
and (4) optimality validation of noise distributions used by randomized
smoothing. We conduct extensive experiments to validate the above benefits of
UniCR and the advantages of UniCR over state-of-the-art certified defenses
against $\ell_p$ perturbations.
</p></li>
</ul>

<h3>Title: Stochastic optimal well control in subsurface reservoirs using reinforcement learning. (arXiv:2207.03456v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.03456">http://arxiv.org/abs/2207.03456</a></li>
<li>Code URL: <a href="https://github.com/atishdixit16/rl_robust_owc">https://github.com/atishdixit16/rl_robust_owc</a></li>
<li>Summary: <p>We present a case study of model-free reinforcement learning (RL) framework
to solve stochastic optimal control for a predefined parameter uncertainty
distribution and partially observable system. We focus on robust optimal well
control problem which is a subject of intensive research activities in the
field of subsurface reservoir management. For this problem, the system is
partially observed since the data is only available at well locations.
Furthermore, the model parameters are highly uncertain due to sparsity of
available field data. In principle, RL algorithms are capable of learning
optimal action policies -- a map from states to actions -- to maximize a
numerical reward signal. In deep RL, this mapping from state to action is
parameterized using a deep neural network. In the RL formulation of the robust
optimal well control problem, the states are represented by saturation and
pressure values at well locations while the actions represent the valve
openings controlling the flow through wells. The numerical reward refers to the
total sweep efficiency and the uncertain model parameter is the subsurface
permeability field. The model parameter uncertainties are handled by
introducing a domain randomisation scheme that exploits cluster analysis on its
uncertainty distribution. We present numerical results using two
state-of-the-art RL algorithms, proximal policy optimization (PPO) and
advantage actor-critic (A2C), on two subsurface flow test cases representing
two distinct uncertainty distributions of permeability field. The results were
benchmarked against optimisation results obtained using differential evolution
algorithm. Furthermore, we demonstrate the robustness of the proposed use of RL
by evaluating the learned control policy on unseen samples drawn from the
parameter uncertainty distribution that were not used during the training
process.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: A Survey of Task-Based Machine Learning Content Extraction Services for VIDINT. (arXiv:2207.04158v1 [cs.ET])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04158">http://arxiv.org/abs/2207.04158</a></li>
<li>Code URL: null</li>
<li>Summary: <p>This paper provides a comparison of current video content extraction tools
with a focus on comparing commercial task-based machine learning services.
Video intelligence (VIDINT) data has become a critical intelligence source in
the past decade. The need for AI-based analytics and automation tools to
extract and structure content from video has quickly become a priority for
organizations needing to search, analyze and exploit video at scale. With rapid
growth in machine learning technology, the maturity of machine transcription,
machine translation, topic tagging, and object recognition tasks are improving
at an exponential rate, breaking performance records in speed and accuracy as
new applications evolve. Each section of this paper reviews and compares
products, software resources and video analytics capabilities based on tasks
relevant to extracting information from video with machine learning techniques.
</p></li>
</ul>

<h3>Title: Dual-path Attention is All You Need for Audio-Visual Speech Extraction. (arXiv:2207.04213v1 [cs.MM])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04213">http://arxiv.org/abs/2207.04213</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Audio-visual target speech extraction, which aims to extract a certain
speaker's speech from the noisy mixture by looking at lip movements, has made
significant progress combining time-domain speech separation models and visual
feature extractors (CNN). One problem of fusing audio and video information is
that they have different time resolutions. Most current research upsamples the
visual features along the time dimension so that audio and video features are
able to align in time. However, we believe that lip movement should mostly
contain long-term, or phone-level information. Based on this assumption, we
propose a new way to fuse audio-visual features. We observe that for DPRNN
\cite{dprnn}, the interchunk dimension's time resolution could be very close to
the time resolution of video frames. Like \cite{sepformer}, the LSTM in DPRNN
is replaced by intra-chunk and inter-chunk self-attention, but in the proposed
algorithm, inter-chunk attention incorporates the visual features as an
additional feature stream. This prevents the upsampling of visual cues,
resulting in more efficient audio-visual fusion. The result shows we achieve
superior results compared with other time-domain based audio-visual fusion
models.
</p></li>
</ul>

<h3>Title: Rank-Enhanced Low-Dimensional Convolution Set for Hyperspectral Image Denoising. (arXiv:2207.04266v1 [eess.IV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04266">http://arxiv.org/abs/2207.04266</a></li>
<li>Code URL: null</li>
<li>Summary: <p>This paper tackles the challenging problem of hyperspectral (HS) image
denoising. Unlike existing deep learning-based methods usually adopting
complicated network architectures or empirically stacking off-the-shelf modules
to pursue performance improvement, we focus on the efficient and effective
feature extraction manner for capturing the high-dimensional characteristics of
HS images. To be specific, based on the theoretical analysis that increasing
the rank of the matrix formed by the unfolded convolutional kernels can promote
feature diversity, we propose rank-enhanced low-dimensional convolution set
(Re-ConvSet), which separately performs 1-D convolution along the three
dimensions of an HS image side-by-side, and then aggregates the resulting
spatial-spectral embeddings via a learnable compression layer. Re-ConvSet not
only learns the diverse spatial-spectral features of HS images, but also
reduces the parameters and complexity of the network. We then incorporate
Re-ConvSet into the widely-used U-Net architecture to construct an HS image
denoising method. Surprisingly, we observe such a concise framework outperforms
the most recent method to a large extent in terms of quantitative metrics,
visual results, and efficiency. We believe our work may shed light on deep
learning-based HS image processing and analysis.
</p></li>
</ul>

<h3>Title: Classification of COVID-19 on chest X-Ray images using Deep Learning model with Histogram Equalization and Lungs Segmentation. (arXiv:2112.02478v3 [eess.IV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2112.02478">http://arxiv.org/abs/2112.02478</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Background and Objective: Artificial intelligence (AI) methods coupled with
biomedical analysis has a critical role during pandemics as it helps to release
the overwhelming pressure from healthcare systems and physicians. As the
ongoing COVID-19 crisis worsens in countries having dense populations and
inadequate testing kits like Brazil and India, radiological imaging can act as
an important diagnostic tool to accurately classify covid-19 patients and
prescribe the necessary treatment in due time. With this motivation, we present
our study based on deep learning architecture for detecting covid-19 infected
lungs using chest X-rays. Dataset: We collected a total of 2470 images for
three different class labels, namely, healthy lungs, ordinary pneumonia, and
covid-19 infected pneumonia, out of which 470 X-ray images belong to the
covid-19 category. Methods: We first pre-process all the images using histogram
equalization techniques and segment them using U-net architecture. VGG-16
network is then used for feature extraction from the pre-processed images which
is further sampled by SMOTE oversampling technique to achieve a balanced
dataset. Finally, the class-balanced features are classified using a support
vector machine (SVM) classifier with 10-fold cross-validation and the accuracy
is evaluated. Result and Conclusion: Our novel approach combining well-known
pre-processing techniques, feature extraction methods, and dataset balancing
method, lead us to an outstanding rate of recognition of 98% for COVID-19
images over a dataset of 2470 X-ray images. Our model is therefore fit to be
utilized in healthcare facilities for screening purposes.
</p></li>
</ul>

<h3>Title: Looking Beyond Corners: Contrastive Learning of Visual Representations for Keypoint Detection and Description Extraction. (arXiv:2112.12002v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2112.12002">http://arxiv.org/abs/2112.12002</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Learnable keypoint detectors and descriptors are beginning to outperform
classical hand-crafted feature extraction methods. Recent studies on
self-supervised learning of visual representations have driven the increasing
performance of learnable models based on deep networks. By leveraging
traditional data augmentations and homography transformations, these networks
learn to detect corners under adverse conditions such as extreme illumination
changes. However, their generalization capabilities are limited to corner-like
features detected a priori by classical methods or synthetically generated
data.
</p></li>
</ul>

<p>In this paper, we propose the Correspondence Network (CorrNet) that learns to
detect repeatable keypoints and to extract discriminative descriptions via
unsupervised contrastive learning under spatial constraints. Our experiments
show that CorrNet is not only able to detect low-level features such as
corners, but also high-level features that represent similar objects present in
a pair of input images through our proposed joint guided backpropagation of
their latent space. Our approach obtains competitive results under viewpoint
changes and achieves state-of-the-art performance under illumination changes.
</p>

<h3>Title: Learning Rich Representation of Keyphrases from Text. (arXiv:2112.08547v2 [cs.CL] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2112.08547">http://arxiv.org/abs/2112.08547</a></li>
<li>Code URL: <a href="https://github.com/bloomberg/kbir_keybart">https://github.com/bloomberg/kbir_keybart</a></li>
<li>Summary: <p>In this work, we explore how to train task-specific language models aimed
towards learning rich representation of keyphrases from text documents. We
experiment with different masking strategies for pre-training transformer
language models (LMs) in discriminative as well as generative settings. In the
discriminative setting, we introduce a new pre-training objective - Keyphrase
Boundary Infilling with Replacement (KBIR), showing large gains in performance
(upto 8.16 points in F1) over SOTA, when the LM pre-trained using KBIR is
fine-tuned for the task of keyphrase extraction. In the generative setting, we
introduce a new pre-training setup for BART - KeyBART, that reproduces the
keyphrases related to the input text in the CatSeq format, instead of the
denoised original input. This also led to gains in performance (upto 4.33
points in F1@M) over SOTA for keyphrase generation. Additionally, we also
fine-tune the pre-trained language models on named entity recognition (NER),
question answering (QA), relation extraction (RE), abstractive summarization
and achieve comparable performance with that of the SOTA, showing that learning
rich representation of keyphrases is indeed beneficial for many other
fundamental NLP tasks.
</p></li>
</ul>

<h3>Title: DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles. (arXiv:2207.01079v2 [cs.CL] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.01079">http://arxiv.org/abs/2207.01079</a></li>
<li>Code URL: null</li>
<li>Summary: <p>A crucial component in the curation of KB for a scientific domain is
information extraction from tables in the domain's published articles -- tables
carry important information (often numeric), which must be adequately extracted
for a comprehensive machine understanding of an article. Existing table
extractors assume prior knowledge of table structure and format, which may not
be known in scientific tables. We study a specific and challenging table
extraction problem: extracting compositions of materials (e.g., glasses,
alloys). We first observe that materials science researchers organize similar
compositions in a wide variety of table styles, necessitating an intelligent
model for table understanding and composition extraction. Consequently, we
define this novel task as a challenge for the ML community and create a
training dataset comprising 4,408 distantly supervised tables, along with 1,475
manually annotated dev and test tables. We also present DiSCoMaT, a strong
baseline geared towards this specific task, which combines multiple graph
neural networks with several task-specific regular expressions, features, and
constraints. We show that DiSCoMaT outperforms recent table processing
architectures by significant margins.
</p></li>
</ul>

<h3>Title: Multi-Frequency Information Enhanced Channel Attention Module for Speaker Representation Learning. (arXiv:2207.04540v1 [eess.AS])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04540">http://arxiv.org/abs/2207.04540</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Recently, attention mechanisms have been applied successfully in neural
network-based speaker verification systems. Incorporating the
Squeeze-and-Excitation block into convolutional neural networks has achieved
remarkable performance. However, it uses global average pooling (GAP) to simply
average the features along time and frequency dimensions, which is incapable of
preserving sufficient speaker information in the feature maps. In this study,
we show that GAP is a special case of a discrete cosine transform (DCT) on
time-frequency domain mathematically using only the lowest frequency component
in frequency decomposition. To strengthen the speaker information extraction
ability, we propose to utilize multi-frequency information and design two novel
and effective attention modules, called Single-Frequency Single-Channel (SFSC)
attention module and Multi-Frequency Single-Channel (MFSC) attention module.
The proposed attention modules can effectively capture more speaker information
from multiple frequency components on the basis of DCT. We conduct
comprehensive experiments on the VoxCeleb datasets and a probe evaluation on
the 1st 48-UTD forensic corpus. Experimental results demonstrate that our
proposed SFSC and MFSC attention modules can efficiently generate more
discriminative speaker representations and outperform ResNet34-SE and
ECAPA-TDNN systems with relative 20.9% and 20.2% reduction in EER, without
adding extra network parameters.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: StatMix: Data augmentation method that relies on image statistics in federated learning. (arXiv:2207.04103v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04103">http://arxiv.org/abs/2207.04103</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Availability of large amount of annotated data is one of the pillars of deep
learning success. Although numerous big datasets have been made available for
research, this is often not the case in real life applications (e.g. companies
are not able to share data due to GDPR or concerns related to intellectual
property rights protection). Federated learning (FL) is a potential solution to
this problem, as it enables training a global model on data scattered across
multiple nodes, without sharing local data itself. However, even FL methods
pose a threat to data privacy, if not handled properly. Therefore, we propose
StatMix, an augmentation approach that uses image statistics, to improve
results of FL scenario(s). StatMix is empirically tested on CIFAR-10 and
CIFAR-100, using two neural network architectures. In all FL experiments,
application of StatMix improves the average accuracy, compared to the baseline
training (with no use of StatMix). Some improvement can also be observed in
non-FL setups.
</p></li>
</ul>

<h3>Title: Smart Multi-tenant Federated Learning. (arXiv:2207.04202v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04202">http://arxiv.org/abs/2207.04202</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Federated learning (FL) is an emerging distributed machine learning method
that empowers in-situ model training on decentralized edge devices. However,
multiple simultaneous training activities could overload resource-constrained
devices. In this work, we propose a smart multi-tenant FL system, MuFL, to
effectively coordinate and execute simultaneous training activities. We first
formalize the problem of multi-tenant FL, define multi-tenant FL scenarios, and
introduce a vanilla multi-tenant FL system that trains activities sequentially
to form baselines. Then, we propose two approaches to optimize multi-tenant FL:
1) activity consolidation merges training activities into one activity with a
multi-task architecture; 2) after training it for rounds, activity splitting
divides it into groups by employing affinities among activities such that
activities within a group have better synergy. Extensive experiments
demonstrate that MuFL outperforms other methods while consuming 40% less
energy. We hope this work will inspire the community to further study and
optimize multi-tenant FL.
</p></li>
</ul>

<h3>Title: On Bridging Generic and Personalized Federated Learning for Image Classification. (arXiv:2107.00778v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2107.00778">http://arxiv.org/abs/2107.00778</a></li>
<li>Code URL: <a href="https://github.com/hongyouc/fed-rod">https://github.com/hongyouc/fed-rod</a></li>
<li>Summary: <p>Federated learning is promising for its capability to collaboratively train
models with multiple clients without accessing their data, but vulnerable when
clients' data distributions diverge from each other. This divergence further
leads to a dilemma: "Should we prioritize the learned model's generic
performance (for future use at the server) or its personalized performance (for
each client)?" These two, seemingly competing goals have divided the community
to focus on one or the other, yet in this paper we show that it is possible to
approach both at the same time. Concretely, we propose a novel federated
learning framework that explicitly decouples a model's dual duties with two
prediction tasks. On the one hand, we introduce a family of losses that are
robust to non-identical class distributions, enabling clients to train a
generic predictor with a consistent objective across them. On the other hand,
we formulate the personalized predictor as a lightweight adaptive module that
is learned to minimize each client's empirical risk on top of the generic
predictor. With this two-loss, two-predictor framework which we name Federated
Robust Decoupling (Fed-RoD), the learned model can simultaneously achieve
state-of-the-art generic and personalized performance, essentially bridging the
two tasks.
</p></li>
</ul>

<h3>Title: Multi-Model Federated Learning with Provable Guarantees. (arXiv:2207.04330v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04330">http://arxiv.org/abs/2207.04330</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Federated Learning (FL) is a variant of distributed learning where edge
devices collaborate to learn a model without sharing their data with the
central server or each other. We refer to the process of training multiple
independent models simultaneously in a federated setting using a common pool of
clients as multi-model FL. In this work, we propose two variants of the popular
FedAvg algorithm for multi-model FL, with provable convergence guarantees. We
further show that for the same amount of computation, multi-model FL can have
better performance than training each model separately. We supplement our
theoretical results with experiments in strongly convex, convex, and non-convex
settings.
</p></li>
</ul>

<h3>Title: Variance Reduced ProxSkip: Algorithm, Theory and Application to Federated Learning. (arXiv:2207.04338v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04338">http://arxiv.org/abs/2207.04338</a></li>
<li>Code URL: null</li>
<li>Summary: <p>We study distributed optimization methods based on the {\em local training
(LT)} paradigm: achieving communication efficiency by performing richer local
gradient-based training on the clients before parameter averaging. Looking back
at the progress of the field, we {\em identify 5 generations of LT methods}: 1)
heuristic, 2) homogeneous, 3) sublinear, 4) linear, and 5) accelerated. The
5${}^{\rm th}$ generation, initiated by the ProxSkip method of Mishchenko,
Malinovsky, Stich and Richt\'{a}rik (2022) and its analysis, is characterized
by the first theoretical confirmation that LT is a communication acceleration
mechanism. Inspired by this recent progress, we contribute to the 5${}^{\rm
th}$ generation of LT methods by showing that it is possible to enhance them
further using {\em variance reduction}. While all previous theoretical results
for LT methods ignore the cost of local work altogether, and are framed purely
in terms of the number of communication rounds, we show that our methods can be
substantially faster in terms of the {\em total training cost} than the
state-of-the-art method ProxSkip in theory and practice in the regime when
local computation is sufficiently expensive. We characterize this threshold
theoretically, and confirm our theoretical predictions with empirical results.
</p></li>
</ul>

<h3>Title: Bitwidth Heterogeneous Federated Learning with Progressive Weight Dequantization. (arXiv:2202.11453v4 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2202.11453">http://arxiv.org/abs/2202.11453</a></li>
<li>Code URL: null</li>
<li>Summary: <p>In practical federated learning scenarios, the participating devices may have
different bitwidths for computation and memory storage by design. However,
despite the progress made in device-heterogeneous federated learning scenarios,
the heterogeneity in the bitwidth specifications in the hardware has been
mostly overlooked. We introduce a pragmatic FL scenario with bitwidth
heterogeneity across the participating devices, dubbed as Bitwidth
Heterogeneous Federated Learning (BHFL). BHFL brings in a new challenge, that
the aggregation of model parameters with different bitwidths could result in
severe performance degeneration, especially for high-bitwidth models. To tackle
this problem, we propose ProWD framework, which has a trainable weight
dequantizer at the central server that progressively reconstructs the
low-bitwidth weights into higher bitwidth weights, and finally into
full-precision weights. ProWD further selectively aggregates the model
parameters to maximize the compatibility across bit-heterogeneous weights. We
validate ProWD against relevant FL baselines on the benchmark datasets, using
clients with varying bitwidths. Our ProWD largely outperforms the baseline FL
algorithms as well as naive approaches (e.g. grouped averaging) under the
proposed BHFL scenario.
</p></li>
</ul>

<h3>Title: Motley: Benchmarking Heterogeneity and Personalization in Federated Learning. (arXiv:2206.09262v3 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2206.09262">http://arxiv.org/abs/2206.09262</a></li>
<li>Code URL: <a href="https://github.com/google-research/federated">https://github.com/google-research/federated</a></li>
<li>Summary: <p>Personalized federated learning considers learning models unique to each
client in a heterogeneous network. The resulting client-specific models have
been purported to improve metrics such as accuracy, fairness, and robustness in
federated networks. However, despite a plethora of work in this area, it
remains unclear: (1) which personalization techniques are most effective in
various settings, and (2) how important personalization truly is for realistic
federated applications. To better answer these questions, we propose Motley, a
benchmark for personalized federated learning. Motley consists of a suite of
cross-device and cross-silo federated datasets from varied problem domains, as
well as thorough evaluation metrics for better understanding the possible
impacts of personalization. We establish baselines on the benchmark by
comparing a number of representative personalized federated learning methods.
These initial results highlight strengths and weaknesses of existing
approaches, and raise several open questions for the community. Motley aims to
provide a reproducible means with which to advance developments in personalized
and heterogeneity-aware federated learning, as well as the related areas of
transfer learning, meta-learning, and multi-task learning.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Probing Classifiers are Unreliable for Concept Removal and Detection. (arXiv:2207.04153v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04153">http://arxiv.org/abs/2207.04153</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Neural network models trained on text data have been found to encode
undesired linguistic or sensitive attributes in their representation. Removing
such attributes is non-trivial because of a complex relationship between the
attribute, text input, and the learnt representation. Recent work has proposed
post-hoc and adversarial methods to remove such unwanted attributes from a
model's representation. Through an extensive theoretical and empirical
analysis, we show that these methods can be counter-productive: they are unable
to remove the attributes entirely, and in the worst case may end up destroying
all task-relevant features. The reason is the methods' reliance on a probing
classifier as a proxy for the attribute. Even under the most favorable
conditions when an attribute's features in representation space can alone
provide 100% accuracy for learning the probing classifier, we prove that
post-hoc or adversarial methods will fail to remove the attribute correctly.
These theoretical implications are confirmed by empirical experiments on models
trained on synthetic, Multi-NLI, and Twitter datasets. For sensitive
applications of attribute removal such as fairness, we recommend caution
against using these methods and propose a spuriousness metric to gauge the
quality of the final classifier.
</p></li>
</ul>

<h3>Title: FairDistillation: Mitigating Stereotyping in Language Models. (arXiv:2207.04546v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04546">http://arxiv.org/abs/2207.04546</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Large pre-trained language models are successfully being used in a variety of
tasks, across many languages. With this ever-increasing usage, the risk of
harmful side effects also rises, for example by reproducing and reinforcing
stereotypes. However, detecting and mitigating these harms is difficult to do
in general and becomes computationally expensive when tackling multiple
languages or when considering different biases. To address this, we present
FairDistillation: a cross-lingual method based on knowledge distillation to
construct smaller language models while controlling for specific biases. We
found that our distillation method does not negatively affect the downstream
performance on most tasks and successfully mitigates stereotyping and
representational harms. We demonstrate that FairDistillation can create fairer
language models at a considerably lower cost than alternative approaches.
</p></li>
</ul>

<h3>Title: On the Need and Applicability of Causality for Fair Machine Learning. (arXiv:2207.04053v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04053">http://arxiv.org/abs/2207.04053</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Causal reasoning has an indispensable role in how humans make sense of the
world and come to decisions in everyday life. While $20th$ century science was
reserved from making causal claims as too strong and not achievable, the $21st$
century is marked by the return of causality encouraged by the mathematization
of causal notions and the introduction of the non-deterministic concept of
cause~\cite{illari2011look}. Besides its common use cases in epidemiology,
political, and social sciences, causality turns out to be crucial in evaluating
the fairness of automated decisions, both in a legal and everyday sense. We
provide arguments and examples of why causality is particularly important for
fairness evaluation. In particular, we point out the social impact of
non-causal predictions and the legal anti-discrimination process that relies on
causal claims. We conclude with a discussion about the challenges and
limitations of applying causality in practical scenarios as well as possible
solutions.
</p></li>
</ul>

<h3>Title: Towards Substantive Conceptions of Algorithmic Fairness: Normative Guidance from Equal Opportunity Doctrines. (arXiv:2207.02912v2 [cs.CY] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.02912">http://arxiv.org/abs/2207.02912</a></li>
<li>Code URL: null</li>
<li>Summary: <p>In this work we use Equal Oppportunity (EO) doctrines from political
philosophy to make explicit the normative judgements embedded in different
conceptions of algorithmic fairness. We contrast formal EO approaches that
narrowly focus on fair contests at discrete decision points, with substantive
EO doctrines that look at people's fair life chances more holistically over the
course of a lifetime. We use this taxonomy to provide a moral interpretation of
the impossibility results as the incompatibility between different conceptions
of a fair contest -- foward-looking versus backward-looking -- when people do
not have fair life chances. We use this result to motivate substantive
conceptions of algorithmic fairness and outline two plausible procedures based
on the luck-egalitarian doctrine of EO, and Rawls's principle of fair equality
of opportunity.
</p></li>
</ul>

<h3>Title: Ablation Study of How Run Time Assurance Impacts the Training and Performance of Reinforcement Learning Agents. (arXiv:2207.04117v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04117">http://arxiv.org/abs/2207.04117</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Reinforcement Learning (RL) has become an increasingly important research
area as the success of machine learning algorithms and methods grows. To combat
the safety concerns surrounding the freedom given to RL agents while training,
there has been an increase in work concerning Safe Reinforcement Learning
(SRL). However, these new and safe methods have been held to less scrutiny than
their unsafe counterparts. For instance, comparisons among safe methods often
lack fair evaluation across similar initial condition bounds and hyperparameter
settings, use poor evaluation metrics, and cherry-pick the best training runs
rather than averaging over multiple random seeds. In this work, we conduct an
ablation study using evaluation best practices to investigate the impact of run
time assurance (RTA), which monitors the system state and intervenes to assure
safety, on effective learning. By studying multiple RTA approaches in both
on-policy and off-policy RL algorithms, we seek to understand which RTA methods
are most effective, whether the agents become dependent on the RTA, and the
importance of reward shaping versus safe exploration in RL agent training. Our
conclusions shed light on the most promising directions of SRL, and our
evaluation methodology lays the groundwork for creating better comparisons in
future SRL work.
</p></li>
</ul>

<h3>Title: On Graph Neural Network Fairness in the Presence of Heterophilous Neighborhoods. (arXiv:2207.04376v1 [cs.SI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04376">http://arxiv.org/abs/2207.04376</a></li>
<li>Code URL: null</li>
<li>Summary: <p>We study the task of node classification for graph neural networks (GNNs) and
establish a connection between group fairness, as measured by statistical
parity and equal opportunity, and local assortativity, i.e., the tendency of
linked nodes to have similar attributes. Such assortativity is often induced by
homophily, the tendency for nodes of similar properties to connect. Homophily
can be common in social networks where systemic factors have forced individuals
into communities which share a sensitive attribute. Through synthetic graphs,
we study the interplay between locally occurring homophily and fair
predictions, finding that not all node neighborhoods are equal in this respect
-- neighborhoods dominated by one category of a sensitive attribute often
struggle to obtain fair treatment, especially in the case of diverging local
class and sensitive attribute homophily. After determining that a relationship
between local homophily and fairness exists, we investigate if the issue of
unfairness can be associated to the design of the applied GNN model. We show
that by adopting heterophilous GNN designs capable of handling disassortative
group labels, group fairness in locally heterophilous neighborhoods can be
improved by up to 25% over homophilous designs in real and synthetic datasets.
</p></li>
</ul>

<h3>Title: Sampling Random Group Fair Rankings. (arXiv:2203.00887v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2203.00887">http://arxiv.org/abs/2203.00887</a></li>
<li>Code URL: null</li>
<li>Summary: <p>In this paper, we take an axiomatic approach to define random group-fair
rankings that satisfy a natural set of consistency and fairness axioms. We show
that this leads to a unique distribution $\mathcal{D}$ over rankings obtained
by merging given ranked list of items from different sensitive demographic
groups while satisfying given lower and upper bounds on the representation of
each group in the top ranks. Randomized or stochastic rankings have been of
interest in recent literature for offering better fairness and robustness than
deterministic rankings. Our problem formulation works even when there is
implicit bias, incomplete relevance information, or when only ordinal ranking
is available instead of relevance scores or utility values.
</p></li>
</ul>

<p>We propose three algorithms to sample a random group fair ranking from the
distribution $\mathcal{D}$ mentioned above. Our first algorithm samples
rankings from a distribution $\epsilon$-close to $\mathcal{D}$ in total
variation distance, and has expected running time polynomial in all input
parameters and $1/\epsilon$, when there is a sufficient gap between upper and
lower bound representation constraints for all the groups. Our second algorithm
samples rankings from $\mathcal{D}$ exactly, in time exponential in the number
of groups. Our third algorithm samples random group fair rankings from
$\mathcal{D}$ exactly and is faster than the first algorithm when the gap
between upper and lower bounds on the representation for each group is small.
We experimentally validate the above guarantees of our algorithms for group
fairness in top ranks and representation in every rank on real-world data sets.
</p>

<h2>interpretability</h2>
<h3>Title: Learning Structured Representations of Visual Scenes. (arXiv:2207.04200v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04200">http://arxiv.org/abs/2207.04200</a></li>
<li>Code URL: null</li>
<li>Summary: <p>As the intermediate-level representations bridging the two levels, structured
representations of visual scenes, such as visual relationships between pairwise
objects, have been shown to not only benefit compositional models in learning
to reason along with the structures but provide higher interpretability for
model decisions. Nevertheless, these representations receive much less
attention than traditional recognition tasks, leaving numerous open challenges
unsolved. In the thesis, we study how machines can describe the content of the
individual image or video with visual relationships as the structured
representations. Specifically, we explore how structured representations of
visual scenes can be effectively constructed and learned in both the
static-image and video settings, with improvements resulting from external
knowledge incorporation, bias-reducing mechanism, and enhanced representation
models. At the end of this thesis, we also discuss some open challenges and
limitations to shed light on future directions of structured representation
learning for visual scenes.
</p></li>
</ul>

<h3>Title: Linear Adversarial Concept Erasure. (arXiv:2201.12091v3 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2201.12091">http://arxiv.org/abs/2201.12091</a></li>
<li>Code URL: <a href="https://github.com/shauli-ravfogel/rlace-icml">https://github.com/shauli-ravfogel/rlace-icml</a></li>
<li>Summary: <p>Modern neural models trained on textual data rely on pre-trained
representations that emerge without direct supervision. As these
representations are increasingly being used in real-world applications, the
inability to \emph{control} their content becomes an increasingly important
problem.
</p></li>
</ul>

<p>We formulate the problem of identifying and erasing a linear subspace that
corresponds to a given concept, in order to prevent linear predictors from
recovering the concept. We model this problem as a constrained, linear minimax
game, and show that existing solutions are generally not optimal for this task.
We derive a closed-form solution for certain objectives, and propose a convex
relaxation, R-LACE, that works well for others. When evaluated in the context
of binary gender removal, the method recovers a low-dimensional subspace whose
removal mitigates bias by intrinsic and extrinsic evaluation. We show that the
method -- despite being linear -- is highly expressive, effectively mitigating
bias in deep nonlinear classifiers while maintaining tractability and
interpretability.
</p>

<h3>Title: Semi-Structured Distributional Regression -- Extending Structured Additive Models by Arbitrary Deep Neural Networks and Data Modalities. (arXiv:2002.05777v5 [stat.ML] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2002.05777">http://arxiv.org/abs/2002.05777</a></li>
<li>Code URL: <a href="https://github.com/davidruegamer/semi-structured_distributional_regression">https://github.com/davidruegamer/semi-structured_distributional_regression</a></li>
<li>Summary: <p>Combining additive models and neural networks allows to broaden the scope of
statistical regression and extend deep learning-based approaches by
interpretable structured additive predictors at the same time. Existing
attempts uniting the two modeling approaches are, however, limited to very
specific combinations and, more importantly, involve an identifiability issue.
As a consequence, interpretability and stable estimation are typically lost. We
propose a general framework to combine structured regression models and deep
neural networks into a unifying network architecture. To overcome the inherent
identifiability issues between different model parts, we construct an
orthogonalization cell that projects the deep neural network into the
orthogonal complement of the statistical model predictor. This enables proper
estimation of structured model parts and thereby interpretability. We
demonstrate the framework's efficacy in numerical experiments and illustrate
its special merits in benchmarks and real-world applications.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
