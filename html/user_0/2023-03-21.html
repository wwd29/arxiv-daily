<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Evolution of Automated Weakness Detection in Ethereum Bytecode: a Comprehensive Study. (arXiv:2303.10517v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10517">http://arxiv.org/abs/2303.10517</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10517] Evolution of Automated Weakness Detection in Ethereum Bytecode: a Comprehensive Study](http://arxiv.org/abs/2303.10517) #security</code></li>
<li>Summary: <p>Blockchain programs manage valuable assets like crypto-currencies and tokens,
and implement protocols for decentralized finance (DeFi), logistics and
logging, where security is important. To find potential issues, numerous tools
support developers and analysts. Being a recent technology, blockchain
technology and programs still evolve fast, making it challenging for tools and
developers to keep up with the changes. In this work, we study the evolution of
tools and patterns detected. We focus on Ethereum, the crypto ecosystem with
most developers and most contracts, by far. We investigate the changes in the
tools' behavior in terms of detected weaknesses, quality and behavior, and
agreements between the tools. We are the first to fully cover the entire body
of deployed bytecode on the Ethereum mainchain. We achieve full coverage by
considering bytecodes as equivalent if they share the same skeleton. The
skeleton of a bytecode is obtained by omitting functionally irrelevant parts.
This reduces the 48 million contracts deployed on Ethereum to 248,328 contracts
with distinct skeletons. For bulk execution, we utilize the open-source
framework SmartBugs that facilitates the analysis of Solidity smart contracts,
and enhance it to also accept bytecode as the only input. Moreover, we
integrate six further tools that accept bytecode. The execution of the 13
included tools took 31 years in total. While the tools are reporting a total of
1,307,486 potential weaknesses, over time we observe a decreasing number of
reported vulnerabilities and tools degrading to varying degrees.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Report of the Medical Image De-Identification (MIDI) Task Group -- Best Practices and Recommendations. (arXiv:2303.10473v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10473">http://arxiv.org/abs/2303.10473</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10473] Report of the Medical Image De-Identification (MIDI) Task Group -- Best Practices and Recommendations](http://arxiv.org/abs/2303.10473) #privacy</code></li>
<li>Summary: <p>This report addresses the technical aspects of de-identification of medical
images of human subjects and biospecimens, such that re-identification risk of
ethical, moral, and legal concern is sufficiently reduced to allow unrestricted
public sharing for any purpose, regardless of the jurisdiction of the source
and distribution sites. All medical images, regardless of the mode of
acquisition, are considered, though the primary emphasis is on those with
accompanying data elements, especially those encoded in formats in which the
data elements are embedded, particularly Digital Imaging and Communications in
Medicine (DICOM). These images include image-like objects such as
Segmentations, Parametric Maps, and Radiotherapy (RT) Dose objects. The scope
also includes related non-image objects, such as RT Structure Sets, Plans and
Dose Volume Histograms, Structured Reports, and Presentation States. Only
de-identification of publicly released data is considered, and alternative
approaches to privacy preservation, such as federated learning for artificial
intelligence (AI) model development, are out of scope, as are issues of privacy
leakage from AI model sharing. Only technical issues of public sharing are
addressed.
</p></li>
</ul>

<h3>Title: The Challenge of Differentially Private Screening Rules. (arXiv:2303.10303v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10303">http://arxiv.org/abs/2303.10303</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10303] The Challenge of Differentially Private Screening Rules](http://arxiv.org/abs/2303.10303) #privacy</code></li>
<li>Summary: <p>Linear $L_1$-regularized models have remained one of the simplest and most
effective tools in data analysis, especially in information retrieval problems
where n-grams over text with TF-IDF or Okapi feature values are a strong and
easy baseline. Over the past decade, screening rules have risen in popularity
as a way to reduce the runtime for producing the sparse regression weights of
$L_1$ models. However, despite the increasing need of privacy-preserving models
in information retrieval, to the best of our knoweledge, no differentially
private screening rule exists. In this paper, we develop the first
differentially private screening rule for linear and logistic regression. In
doing so, we discover difficulties in the task of making a useful private
screening rule due to the amount of noise added to ensure privacy. We provide
theoretical arguments and experimental evidence that this difficulty arises
from the screening step itself and not the private optimizer. Based on our
results, we highlight that developing an effective private $L_1$ screening
method is an open problem in the differential privacy literature.
</p></li>
</ul>

<h3>Title: How to Model Privacy Threats in the Automotive Domain. (arXiv:2303.10370v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10370">http://arxiv.org/abs/2303.10370</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10370] How to Model Privacy Threats in the Automotive Domain](http://arxiv.org/abs/2303.10370) #privacy</code></li>
<li>Summary: <p>This paper questions how to approach threat modelling in the automotive
domain at both an abstract level that features no domain-specific entities such
as the CAN bus and, separately, at a detailed level. It addresses such
questions by contributing a systematic method that is currently affected by the
analyst's subjectivity because most of its inner operations are only defined
informally. However, this potential limitation is overcome when candidate
threats are identified and left to everyone's scrutiny. The systematic method
is demonstrated on the established LINDDUN threat modelling methodology with
respect to 4 pivotal works on privacy threat modelling in automotive. As a
result, 8 threats that the authors deem not representable in LINDDUN are
identified and suggested as possible candidate extensions to LINDDUN. Also, 56
threats are identified providing a detailed, automotive-specific model of
threats.
</p></li>
</ul>

<h3>Title: DC-CCL: Device-Cloud Collaborative Controlled Learning for Large Vision Models. (arXiv:2303.10361v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10361">http://arxiv.org/abs/2303.10361</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10361] DC-CCL: Device-Cloud Collaborative Controlled Learning for Large Vision Models](http://arxiv.org/abs/2303.10361) #privacy</code></li>
<li>Summary: <p>Many large vision models have been deployed on the cloud for real-time
services. Meanwhile, fresh samples are continuously generated on the served
mobile device. How to leverage the device-side samples to improve the
cloud-side large model becomes a practical requirement, but falls into the
dilemma of no raw sample up-link and no large model down-link. Specifically,
the user may opt out of sharing raw samples with the cloud due to the concern
of privacy or communication overhead, while the size of some large vision
models far exceeds the mobile device's runtime capacity. In this work, we
propose a device-cloud collaborative controlled learning framework, called
DC-CCL, enabling a cloud-side large vision model that cannot be directly
deployed on the mobile device to still benefit from the device-side local
samples. In particular, DC-CCL vertically splits the base model into two
submodels, one large submodel for learning from the cloud-side samples and the
other small submodel for learning from the device-side samples and performing
device-cloud knowledge fusion. Nevertheless, on-device training of the small
submodel requires the output of the cloud-side large submodel to compute the
desired gradients. DC-CCL thus introduces a light-weight model to mimic the
large cloud-side submodel with knowledge distillation, which can be offloaded
to the mobile device to control its small submodel's optimization direction.
Given the decoupling nature of two submodels in collaborative learning, DC-CCL
also allows the cloud to take a pre-trained model and the mobile device to take
another model with a different backbone architecture.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: DeAR: Debiasing Vision-Language Models with Additive Residuals. (arXiv:2303.10431v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10431">http://arxiv.org/abs/2303.10431</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10431] DeAR: Debiasing Vision-Language Models with Additive Residuals](http://arxiv.org/abs/2303.10431) #protect</code></li>
<li>Summary: <p>Large pre-trained vision-language models (VLMs) reduce the time for
developing predictive models for various vision-grounded language downstream
tasks by providing rich, adaptable image and text representations. However,
these models suffer from societal biases owing to the skewed distribution of
various identity groups in the training data. These biases manifest as the
skewed similarity between the representations for specific text concepts and
images of people of different identity groups and, therefore, limit the
usefulness of such models in real-world high-stakes applications. In this work,
we present DeAR (Debiasing with Additive Residuals), a novel debiasing method
that learns additive residual image representations to offset the original
representations, ensuring fair output representations. In doing so, it reduces
the ability of the representations to distinguish between the different
identity groups. Further, we observe that the current fairness tests are
performed on limited face image datasets that fail to indicate why a specific
text concept should/should not apply to them. To bridge this gap and better
evaluate DeAR, we introduce the Protected Attribute Tag Association (PATA)
dataset - a new context-based bias benchmarking dataset for evaluating the
fairness of large pre-trained VLMs. Additionally, PATA provides visual context
for a diverse human population in different scenarios with both positive and
negative connotations. Experimental results for fairness and zero-shot
performance preservation using multiple datasets demonstrate the efficacy of
our framework.
</p></li>
</ul>

<h3>Title: FedRight: An Effective Model Copyright Protection for Federated Learning. (arXiv:2303.10399v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10399">http://arxiv.org/abs/2303.10399</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10399] FedRight: An Effective Model Copyright Protection for Federated Learning](http://arxiv.org/abs/2303.10399) #protect</code></li>
<li>Summary: <p>Federated learning (FL), an effective distributed machine learning framework,
implements model training and meanwhile protects local data privacy. It has
been applied to a broad variety of practice areas due to its great performance
and appreciable profits. Who owns the model, and how to protect the copyright
has become a real problem. Intuitively, the existing property rights protection
methods in centralized scenarios (e.g., watermark embedding and model
fingerprints) are possible solutions for FL. But they are still challenged by
the distributed nature of FL in aspects of the no data sharing, parameter
aggregation, and federated training settings. For the first time, we formalize
the problem of copyright protection for FL, and propose FedRight to protect
model copyright based on model fingerprints, i.e., extracting model features by
generating adversarial examples as model fingerprints. FedRight outperforms
previous works in four key aspects: (i) Validity: it extracts model features to
generate transferable fingerprints to train a detector to verify the copyright
of the model. (ii) Fidelity: it is with imperceptible impact on the federated
training, thus promising good main task performance. (iii) Robustness: it is
empirically robust against malicious attacks on copyright protection, i.e.,
fine-tuning, model pruning, and adaptive attacks. (iv) Black-box: it is valid
in the black-box forensic scenario where only application programming interface
calls to the model are available. Extensive evaluations across 3 datasets and 9
model structures demonstrate FedRight's superior fidelity, validity, and
robustness.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Detection of Uncertainty in Exceedance of Threshold (DUET): An Adversarial Patch Localizer. (arXiv:2303.10291v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10291">http://arxiv.org/abs/2303.10291</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10291] Detection of Uncertainty in Exceedance of Threshold (DUET): An Adversarial Patch Localizer](http://arxiv.org/abs/2303.10291) #defense</code></li>
<li>Summary: <p>Development of defenses against physical world attacks such as adversarial
patches is gaining traction within the research community. We contribute to the
field of adversarial patch detection by introducing an uncertainty-based
adversarial patch localizer which localizes adversarial patch on an image,
permitting post-processing patch-avoidance or patch-reconstruction. We quantify
our prediction uncertainties with the development of \textit{\textbf{D}etection
of \textbf{U}ncertainties in the \textbf{E}xceedance of \textbf{T}hreshold}
(DUET) algorithm. This algorithm provides a framework to ascertain confidence
in the adversarial patch localization, which is essential for safety-sensitive
applications such as self-driving cars and medical imaging. We conducted
experiments on localizing adversarial patches and found our proposed DUET model
outperforms baseline models. We then conduct further analyses on our choice of
model priors and the adoption of Bayesian Neural Networks in different layers
within our model architecture. We found that isometric gaussian priors in
Bayesian Neural Networks are suitable for patch localization tasks and the
presence of Bayesian layers in the earlier neural network blocks facilitates
top-end localization performance, while Bayesian layers added in the later
neural network blocks contribute to better model generalization. We then
propose two different well-performing models to tackle different use cases.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Revisiting LiDAR Spoofing Attack Capabilities against Object Detection: Improvements, Measurement, and New Attack. (arXiv:2303.10555v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10555">http://arxiv.org/abs/2303.10555</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10555] Revisiting LiDAR Spoofing Attack Capabilities against Object Detection: Improvements, Measurement, and New Attack](http://arxiv.org/abs/2303.10555) #attack</code></li>
<li>Summary: <p>LiDAR (Light Detection And Ranging) is an indispensable sensor for precise
long- and wide-range 3D sensing, which directly benefited the recent rapid
deployment of autonomous driving (AD). Meanwhile, such a safety-critical
application strongly motivates its security research. A recent line of research
demonstrates that one can manipulate the LiDAR point cloud and fool object
detection by firing malicious lasers against LiDAR. However, these efforts face
3 critical research gaps: (1) evaluating only on a specific LiDAR (VLP-16); (2)
assuming unvalidated attack capabilities; and (3) evaluating with models
trained on limited datasets.
</p></li>
</ul>

<p>To fill these critical research gaps, we conduct the first large-scale
measurement study on LiDAR spoofing attack capabilities on object detectors
with 9 popular LiDARs in total and 3 major types of object detectors. To
perform this measurement, we significantly improved the LiDAR spoofing
capability with more careful optics and functional electronics, which allows us
to be the first to clearly demonstrate and quantify key attack capabilities
assumed in prior works. However, we further find that such key assumptions
actually can no longer hold for all the other (8 out of 9) LiDARs that are more
recent than VLP-16 due to various recent LiDAR features. To this end, we
further identify a new type of LiDAR spoofing attack that can improve on this
and be applicable to a much more general and recent set of LiDARs. We find that
its attack capability is enough to (1) cause end-to-end safety hazards in
simulated AD scenarios, and (2) remove real vehicles in the physical world. We
also discuss the defense side.
</p>

<h3>Title: AdaptGuard: Defending Against Universal Attacks for Model Adaptation. (arXiv:2303.10594v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10594">http://arxiv.org/abs/2303.10594</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10594] AdaptGuard: Defending Against Universal Attacks for Model Adaptation](http://arxiv.org/abs/2303.10594) #attack</code></li>
<li>Summary: <p>Model adaptation aims at solving the domain transfer problem under the
constraint of only accessing the pretrained source models. With the increasing
considerations of data privacy and transmission efficiency, this paradigm has
been gaining recent popularity. This paper studies the vulnerability to
universal attacks transferred from the source domain during model adaptation
algorithms due to the existence of the malicious providers. We explore both
universal adversarial perturbations and backdoor attacks as loopholes on the
source side and discover that they still survive in the target models after
adaptation. To address this issue, we propose a model preprocessing framework,
named AdaptGuard, to improve the security of model adaptation algorithms.
AdaptGuard avoids direct use of the risky source parameters through knowledge
distillation and utilizes the pseudo adversarial samples under adjusted radius
to enhance the robustness. AdaptGuard is a plug-and-play module that requires
neither robust pretrained models nor any changes for the following model
adaptation algorithms. Extensive results on three commonly used datasets and
two popular adaptation methods validate that AdaptGuard can effectively defend
against universal attacks and maintain clean accuracy in the target domain
simultaneously. We hope this research will shed light on the safety and
robustness of transfer learning.
</p></li>
</ul>

<h3>Title: NoisyHate: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online. (arXiv:2303.10430v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10430">http://arxiv.org/abs/2303.10430</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10430] NoisyHate: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online](http://arxiv.org/abs/2303.10430) #attack</code></li>
<li>Summary: <p>Online texts with toxic content are a threat in social media that might cause
cyber harassment. Although many platforms applied measures, such as machine
learning-based hate-speech detection systems, to diminish their effect, those
toxic content publishers can still evade the system by modifying the spelling
of toxic words. Those modified words are also known as human-written text
perturbations. Many research works developed certain techniques to generate
adversarial samples to help the machine learning models obtain the ability to
recognize those perturbations. However, there is still a gap between those
machine-generated perturbations and human-written perturbations. In this paper,
we introduce a benchmark test set containing human-written perturbations online
for toxic speech detection models. We also recruited a group of workers to
evaluate the quality of this test set and dropped low-quality samples.
Meanwhile, to check if our perturbation can be normalized to its clean version,
we applied spell corrector algorithms on this dataset. Finally, we test this
data on state-of-the-art language models, such as BERT and RoBERTa, and black
box APIs, such as perspective API, to demonstrate the adversarial attack with
real human-written perturbations is still effective.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: ASymReg: Robust symmetric image registration using anti-symmetric formulation and deformation inversion layers. (arXiv:2303.10211v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10211">http://arxiv.org/abs/2303.10211</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10211] ASymReg: Robust symmetric image registration using anti-symmetric formulation and deformation inversion layers](http://arxiv.org/abs/2303.10211) #robust</code></li>
<li>Summary: <p>Deep learning based deformable medical image registration methods have
emerged as a strong alternative for classical iterative registration methods.
However, the currently published deep learning methods do not fulfill as strict
symmetry properties with respect to the inputs as some classical registration
methods, for which the registration outcome is the same regardless of the order
of the inputs. While some deep learning methods label themselves as symmetric,
they are either symmetric only a priori, which does not guarantee symmetry for
any given input pair, or they do not generate accurate explicit inverses. In
this work, we propose a novel registration architecture which by construction
makes the registration network anti-symmetric with respect to its inputs. We
demonstrate on two datasets that the proposed method achieves state-of-the-art
results in terms of registration accuracy and that the generated deformations
have accurate explicit inverses.
</p></li>
</ul>

<h3>Title: Video shutter angle estimation using optical flow and linear blur. (arXiv:2303.10247v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10247">http://arxiv.org/abs/2303.10247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10247] Video shutter angle estimation using optical flow and linear blur](http://arxiv.org/abs/2303.10247) #robust</code></li>
<li>Summary: <p>We present a method for estimating the shutter angle, a.k.a. exposure
fraction -- the ratio of the exposure time and the reciprocal of frame rate --
of videoclips containing motion. The approach exploits the relation of the
exposure fraction, optical flow, and linear motion blur. Robustness is achieved
by selecting image patches where both the optical flow and blur estimates are
reliable, checking their consistency. The method was evaluated on the publicly
available Beam-Splitter Dataset with a range of exposure fractions from 0.015
to 0.36. The best achieved mean absolute error of estimates was 0.039. We
successfully test the suitability of the method for a forensic application of
detection of video tampering by frame removal or insertion.
</p></li>
</ul>

<h3>Title: Unleashing the Potential of Spiking Neural Networks by Dynamic Confidence. (arXiv:2303.10276v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10276">http://arxiv.org/abs/2303.10276</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10276] Unleashing the Potential of Spiking Neural Networks by Dynamic Confidence](http://arxiv.org/abs/2303.10276) #robust</code></li>
<li>Summary: <p>This paper presents a new methodology to alleviate the fundamental trade-off
between accuracy and latency in spiking neural networks (SNNs). The approach
involves decoding confidence information over time from the SNN outputs and
using it to develop a decision-making agent that can dynamically determine when
to terminate each inference.
</p></li>
</ul>

<p>The proposed method, Dynamic Confidence, provides several significant
benefits to SNNs. 1. It can effectively optimize latency dynamically at
runtime, setting it apart from many existing low-latency SNN algorithms. Our
experiments on CIFAR-10 and ImageNet datasets have demonstrated an average 40%
speedup across eight different settings after applying Dynamic Confidence. 2.
The decision-making agent in Dynamic Confidence is straightforward to construct
and highly robust in parameter space, making it extremely easy to implement. 3.
The proposed method enables visualizing the potential of any given SNN, which
sets a target for current SNNs to approach. For instance, if an SNN can
terminate at the most appropriate time point for each input sample, a ResNet-50
SNN can achieve an accuracy as high as 82.47% on ImageNet within just 4.71 time
steps on average. Unlocking the potential of SNNs needs a highly-reliable
decision-making agent to be constructed and fed with a high-quality estimation
of ground truth. In this regard, Dynamic Confidence represents a meaningful
step toward realizing the potential of SNNs.
</p>

<h3>Title: Synthetic-to-Real Domain Adaptation for Action Recognition: A Dataset and Baseline Performances. (arXiv:2303.10280v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10280">http://arxiv.org/abs/2303.10280</a></li>
<li>Code URL: <a href="https://github.com/reddyav1/rocog-v2">https://github.com/reddyav1/rocog-v2</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10280] Synthetic-to-Real Domain Adaptation for Action Recognition: A Dataset and Baseline Performances](http://arxiv.org/abs/2303.10280) #robust</code></li>
<li>Summary: <p>Human action recognition is a challenging problem, particularly when there is
high variability in factors such as subject appearance, backgrounds and
viewpoint. While deep neural networks (DNNs) have been shown to perform well on
action recognition tasks, they typically require large amounts of high-quality
labeled data to achieve robust performance across a variety of conditions.
Synthetic data has shown promise as a way to avoid the substantial costs and
potential ethical concerns associated with collecting and labeling enormous
amounts of data in the real-world. However, synthetic data may differ from real
data in important ways. This phenomenon, known as \textit{domain shift}, can
limit the utility of synthetic data in robotics applications. To mitigate the
effects of domain shift, substantial effort is being dedicated to the
development of domain adaptation (DA) techniques. Yet, much remains to be
understood about how best to develop these techniques. In this paper, we
introduce a new dataset called Robot Control Gestures (RoCoG-v2). The dataset
is composed of both real and synthetic videos from seven gesture classes, and
is intended to support the study of synthetic-to-real domain shift for
video-based action recognition. Our work expands upon existing datasets by
focusing the action classes on gestures for human-robot teaming, as well as by
enabling investigation of domain shift in both ground and aerial views. We
present baseline results using state-of-the-art action recognition and domain
adaptation algorithms and offer initial insight on tackling the
synthetic-to-real and ground-to-air domain shifts.
</p></li>
</ul>

<h3>Title: Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks. (arXiv:2303.10310v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10310">http://arxiv.org/abs/2303.10310</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10310] Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks](http://arxiv.org/abs/2303.10310) #robust</code></li>
<li>Summary: <p>The ability to classify images accurately and efficiently is dependent on
having access to large labeled datasets and testing on data from the same
domain that the model is trained on. Classification becomes more challenging
when dealing with new data from a different domain, where collecting a large
labeled dataset and training a new classifier from scratch is time-consuming,
expensive, and sometimes infeasible or impossible. Cross-domain classification
frameworks were developed to handle this data domain shift problem by utilizing
unsupervised image-to-image (UI2I) translation models to translate an input
image from the unlabeled domain to the labeled domain. The problem with these
unsupervised models lies in their unsupervised nature. For lack of annotations,
it is not possible to use the traditional supervised metrics to evaluate these
translation models to pick the best-saved checkpoint model. In this paper, we
introduce a new method called Pseudo Supervised Metrics that was designed
specifically to support cross-domain classification applications contrary to
other typically used metrics such as the FID which was designed to evaluate the
model in terms of the quality of the generated image from a human-eye
perspective. We show that our metric not only outperforms unsupervised metrics
such as the FID, but is also highly correlated with the true supervised
metrics, robust, and explainable. Furthermore, we demonstrate that it can be
used as a standard metric for future research in this field by applying it to a
critical real-world problem (the boiling crisis problem).
</p></li>
</ul>

<h3>Title: LossMix: Simplify and Generalize Mixup for Object Detection and Beyond. (arXiv:2303.10343v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10343">http://arxiv.org/abs/2303.10343</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10343] LossMix: Simplify and Generalize Mixup for Object Detection and Beyond](http://arxiv.org/abs/2303.10343) #robust</code></li>
<li>Summary: <p>The success of data mixing augmentations in image classification tasks has
been well-received. However, these techniques cannot be readily applied to
object detection due to challenges such as spatial misalignment,
foreground/background distinction, and plurality of instances. To tackle these
issues, we first introduce a novel conceptual framework called Supervision
Interpolation, which offers a fresh perspective on interpolation-based
augmentations by relaxing and generalizing Mixup. Building on this framework,
we propose LossMix, a simple yet versatile and effective regularization that
enhances the performance and robustness of object detectors and more. Our key
insight is that we can effectively regularize the training on mixed data by
interpolating their loss errors instead of ground truth labels. Empirical
results on the PASCAL VOC and MS COCO datasets demonstrate that LossMix
consistently outperforms currently popular mixing strategies. Furthermore, we
design a two-stage domain mixing method that leverages LossMix to surpass
Adaptive Teacher (CVPR 2022) and set a new state of the art for unsupervised
domain adaptation.
</p></li>
</ul>

<h3>Title: Local-to-Global Panorama Inpainting for Locale-Aware Indoor Lighting Prediction. (arXiv:2303.10344v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10344">http://arxiv.org/abs/2303.10344</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10344] Local-to-Global Panorama Inpainting for Locale-Aware Indoor Lighting Prediction](http://arxiv.org/abs/2303.10344) #robust</code></li>
<li>Summary: <p>Predicting panoramic indoor lighting from a single perspective image is a
fundamental but highly ill-posed problem in computer vision and graphics. To
achieve locale-aware and robust prediction, this problem can be decomposed into
three sub-tasks: depth-based image warping, panorama inpainting and
high-dynamic-range (HDR) reconstruction, among which the success of panorama
inpainting plays a key role. Recent methods mostly rely on convolutional neural
networks (CNNs) to fill the missing contents in the warped panorama. However,
they usually achieve suboptimal performance since the missing contents occupy a
very large portion in the panoramic space while CNNs are plagued by limited
receptive fields. The spatially-varying distortion in the spherical signals
further increases the difficulty for conventional CNNs. To address these
issues, we propose a local-to-global strategy for large-scale panorama
inpainting. In our method, a depth-guided local inpainting is first applied on
the warped panorama to fill small but dense holes. Then, a transformer-based
network, dubbed PanoTransformer, is designed to hallucinate reasonable global
structures in the large holes. To avoid distortion, we further employ cubemap
projection in our design of PanoTransformer. The high-quality panorama
recovered at any locale helps us to capture spatially-varying indoor
illumination with physically-plausible global structures and fine details.
</p></li>
</ul>

<h3>Title: SOCS: Semantically-aware Object Coordinate Space for Category-Level 6D Object Pose Estimation under Large Shape Variations. (arXiv:2303.10346v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10346">http://arxiv.org/abs/2303.10346</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10346] SOCS: Semantically-aware Object Coordinate Space for Category-Level 6D Object Pose Estimation under Large Shape Variations](http://arxiv.org/abs/2303.10346) #robust</code></li>
<li>Summary: <p>Most learning-based approaches to category-level 6D pose estimation are
design around normalized object coordinate space (NOCS). While being
successful, NOCS-based methods become inaccurate and less robust when handling
objects of a category containing significant intra-category shape variations.
This is because the object coordinates induced by global and rigid alignment of
objects are semantically incoherent, making the coordinate regression hard to
learn and generalize. We propose Semantically-aware Object Coordinate Space
(SOCS) built by warping-and-aligning the objects guided by a sparse set of
keypoints with semantically meaningful correspondence. SOCS is semantically
coherent: Any point on the surface of a object can be mapped to a semantically
meaningful location in SOCS, allowing for accurate pose and size estimation
under large shape variations. To learn effective coordinate regression to SOCS,
we propose a novel multi-scale coordinate-based attention network. Evaluations
demonstrate that our method is easy to train, well-generalizing for large
intra-category shape variations and robust to inter-object occlusions.
</p></li>
</ul>

<h3>Title: HGIB: Prognosis for Alzheimer's Disease via Hypergraph Information Bottleneck. (arXiv:2303.10390v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10390">http://arxiv.org/abs/2303.10390</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10390] HGIB: Prognosis for Alzheimer's Disease via Hypergraph Information Bottleneck](http://arxiv.org/abs/2303.10390) #robust</code></li>
<li>Summary: <p>Alzheimer's disease prognosis is critical for early Mild Cognitive Impairment
patients for timely treatment to improve the patient's quality of life. Whilst
existing prognosis techniques demonstrate potential results, they are highly
limited in terms of using a single modality. Most importantly, they fail in
considering a key element for prognosis: not all features extracted at the
current moment may contribute to the prognosis prediction several years later.
To address the current drawbacks of the literature, we propose a novel
hypergraph framework based on an information bottleneck strategy (HGIB).
Firstly, our framework seeks to discriminate irrelevant information, and
therefore, solely focus on harmonising relevant information for future MCI
conversion prediction e.g., two years later). Secondly, our model
simultaneously accounts for multi-modal data based on imaging and non-imaging
modalities. HGIB uses a hypergraph structure to represent the multi-modality
data and accounts for various data modality types. Thirdly, the key of our
model is based on a new optimisation scheme. It is based on modelling the
principle of information bottleneck into loss functions that can be integrated
into our hypergraph neural network. We demonstrate, through extensive
experiments on ADNI, that our proposed HGIB framework outperforms existing
state-of-the-art hypergraph neural networks for Alzheimer's disease prognosis.
We showcase our model even under fewer labels. Finally, we further support the
robustness and generalisation capabilities of our framework under both
topological and feature perturbations.
</p></li>
</ul>

<h3>Title: MotionTrack: Learning Robust Short-term and Long-term Motions for Multi-Object Tracking. (arXiv:2303.10404v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10404">http://arxiv.org/abs/2303.10404</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10404] MotionTrack: Learning Robust Short-term and Long-term Motions for Multi-Object Tracking](http://arxiv.org/abs/2303.10404) #robust</code></li>
<li>Summary: <p>The main challenge of Multi-Object Tracking~(MOT) lies in maintaining a
continuous trajectory for each target. Existing methods often learn reliable
motion patterns to match the same target between adjacent frames and
discriminative appearance features to re-identify the lost targets after a long
period. However, the reliability of motion prediction and the discriminability
of appearances can be easily hurt by dense crowds and extreme occlusions in the
tracking process. In this paper, we propose a simple yet effective multi-object
tracker, i.e., MotionTrack, which learns robust short-term and long-term
motions in a unified framework to associate trajectories from a short to long
range. For dense crowds, we design a novel Interaction Module to learn
interaction-aware motions from short-term trajectories, which can estimate the
complex movement of each target. For extreme occlusions, we build a novel
Refind Module to learn reliable long-term motions from the target's history
trajectory, which can link the interrupted trajectory with its corresponding
detection. Our Interaction Module and Refind Module are embedded in the
well-known tracking-by-detection paradigm, which can work in tandem to maintain
superior performance. Extensive experimental results on MOT17 and MOT20
datasets demonstrate the superiority of our approach in challenging scenarios,
and it achieves state-of-the-art performances at various MOT metrics.
</p></li>
</ul>

<h3>Title: ExplainFix: Explainable Spatially Fixed Deep Networks. (arXiv:2303.10408v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10408">http://arxiv.org/abs/2303.10408</a></li>
<li>Code URL: <a href="https://github.com/adgaudio/explainfix">https://github.com/adgaudio/explainfix</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10408] ExplainFix: Explainable Spatially Fixed Deep Networks](http://arxiv.org/abs/2303.10408) #robust</code></li>
<li>Summary: <p>Is there an initialization for deep networks that requires no learning?
ExplainFix adopts two design principles: the "fixed filters" principle that all
spatial filter weights of convolutional neural networks can be fixed at
initialization and never learned, and the "nimbleness" principle that only few
network parameters suffice. We contribute (a) visual model-based explanations,
(b) speed and accuracy gains, and (c) novel tools for deep convolutional neural
networks. ExplainFix gives key insights that spatially fixed networks should
have a steered initialization, that spatial convolution layers tend to
prioritize low frequencies, and that most network parameters are not necessary
in spatially fixed models. ExplainFix models have up to 100x fewer spatial
filter kernels than fully learned models and matching or improved accuracy. Our
extensive empirical analysis confirms that ExplainFix guarantees nimbler models
(train up to 17\% faster with channel pruning), matching or improved predictive
performance (spanning 13 distinct baseline models, four architectures and two
medical image datasets), improved robustness to larger learning rate, and
robustness to varying model size. We are first to demonstrate that all spatial
filters in state-of-the-art convolutional deep networks can be fixed at
initialization, not learned.
</p></li>
</ul>

<h3>Title: Identification of Novel Classes for Improving Few-Shot Object Detection. (arXiv:2303.10422v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10422">http://arxiv.org/abs/2303.10422</a></li>
<li>Code URL: <a href="https://github.com/zshanggu/htrpn">https://github.com/zshanggu/htrpn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10422] Identification of Novel Classes for Improving Few-Shot Object Detection](http://arxiv.org/abs/2303.10422) #robust</code></li>
<li>Summary: <p>Conventional training of deep neural networks requires a large number of the
annotated image which is a laborious and time-consuming task, particularly for
rare objects. Few-shot object detection (FSOD) methods offer a remedy by
realizing robust object detection using only a few training samples per class.
An unexplored challenge for FSOD is that instances from unlabeled novel classes
that do not belong to the fixed set of training classes appear in the
background. These objects behave similarly to label noise, leading to FSOD
performance degradation. We develop a semi-supervised algorithm to detect and
then utilize these unlabeled novel objects as positive samples during training
to improve FSOD performance. Specifically, we propose a hierarchical ternary
classification region proposal network (HTRPN) to localize the potential
unlabeled novel objects and assign them new objectness labels. Our improved
hierarchical sampling strategy for the region proposal network (RPN) also
boosts the perception ability of the object detection model for large objects.
Our experimental results indicate that our method is effective and outperforms
the existing state-of-the-art (SOTA) FSOD methods.
</p></li>
</ul>

<h3>Title: Augmenting and Aligning Snippets for Few-Shot Video Domain Adaptation. (arXiv:2303.10451v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10451">http://arxiv.org/abs/2303.10451</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10451] Augmenting and Aligning Snippets for Few-Shot Video Domain Adaptation](http://arxiv.org/abs/2303.10451) #robust</code></li>
<li>Summary: <p>For video models to be transferred and applied seamlessly across video tasks
in varied environments, Video Unsupervised Domain Adaptation (VUDA) has been
introduced to improve the robustness and transferability of video models.
However, current VUDA methods rely on a vast amount of high-quality unlabeled
target data, which may not be available in real-world cases. We thus consider a
more realistic \textit{Few-Shot Video-based Domain Adaptation} (FSVDA) scenario
where we adapt video models with only a few target video samples. While a few
methods have touched upon Few-Shot Domain Adaptation (FSDA) in images and in
FSVDA, they rely primarily on spatial augmentation for target domain expansion
with alignment performed statistically at the instance level. However, videos
contain more knowledge in terms of rich temporal and semantic information,
which should be fully considered while augmenting target domains and performing
alignment in FSVDA. We propose a novel SSA2lign to address FSVDA at the snippet
level, where the target domain is expanded through a simple snippet-level
augmentation followed by the attentive alignment of snippets both semantically
and statistically, where semantic alignment of snippets is conducted through
multiple perspectives. Empirical results demonstrate state-of-the-art
performance of SSA2lign across multiple cross-domain action recognition
benchmarks.
</p></li>
</ul>

<h3>Title: Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks. (arXiv:2303.10455v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10455">http://arxiv.org/abs/2303.10455</a></li>
<li>Code URL: <a href="https://github.com/neurai-lab/lure">https://github.com/neurai-lab/lure</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10455] Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks](http://arxiv.org/abs/2303.10455) #robust</code></li>
<li>Summary: <p>Deep neural networks (DNNs) are often trained on the premise that the
complete training data set is provided ahead of time. However, in real-world
scenarios, data often arrive in chunks over time. This leads to important
considerations about the optimal strategy for training DNNs, such as whether to
fine-tune them with each chunk of incoming data (warm-start) or to retrain them
from scratch with the entire corpus of data whenever a new chunk is available.
While employing the latter for training can be resource-intensive, recent work
has pointed out the lack of generalization in warm-start models. Therefore, to
strike a balance between efficiency and generalization, we introduce Learn,
Unlearn, and Relearn (LURE) an online learning paradigm for DNNs. LURE
interchanges between the unlearning phase, which selectively forgets the
undesirable information in the model through weight reinitialization in a
data-dependent manner, and the relearning phase, which emphasizes learning on
generalizable features. We show that our training paradigm provides consistent
performance gains across datasets in both classification and few-shot settings.
We further show that it leads to more robust and well-calibrated models.
</p></li>
</ul>

<h3>Title: Exploring Partial Knowledge Base Inference in Biomedical Entity Linking. (arXiv:2303.10330v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10330">http://arxiv.org/abs/2303.10330</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10330] Exploring Partial Knowledge Base Inference in Biomedical Entity Linking](http://arxiv.org/abs/2303.10330) #robust</code></li>
<li>Summary: <p>Biomedical entity linking (EL) consists of named entity recognition (NER) and
named entity disambiguation (NED). EL models are trained on corpora labeled by
a predefined KB. However, it is a common scenario that only entities within a
subset of the KB are precious to stakeholders. We name this scenario partial
knowledge base inference: training an EL model with one KB and inferring on the
part of it without further training. In this work, we give a detailed
definition and evaluation procedures for this practically valuable but
significantly understudied scenario and evaluate methods from three
representative EL paradigms. We construct partial KB inference benchmarks and
witness a catastrophic degradation in EL performance due to dramatically
precision drop. Our findings reveal these EL paradigms can not correctly handle
unlinkable mentions (NIL), so they are not robust to partial KB inference. We
also propose two simple-and-effective redemption methods to combat the NIL
issue with little computational overhead.
</p></li>
</ul>

<h3>Title: A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models. (arXiv:2303.10420v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10420">http://arxiv.org/abs/2303.10420</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10420] A Comprehensive Capability Analysis of GPT-3 and GPT-3](http://arxiv.org/abs/2303.10420) #robust</code></li>
<li>Summary: <p>GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on,
have gained considerable attention due to their exceptional natural language
processing capabilities. However, despite the abundance of research on the
difference in capabilities between GPT series models and fine-tuned models,
there has been limited attention given to the evolution of GPT series models'
capabilities over time. To conduct a comprehensive analysis of the capabilities
of GPT series models, we select six representative models, comprising two GPT-3
series models (i.e., davinci and text-davinci-001) and four GPT-3.5 series
models (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and
gpt-3.5-turbo). We evaluate their performance on nine natural language
understanding (NLU) tasks using 21 datasets. In particular, we compare the
performance and robustness of different models for each task under zero-shot
and few-shot scenarios. Our extensive experiments reveal that the overall
ability of GPT series models on NLU tasks does not increase gradually as the
models evolve, especially with the introduction of the RLHF training strategy.
While this strategy enhances the models' ability to generate human-like
responses, it also compromises their ability to solve some tasks. Furthermore,
our findings indicate that there is still room for improvement in areas such as
model robustness.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: ABC: Attention with Bilinear Correlation for Infrared Small Target Detection. (arXiv:2303.10321v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10321">http://arxiv.org/abs/2303.10321</a></li>
<li>Code URL: <a href="https://github.com/panpeiwen/abc">https://github.com/panpeiwen/abc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10321] ABC: Attention with Bilinear Correlation for Infrared Small Target Detection](http://arxiv.org/abs/2303.10321) #extraction</code></li>
<li>Summary: <p>Infrared small target detection (ISTD) has a wide range of applications in
early warning, rescue, and guidance. However, CNN based deep learning methods
are not effective at segmenting infrared small target (IRST) that it lack of
clear contour and texture features, and transformer based methods also struggle
to achieve significant results due to the absence of convolution induction
bias. To address these issues, we propose a new model called attention with
bilinear correlation (ABC), which is based on the transformer architecture and
includes a convolution linear fusion transformer (CLFT) module with a novel
attention mechanism for feature extraction and fusion, which effectively
enhances target features and suppresses noise. Additionally, our model includes
a u-shaped convolution-dilated convolution (UCDC) module located deeper layers
of the network, which takes advantage of the smaller resolution of deeper
features to obtain finer semantic information. Experimental results on public
datasets demonstrate that our approach achieves state-of-the-art performance.
Code is available at https://github.com/PANPEIWEN/ABC
</p></li>
</ul>

<h3>Title: Mutilmodal Feature Extraction and Attention-based Fusion for Emotion Estimation in Videos. (arXiv:2303.10421v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10421">http://arxiv.org/abs/2303.10421</a></li>
<li>Code URL: <a href="https://github.com/xkwangcn/abaw-5th-rt-iai">https://github.com/xkwangcn/abaw-5th-rt-iai</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10421] Mutilmodal Feature Extraction and Attention-based Fusion for Emotion Estimation in Videos](http://arxiv.org/abs/2303.10421) #extraction</code></li>
<li>Summary: <p>The continuous improvement of human-computer interaction technology makes it
possible to compute emotions. In this paper, we introduce our submission to the
CVPR 2023 Competition on Affective Behavior Analysis in-the-wild (ABAW).
Sentiment analysis in human-computer interaction should, as far as possible
Start with multiple dimensions, fill in the single imperfect emotion channel,
and finally determine the emotion tendency by fitting multiple results.
Therefore, We exploited multimodal features extracted from video of different
lengths from the competition dataset, including audio, pose and images.
Well-informed emotion representations drive us to propose a Attention-based
multimodal framework for emotion estimation. Our system achieves the
performance of 0.361 on the validation dataset. The code is available at
[https://github.com/xkwangcn/ABAW-5th-RT-IAI].
</p></li>
</ul>

<h3>Title: Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations. (arXiv:2303.10523v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10523">http://arxiv.org/abs/2303.10523</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10523] Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations](http://arxiv.org/abs/2303.10523) #extraction</code></li>
<li>Summary: <p>An important line of research attempts to explain CNN image classifier
predictions and intermediate layer representations in terms of human
understandable concepts. In this work, we expand on previous works in the
literature that use annotated concept datasets to extract interpretable feature
space directions and propose an unsupervised post-hoc method to extract a
disentangling interpretable basis by looking for the rotation of the feature
space that explains sparse one-hot thresholded transformed representations of
pixel activations. We do experimentation with existing popular CNNs and
demonstrate the effectiveness of our method in extracting an interpretable
basis across network architectures and training datasets. We make extensions to
the existing basis interpretability metrics found in the literature and show
that, intermediate layer representations become more interpretable when
transformed to the bases extracted with our method. Finally, using the basis
interpretability metrics, we compare the bases extracted with our method with
the bases derived with a supervised approach and find that, in one aspect, the
proposed unsupervised approach has a strength that constitutes a limitation of
the supervised one and give potential directions for future research.
</p></li>
</ul>

<h3>Title: Wheat Head Counting by Estimating a Density Map with Convolutional Neural Networks. (arXiv:2303.10542v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10542">http://arxiv.org/abs/2303.10542</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10542] Wheat Head Counting by Estimating a Density Map with Convolutional Neural Networks](http://arxiv.org/abs/2303.10542) #extraction</code></li>
<li>Summary: <p>Wheat is one of the most significant crop species with an annual worldwide
grain production of 700 million tonnes. Assessing the production of wheat
spikes can help us measure the grain production. Thus, detecting and
characterizing spikes from images of wheat fields is an essential component in
a wheat breeding process. In this study, we propose three wheat head counting
networks (WHCNet_1, WHCNet_2 and WHCNet_3) to accurately estimate the wheat
head count from an individual image and construct high quality density map,
which illustrates the distribution of wheat heads in the image. The WHCNets are
composed of two major components: a convolutional neural network (CNN) as the
front-end for wheat head image feature extraction and a CNN with skip
connections for the back-end to generate high-quality density maps. The dataset
used in this study is the Global Wheat Head Detection (GWHD) dataset, which is
a large, diverse, and well-labelled dataset of wheat images and built by a
joint international collaborative effort. We compare our methods with CSRNet, a
deep learning method which developed for highly congested scenes understanding
and performing accurate count estimation as well as presenting high quality
density maps. By taking the advantage of the skip connections between CNN
layers, WHCNets integrate features from low CNN layers to high CNN layers,
thus, the output density maps have both high spatial resolution and detailed
representations of the input images. The experiments showed that our methods
outperformed CSRNet in terms of the evaluation metrics, mean absolute error
(MAE) and the root mean squared error (RMSE) with smaller model sizes. The code
has been deposited on GitHub (\url{https://github.com/hyguozz}).
</p></li>
</ul>

<h3>Title: Multi-modal Facial Action Unit Detection with Large Pre-trained Models for the 5th Competition on Affective Behavior Analysis in-the-wild. (arXiv:2303.10590v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10590">http://arxiv.org/abs/2303.10590</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10590] Multi-modal Facial Action Unit Detection with Large Pre-trained Models for the 5th Competition on Affective Behavior Analysis in-the-wild](http://arxiv.org/abs/2303.10590) #extraction</code></li>
<li>Summary: <p>Facial action unit detection has emerged as an important task within facial
expression analysis, aimed at detecting specific pre-defined, objective facial
expressions, such as lip tightening and cheek raising. This paper presents our
submission to the Affective Behavior Analysis in-the-wild (ABAW) 2023
Competition for AU detection. We propose a multi-modal method for facial action
unit detection with visual, acoustic, and lexical features extracted from the
large pre-trained models. To provide high-quality details for visual feature
extraction, we apply super-resolution and face alignment to the training data
and show potential performance gain. Our approach achieves the F1 score of
52.3\% on the official validation set of the 5th ABAW Challenge.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: An Empirical Evaluation of Federated Contextual Bandit Algorithms. (arXiv:2303.10218v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10218">http://arxiv.org/abs/2303.10218</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10218] An Empirical Evaluation of Federated Contextual Bandit Algorithms](http://arxiv.org/abs/2303.10218) #federate</code></li>
<li>Summary: <p>As the adoption of federated learning increases for learning from sensitive
data local to user devices, it is natural to ask if the learning can be done
using implicit signals generated as users interact with the applications of
interest, rather than requiring access to explicit labels which can be
difficult to acquire in many tasks. We approach such problems with the
framework of federated contextual bandits, and develop variants of prominent
contextual bandit algorithms from the centralized seting for the federated
setting. We carefully evaluate these algorithms in a range of scenarios
simulated using publicly available datasets. Our simulations model typical
setups encountered in the real-world, such as various misalignments between an
initial pre-trained model and the subsequent user interactions due to
non-stationarity in the data and/or heterogeneity across clients. Our
experiments reveal the surprising effectiveness of the simple and commonly used
softmax heuristic in balancing the well-know exploration-exploitation tradeoff
across the breadth of our settings.
</p></li>
</ul>

<h3>Title: Multi-Task Model Personalization for Federated Supervised SVM in Heterogeneous Networks. (arXiv:2303.10254v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10254">http://arxiv.org/abs/2303.10254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10254] Multi-Task Model Personalization for Federated Supervised SVM in Heterogeneous Networks](http://arxiv.org/abs/2303.10254) #federate</code></li>
<li>Summary: <p>In this paper, we design an efficient distributed iterative learning method
based on support vector machines (SVMs), which tackles federated classification
and regression. The proposed method supports efficient computations and model
exchange in a network of heterogeneous nodes and allows personalization of the
learning model in the presence of non-i.i.d. data. To further enhance privacy,
we introduce a random mask procedure that helps avoid data inversion. Finally,
we analyze the impact of the proposed privacy mechanisms and the heterogeneity
of participant hardware and data on the system performance.
</p></li>
</ul>

<h3>Title: Client Selection for Generalization in Accelerated Federated Learning: A Multi-Armed Bandit Approach. (arXiv:2303.10373v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10373">http://arxiv.org/abs/2303.10373</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10373] Client Selection for Generalization in Accelerated Federated Learning: A Multi-Armed Bandit Approach](http://arxiv.org/abs/2303.10373) #federate</code></li>
<li>Summary: <p>Federated learning (FL) is an emerging machine learning (ML) paradigm used to
train models across multiple nodes (i.e., clients) holding local data sets,
without explicitly exchanging the data. It has attracted a growing interest in
recent years due to its advantages in terms of privacy considerations, and
communication resources. In FL, selected clients train their local models and
send a function of the models to the server, which consumes a random processing
and transmission time. The server updates the global model and broadcasts it
back to the clients. The client selection problem in FL is to schedule a subset
of the clients for training and transmission at each given time so as to
optimize the learning performance. In this paper, we present a novel
multi-armed bandit (MAB)-based approach for client selection to minimize the
training latency without harming the ability of the model to generalize, that
is, to provide reliable predictions for new observations. We develop a novel
algorithm to achieve this goal, dubbed Bandit Scheduling for FL (BSFL). We
analyze BSFL theoretically, and show that it achieves a logarithmic regret,
defined as the loss of BSFL as compared to a genie that has complete knowledge
about the latency means of all clients. Furthermore, simulation results using
synthetic and real datasets demonstrate that BSFL is superior to existing
methods.
</p></li>
</ul>

<h3>Title: Hierarchical Personalized Federated Learning Over Massive Mobile Edge Computing Networks. (arXiv:2303.10580v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10580">http://arxiv.org/abs/2303.10580</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10580] Hierarchical Personalized Federated Learning Over Massive Mobile Edge Computing Networks](http://arxiv.org/abs/2303.10580) #federate</code></li>
<li>Summary: <p>Personalized Federated Learning (PFL) is a new Federated Learning (FL)
paradigm, particularly tackling the heterogeneity issues brought by various
mobile user equipments (UEs) in mobile edge computing (MEC) networks. However,
due to the ever-increasing number of UEs and the complicated administrative
work it brings, it is desirable to switch the PFL algorithm from its
conventional two-layer framework to a multiple-layer one. In this paper, we
propose hierarchical PFL (HPFL), an algorithm for deploying PFL over massive
MEC networks. The UEs in HPFL are divided into multiple clusters, and the UEs
in each cluster forward their local updates to the edge server (ES)
synchronously for edge model aggregation, while the ESs forward their edge
models to the cloud server semi-asynchronously for global model aggregation.
The above training manner leads to a tradeoff between the training loss in each
round and the round latency. HPFL combines the objectives of training loss
minimization and round latency minimization while jointly determining the
optimal bandwidth allocation as well as the ES scheduling policy in the
hierarchical learning framework. Extensive experiments verify that HPFL not
only guarantees convergence in hierarchical aggregation frameworks but also has
advantages in round training loss maximization and round latency minimization.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Revisiting Automatic Question Summarization Evaluation in the Biomedical Domain. (arXiv:2303.10328v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10328">http://arxiv.org/abs/2303.10328</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10328] Revisiting Automatic Question Summarization Evaluation in the Biomedical Domain](http://arxiv.org/abs/2303.10328) #fair</code></li>
<li>Summary: <p>Automatic evaluation metrics have been facilitating the rapid development of
automatic summarization methods by providing instant and fair assessments of
the quality of summaries. Most metrics have been developed for the general
domain, especially news and meeting notes, or other language-generation tasks.
However, these metrics are applied to evaluate summarization systems in
different domains, such as biomedical question summarization. To better
understand whether commonly used evaluation metrics are capable of evaluating
automatic summarization in the biomedical domain, we conduct human evaluations
of summarization quality from four different aspects of a biomedical question
summarization task. Based on human judgments, we identify different noteworthy
features for current automatic metrics and summarization systems as well. We
also release a dataset of our human annotations to aid the research of
summarization evaluation metrics in the biomedical domain.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Interpretable Reinforcement Learning via Neural Additive Models for Inventory Management. (arXiv:2303.10382v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10382">http://arxiv.org/abs/2303.10382</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10382] Interpretable Reinforcement Learning via Neural Additive Models for Inventory Management](http://arxiv.org/abs/2303.10382) #interpretability</code></li>
<li>Summary: <p>The COVID-19 pandemic has highlighted the importance of supply chains and the
role of digital management to react to dynamic changes in the environment. In
this work, we focus on developing dynamic inventory ordering policies for a
multi-echelon, i.e. multi-stage, supply chain. Traditional inventory
optimization methods aim to determine a static reordering policy. Thus, these
policies are not able to adjust to dynamic changes such as those observed
during the COVID-19 crisis. On the other hand, conventional strategies offer
the advantage of being interpretable, which is a crucial feature for supply
chain managers in order to communicate decisions to their stakeholders. To
address this limitation, we propose an interpretable reinforcement learning
approach that aims to be as interpretable as the traditional static policies
while being as flexible and environment-agnostic as other deep learning-based
reinforcement learning solutions. We propose to use Neural Additive Models as
an interpretable dynamic policy of a reinforcement learning agent, showing that
this approach is competitive with a standard full connected policy. Finally, we
use the interpretability property to gain insights into a complex ordering
strategy for a simple, linear three-echelon inventory supply chain.
</p></li>
</ul>

<h3>Title: Machine learning with data assimilation and uncertainty quantification for dynamical systems: a review. (arXiv:2303.10462v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10462">http://arxiv.org/abs/2303.10462</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10462] Machine learning with data assimilation and uncertainty quantification for dynamical systems: a review](http://arxiv.org/abs/2303.10462) #interpretability</code></li>
<li>Summary: <p>Data Assimilation (DA) and Uncertainty quantification (UQ) are extensively
used in analysing and reducing error propagation in high-dimensional
spatial-temporal dynamics. Typical applications span from computational fluid
dynamics (CFD) to geoscience and climate systems. Recently, much effort has
been given in combining DA, UQ and machine learning (ML) techniques. These
research efforts seek to address some critical challenges in high-dimensional
dynamical systems, including but not limited to dynamical system
identification, reduced order surrogate modelling, error covariance
specification and model error correction. A large number of developed
techniques and methodologies exhibit a broad applicability across numerous
domains, resulting in the necessity for a comprehensive guide. This paper
provides the first overview of the state-of-the-art researches in this
interdisciplinary field, covering a wide range of applications. This review
aims at ML scientists who attempt to apply DA and UQ techniques to improve the
accuracy and the interpretability of their models, but also at DA and UQ
experts who intend to integrate cutting-edge ML approaches to their systems.
Therefore, this article has a special focus on how ML methods can overcome the
existing limits of DA and UQ, and vice versa. Some exciting perspectives of
this rapidly developing research field are also discussed.
</p></li>
</ul>

<h3>Title: LNO: Laplace Neural Operator for Solving Differential Equations. (arXiv:2303.10528v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10528">http://arxiv.org/abs/2303.10528</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10528] LNO: Laplace Neural Operator for Solving Differential Equations](http://arxiv.org/abs/2303.10528) #interpretability</code></li>
<li>Summary: <p>We introduce the Laplace neural operator (LNO), which leverages the Laplace
transform to decompose the input space. Unlike the Fourier Neural Operator
(FNO), LNO can handle non-periodic signals, account for transient responses,
and exhibit exponential convergence. LNO incorporates the pole-residue
relationship between the input and the output space, enabling greater
interpretability and improved generalization ability. Herein, we demonstrate
the superior approximation accuracy of a single Laplace layer in LNO over four
Fourier modules in FNO in approximating the solutions of three ODEs (Duffing
oscillator, driven gravity pendulum, and Lorenz system) and three PDEs
(Euler-Bernoulli beam, diffusion equation, and reaction-diffusion system).
Notably, LNO outperforms FNO in capturing transient responses in undamped
scenarios. For the linear Euler-Bernoulli beam and diffusion equation, LNO's
exact representation of the pole-residue formulation yields significantly
better results than FNO. For the nonlinear reaction-diffusion system, LNO's
errors are smaller than those of FNO, demonstrating the effectiveness of using
system poles and residues as network parameters for operator learning. Overall,
our results suggest that LNO represents a promising new approach for learning
neural operators that map functions between infinite-dimensional spaces.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: 3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process. (arXiv:2303.10406v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10406">http://arxiv.org/abs/2303.10406</a></li>
<li>Code URL: <a href="https://github.com/colorful-liyu/3dqd">https://github.com/colorful-liyu/3dqd</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10406] 3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process](http://arxiv.org/abs/2303.10406) #diffusion</code></li>
<li>Summary: <p>We develop a generalized 3D shape generation prior model, tailored for
multiple 3D tasks including unconditional shape generation, point cloud
completion, and cross-modality shape generation, etc. On one hand, to precisely
capture local fine detailed shape information, a vector quantized variational
autoencoder (VQ-VAE) is utilized to index local geometry from a compactly
learned codebook based on a broad set of task training data. On the other hand,
a discrete diffusion generator is introduced to model the inherent structural
dependencies among different tokens. In the meantime, a multi-frequency fusion
module (MFM) is developed to suppress high-frequency shape feature
fluctuations, guided by multi-frequency contextual information. The above
designs jointly equip our proposed 3D shape prior model with high-fidelity,
diverse features as well as the capability of cross-modality alignment, and
extensive experiments have demonstrated superior performances on various 3D
shape generation tasks.
</p></li>
</ul>

<h3>Title: DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification. (arXiv:2303.10610v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.10610">http://arxiv.org/abs/2303.10610</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.10610] DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification](http://arxiv.org/abs/2303.10610) #diffusion</code></li>
<li>Summary: <p>Diffusion Probabilistic Models have recently shown remarkable performance in
generative image modeling, attracting significant attention in the computer
vision community. However, while a substantial amount of diffusion-based
research has focused on generative tasks, few studies have applied diffusion
models to general medical image classification. In this paper, we propose the
first diffusion-based model (named DiffMIC) to address general medical image
classification by eliminating unexpected noise and perturbations in medical
images and robustly capturing semantic representation. To achieve this goal, we
devise a dual conditional guidance strategy that conditions each diffusion step
with multiple granularities to improve step-wise regional attention.
Furthermore, we propose learning the mutual information in each granularity by
enforcing Maximum-Mean Discrepancy regularization during the diffusion forward
process. We evaluate the effectiveness of our DiffMIC on three medical
classification tasks with different image modalities, including placental
maturity grading on ultrasound images, skin lesion classification using
dermatoscopic images, and diabetic retinopathy grading using fundus images. Our
experimental results demonstrate that DiffMIC outperforms state-of-the-art
methods by a significant margin, indicating the universality and effectiveness
of the proposed model.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
