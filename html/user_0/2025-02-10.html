<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-02-10</h1>
<h3>Title: JingFang: A Traditional Chinese Medicine Large Language Model of Expert-Level Medical Diagnosis and Syndrome Differentiation-Based Treatment</h3>
<ul>
<li><strong>Authors: </strong>Yehan Yan, Tianhao Ma, Ruotai Li, Xinhan Zheng, Guodong Shan, Chisheng Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04345">https://arxiv.org/abs/2502.04345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04345">https://arxiv.org/pdf/2502.04345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04345]] JingFang: A Traditional Chinese Medicine Large Language Model of Expert-Level Medical Diagnosis and Syndrome Differentiation-Based Treatment(https://arxiv.org/abs/2502.04345)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>Traditional Chinese medicine (TCM) plays a vital role in health protection and disease treatment, but its practical application requires extensive medical knowledge and clinical experience. Existing TCM Large Language Models (LLMs) exhibit critical limitations of uncomprehensive medical consultation and diagnoses, and inaccurate syndrome differentiation-based treatment. To address these issues, this study establishes JingFang (JF): a novel TCM Large Language Model that demonstrates the expert-level capability of medical diagnosis and syndrome differentiation-based treatment. We innovate a Multi-agent Dynamic Collaborative Chain-of-Thought Mechanism (MDCCTM) for medical consultation, enabling JF with effective and accurate diagnostic ability. In addition, a Syndrome Agent and a Dual-Stage Retrieval Scheme (DSRS) are developed to significantly enhance the capacity of JF for disease treatment based on syndrome differentiation. JingFang not only facilitates the application of LLMs but also promotes the effective practice of TCM in human health protection and disease treatment.</li>
</ul>

<h3>Title: Multi-Lingual Cyber Threat Detection in Tweets/X Using ML, DL, and LLM: A Comparative Analysis</h3>
<ul>
<li><strong>Authors: </strong>Saydul Akbar Murad, Ashim Dahal, Nick Rahimi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04346">https://arxiv.org/abs/2502.04346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04346">https://arxiv.org/pdf/2502.04346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04346]] Multi-Lingual Cyber Threat Detection in Tweets/X Using ML, DL, and LLM: A Comparative Analysis(https://arxiv.org/abs/2502.04346)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Cyber threat detection has become an important area of focus in today's digital age due to the growing spread of fake information and harmful content on social media platforms such as Twitter (now 'X'). These cyber threats, often disguised within tweets, pose significant risks to individuals, communities, and even nations, emphasizing the need for effective detection systems. While previous research has explored tweet-based threats, much of the work is limited to specific languages, domains, or locations, or relies on single-model approaches, reducing their applicability to diverse real-world scenarios. To address these gaps, our study focuses on multi-lingual tweet cyber threat detection using a variety of advanced models. The research was conducted in three stages: (1) We collected and labeled tweet datasets in four languages English, Chinese, Russian, and Arabic employing both manual and polarity-based labeling methods to ensure high-quality annotations. (2) Each dataset was analyzed individually using machine learning (ML) and deep learning (DL) models to assess their performance on distinct languages. (3) Finally, we combined all four datasets into a single multi-lingual dataset and applied DL and large language model (LLM) architectures to evaluate their efficacy in identifying cyber threats across various languages. Our results show that among machine learning models, Random Forest (RF) attained the highest performance; however, the Bi-LSTM architecture consistently surpassed other DL and LLM architectures across all datasets. These findings underline the effectiveness of Bi-LSTM in multilingual cyber threat detection. The code for this paper can be found at this link: this https URL.</li>
</ul>

<h3>Title: SCALM: Detecting Bad Practices in Smart Contracts Through LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zongwei Li, Xiaoqi Li, Wenkai Li, Xin Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04347">https://arxiv.org/abs/2502.04347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04347">https://arxiv.org/pdf/2502.04347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04347]] SCALM: Detecting Bad Practices in Smart Contracts Through LLMs(https://arxiv.org/abs/2502.04347)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>As the Ethereum platform continues to mature and gain widespread usage, it is crucial to maintain high standards of smart contract writing practices. While bad practices in smart contracts may not directly lead to security issues, they do elevate the risk of encountering problems. Therefore, to understand and avoid these bad practices, this paper introduces the first systematic study of bad practices in smart contracts, delving into over 35 specific issues. Specifically, we propose a large language models (LLMs)-based framework, SCALM. It combines Step-Back Prompting and Retrieval-Augmented Generation (RAG) to identify and address various bad practices effectively. Our extensive experiments using multiple LLMs and datasets have shown that SCALM outperforms existing tools in detecting bad practices in smart contracts.</li>
</ul>

<h3>Title: Prompt-based Depth Pruning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Juyun Wee, Minjae Park, Jaeho Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04348">https://arxiv.org/abs/2502.04348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04348">https://arxiv.org/pdf/2502.04348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04348]] Prompt-based Depth Pruning of Large Language Models(https://arxiv.org/abs/2502.04348)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Depth pruning aims to reduce the inference cost of a large language model without any hardware-specific complications, by simply removing several less important transformer blocks. However, our empirical findings suggest that the importance of a transformer block may be highly task-dependent -- a block that is crucial for a task can be removed without degrading the accuracy on another task. Based on this observation, we develop a dynamic depth pruning algorithm, coined PuDDing (Prompt-routed Dynamic Depth Pruning), which determines which blocks to omit from the model based on the input prompt. PuDDing operates by training a lightweight router to predict the best omission set among a set of options, where this option set has also been constructed in a data-driven manner. Empirical results on commonsense reasoning benchmarks demonstrate that PuDDing effectively accelerates the inference language models, and achieves better on-task performance than static depth pruning baselines.</li>
</ul>

<h3>Title: Dynamic benchmarking framework for LLM-based conversational data capture</h3>
<ul>
<li><strong>Authors: </strong>Pietro Alessandro Aluffi, Patrick Zietkiewicz, Marya Bazzi, Matt Arderne, Vladimirs Murevics</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04349">https://arxiv.org/abs/2502.04349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04349">https://arxiv.org/pdf/2502.04349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04349]] Dynamic benchmarking framework for LLM-based conversational data capture(https://arxiv.org/abs/2502.04349)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>The rapid evolution of large language models (LLMs) has transformed conversational agents, enabling complex human-machine interactions. However, evaluation frameworks often focus on single tasks, failing to capture the dynamic nature of multi-turn dialogues. This paper introduces a dynamic benchmarking framework to assess LLM-based conversational agents through interactions with synthetic users. The framework integrates generative agent simulation to evaluate performance on key dimensions: information extraction, context awareness, and adaptive engagement. By simulating various aspects of user behavior, our work provides a scalable, automated, and flexible benchmarking approach. Experimental evaluation - within a loan application use case - demonstrates the framework's effectiveness under one-shot and few-shot extraction conditions. Results show that adaptive strategies improve data extraction accuracy, especially when handling ambiguous responses. Future work will extend its applicability to broader domains and incorporate additional metrics (e.g., conversational coherence, user engagement). This study contributes a structured, scalable approach to evaluating LLM-based conversational agents, facilitating real-world deployment.</li>
</ul>

<h3>Title: CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance</h3>
<ul>
<li><strong>Authors: </strong>Yongchao Chen, Yilun Hao, Yueying Liu, Yang Zhang, Chuchu Fan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SC, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04350">https://arxiv.org/abs/2502.04350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04350">https://arxiv.org/pdf/2502.04350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04350]] CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance(https://arxiv.org/abs/2502.04350)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at this https URL.</li>
</ul>

<h3>Title: Investigating the Robustness of Deductive Reasoning with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fabian Hoppe, Filip Ilievski, Jan-Christoph Kalo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04352">https://arxiv.org/abs/2502.04352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04352">https://arxiv.org/pdf/2502.04352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04352]] Investigating the Robustness of Deductive Reasoning with Large Language Models(https://arxiv.org/abs/2502.04352)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been shown to achieve impressive results for many reasoning-based Natural Language Processing (NLP) tasks, suggesting a degree of deductive reasoning capability. However, it remains unclear to which extent LLMs, in both informal and autoformalisation methods, are robust on logical deduction tasks. Moreover, while many LLM-based deduction methods have been proposed, there is a lack of a systematic study that analyses the impact of their design components. Addressing these two challenges, we propose the first study of the robustness of LLM-based deductive reasoning methods. We devise a framework with two families of perturbations: adversarial noise and counterfactual statements, which jointly generate seven perturbed datasets. We organize the landscape of LLM reasoners according to their reasoning format, formalisation syntax, and feedback for error recovery. The results show that adversarial noise affects autoformalisation, while counterfactual statements influence all approaches. Detailed feedback does not improve overall accuracy despite reducing syntax errors, pointing to the challenge of LLM-based methods to self-correct effectively.</li>
</ul>

<h3>Title: CognArtive: Large Language Models for Automating Art Analysis and Decoding Aesthetic Elements</h3>
<ul>
<li><strong>Authors: </strong>Afshin Khadangi, Amir Sartipi, Igor Tchappi, Gilbert Fridgen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04353">https://arxiv.org/abs/2502.04353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04353">https://arxiv.org/pdf/2502.04353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04353]] CognArtive: Large Language Models for Automating Art Analysis and Decoding Aesthetic Elements(https://arxiv.org/abs/2502.04353)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Art, as a universal language, can be interpreted in diverse ways, with artworks embodying profound meanings and nuances. The advent of Large Language Models (LLMs) and the availability of Multimodal Large Language Models (MLLMs) raise the question of how these transformative models can be used to assess and interpret the artistic elements of artworks. While research has been conducted in this domain, to the best of our knowledge, a deep and detailed understanding of the technical and expressive features of artworks using LLMs has not been explored. In this study, we investigate the automation of a formal art analysis framework to analyze a high-throughput number of artworks rapidly and examine how their patterns evolve over time. We explore how LLMs can decode artistic expressions, visual elements, composition, and techniques, revealing emerging patterns that develop across periods. Finally, we discuss the strengths and limitations of LLMs in this context, emphasizing their ability to process vast quantities of art-related data and generate insightful interpretations. Due to the exhaustive and granular nature of the results, we have developed interactive data visualizations, available online this https URL, to enhance understanding and accessibility.</li>
</ul>

<h3>Title: Reviving The Classics: Active Reward Modeling in Large Language Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yunyi Shen, Hao Sun, Jean-François Ton</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04354">https://arxiv.org/abs/2502.04354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04354">https://arxiv.org/pdf/2502.04354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04354]] Reviving The Classics: Active Reward Modeling in Large Language Model Alignment(https://arxiv.org/abs/2502.04354)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Building neural reward models from human preferences is a pivotal component in reinforcement learning from human feedback (RLHF) and large language model alignment research. Given the scarcity and high cost of human annotation, how to select the most informative pairs to annotate is an essential yet challenging open problem. In this work, we highlight the insight that an ideal comparison dataset for reward modeling should balance exploration of the representation space and make informative comparisons between pairs with moderate reward differences. Technically, challenges arise in quantifying the two objectives and efficiently prioritizing the comparisons to be annotated. To address this, we propose the Fisher information-based selection strategies, adapt theories from the classical experimental design literature, and apply them to the final linear layer of the deep neural network-based reward modeling tasks. Empirically, our method demonstrates remarkable performance, high computational efficiency, and stability compared to other selection methods from deep learning and classical statistical literature across multiple open-source LLMs and datasets. Further ablation studies reveal that incorporating cross-prompt comparisons in active reward modeling significantly enhances labeling efficiency, shedding light on the potential for improved annotation strategies in RLHF.</li>
</ul>

<h3>Title: LLM-ProS: Analyzing Large Language Models' Performance in Competitive Problem Solving</h3>
<ul>
<li><strong>Authors: </strong>Md Sifat Hossain, Anika Tabassum, Md. Fahim Arefin, Tarannum Shaila Zaman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04355">https://arxiv.org/abs/2502.04355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04355">https://arxiv.org/pdf/2502.04355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04355]] LLM-ProS: Analyzing Large Language Models' Performance in Competitive Problem Solving(https://arxiv.org/abs/2502.04355)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models has opened new avenues for automating complex problem-solving tasks such as algorithmic coding and competitive programming. This paper introduces a novel evaluation technique, LLM-ProS, to assess the performance of state-of-the-art LLMs on International Collegiate Programming Contest (ICPC) problems. Using a curated dataset of 166 World Finals problems from 2011 to 2024, we benchmark the models' reasoning, accuracy, and efficiency. We evaluate the five models-GPT-4o, Mistral Large, Llama-3.1-405B, and the o1 family, consisting of o1-mini and o1-preview, across critical metrics like correctness, resource utilization, and response calibration. Our results reveal significant differences in the models' abilities to generalize, adapt, and solve novel problems. We also investigated the impact of training methodologies, dataset contamination, and chain-of-thought reasoning on model performance. The findings provide new insights into optimizing LLMs for algorithmic tasks, highlighting both strengths and limitations of current models.</li>
</ul>

<h3>Title: Open Foundation Models in Healthcare: Challenges, Paradoxes, and Opportunities with GenAI Driven Personalized Prescription</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Alkaeed, Sofiat Abioye, Adnan Qayyum, Yosra Magdi Mekki, Ilhem Berrou, Mohamad Abdallah, Ala Al-Fuqaha, Muhammad Bilal, Junaid Qadir</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04356">https://arxiv.org/abs/2502.04356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04356">https://arxiv.org/pdf/2502.04356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04356]] Open Foundation Models in Healthcare: Challenges, Paradoxes, and Opportunities with GenAI Driven Personalized Prescription(https://arxiv.org/abs/2502.04356)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In response to the success of proprietary Large Language Models (LLMs) such as OpenAI's GPT-4, there is a growing interest in developing open, non-proprietary LLMs and AI foundation models (AIFMs) for transparent use in academic, scientific, and non-commercial applications. Despite their inability to match the refined functionalities of their proprietary counterparts, open models hold immense potential to revolutionize healthcare applications. In this paper, we examine the prospects of open-source LLMs and AIFMs for developing healthcare applications and make two key contributions. Firstly, we present a comprehensive survey of the current state-of-the-art open-source healthcare LLMs and AIFMs and introduce a taxonomy of these open AIFMs, categorizing their utility across various healthcare tasks. Secondly, to evaluate the general-purpose applications of open LLMs in healthcare, we present a case study on personalized prescriptions. This task is particularly significant due to its critical role in delivering tailored, patient-specific medications that can greatly improve treatment outcomes. In addition, we compare the performance of open-source models with proprietary models in settings with and without Retrieval-Augmented Generation (RAG). Our findings suggest that, although less refined, open LLMs can achieve performance comparable to proprietary models when paired with grounding techniques such as RAG. Furthermore, to highlight the clinical significance of LLMs-empowered personalized prescriptions, we perform subjective assessment through an expert clinician. We also elaborate on ethical considerations and potential risks associated with the misuse of powerful LLMs and AIFMs, highlighting the need for a cautious and responsible implementation in healthcare.</li>
</ul>

<h3>Title: Reusing Embeddings: Reproducible Reward Model Research in Large Language Model Alignment without GPUs</h3>
<ul>
<li><strong>Authors: </strong>Hao Sun, Yunyi Shen, Jean-Francois Ton, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04357">https://arxiv.org/abs/2502.04357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04357">https://arxiv.org/pdf/2502.04357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04357]] Reusing Embeddings: Reproducible Reward Model Research in Large Language Model Alignment without GPUs(https://arxiv.org/abs/2502.04357)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have made substantial strides in structured tasks through Reinforcement Learning (RL), demonstrating proficiency in mathematical reasoning and code generation. However, applying RL in broader domains like chatbots and content generation -- through the process known as Reinforcement Learning from Human Feedback (RLHF) -- presents unique challenges. Reward models in RLHF are critical, acting as proxies that evaluate the alignment of LLM outputs with human intent. Despite advancements, the development of reward models is hindered by challenges such as computational heavy training, costly evaluation, and therefore poor reproducibility. We advocate for using embedding-based input in reward model research as an accelerated solution to those challenges. By leveraging embeddings for reward modeling, we can enhance reproducibility, reduce computational demands on hardware, improve training stability, and significantly reduce training and evaluation costs, hence facilitating fair and efficient comparisons in this active research area. We then show a case study of reproducing existing reward model ensemble research using embedding-based reward models. We discussed future avenues for research, aiming to contribute to safer and more effective LLM deployments.</li>
</ul>

<h3>Title: Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives</h3>
<ul>
<li><strong>Authors: </strong>Elliot Meyerson, Xin Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CC, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04358">https://arxiv.org/abs/2502.04358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04358">https://arxiv.org/pdf/2502.04358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04358]] Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives(https://arxiv.org/abs/2502.04358)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team. How close are these role decompositions to optimal? This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems. In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.</li>
</ul>

<h3>Title: MARAGE: Transferable Multi-Model Adversarial Attack for Retrieval-Augmented Generation Data Extraction</h3>
<ul>
<li><strong>Authors: </strong>Xiao Hu, Eric Liu, Weizhou Wang, Xiangyu Guo, David Lie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04360">https://arxiv.org/abs/2502.04360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04360">https://arxiv.org/pdf/2502.04360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04360]] MARAGE: Transferable Multi-Model Adversarial Attack for Retrieval-Augmented Generation Data Extraction(https://arxiv.org/abs/2502.04360)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) offers a solution to mitigate hallucinations in Large Language Models (LLMs) by grounding their outputs to knowledge retrieved from external sources. The use of private resources and data in constructing these external data stores can expose them to risks of extraction attacks, in which attackers attempt to steal data from these private databases. Existing RAG extraction attacks often rely on manually crafted prompts, which limit their effectiveness. In this paper, we introduce a framework called MARAGE for optimizing an adversarial string that, when appended to user queries submitted to a target RAG system, causes outputs containing the retrieved RAG data verbatim. MARAGE leverages a continuous optimization scheme that integrates gradients from multiple models with different architectures simultaneously to enhance the transferability of the optimized string to unseen models. Additionally, we propose a strategy that emphasizes the initial tokens in the target RAG data, further improving the attack's generalizability. Evaluations show that MARAGE consistently outperforms both manual and optimization-based baselines across multiple LLMs and RAG datasets, while maintaining robust transferability to previously unseen models. Moreover, we conduct probing tasks to shed light on the reasons why MARAGE is more effective compared to the baselines and to analyze the impact of our approach on the model's internal state.</li>
</ul>

<h3>Title: Predicting 3D Motion from 2D Video for Behavior-Based VR Biometrics</h3>
<ul>
<li><strong>Authors: </strong>Mingjun Li, Natasha Kholgade Banerjee, Sean Banerjee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04361">https://arxiv.org/abs/2502.04361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04361">https://arxiv.org/pdf/2502.04361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04361]] Predicting 3D Motion from 2D Video for Behavior-Based VR Biometrics(https://arxiv.org/abs/2502.04361)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric, transformer</a></li>
<li><strong>Abstract: </strong>Critical VR applications in domains such as healthcare, education, and finance that use traditional credentials, such as PIN, password, or multi-factor authentication, stand the chance of being compromised if a malicious person acquires the user credentials or if the user hands over their credentials to an ally. Recently, a number of approaches on user authentication have emerged that use motions of VR head-mounted displays (HMDs) and hand controllers during user interactions in VR to represent the user's behavior as a VR biometric signature. One of the fundamental limitations of behavior-based approaches is that current on-device tracking for HMDs and controllers lacks capability to perform tracking of full-body joint articulation, losing key signature data encapsulated by the user articulation. In this paper, we propose an approach that uses 2D body joints, namely shoulder, elbow, wrist, hip, knee, and ankle, acquired from the right side of the participants using an external 2D camera. Using a Transformer-based deep neural network, our method uses the 2D data of body joints that are not tracked by the VR device to predict past and future 3D tracks of the right controller, providing the benefit of augmenting 3D knowledge in authentication. Our approach provides a minimum equal error rate (EER) of 0.025, and a maximum EER drop of 0.040 over prior work that uses single-unit 3D trajectory as the input.</li>
</ul>

<h3>Title: LLMs can be easily Confused by Instructional Distractions</h3>
<ul>
<li><strong>Authors: </strong>Yerin Hwang, Yongil Kim, Jahyun Koo, Taegwan Kang, Hyunkyung Bae, Kyomin Jung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04362">https://arxiv.org/abs/2502.04362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04362">https://arxiv.org/pdf/2502.04362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04362]] LLMs can be easily Confused by Instructional Distractions(https://arxiv.org/abs/2502.04362)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the fact that large language models (LLMs) show exceptional skill in instruction following tasks, this strength can turn into a vulnerability when the models are required to disregard certain instructions. Instruction-following tasks typically involve a clear task description and input text containing the target data to be processed. However, when the input itself resembles an instruction, confusion may arise, even if there is explicit prompting to distinguish between the task instruction and the input. We refer to this phenomenon as instructional distraction. In this paper, we introduce a novel benchmark, named DIM-Bench, specifically designed to assess LLMs' performance under instructional distraction. The benchmark categorizes real-world instances of instructional distraction and evaluates LLMs across four instruction tasks: rewriting, proofreading, translation, and style transfer -- alongside five input tasks: reasoning, code generation, mathematical reasoning, bias detection, and question answering. Our experimental results reveal that even the most advanced LLMs are susceptible to instructional distraction, often failing to accurately follow user intent in such cases.</li>
</ul>

<h3>Title: On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices</h3>
<ul>
<li><strong>Authors: </strong>Bosung Kim, Kyuhwan Lee, Isu Jeong, Jungmin Cheon, Yeojin Lee, Seulki Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04363">https://arxiv.org/abs/2502.04363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04363">https://arxiv.org/pdf/2502.04363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04363]] On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices(https://arxiv.org/abs/2502.04363)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present On-device Sora, a first pioneering solution for diffusion-based on-device text-to-video generation that operates efficiently on smartphone-grade devices. Building on Open-Sora, On-device Sora applies three novel techniques to address the challenges of diffusion-based text-to-video generation on computation- and memory-limited mobile devices. First, Linear Proportional Leap (LPL) reduces the excessive denoising steps required in video diffusion through an efficient leap-based approach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive token-processing computation in attention layers by merging consecutive tokens along the temporal dimension. Third, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions large models into smaller blocks and loads them into memory for concurrent model inference, effectively addressing the challenges of limited device memory. We implement On-device Sora on the iPhone 15 Pro, and the experimental evaluations demonstrate that it is capable of generating high-quality videos on the device, comparable to those produced by Open-Sora running on high-end GPUs. These results show that On-device Sora enables efficient and high-quality video generation on resource-constrained mobile devices, expanding accessibility, ensuring user privacy, reducing dependence on cloud infrastructure, and lowering associated costs. We envision the proposed On-device Sora as a significant first step toward democratizing state-of-the-art generative technologies, enabling video generation capabilities on commodity mobile and embedded devices. The code implementation is publicly available at an GitHub repository: this https URL.</li>
</ul>

<h3>Title: Lost in Edits? A $\lambda$-Compass for AIGC Provenance</h3>
<ul>
<li><strong>Authors: </strong>Wenhao You, Bryan Hooi, Yiwei Wang, Euijin Choo, Ming-Hsuan Yang, Junsong Yuan, Zi Huang, Yujun Cai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04364">https://arxiv.org/abs/2502.04364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04364">https://arxiv.org/pdf/2502.04364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04364]] Lost in Edits? A $\lambda$-Compass for AIGC Provenance(https://arxiv.org/abs/2502.04364)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion models have driven the growth of text-guided image editing tools, enabling precise and iterative modifications of synthesized content. However, as these tools become increasingly accessible, they also introduce significant risks of misuse, emphasizing the critical need for robust attribution methods to ensure content authenticity and traceability. Despite the creative potential of such tools, they pose significant challenges for attribution, particularly in adversarial settings where edits can be layered to obscure an image's origins. We propose LambdaTracer, a novel latent-space attribution method that robustly identifies and differentiates authentic outputs from manipulated ones without requiring any modifications to generative or editing pipelines. By adaptively calibrating reconstruction losses, LambdaTracer remains effective across diverse iterative editing processes, whether automated through text-guided editing tools such as InstructPix2Pix and ControlNet or performed manually with editing software such as Adobe Photoshop. Extensive experiments reveal that our method consistently outperforms baseline approaches in distinguishing maliciously edited images, providing a practical solution to safeguard ownership, creativity, and credibility in the open, fast-evolving AI ecosystems.</li>
</ul>

<h3>Title: AI-Based Thermal Video Analysis in Privacy-Preserving Healthcare: A Case Study on Detecting Time of Birth</h3>
<ul>
<li><strong>Authors: </strong>Jorge García-Torres, Øyvind Meinich-Bache, Siren Rettedal, Kjersti Engan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04365">https://arxiv.org/abs/2502.04365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04365">https://arxiv.org/pdf/2502.04365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04365]] AI-Based Thermal Video Analysis in Privacy-Preserving Healthcare: A Case Study on Detecting Time of Birth(https://arxiv.org/abs/2502.04365)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Approximately 10% of newborns need some assistance to start breathing and 5\% proper ventilation. It is crucial that interventions are initiated as soon as possible after birth. Accurate documentation of Time of Birth (ToB) is thereby essential for documenting and improving newborn resuscitation performance. However, current clinical practices rely on manual recording of ToB, typically with minute precision. In this study, we present an AI-driven, video-based system for automated ToB detection using thermal imaging, designed to preserve the privacy of healthcare providers and mothers by avoiding the use of identifiable visual data. Our approach achieves 91.4% precision and 97.4% recall in detecting ToB within thermal video clips during performance evaluation. Additionally, our system successfully identifies ToB in 96% of test cases with an absolute median deviation of 1 second compared to manual annotations. This method offers a reliable solution for improving ToB documentation and enhancing newborn resuscitation outcomes.</li>
</ul>

<h3>Title: Contrastive Token-level Explanations for Graph-based Rumour Detection</h3>
<ul>
<li><strong>Authors: </strong>Daniel Wai Kit Chin, Roy Ka-Wei Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04366">https://arxiv.org/abs/2502.04366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04366">https://arxiv.org/pdf/2502.04366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04366]] Contrastive Token-level Explanations for Graph-based Rumour Detection(https://arxiv.org/abs/2502.04366)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>The widespread use of social media has accelerated the dissemination of information, but it has also facilitated the spread of harmful rumours, which can disrupt economies, influence political outcomes, and exacerbate public health crises, such as the COVID-19 pandemic. While Graph Neural Network (GNN)-based approaches have shown significant promise in automated rumour detection, they often lack transparency, making their predictions difficult to interpret. Existing graph explainability techniques fall short in addressing the unique challenges posed by the dependencies among feature dimensions in high-dimensional text embeddings used in GNN-based models. In this paper, we introduce Contrastive Token Layerwise Relevance Propagation (CT-LRP), a novel framework designed to enhance the explainability of GNN-based rumour detection. CT-LRP extends current graph explainability methods by providing token-level explanations that offer greater granularity and interpretability. We evaluate the effectiveness of CT-LRP across multiple GNN models trained on three publicly available rumour detection datasets, demonstrating that it consistently produces high-fidelity, meaningful explanations, paving the way for more robust and trustworthy rumour detection systems.</li>
</ul>

<h3>Title: Mining Unstructured Medical Texts With Conformal Active Learning</h3>
<ul>
<li><strong>Authors: </strong>Juliano Genari, Guilherme Tegoni Goedert</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04372">https://arxiv.org/abs/2502.04372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04372">https://arxiv.org/pdf/2502.04372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04372]] Mining Unstructured Medical Texts With Conformal Active Learning(https://arxiv.org/abs/2502.04372)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction</a></li>
<li><strong>Abstract: </strong>The extraction of relevant data from Electronic Health Records (EHRs) is crucial to identifying symptoms and automating epidemiological surveillance processes. By harnessing the vast amount of unstructured text in EHRs, we can detect patterns that indicate the onset of disease outbreaks, enabling faster, more targeted public health responses. Our proposed framework provides a flexible and efficient solution for mining data from unstructured texts, significantly reducing the need for extensive manual labeling by specialists. Experiments show that our framework achieving strong performance with as few as 200 manually labeled texts, even for complex classification problems. Additionally, our approach can function with simple lightweight models, achieving competitive and occasionally even better results compared to more resource-intensive deep learning models. This capability not only accelerates processing times but also preserves patient privacy, as the data can be processed on weaker on-site hardware rather than being transferred to external systems. Our methodology, therefore, offers a practical, scalable, and privacy-conscious approach to real-time epidemiological monitoring, equipping health institutions to respond rapidly and effectively to emerging health threats.</li>
</ul>

<h3>Title: An Analysis for Reasoning Bias of Language Models with Small Initialization</h3>
<ul>
<li><strong>Authors: </strong>Junjie Yao, Zhongwang Zhang, Zhi-Qin John Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04375">https://arxiv.org/abs/2502.04375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04375">https://arxiv.org/pdf/2502.04375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04375]] An Analysis for Reasoning Bias of Language Models with Small Initialization(https://arxiv.org/abs/2502.04375)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based Large Language Models (LLMs) have revolutionized Natural Language Processing by demonstrating exceptional performance across diverse tasks. This study investigates the impact of the parameter initialization scale on the training behavior and task preferences of LLMs. We discover that smaller initialization scales encourage models to favor reasoning tasks, whereas larger initialization scales lead to a preference for memorization tasks. We validate this reasoning bias via real datasets and meticulously designed anchor functions. Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena. Additionally, experiments on real-world language tasks corroborate our theoretical insights. This work enhances our understanding of how initialization strategies influence LLM performance on reasoning tasks and offers valuable guidelines for training models.</li>
</ul>

<h3>Title: MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf</h3>
<ul>
<li><strong>Authors: </strong>Lingxiang Hu, Shurun Yuan, Xiaoting Qin, Jue Zhang, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04376">https://arxiv.org/abs/2502.04376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04376">https://arxiv.org/pdf/2502.04376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04376]] MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf(https://arxiv.org/abs/2502.04376)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In contemporary workplaces, meetings are essential for exchanging ideas and ensuring team alignment but often face challenges such as time consumption, scheduling conflicts, and inefficient participation. Recent advancements in Large Language Models (LLMs) have demonstrated their strong capabilities in natural language generation and reasoning, prompting the question: can LLMs effectively delegate participants in meetings? To explore this, we develop a prototype LLM-powered meeting delegate system and create a comprehensive benchmark using real meeting transcripts. Our evaluation reveals that GPT-4/4o maintain balanced performance between active and cautious engagement strategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini 1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60\% of responses address at least one key point from the ground-truth. However, improvements are needed to reduce irrelevant or repetitive content and enhance tolerance for transcription errors commonly found in real-world settings. Additionally, we implement the system in practical settings and collect real-world feedback from demos. Our findings underscore the potential and challenges of utilizing LLMs as meeting delegates, offering valuable insights into their practical application for alleviating the burden of meetings.</li>
</ul>

<h3>Title: MapFusion: A Novel BEV Feature Fusion Network for Multi-modal Map Construction</h3>
<ul>
<li><strong>Authors: </strong>Xiaoshuai Hao, Yunfeng Diao, Mengchuan Wei, Yifan Yang, Peng Hao, Rong Yin, Hui Zhang, Weiming Li, Shu Zhao, Yu Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04377">https://arxiv.org/abs/2502.04377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04377">https://arxiv.org/pdf/2502.04377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04377]] MapFusion: A Novel BEV Feature Fusion Network for Multi-modal Map Construction(https://arxiv.org/abs/2502.04377)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Map construction task plays a vital role in providing precise and comprehensive static environmental information essential for autonomous driving systems. Primary sensors include cameras and LiDAR, with configurations varying between camera-only, LiDAR-only, or camera-LiDAR fusion, based on cost-performance considerations. While fusion-based methods typically perform best, existing approaches often neglect modality interaction and rely on simple fusion strategies, which suffer from the problems of misalignment and information loss. To address these issues, we propose MapFusion, a novel multi-modal Bird's-Eye View (BEV) feature fusion method for map construction. Specifically, to solve the semantic misalignment problem between camera and LiDAR BEV features, we introduce the Cross-modal Interaction Transform (CIT) module, enabling interaction between two BEV feature spaces and enhancing feature representation through a self-attention mechanism. Additionally, we propose an effective Dual Dynamic Fusion (DDF) module to adaptively select valuable information from different modalities, which can take full advantage of the inherent information between different modalities. Moreover, MapFusion is designed to be simple and plug-and-play, easily integrated into existing pipelines. We evaluate MapFusion on two map construction tasks, including High-definition (HD) map and BEV map segmentation, to show its versatility and effectiveness. Compared with the state-of-the-art methods, MapFusion achieves 3.6% and 6.2% absolute improvements on the HD map construction and BEV map segmentation tasks on the nuScenes dataset, respectively, demonstrating the superiority of our approach.</li>
</ul>

<h3>Title: DILLEMA: Diffusion and Large Language Models for Multi-Modal Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Luciano Baresi, Davide Yi Xian Hu, Muhammad Irfan Mas'udi, Giovanni Quattrocchi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04378">https://arxiv.org/abs/2502.04378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04378">https://arxiv.org/pdf/2502.04378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04378]] DILLEMA: Diffusion and Large Language Models for Multi-Modal Augmentation(https://arxiv.org/abs/2502.04378)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Ensuring the robustness of deep learning models requires comprehensive and diverse testing. Existing approaches, often based on simple data augmentation techniques or generative adversarial networks, are limited in producing realistic and varied test cases. To address these limitations, we present a novel framework for testing vision neural networks that leverages Large Language Models and control-conditioned Diffusion Models to generate synthetic, high-fidelity test cases. Our approach begins by translating images into detailed textual descriptions using a captioning model, allowing the language model to identify modifiable aspects of the image and generate counterfactual descriptions. These descriptions are then used to produce new test images through a text-to-image diffusion process that preserves spatial consistency and maintains the critical elements of the scene. We demonstrate the effectiveness of our method using two datasets: ImageNet1K for image classification and SHIFT for semantic segmentation in autonomous driving. The results show that our approach can generate significant test cases that reveal weaknesses and improve the robustness of the model through targeted retraining. We conducted a human assessment using Mechanical Turk to validate the generated images. The responses from the participants confirmed, with high agreement among the voters, that our approach produces valid and realistic images.</li>
</ul>

<h3>Title: Can Large Language Models Capture Video Game Engagement?</h3>
<ul>
<li><strong>Authors: </strong>David Melhart, Matthew Barthet, Georgios N. Yannakakis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04379">https://arxiv.org/abs/2502.04379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04379">https://arxiv.org/pdf/2502.04379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04379]] Can Large Language Models Capture Video Game Engagement?(https://arxiv.org/abs/2502.04379)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Can out-of-the-box pretrained Large Language Models (LLMs) detect human affect successfully when observing a video? To address this question, for the first time, we evaluate comprehensively the capacity of popular LLMs to annotate and successfully predict continuous affect annotations of videos when prompted by a sequence of text and video frames in a multimodal fashion. Particularly in this paper, we test LLMs' ability to correctly label changes of in-game engagement in 80 minutes of annotated videogame footage from 20 first-person shooter games of the GameVibe corpus. We run over 2,400 experiments to investigate the impact of LLM architecture, model size, input modality, prompting strategy, and ground truth processing method on engagement prediction. Our findings suggest that while LLMs rightfully claim human-like performance across multiple domains, they generally fall behind capturing continuous experience annotations provided by humans. We examine some of the underlying causes for the relatively poor overall performance, highlight the cases where LLMs exceed expectations, and draw a roadmap for the further exploration of automated emotion labelling via LLMs.</li>
</ul>

<h3>Title: Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data</h3>
<ul>
<li><strong>Authors: </strong>Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, Ying Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04380">https://arxiv.org/abs/2502.04380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04380">https://arxiv.org/pdf/2502.04380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04380]] Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data(https://arxiv.org/abs/2502.04380)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) using diverse datasets is crucial for enhancing their overall performance across various domains. In practical scenarios, existing methods based on modeling the mixture proportions of data composition often struggle with data whose domain labels are missing, imprecise or non-normalized, while methods based on data selection usually encounter difficulties in balancing multi-domain performance. To address these challenges, in this paper, we study the role of data diversity in enhancing the overall abilities of LLMs by empirically constructing contrastive data pools and theoretically deriving explanations for both inter- and intra-diversity. Building upon the insights gained, we propose a new method that gives the LLM a dual identity: an output model to cognitively probe and select data based on diversity reward, as well as an input model to be tuned with the selected data. Extensive experiments show that the proposed method notably boosts performance across domain-undetermined data and a series of foundational downstream tasks when applied to various advanced LLMs. We release our code and hope this study can shed light on the understanding of data diversity and advance feedback-driven data-model co-development for LLMs.</li>
</ul>

<h3>Title: Limitations of Large Language Models in Clinical Problem-Solving Arising from Inflexible Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Kim, Anna Podlasek, Kie Shidara, Feng Liu, Ahmed Alaa, Danilo Bernardo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04381">https://arxiv.org/abs/2502.04381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04381">https://arxiv.org/pdf/2502.04381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04381]] Limitations of Large Language Models in Clinical Problem-Solving Arising from Inflexible Reasoning(https://arxiv.org/abs/2502.04381)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have attained human-level accuracy on medical question-answer (QA) benchmarks. However, their limitations in navigating open-ended clinical scenarios have recently been shown, raising concerns about the robustness and generalizability of LLM reasoning across diverse, real-world medical tasks. To probe potential LLM failure modes in clinical problem-solving, we present the medical abstraction and reasoning corpus (M-ARC). M-ARC assesses clinical reasoning through scenarios designed to exploit the Einstellung effect -- the fixation of thought arising from prior experience, targeting LLM inductive biases toward inflexible pattern matching from their training data rather than engaging in flexible reasoning. We find that LLMs, including current state-of-the-art o1 and Gemini models, perform poorly compared to physicians on M-ARC, often demonstrating lack of commonsense medical reasoning and a propensity to hallucinate. In addition, uncertainty estimation analyses indicate that LLMs exhibit overconfidence in their answers, despite their limited accuracy. The failure modes revealed by M-ARC in LLM medical reasoning underscore the need to exercise caution when deploying these models in clinical settings.</li>
</ul>

<h3>Title: Enhancing Reasoning to Adapt Large Language Models for Domain-Specific Applications</h3>
<ul>
<li><strong>Authors: </strong>Bo Wen, Xin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04384">https://arxiv.org/abs/2502.04384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04384">https://arxiv.org/pdf/2502.04384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04384]] Enhancing Reasoning to Adapt Large Language Models for Domain-Specific Applications(https://arxiv.org/abs/2502.04384)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents SOLOMON, a novel Neuro-inspired Large Language Model (LLM) Reasoning Network architecture that enhances the adaptability of foundation models for domain-specific applications. Through a case study in semiconductor layout design, we demonstrate how SOLOMON enables swift adaptation of general-purpose LLMs to specialized tasks by leveraging Prompt Engineering and In-Context Learning techniques. Our experiments reveal the challenges LLMs face in spatial reasoning and applying domain knowledge to practical problems. Results show that SOLOMON instances significantly outperform their baseline LLM counterparts and achieve performance comparable to state-of-the-art reasoning model, o1-preview. We discuss future research directions for developing more adaptive AI systems that can continually learn, adapt, and evolve in response to new information and changing requirements.</li>
</ul>

<h3>Title: TexLiDAR: Automated Text Understanding for Panoramic LiDAR Data</h3>
<ul>
<li><strong>Authors: </strong>Naor Cohen, Roy Orfaig, Ben-Zion Bobrovsky</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04385">https://arxiv.org/abs/2502.04385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04385">https://arxiv.org/pdf/2502.04385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04385]] TexLiDAR: Automated Text Understanding for Panoramic LiDAR Data(https://arxiv.org/abs/2502.04385)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Efforts to connect LiDAR data with text, such as LidarCLIP, have primarily focused on embedding 3D point clouds into CLIP text-image space. However, these approaches rely on 3D point clouds, which present challenges in encoding efficiency and neural network processing. With the advent of advanced LiDAR sensors like Ouster OS1, which, in addition to 3D point clouds, produce fixed resolution depth, signal, and ambient panoramic 2D images, new opportunities emerge for LiDAR based tasks. In this work, we propose an alternative approach to connect LiDAR data with text by leveraging 2D imagery generated by the OS1 sensor instead of 3D point clouds. Using the Florence 2 large model in a zero-shot setting, we perform image captioning and object detection. Our experiments demonstrate that Florence 2 generates more informative captions and achieves superior performance in object detection tasks compared to existing methods like CLIP. By combining advanced LiDAR sensor data with a large pre-trained model, our approach provides a robust and accurate solution for challenging detection scenarios, including real-time applications requiring high accuracy and robustness.</li>
</ul>

<h3>Title: Towards Fair Medical AI: Adversarial Debiasing of 3D CT Foundation Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Guangyao Zheng, Michael A. Jacobs, Vladimir Braverman, Vishwa S. Parekh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04386">https://arxiv.org/abs/2502.04386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04386">https://arxiv.org/pdf/2502.04386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04386]] Towards Fair Medical AI: Adversarial Debiasing of 3D CT Foundation Embeddings(https://arxiv.org/abs/2502.04386)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, extraction, fair</a></li>
<li><strong>Abstract: </strong>Self-supervised learning has revolutionized medical imaging by enabling efficient and generalizable feature extraction from large-scale unlabeled datasets. Recently, self-supervised foundation models have been extended to three-dimensional (3D) computed tomography (CT) data, generating compact, information-rich embeddings with 1408 features that achieve state-of-the-art performance on downstream tasks such as intracranial hemorrhage detection and lung cancer risk forecasting. However, these embeddings have been shown to encode demographic information, such as age, sex, and race, which poses a significant risk to the fairness of clinical applications. In this work, we propose a Variation Autoencoder (VAE) based adversarial debiasing framework to transform these embeddings into a new latent space where demographic information is no longer encoded, while maintaining the performance of critical downstream tasks. We validated our approach on the NLST lung cancer screening dataset, demonstrating that the debiased embeddings effectively eliminate multiple encoded demographic information and improve fairness without compromising predictive accuracy for lung cancer risk at 1-year and 2-year intervals. Additionally, our approach ensures the embeddings are robust against adversarial bias attacks. These results highlight the potential of adversarial debiasing techniques to ensure fairness and equity in clinical applications of self-supervised 3D CT embeddings, paving the way for their broader adoption in unbiased medical decision-making.</li>
</ul>

<h3>Title: FedP$^2$EFT: Federated Learning to Personalize Parameter Efficient Fine-Tuning for Multilingual LLMs</h3>
<ul>
<li><strong>Authors: </strong>Royson Lee, Minyoung Kim, Fady Rezk, Rui Li, Stylianos I. Venieris, Timothy Hospedales</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04387">https://arxiv.org/abs/2502.04387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04387">https://arxiv.org/pdf/2502.04387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04387]] FedP$^2$EFT: Federated Learning to Personalize Parameter Efficient Fine-Tuning for Multilingual LLMs(https://arxiv.org/abs/2502.04387)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, large language model</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) has enabled the training of multilingual large language models (LLMs) on diverse and decentralized multilingual data, especially on low-resource languages. To improve client-specific performance, personalization via the use of parameter-efficient fine-tuning (PEFT) modules such as LoRA is common. This involves a personalization strategy (PS), such as the design of the PEFT adapter structures (e.g., in which layers to add LoRAs and what ranks) and choice of hyperparameters (e.g., learning rates) for fine-tuning. Instead of manual PS configuration, we propose FedP$^2$EFT, a federated learning-to-personalize method for multilingual LLMs in cross-device FL settings. Unlike most existing PEFT structure selection methods, which are prone to overfitting low-data regimes, FedP$^2$EFT collaboratively learns the optimal personalized PEFT structure for each client via Bayesian sparse rank selection. Evaluations on both simulated and real-world multilingual FL benchmarks demonstrate that FedP$^2$EFT largely outperforms existing personalized fine-tuning methods, while complementing a range of existing FL methods.</li>
</ul>

<h3>Title: In Praise of Stubbornness: The Case for Cognitive-Dissonance-Aware Knowledge Updates in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Simone Clemente, Zied Ben Houidi, Alexis Huet, Dario Rossi, Giulio Franzese, Pietro Michiardi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04390">https://arxiv.org/abs/2502.04390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04390">https://arxiv.org/pdf/2502.04390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04390]] In Praise of Stubbornness: The Case for Cognitive-Dissonance-Aware Knowledge Updates in LLMs(https://arxiv.org/abs/2502.04390)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Despite remarkable capabilities, large language models (LLMs) struggle to continually update their knowledge without catastrophic forgetting. In contrast, humans effortlessly integrate new information, detect conflicts with existing beliefs, and selectively update their mental models. This paper introduces a cognitive-inspired investigation paradigm to study continual knowledge updating in LLMs. We implement two key components inspired by human cognition: (1) Dissonance and Familiarity Awareness, analyzing model behavior to classify information as novel, familiar, or dissonant; and (2) Targeted Network Updates, which track neural activity to identify frequently used (stubborn) and rarely used (plastic) neurons. Through carefully designed experiments in controlled settings, we uncover a number of empirical findings demonstrating the potential of this approach. First, dissonance detection is feasible using simple activation and gradient features, suggesting potential for cognitive-inspired training. Second, we find that non-dissonant updates largely preserve prior knowledge regardless of targeting strategy, revealing inherent robustness in LLM knowledge integration. Most critically, we discover that dissonant updates prove catastrophically destructive to the model's knowledge base, indiscriminately affecting even information unrelated to the current updates. This suggests fundamental limitations in how neural networks handle contradictions and motivates the need for new approaches to knowledge updating that better mirror human cognitive mechanisms.</li>
</ul>

<h3>Title: Towards Fair and Robust Face Parsing for Generative AI: A Multi-Objective Approach</h3>
<ul>
<li><strong>Authors: </strong>Sophia J. Abraham, Jonathan D. Hauenstein, Walter J. Scheirer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04391">https://arxiv.org/abs/2502.04391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04391">https://arxiv.org/pdf/2502.04391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04391]] Towards Fair and Robust Face Parsing for Generative AI: A Multi-Objective Approach(https://arxiv.org/abs/2502.04391)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Face parsing is a fundamental task in computer vision, enabling applications such as identity verification, facial editing, and controllable image synthesis. However, existing face parsing models often lack fairness and robustness, leading to biased segmentation across demographic groups and errors under occlusions, noise, and domain shifts. These limitations affect downstream face synthesis, where segmentation biases can degrade generative model outputs. We propose a multi-objective learning framework that optimizes accuracy, fairness, and robustness in face parsing. Our approach introduces a homotopy-based loss function that dynamically adjusts the importance of these objectives during training. To evaluate its impact, we compare multi-objective and single-objective U-Net models in a GAN-based face synthesis pipeline (Pix2PixHD). Our results show that fairness-aware and robust segmentation improves photorealism and consistency in face generation. Additionally, we conduct preliminary experiments using ControlNet, a structured conditioning model for diffusion-based synthesis, to explore how segmentation quality influences guided image generation. Our findings demonstrate that multi-objective face parsing improves demographic consistency and robustness, leading to higher-quality GAN-based synthesis.</li>
</ul>

<h3>Title: Division-of-Thoughts: Harnessing Hybrid Language Model Synergy for Efficient On-Device Agents</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Shao, Xinyuan Hu, Yutang Lin, Fengli Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04392">https://arxiv.org/abs/2502.04392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04392">https://arxiv.org/pdf/2502.04392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04392]] Division-of-Thoughts: Harnessing Hybrid Language Model Synergy for Efficient On-Device Agents(https://arxiv.org/abs/2502.04392)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid expansion of web content has made on-device AI assistants indispensable for helping users manage the increasing complexity of online tasks. The emergent reasoning ability in large language models offer a promising path for next-generation on-device AI agents. However, deploying full-scale Large Language Models (LLMs) on resource-limited local devices is challenging. In this paper, we propose Division-of-Thoughts (DoT), a collaborative reasoning framework leveraging the synergy between locally deployed Smaller-scale Language Models (SLMs) and cloud-based LLMs. DoT leverages a Task Decomposer to elicit the inherent planning abilities in language models to decompose user queries into smaller sub-tasks, which allows hybrid language models to fully exploit their respective strengths. Besides, DoT employs a Task Scheduler to analyze the pair-wise dependency of sub-tasks and create a dependency graph, facilitating parallel reasoning of sub-tasks and the identification of key steps. To allocate the appropriate model based on the difficulty of sub-tasks, DoT leverages a Plug-and-Play Adapter, which is an additional task head attached to the SLM that does not alter the SLM's parameters. To boost adapter's task allocation capability, we propose a self-reinforced training method that relies solely on task execution feedback. Extensive experiments on various benchmarks demonstrate that our DoT significantly reduces LLM costs while maintaining competitive reasoning accuracy. Specifically, DoT reduces the average reasoning time and API costs by 66.12% and 83.57%, while achieving comparable reasoning accuracy with the best baseline methods.</li>
</ul>

<h3>Title: UniCP: A Unified Caching and Pruning Framework for Efficient Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Wenzhang Sun, Qirui Hou, Donglin Di, Jiahui Yang, Yongjia Ma, Jianxun Cui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04393">https://arxiv.org/abs/2502.04393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04393">https://arxiv.org/pdf/2502.04393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04393]] UniCP: A Unified Caching and Pruning Framework for Efficient Video Generation(https://arxiv.org/abs/2502.04393)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiT) excel in video generation but encounter significant computational challenges due to the quadratic complexity of attention. Notably, attention differences between adjacent diffusion steps follow a U-shaped pattern. Current methods leverage this property by caching attention blocks, however, they still struggle with sudden error spikes and large discrepancies. To address these issues, we propose UniCP a unified caching and pruning framework for efficient video generation. UniCP optimizes both temporal and spatial dimensions through. Error Aware Dynamic Cache Window (EDCW): Dynamically adjusts cache window sizes for different blocks at various timesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and Dynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS integrates caching and pruning by enabling dynamic switching between pruned and cached outputs. By adjusting cache windows and pruning redundant components, UniCP enhances computational efficiency and maintains video detail fidelity. Experimental results show that UniCP outperforms existing methods in both performance and efficiency.</li>
</ul>

<h3>Title: DECT: Harnessing LLM-assisted Fine-Grained Linguistic Knowledge and Label-Switched and Label-Preserved Data Generation for Diagnosis of Alzheimer's Disease</h3>
<ul>
<li><strong>Authors: </strong>Tingyu Mo, Jacqueline C. K. Lam, Victor O.K. Li, Lawrence Y. L. Cheung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04394">https://arxiv.org/abs/2502.04394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04394">https://arxiv.org/pdf/2502.04394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04394]] DECT: Harnessing LLM-assisted Fine-Grained Linguistic Knowledge and Label-Switched and Label-Preserved Data Generation for Diagnosis of Alzheimer's Disease(https://arxiv.org/abs/2502.04394)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Alzheimer's Disease (AD) is an irreversible neurodegenerative disease affecting 50 million people worldwide. Low-cost, accurate identification of key markers of AD is crucial for timely diagnosis and intervention. Language impairment is one of the earliest signs of cognitive decline, which can be used to discriminate AD patients from normal control individuals. Patient-interviewer dialogues may be used to detect such impairments, but they are often mixed with ambiguous, noisy, and irrelevant information, making the AD detection task difficult. Moreover, the limited availability of AD speech samples and variability in their speech styles pose significant challenges in developing robust speech-based AD detection models. To address these challenges, we propose DECT, a novel speech-based domain-specific approach leveraging large language models (LLMs) for fine-grained linguistic analysis and label-switched label-preserved data generation. Our study presents four novelties: We harness the summarizing capabilities of LLMs to identify and distill key Cognitive-Linguistic information from noisy speech transcripts, effectively filtering irrelevant information. We leverage the inherent linguistic knowledge of LLMs to extract linguistic markers from unstructured and heterogeneous audio transcripts. We exploit the compositional ability of LLMs to generate AD speech transcripts consisting of diverse linguistic patterns to overcome the speech data scarcity challenge and enhance the robustness of AD detection models. We use the augmented AD textual speech transcript dataset and a more fine-grained representation of AD textual speech transcript data to fine-tune the AD detection model. The results have shown that DECT demonstrates superior model performance with an 11% improvement in AD detection accuracy on the datasets from DementiaBank compared to the baselines.</li>
</ul>

<h3>Title: Adaptive Prototype Knowledge Transfer for Federated Learning with Mixed Modalities and Heterogeneous Tasks</h3>
<ul>
<li><strong>Authors: </strong>Keke Gai, Mohan Wang, Jing Yu, Dongjue Wang, Qi Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04400">https://arxiv.org/abs/2502.04400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04400">https://arxiv.org/pdf/2502.04400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04400]] Adaptive Prototype Knowledge Transfer for Federated Learning with Mixed Modalities and Heterogeneous Tasks(https://arxiv.org/abs/2502.04400)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Multimodal Federated Learning (MFL) enables multiple clients to collaboratively train models on multimodal data while ensuring clients' privacy. However, modality and task heterogeneity hinder clients from learning a unified representation, weakening local model generalization, especially in MFL with mixed modalities where only some clients have multimodal data. In this work, we propose an Adaptive prototype-based Multimodal Federated Learning (AproMFL) framework for mixed modalities and heterogeneous tasks to address the aforementioned issues. Our AproMFL transfers knowledge through adaptively-constructed prototypes without a prior public dataset. Clients adaptively select prototype construction methods in line with tasks; server converts client prototypes into unified multimodal prototypes and aggregates them to form global prototypes, avoid clients keeping unified labels. We divide the model into various modules and only aggregate mapping modules to reduce communication and computation overhead. To address aggregation issues in heterogeneity, we develop a client relationship graph-based scheme to dynamically adjust aggregation weights. Extensive experiments on representative datasets evidence effectiveness of AproMFL.</li>
</ul>

<h3>Title: Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiao-Wen Yang, Xuan-Yi Zhu, Wen-Da Wei, Ding-Chu Zhang, Jie-Jing Shao, Zhi Zhou, Lan-Zhe Guo, Yu-Feng Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04404">https://arxiv.org/abs/2502.04404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04404">https://arxiv.org/pdf/2502.04404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04404]] Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models(https://arxiv.org/abs/2502.04404)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners.</li>
</ul>

<h3>Title: FAS: Fast ANN-SNN Conversion for Spiking Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Long Chen, Xiaotian Song, Andy Song, BaDong Chen, Jiancheng Lv, Yanan Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04405">https://arxiv.org/abs/2502.04405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04405">https://arxiv.org/pdf/2502.04405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04405]] FAS: Fast ANN-SNN Conversion for Spiking Large Language Models(https://arxiv.org/abs/2502.04405)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Spiking Large Language Models have been shown as a good alternative to LLMs in various scenarios. Existing methods for creating Spiking LLMs, i.e., direct training and ANN-SNN conversion, often suffer from performance degradation and relatively high computational costs. To address these issues, we propose a novel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking LLMs in two stages. The first stage employs a full-parameter fine-tuning of pre-trained models, so it does not need any direct training from scratch. The second stage introduces a coarse-to-fine calibration method to reduce conversion errors and improve accuracy. Our experiments on both language and vision-language tasks across four different scales of LLMs demonstrate that FAS can achieve state-of-the-art performance yet with significantly reduced inference latency and computational costs. For example, FAS only takes 8 timesteps to achieve an accuracy of 3% higher than that of the OPT-7B model, while reducing energy consumption by 96.63%.</li>
</ul>

<h3>Title: Calibrated Physics-Informed Uncertainty Quantification</h3>
<ul>
<li><strong>Authors: </strong>Vignesh Gopakumar, Ander Gray, Lorenzo Zanisi, Timothy Nunn, Stanislas Pamela, Daniel Giles, Matt J. Kusner, Marc Peter Deisenroth</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04406">https://arxiv.org/abs/2502.04406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04406">https://arxiv.org/pdf/2502.04406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04406]] Calibrated Physics-Informed Uncertainty Quantification(https://arxiv.org/abs/2502.04406)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, data-free</a></li>
<li><strong>Abstract: </strong>Neural PDEs offer efficient alternatives to computationally expensive numerical PDE solvers for simulating complex physical systems. However, their lack of robust uncertainty quantification (UQ) limits deployment in critical applications. We introduce a model-agnostic, physics-informed conformal prediction (CP) framework that provides guaranteed uncertainty estimates without requiring labelled data. By utilising a physics-based approach, we are able to quantify and calibrate the model's inconsistencies with the PDE rather than the uncertainty arising from the data. Our approach uses convolutional layers as finite-difference stencils and leverages physics residual errors as nonconformity scores, enabling data-free UQ with marginal and joint coverage guarantees across prediction domains for a range of complex PDEs. We further validate the efficacy of our method on neural PDE models for plasma modelling and shot design in fusion reactors.</li>
</ul>

<h3>Title: Illuminating Spaces: Deep Reinforcement Learning and Laser-Wall Partitioning for Architectural Layout Generation</h3>
<ul>
<li><strong>Authors: </strong>Reza Kakooee, Benjamin Dillenburger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04407">https://arxiv.org/abs/2502.04407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04407">https://arxiv.org/pdf/2502.04407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04407]] Illuminating Spaces: Deep Reinforcement Learning and Laser-Wall Partitioning for Architectural Layout Generation(https://arxiv.org/abs/2502.04407)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Space layout design (SLD), occurring in the early stages of the design process, nonetheless influences both the functionality and aesthetics of the ultimate architectural outcome. The complexity of SLD necessitates innovative approaches to efficiently explore vast solution spaces. While image-based generative AI has emerged as a potential solution, they often rely on pixel-based space composition methods that lack intuitive representation of architectural processes. This paper leverages deep Reinforcement Learning (RL), as it offers a procedural approach that intuitively mimics the process of human designers. Effectively using RL for SLD requires an explorative space composing method to generate desirable design solutions. We introduce "laser-wall", a novel space partitioning method that conceptualizes walls as emitters of imaginary light beams to partition spaces. This approach bridges vector-based and pixel-based partitioning methods, offering both flexibility and exploratory power in generating diverse layouts. We present two planning strategies: one-shot planning, which generates entire layouts in a single pass, and dynamic planning, which allows for adaptive refinement by continuously transforming laser-walls. Additionally, we introduce on-light and off-light wall transformations for smooth and fast layout refinement, as well as identity-less and identity-full walls for versatile room assignment. We developed SpaceLayoutGym, an open-source OpenAI Gym compatible simulator for generating and evaluating space layouts. The RL agent processes the input design scenarios and generates solutions following a reward function that balances geometrical and topological requirements. Our results demonstrate that the RL-based laser-wall approach can generate diverse and functional space layouts that satisfy both geometric constraints and topological requirements and is architecturally intuitive.</li>
</ul>

<h3>Title: Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing</h3>
<ul>
<li><strong>Authors: </strong>Kunfeng Lai, Zhenheng Tang, Xinglin Pan, Peijie Dong, Xiang Liu, Haolan Chen, Li Shen, Bo Li, Xiaowen Chu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04411">https://arxiv.org/abs/2502.04411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04411">https://arxiv.org/pdf/2502.04411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04411]] Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing(https://arxiv.org/abs/2502.04411)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Model merging aggregates Large Language Models (LLMs) finetuned on different tasks into a stronger one. However, parameter conflicts between models leads to performance degradation in averaging. While model routing addresses this issue by selecting individual models during inference, it imposes excessive storage and compute costs, and fails to leverage the common knowledge from different models. In this work, we observe that different layers exhibit varying levels of parameter conflicts. Building on this insight, we average layers with minimal parameter conflicts and use a novel task-level expert routing for layers with significant conflicts. To further reduce storage costs, inspired by task arithmetic sparsity, we decouple multiple fine-tuned experts into a dense expert and several sparse experts. Considering the out-of-distribution samples, we select and merge appropriate experts based on the task uncertainty of the input data. We conduct extensive experiments on both LLaMA and Qwen with varying parameter scales, and evaluate on real-world reasoning tasks. Results demonstrate that our method consistently achieves significant performance improvements while requiring less system cost compared to existing methods.</li>
</ul>

<h3>Title: Decoder-Only LLMs are Better Controllers for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Dong, Yao Xiao, Pengxu Wei, Liang Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04412">https://arxiv.org/abs/2502.04412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04412">https://arxiv.org/pdf/2502.04412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04412]] Decoder-Only LLMs are Better Controllers for Diffusion Models(https://arxiv.org/abs/2502.04412)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Groundbreaking advancements in text-to-image generation have recently been achieved with the emergence of diffusion models. These models exhibit a remarkable ability to generate highly artistic and intricately detailed images based on textual prompts. However, obtaining desired generation outcomes often necessitates repetitive trials of manipulating text prompts just like casting spells on a magic mirror, and the reason behind that is the limited capability of semantic understanding inherent in current image generation models. Specifically, existing diffusion models encode the text prompt input with a pre-trained encoder structure, which is usually trained on a limited number of image-caption pairs. The state-of-the-art large language models (LLMs) based on the decoder-only structure have shown a powerful semantic understanding capability as their architectures are more suitable for training on very large-scale unlabeled data. In this work, we propose to enhance text-to-image diffusion models by borrowing the strength of semantic understanding from large language models, and devise a simple yet effective adapter to allow the diffusion models to be compatible with the decoder-only structure. Meanwhile, we also provide a supporting theoretical analysis with various architectures (e.g., encoder-only, encoder-decoder, and decoder-only), and conduct extensive empirical evaluations to verify its effectiveness. The experimental results show that the enhanced models with our adapter module are superior to the stat-of-the-art models in terms of text-to-image generation quality and reliability.</li>
</ul>

<h3>Title: MedRAG: Enhancing Retrieval-augmented Generation with Knowledge Graph-Elicited Reasoning for Healthcare Copilot</h3>
<ul>
<li><strong>Authors: </strong>Xuejiao Zhao, Siyan Liu, Su-Yin Yang, Chunyan Miao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04413">https://arxiv.org/abs/2502.04413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04413">https://arxiv.org/pdf/2502.04413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04413]] MedRAG: Enhancing Retrieval-augmented Generation with Knowledge Graph-Elicited Reasoning for Healthcare Copilot(https://arxiv.org/abs/2502.04413)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) is a well-suited technique for retrieving privacy-sensitive Electronic Health Records (EHR). It can serve as a key module of the healthcare copilot, helping reduce misdiagnosis for healthcare practitioners and patients. However, the diagnostic accuracy and specificity of existing heuristic-based RAG models used in the medical domain are inadequate, particularly for diseases with similar manifestations. This paper proposes MedRAG, a RAG model enhanced by knowledge graph (KG)-elicited reasoning for the medical domain that retrieves diagnosis and treatment recommendations based on manifestations. MedRAG systematically constructs a comprehensive four-tier hierarchical diagnostic KG encompassing critical diagnostic differences of various diseases. These differences are dynamically integrated with similar EHRs retrieved from an EHR database, and reasoned within a large language model. This process enables more accurate and specific decision support, while also proactively providing follow-up questions to enhance personalized medical decision-making. MedRAG is evaluated on both a public dataset DDXPlus and a private chronic pain diagnostic dataset (CPDD) collected from Tan Tock Seng Hospital, and its performance is compared against various existing RAG methods. Experimental results show that, leveraging the information integration and relational abilities of the KG, our MedRAG provides more specific diagnostic insights and outperforms state-of-the-art models in reducing misdiagnosis rates. Our code will be available at this https URL</li>
</ul>

<h3>Title: CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Zehua Pei, Lancheng Zou, Hui-Ling Zhen, Xianzhi Yu, Wulong Liu, Sinno Jialin Pan, Mingxuan Yuan, Bei Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04416">https://arxiv.org/abs/2502.04416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04416">https://arxiv.org/pdf/2502.04416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04416]] CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference(https://arxiv.org/abs/2502.04416)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve impressive performance by scaling model parameters, but this comes with significant inference overhead. Feed-forward networks (FFNs), which dominate LLM parameters, exhibit high activation sparsity in hidden neurons. To exploit this, researchers have proposed using a mixture-of-experts (MoE) architecture, where only a subset of parameters is activated. However, existing approaches often require extensive training data and resources, limiting their practicality. We propose CMoE (Carved MoE), a novel framework to efficiently carve MoE models from dense models. CMoE achieves remarkable performance through efficient expert grouping and lightweight adaptation. First, neurons are grouped into shared and routed experts based on activation rates. Next, we construct a routing mechanism without training from scratch, incorporating a differentiable routing process and load balancing. Using modest data, CMoE produces a well-designed, usable MoE from a 7B dense model within five minutes. With lightweight fine-tuning, it achieves high-performance recovery in under an hour. We make our code publicly available at this https URL.</li>
</ul>

<h3>Title: Autotelic Reinforcement Learning: Exploring Intrinsic Motivations for Skill Acquisition in Open-Ended Environments</h3>
<ul>
<li><strong>Authors: </strong>Prakhar Srivastava, Jasmeet Singh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04418">https://arxiv.org/abs/2502.04418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04418">https://arxiv.org/pdf/2502.04418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04418]] Autotelic Reinforcement Learning: Exploring Intrinsic Motivations for Skill Acquisition in Open-Ended Environments(https://arxiv.org/abs/2502.04418)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive overview of autotelic Reinforcement Learning (RL), emphasizing the role of intrinsic motivations in the open-ended formation of skill repertoires. We delineate the distinctions between knowledge-based and competence-based intrinsic motivations, illustrating how these concepts inform the development of autonomous agents capable of generating and pursuing self-defined goals. The typology of Intrinsically Motivated Goal Exploration Processes (IMGEPs) is explored, with a focus on the implications for multi-goal RL and developmental robotics. The autotelic learning problem is framed within a reward-free Markov Decision Process (MDP), WHERE agents must autonomously represent, generate, and master their own goals. We address the unique challenges in evaluating such agents, proposing various metrics for measuring exploration, generalization, and robustness in complex environments. This work aims to advance the understanding of autotelic RL agents and their potential for enhancing skill acquisition in a diverse and dynamic setting.</li>
</ul>

<h3>Title: Understanding and Mitigating the Bias Inheritance in LLM-based Data Augmentation on Downstream Tasks</h3>
<ul>
<li><strong>Authors: </strong>Miaomiao Li, Hao Chen, Yang Wang, Tingyuan Zhu, Weijia Zhang, Kaijie Zhu, Kam-Fai Wong, Jindong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04419">https://arxiv.org/abs/2502.04419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04419">https://arxiv.org/pdf/2502.04419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04419]] Understanding and Mitigating the Bias Inheritance in LLM-based Data Augmentation on Downstream Tasks(https://arxiv.org/abs/2502.04419)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Generating synthetic datasets via large language models (LLMs) themselves has emerged as a promising approach to improve LLM performance. However, LLMs inherently reflect biases present in their training data, leading to a critical challenge: when these models generate synthetic data for training, they may propagate and amplify their inherent biases that can significantly impact model fairness and robustness on downstream tasks--a phenomenon we term bias inheritance. This work presents the first systematic investigation in understanding, analyzing, and mitigating bias inheritance. We study this problem by fine-tuning LLMs with a combined dataset consisting of original and LLM-augmented data, where bias ratio represents the proportion of augmented data. Through systematic experiments across 10 classification and generation tasks, we analyze how 6 different types of biases manifest at varying bias ratios. Our results reveal that bias inheritance has nuanced effects on downstream tasks, influencing both classification tasks and generation tasks differently. Then, our analysis identifies three key misalignment factors: misalignment of values, group data, and data distributions. Based on these insights, we propose three mitigation strategies: token-based, mask-based, and loss-based approaches. Experiments demonstrate that these strategies also work differently on various tasks and bias, indicating the substantial challenges to fully mitigate bias inheritance. We hope this work can provide valuable insights to the research of LLM data augmentation.</li>
</ul>

<h3>Title: KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Xing Li, Zeyu Xing, Yiming Li, Linping Qu, Hui-Ling Zhen, Wulong Liu, Yiwu Yao, Sinno Jialin Pan, Mingxuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04420">https://arxiv.org/abs/2502.04420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04420">https://arxiv.org/pdf/2502.04420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04420]] KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference(https://arxiv.org/abs/2502.04420)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>KV cache quantization can improve Large Language Models (LLMs) inference throughput and latency in long contexts and large batch-size scenarios while preserving LLMs effectiveness. However, current methods have three unsolved issues: overlooking layer-wise sensitivity to KV cache quantization, high overhead of online fine-grained decision-making, and low flexibility to different LLMs and constraints. Therefore, we thoroughly analyze the inherent correlation of layer-wise transformer attention patterns to KV cache quantization errors and study why key cache is more important than value cache for quantization error reduction. We further propose a simple yet effective framework KVTuner to adaptively search for the optimal hardware-friendly layer-wise KV quantization precision pairs for coarse-grained KV cache with multi-objective optimization and directly utilize the offline searched configurations during online inference. To reduce the computational cost of offline calibration, we utilize the intra-layer KV precision pair pruning and inter-layer clustering to reduce the search space. Experimental results show that we can achieve nearly lossless 3.25-bit mixed precision KV cache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum inference throughput can be improved by 38.3% compared with KV8 quantization over various context lengths.</li>
</ul>

<h3>Title: Assessing and Prioritizing Ransomware Risk Based on Historical Victim Data</h3>
<ul>
<li><strong>Authors: </strong>Spencer Massengale, Philip Huff</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04421">https://arxiv.org/abs/2502.04421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04421">https://arxiv.org/pdf/2502.04421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04421]] Assessing and Prioritizing Ransomware Risk Based on Historical Victim Data(https://arxiv.org/abs/2502.04421)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>We present an approach to identifying which ransomware adversaries are most likely to target specific entities, thereby assisting these entities in formulating better protection strategies. Ransomware poses a formidable cybersecurity threat characterized by profit-driven motives, a complex underlying economy supporting criminal syndicates, and the overt nature of its attacks. This type of malware has consistently ranked among the most prevalent, with a rapid escalation in activity observed. Recent estimates indicate that approximately two-thirds of organizations experienced ransomware attacks in 2023 \cite{Sophos2023Ransomware}. A central tactic in ransomware campaigns is publicizing attacks to coerce victims into paying ransoms. Our study utilizes public disclosures from ransomware victims to predict the likelihood of an entity being targeted by a specific ransomware variant. We employ a Large Language Model (LLM) architecture that uses a unique chain-of-thought, multi-shot prompt methodology to define adversary SKRAM (Skills, Knowledge, Resources, Authorities, and Motivation) profiles from ransomware bulletins, threat reports, and news items. This analysis is enriched with publicly available victim data and is further enhanced by a heuristic for generating synthetic data that reflects victim profiles. Our work culminates in the development of a machine learning model that assists organizations in prioritizing ransomware threats and formulating defenses based on the tactics, techniques, and procedures (TTP) of the most likely attackers.</li>
</ul>

<h3>Title: Primary Care Diagnoses as a Reliable Predictor for Orthopedic Surgical Interventions</h3>
<ul>
<li><strong>Authors: </strong>Khushboo Verma, Alan Michels, Ergi Gumusaneli, Shilpa Chitnis, Smita Sinha Kumar, Christopher Thompson, Lena Esmail, Guruprasath Srinivasan, Chandini Panchada, Sushovan Guha, Satwant Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04423">https://arxiv.org/abs/2502.04423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04423">https://arxiv.org/pdf/2502.04423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04423]] Primary Care Diagnoses as a Reliable Predictor for Orthopedic Surgical Interventions(https://arxiv.org/abs/2502.04423)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Referral workflow inefficiencies, including misaligned referrals and delays, contribute to suboptimal patient outcomes and higher healthcare costs. In this study, we investigated the possibility of predicting procedural needs based on primary care diagnostic entries, thereby improving referral accuracy, streamlining workflows, and providing better care to patients. A de-identified dataset of 2,086 orthopedic referrals from the University of Texas Health at Tyler was analyzed using machine learning models built on Base General Embeddings (BGE) for semantic extraction. To ensure real-world applicability, noise tolerance experiments were conducted, and oversampling techniques were employed to mitigate class imbalance. The selected optimum and parsimonious embedding model demonstrated high predictive accuracy (ROC-AUC: 0.874, Matthews Correlation Coefficient (MCC): 0.540), effectively distinguishing patients requiring surgical intervention. Dimensionality reduction techniques confirmed the model's ability to capture meaningful clinical relationships. A threshold sensitivity analysis identified an optimal decision threshold (0.30) to balance precision and recall, maximizing referral efficiency. In the predictive modeling analysis, the procedure rate increased from 11.27% to an optimal 60.1%, representing a 433% improvement with significant implications for operational efficiency and healthcare revenue. The results of our study demonstrate that referral optimization can enhance primary and surgical care integration. Through this approach, precise and timely predictions of procedural requirements can be made, thereby minimizing delays, improving surgical planning, and reducing administrative burdens. In addition, the findings highlight the potential of clinical decision support as a scalable solution for improving patient outcomes and the efficiency of the healthcare system.</li>
</ul>

<h3>Title: EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>He Hu, Yucheng Zhou, Lianzhong You, Hongbo Xu, Qianning Wang, Zheng Lian, Fei Richard Yu, Fei Ma, Laizhong Cui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04424">https://arxiv.org/abs/2502.04424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04424">https://arxiv.org/pdf/2502.04424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04424]] EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models(https://arxiv.org/abs/2502.04424)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the integration of Multimodal large language models (MLLMs) into robotic systems and various AI applications, embedding emotional intelligence (EI) capabilities into these models is essential for enabling robots to effectively address human emotional needs and interact seamlessly in real-world scenarios. Existing static, text-based, or text-image benchmarks overlook the multimodal complexities of real-world interactions and fail to capture the dynamic, multimodal nature of emotional expressions, making them inadequate for evaluating MLLMs' EI. Based on established psychological theories of EI, we build EmoBench-M, a novel benchmark designed to evaluate the EI capability of MLLMs across 13 valuation scenarios from three key dimensions: foundational emotion recognition, conversational emotion understanding, and socially complex emotion analysis. Evaluations of both open-source and closed-source MLLMs on EmoBench-M reveal a significant performance gap between them and humans, highlighting the need to further advance their EI capabilities. All benchmark resources, including code and datasets, are publicly available at this https URL.</li>
</ul>

<h3>Title: Decoding AI Judgment: How LLMs Assess News Credibility and Bias</h3>
<ul>
<li><strong>Authors: </strong>Edoardo Loru, Jacopo Nudo, Niccolò Di Marco, Matteo Cinelli, Walter Quattrociocchi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04426">https://arxiv.org/abs/2502.04426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04426">https://arxiv.org/pdf/2502.04426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04426]] Decoding AI Judgment: How LLMs Assess News Credibility and Bias(https://arxiv.org/abs/2502.04426)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used to assess news credibility, yet little is known about how they make these judgments. While prior research has examined political bias in LLM outputs or their potential for automated fact-checking, their internal evaluation processes remain largely unexamined. Understanding how LLMs assess credibility provides insights into AI behavior and how credibility is structured and applied in large-scale language models. This study benchmarks the reliability and political classifications of state-of-the-art LLMs - Gemini 1.5 Flash (Google), GPT-4o mini (OpenAI), and LLaMA 3.1 (Meta) - against structured, expert-driven rating systems such as NewsGuard and Media Bias Fact Check. Beyond assessing classification performance, we analyze the linguistic markers that shape LLM decisions, identifying which words and concepts drive their evaluations. We uncover patterns in how LLMs associate credibility with specific linguistic features by examining keyword frequency, contextual determinants, and rank distributions. Beyond static classification, we introduce a framework in which LLMs refine their credibility assessments by retrieving external information, querying other models, and adapting their responses. This allows us to investigate whether their assessments reflect structured reasoning or rely primarily on prior learned associations.</li>
</ul>

<h3>Title: Confident or Seek Stronger: Exploring Uncertainty-Based On-device LLM Routing From Benchmarking to Generalization</h3>
<ul>
<li><strong>Authors: </strong>Yu-Neng Chuang, Leisheng Yu, Guanchu Wang, Lizhe Zhang, Zirui Liu, Xuanting Cai, Yang Sui, Vladimir Braverman, Xia Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04428">https://arxiv.org/abs/2502.04428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04428">https://arxiv.org/pdf/2502.04428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04428]] Confident or Seek Stronger: Exploring Uncertainty-Based On-device LLM Routing From Benchmarking to Generalization(https://arxiv.org/abs/2502.04428)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed and democratized on edge devices. To improve the efficiency of on-device deployment, small language models (SLMs) are often adopted due to their efficient decoding latency and reduced energy consumption. However, these SLMs often generate inaccurate responses when handling complex queries. One promising solution is uncertainty-based SLM routing, offloading high-stakes queries to stronger LLMs when resulting in low-confidence responses on SLM. This follows the principle of "If you lack confidence, seek stronger support" to enhance reliability. Relying on more powerful LLMs is yet effective but increases invocation costs. Therefore, striking a routing balance between efficiency and efficacy remains a critical challenge. Additionally, efficiently generalizing the routing strategy to new datasets remains under-explored. In this paper, we conduct a comprehensive investigation into benchmarking and generalization of uncertainty-driven routing strategies from SLMs to LLMs over 1500+ settings. Our findings highlight: First, uncertainty-correctness alignment in different uncertainty quantification (UQ) methods significantly impacts routing performance. Second, uncertainty distributions depend more on both the specific SLM and the chosen UQ method, rather than downstream data. Building on the insight, we propose a calibration data construction instruction pipeline and open-source a constructed hold-out set to enhance routing generalization on new downstream scenarios. The experimental results indicate calibration data effectively bootstraps routing performance without any new data.</li>
</ul>

<h3>Title: Training Language Models to Reason Efficiently</h3>
<ul>
<li><strong>Authors: </strong>Daman Arora, Andrea Zanette</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04463">https://arxiv.org/abs/2502.04463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04463">https://arxiv.org/pdf/2502.04463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04463]] Training Language Models to Reason Efficiently(https://arxiv.org/abs/2502.04463)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scaling model size and training data has led to great advances in the performance of Large Language Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particularly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of-thoughts, bring unprecedented breakthroughs in problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the economic feasibility, user experience, and environmental sustainability of these models. In this work, we propose to train large reasoning models to reason efficiently. More precisely, we use reinforcement learning (RL) to train reasoning models to dynamically allocate inference-time compute based on task complexity. Our method incentivizes models to minimize unnecessary computational overhead while maintaining accuracy, thereby achieving substantial efficiency gains. It enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experiments on two open-weight large reasoning models demonstrate significant reductions in inference cost while preserving most of the accuracy.</li>
</ul>

<h3>Title: FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks</h3>
<ul>
<li><strong>Authors: </strong>Luca Della Libera, Francesco Paissan, Cem Subakan, Mirco Ravanelli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04465">https://arxiv.org/abs/2502.04465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04465">https://arxiv.org/pdf/2502.04465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04465]] FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks(https://arxiv.org/abs/2502.04465)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples, code and checkpoints are available at this https URL.</li>
</ul>

<h3>Title: Iterative Importance Fine-tuning of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Alexander Denker, Shreyas Padhy, Francisco Vargas, Johannes Hertrich</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04468">https://arxiv.org/abs/2502.04468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04468">https://arxiv.org/pdf/2502.04468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04468]] Iterative Importance Fine-tuning of Diffusion Models(https://arxiv.org/abs/2502.04468)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are an important tool for generative modelling, serving as effective priors in applications such as imaging and protein design. A key challenge in applying diffusion models for downstream tasks is efficiently sampling from resulting posterior distributions, which can be addressed using the $h$-transform. This work introduces a self-supervised algorithm for fine-tuning diffusion models by estimating the $h$-transform, enabling amortised conditional sampling. Our method iteratively refines the $h$-transform using a synthetic dataset resampled with path-based importance weights. We demonstrate the effectiveness of this framework on class-conditional sampling and reward fine-tuning for text-to-image diffusion models.</li>
</ul>

<h3>Title: No Images, No Problem: Retaining Knowledge in Continual VQA with Questions-Only Memory</h3>
<ul>
<li><strong>Authors: </strong>Imad Eddine Marouf, Enzo Tartaglione, Stephane Lathuiliere, Joost van de Weijer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04469">https://arxiv.org/abs/2502.04469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04469">https://arxiv.org/pdf/2502.04469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04469]] No Images, No Problem: Retaining Knowledge in Continual VQA with Questions-Only Memory(https://arxiv.org/abs/2502.04469)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Continual Learning in Visual Question Answering (VQACL) requires models to learn new visual-linguistic tasks (plasticity) while retaining knowledge from previous tasks (stability). The multimodal nature of VQACL presents unique challenges, requiring models to balance stability across visual and textual domains while maintaining plasticity to adapt to novel objects and reasoning tasks. Existing methods, predominantly designed for unimodal tasks, often struggle to balance these demands effectively. In this work, we introduce QUestion-only replay with Attention Distillation (QUAD), a novel approach for VQACL that leverages only past task questions for regularisation, eliminating the need to store visual data and addressing both memory and privacy concerns. QUAD achieves stability by introducing a question-only replay mechanism that selectively uses questions from previous tasks to prevent overfitting to the current task's answer space, thereby mitigating the out-of-answer-set problem. Complementing this, we propose attention consistency distillation, which uniquely enforces both intra-modal and inter-modal attention consistency across tasks, preserving essential visual-linguistic associations. Extensive experiments on VQAv2 and NExT-QA demonstrate that QUAD significantly outperforms state-of-the-art methods, achieving robust performance in continual VQA.</li>
</ul>

<h3>Title: Augmented Conditioning Is Enough For Effective Training Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Chen, Amy Zhang, Adriana Romero-Soriano</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04475">https://arxiv.org/abs/2502.04475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04475">https://arxiv.org/pdf/2502.04475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04475]] Augmented Conditioning Is Enough For Effective Training Image Generation(https://arxiv.org/abs/2502.04475)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image generation abilities of text-to-image diffusion models have significantly advanced, yielding highly photo-realistic images from descriptive text and increasing the viability of leveraging synthetic images to train computer vision models. To serve as effective training data, generated images must be highly realistic while also sufficiently diverse within the support of the target data distribution. Yet, state-of-the-art conditional image generation models have been primarily optimized for creative applications, prioritizing image realism and prompt adherence over conditional diversity. In this paper, we investigate how to improve the diversity of generated images with the goal of increasing their effectiveness to train downstream image classification models, without fine-tuning the image generation model. We find that conditioning the generation process on an augmented real image and text prompt produces generations that serve as effective synthetic datasets for downstream training. Conditioning on real training images contextualizes the generation process to produce images that are in-domain with the real image distribution, while data augmentations introduce visual diversity that improves the performance of the downstream classifier. We validate augmentation-conditioning on a total of five established long-tail and few-shot image classification benchmarks and show that leveraging augmentations to condition the generation process results in consistent improvements over the state-of-the-art on the long-tailed benchmark and remarkable gains in extreme few-shot regimes of the remaining four benchmarks. These results constitute an important step towards effectively leveraging synthetic data for downstream training.</li>
</ul>

<h3>Title: OneTrack-M: A multitask approach to transformer-based MOT models</h3>
<ul>
<li><strong>Authors: </strong>Luiz C. S. de Araujo, Carlos M. S. Figueiredo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04478">https://arxiv.org/abs/2502.04478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04478">https://arxiv.org/pdf/2502.04478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04478]] OneTrack-M: A multitask approach to transformer-based MOT models(https://arxiv.org/abs/2502.04478)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multi-Object Tracking (MOT) is a critical problem in computer vision, essential for understanding how objects move and interact in videos. This field faces significant challenges such as occlusions and complex environmental dynamics, impacting model accuracy and efficiency. While traditional approaches have relied on Convolutional Neural Networks (CNNs), introducing transformers has brought substantial advancements. This work introduces OneTrack-M, a transformer-based MOT model designed to enhance tracking computational efficiency and accuracy. Our approach simplifies the typical transformer-based architecture by eliminating the need for a decoder model for object detection and tracking. Instead, the encoder alone serves as the backbone for temporal data interpretation, significantly reducing processing time and increasing inference speed. Additionally, we employ innovative data pre-processing and multitask training techniques to address occlusion and diverse objective challenges within a single set of weights. Experimental results demonstrate that OneTrack-M achieves at least 25% faster inference times compared to state-of-the-art models in the literature while maintaining or improving tracking accuracy metrics. These improvements highlight the potential of the proposed solution for real-time applications such as autonomous vehicles, surveillance systems, and robotics, where rapid responses are crucial for system effectiveness.</li>
</ul>

<h3>Title: Active Task Disambiguation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Katarzyna Kobalczyk, Nicolas Astorga, Tennison Liu, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04485">https://arxiv.org/abs/2502.04485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04485">https://arxiv.org/pdf/2502.04485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04485]] Active Task Disambiguation with LLMs(https://arxiv.org/abs/2502.04485)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the impressive performance of large language models (LLMs) across various benchmarks, their ability to address ambiguously specified problems--frequent in real-world interactions--remains underexplored. To address this gap, we introduce a formal definition of task ambiguity and frame the problem of task disambiguation through the lens of Bayesian Experimental Design. By posing clarifying questions, LLM agents can acquire additional task specifications, progressively narrowing the space of viable solutions and reducing the risk of generating unsatisfactory outputs. Yet, generating effective clarifying questions requires LLM agents to engage in a form of meta-cognitive reasoning, an ability LLMs may presently lack. Our proposed approach of active task disambiguation enables LLM agents to generate targeted questions maximizing the information gain. Effectively, this approach shifts the load from implicit to explicit reasoning about the space of viable solutions. Empirical results demonstrate that this form of question selection leads to more effective task disambiguation in comparison to approaches relying on reasoning solely within the space of questions.</li>
</ul>

<h3>Title: Building A Unified AI-centric Language System: analysis, framework and future work</h3>
<ul>
<li><strong>Authors: </strong>Edward Hong Wang, Cynthia Xin Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04488">https://arxiv.org/abs/2502.04488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04488">https://arxiv.org/pdf/2502.04488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04488]] Building A Unified AI-centric Language System: analysis, framework and future work(https://arxiv.org/abs/2502.04488)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models have demonstrated that extended inference through techniques can markedly improve performance, yet these gains come with increased computational costs and the propagation of inherent biases found in natural languages. This paper explores the design of a unified AI-centric language system that addresses these challenges by offering a more concise, unambiguous, and computationally efficient alternative to traditional human languages. We analyze the limitations of natural language such as gender bias, morphological irregularities, and contextual ambiguities and examine how these issues are exacerbated within current Transformer architectures, where redundant attention heads and token inefficiencies prevail. Drawing on insights from emergent artificial communication systems and constructed languages like Esperanto and Lojban, we propose a framework that translates diverse natural language inputs into a streamlined AI-friendly language, enabling more efficient model training and inference while reducing memory footprints. Finally, we outline a pathway for empirical validation through controlled experiments, paving the way for a universal interchange format that could revolutionize AI-to-AI and human-to-AI interactions by enhancing clarity, fairness, and overall performance.</li>
</ul>

<h3>Title: CNN Autoencoders for Hierarchical Feature Extraction and Fusion in Multi-sensor Human Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Saeed Arabzadeh, Farshad Almasganj, Mohammad Mahdi Ahmadi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04489">https://arxiv.org/abs/2502.04489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04489">https://arxiv.org/pdf/2502.04489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04489]] CNN Autoencoders for Hierarchical Feature Extraction and Fusion in Multi-sensor Human Activity Recognition(https://arxiv.org/abs/2502.04489)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Deep learning methods have been widely used for Human Activity Recognition (HAR) using recorded signals from Iner-tial Measurement Units (IMUs) sensors that are installed on various parts of the human body. For this type of HAR, sev-eral challenges exist, the most significant of which is the analysis of multivarious IMU sensors data. Here, we introduce a Hierarchically Unsupervised Fusion (HUF) model designed to extract, and fuse features from IMU sensors data via a hybrid structure of Convolutional Neural Networks (CNN)s and Autoencoders (AE)s. First, we design a stack CNN-AE to embed short-time signals into sets of high dimensional features. Second, we develop another CNN-AE network to locally fuse the extracted features from each sensor unit. Finally, we unify all the sensor features through a third CNN-AE architecture as globally feature fusion to create a unique feature set. Additionally, we analyze the effects of varying the model hyperparameters. The best results are achieved with eight convolutional layers in each AE. Furthermore, it is determined that an overcomplete AE with 256 kernels in the code layer is suitable for feature extraction in the first block of the proposed HUF model; this number reduces to 64 in the last block of the model to customize the size of the applied features to the classifier. The tuned model is applied to the UCI-HAR, DaLiAc, and Parkinson's disease gait da-tasets, achieving the classification accuracies of 97%, 97%, and 88%, respectively, which are nearly 3% better com-pared to the state-of-the-art supervised methods.</li>
</ul>

<h3>Title: Provable Sample-Efficient Transfer Learning Conditional Diffusion Models via Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Ziheng Cheng, Tianyu Xie, Shiyue Zhang, Cheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04491">https://arxiv.org/abs/2502.04491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04491">https://arxiv.org/pdf/2502.04491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04491]] Provable Sample-Efficient Transfer Learning Conditional Diffusion Models via Representation Learning(https://arxiv.org/abs/2502.04491)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While conditional diffusion models have achieved remarkable success in various applications, they require abundant data to train from scratch, which is often infeasible in practice. To address this issue, transfer learning has emerged as an essential paradigm in small data regimes. Despite its empirical success, the theoretical underpinnings of transfer learning conditional diffusion models remain unexplored. In this paper, we take the first step towards understanding the sample efficiency of transfer learning conditional diffusion models through the lens of representation learning. Inspired by practical training procedures, we assume that there exists a low-dimensional representation of conditions shared across all tasks. Our analysis shows that with a well-learned representation from source tasks, the samplecomplexity of target tasks can be reduced substantially. In addition, we investigate the practical implications of our theoretical results in several real-world applications of conditional diffusion models. Numerical experiments are also conducted to verify our results.</li>
</ul>

<h3>Title: Multi-Agent Reinforcement Learning with Focal Diversity Optimization</h3>
<ul>
<li><strong>Authors: </strong>Selim Furkan Tekin, Fatih Ilhan, Tiansheng Huang, Sihao Hu, Zachary Yahn, Ling Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04492">https://arxiv.org/abs/2502.04492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04492">https://arxiv.org/pdf/2502.04492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04492]] Multi-Agent Reinforcement Learning with Focal Diversity Optimization(https://arxiv.org/abs/2502.04492)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The advancement of Large Language Models (LLMs) and their finetuning strategies has triggered the renewed interests in multi-agent reinforcement learning. In this paper, we introduce a focal diversity-optimized multi-agent reinforcement learning approach, coined as MARL-Focal, with three unique characteristics. First, we develop an agent-fusion framework for encouraging multiple LLM based agents to collaborate in producing the final inference output for each LLM query. Second, we develop a focal-diversity optimized agent selection algorithm that can choose a small subset of the available agents based on how well they can complement one another to generate the query output. Finally, we design a conflict-resolution method to detect output inconsistency among multiple agents and produce our MARL-Focal output through reward-aware and policy-adaptive inference fusion. Extensive evaluations on five benchmarks show that MARL-Focal is cost-efficient and adversarial-robust. Our multi-agent fusion model achieves performance improvement of 5.51\% compared to the best individual LLM-agent and offers stronger robustness over the TruthfulQA benchmark. Code is available at this https URL</li>
</ul>

<h3>Title: Verifiable Format Control for Large Language Model Generations</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyang Wang, Jinqi Jiang, Huichi Zhou, Wenhao Zheng, Xuchao Zhang, Chetan Bansal, Huaxiu Yao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04498">https://arxiv.org/abs/2502.04498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04498">https://arxiv.org/pdf/2502.04498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04498]] Verifiable Format Control for Large Language Model Generations(https://arxiv.org/abs/2502.04498)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent Large Language Models (LLMs) have demonstrated satisfying general instruction following ability. However, small LLMs with about 7B parameters still struggle fine-grained format following (e.g., JSON format), which seriously hinder the advancements of their applications. Most existing methods focus on benchmarking general instruction following while overlook how to improve the specific format following ability for small LLMs. Besides, these methods often rely on evaluations based on advanced LLMs (e.g., GPT-4), which can introduce the intrinsic bias of LLMs and be costly due to the API calls. In this paper, we first curate a fully verifiable format following dataset VFF. In contrast to existing works often adopting external LLMs for instruction-following validations, every sample of VFF can be easily validated with a Python function. Further, we propose to leverage this verifiable feature to synthesize massive data for progressively training small LLMs, in order to improve their format following abilities. Experimental results highlight the prevalent limitations in the format following capabilities of 7B level open-source LLMs and demonstrate the effectiveness of our method in enhancing this essential ability.</li>
</ul>

<h3>Title: ULPT: Prompt Tuning with Ultra-Low-Dimensional Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zijun Wu, Yongchang Hao, Lili Mou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04501">https://arxiv.org/abs/2502.04501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04501">https://arxiv.org/pdf/2502.04501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04501]] ULPT: Prompt Tuning with Ultra-Low-Dimensional Optimization(https://arxiv.org/abs/2502.04501)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models achieve state-of-the-art performance but are costly to fine-tune due to their size. Parameter-efficient fine-tuning methods, such as prompt tuning, address this by reducing trainable parameters while maintaining strong performance. However, prior methods tie prompt embeddings to the model's dimensionality, which may not scale well with larger LLMs and more customized LLMs. In this paper, we propose Ultra-Low-dimensional Prompt Tuning (ULPT), which optimizes prompts in a low-dimensional space (e.g., 2D) and use a random but frozen matrix for the up-projection. To enhance alignment, we introduce learnable shift and scale embeddings. ULPT drastically reduces the trainable parameters, e.g., 2D only using 2% parameters compared with vanilla prompt tuning while retaining most of the performance across 21 NLP tasks. Our theoretical analysis shows that random projections can capture high-rank structures effectively, and experimental results demonstrate ULPT's competitive performance over existing parameter-efficient methods.</li>
</ul>

<h3>Title: Fast Video Generation with Sliding Tile Attention</h3>
<ul>
<li><strong>Authors: </strong>Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhenghong Liu, Hao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04507">https://arxiv.org/abs/2502.04507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04507">https://arxiv.org/pdf/2502.04507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04507]] Fast Video Generation with Sliding Tile Attention(https://arxiv.org/abs/2502.04507)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being hardware-efficient. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over FlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s without quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only a 0.09% drop on VBench.</li>
</ul>

<h3>Title: MedGNN: Towards Multi-resolution Spatiotemporal Graph Learning for Medical Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Wei Fan, Jingru Fei, Dingyu Guo, Kun Yi, Xiaozhuang Song, Haolong Xiang, Hangting Ye, Min Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04515">https://arxiv.org/abs/2502.04515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04515">https://arxiv.org/pdf/2502.04515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04515]] MedGNN: Towards Multi-resolution Spatiotemporal Graph Learning for Medical Time Series Classification(https://arxiv.org/abs/2502.04515)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Medical time series has been playing a vital role in real-world healthcare systems as valuable information in monitoring health conditions of patients. Accurate classification for medical time series, e.g., Electrocardiography (ECG) signals, can help for early detection and diagnosis. Traditional methods towards medical time series classification rely on handcrafted feature extraction and statistical methods; with the recent advancement of artificial intelligence, the machine learning and deep learning methods have become more popular. However, existing methods often fail to fully model the complex spatial dynamics under different scales, which ignore the dynamic multi-resolution spatial and temporal joint inter-dependencies. Moreover, they are less likely to consider the special baseline wander problem as well as the multi-view characteristics of medical time series, which largely hinders their prediction performance. To address these limitations, we propose a Multi-resolution Spatiotemporal Graph Learning framework, MedGNN, for medical time series classification. Specifically, we first propose to construct multi-resolution adaptive graph structures to learn dynamic multi-scale embeddings. Then, to address the baseline wander problem, we propose Difference Attention Networks to operate self-attention mechanisms on the finite difference for temporal modeling. Moreover, to learn the multi-view characteristics, we utilize the Frequency Convolution Networks to capture complementary information of medical time series from the frequency domain. In addition, we introduce the Multi-resolution Graph Transformer architecture to model the dynamic dependencies and fuse the information from different resolutions. Finally, we have conducted extensive experiments on multiple medical real-world datasets that demonstrate the superior performance of our method. Our Code is available.</li>
</ul>

<h3>Title: Group-Adaptive Threshold Optimization for Robust AI-Generated Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Minseok Jung, Cynthia Fuertes Panizo, Liam Dugan, May Fung, Pin-Yu Chen, Paul Pu Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04528">https://arxiv.org/abs/2502.04528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04528">https://arxiv.org/pdf/2502.04528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04528]] Group-Adaptive Threshold Optimization for Robust AI-Generated Text Detection(https://arxiv.org/abs/2502.04528)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>The advancement of large language models (LLMs) has made it difficult to differentiate human-written text from AI-generated text. Several AI-text detectors have been developed in response, which typically utilize a fixed global threshold (e.g., {\theta} = 0.5) to classify machine-generated text. However, we find that one universal threshold can fail to account for subgroup-specific distributional variations. For example, when using a fixed threshold, detectors make more false positive errors on shorter human-written text than longer, and more positive classifications on neurotic writing styles than open among long text. These discrepancies can lead to misclassification that disproportionately affects certain groups. We address this critical limitation by introducing FairOPT, an algorithm for group-specific threshold optimization in AI-generated content classifiers. Our approach partitions data into subgroups based on attributes (e.g., text length and writing style) and learns decision thresholds for each group, which enables careful balancing of performance and fairness metrics within each subgroup. In experiments with four AI text classifiers on three datasets, FairOPT enhances overall F1 score and decreases balanced error rate (BER) discrepancy across subgroups. Our framework paves the way for more robust and fair classification criteria in AI-generated output detection.</li>
</ul>

<h3>Title: Agricultural Field Boundary Detection through Integration of "Simple Non-Iterative Clustering (SNIC) Super Pixels" and "Canny Edge Detection Method"</h3>
<ul>
<li><strong>Authors: </strong>Artughrul Gayibov (Baku Engineering University)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04529">https://arxiv.org/abs/2502.04529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04529">https://arxiv.org/pdf/2502.04529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04529]] Agricultural Field Boundary Detection through Integration of "Simple Non-Iterative Clustering (SNIC) Super Pixels" and "Canny Edge Detection Method"(https://arxiv.org/abs/2502.04529)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Efficient use of cultivated areas is a necessary factor for sustainable development of agriculture and ensuring food security. Along with the rapid development of satellite technologies in developed countries, new methods are being searched for accurate and operational identification of cultivated areas. In this context, identification of cropland boundaries based on spectral analysis of data obtained from satellite images is considered one of the most optimal and accurate methods in modern agriculture. This article proposes a new approach to determine the suitability and green index of cultivated areas using satellite data obtained through the "Google Earth Engine" (GEE) platform. In this approach, two powerful algorithms, "SNIC (Simple Non-Iterative Clustering) Super Pixels" and "Canny Edge Detection Method", are combined. The SNIC algorithm combines pixels in a satellite image into larger regions (super pixels) with similar characteristics, thereby providing better image analysis. The Canny Edge Detection Method detects sharp changes (edges) in the image to determine the precise boundaries of agricultural fields. This study, carried out using high-resolution multispectral data from the Sentinel-2 satellite and the Google Earth Engine JavaScript API, has shown that the proposed method is effective in accurately and reliably classifying randomly selected agricultural fields. The combined use of these two tools allows for more accurate determination of the boundaries of agricultural fields by minimizing the effects of outliers in satellite images. As a result, more accurate and reliable maps can be created for agricultural monitoring and resource management over large areas based on the obtained data. By expanding the application capabilities of cloud-based platforms and artificial intelligence methods in the agricultural field.</li>
</ul>

<h3>Title: A Decoding Algorithm for Length-Control Summarization Based on Directed Acyclic Transformers</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Huang, Hao Zhou, Cameron Jen, Kangjie Zheng, Osmar R. Zaïane, Lili Mou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04535">https://arxiv.org/abs/2502.04535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04535">https://arxiv.org/pdf/2502.04535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04535]] A Decoding Algorithm for Length-Control Summarization Based on Directed Acyclic Transformers(https://arxiv.org/abs/2502.04535)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Length-control summarization aims to condense long texts into a short one within a certain length limit. Previous approaches often use autoregressive (AR) models and treat the length requirement as a soft constraint, which may not always be satisfied. In this study, we propose a novel length-control decoding algorithm based on the Directed Acyclic Transformer (DAT). Our approach allows for multiple plausible sequence fragments and predicts a \emph{path} to connect them. In addition, we propose a Sequence Maximum a Posteriori (SeqMAP) decoding algorithm that marginalizes different possible paths and finds the most probable summary satisfying the length budget. Our algorithm is based on beam search, which further facilitates a reranker for performance improvement. Experimental results on the Gigaword and DUC2004 datasets demonstrate our state-of-the-art performance for length-control summarization.</li>
</ul>

<h3>Title: Multilingual Non-Autoregressive Machine Translation without Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Huang, Fei Huang, Zaixiang Zheng, Osmar R. Zaïane, Hao Zhou, Lili Mou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04537">https://arxiv.org/abs/2502.04537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04537">https://arxiv.org/pdf/2502.04537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04537]] Multilingual Non-Autoregressive Machine Translation without Knowledge Distillation(https://arxiv.org/abs/2502.04537)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multilingual neural machine translation (MNMT) aims at using one single model for multiple translation directions. Recent work applies non-autoregressive Transformers to improve the efficiency of MNMT, but requires expensive knowledge distillation (KD) processes. To this end, we propose an M-DAT approach to non-autoregressive multilingual machine translation. Our system leverages the recent advance of the directed acyclic Transformer (DAT), which does not require KD. We further propose a pivot back-translation (PivotBT) approach to improve the generalization to unseen translation directions. Experiments show that our M-DAT achieves state-of-the-art performance in non-autoregressive MNMT.</li>
</ul>

<h3>Title: SoK: "Interoperability vs Security" Arguments: A Technical Framework</h3>
<ul>
<li><strong>Authors: </strong>Daji Landis, Elettra Bietti, Sunoo Park</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04538">https://arxiv.org/abs/2502.04538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04538">https://arxiv.org/pdf/2502.04538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04538]] SoK: "Interoperability vs Security" Arguments: A Technical Framework(https://arxiv.org/abs/2502.04538)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Concerns about big tech's monopoly power have featured prominently in recent media and policy discourse, and regulators across the US, the EU, and beyond have ramped up efforts to promote healthier competition in the market. One of the favored approaches is to require certain kinds of interoperation between platforms, to mitigate the current concentration of power in the biggest companies. Unsurprisingly, interoperability initiatives have generally been met with vocal resistance by big tech companies. Perhaps more surprisingly, a significant part of that pushback has been in the name of security -- that is, arguing against interoperation on the basis that it will undermine security. We conduct a detailed examination of "security vs. interoperability" arguments in the context of recent antitrust proceedings in the US and the EU. First, we propose a taxonomy of such arguments. Second, we provide several detailed case studies, which illustrate our taxonomy's utility in disentangling where security and interoperability are and are not in tension, where securing interoperable systems presents novel engineering challenges, and where "security arguments" against interoperability are really more about anti-competitive behavior than security. Third, we undertake a comparative analysis that highlights key considerations around the interplay of economic incentives, market power, and security across diverse contexts where security and interoperability may appear to be in tension. We believe systematically distinguishing cases and patterns within our taxonomy and analytical framework can be a valuable analytical tool for experts and non-experts alike in today's fast-paced regulatory landscape.</li>
</ul>

<h3>Title: The Phantom of the Elytra -- Phylogenetic Trait Extraction from Images of Rove Beetles Using Deep Learning -- Is the Mask Enough?</h3>
<ul>
<li><strong>Authors: </strong>Roberta Hunt, Kim Steenstrup Pedersen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04541">https://arxiv.org/abs/2502.04541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04541">https://arxiv.org/pdf/2502.04541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04541]] The Phantom of the Elytra -- Phylogenetic Trait Extraction from Images of Rove Beetles Using Deep Learning -- Is the Mask Enough?(https://arxiv.org/abs/2502.04541)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, explainability, segmentation</a></li>
<li><strong>Abstract: </strong>Phylogenetic analysis traditionally relies on labor-intensive manual extraction of morphological traits, limiting its scalability for large datasets. Recent advances in deep learning offer the potential to automate this process, but the effectiveness of different morphological representations for phylogenetic trait extraction remains poorly understood. In this study, we compare the performance of deep learning models using three distinct morphological representations - full segmentations, binary masks, and Fourier descriptors of beetle outlines. We test this on the Rove-Tree-11 dataset, a curated collection of images from 215 rove beetle species. Our results demonstrate that the mask-based model outperformed the others, achieving a normalized Align Score of 0.33 plus/minus 0.02 on the test set, compared to 0.45 plus/minus 0.01 for the Fourier-based model and 0.39 plus/minus 0.07 for the segmentation-based model. The performance of the mask-based model likely reflects its ability to capture shape features while taking advantage of the depth and capacity of the ResNet50 architecture. These results also indicate that dorsal textural features, at least in this group of beetles, may be of lowered phylogenetic relevance, though further investigation is necessary to confirm this. In contrast, the Fourier-based model suffered from reduced capacity and occasional inaccuracies in outline approximations, particularly in fine structures like legs. These findings highlight the importance of selecting appropriate morphological representations for automated phylogenetic studies and the need for further research into explainability in automatic morphological trait extraction.</li>
</ul>

<h3>Title: Contextual Gradient Flow Modeling for Large Language Model Generalization in Multi-Scale Feature Spaces</h3>
<ul>
<li><strong>Authors: </strong>Daphne Quillington, Kingsley Fairbrother, Xavier Tattershall, Irin Kabakum</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04548">https://arxiv.org/abs/2502.04548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04548">https://arxiv.org/pdf/2502.04548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04548]] Contextual Gradient Flow Modeling for Large Language Model Generalization in Multi-Scale Feature Spaces(https://arxiv.org/abs/2502.04548)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Optimization methodologies for training large-scale neural architectures often rely on uniform gradient propagation mechanisms that fail to align with hierarchical linguistic structures, limiting their capacity to generalize across diverse language distributions. A structured gradient refinement framework was introduced to incorporate multi-scale contextual adjustments, improving parameter adaptation through dynamic weighting strategies that enhanced representation coherence. Empirical evaluations demonstrated that structured propagation mechanisms contributed to reductions in gradient oscillations, resulting in more stable training dynamics and improved optimization efficiency. The comparative performance assessment indicated that models incorporating hierarchical propagation strategies exhibited greater robustness in long-range dependency retention and cross-domain adaptation. The hierarchical adjustment of weight updates provided an alternative to conventional backpropagation, reducing sensitivity to initialization conditions while improving overall convergence efficiency. The experimental results confirmed that structured gradient propagation influenced representation learning trajectories, aligning parameter updates with broader linguistic dependencies rather than isolated token-level relationships. Statistical evaluations indicated that structured optimization strategies mitigated overfitting while preserving adaptability across heterogeneous text distributions. The findings established that structured gradient propagation provided an empirically validated framework for refining hierarchical representation learning, supporting more effective integration of linguistic dependencies into optimization dynamics.</li>
</ul>

<h3>Title: Mechanisms of Projective Composition of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Arwen Bradley, Preetum Nakkiran, David Berthelot, James Thornton, Joshua M. Susskind</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04549">https://arxiv.org/abs/2502.04549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04549">https://arxiv.org/pdf/2502.04549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04549]] Mechanisms of Projective Composition of Diffusion Models(https://arxiv.org/abs/2502.04549)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We study the theoretical foundations of composition in diffusion models, with a particular focus on out-of-distribution extrapolation and length-generalization. Prior work has shown that composing distributions via linear score combination can achieve promising results, including length-generalization in some cases (Du et al., 2023; Liu et al., 2022). However, our theoretical understanding of how and why such compositions work remains incomplete. In fact, it is not even entirely clear what it means for composition to "work". This paper starts to address these fundamental gaps. We begin by precisely defining one possible desired result of composition, which we call projective composition. Then, we investigate: (1) when linear score combinations provably achieve projective composition, (2) whether reverse-diffusion sampling can generate the desired composition, and (3) the conditions under which composition fails. Finally, we connect our theoretical analysis to prior empirical observations where composition has either worked or failed, for reasons that were unclear at the time.</li>
</ul>

<h3>Title: TruthFlow: Truthful LLM Generation via Representation Flow Correction</h3>
<ul>
<li><strong>Authors: </strong>Hanyu Wang, Bochuan Cao, Yuanpu Cao, Jinghui Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04556">https://arxiv.org/abs/2502.04556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04556">https://arxiv.org/pdf/2502.04556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04556]] TruthFlow: Truthful LLM Generation via Representation Flow Correction(https://arxiv.org/abs/2502.04556)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are known to struggle with consistently generating truthful responses. While various representation intervention techniques have been proposed, these methods typically apply a universal representation correction vector to all input queries, limiting their effectiveness against diverse queries in practice. In this study, we introduce TruthFlow, a novel method that leverages the Flow Matching technique for query-specific truthful representation correction. Specifically, TruthFlow first uses a flow model to learn query-specific correction vectors that transition representations from hallucinated to truthful states. Then, during inference, the trained flow model generates these correction vectors to enhance the truthfulness of LLM outputs. Experimental results demonstrate that TruthFlow significantly improves performance on open-ended generation tasks across various advanced LLMs evaluated on TruthfulQA. Moreover, the trained TruthFlow model exhibits strong transferability, performing effectively on other unseen hallucination benchmarks.</li>
</ul>

<h3>Title: Speeding up Speculative Decoding via Approximate Verification</h3>
<ul>
<li><strong>Authors: </strong>Meiyu Zhong, Noel Teku, Ravi Tandon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04557">https://arxiv.org/abs/2502.04557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04557">https://arxiv.org/pdf/2502.04557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04557]] Speeding up Speculative Decoding via Approximate Verification(https://arxiv.org/abs/2502.04557)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speculative Decoding (SD) is a recently proposed technique for faster inference using Large Language Models (LLMs). SD operates by using a smaller draft LLM for autoregressively generating a sequence of tokens and a larger target LLM for parallel verification to ensure statistical consistency. However, periodic parallel calls to the target LLM for verification prevent SD from achieving even lower latencies. We propose SPRINTER, which utilizes a low-complexity verifier trained to predict if tokens generated from a draft LLM would be accepted by the target LLM. By performing approximate sequential verification, SPRINTER does not require verification by the target LLM and is only invoked when a token is deemed unacceptable. This leads to reducing the number of calls to the larger LLM and can achieve further speedups. We present a theoretical analysis of SPRINTER, examining the statistical properties of the generated tokens, as well as the expected reduction in latency as a function of the verifier. We evaluate SPRINTER on several datasets and model pairs, demonstrating that approximate verification can still maintain high quality generation while further reducing latency. For instance, on Wiki-Summaries dataset, SPRINTER achieves a 1.7x latency speedup and requires 8.3x fewer flops relative to SD, while still generating high-quality responses when using GPT2-Small and GPT2-XL as draft/target models.</li>
</ul>

<h3>Title: My LLM might Mimic AAE -- But When Should it?</h3>
<ul>
<li><strong>Authors: </strong>Sandra C. Sandoval, Christabel Acquaye, Kwesi Cobbina, Mohammad Nayeem Teli, Hal Daumé III</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04564">https://arxiv.org/abs/2502.04564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04564">https://arxiv.org/pdf/2502.04564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04564]] My LLM might Mimic AAE -- But When Should it?(https://arxiv.org/abs/2502.04564)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We examine the representation of African American English (AAE) in large language models (LLMs), exploring (a) the perceptions Black Americans have of how effective these technologies are at producing authentic AAE, and (b) in what contexts Black Americans find this desirable. Through both a survey of Black Americans ($n=$ 104) and annotation of LLM-produced AAE by Black Americans ($n=$ 228), we find that Black Americans favor choice and autonomy in determining when AAE is appropriate in LLM output. They tend to prefer that LLMs default to communicating in Mainstream U.S. English in formal settings, with greater interest in AAE production in less formal settings. When LLMs were appropriately prompted and provided in context examples, our participants found their outputs to have a level of AAE authenticity on par with transcripts of Black American speech. Select code and data for our project can be found here: this https URL</li>
</ul>

<h3>Title: Private Federated Learning In Real World Application -- A Case Study</h3>
<ul>
<li><strong>Authors: </strong>An Ji, Bortik Bandyopadhyay, Congzheng Song, Natarajan Krishnaswami, Prabal Vashish, Rigel Smiroldo, Isabel Litton, Sayantan Mahinder, Mona Chitnis, Andrew W Hill</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04565">https://arxiv.org/abs/2502.04565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04565">https://arxiv.org/pdf/2502.04565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04565]] Private Federated Learning In Real World Application -- A Case Study(https://arxiv.org/abs/2502.04565)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>This paper presents an implementation of machine learning model training using private federated learning (PFL) on edge devices. We introduce a novel framework that uses PFL to address the challenge of training a model using users' private data. The framework ensures that user data remain on individual devices, with only essential model updates transmitted to a central server for aggregation with privacy guarantees. We detail the architecture of our app selection model, which incorporates a neural network with attention mechanisms and ambiguity handling through uncertainty management. Experiments conducted through off-line simulations and on device training demonstrate the feasibility of our approach in real-world scenarios. Our results show the potential of PFL to improve the accuracy of an app selection model by adapting to changes in user behavior over time, while adhering to privacy standards. The insights gained from this study are important for industries looking to implement PFL, offering a robust strategy for training a predictive model directly on edge devices while ensuring user data privacy.</li>
</ul>

<h3>Title: Zero-shot Meta-learning for Tabular Prediction Tasks with Adversarially Pre-trained Transformer</h3>
<ul>
<li><strong>Authors: </strong>Yulun Wu, Doron L. Bergman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04573">https://arxiv.org/abs/2502.04573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04573">https://arxiv.org/pdf/2502.04573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04573]] Zero-shot Meta-learning for Tabular Prediction Tasks with Adversarially Pre-trained Transformer(https://arxiv.org/abs/2502.04573)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present an Adversarially Pre-trained Transformer (APT) that is able to perform zero-shot meta-learning on tabular prediction tasks without pre-training on any real-world dataset, extending on the recent development of Prior-Data Fitted Networks (PFNs) and TabPFN. Specifically, APT is pre-trained with adversarial synthetic data agents, who continue to shift their underlying data generating distribution and deliberately challenge the model with different synthetic datasets. In addition, we propose a mixture block architecture that is able to handle classification tasks with arbitrary number of classes, addressing the class size limitation -- a crucial weakness of prior deep tabular zero-shot learners. In experiments, we show that our framework matches state-of-the-art performance on small classification tasks without filtering on dataset characteristics such as number of classes and number of missing values, while maintaining an average runtime under one second. On common benchmark dataset suites in both classification and regression, we show that adversarial pre-training was able to enhance TabPFN's performance. In our analysis, we demonstrate that the adversarial synthetic data agents were able to generate a more diverse collection of data compared to the ordinary random generator in TabPFN. In addition, we demonstrate that our mixture block neural design has improved generalizability and greatly accelerated pre-training.</li>
</ul>

<h3>Title: Self-Regulation and Requesting Interventions</h3>
<ul>
<li><strong>Authors: </strong>So Yeon Min, Yue Wu, Jimin Sun, Max Kaufmann, Fahim Tajwar, Yonatan Bisk, Ruslan Salakhutdinov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04576">https://arxiv.org/abs/2502.04576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04576">https://arxiv.org/pdf/2502.04576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04576]] Self-Regulation and Requesting Interventions(https://arxiv.org/abs/2502.04576)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Human intelligence involves metacognitive abilities like self-regulation, recognizing limitations, and seeking assistance only when needed. While LLM Agents excel in many domains, they often lack this awareness. Overconfident agents risk catastrophic failures, while those that seek help excessively hinder efficiency. A key challenge is enabling agents with a limited intervention budget $C$ is to decide when to request assistance. In this paper, we propose an offline framework that trains a "helper" policy to request interventions, such as more powerful models or test-time compute, by combining LLM-based process reward models (PRMs) with tabular reinforcement learning. Using state transitions collected offline, we score optimal intervention timing with PRMs and train the helper model on these labeled trajectories. This offline approach significantly reduces costly intervention calls during training. Furthermore, the integration of PRMs with tabular RL enhances robustness to off-policy data while avoiding the inefficiencies of deep RL. We empirically find that our method delivers optimal helper behavior.</li>
</ul>

<h3>Title: Position-aware Automatic Circuit Discovery</h3>
<ul>
<li><strong>Authors: </strong>Tal Haklay, Hadas Orgad, David Bau, Aaron Mueller, Yonatan Belinkov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04577">https://arxiv.org/abs/2502.04577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04577">https://arxiv.org/pdf/2502.04577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04577]] Position-aware Automatic Circuit Discovery(https://arxiv.org/abs/2502.04577)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A widely used strategy to discover and understand language model mechanisms is circuit analysis. A circuit is a minimal subgraph of a model's computation graph that executes a specific task. We identify a gap in existing circuit discovery methods: they assume circuits are position-invariant, treating model components as equally relevant across input positions. This limits their ability to capture cross-positional interactions or mechanisms that vary across positions. To address this gap, we propose two improvements to incorporate positionality into circuits, even on tasks containing variable-length examples. First, we extend edge attribution patching, a gradient-based method for circuit discovery, to differentiate between token positions. Second, we introduce the concept of a dataset schema, which defines token spans with similar semantics across examples, enabling position-aware circuit discovery in datasets with variable length examples. We additionally develop an automated pipeline for schema generation and application using large language models. Our approach enables fully automated discovery of position-sensitive circuits, yielding better trade-offs between circuit size and faithfulness compared to prior work.</li>
</ul>

<h3>Title: Technical Debt in In-Context Learning: Diminishing Efficiency in Long Context</h3>
<ul>
<li><strong>Authors: </strong>Taejong Joo, Diego Klabjan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04580">https://arxiv.org/abs/2502.04580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04580">https://arxiv.org/pdf/2502.04580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04580]] Technical Debt in In-Context Learning: Diminishing Efficiency in Long Context(https://arxiv.org/abs/2502.04580)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers have demonstrated remarkable in-context learning (ICL) capabilities, adapting to new tasks by simply conditioning on demonstrations without parameter updates. Compelling empirical and theoretical evidence suggests that ICL, as a general-purpose learner, could outperform task-specific models. However, it remains unclear to what extent the transformers optimally learn in-context compared to principled learning algorithms. To bridge this gap, we introduce a new framework for quantifying optimality of ICL as a learning algorithm in stylized settings. Our findings reveal a striking dichotomy: while ICL initially matches the efficiency of a Bayes optimal estimator, its efficiency significantly deteriorates in long context. Through an information-theoretic analysis, we show that the diminishing efficiency is inherent to ICL. These results clarify the trade-offs in adopting ICL as a universal problem solver, motivating a new generation of on-the-fly adaptive methods without the diminishing efficiency.</li>
</ul>

<h3>Title: CAMEF: Causal-Augmented Multi-Modality Event-Driven Financial Forecasting by Integrating Time Series Patterns and Salient Macroeconomic Announcements</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhang, Wenbo Yang, Jun Wang, Qiang Ma, Jie Xiong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04592">https://arxiv.org/abs/2502.04592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04592">https://arxiv.org/pdf/2502.04592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04592]] CAMEF: Causal-Augmented Multi-Modality Event-Driven Financial Forecasting by Integrating Time Series Patterns and Salient Macroeconomic Announcements(https://arxiv.org/abs/2502.04592)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurately forecasting the impact of macroeconomic events is critical for investors and policymakers. Salient events like monetary policy decisions and employment reports often trigger market movements by shaping expectations of economic growth and risk, thereby establishing causal relationships between events and market behavior. Existing forecasting methods typically focus either on textual analysis or time-series modeling, but fail to capture the multi-modal nature of financial markets and the causal relationship between events and price movements. To address these gaps, we propose CAMEF (Causal-Augmented Multi-Modality Event-Driven Financial Forecasting), a multi-modality framework that effectively integrates textual and time-series data with a causal learning mechanism and an LLM-based counterfactual event augmentation technique for causal-enhanced financial forecasting. Our contributions include: (1) a multi-modal framework that captures causal relationships between policy texts and historical price data; (2) a new financial dataset with six types of macroeconomic releases from 2008 to April 2024, and high-frequency real trading data for five key U.S. financial assets; and (3) an LLM-based counterfactual event augmentation strategy. We compare CAMEF to state-of-the-art transformer-based time-series and multi-modal baselines, and perform ablation studies to validate the effectiveness of the causal learning mechanism and event types.</li>
</ul>

<h3>Title: The $\alpha$-Alternator: Dynamic Adaptation To Varying Noise Levels In Sequences Using The Vendi Score For Improved Robustness and Performance</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Reza Rezaei, Adji Bousso Dieng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04593">https://arxiv.org/abs/2502.04593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04593">https://arxiv.org/pdf/2502.04593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04593]] The $\alpha$-Alternator: Dynamic Adaptation To Varying Noise Levels In Sequences Using The Vendi Score For Improved Robustness and Performance(https://arxiv.org/abs/2502.04593)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Current state-of-the-art dynamical models, such as Mamba, assume the same level of noisiness for all elements of a given sequence, which limits their performance on noisy temporal data. In this paper, we introduce the $\alpha$-Alternator, a novel generative model for time-dependent data that dynamically adapts to the complexity introduced by varying noise levels in sequences. The $\alpha$-Alternator leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to adjust, at each time step $t$, the influence of the sequence element at time $t$ and the latent representation of the dynamics up to that time step on the predicted future dynamics. This influence is captured by a parameter that is learned and shared across all sequences in a given dataset. The sign of this parameter determines the direction of influence. A negative value indicates a noisy dataset, where a sequence element that increases the VS is considered noisy, and the model relies more on the latent history when processing that element. Conversely, when the parameter is positive, a sequence element that increases the VS is considered informative, and the $\alpha$-Alternator relies more on this new input than on the latent history when updating its predicted latent dynamics. The $\alpha$-Alternator is trained using a combination of observation masking and Alternator loss minimization. Masking simulates varying noise levels in sequences, enabling the model to be more robust to these fluctuations and improving its performance in trajectory prediction, imputation, and forecasting. Our experimental results demonstrate that the $\alpha$-Alternator outperforms both Alternators and state-of-the-art state-space models across neural decoding and time-series forecasting benchmarks.</li>
</ul>

<h3>Title: LATTEO: A Framework to Support Learning Asynchronously Tempered with Trusted Execution and Obfuscation</h3>
<ul>
<li><strong>Authors: </strong>Abhinav Kumar, George Torres, Noah Guzinski, Gaurav Panwar, Reza Tourani, Satyajayant Misra, Marcin Spoczynski, Mona Vij, Nageen Himayat</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04601">https://arxiv.org/abs/2502.04601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04601">https://arxiv.org/pdf/2502.04601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04601]] LATTEO: A Framework to Support Learning Asynchronously Tempered with Trusted Execution and Obfuscation(https://arxiv.org/abs/2502.04601)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>The privacy vulnerabilities of the federated learning (FL) paradigm, primarily caused by gradient leakage, have prompted the development of various defensive measures. Nonetheless, these solutions have predominantly been crafted for and assessed in the context of synchronous FL systems, with minimal focus on asynchronous FL. This gap arises in part due to the unique challenges posed by the asynchronous setting, such as the lack of coordinated updates, increased variability in client participation, and the potential for more severe privacy risks. These concerns have stymied the adoption of asynchronous FL. In this work, we first demonstrate the privacy vulnerabilities of asynchronous FL through a novel data reconstruction attack that exploits gradient updates to recover sensitive client data. To address these vulnerabilities, we propose a privacy-preserving framework that combines a gradient obfuscation mechanism with Trusted Execution Environments (TEEs) for secure asynchronous FL aggregation at the network edge. To overcome the limitations of conventional enclave attestation, we introduce a novel data-centric attestation mechanism based on Multi-Authority Attribute-Based Encryption. This mechanism enables clients to implicitly verify TEE-based aggregation services, effectively handle on-demand client participation, and scale seamlessly with an increasing number of asynchronous connections. Our gradient obfuscation mechanism reduces the structural similarity index of data reconstruction by 85% and increases reconstruction error by 400%, while our framework improves attestation efficiency by lowering average latency by up to 1500% compared to RA-TLS, without additional overhead.</li>
</ul>

<h3>Title: Extracting and Understanding the Superficial Knowledge in Alignment</h3>
<ul>
<li><strong>Authors: </strong>Runjin Chen, Gabriel Jacob Perin, Xuxi Chen, Xilun Chen, Yan Han, Nina S. T. Hirata, Junyuan Hong, Bhavya Kailkhura</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04602">https://arxiv.org/abs/2502.04602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04602">https://arxiv.org/pdf/2502.04602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04602]] Extracting and Understanding the Superficial Knowledge in Alignment(https://arxiv.org/abs/2502.04602)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Alignment of large language models (LLMs) with human values and preferences, often achieved through fine-tuning based on human feedback, is essential for ensuring safe and responsible AI behaviors. However, the process typically requires substantial data and computation resources. Recent studies have revealed that alignment might be attainable at lower costs through simpler methods, such as in-context learning. This leads to the question: Is alignment predominantly superficial? In this paper, we delve into this question and provide a quantitative analysis. We formalize the concept of superficial knowledge, defining it as knowledge that can be acquired through easily token restyling, without affecting the model's ability to capture underlying causal relationships between tokens. We propose a method to extract and isolate superficial knowledge from aligned models, focusing on the shallow modifications to the final token selection process. By comparing models augmented only with superficial knowledge to fully aligned models, we quantify the superficial portion of alignment. Our findings reveal that while superficial knowledge constitutes a significant portion of alignment, particularly in safety and detoxification tasks, it is not the whole story. Tasks requiring reasoning and contextual understanding still rely on deeper knowledge. Additionally, we demonstrate two practical advantages of isolated superficial knowledge: (1) it can be transferred between models, enabling efficient offsite alignment of larger models using extracted superficial knowledge from smaller models, and (2) it is recoverable, allowing for the restoration of alignment in compromised models without sacrificing performance.</li>
</ul>

<h3>Title: Neural Clustering for Prefractured Mesh Generation in Real-time Object Destruction</h3>
<ul>
<li><strong>Authors: </strong>Seunghwan Kim, Sunha Park, Seungkyu Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04615">https://arxiv.org/abs/2502.04615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04615">https://arxiv.org/pdf/2502.04615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04615]] Neural Clustering for Prefractured Mesh Generation in Real-time Object Destruction(https://arxiv.org/abs/2502.04615)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Prefracture method is a practical implementation for real-time object destruction that is hardly achievable within performance constraints, but can produce unrealistic results due to its heuristic nature. To mitigate it, we approach the clustering of prefractured mesh generation as an unordered segmentation on point cloud data, and propose leveraging the deep neural network trained on a physics-based dataset. Our novel paradigm successfully predicts the structural weakness of object that have been limited, exhibiting ready-to-use results with remarkable quality.</li>
</ul>

<h3>Title: HetSSNet: Spatial-Spectral Heterogeneous Graph Learning Network for Panchromatic and Multispectral Images Fusion</h3>
<ul>
<li><strong>Authors: </strong>Mengting Ma, Yizhen Jiang, Mengjiao Zhao, Jiaxin Li, Wei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04623">https://arxiv.org/abs/2502.04623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04623">https://arxiv.org/pdf/2502.04623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04623]] HetSSNet: Spatial-Spectral Heterogeneous Graph Learning Network for Panchromatic and Multispectral Images Fusion(https://arxiv.org/abs/2502.04623)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Remote sensing pansharpening aims to reconstruct spatial-spectral properties during the fusion of panchromatic (PAN) images and low-resolution multi-spectral (LR-MS) images, finally generating the high-resolution multi-spectral (HR-MS) images. In the mainstream modeling strategies, i.e., CNN and Transformer, the input images are treated as the equal-sized grid of pixels in the Euclidean space. They have limitations in facing remote sensing images with irregular ground objects. Graph is the more flexible structure, however, there are two major challenges when modeling spatial-spectral properties with graph: \emph{1) constructing the customized graph structure for spatial-spectral relationship priors}; \emph{2) learning the unified spatial-spectral representation through the graph}. To address these challenges, we propose the spatial-spectral heterogeneous graph learning network, named \textbf{HetSSNet}. Specifically, HetSSNet initially constructs the heterogeneous graph structure for pansharpening, which explicitly describes pansharpening-specific relationships. Subsequently, the basic relationship pattern generation module is designed to extract the multiple relationship patterns from the heterogeneous graph. Finally, relationship pattern aggregation module is exploited to collaboratively learn unified spatial-spectral representation across different relationships among nodes with adaptive importance learning from local and global perspectives. Extensive experiments demonstrate the significant superiority and generalization of HetSSNet.</li>
</ul>

<h3>Title: Phonetic Reconstruction of the Consonant System of Middle Chinese via Mixed Integer Optimization</h3>
<ul>
<li><strong>Authors: </strong>Weiwei Sun (1), Xiaoxi Luo (2) ((1) Department of Computer Science and Technology, University of Cambridge, (2) Yuanpei College, Peking University)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04625">https://arxiv.org/abs/2502.04625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04625">https://arxiv.org/pdf/2502.04625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04625]] Phonetic Reconstruction of the Consonant System of Middle Chinese via Mixed Integer Optimization(https://arxiv.org/abs/2502.04625)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper is concerned with phonetic reconstruction of the consonant system of Middle Chinese. We propose to cast the problem as a Mixed Integer Programming problem, which is able to automatically explore homophonic information from ancient rhyme dictionaries and phonetic information from modern Chinese dialects, the descendants of Middle Chinese. Numerical evaluation on a wide range of synthetic and real data demonstrates the effectiveness and robustness of the new method. We apply the method to information from Guangyun and 20 modern Chinese dialects to obtain a new phonetic reconstruction result. A linguistically-motivated discussion of this result is also provided.</li>
</ul>

<h3>Title: AIQViT: Architecture-Informed Post-Training Quantization for Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Runqing Jiang, Ye Zhang, Longguang Wang, Pengpeng Yu, Yulan Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04628">https://arxiv.org/abs/2502.04628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04628">https://arxiv.org/pdf/2502.04628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04628]] AIQViT: Architecture-Informed Post-Training Quantization for Vision Transformers(https://arxiv.org/abs/2502.04628)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Post-training quantization (PTQ) has emerged as a promising solution for reducing the storage and computational cost of vision transformers (ViTs). Recent advances primarily target at crafting quantizers to deal with peculiar activations characterized by ViTs. However, most existing methods underestimate the information loss incurred by weight quantization, resulting in significant performance deterioration, particularly in low-bit cases. Furthermore, a common practice in quantizing post-Softmax activations of ViTs is to employ logarithmic transformations, which unfortunately prioritize less informative values around zero. This approach introduces additional redundancies, ultimately leading to suboptimal quantization efficacy. To handle these, this paper proposes an innovative PTQ method tailored for ViTs, termed AIQViT (Architecture-Informed Post-training Quantization for ViTs). First, we design an architecture-informed low rank compensation mechanism, wherein learnable low-rank weights are introduced to compensate for the degradation caused by weight quantization. Second, we design a dynamic focusing quantizer to accommodate the unbalanced distribution of post-Softmax activations, which dynamically selects the most valuable interval for higher quantization resolution. Extensive experiments on five vision tasks, including image classification, object detection, instance segmentation, point cloud classification, and point cloud part segmentation, demonstrate the superiority of AIQViT over state-of-the-art PTQ methods.</li>
</ul>

<h3>Title: An Empirical Study of Code Obfuscation Practices in the Google Play Store</h3>
<ul>
<li><strong>Authors: </strong>Akila Niroshan, Suranga Seneviratne, Aruna Seneviratne</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04636">https://arxiv.org/abs/2502.04636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04636">https://arxiv.org/pdf/2502.04636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04636]] An Empirical Study of Code Obfuscation Practices in the Google Play Store(https://arxiv.org/abs/2502.04636)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>The Android ecosystem is vulnerable to issues such as app repackaging, counterfeiting, and piracy, threatening both developers and users. To mitigate these risks, developers often employ code obfuscation techniques. However, while effective in protecting legitimate applications, obfuscation also hinders security investigations as it is often exploited for malicious purposes. As such, it is important to understand code obfuscation practices in Android apps. In this paper, we analyze over 500,000 Android APKs from Google Play, spanning an eight-year period, to investigate the evolution and prevalence of code obfuscation techniques. First, we propose a set of classifiers to detect obfuscated code, tools, and techniques and then conduct a longitudinal analysis to identify trends. Our results show a 13% increase in obfuscation from 2016 to 2023, with ProGuard and Allatori as the most commonly used tools. We also show that obfuscation is more prevalent in top-ranked apps and gaming genres such as Casino apps. To our knowledge, this is the first large-scale study of obfuscation adoption in the Google Play Store, providing insights for developers and security analysts.</li>
</ul>

<h3>Title: Confidence Elicitation: A New Attack Vector for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Brian Formento, Chuan Sheng Foo, See-Kiong Ng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04643">https://arxiv.org/abs/2502.04643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04643">https://arxiv.org/pdf/2502.04643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04643]] Confidence Elicitation: A New Attack Vector for Large Language Models(https://arxiv.org/abs/2502.04643)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>A fundamental issue in deep learning has been adversarial robustness. As these systems have scaled, such issues have persisted. Currently, large language models (LLMs) with billions of parameters suffer from adversarial attacks just like their earlier, smaller counterparts. However, the threat models have changed. Previously, having gray-box access, where input embeddings or output logits/probabilities were visible to the user, might have been reasonable. However, with the introduction of closed-source models, no information about the model is available apart from the generated output. This means that current black-box attacks can only utilize the final prediction to detect if an attack is successful. In this work, we investigate and demonstrate the potential of attack guidance, akin to using output probabilities, while having only black-box access in a classification setting. This is achieved through the ability to elicit confidence from the model. We empirically show that the elicited confidence is calibrated and not hallucinated for current LLMs. By minimizing the elicited confidence, we can therefore increase the likelihood of misclassification. Our new proposed paradigm demonstrates promising state-of-the-art results on three datasets across two models (LLaMA-3-8B-Instruct and Mistral-7B-Instruct-V0.3) when comparing our technique to existing hard-label black-box attack methods that introduce word-level substitutions.</li>
</ul>

<h3>Title: Importance Sampling via Score-based Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Heasung Kim, Taekyun Lee, Hyeji Kim, Gustavo de Veciana</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04646">https://arxiv.org/abs/2502.04646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04646">https://arxiv.org/pdf/2502.04646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04646]] Importance Sampling via Score-based Generative Models(https://arxiv.org/abs/2502.04646)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Importance sampling, which involves sampling from a probability density function (PDF) proportional to the product of an importance weight function and a base PDF, is a powerful technique with applications in variance reduction, biased or customized sampling, data augmentation, and beyond. Inspired by the growing availability of score-based generative models (SGMs), we propose an entirely training-free Importance sampling framework that relies solely on an SGM for the base PDF. Our key innovation is realizing the importance sampling process as a backward diffusion process, expressed in terms of the score function of the base PDF and the specified importance weight function--both readily available--eliminating the need for any additional training. We conduct a thorough analysis demonstrating the method's scalability and effectiveness across diverse datasets and tasks, including importance sampling for industrial and natural images with neural importance weight functions. The training-free aspect of our method is particularly compelling in real-world scenarios where a single base distribution underlies multiple biased sampling tasks, each requiring a different importance weight function. To the best of our knowledge our approach is the first importance sampling framework to achieve this.</li>
</ul>

<h3>Title: Toward Automated Potential Primary Asset Identification in Verilog Designs</h3>
<ul>
<li><strong>Authors: </strong>Subroto Kumer Deb Nath, Benjamin Tan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04648">https://arxiv.org/abs/2502.04648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04648">https://arxiv.org/pdf/2502.04648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04648]] Toward Automated Potential Primary Asset Identification in Verilog Designs(https://arxiv.org/abs/2502.04648)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>With greater design complexity, the challenge to anticipate and mitigate security issues provides more responsibility for the designer. As hardware provides the foundation of a secure system, we need tools and techniques that support engineers to improve trust and help them address security concerns. Knowing the security assets in a design is fundamental to downstream security analyses, such as threat modeling, weakness identification, and verification. This paper proposes an automated approach for the initial identification of potential security assets in a Verilog design. Taking inspiration from manual asset identification methodologies, we analyze open-source hardware designs in three IP families and identify patterns and commonalities likely to indicate structural assets. Through iterative refinement, we provide a potential set of primary security assets and thus help to reduce the manual search space.</li>
</ul>

<h3>Title: $\mathsf{CRATE}$: Cross-Rollup Atomic Transaction Execution</h3>
<ul>
<li><strong>Authors: </strong>Ioannis Kaklamanis, Fan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04659">https://arxiv.org/abs/2502.04659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04659">https://arxiv.org/pdf/2502.04659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04659]] $\mathsf{CRATE}$: Cross-Rollup Atomic Transaction Execution(https://arxiv.org/abs/2502.04659)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Blockchains have revolutionized decentralized applications, with composability enabling atomic, trustless interactions across smart contracts. However, layer 2 (L2) scalability solutions like rollups introduce fragmentation and hinder composability. Current cross-chain protocols, including atomic swaps, bridges, and shared sequencers, lack the necessary coordination mechanisms or rely on trust assumptions, and are thus not sufficient to support full cross-rollup composability. This paper presents $\mathsf{CRATE}$, a secure protocol for cross-rollup composability that ensures all-or-nothing and serializable execution of cross-rollup transactions (CRTs). $\mathsf{CRATE}$ supports rollups on distinct layer 1 (L1) chains, achieves finality in 4 rounds on L1, and only relies on the underlying L1s and the liveness of L2s. We introduce two formal models for CRTs, define atomicity within them, and formally prove the security of $\mathsf{CRATE}$. We also provide an implementation of $\mathsf{CRATE}$ along with a cross-rollup flash loan application; our experiments demonstrate that $\mathsf{CRATE}$ is practical in terms of gas usage on L1.</li>
</ul>

<h3>Title: Adversarially-Robust TD Learning with Markovian Data: Finite-Time Rates and Fundamental Limits</h3>
<ul>
<li><strong>Authors: </strong>Sreejeet Maity, Aritra Mitra</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04662">https://arxiv.org/abs/2502.04662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04662">https://arxiv.org/pdf/2502.04662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04662]] Adversarially-Robust TD Learning with Markovian Data: Finite-Time Rates and Fundamental Limits(https://arxiv.org/abs/2502.04662)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>One of the most basic problems in reinforcement learning (RL) is policy evaluation: estimating the long-term return, i.e., value function, corresponding to a given fixed policy. The celebrated Temporal Difference (TD) learning algorithm addresses this problem, and recent work has investigated finite-time convergence guarantees for this algorithm and variants thereof. However, these guarantees hinge on the reward observations being always generated from a well-behaved (e.g., sub-Gaussian) true reward distribution. Motivated by harsh, real-world environments where such an idealistic assumption may no longer hold, we revisit the policy evaluation problem from the perspective of adversarial robustness. In particular, we consider a Huber-contaminated reward model where an adversary can arbitrarily corrupt each reward sample with a small probability $\epsilon$. Under this observation model, we first show that the adversary can cause the vanilla TD algorithm to converge to any arbitrary value function. We then develop a novel algorithm called Robust-TD and prove that its finite-time guarantees match that of vanilla TD with linear function approximation up to a small $O(\epsilon)$ term that captures the effect of corruption. We complement this result with a minimax lower bound, revealing that such an additive corruption-induced term is unavoidable. To our knowledge, these results are the first of their kind in the context of adversarial robustness of stochastic approximation schemes driven by Markov noise. The key new technical tool that enables our results is an analysis of the Median-of-Means estimator with corrupted, time-correlated data that might be of independent interest to the literature on robust statistics.</li>
</ul>

<h3>Title: Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization</h3>
<ul>
<li><strong>Authors: </strong>Xinhao Yao, Ruifeng Ren, Yun Liao, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04667">https://arxiv.org/abs/2502.04667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04667">https://arxiv.org/pdf/2502.04667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04667]] Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization(https://arxiv.org/abs/2502.04667)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Training large language models (LLMs) with high-quality Chain-of-Thought (CoT) annotations has become a widely adopted strategy due to its significant enhancement of reasoning capabilities. To fully comprehend this approach, two questions naturally arise: (Q1) What advantages does training with CoT offer compared to training without CoT? (Q2) If there are advantages, what are the underlying mechanisms of explicit CoT training? Analyzing the advantages and mechanisms of CoT training is challenging due to the many factors involved. To address this, we conduct a detailed analysis using clear and controllable data distributions and, for the first time, reveal that CoT training offers the following advantages: (1) Training with CoT markedly improves reasoning generalization, extending it from in-distribution (ID) to both ID and out-of-distribution (OOD) scenarios, while also speeding up convergence; (2) Even when training with CoT includes a certain range of erroneous reasoning steps, it still enables the model to learn reasoning patterns, leading to systematic generalization. We further explore the underlying mechanisms from a circuit perspective: (1) The data distribution (e.g., ratio $\lambda$ and pattern) plays a crucial role in influencing the model's systematic generalization; (2) CoT training (with two-hop facts) internalizes reasoning into a two-stage generalizing circuit, where the number of stages corresponds to the explicit reasoning steps during training. Our findings elucidate the mechanisms underlying explicit CoT training and offer critical insights into tuning strategies for LLMs to achieve robust generalization.</li>
</ul>

<h3>Title: A Comprehensive Review on Noise Control of Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Zhehao Guo, Jiedong Lang, Shuyu Huang, Yunfei Gao, Xintong Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04669">https://arxiv.org/abs/2502.04669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04669">https://arxiv.org/pdf/2502.04669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04669]] A Comprehensive Review on Noise Control of Diffusion Model(https://arxiv.org/abs/2502.04669)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently emerged as powerful generative frameworks for producing high-quality images. A pivotal component of these models is the noise schedule, which governs the rate of noise injection during the diffusion process. Since the noise schedule substantially influences sampling quality and training quality, understanding its design and implications is crucial. In this discussion, various noise schedules are examined, and their distinguishing features and performance characteristics are highlighted.</li>
</ul>

<h3>Title: CCS: Controllable and Constrained Sampling with Diffusion Models via Initial Noise Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Bowen Song, Zecheng Zhang, Zhaoxu Luo, Jason Hu, Wei Yuan, Jing Jia, Zhengxu Tang, Guanyang Wang, Liyue Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04670">https://arxiv.org/abs/2502.04670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04670">https://arxiv.org/pdf/2502.04670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04670]] CCS: Controllable and Constrained Sampling with Diffusion Models via Initial Noise Perturbation(https://arxiv.org/abs/2502.04670)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as powerful tools for generative tasks, producing high-quality outputs across diverse domains. However, how the generated data responds to the initial noise perturbation in diffusion models remains under-explored, which hinders understanding the controllability of the sampling process. In this work, we first observe an interesting phenomenon: the relationship between the change of generation outputs and the scale of initial noise perturbation is highly linear through the diffusion ODE sampling. Then we provide both theoretical and empirical study to justify this linearity property of this input-output (noise-generation data) relationship. Inspired by these new insights, we propose a novel Controllable and Constrained Sampling method (CCS) together with a new controller algorithm for diffusion models to sample with desired statistical properties while preserving good sample quality. We perform extensive experiments to compare our proposed sampling approach with other methods on both sampling controllability and sampled data quality. Results show that our CCS method achieves more precisely controlled sampling while maintaining superior sample quality and diversity.</li>
</ul>

<h3>Title: Mechanistic Understandings of Representation Vulnerabilities and Engineering Robust Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Chashi Mahiul Islam, Samuel Jacob Chacko, Mao Nishino, Xiuwen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04679">https://arxiv.org/abs/2502.04679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04679">https://arxiv.org/pdf/2502.04679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04679]] Mechanistic Understandings of Representation Vulnerabilities and Engineering Robust Vision Transformers(https://arxiv.org/abs/2502.04679)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>While transformer-based models dominate NLP and vision applications, their underlying mechanisms to map the input space to the label space semantically are not well understood. In this paper, we study the sources of known representation vulnerabilities of vision transformers (ViT), where perceptually identical images can have very different representations and semantically unrelated images can have the same representation. Our analysis indicates that imperceptible changes to the input can result in significant representation changes, particularly in later layers, suggesting potential instabilities in the performance of ViTs. Our comprehensive study reveals that adversarial effects, while subtle in early layers, propagate and amplify through the network, becoming most pronounced in middle to late layers. This insight motivates the development of NeuroShield-ViT, a novel defense mechanism that strategically neutralizes vulnerable neurons in earlier layers to prevent the cascade of adversarial effects. We demonstrate NeuroShield-ViT's effectiveness across various attacks, particularly excelling against strong iterative attacks, and showcase its remarkable zero-shot generalization capabilities. Without fine-tuning, our method achieves a competitive accuracy of 77.8% on adversarial examples, surpassing conventional robustness methods. Our results shed new light on how adversarial effects propagate through ViT layers, while providing a promising approach to enhance the robustness of vision transformers against adversarial attacks. Additionally, they provide a promising approach to enhance the robustness of vision transformers against adversarial attacks.</li>
</ul>

<h3>Title: Performance Evaluation of Image Enhancement Techniques on Transfer Learning for Touchless Fingerprint Recognition</h3>
<ul>
<li><strong>Authors: </strong>S Sreehari, Dilavar P D, S M Anzar, Alavikunhu Panthakkan, Saad Ali Amin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04680">https://arxiv.org/abs/2502.04680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04680">https://arxiv.org/pdf/2502.04680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04680]] Performance Evaluation of Image Enhancement Techniques on Transfer Learning for Touchless Fingerprint Recognition(https://arxiv.org/abs/2502.04680)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>Fingerprint recognition remains one of the most reliable biometric technologies due to its high accuracy and uniqueness. Traditional systems rely on contact-based scanners, which are prone to issues such as image degradation from surface contamination and inconsistent user interaction. To address these limitations, contactless fingerprint recognition has emerged as a promising alternative, providing non-intrusive and hygienic authentication. This study evaluates the impact of image enhancement tech-niques on the performance of pre-trained deep learning models using transfer learning for touchless fingerprint recognition. The IIT-Bombay Touchless and Touch-Based Fingerprint Database, containing data from 200 subjects, was employed to test the per-formance of deep learning architectures such as VGG-16, VGG-19, Inception-V3, and ResNet-50. Experimental results reveal that transfer learning methods with fingerprint image enhance-ment (indirect method) significantly outperform those without enhancement (direct method). Specifically, VGG-16 achieved an accuracy of 98% in training and 93% in testing when using the enhanced images, demonstrating superior performance compared to the direct method. This paper provides a detailed comparison of the effectiveness of image enhancement in improving the accuracy of transfer learning models for touchless fingerprint recognition, offering key insights for developing more efficient biometric systems.</li>
</ul>

<h3>Title: G2PDiffusion: Genotype-to-Phenotype Prediction with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mengdi Liu, Zhangyang Gao, Hong Chang, Stan Z. Li, Shiguang Shan, Xinlin Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04684">https://arxiv.org/abs/2502.04684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04684">https://arxiv.org/pdf/2502.04684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04684]] G2PDiffusion: Genotype-to-Phenotype Prediction with Diffusion Models(https://arxiv.org/abs/2502.04684)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discovering the genotype-phenotype relationship is crucial for genetic engineering, which will facilitate advances in fields such as crop breeding, conservation biology, and personalized medicine. Current research usually focuses on single species and small datasets due to limitations in phenotypic data collection, especially for traits that require visual assessments or physical measurements. Deciphering complex and composite phenotypes, such as morphology, from genetic data at scale remains an open question. To break through traditional generic models that rely on simplified assumptions, this paper introduces G2PDiffusion, the first-of-its-kind diffusion model designed for genotype-to-phenotype generation across multiple species. Specifically, we use images to represent morphological phenotypes across species and redefine phenotype prediction as conditional image generation. To this end, this paper introduces an environment-enhanced DNA sequence conditioner and trains a stable diffusion model with a novel alignment method to improve genotype-to-phenotype consistency. Extensive experiments demonstrate that our approach enhances phenotype prediction accuracy across species, capturing subtle genetic variations that contribute to observable traits.</li>
</ul>

<h3>Title: M-IFEval: Multilingual Instruction-Following Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Antoine Dussolle, Andrea Cardeña Díaz, Shota Sato, Peter Devine</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04688">https://arxiv.org/abs/2502.04688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04688">https://arxiv.org/pdf/2502.04688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04688]] M-IFEval: Multilingual Instruction-Following Evaluation(https://arxiv.org/abs/2502.04688)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Instruction following is a core capability of modern Large language models (LLMs), making evaluating this capability essential to understanding these models. The Instruction Following Evaluation (IFEval) benchmark from the literature does this using objective criteria, offering a measure of LLM performance without subjective AI or human judgement. However, it only includes English instructions, limiting its ability to assess LLMs in other languages. We propose the Multilingual Instruction Following Evaluation (M-IFEval) benchmark, expanding the evaluation to French, Japanese, and Spanish, with both general and language-specific instructions. Applying this benchmark to 8 state-of-the-art LLMs, we find that benchmark performance across languages and instruction types can vary widely, underscoring the importance of a multilingual benchmark for evaluating LLMs in a diverse cultural context.</li>
</ul>

<h3>Title: ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yuwei Yin, Giuseppe Carenini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04689">https://arxiv.org/abs/2502.04689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04689">https://arxiv.org/pdf/2502.04689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04689]] ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning(https://arxiv.org/abs/2502.04689)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiple-choice question-answering (QA) tasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance ("think step by step"). This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT. Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning. Notably, intent analysis plays a vital role in ARR. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR.</li>
</ul>

<h3>Title: Evaluating Text Style Transfer Evaluation: Are There Any Reliable Metrics?</h3>
<ul>
<li><strong>Authors: </strong>Sourabrata Mukherjee, Atul Kr. Ojha, John P. McCrae, Ondrej Dusek</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04718">https://arxiv.org/abs/2502.04718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04718">https://arxiv.org/pdf/2502.04718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04718]] Evaluating Text Style Transfer Evaluation: Are There Any Reliable Metrics?(https://arxiv.org/abs/2502.04718)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text Style Transfer (TST) is the task of transforming a text to reflect a particular style while preserving its original content. Evaluating TST outputs is a multidimensional challenge, requiring the assessment of style transfer accuracy, content preservation, and naturalness. Using human evaluation is ideal but costly, same as in other natural language processing (NLP) tasks, however, automatic metrics for TST have not received as much attention as metrics for, e.g., machine translation or summarization. In this paper, we examine both set of existing and novel metrics from broader NLP tasks for TST evaluation, focusing on two popular subtasks-sentiment transfer and detoxification-in a multilingual context comprising English, Hindi, and Bengali. By conducting meta-evaluation through correlation with human judgments, we demonstrate the effectiveness of these metrics when used individually and in ensembles. Additionally, we investigate the potential of Large Language Models (LLMs) as tools for TST evaluation. Our findings highlight that certain advanced NLP metrics and experimental-hybrid-techniques, provide better insights than existing TST metrics for delivering more accurate, consistent, and reproducible TST evaluations.</li>
</ul>

<h3>Title: Tolerance-Aware Deep Optics</h3>
<ul>
<li><strong>Authors: </strong>Jun Dai, Liqun Chen, Xinge Yang, Yuyao Hu, Jinwei Gu, Tianfan Xue</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04719">https://arxiv.org/abs/2502.04719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04719">https://arxiv.org/pdf/2502.04719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04719]] Tolerance-Aware Deep Optics(https://arxiv.org/abs/2502.04719)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep optics has emerged as a promising approach by co-designing optical elements with deep learning algorithms. However, current research typically overlooks the analysis and optimization of manufacturing and assembly tolerances. This oversight creates a significant performance gap between designed and fabricated optical systems. To address this challenge, we present the first end-to-end tolerance-aware optimization framework that incorporates multiple tolerance types into the deep optics design pipeline. Our method combines physics-informed modelling with data-driven training to enhance optical design by accounting for and compensating for structural deviations in manufacturing and assembly. We validate our approach through computational imaging applications, demonstrating results in both simulations and real-world experiments. We further examine how our proposed solution improves the robustness of optical systems and vision algorithms against tolerances through qualitative and quantitative analyses. Code and additional visual results are available at this http URL.</li>
</ul>

<h3>Title: Can Diffusion Models Learn Hidden Inter-Feature Rules Behind Images?</h3>
<ul>
<li><strong>Authors: </strong>Yujin Han, Andi Han, Wei Huang, Chaochao Lu, Difan Zou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04725">https://arxiv.org/abs/2502.04725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04725">https://arxiv.org/pdf/2502.04725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04725]] Can Diffusion Models Learn Hidden Inter-Feature Rules Behind Images?(https://arxiv.org/abs/2502.04725)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of diffusion models (DMs) in data generation, they exhibit specific failure cases with unsatisfactory outputs. We focus on one such limitation: the ability of DMs to learn hidden rules between image features. Specifically, for image data with dependent features ($\mathbf{x}$) and ($\mathbf{y}$) (e.g., the height of the sun ($\mathbf{x}$) and the length of the shadow ($\mathbf{y}$)), we investigate whether DMs can accurately capture the inter-feature rule ($p(\mathbf{y}|\mathbf{x})$). Empirical evaluations on mainstream DMs (e.g., Stable Diffusion 3.5) reveal consistent failures, such as inconsistent lighting-shadow relationships and mismatched object-mirror reflections. Inspired by these findings, we design four synthetic tasks with strongly correlated features to assess DMs' rule-learning abilities. Extensive experiments show that while DMs can identify coarse-grained rules, they struggle with fine-grained ones. Our theoretical analysis demonstrates that DMs trained via denoising score matching (DSM) exhibit constant errors in learning hidden rules, as the DSM objective is not compatible with rule conformity. To mitigate this, we introduce a common technique - incorporating additional classifier guidance during sampling, which achieves (limited) improvements. Our analysis reveals that the subtle signals of fine-grained rules are challenging for the classifier to capture, providing insights for future exploration.</li>
</ul>

<h3>Title: SelaFD:Seamless Adaptation of Vision Transformer Fine-tuning for Radar-based Human Activity</h3>
<ul>
<li><strong>Authors: </strong>Yijun Wang, Yong Wang, Chendong xu, Shuai Yao, Qisong Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04740">https://arxiv.org/abs/2502.04740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04740">https://arxiv.org/pdf/2502.04740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04740]] SelaFD:Seamless Adaptation of Vision Transformer Fine-tuning for Radar-based Human Activity(https://arxiv.org/abs/2502.04740)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Human Activity Recognition (HAR) such as fall detection has become increasingly critical due to the aging population, necessitating effective monitoring systems to prevent serious injuries and fatalities associated with falls. This study focuses on fine-tuning the Vision Transformer (ViT) model specifically for HAR using radar-based Time-Doppler signatures. Unlike traditional image datasets, these signals present unique challenges due to their non-visual nature and the high degree of similarity among various activities. Directly fine-tuning the ViT with all parameters proves suboptimal for this application. To address this challenge, we propose a novel approach that employs Low-Rank Adaptation (LoRA) fine-tuning in the weight space to facilitate knowledge transfer from pre-trained ViT models. Additionally, to extract fine-grained features, we enhance feature representation through the integration of a serial-parallel adapter in the feature space. Our innovative joint fine-tuning method, tailored for radar-based Time-Doppler signatures, significantly improves HAR accuracy, surpassing existing state-of-the-art methodologies in this domain. Our code is released at this https URL.</li>
</ul>

<h3>Title: Self-Supervised Learning for Pre-training Capsule Networks: Overcoming Medical Imaging Dataset Challenges</h3>
<ul>
<li><strong>Authors: </strong>Heba El-Shimy, Hind Zantout, Michael A. Lones, Neamat El Gayar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04748">https://arxiv.org/abs/2502.04748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04748">https://arxiv.org/pdf/2502.04748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04748]] Self-Supervised Learning for Pre-training Capsule Networks: Overcoming Medical Imaging Dataset Challenges(https://arxiv.org/abs/2502.04748)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Deep learning techniques are increasingly being adopted in diagnostic medical imaging. However, the limited availability of high-quality, large-scale medical datasets presents a significant challenge, often necessitating the use of transfer learning approaches. This study investigates self-supervised learning methods for pre-training capsule networks in polyp diagnostics for colon cancer. We used the PICCOLO dataset, comprising 3,433 samples, which exemplifies typical challenges in medical datasets: small size, class imbalance, and distribution shifts between data splits. Capsule networks offer inherent interpretability due to their architecture and inter-layer information routing mechanism. However, their limited native implementation in mainstream deep learning frameworks and the lack of pre-trained versions pose a significant challenge. This is particularly true if aiming to train them on small medical datasets, where leveraging pre-trained weights as initial parameters would be beneficial. We explored two auxiliary self-supervised learning tasks, colourisation and contrastive learning, for capsule network pre-training. We compared self-supervised pre-trained models against alternative initialisation strategies. Our findings suggest that contrastive learning and in-painting techniques are suitable auxiliary tasks for self-supervised learning in the medical domain. These techniques helped guide the model to capture important visual features that are beneficial for the downstream task of polyp classification, increasing its accuracy by 5.26% compared to other weight initialisation methods.</li>
</ul>

<h3>Title: Assessing the Aftermath: the Effects of a Global Takedown against DDoS-for-hire Services</h3>
<ul>
<li><strong>Authors: </strong>Anh V. Vu, Ben Collier, Daniel R. Thomas, John Kristoff, Richard Clayton, Alice Hutchings</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04753">https://arxiv.org/abs/2502.04753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04753">https://arxiv.org/pdf/2502.04753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04753]] Assessing the Aftermath: the Effects of a Global Takedown against DDoS-for-hire Services(https://arxiv.org/abs/2502.04753)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, fair</a></li>
<li><strong>Abstract: </strong>Law enforcement and private-sector partners have in recent years conducted various interventions to disrupt the DDoS-for-hire market. Drawing on multiple quantitative datasets, including web traffic and ground-truth visits to seized websites, millions of DDoS attack records from academic, industry, and self-reported statistics, along with chats on underground forums and Telegram channels, we assess the effects of an ongoing global intervention against DDoS-for-hire services since December 2022. This is the most extensive booter takedown to date conducted, combining targeting infrastructure with digital influence tactics in a concerted effort by law enforcement across several countries with two waves of website takedowns and the use of deceptive domains. We found over half of the seized sites in the first wave returned within a median of one day, while all booters seized in the second wave returned within a median of two days. Re-emerged booter domains, despite closely resembling old ones, struggled to attract visitors (80-90% traffic reduction). While the first wave cut the global DDoS attack volume by 20-40% with a statistically significant effect specifically on UDP-based DDoS attacks (commonly attributed to booters), the impact of the second wave appeared minimal. Underground discussions indicated a cumulative impact, leading to changes in user perceptions of safety and causing some operators to leave the market. Despite the extensive intervention efforts, all DDoS datasets consistently suggest that the illicit market is fairly resilient, with an overall short-lived effect on the global DDoS attack volume lasting for at most only around six weeks.</li>
</ul>

<h3>Title: Concept Navigation and Classification via Open Source Large Language Model Processing</h3>
<ul>
<li><strong>Authors: </strong>Maël Kubli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04756">https://arxiv.org/abs/2502.04756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04756">https://arxiv.org/pdf/2502.04756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04756]] Concept Navigation and Classification via Open Source Large Language Model Processing(https://arxiv.org/abs/2502.04756)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a novel methodological framework for detecting and classifying latent constructs, including frames, narratives, and topics, from textual data using Open-Source Large Language Models (LLMs). The proposed hybrid approach combines automated summarization with human-in-the-loop validation to enhance the accuracy and interpretability of construct identification. By employing iterative sampling coupled with expert refinement, the framework guarantees methodological robustness and ensures conceptual precision. Applied to diverse data sets, including AI policy debates, newspaper articles on encryption, and the 20 Newsgroups data set, this approach demonstrates its versatility in systematically analyzing complex political discourses, media framing, and topic classification tasks.</li>
</ul>

<h3>Title: ELITE: Enhanced Language-Image Toxicity Evaluation for Safety</h3>
<ul>
<li><strong>Authors: </strong>Wonjun Lee, Doehyeon Lee, Eugene Choi, Sangyoon Yu, Ashkan Yousefpour, Haon Park, Bumsub Ham, Suhyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04757">https://arxiv.org/abs/2502.04757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04757">https://arxiv.org/pdf/2502.04757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04757]] ELITE: Enhanced Language-Image Toxicity Evaluation for Safety(https://arxiv.org/abs/2502.04757)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Current Vision Language Models (VLMs) remain vulnerable to malicious prompts that induce harmful outputs. Existing safety benchmarks for VLMs primarily rely on automated evaluation methods, but these methods struggle to detect implicit harmful content or produce inaccurate evaluations. Therefore, we found that existing benchmarks have low levels of harmfulness, ambiguous data, and limited diversity in image-text pair combinations. To address these issues, we propose the ELITE {\em benchmark}, a high-quality safety evaluation benchmark for VLMs, underpinned by our enhanced evaluation method, the ELITE {\em evaluator}. The ELITE evaluator explicitly incorporates a toxicity score to accurately assess harmfulness in multimodal contexts, where VLMs often provide specific, convincing, but unharmful descriptions of images. We filter out ambiguous and low-quality image-text pairs from existing benchmarks using the ELITE evaluator and generate diverse combinations of safe and unsafe image-text pairs. Our experiments demonstrate that the ELITE evaluator achieves superior alignment with human evaluations compared to prior automated methods, and the ELITE benchmark offers enhanced benchmark quality and diversity. By introducing ELITE, we pave the way for safer, more robust VLMs, contributing essential tools for evaluating and mitigating safety risks in real-world applications.</li>
</ul>

<h3>Title: Enhancing Phishing Email Identification with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Catherine Lee</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04759">https://arxiv.org/abs/2502.04759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04759">https://arxiv.org/pdf/2502.04759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04759]] Enhancing Phishing Email Identification with Large Language Models(https://arxiv.org/abs/2502.04759)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Phishing has long been a common tactic used by cybercriminals and continues to pose a significant threat in today's digital world. When phishing attacks become more advanced and sophisticated, there is an increasing need for effective methods to detect and prevent them. To address the challenging problem of detecting phishing emails, researchers have developed numerous solutions, in particular those based on machine learning (ML) algorithms. In this work, we take steps to study the efficacy of large language models (LLMs) in detecting phishing emails. The experiments show that the LLM achieves a high accuracy rate at high precision; importantly, it also provides interpretable evidence for the decisions.</li>
</ul>

<h3>Title: Graph Federated Learning Based Proactive Content Caching in Edge Computing</h3>
<ul>
<li><strong>Authors: </strong>Rui Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04760">https://arxiv.org/abs/2502.04760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04760">https://arxiv.org/pdf/2502.04760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04760]] Graph Federated Learning Based Proactive Content Caching in Edge Computing(https://arxiv.org/abs/2502.04760)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>With the rapid growth of mobile data traffic and the increasing prevalence of video streaming, proactive content caching in edge computing has become crucial for reducing latency and alleviating network congestion. However, traditional caching strategies such as FIFO, LRU, and LFU fail to effectively predict future content popularity, while existing proactive caching approaches often require users to upload data to a central server, raising concerns regarding privacy and scalability. To address these challenges, this paper proposes a Graph Federated Learning-based Proactive Content Caching (GFPCC) scheme that enhances caching efficiency while preserving user privacy. The proposed approach integrates federated learning and graph neural networks, enabling users to locally train Light Graph Convolutional Networks (LightGCN) to capture user-item relationships and predict content popularity. Instead of sharing raw data, only the trained model parameters are transmitted to the central server, where a federated averaging algorithm aggregates updates, refines the global model, and selects the most popular files for proactive caching. Experimental evaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC outperforms baseline caching algorithms by achieving higher cache efficiency through more accurate content popularity predictions. Moreover, the federated learning framework strengthens privacy protection while maintaining efficient model training; however, scalability remains a challenge in large-scale networks with dynamic user preferences.</li>
</ul>

<h3>Title: Autoregressive Generation of Static and Growing Trees</h3>
<ul>
<li><strong>Authors: </strong>Hanxiao Wang, Biao Zhang, Jonathan Klein, Dominik L. Michels, Dongming Yan, Peter Wonka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04762">https://arxiv.org/abs/2502.04762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04762">https://arxiv.org/pdf/2502.04762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04762]] Autoregressive Generation of Static and Growing Trees(https://arxiv.org/abs/2502.04762)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We propose a transformer architecture and training strategy for tree generation. The architecture processes data at multiple resolutions and has an hourglass shape, with middle layers processing fewer tokens than outer layers. Similar to convolutional networks, we introduce longer range skip connections to completent this multi-resolution approach. The key advantage of this architecture is the faster processing speed and lower memory consumption. We are therefore able to process more complex trees than would be possible with a vanilla transformer architecture. Furthermore, we extend this approach to perform image-to-tree and point-cloud-to-tree conditional generation and to simulate the tree growth processes, generating 4D trees. Empirical results validate our approach in terms of speed, memory consumption, and generation quality.</li>
</ul>

<h3>Title: DMPA: Model Poisoning Attacks on Decentralized Federated Learning for Model Differences</h3>
<ul>
<li><strong>Authors: </strong>Chao Feng, Yunlong Li, Yuanzhe Gao, Alberto Huertas Celdrán, Jan von der Assen, Gérôme Bovet, Burkhard Stiller</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04771">https://arxiv.org/abs/2502.04771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04771">https://arxiv.org/pdf/2502.04771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04771]] DMPA: Model Poisoning Attacks on Decentralized Federated Learning for Model Differences(https://arxiv.org/abs/2502.04771)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) has garnered significant attention as a prominent privacy-preserving Machine Learning (ML) paradigm. Decentralized FL (DFL) eschews traditional FL's centralized server architecture, enhancing the system's robustness and scalability. However, these advantages of DFL also create new vulnerabilities for malicious participants to execute adversarial attacks, especially model poisoning attacks. In model poisoning attacks, malicious participants aim to diminish the performance of benign models by creating and disseminating the compromised model. Existing research on model poisoning attacks has predominantly concentrated on undermining global models within the Centralized FL (CFL) paradigm, while there needs to be more research in DFL. To fill the research gap, this paper proposes an innovative model poisoning attack called DMPA. This attack calculates the differential characteristics of multiple malicious client models and obtains the most effective poisoning strategy, thereby orchestrating a collusive attack by multiple participants. The effectiveness of this attack is validated across multiple datasets, with results indicating that the DMPA approach consistently surpasses existing state-of-the-art FL model poisoning attack strategies.</li>
</ul>

<h3>Title: SeDi-Instruct: Enhancing Alignment of Language Models through Self-Directed Instruction Generation</h3>
<ul>
<li><strong>Authors: </strong>Jungwoo Kim, Minsang Kim, Sungjin Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04774">https://arxiv.org/abs/2502.04774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04774">https://arxiv.org/pdf/2502.04774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04774]] SeDi-Instruct: Enhancing Alignment of Language Models through Self-Directed Instruction Generation(https://arxiv.org/abs/2502.04774)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid evolution of Large Language Models (LLMs) has enabled the industry to develop various AI-based services. Instruction tuning is considered essential in adapting foundation models for target domains to provide high-quality services to customers. A key challenge in instruction tuning is obtaining high-quality instruction data. Self-Instruct, which automatically generates instruction data using ChatGPT APIs, alleviates the data scarcity problem. To improve the quality of instruction data, Self-Instruct discards many of the instructions generated from ChatGPT, even though it is inefficient in terms of cost owing to many useless API calls. To generate high-quality instruction data at a low cost, we propose a novel data generation framework, Self-Direct Instruction generation (SeDi-Instruct), which employs diversity-based filtering and iterative feedback task generation. Diversity-based filtering maintains model accuracy without excessively discarding low-quality generated instructions by enhancing the diversity of instructions in a batch. This reduces the cost of synthesizing instruction data. The iterative feedback task generation integrates instruction generation and training tasks and utilizes information obtained during the training to create high-quality instruction sets. Our results show that SeDi-Instruct enhances the accuracy of AI models by 5.2%, compared with traditional methods, while reducing data generation costs by 36%.</li>
</ul>

<h3>Title: Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Chen-Xiao Gao, Chenyang Wu, Mingjun Cao, Chenjun Xiao, Yang Yu, Zongzhang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04778">https://arxiv.org/abs/2502.04778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04778">https://arxiv.org/pdf/2502.04778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04778]] Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning(https://arxiv.org/abs/2502.04778)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>The primary focus of offline reinforcement learning (RL) is to manage the risk of hazardous exploitation of out-of-distribution actions. An effective approach to achieve this goal is through behavior regularization, which augments conventional RL objectives by incorporating constraints that enforce the policy to remain close to the behavior policy. Nevertheless, existing literature on behavior-regularized RL primarily focuses on explicit policy parameterizations, such as Gaussian policies. Consequently, it remains unclear how to extend this framework to more advanced policy parameterizations, such as diffusion models. In this paper, we introduce BDPO, a principled behavior-regularized RL framework tailored for diffusion-based policies, thereby combining the expressive power of diffusion policies and the robustness provided by regularization. The key ingredient of our method is to calculate the Kullback-Leibler (KL) regularization analytically as the accumulated discrepancies in reverse-time transition kernels along the diffusion trajectory. By integrating the regularization, we develop an efficient two-time-scale actor-critic RL algorithm that produces the optimal policy while respecting the behavior constraint. Comprehensive evaluations conducted on synthetic 2D tasks and continuous control tasks from the D4RL benchmark validate its effectiveness and superior performance.</li>
</ul>

<h3>Title: Enhancing SQL Injection Detection and Prevention Using Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Naga Sai Dasari, Atta Badii, Armin Moin, Ahmed Ashlam</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04786">https://arxiv.org/abs/2502.04786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04786">https://arxiv.org/pdf/2502.04786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04786]] Enhancing SQL Injection Detection and Prevention Using Generative Models(https://arxiv.org/abs/2502.04786)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, generative</a></li>
<li><strong>Abstract: </strong>SQL Injection (SQLi) continues to pose a significant threat to the security of web applications, enabling attackers to manipulate databases and access sensitive information without authorisation. Although advancements have been made in detection techniques, traditional signature-based methods still struggle to identify sophisticated SQL injection attacks that evade predefined patterns. As SQLi attacks evolve, the need for more adaptive detection systems becomes crucial. This paper introduces an innovative approach that leverages generative models to enhance SQLi detection and prevention mechanisms. By incorporating Variational Autoencoders (VAE), Conditional Wasserstein GAN with Gradient Penalty (CWGAN-GP), and U-Net, synthetic SQL queries were generated to augment training datasets for machine learning models. The proposed method demonstrated improved accuracy in SQLi detection systems by reducing both false positives and false negatives. Extensive empirical testing further illustrated the ability of the system to adapt to evolving SQLi attack patterns, resulting in enhanced precision and robustness.</li>
</ul>

<h3>Title: Probing Internal Representations of Multi-Word Verbs in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hassane Kissane, Achim Schilling, Patrick Krauss</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04789">https://arxiv.org/abs/2502.04789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04789">https://arxiv.org/pdf/2502.04789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04789]] Probing Internal Representations of Multi-Word Verbs in Large Language Models(https://arxiv.org/abs/2502.04789)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>This study investigates the internal representations of verb-particle combinations, called multi-word verbs, within transformer-based large language models (LLMs), specifically examining how these models capture lexical and syntactic properties at different neural network layers. Using the BERT architecture, we analyze the representations of its layers for two different verb-particle constructions: phrasal verbs like 'give up' and prepositional verbs like 'look at'. Our methodology includes training probing classifiers on the internal representations to classify these categories at both word and sentence levels. The results indicate that the model's middle layers achieve the highest classification accuracies. To further analyze the nature of these distinctions, we conduct a data separability test using the Generalized Discrimination Value (GDV). While GDV results show weak linear separability between the two verb types, probing classifiers still achieve high accuracy, suggesting that representations of these linguistic categories may be non-linearly separable. This aligns with previous research indicating that linguistic distinctions in neural networks are not always encoded in a linearly separable manner. These findings computationally support usage-based claims on the representation of verb-particle constructions and highlight the complex interaction between neural network architectures and linguistic structures.</li>
</ul>

<h3>Title: S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Yuting Zeng, Weizhe Huang, Lei Jiang, Tongxuan Liu, Xitai Jin, Chen Tianying Tiana, Jing Li, Xiaohua Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04790">https://arxiv.org/abs/2502.04790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04790">https://arxiv.org/pdf/2502.04790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04790]] S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate Efficiency(https://arxiv.org/abs/2502.04790)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities across various natural language processing (NLP) scenarios, but they still face challenges when handling complex arithmetic and logical reasoning tasks. While Chain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction strategies have attempted to guide models in sequential, multi-step reasoning, Multi-agent Debate (MAD) has emerged as a viable approach for enhancing the reasoning capabilities of LLMs. By increasing both the number of agents and the frequency of debates, the performance of LLMs improves significantly. However, this strategy results in a significant increase in token costs, presenting a barrier to scalability. To address this challenge, we introduce a novel sparsification strategy designed to reduce token costs within MAD. This approach minimizes ineffective exchanges of information and unproductive discussions among agents, thereby enhancing the overall efficiency of the debate process. We conduct comparative experiments on multiple datasets across various models, demonstrating that our approach significantly reduces the token costs in MAD to a considerable extent. Specifically, compared to MAD, our approach achieves an impressive reduction of up to 94.5\% in token costs while maintaining performance degradation below 2.0\%.</li>
</ul>

<h3>Title: Developmentally-plausible Working Memory Shapes a Critical Period for Language Acquisition</h3>
<ul>
<li><strong>Authors: </strong>Masato Mita, Ryo Yoshida, Yohei Oseki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04795">https://arxiv.org/abs/2502.04795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04795">https://arxiv.org/pdf/2502.04795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04795]] Developmentally-plausible Working Memory Shapes a Critical Period for Language Acquisition(https://arxiv.org/abs/2502.04795)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models exhibit general linguistic abilities but significantly differ from humans in their efficiency of language acquisition. This study proposes a method for integrating the developmental characteristics of working memory during the critical period, a stage when human language acquisition is particularly efficient, into language models. The proposed method introduces a mechanism that initially constrains working memory during the early stages of training and gradually relaxes this constraint in an exponential manner as learning progresses. Targeted syntactic evaluation shows that the proposed method outperforms conventional models without memory constraints or with static memory constraints. These findings not only provide new directions for designing data-efficient language models but also offer indirect evidence supporting the underlying mechanisms of the critical period hypothesis in human language acquisition.</li>
</ul>

<h3>Title: Optimistic Gradient Learning with Hessian Corrections for High-Dimensional Black-Box Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yedidya Kfir, Elad Sarafian, Sarit Kraus, Yoram Louzoun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04829">https://arxiv.org/abs/2502.04829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04829">https://arxiv.org/pdf/2502.04829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04829]] Optimistic Gradient Learning with Hessian Corrections for High-Dimensional Black-Box Optimization(https://arxiv.org/abs/2502.04829)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Black-box algorithms are designed to optimize functions without relying on their underlying analytical structure or gradient information, making them essential when gradients are inaccessible or difficult to compute. Traditional methods for solving black-box optimization (BBO) problems predominantly rely on non-parametric models and struggle to scale to large input spaces. Conversely, parametric methods that model the function with neural estimators and obtain gradient signals via backpropagation may suffer from significant gradient errors. A recent alternative, Explicit Gradient Learning (EGL), which directly learns the gradient using a first-order Taylor approximation, has demonstrated superior performance over both parametric and non-parametric methods. In this work, we propose two novel gradient learning variants to address the robustness challenges posed by high-dimensional, complex, and highly non-linear problems. Optimistic Gradient Learning (OGL) introduces a bias toward lower regions in the function landscape, while Higher-order Gradient Learning (HGL) incorporates second-order Taylor corrections to improve gradient accuracy. We combine these approaches into the unified OHGL algorithm, achieving state-of-the-art (SOTA) performance on the synthetic COCO suite. Additionally, we demonstrate OHGLs applicability to high-dimensional real-world machine learning (ML) tasks such as adversarial training and code generation. Our results highlight OHGLs ability to generate stronger candidates, offering a valuable tool for ML researchers and practitioners tackling high-dimensional, non-linear optimization challenges</li>
</ul>

<h3>Title: HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Qijun Gan, Yi Ren, Chen Zhang, Zhenhui Ye, Pan Xie, Xiang Yin, Zehuan Yuan, Bingyue Peng, Jianke Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04847">https://arxiv.org/abs/2502.04847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04847">https://arxiv.org/pdf/2502.04847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04847]] HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation(https://arxiv.org/abs/2502.04847)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Human motion video generation has advanced significantly, while existing methods still struggle with accurately rendering detailed body parts like hands and faces, especially in long sequences and intricate motions. Current approaches also rely on fixed resolution and struggle to maintain visual consistency. To address these limitations, we propose HumanDiT, a pose-guided Diffusion Transformer (DiT)-based framework trained on a large and wild dataset containing 14,000 hours of high-quality video to produce high-fidelity videos with fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT, supports numerous video resolutions and variable sequence lengths, facilitating learning for long-sequence video generation; (ii) we introduce a prefix-latent reference strategy to maintain personalized characteristics across extended sequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to generate subsequent pose sequences, facilitating video continuation from static images or existing videos. It also utilizes a Pose Adapter to enable pose transfer with given sequences. Extensive experiments demonstrate its superior performance in generating long-form, pose-accurate videos across diverse scenarios.</li>
</ul>

<h3>Title: Aequa: Fair Model Rewards in Collaborative Learning via Slimmable Networks</h3>
<ul>
<li><strong>Authors: </strong>Nurbek Tastan, Samuel Horvath, Karthik Nandakumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04850">https://arxiv.org/abs/2502.04850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04850">https://arxiv.org/pdf/2502.04850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04850]] Aequa: Fair Model Rewards in Collaborative Learning via Slimmable Networks(https://arxiv.org/abs/2502.04850)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Collaborative learning enables multiple participants to learn a single global model by exchanging focused updates instead of sharing data. One of the core challenges in collaborative learning is ensuring that participants are rewarded fairly for their contributions, which entails two key sub-problems: contribution assessment and reward allocation. This work focuses on fair reward allocation, where the participants are incentivized through model rewards - differentiated final models whose performance is commensurate with the contribution. In this work, we leverage the concept of slimmable neural networks to collaboratively learn a shared global model whose performance degrades gracefully with a reduction in model width. We also propose a post-training fair allocation algorithm that determines the model width for each participant based on their contributions. We theoretically study the convergence of our proposed approach and empirically validate it using extensive experiments on different datasets and architectures. We also extend our approach to enable training-time model reward allocation.</li>
</ul>

<h3>Title: Enhancing Disinformation Detection with Explainable AI and Named Entity Replacement</h3>
<ul>
<li><strong>Authors: </strong>Santiago González-Silot, Andrés Montoro-Montarroso, Eugenio Martínez Cámara, Juan Gómez-Romero</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04863">https://arxiv.org/abs/2502.04863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04863">https://arxiv.org/pdf/2502.04863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04863]] Enhancing Disinformation Detection with Explainable AI and Named Entity Replacement(https://arxiv.org/abs/2502.04863)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>The automatic detection of disinformation presents a significant challenge in the field of natural language processing. This task addresses a multifaceted societal and communication issue, which needs approaches that extend beyond the identification of general linguistic patterns through data-driven algorithms. In this research work, we hypothesise that text classification methods are not able to capture the nuances of disinformation and they often ground their decision in superfluous features. Hence, we apply a post-hoc explainability method (SHAP, SHapley Additive exPlanations) to identify spurious elements with high impact on the classification models. Our findings show that non-informative elements (e.g., URLs and emoticons) should be removed and named entities (e.g., Rwanda) should be pseudo-anonymized before training to avoid models' bias and increase their generalization capabilities. We evaluate this methodology with internal dataset and external dataset before and after applying extended data preprocessing and named entity replacement. The results show that our proposal enhances on average the performance of a disinformation classification method with external test data in 65.78% without a significant decrease of the internal test performance.</li>
</ul>

<h3>Title: IPSeg: Image Posterior Mitigates Semantic Drift in Class-Incremental Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xiao Yu, Yan Fang, Yao Zhao, Yunchao Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04870">https://arxiv.org/abs/2502.04870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04870">https://arxiv.org/pdf/2502.04870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04870]] IPSeg: Image Posterior Mitigates Semantic Drift in Class-Incremental Segmentation(https://arxiv.org/abs/2502.04870)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Class incremental learning aims to enable models to learn from sequential, non-stationary data streams across different tasks without catastrophic forgetting. In class incremental semantic segmentation (CISS), the semantic content of image pixels evolves over incremental phases, known as semantic drift. In this work, we identify two critical challenges in CISS that contribute to semantic drift and degrade performance. First, we highlight the issue of separate optimization, where different parts of the model are optimized in distinct incremental stages, leading to misaligned probability scales. Second, we identify noisy semantics arising from inappropriate pseudo-labeling, which results in sub-optimal results. To address these challenges, we propose a novel and effective approach, Image Posterior and Semantics Decoupling for Segmentation (IPSeg). IPSeg introduces two key mechanisms: (1) leveraging image posterior probabilities to align optimization across stages and mitigate the effects of separate optimization, and (2) employing semantics decoupling to handle noisy semantics and tailor learning strategies for different semantics. Extensive experiments on the Pascal VOC 2012 and ADE20K datasets demonstrate that IPSeg achieves superior performance compared to state-of-the-art methods, particularly in challenging long-term incremental scenarios.</li>
</ul>

<h3>Title: Sparse Autoencoders Do Not Find Canonical Units of Analysis</h3>
<ul>
<li><strong>Authors: </strong>Patrick Leask, Bart Bussmann, Michael Pearce, Joseph Bloom, Curt Tigges, Noura Al Moubayed, Lee Sharkey, Neel Nanda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04878">https://arxiv.org/abs/2502.04878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04878">https://arxiv.org/pdf/2502.04878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04878]] Sparse Autoencoders Do Not Find Canonical Units of Analysis(https://arxiv.org/abs/2502.04878)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>A common goal of mechanistic interpretability is to decompose the activations of neural networks into features: interpretable properties of the input computed by the model. Sparse autoencoders (SAEs) are a popular method for finding these features in LLMs, and it has been postulated that they can be used to find a \textit{canonical} set of units: a unique and complete list of atomic features. We cast doubt on this belief using two novel techniques: SAE stitching to show they are incomplete, and meta-SAEs to show they are not atomic. SAE stitching involves inserting or swapping latents from a larger SAE into a smaller one. Latents from the larger SAE can be divided into two categories: \emph{novel latents}, which improve performance when added to the smaller SAE, indicating they capture novel information, and \emph{reconstruction latents}, which can replace corresponding latents in the smaller SAE that have similar behavior. The existence of novel features indicates incompleteness of smaller SAEs. Using meta-SAEs -- SAEs trained on the decoder matrix of another SAE -- we find that latents in SAEs often decompose into combinations of latents from a smaller SAE, showing that larger SAE latents are not atomic. The resulting decompositions are often interpretable; e.g. a latent representing ``Einstein'' decomposes into ``scientist'', ``Germany'', and ``famous person''. Even if SAEs do not find canonical units of analysis, they may still be useful tools. We suggest that future research should either pursue different approaches for identifying such units, or pragmatically choose the SAE size suited to their task. We provide an interactive dashboard to explore meta-SAEs: this https URL</li>
</ul>

<h3>Title: pytopicgram: A library for data extraction and topic modeling from Telegram channels</h3>
<ul>
<li><strong>Authors: </strong>J. Gómez-Romero, J. Cantón Correa, R. Pérez Mercado, F. Prados Abad, M. Molina-Solana, W. Fajardo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04882">https://arxiv.org/abs/2502.04882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04882">https://arxiv.org/pdf/2502.04882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04882]] pytopicgram: A library for data extraction and topic modeling from Telegram channels(https://arxiv.org/abs/2502.04882)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Telegram is a popular platform for public communication, generating large amounts of messages through its channels. pytopicgram is a Python library that helps researchers collect, organize, and analyze these Telegram messages. The library offers key features such as easy message retrieval, detailed channel information, engagement metrics, and topic identification using advanced modeling techniques. By simplifying data extraction and analysis, pytopicgram allows users to understand how content spreads and how audiences interact on Telegram. This paper describes the design, main features, and practical uses of \pytopicgram, showcasing its effectiveness for studying public conversations on Telegram.</li>
</ul>

<h3>Title: Exploit Gradient Skewness to Circumvent Byzantine Defenses for Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Liu, Chen Chen, Lingjuan Lyu, Yaochu Jin, Gang Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04890">https://arxiv.org/abs/2502.04890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04890">https://arxiv.org/pdf/2502.04890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04890]] Exploit Gradient Skewness to Circumvent Byzantine Defenses for Federated Learning(https://arxiv.org/abs/2502.04890)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is notorious for its vulnerability to Byzantine attacks. Most current Byzantine defenses share a common inductive bias: among all the gradients, the densely distributed ones are more likely to be honest. However, such a bias is a poison to Byzantine robustness due to a newly discovered phenomenon in this paper - gradient skew. We discover that a group of densely distributed honest gradients skew away from the optimal gradient (the average of honest gradients) due to heterogeneous data. This gradient skew phenomenon allows Byzantine gradients to hide within the densely distributed skewed gradients. As a result, Byzantine defenses are confused into believing that Byzantine gradients are honest. Motivated by this observation, we propose a novel skew-aware attack called STRIKE: first, we search for the skewed gradients; then, we construct Byzantine gradients within the skewed gradients. Experiments on three benchmark datasets validate the effectiveness of our attack</li>
</ul>

<h3>Title: A Foundational Brain Dynamics Model via Stochastic Optimal Control</h3>
<ul>
<li><strong>Authors: </strong>Joonhyeong Park, Byoungwoo Park, Chang-Bae Bang, Jungwon Choi, Hyungjin Chung, Byung-Hoon Kim, Juho Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04892">https://arxiv.org/abs/2502.04892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04892">https://arxiv.org/pdf/2502.04892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04892]] A Foundational Brain Dynamics Model via Stochastic Optimal Control(https://arxiv.org/abs/2502.04892)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce a foundational model for brain dynamics that utilizes stochastic optimal control (SOC) and amortized inference. Our method features a continuous-discrete state space model (SSM) that can robustly handle the intricate and noisy nature of fMRI signals. To address computational limitations, we implement an approximation strategy grounded in the SOC framework. Additionally, we present a simulation-free latent dynamics approach that employs locally linear approximations, facilitating efficient and scalable inference. For effective representation learning, we derive an Evidence Lower Bound (ELBO) from the SOC formulation, which integrates smoothly with recent advancements in self-supervised learning (SSL), thereby promoting robust and transferable representations. Pre-trained on extensive datasets such as the UKB, our model attains state-of-the-art results across a variety of downstream tasks, including demographic prediction, trait analysis, disease diagnosis, and prognosis. Moreover, evaluating on external datasets such as HCP-A, ABIDE, and ADHD200 further validates its superior abilities and resilience across different demographic and clinical distributions. Our foundational model provides a scalable and efficient approach for deciphering brain dynamics, opening up numerous applications in neuroscience.</li>
</ul>

<h3>Title: Goku: Flow Based Video Generative Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, Ting-Che Lin, Shilong Zhang, Fu Li, Chuan Li, Xing Wang, Yanghua Peng, Peize Sun, Ping Luo, Yi Jiang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04896">https://arxiv.org/abs/2502.04896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04896">https://arxiv.org/pdf/2502.04896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04896]] Goku: Flow Based Video Generative Foundation Models(https://arxiv.org/abs/2502.04896)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>This paper introduces Goku, a state-of-the-art family of joint image-and-video generation models leveraging rectified flow Transformers to achieve industry-leading performance. We detail the foundational elements enabling high-quality visual generation, including the data curation pipeline, model architecture design, flow formulation, and advanced infrastructure for efficient and robust large-scale training. The Goku models demonstrate superior performance in both qualitative and quantitative evaluations, setting new benchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for text-to-video tasks. We believe that this work provides valuable insights and practical advancements for the research community in developing joint image-and-video generation models.</li>
</ul>

<h3>Title: Unified Approaches in Self-Supervised Event Stream Modeling: Progress and Prospects</h3>
<ul>
<li><strong>Authors: </strong>Levente Zólyomi, Tianze Wang, Sofiane Ennadir, Oleg Smirnov, Lele Cao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04899">https://arxiv.org/abs/2502.04899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04899">https://arxiv.org/pdf/2502.04899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04899]] Unified Approaches in Self-Supervised Event Stream Modeling: Progress and Prospects(https://arxiv.org/abs/2502.04899)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The proliferation of digital interactions across diverse domains, such as healthcare, e-commerce, gaming, and finance, has resulted in the generation of vast volumes of event stream (ES) data. ES data comprises continuous sequences of timestamped events that encapsulate detailed contextual information relevant to each domain. While ES data holds significant potential for extracting actionable insights and enhancing decision-making, its effective utilization is hindered by challenges such as the scarcity of labeled data and the fragmented nature of existing research efforts. Self-Supervised Learning (SSL) has emerged as a promising paradigm to address these challenges by enabling the extraction of meaningful representations from unlabeled ES data. In this survey, we systematically review and synthesize SSL methodologies tailored for ES modeling across multiple domains, bridging the gaps between domain-specific approaches that have traditionally operated in isolation. We present a comprehensive taxonomy of SSL techniques, encompassing both predictive and contrastive paradigms, and analyze their applicability and effectiveness within different application contexts. Furthermore, we identify critical gaps in current research and propose a future research agenda aimed at developing scalable, domain-agnostic SSL frameworks for ES modeling. By unifying disparate research efforts and highlighting cross-domain synergies, this survey aims to accelerate innovation, improve reproducibility, and expand the applicability of SSL to diverse real-world ES challenges.</li>
</ul>

<h3>Title: On the Difficulty of Constructing a Robust and Publicly-Detectable Watermark</h3>
<ul>
<li><strong>Authors: </strong>Jaiden Fairoze, Guillermo Ortiz-Jiménez, Mel Vecerik, Somesh Jha, Sven Gowal</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04901">https://arxiv.org/abs/2502.04901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04901">https://arxiv.org/pdf/2502.04901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04901]] On the Difficulty of Constructing a Robust and Publicly-Detectable Watermark(https://arxiv.org/abs/2502.04901)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark</a></li>
<li><strong>Abstract: </strong>This work investigates the theoretical boundaries of creating publicly-detectable schemes to enable the provenance of watermarked imagery. Metadata-based approaches like C2PA provide unforgeability and public-detectability. ML techniques offer robust retrieval and watermarking. However, no existing scheme combines robustness, unforgeability, and public-detectability. In this work, we formally define such a scheme and establish its existence. Although theoretically possible, we find that at present, it is intractable to build certain components of our scheme without a leap in deep learning capabilities. We analyze these limitations and propose research directions that need to be addressed before we can practically realize robust and publicly-verifiable provenance.</li>
</ul>

<h3>Title: On the Power of Heuristics in Temporal Graphs</h3>
<ul>
<li><strong>Authors: </strong>Filip Cornell, Oleg Smirnov, Gabriela Zarzar Gandler, Lele Cao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04910">https://arxiv.org/abs/2502.04910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04910">https://arxiv.org/pdf/2502.04910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04910]] On the Power of Heuristics in Temporal Graphs(https://arxiv.org/abs/2502.04910)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust, fair</a></li>
<li><strong>Abstract: </strong>Dynamic graph datasets often exhibit strong temporal patterns, such as recency, which prioritizes recent interactions, and popularity, which favors frequently occurring nodes. We demonstrate that simple heuristics leveraging only these patterns can perform on par or outperform state-of-the-art neural network models under standard evaluation protocols. To further explore these dynamics, we introduce metrics that quantify the impact of recency and popularity across datasets. Our experiments on BenchTemp and the Temporal Graph Benchmark show that our approaches achieve state-of-the-art performance across all datasets in the latter and secure top ranks on multiple datasets in the former. These results emphasize the importance of refined evaluation schemes to enable fair comparisons and promote the development of more robust temporal graph models. Additionally, they reveal that current deep learning methods often struggle to capture the key patterns underlying predictions in real-world temporal graphs. For reproducibility, we have made our code publicly available.</li>
</ul>

<h3>Title: Securing 5G Bootstrapping: A Two-Layer IBS Authentication Protocol</h3>
<ul>
<li><strong>Authors: </strong>Yilu Dong (1), Rouzbeh Behnia (2), Attila A. Yavuz (2), Syed Rafiul Hussain (1) ((1) The Pennsylvania State University, (2) University of South Florida)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04915">https://arxiv.org/abs/2502.04915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04915">https://arxiv.org/pdf/2502.04915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04915]] Securing 5G Bootstrapping: A Two-Layer IBS Authentication Protocol(https://arxiv.org/abs/2502.04915)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>The lack of authentication during the initial bootstrapping phase between cellular devices and base stations allows attackers to deploy fake base stations and send malicious messages to the devices. These attacks have been a long-existing problem in cellular networks, enabling adversaries to launch denial-of-service (DoS), information leakage, and location-tracking attacks. While some defense mechanisms are introduced in 5G, (e.g., encrypting user identifiers to mitigate IMSI catchers), the initial communication between devices and base stations remains unauthenticated, leaving a critical security gap. To address this, we propose E2IBS, a novel and efficient two-layer identity-based signature scheme designed for seamless integration with existing cellular protocols. We implement E2IBS on an open-source 5G stack and conduct a comprehensive performance evaluation against alternative solutions. Compared to the state-of-the-art Schnorr-HIBS, E2IBS reduces attack surfaces, enables fine-grained lawful interception, and achieves 2x speed in verification, making it a practical solution for securing 5G base station authentication.</li>
</ul>

<h3>Title: Generative-enhanced optimization for knapsack problems: an industry-relevant study</h3>
<ul>
<li><strong>Authors: </strong>Yelyzaveta Vodovozova, Abhishek Awasthi, Caitlin Jones, Joseph Doetsch, Karen Wintersperger, Florian Krellner, Carlos A. Riofrío</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04928">https://arxiv.org/abs/2502.04928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04928">https://arxiv.org/pdf/2502.04928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04928]] Generative-enhanced optimization for knapsack problems: an industry-relevant study(https://arxiv.org/abs/2502.04928)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Optimization is a crucial task in various industries such as logistics, aviation, manufacturing, chemical, pharmaceutical, and insurance, where finding the best solution to a problem can result in significant cost savings and increased efficiency. Tensor networks (TNs) have gained prominence in recent years in modeling classical systems with quantum-inspired approaches. More recently, TN generative-enhanced optimization (TN-GEO) has been proposed as a strategy which uses generative modeling to efficiently sample valid solutions with respect to certain constraints of optimization problems. Moreover, it has been shown that symmetric TNs (STNs) can encode certain constraints of optimization problems, thus aiding in their solution process. In this work, we investigate the applicability of TN- and STN-GEO to an industry relevant problem class, a multi-knapsack problem, in which each object must be assigned to an available knapsack. We detail a prescription for practitioners to use the TN-and STN-GEO methodology and study its scaling behavior and dependence on its hyper-parameters. We benchmark 60 different problem instances and find that TN-GEO and STN-GEO produce results of similar quality to simulated annealing.</li>
</ul>

<h3>Title: Conformal Prediction for Electricity Price Forecasting in the Day-Ahead and Real-Time Balancing Market</h3>
<ul>
<li><strong>Authors: </strong>Ciaran O'Connor, Mohamed Bahloul, Roberto Rossi, Steven Prestwich, Andrea Visentin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04935">https://arxiv.org/abs/2502.04935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04935">https://arxiv.org/pdf/2502.04935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04935]] Conformal Prediction for Electricity Price Forecasting in the Day-Ahead and Real-Time Balancing Market(https://arxiv.org/abs/2502.04935)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The integration of renewable energy into electricity markets poses significant challenges to price stability and increases the complexity of market operations. Accurate and reliable electricity price forecasting is crucial for effective market participation, where price dynamics can be significantly more challenging to predict. Probabilistic forecasting, through prediction intervals, efficiently quantifies the inherent uncertainties in electricity prices, supporting better decision-making for market participants. This study explores the enhancement of probabilistic price prediction using Conformal Prediction (CP) techniques, specifically Ensemble Batch Prediction Intervals and Sequential Predictive Conformal Inference. These methods provide precise and reliable prediction intervals, outperforming traditional models in validity metrics. We propose an ensemble approach that combines the efficiency of quantile regression models with the robust coverage properties of time series adapted CP techniques. This ensemble delivers both narrow prediction intervals and high coverage, leading to more reliable and accurate forecasts. We further evaluate the practical implications of CP techniques through a simulated trading algorithm applied to a battery storage system. The ensemble approach demonstrates improved financial returns in energy trading in both the Day-Ahead and Balancing Markets, highlighting its practical benefits for market participants.</li>
</ul>

<h3>Title: The Rising Threat to Emerging AI-Powered Search Engines</h3>
<ul>
<li><strong>Authors: </strong>Zeren Luo, Zifan Peng, Yule Liu, Zhen Sun, Mingchen Li, Jingyi Zheng, Xinlei He</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04951">https://arxiv.org/abs/2502.04951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04951">https://arxiv.org/pdf/2502.04951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04951]] The Rising Threat to Emerging AI-Powered Search Engines(https://arxiv.org/abs/2502.04951)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have significantly enhanced the capabilities of AI-Powered Search Engines (AIPSEs), offering precise and efficient responses by integrating external databases with pre-existing knowledge. However, we observe that these AIPSEs raise risks such as quoting malicious content or citing malicious websites, leading to harmful or unverified information dissemination. In this study, we conduct the first safety risk quantification on seven production AIPSEs by systematically defining the threat model, risk level, and evaluating responses to various query types. With data collected from PhishTank, ThreatBook, and LevelBlue, our findings reveal that AIPSEs frequently generate harmful content that contains malicious URLs even with benign queries (e.g., with benign keywords). We also observe that directly query URL will increase the risk level while query with natural language will mitigate such risk. We further perform two case studies on online document spoofing and phishing to show the ease of deceiving AIPSEs in the real-world setting. To mitigate these risks, we develop an agent-based defense with a GPT-4o-based content refinement tool and an XGBoost-based URL detector. Our evaluation shows that our defense can effectively reduce the risk but with the cost of reducing available information. Our research highlights the urgent need for robust safety measures in AIPSEs.</li>
</ul>

<h3>Title: A Systematic Literature Review on Automated Exploit and Security Test Generation</h3>
<ul>
<li><strong>Authors: </strong>Quang-Cuong Bui, Emanuele Iannone, Maria Camporese, Torge Hinrichs, Catherine Tony, László Tóth, Fabio Palomba, Péter Hegedűs, Fabio Massacci, Riccardo Scandariato</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04953">https://arxiv.org/abs/2502.04953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04953">https://arxiv.org/pdf/2502.04953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04953]] A Systematic Literature Review on Automated Exploit and Security Test Generation(https://arxiv.org/abs/2502.04953)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The exploit or the Proof of Concept of the vulnerability plays an important role in developing superior vulnerability repair techniques, as it can be used as an oracle to verify the correctness of the patches generated by the tools. However, the vulnerability exploits are often unavailable and require time and expert knowledge to craft. Obtaining them from the exploit generation techniques is another potential solution. The goal of this survey is to aid the researchers and practitioners in understanding the existing techniques for exploit generation through the analysis of their characteristics and their usability in practice. We identify a list of exploit generation techniques from literature and group them into four categories: automated exploit generation, security testing, fuzzing, and other techniques. Most of the techniques focus on the memory-based vulnerabilities in C/C++ programs and web-based injection vulnerabilities in PHP and Java applications. We found only a few studies that publicly provided usable tools associated with their techniques.</li>
</ul>

<h3>Title: Claim Extraction for Fact-Checking: Data, Models, and Automated Metrics</h3>
<ul>
<li><strong>Authors: </strong>Herbert Ullrich, Tomáš Mlynář, Jan Drchal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04955">https://arxiv.org/abs/2502.04955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04955">https://arxiv.org/pdf/2502.04955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04955]] Claim Extraction for Fact-Checking: Data, Models, and Automated Metrics(https://arxiv.org/abs/2502.04955)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In this paper, we explore the problem of Claim Extraction using one-to-many text generation methods, comparing LLMs, small summarization models finetuned for the task, and a previous NER-centric baseline QACG. As the current publications on Claim Extraction, Fact Extraction, Claim Generation and Check-worthy Claim Detection are quite scattered in their means and terminology, we compile their common objectives, releasing the FEVERFact dataset, with 17K atomic factual claims extracted from 4K contextualised Wikipedia sentences, adapted from the original FEVER. We compile the known objectives into an Evaluation framework of: Atomicity, Fluency, Decontextualization, Faithfulness checked for each generated claim separately, and Focus and Coverage measured against the full set of predicted claims for a single input. For each metric, we implement a scale using a reduction to an already-explored NLP task. We validate our metrics against human grading of generic claims, to see that the model ranking on $F_{fact}$, our hardest metric, did not change and the evaluation framework approximates human grading very closely in terms of $F_1$ and RMSE.</li>
</ul>

<h3>Title: SSMLoRA: Enhancing Low-Rank Adaptation with State Space Model</h3>
<ul>
<li><strong>Authors: </strong>Jiayang Yu, Yihang Zhang, Bin Wang, Peiqin Lin, Yongkang Liu, Shi Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04958">https://arxiv.org/abs/2502.04958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04958">https://arxiv.org/pdf/2502.04958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04958]] SSMLoRA: Enhancing Low-Rank Adaptation with State Space Model(https://arxiv.org/abs/2502.04958)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Fine-tuning is a key approach for adapting language models to specific downstream tasks, but updating all model parameters becomes impractical as model sizes increase. Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), address this challenge by introducing additional adaptation parameters into pre-trained weight matrices. However, LoRA's performance varies across different insertion points within the model, highlighting potential parameter inefficiency due to unnecessary insertions. To this end, we propose SSMLoRA (State Space Model Low-Rank Adaptation), an extension of LoRA that incorporates a State Space Model (SSM) to interconnect low-rank matrices. SSMLoRA ensures that performance is maintained even with sparser insertions. SSMLoRA allows the model to not only map inputs to a low-rank space for better feature extraction but also leverage the computations from the previous low-rank space. Our method achieves comparable performance to LoRA on the General Language Understanding Evaluation (GLUE) benchmark while using only half the parameters. Additionally, due to its structure, SSMLoRA shows promise in handling tasks with longer input sequences. .You can find our code here:this https URL.</li>
</ul>

<h3>Title: Commonality and Individuality! Integrating Humor Commonality with Speaker Individuality for Humor Recognition</h3>
<ul>
<li><strong>Authors: </strong>Haohao Zhu, Junyu Lu, Zeyuan Zeng, Zewen Bai, Xiaokun Zhang, Liang Yang, Hongfei Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04960">https://arxiv.org/abs/2502.04960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04960">https://arxiv.org/pdf/2502.04960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04960]] Commonality and Individuality! Integrating Humor Commonality with Speaker Individuality for Humor Recognition(https://arxiv.org/abs/2502.04960)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Humor recognition aims to identify whether a specific speaker's text is humorous. Current methods for humor recognition mainly suffer from two limitations: (1) they solely focus on one aspect of humor commonalities, ignoring the multifaceted nature of humor; and (2) they typically overlook the critical role of speaker individuality, which is essential for a comprehensive understanding of humor expressions. To bridge these gaps, we introduce the Commonality and Individuality Incorporated Network for Humor Recognition (CIHR), a novel model designed to enhance humor recognition by integrating multifaceted humor commonalities with the distinctive individuality of speakers. The CIHR features a Humor Commonality Analysis module that explores various perspectives of multifaceted humor commonality within user texts, and a Speaker Individuality Extraction module that captures both static and dynamic aspects of a speaker's profile to accurately model their distinctive individuality. Additionally, Static and Dynamic Fusion modules are introduced to effectively incorporate the humor commonality with speaker's individuality in the humor recognition process. Extensive experiments demonstrate the effectiveness of CIHR, underscoring the importance of concurrently addressing both multifaceted humor commonality and distinctive speaker individuality in humor recognition.</li>
</ul>

<h3>Title: Fast Adaptive Anti-Jamming Channel Access via Deep Q Learning and Coarse-Grained Spectrum Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jianshu Zhang, Xiaofu Wu, Junquan Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04963">https://arxiv.org/abs/2502.04963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04963">https://arxiv.org/pdf/2502.04963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04963]] Fast Adaptive Anti-Jamming Channel Access via Deep Q Learning and Coarse-Grained Spectrum Prediction(https://arxiv.org/abs/2502.04963)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>This paper investigates the anti-jamming channel access problem in complex and unknown jamming environments, where the jammer could dynamically adjust its strategies to target different channels. Traditional channel hopping anti-jamming approaches using fixed patterns are ineffective against such dynamic jamming attacks. Although the emerging deep reinforcement learning (DRL) based dynamic channel access approach could achieve the Nash equilibrium under fast-changing jamming attacks, it requires extensive training episodes. To address this issue, we propose a fast adaptive anti-jamming channel access approach guided by the intuition of ``learning faster than the jammer", where a synchronously updated coarse-grained spectrum prediction serves as an auxiliary task for the deep Q learning (DQN) based anti-jamming model. This helps the model identify a superior Q-function compared to standard DRL while significantly reducing the number of training episodes. Numerical results indicate that the proposed approach significantly accelerates the rate of convergence in model training, reducing the required training episodes by up to 70% compared to standard DRL. Additionally, it also achieves a 10% improvement in throughput over NE strategies, owing to the effective use of coarse-grained spectrum prediction.</li>
</ul>

<h3>Title: CoCoA: A Generalized Approach to Uncertainty Quantification by Integrating Confidence and Consistency of LLM Outputs</h3>
<ul>
<li><strong>Authors: </strong>Roman Vashurin (1), Maiya Goloburda (1), Preslav Nakov (1), Artem Shelmanov (1), Maxim Panov (1) ((1) Mohamed bin Zayed University of Artificial Intelligence)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04964">https://arxiv.org/abs/2502.04964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04964">https://arxiv.org/pdf/2502.04964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04964]] CoCoA: A Generalized Approach to Uncertainty Quantification by Integrating Confidence and Consistency of LLM Outputs(https://arxiv.org/abs/2502.04964)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Uncertainty quantification (UQ) methods for Large Language Models (LLMs) encompasses a variety of approaches, with two major types being particularly prominent: information-based, which focus on model confidence expressed as token probabilities, and consistency-based, which assess the semantic relationship between multiple outputs generated using repeated sampling. Several recent methods have combined these two approaches and shown impressive performance in various applications. However, they sometimes fail to outperform much simpler baseline methods. Our investigation reveals distinctive characteristics of LLMs as probabilistic models, which help to explain why these UQ methods underperform in certain tasks. Based on these findings, we propose a new way of synthesizing model confidence and output consistency that leads to a family of efficient and robust UQ methods. We evaluate our approach across a variety of tasks such as question answering, abstractive summarization, and machine translation, demonstrating sizable improvements over state-of-the-art UQ approaches.</li>
</ul>

<h3>Title: DE-PADA: Personalized Augmentation and Domain Adaptation for ECG Biometrics Across Physiological States</h3>
<ul>
<li><strong>Authors: </strong>Amro Abu Saleh, Elliot Sprecher, Kfir Y. Levy, Daniel H. Lange</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04973">https://arxiv.org/abs/2502.04973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04973">https://arxiv.org/pdf/2502.04973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04973]] DE-PADA: Personalized Augmentation and Domain Adaptation for ECG Biometrics Across Physiological States(https://arxiv.org/abs/2502.04973)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, biometric, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Electrocardiogram (ECG)-based biometrics offer a promising method for user identification, combining intrinsic liveness detection with morphological uniqueness. However, elevated heart rates introduce significant physiological variability, posing challenges to pattern recognition systems and leading to a notable performance gap between resting and post-exercise conditions. Addressing this gap is critical for advancing ECG-based biometric systems for real-world applications. We propose DE-PADA, a Dual Expert model with Personalized Augmentation and Domain Adaptation, designed to enhance robustness across diverse physiological states. The model is trained primarily on resting-state data from the evaluation dataset, without direct exposure to their exercise data. To address variability, DE-PADA incorporates ECG-specific innovations, including heartbeat segmentation into the PQRS interval, known for its relative temporal consistency, and the heart rate-sensitive ST interval, enabling targeted feature extraction tailored to each region's unique characteristics. Personalized augmentation simulates subject-specific T-wave variability across heart rates using individual T-wave peak predictions to adapt augmentation ranges. Domain adaptation further improves generalization by leveraging auxiliary data from supplementary subjects used exclusively for training, including both resting and exercise conditions. Experiments on the University of Toronto ECG Database demonstrate the model's effectiveness. DE-PADA achieves relative improvements in post-exercise identification rates of 26.75% in the initial recovery phase and 11.72% in the late recovery phase, while maintaining a 98.12% identification rate in the sitting position. These results highlight DE-PADA's ability to address intra-subject variability and enhance the robustness of ECG-based biometric systems across diverse physiological states.</li>
</ul>

<h3>Title: Enhancing Pre-Trained Decision Transformers with Prompt-Tuning Bandits</h3>
<ul>
<li><strong>Authors: </strong>Finn Rietz, Oleg Smirnov, Sara Karimi, Lele Cao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04979">https://arxiv.org/abs/2502.04979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04979">https://arxiv.org/pdf/2502.04979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04979]] Enhancing Pre-Trained Decision Transformers with Prompt-Tuning Bandits(https://arxiv.org/abs/2502.04979)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Harnessing large offline datasets is vital for training foundation models that can generalize across diverse tasks. Offline Reinforcement Learning (RL) offers a powerful framework for these scenarios, enabling the derivation of optimal policies even from suboptimal data. The Prompting Decision Transformer (PDT) is an offline RL multi-task model that distinguishes tasks through stochastic trajectory prompts, which are task-specific tokens maintained in context during rollouts. However, PDT samples these tokens uniformly at random from per-task demonstration datasets, failing to account for differences in token informativeness and potentially leading to performance degradation. To address this limitation, we introduce a scalable bandit-based prompt-tuning method that dynamically learns to construct high-performance trajectory prompts. Our approach significantly enhances downstream task performance without modifying the pre-trained Transformer backbone. Empirical results on benchmark tasks and a newly designed multi-task environment demonstrate the effectiveness of our method, creating a seamless bridge between general multi-task offline pre-training and task-specific online adaptation.</li>
</ul>

<h3>Title: Aligning Black-box Language Models with Human Judgments</h3>
<ul>
<li><strong>Authors: </strong>Gerrit J. J. van den Burg, Gen Suzuki, Wei Liu, Murat Sensoy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04997">https://arxiv.org/abs/2502.04997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04997">https://arxiv.org/pdf/2502.04997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04997]] Aligning Black-box Language Models with Human Judgments(https://arxiv.org/abs/2502.04997)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used as automated judges to evaluate recommendation systems, search engines, and other subjective tasks, where relying on human evaluators can be costly, time-consuming, and unscalable. LLMs offer an efficient solution for continuous, automated evaluation. However, since the systems that are built and improved with these judgments are ultimately designed for human use, it is crucial that LLM judgments align closely with human evaluators to ensure such systems remain human-centered. On the other hand, aligning LLM judgments with human evaluators is challenging due to individual variability and biases in human judgments. We propose a simple yet effective framework to align LLM judgments with individual human evaluators or their aggregated judgments, without retraining or fine-tuning the LLM. Our approach learns a linear mapping between the LLM's outputs and human judgments, achieving over 142% average improvement in agreement across 29 tasks with only a small number of calibration examples used for training. Notably, our method works in zero-shot and few-shot settings, exceeds inter-human agreement on four out of six tasks, and enables smaller LLMs to achieve performance comparable to that of larger models.</li>
</ul>

<h3>Title: Robust Graph Learning Against Adversarial Evasion Attacks via Prior-Free Diffusion-Based Structure Purification</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Luo, Qingyun Sun, Haonan Yuan, Xingcheng Fu, Jianxin Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05000">https://arxiv.org/abs/2502.05000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05000">https://arxiv.org/pdf/2502.05000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05000]] Robust Graph Learning Against Adversarial Evasion Attacks via Prior-Free Diffusion-Based Structure Purification(https://arxiv.org/abs/2502.05000)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Adversarial evasion attacks pose significant threats to graph learning, with lines of studies that have improved the robustness of Graph Neural Networks (GNNs). However, existing works rely on priors about clean graphs or attacking strategies, which are often heuristic and inconsistent. To achieve robust graph learning over different types of evasion attacks and diverse datasets, we investigate this problem from a prior-free structure purification perspective. Specifically, we propose a novel Diffusion-based Structure Purification framework named DiffSP, which creatively incorporates the graph diffusion model to learn intrinsic distributions of clean graphs and purify the perturbed structures by removing adversaries under the direction of the captured predictive patterns without relying on priors. DiffSP is divided into the forward diffusion process and the reverse denoising process, during which structure purification is achieved. To avoid valuable information loss during the forward process, we propose an LID-driven nonisotropic diffusion mechanism to selectively inject noise anisotropically. To promote semantic alignment between the clean graph and the purified graph generated during the reverse process, we reduce the generation uncertainty by the proposed graph transfer entropy guided denoising mechanism. Extensive experiments demonstrate the superior robustness of DiffSP against evasion attacks.</li>
</ul>

<h3>Title: QuEST: Stable Training of LLMs with 1-Bit Weights and Activations</h3>
<ul>
<li><strong>Authors: </strong>Andrei Panferov, Jiale Chen, Soroush Tabesh, Roberto L. Castro, Mahdi Nikdan, Dan Alistarh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05003">https://arxiv.org/abs/2502.05003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05003">https://arxiv.org/pdf/2502.05003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05003]] QuEST: Stable Training of LLMs with 1-Bit Weights and Activations(https://arxiv.org/abs/2502.05003)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>One approach to reducing the massive costs of large language models (LLMs) is the use of quantized or sparse representations for training or deployment. While post-training compression methods are very popular, the question of obtaining even more accurate compressed models by directly training over such representations, i.e., Quantization-Aware Training (QAT), is still open: for example, a recent study (arXiv:2411.04330v2) put the "optimal" bit-width at which models can be trained using QAT, while staying accuracy-competitive with standard FP16/BF16 precision, at 8-bits weights and activations. We advance this state-of-the-art via a new method called QuEST, which is Pareto-competitive with FP16, i.e., it provides better accuracy at lower model size, while training models with weights and activations in 4-bits or less. Moreover, QuEST allows stable training with 1-bit weights and activations. QuEST achieves this by improving two key aspects of QAT methods: (1) accurate and fast quantization of the (continuous) distributions of weights and activations via Hadamard normalization and MSE-optimal fitting; (2) a new trust gradient estimator based on the idea of explicitly minimizing the error between the noisy gradient computed over quantized states and the "true" (but unknown) full-precision gradient. Experiments on Llama-type architectures show that QuEST induces stable scaling laws across the entire range of hardware-supported precisions, and can be extended to sparse representations. We provide GPU kernel support showing that models produced by QuEST can be executed efficiently. Our code is available at this https URL.</li>
</ul>

<h3>Title: Learning the Language of NVMe Streams for Ransomware Detection</h3>
<ul>
<li><strong>Authors: </strong>Barak Bringoltz, Elisha Halperin, Ran Feraru, Evgeny Blaichman, Amit Berman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05011">https://arxiv.org/abs/2502.05011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05011">https://arxiv.org/pdf/2502.05011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05011]] Learning the Language of NVMe Streams for Ransomware Detection(https://arxiv.org/abs/2502.05011)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We apply language modeling techniques to detect ransomware activity in NVMe command sequences. We design and train two types of transformer-based models: the Command-Level Transformer (CLT) performs in-context token classification to determine whether individual commands are initiated by ransomware, and the Patch-Level Transformer (PLT) predicts the volume of data accessed by ransomware within a patch of commands. We present both model designs and the corresponding tokenization and embedding schemes and show that they improve over state-of-the-art tabular methods by up to 24% in missed-detection rate, 66% in data loss prevention, and 84% in identifying data accessed by ransomware.</li>
</ul>

<h3>Title: MindAligner: Explicit Brain Functional Alignment for Cross-Subject Visual Decoding from Limited fMRI Data</h3>
<ul>
<li><strong>Authors: </strong>Yuqin Dai, Zhouheng Yao, Chunfeng Song, Qihao Zheng, Weijian Mai, Kunyu Peng, Shuai Lu, Wanli Ouyang, Jian Yang, Jiamin Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05034">https://arxiv.org/abs/2502.05034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05034">https://arxiv.org/pdf/2502.05034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05034]] MindAligner: Explicit Brain Functional Alignment for Cross-Subject Visual Decoding from Limited fMRI Data(https://arxiv.org/abs/2502.05034)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Brain decoding aims to reconstruct visual perception of human subject from fMRI signals, which is crucial for understanding brain's perception mechanisms. Existing methods are confined to the single-subject paradigm due to substantial brain variability, which leads to weak generalization across individuals and incurs high training costs, exacerbated by limited availability of fMRI data. To address these challenges, we propose MindAligner, an explicit functional alignment framework for cross-subject brain decoding from limited fMRI data. The proposed MindAligner enjoys several merits. First, we learn a Brain Transfer Matrix (BTM) that projects the brain signals of an arbitrary new subject to one of the known subjects, enabling seamless use of pre-trained decoding models. Second, to facilitate reliable BTM learning, a Brain Functional Alignment module is proposed to perform soft cross-subject brain alignment under different visual stimuli with a multi-level brain alignment loss, uncovering fine-grained functional correspondences with high interpretability. Experiments indicate that MindAligner not only outperforms existing methods in visual decoding under data-limited conditions, but also provides valuable neuroscience insights in cross-subject functional analysis. The code will be made publicly available.</li>
</ul>

<h3>Title: nvAgent: Automated Data Visualization from Natural Language via Collaborative Agent Workflow</h3>
<ul>
<li><strong>Authors: </strong>Geliang Ouyang, Jingyao Chen, Zhihe Nie, Yi Gui, Yao Wan, Hongyu Zhang, Dongping Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05036">https://arxiv.org/abs/2502.05036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05036">https://arxiv.org/pdf/2502.05036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05036]] nvAgent: Automated Data Visualization from Natural Language via Collaborative Agent Workflow(https://arxiv.org/abs/2502.05036)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Natural Language to Visualization (NL2Vis) seeks to convert natural-language descriptions into visual representations of given tables, empowering users to derive insights from large-scale data. Recent advancements in Large Language Models (LLMs) show promise in automating code generation to transform tabular data into accessible visualizations. However, they often struggle with complex queries that require reasoning across multiple tables. To address this limitation, we propose a collaborative agent workflow, termed nvAgent, for NL2Vis. Specifically, nvAgent comprises three agents: a processor agent for database processing and context filtering, a composer agent for planning visualization generation, and a validator agent for code translation and output verification. Comprehensive evaluations on the new VisEval benchmark demonstrate that nvAgent consistently surpasses state-of-the-art baselines, achieving a 7.88% improvement in single-table and a 9.23% improvement in multi-table scenarios. Qualitative analyses further highlight that nvAgent maintains nearly a 20% performance margin over previous models, underscoring its capacity to produce high-quality visual representations from complex, heterogeneous data sources.</li>
</ul>

<h3>Title: GaussRender: Learning 3D Occupancy with Gaussian Rendering</h3>
<ul>
<li><strong>Authors: </strong>Loick Chambon, Eloi Zablocki, Alexandre Boulch, Mickael Chen, Matthieu Cord</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05040">https://arxiv.org/abs/2502.05040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05040">https://arxiv.org/pdf/2502.05040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05040]] GaussRender: Learning 3D Occupancy with Gaussian Rendering(https://arxiv.org/abs/2502.05040)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Understanding the 3D geometry and semantics of driving scenes is critical for developing of safe autonomous vehicles. While 3D occupancy models are typically trained using voxel-based supervision with standard losses (e.g., cross-entropy, Lovasz, dice), these approaches treat voxel predictions independently, neglecting their spatial relationships. In this paper, we propose GaussRender, a plug-and-play 3D-to-2D reprojection loss that enhances voxel-based supervision. Our method projects 3D voxel representations into arbitrary 2D perspectives and leverages Gaussian splatting as an efficient, differentiable rendering proxy of voxels, introducing spatial dependencies across projected elements. This approach improves semantic and geometric consistency, handles occlusions more efficiently, and requires no architectural modifications. Extensive experiments on multiple benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes, SSCBench-KITTI360) demonstrate consistent performance gains across various 3D occupancy models (TPVFormer, SurroundOcc, Symphonies), highlighting the robustness and versatility of our framework. The code is available at this https URL.</li>
</ul>

<h3>Title: Federated Learning for Anomaly Detection in Energy Consumption Data: Assessing the Vulnerability to Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Yohannis Kifle Telila, Damitha Senevirathne, Dumindu Tissera, Apurva Narayan, Miriam A.M. Capretz, Katarina Grolinger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05041">https://arxiv.org/abs/2502.05041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05041">https://arxiv.org/pdf/2502.05041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05041]] Federated Learning for Anomaly Detection in Energy Consumption Data: Assessing the Vulnerability to Adversarial Attacks(https://arxiv.org/abs/2502.05041)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack, federate, transformer</a></li>
<li><strong>Abstract: </strong>Anomaly detection is crucial in the energy sector to identify irregular patterns indicating equipment failures, energy theft, or other issues. Machine learning techniques for anomaly detection have achieved great success, but are typically centralized, involving sharing local data with a central server which raises privacy and security concerns. Federated Learning (FL) has been gaining popularity as it enables distributed learning without sharing local data. However, FL depends on neural networks, which are vulnerable to adversarial attacks that manipulate data, leading models to make erroneous predictions. While adversarial attacks have been explored in the image domain, they remain largely unexplored in time series problems, especially in the energy domain. Moreover, the effect of adversarial attacks in the FL setting is also mostly unknown. This paper assesses the vulnerability of FL-based anomaly detection in energy data to adversarial attacks. Specifically, two state-of-the-art models, Long Short Term Memory (LSTM) and Transformers, are used to detect anomalies in an FL setting, and two white-box attack methods, Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD), are employed to perturb the data. The results show that FL is more sensitive to PGD attacks than to FGSM attacks, attributed to PGD's iterative nature, resulting in an accuracy drop of over 10% even with naive, weaker attacks. Moreover, FL is more affected by these attacks than centralized learning, highlighting the need for defense mechanisms in FL.</li>
</ul>

<h3>Title: New Security Challenges Towards In-Sensor Computing Systems</h3>
<ul>
<li><strong>Authors: </strong>Mashrafi Kajol, Qiaoyan Yu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05046">https://arxiv.org/abs/2502.05046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05046">https://arxiv.org/pdf/2502.05046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05046]] New Security Challenges Towards In-Sensor Computing Systems(https://arxiv.org/abs/2502.05046)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Data collection and processing in advanced health monitoring systems are experiencing revolutionary change. In-Sensor Computing (ISC) systems emerge as a promising alternative to save energy on massive data transmission, analog-to-digital conversion, and ineffective processing. While the new paradigm shift of ISC systems gains increasing attention, the highly compacted systems could incur new challenges from a hardware security perspective. This work first conducts a literature review to highlight the research trend of this topic and then performs comprehensive analyses on the root of security challenges. This is the first work that compares the security challenges of traditional sensor-involved computing systems and emerging ISC systems. Furthermore, new attack scenarios are predicted for board-, chip-, and device-level ISC systems. Two proof-of-concept demos are provided to inspire new countermeasure designs against unique hardware security threats in ISC systems.</li>
</ul>

<h3>Title: Beautiful Images, Toxic Words: Understanding and Addressing Offensive Text in Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Aditya Kumar, Tom Blanchard, Adam Dziedzic, Franziska Boenisch</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05066">https://arxiv.org/abs/2502.05066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05066">https://arxiv.org/pdf/2502.05066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05066]] Beautiful Images, Toxic Words: Understanding and Addressing Offensive Text in Generated Images(https://arxiv.org/abs/2502.05066)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>State-of-the-art visual generation models, such as Diffusion Models (DMs) and Vision Auto-Regressive Models (VARs), produce highly realistic images. While prior work has successfully mitigated Not Safe For Work (NSFW) content in the visual domain, we identify a novel threat: the generation of NSFW text embedded within images. This includes offensive language, such as insults, racial slurs, and sexually explicit terms, posing significant risks to users. We show that all state-of-the-art DMs (e.g., SD3, Flux, DeepFloyd IF) and VARs (e.g., Infinity) are vulnerable to this issue. Through extensive experiments, we demonstrate that existing mitigation techniques, effective for visual content, fail to prevent harmful text generation while substantially degrading benign text generation. As an initial step toward addressing this threat, we explore safety fine-tuning of the text encoder underlying major DM architectures using a customized dataset. Thereby, we suppress NSFW generation while preserving overall image and text generation quality. Finally, to advance research in this area, we introduce ToxicBench, an open-source benchmark for evaluating NSFW text generation in images. ToxicBench provides a curated dataset of harmful prompts, new metrics, and an evaluation pipeline assessing both NSFW-ness and generation quality. Our benchmark aims to guide future efforts in mitigating NSFW text generation in text-to-image models and is available at this https URL</li>
</ul>

<h3>Title: Paying Attention to Facts: Quantifying the Knowledge Capacity of Attention Layers</h3>
<ul>
<li><strong>Authors: </strong>Liang Ze Wong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05076">https://arxiv.org/abs/2502.05076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05076">https://arxiv.org/pdf/2502.05076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05076]] Paying Attention to Facts: Quantifying the Knowledge Capacity of Attention Layers(https://arxiv.org/abs/2502.05076)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate the ability of single-layer attention-only transformers (i.e. attention layers) to memorize facts contained in databases from a linear-algebraic perspective. We associate with each database a 3-tensor, propose the rank of this tensor as a measure of the size of the database, and provide bounds on the rank in terms of properties of the database. We also define a 3-tensor corresponding to an attention layer, and empirically demonstrate the relationship between its rank and database rank on a dataset of toy models and random databases. By highlighting the roles played by the value-output and query-key weights, and the effects of argmax and softmax on rank, our results shed light on the `additive motif' of factual recall in transformers, while also suggesting a way of increasing layer capacity without increasing the number of parameters.</li>
</ul>

<h3>Title: ChallengeMe: An Adversarial Learning-enabled Text Summarization Framework</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Deng, Ye Zhang, Tianmin Guo, Yongzhe Zhang, Zhengjian Kang, Hang Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05084">https://arxiv.org/abs/2502.05084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05084">https://arxiv.org/pdf/2502.05084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05084]] ChallengeMe: An Adversarial Learning-enabled Text Summarization Framework(https://arxiv.org/abs/2502.05084)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The astonishing performance of large language models (LLMs) and their remarkable achievements in production and daily life have led to their widespread application in collaborative tasks. However, current large models face challenges such as hallucination and lack of specificity in content generation in vertical domain tasks. Inspired by the contrast and classification mechanisms in human cognitive processes, this paper constructs an adversarial learning-based prompt framework named ChallengeMe, which includes three cascaded solutions: generation prompts, evaluation prompts, and feedback optimization. In this process, we designed seven core optimization dimensions and set the threshold for adversarial learning. The results of mixed case studies on the text summarization task show that the proposed framework can generate more accurate and fluent text summaries compared to the current advanced mainstream LLMs.</li>
</ul>

<h3>Title: Causality can systematically address the monsters under the bench(marks)</h3>
<ul>
<li><strong>Authors: </strong>Felix Leeb, Zhijing Jin, Bernhard Schölkopf</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05085">https://arxiv.org/abs/2502.05085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05085">https://arxiv.org/pdf/2502.05085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05085]] Causality can systematically address the monsters under the bench(marks)(https://arxiv.org/abs/2502.05085)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Effective and reliable evaluation is essential for advancing empirical machine learning. However, the increasing accessibility of generalist models and the progress towards ever more complex, high-level tasks make systematic evaluation more challenging. Benchmarks are plagued by various biases, artifacts, or leakage, while models may behave unreliably due to poorly explored failure modes. Haphazard treatments and inconsistent formulations of such "monsters" can contribute to a duplication of efforts, a lack of trust in results, and unsupported inferences. In this position paper, we argue causality offers an ideal framework to systematically address these challenges. By making causal assumptions in an approach explicit, we can faithfully model phenomena, formulate testable hypotheses with explanatory power, and leverage principled tools for analysis. To make causal model design more accessible, we identify several useful Common Abstract Topologies (CATs) in causal graphs which help gain insight into the reasoning abilities in large language models. Through a series of case studies, we demonstrate how the precise yet pragmatic language of causality clarifies the strengths and limitations of a method and inspires new approaches for systematic progress.</li>
</ul>

<h3>Title: Mitigating Unintended Memorization with LoRA in Federated Learning for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Thierry Bossy, Julien Vignoud, Tahseen Rabbani, Juan R. Troncoso Pastoriza, Martin Jaggi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05087">https://arxiv.org/abs/2502.05087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05087">https://arxiv.org/pdf/2502.05087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05087]] Mitigating Unintended Memorization with LoRA in Federated Learning for LLMs(https://arxiv.org/abs/2502.05087)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a popular paradigm for collaborative training which avoids direct data exposure between clients. However, data privacy issues still remain: FL-trained large language models are capable of memorizing and completing phrases and sentences contained in training data when given with their prefixes. Thus, it is possible for adversarial and honest-but-curious clients to recover training data of other participants simply through targeted prompting. In this work, we demonstrate that a popular and simple fine-tuning strategy, low-rank adaptation (LoRA), reduces memorization during FL up to a factor of 10. We study this effect by performing a medical question-answering fine-tuning task and injecting multiple replicas of out-of-distribution sensitive sequences drawn from an external clinical dataset. We observe a reduction in memorization for a wide variety of Llama 2 and 3 models, and find that LoRA can reduce memorization in centralized learning as well. Furthermore, we show that LoRA can be combined with other privacy-preserving techniques such as gradient clipping and Gaussian noising, secure aggregation, and Goldfish loss to further improve record-level privacy while maintaining performance.</li>
</ul>

<h3>Title: DCFormer: Efficient 3D Vision-Language Modeling with Decomposed Convolutions</h3>
<ul>
<li><strong>Authors: </strong>Gorkem Can Ates, Kuang Gong, Wei Shao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05091">https://arxiv.org/abs/2502.05091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05091">https://arxiv.org/pdf/2502.05091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05091]] DCFormer: Efficient 3D Vision-Language Modeling with Decomposed Convolutions(https://arxiv.org/abs/2502.05091)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) align visual and textual representations, enabling high-performance zero-shot classification and image-text retrieval in 2D medical imaging. However, extending VLMs to 3D medical imaging remains computationally challenging. Existing 3D VLMs rely on Vision Transformers (ViTs), which are computationally expensive due to self-attention's quadratic complexity, or 3D convolutions, which demand excessive parameters and FLOPs as kernel size increases. We introduce DCFormer, an efficient 3D medical image encoder that factorizes 3D convolutions into three parallel 1D convolutions along depth, height, and width. This design preserves spatial information while significantly reducing computational cost. Integrated into a CLIP-based vision-language framework, DCFormer is evaluated on CT-RATE, a dataset of 50,188 paired 3D chest CT volumes and radiology reports, for zero-shot multi-abnormality detection across 18 pathologies. Compared to ViT, ConvNeXt, PoolFormer, and TransUNet, DCFormer achieves superior efficiency and accuracy, with DCFormer-Tiny reaching 62.0% accuracy and a 46.3% F1-score while using significantly fewer parameters. These results highlight DCFormer's potential for scalable, clinically deployable 3D medical VLMs. Our codes will be publicly available.</li>
</ul>

<h3>Title: Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rohit Saxena, Aryo Pradipta Gema, Pasquale Minervini</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05092">https://arxiv.org/abs/2502.05092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05092">https://arxiv.org/pdf/2502.05092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05092]] Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs(https://arxiv.org/abs/2502.05092)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding time from visual representations is a fundamental cognitive skill, yet it remains a challenge for multimodal large language models (MLLMs). In this work, we investigate the capabilities of MLLMs in interpreting time and date through analogue clocks and yearly calendars. To facilitate this, we curated a structured dataset comprising two subsets: 1) $\textit{ClockQA}$, which comprises various types of clock styles$-$standard, black-dial, no-second-hand, Roman numeral, and arrow-hand clocks$-$paired with time related questions; and 2) $\textit{CalendarQA}$, which consists of yearly calendar images with questions ranging from commonly known dates (e.g., Christmas, New Year's Day) to computationally derived ones (e.g., the 100th or 153rd day of the year). We aim to analyse how MLLMs can perform visual recognition, numerical reasoning, and temporal inference when presented with time-related visual data. Our evaluations show that despite recent advancements, reliably understanding time remains a significant challenge for MLLMs.</li>
</ul>

<h3>Title: Leveraging Hypernetworks and Learnable Kernels for Consumer Energy Forecasting Across Diverse Consumer Types</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Umair Danish, Katarina Grolinger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05104">https://arxiv.org/abs/2502.05104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05104">https://arxiv.org/pdf/2502.05104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05104]] Leveraging Hypernetworks and Learnable Kernels for Consumer Energy Forecasting Across Diverse Consumer Types(https://arxiv.org/abs/2502.05104)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Consumer energy forecasting is essential for managing energy consumption and planning, directly influencing operational efficiency, cost reduction, personalized energy management, and sustainability efforts. In recent years, deep learning techniques, especially LSTMs and transformers, have been greatly successful in the field of energy consumption forecasting. Nevertheless, these techniques have difficulties in capturing complex and sudden variations, and, moreover, they are commonly examined only on a specific type of consumer (e.g., only offices, only schools). Consequently, this paper proposes HyperEnergy, a consumer energy forecasting strategy that leverages hypernetworks for improved modeling of complex patterns applicable across a diversity of consumers. Hypernetwork is responsible for predicting the parameters of the primary prediction network, in our case LSTM. A learnable adaptable kernel, comprised of polynomial and radial basis function kernels, is incorporated to enhance performance. The proposed HyperEnergy was evaluated on diverse consumers including, student residences, detached homes, a home with electric vehicle charging, and a townhouse. Across all consumer types, HyperEnergy consistently outperformed 10 other techniques, including state-of-the-art models such as LSTM, AttentionLSTM, and transformer.</li>
</ul>

<h3>Title: Flexible and Efficient Grammar-Constrained Decoding</h3>
<ul>
<li><strong>Authors: </strong>Kanghee Park, Timothy Zhou, Loris D'Antoni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05111">https://arxiv.org/abs/2502.05111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05111">https://arxiv.org/pdf/2502.05111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05111]] Flexible and Efficient Grammar-Constrained Decoding(https://arxiv.org/abs/2502.05111)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are often asked to generate structured outputs that obey precise syntactic rules, such as code snippets or formatted data. Grammar-constrained decoding (GCD) can guarantee that LLM outputs matches such rules by masking out tokens that will provably lead to outputs that do not belong to a specified context-free grammar (CFG). To guarantee soundness, GCD algorithms have to compute how a given LLM subword tokenizer can align with the tokens used by a given context-free grammar and compute token masks based on this information. Doing so efficiently is challenging and existing GCD algorithms require tens of minutes to preprocess common grammars. We present a new GCD algorithm together with an implementation that offers 17.71x faster offline preprocessing than existing approaches while preserving state-of-the-art efficiency in online mask computation.</li>
</ul>

<h3>Title: Self-supervised Conformal Prediction for Uncertainty Quantification in Imaging Problems</h3>
<ul>
<li><strong>Authors: </strong>Jasper M. Everink, Bernardin Tamo Amougou, Marcelo Pereyra</a></li>
<li><strong>Subjects: </strong>cs.CV, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05127">https://arxiv.org/abs/2502.05127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05127">https://arxiv.org/pdf/2502.05127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05127]] Self-supervised Conformal Prediction for Uncertainty Quantification in Imaging Problems(https://arxiv.org/abs/2502.05127)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Most image restoration problems are ill-conditioned or ill-posed and hence involve significant uncertainty. Quantifying this uncertainty is crucial for reliably interpreting experimental results, particularly when reconstructed images inform critical decisions and science. However, most existing image restoration methods either fail to quantify uncertainty or provide estimates that are highly inaccurate. Conformal prediction has recently emerged as a flexible framework to equip any estimator with uncertainty quantification capabilities that, by construction, have nearly exact marginal coverage. To achieve this, conformal prediction relies on abundant ground truth data for calibration. However, in image restoration problems, reliable ground truth data is often expensive or not possible to acquire. Also, reliance on ground truth data can introduce large biases in situations of distribution shift between calibration and deployment. This paper seeks to develop a more robust approach to conformal prediction for image restoration problems by proposing a self-supervised conformal prediction method that leverages Stein's Unbiased Risk Estimator (SURE) to self-calibrate itself directly from the observed noisy measurements, bypassing the need for ground truth. The method is suitable for any linear imaging inverse problem that is ill-conditioned, and it is especially powerful when used with modern self-supervised image restoration techniques that can also be trained directly from measurement data. The proposed approach is demonstrated through numerical experiments on image denoising and deblurring, where it delivers results that are remarkably accurate and comparable to those obtained by supervised conformal prediction with ground truth data.</li>
</ul>

<h3>Title: CodeSCM: Causal Analysis for Multi-Modal Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Mukur Gupta, Noopur Bhatt, Suman Jana</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05150">https://arxiv.org/abs/2502.05150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05150">https://arxiv.org/pdf/2502.05150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05150]] CodeSCM: Causal Analysis for Multi-Modal Code Generation(https://arxiv.org/abs/2502.05150)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we propose CodeSCM, a Structural Causal Model (SCM) for analyzing multi-modal code generation using large language models (LLMs). By applying interventions to CodeSCM, we measure the causal effects of different prompt modalities, such as natural language, code, and input-output examples, on the model. CodeSCM introduces latent mediator variables to separate the code and natural language semantics of a multi-modal code generation prompt. Using the principles of Causal Mediation Analysis on these mediators we quantify direct effects representing the model's spurious leanings. We find that, in addition to natural language instructions, input-output examples significantly influence code generation.</li>
</ul>

<h3>Title: Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Steffen Eger, Yong Cao, Jennifer D'Souza, Andreas Geiger, Christian Greisinger, Stephanie Gross, Yufang Hou, Brigitte Krenn, Anne Lauscher, Yizhi Li, Chenghua Lin, Nafise Sadat Moosavi, Wei Zhao, Tristan Miller</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05151">https://arxiv.org/abs/2502.05151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05151">https://arxiv.org/pdf/2502.05151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05151]] Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation(https://arxiv.org/abs/2502.05151)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the advent of large multimodal language models, science is now at a threshold of an AI-based technological transformation. Recently, a plethora of new AI models and tools has been proposed, promising to empower researchers and academics worldwide to conduct their research more effectively and efficiently. This includes all aspects of the research cycle, especially (1) searching for relevant literature; (2) generating research ideas and conducting experimentation; generating (3) text-based and (4) multimodal content (e.g., scientific figures and diagrams); and (5) AI-based automatic peer review. In this survey, we provide an in-depth overview over these exciting recent developments, which promise to fundamentally alter the scientific research process for good. Our survey covers the five aspects outlined above, indicating relevant datasets, methods and results (including evaluation) as well as limitations and scope for future research. Ethical concerns regarding shortcomings of these tools and potential for misuse (fake science, plagiarism, harms to research integrity) take a particularly prominent place in our discussion. We hope that our survey will not only become a reference guide for newcomers to the field but also a catalyst for new AI-based initiatives in the area of "AI4Science".</li>
</ul>

<h3>Title: Hummingbird: High Fidelity Image Generation via Multimodal Context Alignment</h3>
<ul>
<li><strong>Authors: </strong>Minh-Quan Le, Gaurav Mittal, Tianjian Meng, A S M Iftekhar, Vishwas Suryanarayanan, Barun Patra, Dimitris Samaras, Mei Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05153">https://arxiv.org/abs/2502.05153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05153">https://arxiv.org/pdf/2502.05153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05153]] Hummingbird: High Fidelity Image Generation via Multimodal Context Alignment(https://arxiv.org/abs/2502.05153)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>While diffusion models are powerful in generating high-quality, diverse synthetic data for object-centric tasks, existing methods struggle with scene-aware tasks such as Visual Question Answering (VQA) and Human-Object Interaction (HOI) Reasoning, where it is critical to preserve scene attributes in generated images consistent with a multimodal context, i.e. a reference image with accompanying text guidance query. To address this, we introduce Hummingbird, the first diffusion-based image generator which, given a multimodal context, generates highly diverse images w.r.t. the reference image while ensuring high fidelity by accurately preserving scene attributes, such as object interactions and spatial relationships from the text guidance. Hummingbird employs a novel Multimodal Context Evaluator that simultaneously optimizes our formulated Global Semantic and Fine-grained Consistency Rewards to ensure generated images preserve the scene attributes of reference images in relation to the text guidance while maintaining diversity. As the first model to address the task of maintaining both diversity and fidelity given a multimodal context, we introduce a new benchmark formulation incorporating MME Perception and Bongard HOI datasets. Benchmark experiments show Hummingbird outperforms all existing methods by achieving superior fidelity while maintaining diversity, validating Hummingbird's potential as a robust multimodal context-aligned image generator in complex visual tasks.</li>
</ul>

<h3>Title: Efficient distributional regression trees learning algorithms for calibrated non-parametric probabilistic forecasts</h3>
<ul>
<li><strong>Authors: </strong>Duchemin Quentin, Obozinski Guillaume</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05157">https://arxiv.org/abs/2502.05157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05157">https://arxiv.org/pdf/2502.05157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05157]] Efficient distributional regression trees learning algorithms for calibrated non-parametric probabilistic forecasts(https://arxiv.org/abs/2502.05157)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>The perspective of developing trustworthy AI for critical applications in science and engineering requires machine learning techniques that are capable of estimating their own uncertainty. In the context of regression, instead of estimating a conditional mean, this can be achieved by producing a predictive interval for the output, or to even learn a model of the conditional probability $p(y|x)$ of an output $y$ given input features $x$. While this can be done under parametric assumptions with, e.g. generalized linear model, these are typically too strong, and non-parametric models offer flexible alternatives. In particular, for scalar outputs, learning directly a model of the conditional cumulative distribution function of $y$ given $x$ can lead to more precise probabilistic estimates, and the use of proper scoring rules such as the weighted interval score (WIS) and the continuous ranked probability score (CRPS) lead to better coverage and calibration properties. This paper introduces novel algorithms for learning probabilistic regression trees for the WIS or CRPS loss functions. These algorithms are made computationally efficient thanks to an appropriate use of known data structures - namely min-max heaps, weight-balanced binary trees and Fenwick trees. Through numerical experiments, we demonstrate that the performance of our methods is competitive with alternative approaches. Additionally, our methods benefit from the inherent interpretability and explainability of trees. As a by-product, we show how our trees can be used in the context of conformal prediction and explain why they are particularly well-suited for achieving group-conditional coverage guarantees.</li>
</ul>

<h3>Title: A Lightweight Method to Disrupt Memorized Sequences in LLM</h3>
<ul>
<li><strong>Authors: </strong>Parjanya Prajakta Prashant, Kaustubh Ponkshe, Babak Salimi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05159">https://arxiv.org/abs/2502.05159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05159">https://arxiv.org/pdf/2502.05159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05159]] A Lightweight Method to Disrupt Memorized Sequences in LLM(https://arxiv.org/abs/2502.05159)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate impressive capabilities across many tasks yet risk reproducing copyrighted content verbatim, raising legal and ethical concerns. Although methods like differential privacy or neuron editing can reduce memorization, they typically require costly retraining or direct access to model weights and may degrade performance. To address these challenges, we propose TokenSwap, a lightweight, post-hoc approach that replaces the probabilities of grammar-related tokens with those from a small auxiliary model (e.g., DistilGPT-2). We run extensive experiments on commercial grade models such as Pythia-6.9b and LLaMA-3-8b and demonstrate that our method effectively reduces well-known cases of memorized generation by upto 10x with little to no impact on downstream tasks. Our approach offers a uniquely accessible and effective solution to users of real-world systems.</li>
</ul>

<h3>Title: DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails</h3>
<ul>
<li><strong>Authors: </strong>Yihe Deng, Yu Yang, Junkai Zhang, Wei Wang, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05163">https://arxiv.org/abs/2502.05163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05163">https://arxiv.org/pdf/2502.05163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05163]] DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails(https://arxiv.org/abs/2502.05163)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages. To address this gap, we propose a novel two-player Reinforcement Learning (RL) framework, where a generator and a guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as a two-player game, proving convergence to a Nash equilibrium. Empirical evaluations show that our model \ours outperforms state-of-the-art models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5x faster at inference with a significantly smaller model (0.5B). We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lower-resource languages in a collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. Code, model, and data will be open-sourced at this https URL.</li>
</ul>

<h3>Title: In-context denoising with one-layer transformers: connections between attention and associative memory retrieval</h3>
<ul>
<li><strong>Authors: </strong>Matthew Smart, Alberto Bietti, Anirvan M. Sengupta</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05164">https://arxiv.org/abs/2502.05164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05164">https://arxiv.org/pdf/2502.05164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05164]] In-context denoising with one-layer transformers: connections between attention and associative memory retrieval(https://arxiv.org/abs/2502.05164)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce in-context denoising, a task that refines the connection between attention-based architectures and dense associative memory (DAM) networks, also known as modern Hopfield networks. Using a Bayesian framework, we show theoretically and empirically that certain restricted denoising problems can be solved optimally even by a single-layer transformer. We demonstrate that a trained attention layer processes each denoising prompt by performing a single gradient descent update on a context-aware DAM energy landscape, where context tokens serve as associative memories and the query token acts as an initial state. This one-step update yields better solutions than exact retrieval of either a context token or a spurious local minimum, providing a concrete example of DAM networks extending beyond the standard retrieval paradigm. Overall, this work solidifies the link between associative memory and attention mechanisms first identified by Ramsauer et al., and demonstrates the relevance of associative memory models in the study of in-context learning.</li>
</ul>

<h3>Title: Multitwine: Multi-Object Compositing with Text and Layout Control</h3>
<ul>
<li><strong>Authors: </strong>Gemma Canet Tarrés, Zhe Lin, Zhifei Zhang, He Zhang, Andrew Gilbert, John Collomosse, Soo Ye Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05165">https://arxiv.org/abs/2502.05165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05165">https://arxiv.org/pdf/2502.05165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05165]] Multitwine: Multi-Object Compositing with Text and Layout Control(https://arxiv.org/abs/2502.05165)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce the first generative model capable of simultaneous multi-object compositing, guided by both text and layout. Our model allows for the addition of multiple objects within a scene, capturing a range of interactions from simple positional relations (e.g., next to, in front of) to complex actions requiring reposing (e.g., hugging, playing guitar). When an interaction implies additional props, like `taking a selfie', our model autonomously generates these supporting objects. By jointly training for compositing and subject-driven generation, also known as customization, we achieve a more balanced integration of textual and visual inputs for text-driven object compositing. As a result, we obtain a versatile model with state-of-the-art performance in both tasks. We further present a data generation pipeline leveraging visual and language models to effortlessly synthesize multimodal, aligned training data.</li>
</ul>

<h3>Title: NoLiMa: Long-Context Evaluation Beyond Literal Matching</h3>
<ul>
<li><strong>Authors: </strong>Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan A. Rossi, Seunghyun Yoon, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05167">https://arxiv.org/abs/2502.05167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05167">https://arxiv.org/pdf/2502.05167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05167]] NoLiMa: Long-Context Evaluation Beyond Literal Matching(https://arxiv.org/abs/2502.05167)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a "needle" (relevant information) from a "haystack" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information.</li>
</ul>

<h3>Title: MELON: Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison</h3>
<ul>
<li><strong>Authors: </strong>Kaijie Zhu, Xianjun Yang, Jindong Wang, Wenbo Guo, William Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05174">https://arxiv.org/abs/2502.05174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05174">https://arxiv.org/pdf/2502.05174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05174]] MELON: Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison(https://arxiv.org/abs/2502.05174)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Recent research has explored that LLM agents are vulnerable to indirect prompt injection (IPI) attacks, where malicious tasks embedded in tool-retrieved information can redirect the agent to take unauthorized actions. Existing defenses against IPI have significant limitations: either require essential model training resources, lack effectiveness against sophisticated attacks, or harm the normal utilities. We present MELON (Masked re-Execution and TooL comparisON), a novel IPI defense. Our approach builds on the observation that under a successful attack, the agent's next action becomes less dependent on user tasks and more on malicious tasks. Following this, we design MELON to detect attacks by re-executing the agent's trajectory with a masked user prompt modified through a masking function. We identify an attack if the actions generated in the original and masked executions are similar. We also include three key designs to reduce the potential false positives and false negatives. Extensive evaluation on the IPI benchmark AgentDojo demonstrates that MELON outperforms SOTA defenses in both attack prevention and utility preservation. Moreover, we show that combining MELON with a SOTA prompt augmentation defense (denoted as MELON-Aug) further improves its performance. We also conduct a detailed ablation study to validate our key designs.</li>
</ul>

<h3>Title: Fillerbuster: Multi-View Scene Completion for Casual Captures</h3>
<ul>
<li><strong>Authors: </strong>Ethan Weber, Norman Müller, Yash Kant, Vasu Agrawal, Michael Zollhöfer, Angjoo Kanazawa, Christian Richardt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05175">https://arxiv.org/abs/2502.05175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05175">https://arxiv.org/pdf/2502.05175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05175]] Fillerbuster: Multi-View Scene Completion for Casual Captures(https://arxiv.org/abs/2502.05175)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>We present Fillerbuster, a method that completes unknown regions of a 3D scene by utilizing a novel large-scale multi-view latent diffusion transformer. Casual captures are often sparse and miss surrounding content behind objects or above the scene. Existing methods are not suitable for handling this challenge as they focus on making the known pixels look good with sparse-view priors, or on creating the missing sides of objects from just one or two photos. In reality, we often have hundreds of input frames and want to complete areas that are missing and unobserved from the input frames. Additionally, the images often do not have known camera parameters. Our solution is to train a generative model that can consume a large context of input frames while generating unknown target views and recovering image poses when desired. We show results where we complete partial captures on two existing datasets. We also present an uncalibrated scene completion task where our unified model predicts both poses and creates new content. Our model is the first to predict many images and poses together for scene completion.</li>
</ul>

<h3>Title: AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360{\deg} Unbounded Scene Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Jie-Ying Lee, Bo-Hsu Ke, Chun-Wei Tuan Mu, Yi-Chuan Huang, Chin-Yang Lin, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05176">https://arxiv.org/abs/2502.05176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05176">https://arxiv.org/pdf/2502.05176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05176]] AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360{\deg} Unbounded Scene Inpainting(https://arxiv.org/abs/2502.05176)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360° unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360° unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. See our project page for video results and the dataset at this https URL.</li>
</ul>

<h3>Title: Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuray</h3>
<ul>
<li><strong>Authors: </strong>Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Xiawu Zheng, Yan Zhang, Yiyi Zhou, Rongrong Ji, Xing Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05177">https://arxiv.org/abs/2502.05177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05177">https://arxiv.org/pdf/2502.05177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05177]] Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuray(https://arxiv.org/abs/2502.05177)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Establishing the long-context capability of large vision-language models is crucial for video understanding, high-resolution image understanding, multi-modal agents and reasoning. We introduce Long-VITA, a simple yet effective large multi-modal model for long-context visual-language understanding tasks. It is adept at concurrently processing and analyzing modalities of image, video, and text over 4K frames or 1M tokens while delivering advanced performances on short-context multi-modal tasks. We propose an effective multi-modal training schema that starts with large language models and proceeds through vision-language alignment, general knowledge learning, and two sequential stages of long-sequence fine-tuning. We further implement context-parallelism distributed inference and logits-masked language modeling head to scale Long-VITA to infinitely long inputs of images and texts during model inference. Regarding training data, Long-VITA is built on a mix of $17$M samples from public datasets only and demonstrates the state-of-the-art performance on various multi-modal benchmarks, compared against recent cutting-edge models with internal data. Long-VITA is fully reproducible and supports both NPU and GPU platforms for training and testing. We hope Long-VITA can serve as a competitive baseline and offer valuable insights for the open-source community in advancing long-context multi-modal understanding.</li>
</ul>

<h3>Title: FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Shilong Zhang, Wenbo Li, Shoufa Chen, Chongjian Ge, Peize Sun, Yida Zhang, Yi Jiang, Zehuan Yuan, Binyue Peng, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05179">https://arxiv.org/abs/2502.05179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05179">https://arxiv.org/pdf/2502.05179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05179]] FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation(https://arxiv.org/abs/2502.05179)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>DiT diffusion models have achieved great success in text-to-video generation, leveraging their scalability in model capacity and data scale. High content and motion fidelity aligned with text prompts, however, often require large model parameters and a substantial number of function evaluations (NFEs). Realistic and visually appealing details are typically reflected in high resolution outputs, further amplifying computational demands especially for single stage DiT models. To address these challenges, we propose a novel two stage framework, FlashVideo, which strategically allocates model capacity and NFEs across stages to balance generation fidelity and quality. In the first stage, prompt fidelity is prioritized through a low resolution generation process utilizing large parameters and sufficient NFEs to enhance computational efficiency. The second stage establishes flow matching between low and high resolutions, effectively generating fine details with minimal NFEs. Quantitative and visual results demonstrate that FlashVideo achieves state-of-the-art high resolution video generation with superior computational efficiency. Additionally, the two-stage design enables users to preview the initial output before committing to full resolution generation, thereby significantly reducing computational costs and wait times as well as enhancing commercial viability .</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
