<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: LFAA: Crafting Transferable Targeted Adversarial Examples with Low-Frequency Perturbations. (arXiv:2310.20175v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20175">http://arxiv.org/abs/2310.20175</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20175]] LFAA: Crafting Transferable Targeted Adversarial Examples with Low-Frequency Perturbations(http://arxiv.org/abs/2310.20175)</code></li>
<li>Summary: <p>Deep neural networks are susceptible to adversarial attacks, which pose a
significant threat to their security and reliability in real-world
applications. The most notable adversarial attacks are transfer-based attacks,
where an adversary crafts an adversarial example to fool one model, which can
also fool other models. While previous research has made progress in improving
the transferability of untargeted adversarial examples, the generation of
targeted adversarial examples that can transfer between models remains a
challenging task. In this work, we present a novel approach to generate
transferable targeted adversarial examples by exploiting the vulnerability of
deep neural networks to perturbations on high-frequency components of images.
We observe that replacing the high-frequency component of an image with that of
another image can mislead deep models, motivating us to craft perturbations
containing high-frequency information to achieve targeted attacks. To this end,
we propose a method called Low-Frequency Adversarial Attack (\name), which
trains a conditional generator to generate targeted adversarial perturbations
that are then added to the low-frequency component of the image. Extensive
experiments on ImageNet demonstrate that our proposed approach significantly
outperforms state-of-the-art methods, improving targeted attack success rates
by a margin from 3.2\% to 15.5\%.
</p></li>
</ul>

<h3>Title: Visible to Thermal image Translation for improving visual task in low light conditions. (arXiv:2310.20190v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20190">http://arxiv.org/abs/2310.20190</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20190]] Visible to Thermal image Translation for improving visual task in low light conditions(http://arxiv.org/abs/2310.20190)</code></li>
<li>Summary: <p>Several visual tasks, such as pedestrian detection and image-to-image
translation, are challenging to accomplish in low light using RGB images. Heat
variation of objects in thermal images can be used to overcome this. In this
work, an end-to-end framework, which consists of a generative network and a
detector network, is proposed to translate RGB image into Thermal ones and
compare generated thermal images with real data. We have collected images from
two different locations using the Parrot Anafi Thermal drone. After that, we
created a two-stream network, preprocessed, augmented, the image data, and
trained the generator and discriminator models from scratch. The findings
demonstrate that it is feasible to translate RGB training data to thermal data
using GAN. As a result, thermal data can now be produced more quickly and
affordably, which is useful for security and surveillance applications.
</p></li>
</ul>

<h3>Title: Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications. (arXiv:2310.19942v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19942">http://arxiv.org/abs/2310.19942</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19942]] Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications(http://arxiv.org/abs/2310.19942)</code></li>
<li>Summary: <p>In this work, we address the NER problem by splitting it into two logical
sub-tasks: (1) Span Detection which simply extracts entity mention spans
irrespective of entity type; (2) Span Classification which classifies the spans
into their entity types. Further, we formulate both sub-tasks as
question-answering (QA) problems and produce two leaner models which can be
optimized separately for each sub-task. Experiments with four cross-domain
datasets demonstrate that this two-step approach is both effective and time
efficient. Our system, SplitNER outperforms baselines on OntoNotes5.0, WNUT17
and a cybersecurity dataset and gives on-par performance on BioNLP13CG. In all
cases, it achieves a significant reduction in training time compared to its QA
baseline counterpart. The effectiveness of our system stems from fine-tuning
the BERT model twice, separately for span detection and classification. The
source code can be found at https://github.com/c3sr/split-ner.
</p></li>
</ul>

<h3>Title: Vignat: Vulnerability identification by learning code semantics via graph attention networks. (arXiv:2310.20067v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20067">http://arxiv.org/abs/2310.20067</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20067]] Vignat: Vulnerability identification by learning code semantics via graph attention networks(http://arxiv.org/abs/2310.20067)</code></li>
<li>Summary: <p>Vulnerability identification is crucial to protect software systems from
attacks for cyber-security. However, huge projects have more than millions of
lines of code, and the complex dependencies make it hard to carry out
traditional static and dynamic methods. Furthermore, the semantic structure of
various types of vulnerabilities differs greatly and may occur simultaneously,
making general rule-based methods difficult to extend. In this paper, we
propose \textit{Vignat}, a novel attention-based framework for identifying
vulnerabilities by learning graph-level semantic representations of code. We
represent codes with code property graphs (CPGs) in fine grain and use graph
attention networks (GATs) for vulnerability detection. The results show that
Vignat is able to achieve $57.38\%$ accuracy on reliable datasets derived from
popular C libraries. Furthermore, the interpretability of our GATs provides
valuable insights into vulnerability patterns.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models. (arXiv:2310.20138v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20138">http://arxiv.org/abs/2310.20138</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20138]] DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models(http://arxiv.org/abs/2310.20138)</code></li>
<li>Summary: <p>Large language models pretrained on a huge amount of data capture rich
knowledge and information in the training data. The ability of data
memorization and regurgitation in pretrained language models, revealed in
previous studies, brings the risk of data leakage. In order to effectively
reduce these risks, we propose a framework DEPN to Detect and Edit Privacy
Neurons in pretrained language models, partially inspired by knowledge neurons
and model editing. In DEPN, we introduce a novel method, termed as privacy
neuron detector, to locate neurons associated with private information, and
then edit these detected privacy neurons by setting their activations to zero.
Furthermore, we propose a privacy neuron aggregator dememorize private
information in a batch processing manner. Experimental results show that our
method can significantly and efficiently reduce the exposure of private data
leakage without deteriorating the performance of the model. Additionally, we
empirically demonstrate the relationship between model memorization and privacy
neurons, from multiple perspectives, including model size, training time,
prompts, privacy neuron distribution, illustrating the robustness of our
approach.
</p></li>
</ul>

<h3>Title: Unlearn What You Want to Forget: Efficient Unlearning for LLMs. (arXiv:2310.20150v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20150">http://arxiv.org/abs/2310.20150</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20150]] Unlearn What You Want to Forget: Efficient Unlearning for LLMs(http://arxiv.org/abs/2310.20150)</code></li>
<li>Summary: <p>Large language models (LLMs) have achieved significant progress from
pre-training on and memorizing a wide range of textual data, however, this
process might suffer from privacy issues and violations of data protection
regulations. As a result, the ability to easily remove data related to
individual users from such models while not deteriorating their predictive
quality after the removal becomes increasingly important. To address these
issues, in this work, we propose an efficient unlearning framework that could
efficiently update LLMs without having to retrain the whole model after data
removals, by introducing lightweight unlearning layers learned with a selective
teacher-student objective into the transformers. In addition, we introduce a
fusion mechanism to effectively combine different unlearning layers that learns
to forget different sets of data to handle a sequence of forgetting operations.
Experiments on classification and generation tasks demonstrate the
effectiveness of our proposed methods compared to the state-of-the-art
baselines.
</p></li>
</ul>

<h3>Title: PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning. (arXiv:2310.19958v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19958">http://arxiv.org/abs/2310.19958</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19958]] PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning(http://arxiv.org/abs/2310.19958)</code></li>
<li>Summary: <p>Federated learning (FL) is a paradigm that allows several client devices and
a server to collaboratively train a global model, by exchanging only model
updates, without the devices sharing their local training data. These devices
are often constrained in terms of communication and computation resources, and
can further benefit from model pruning -- a paradigm that is widely used to
reduce the size and complexity of models. Intuitively, by making local models
coarser, pruning is expected to also provide some protection against privacy
attacks in the context of FL. However this protection has not been previously
characterized, formally or experimentally, and it is unclear if it is
sufficient against state-of-the-art attacks.
</p>
<p>In this paper, we perform the first investigation of privacy guarantees for
model pruning in FL. We derive information-theoretic upper bounds on the amount
of information leaked by pruned FL models. We complement and validate these
theoretical findings, with comprehensive experiments that involve
state-of-the-art privacy attacks, on several state-of-the-art FL pruning
schemes, using benchmark datasets. This evaluation provides valuable insights
into the choices and parameters that can affect the privacy protection provided
by pruning. Based on these insights, we introduce PriPrune -- a privacy-aware
algorithm for local model pruning, which uses a personalized per-client defense
mask and adapts the defense pruning rate so as to jointly optimize privacy and
model performance. PriPrune is universal in that can be applied after any
pruned FL scheme on the client, without modification, and protects against any
inversion attack by the server. Our empirical evaluation demonstrates that
PriPrune significantly improves the privacy-accuracy tradeoff compared to
state-of-the-art pruned FL schemes that do not take privacy into account.
</p></li>
</ul>

<h3>Title: Decentralised, Scalable and Privacy-Preserving Synthetic Data Generation. (arXiv:2310.20062v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20062">http://arxiv.org/abs/2310.20062</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20062]] Decentralised, Scalable and Privacy-Preserving Synthetic Data Generation(http://arxiv.org/abs/2310.20062)</code></li>
<li>Summary: <p>Synthetic data is emerging as a promising way to harness the value of data,
while reducing privacy risks. The potential of synthetic data is not limited to
privacy-friendly data release, but also includes complementing real data in
use-cases such as training machine learning algorithms that are more fair and
robust to distribution shifts etc. There is a lot of interest in algorithmic
advances in synthetic data generation for providing better privacy and
statistical guarantees and for its better utilisation in machine learning
pipelines. However, for responsible and trustworthy synthetic data generation,
it is not sufficient to focus only on these algorithmic aspects and instead, a
holistic view of the synthetic data generation pipeline must be considered. We
build a novel system that allows the contributors of real data to autonomously
participate in differentially private synthetic data generation without relying
on a trusted centre. Our modular, general and scalable solution is based on
three building blocks namely: Solid (Social Linked Data), MPC (Secure
Multi-Party Computation) and Trusted Execution Environments (TEEs). Solid is a
specification that lets people store their data securely in decentralised data
stores called Pods and control access to their data. MPC refers to the set of
cryptographic methods for different parties to jointly compute a function over
their inputs while keeping those inputs private. TEEs such as Intel SGX rely on
hardware based features for confidentiality and integrity of code and data. We
show how these three technologies can be effectively used to address various
challenges in responsible and trustworthy synthetic data generation by
ensuring: 1) contributor autonomy, 2) decentralisation, 3) privacy and 4)
scalability. We support our claims with rigorous empirical results on simulated
and real datasets and different synthetic data generation algorithms.
</p></li>
</ul>

<h3>Title: FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems. (arXiv:2310.20193v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20193">http://arxiv.org/abs/2310.20193</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20193]] FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems(http://arxiv.org/abs/2310.20193)</code></li>
<li>Summary: <p>Preserving privacy and reducing communication costs for edge users pose
significant challenges in recommendation systems. Although federated learning
has proven effective in protecting privacy by avoiding data exchange between
clients and servers, it has been shown that the server can infer user ratings
based on updated non-zero gradients obtained from two consecutive rounds of
user-uploaded gradients. Moreover, federated recommendation systems (FRS) face
the challenge of heterogeneity, leading to decreased recommendation
performance. In this paper, we propose FedRec+, an ensemble framework for FRS
that enhances privacy while addressing the heterogeneity challenge. FedRec+
employs optimal subset selection based on feature similarity to generate
near-optimal virtual ratings for pseudo items, utilizing only the user's local
information. This approach reduces noise without incurring additional
communication costs. Furthermore, we utilize the Wasserstein distance to
estimate the heterogeneity and contribution of each client, and derive optimal
aggregation weights by solving a defined optimization problem. Experimental
results demonstrate the state-of-the-art performance of FedRec+ across various
reference datasets.
</p></li>
</ul>

<h3>Title: Verification of Neural Networks Local Differential Classification Privacy. (arXiv:2310.20299v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20299">http://arxiv.org/abs/2310.20299</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20299]] Verification of Neural Networks Local Differential Classification Privacy(http://arxiv.org/abs/2310.20299)</code></li>
<li>Summary: <p>Neural networks are susceptible to privacy attacks. To date, no verifier can
reason about the privacy of individuals participating in the training set. We
propose a new privacy property, called local differential classification
privacy (LDCP), extending local robustness to a differential privacy setting
suitable for black-box classifiers. Given a neighborhood of inputs, a
classifier is LDCP if it classifies all inputs the same regardless of whether
it is trained with the full dataset or whether any single entry is omitted. A
naive algorithm is highly impractical because it involves training a very large
number of networks and verifying local robustness of the given neighborhood
separately for every network. We propose Sphynx, an algorithm that computes an
abstraction of all networks, with a high probability, from a small set of
networks, and verifies LDCP directly on the abstract network. The challenge is
twofold: network parameters do not adhere to a known distribution probability,
making it difficult to predict an abstraction, and predicting too large
abstraction harms the verification. Our key idea is to transform the parameters
into a distribution given by KDE, allowing to keep the over-approximation error
small. To verify LDCP, we extend a MILP verifier to analyze an abstract
network. Experimental results show that by training only 7% of the networks,
Sphynx predicts an abstract network obtaining 93% verification accuracy and
reducing the analysis time by $1.7\cdot10^4$x.
</p></li>
</ul>

<h3>Title: Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations. (arXiv:2310.19978v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19978">http://arxiv.org/abs/2310.19978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19978]] Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations(http://arxiv.org/abs/2310.19978)</code></li>
<li>Summary: <p>To the best of our knowledge, there are no methods today for training
differentially private regression models on sparse input data. To remedy this,
we adapt the Frank-Wolfe algorithm for $L_1$ penalized linear regression to be
aware of sparse inputs and to use them effectively. In doing so, we reduce the
training time of the algorithm from $\mathcal{O}( T D S + T N S)$ to
$\mathcal{O}(N S + T \sqrt{D} \log{D} + T S^2)$, where $T$ is the number of
iterations and a sparsity rate $S$ of a dataset with $N$ rows and $D$ features.
Our results demonstrate that this procedure can reduce runtime by a factor of
up to $2,200\times$, depending on the value of the privacy parameter $\epsilon$
and the sparsity of the dataset.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Student Certificate Sharing System Using Blockchain and NFTs. (arXiv:2310.20036v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20036">http://arxiv.org/abs/2310.20036</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20036]] Student Certificate Sharing System Using Blockchain and NFTs(http://arxiv.org/abs/2310.20036)</code></li>
<li>Summary: <p>In this paper, we propose a certificate sharing system based on blockchain
that gives students authority and control over their academic certificates. Our
strategy involves developing blockchain-based NFT certifications that can be
shared with institutions or employers using blockchain addresses. Students may
access the data created by each individual institute in a single platform,
filter the view of the relevant courses according to their requirements, and
mint their certificate metadata as NFTs. This method provides accountability of
access, comprehensive records that are permanently maintained in IPFS, and
verifiable provenance for creating, distributing, and accessing certificates.
It also makes it possible to share certificates more safely and efficiently. By
incorporating trust factors through data provenance, our system provides a
countermeasure against issues such as fake and duplicate certificates. It
addresses the challenge of the traditional certificate verification processes,
which are lengthy manual process. With this system, students can manage and
validate their academic credentials from multiple institutions in one location
while ensuring authenticity and confidentiality using digital signatures and
hashing for data protection against unauthorized access. Overall, our suggested
system ensures data safety, accountability, and confidentiality while offering
a novel approach to certificate distribution.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Exploring Geometry of Blind Spots in Vision Models. (arXiv:2310.19889v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19889">http://arxiv.org/abs/2310.19889</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19889]] Exploring Geometry of Blind Spots in Vision Models(http://arxiv.org/abs/2310.19889)</code></li>
<li>Summary: <p>Despite the remarkable success of deep neural networks in a myriad of
settings, several works have demonstrated their overwhelming sensitivity to
near-imperceptible perturbations, known as adversarial attacks. On the other
hand, prior works have also observed that deep networks can be under-sensitive,
wherein large-magnitude perturbations in input space do not induce appreciable
changes to network activations. In this work, we study in detail the phenomenon
of under-sensitivity in vision models such as CNNs and Transformers, and
present techniques to study the geometry and extent of "equi-confidence" level
sets of such networks. We propose a Level Set Traversal algorithm that
iteratively explores regions of high confidence with respect to the input space
using orthogonal components of the local gradients. Given a source image, we
use this algorithm to identify inputs that lie in the same equi-confidence
level set as the source image despite being perceptually similar to arbitrary
images from other classes. We further observe that the source image is linearly
connected by a high-confidence path to these inputs, uncovering a star-like
structure for level sets of deep networks. Furthermore, we attempt to identify
and estimate the extent of these connected higher-dimensional regions over
which the model maintains a high degree of confidence. The code for this
project is publicly available at
https://github.com/SriramB-98/blindspots-neurips-sub
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: $p$-Poisson surface reconstruction in curl-free flow from point clouds. (arXiv:2310.20095v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20095">http://arxiv.org/abs/2310.20095</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20095]] $p$-Poisson surface reconstruction in curl-free flow from point clouds(http://arxiv.org/abs/2310.20095)</code></li>
<li>Summary: <p>The aim of this paper is the reconstruction of a smooth surface from an
unorganized point cloud sampled by a closed surface, with the preservation of
geometric shapes, without any further information other than the point cloud.
Implicit neural representations (INRs) have recently emerged as a promising
approach to surface reconstruction. However, the reconstruction quality of
existing methods relies on ground truth implicit function values or surface
normal vectors. In this paper, we show that proper supervision of partial
differential equations and fundamental properties of differential vector fields
are sufficient to robustly reconstruct high-quality surfaces. We cast the
$p$-Poisson equation to learn a signed distance function (SDF) and the
reconstructed surface is implicitly represented by the zero-level set of the
SDF. For efficient training, we develop a variable splitting structure by
introducing a gradient of the SDF as an auxiliary variable and impose the
$p$-Poisson equation directly on the auxiliary variable as a hard constraint.
Based on the curl-free property of the gradient field, we impose a curl-free
constraint on the auxiliary variable, which leads to a more faithful
reconstruction. Experiments on standard benchmark datasets show that the
proposed INR provides a superior and robust reconstruction. The code is
available at \url{https://github.com/Yebbi/PINC}.
</p></li>
</ul>

<h3>Title: Pose-to-Motion: Cross-Domain Motion Retargeting with Pose Prior. (arXiv:2310.20249v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20249">http://arxiv.org/abs/2310.20249</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20249]] Pose-to-Motion: Cross-Domain Motion Retargeting with Pose Prior(http://arxiv.org/abs/2310.20249)</code></li>
<li>Summary: <p>Creating believable motions for various characters has long been a goal in
computer graphics. Current learning-based motion synthesis methods depend on
extensive motion datasets, which are often challenging, if not impossible, to
obtain. On the other hand, pose data is more accessible, since static posed
characters are easier to create and can even be extracted from images using
recent advancements in computer vision. In this paper, we utilize this
alternative data source and introduce a neural motion synthesis approach
through retargeting. Our method generates plausible motions for characters that
have only pose data by transferring motion from an existing motion capture
dataset of another character, which can have drastically different skeletons.
Our experiments show that our method effectively combines the motion features
of the source character with the pose features of the target character, and
performs robustly with small or noisy pose data sets, ranging from a few
artist-created poses to noisy poses estimated directly from images.
Additionally, a conducted user study indicated that a majority of participants
found our retargeted motion to be more enjoyable to watch, more lifelike in
appearance, and exhibiting fewer artifacts. Project page:
https://cyanzhao42.github.io/pose2motion
</p></li>
</ul>

<h3>Title: A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging. (arXiv:2310.20381v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20381">http://arxiv.org/abs/2310.20381</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20381]] A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging(http://arxiv.org/abs/2310.20381)</code></li>
<li>Summary: <p>This paper presents a comprehensive evaluation of GPT-4V's capabilities
across diverse medical imaging tasks, including Radiology Report Generation,
Medical Visual Question Answering (VQA), and Visual Grounding. While prior
efforts have explored GPT-4V's performance in medical imaging, to the best of
our knowledge, our study represents the first quantitative evaluation on
publicly available benchmarks. Our findings highlight GPT-4V's potential in
generating descriptive reports for chest X-ray images, particularly when guided
by well-structured prompts. However, its performance on the MIMIC-CXR dataset
benchmark reveals areas for improvement in certain evaluation metrics, such as
CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in
distinguishing between question types but falls short of prevailing benchmarks
in terms of accuracy. Furthermore, our analysis finds the limitations of
conventional evaluation metrics like the BLEU score, advocating for the
development of more semantically robust assessment methods. In the field of
Visual Grounding, GPT-4V exhibits preliminary promise in recognizing bounding
boxes, but its precision is lacking, especially in identifying specific medical
organs and signs. Our evaluation underscores the significant potential of
GPT-4V in the medical imaging domain, while also emphasizing the need for
targeted refinements to fully unlock its capabilities.
</p></li>
</ul>

<h3>Title: Generating Continuations in Multilingual Idiomatic Contexts. (arXiv:2310.20195v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20195">http://arxiv.org/abs/2310.20195</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20195]] Generating Continuations in Multilingual Idiomatic Contexts(http://arxiv.org/abs/2310.20195)</code></li>
<li>Summary: <p>The ability to process idiomatic or literal multiword expressions is a
crucial aspect of understanding and generating any language. The task of
generating contextually relevant continuations for narratives containing
idiomatic (or literal) expressions can allow us to test the ability of
generative language models (LMs) in understanding nuanced language containing
non-compositional figurative text. We conduct a series of experiments using
datasets in two distinct languages (English and Portuguese) under three
different training settings (zero-shot, few-shot, and fine-tuned). Our results
suggest that the models are only slightly better at generating continuations
for literal contexts than idiomatic contexts, with exceedingly small margins.
Furthermore, the models studied in this work perform equally well across both
languages, indicating the robustness of generative models in performing this
task.
</p></li>
</ul>

<h3>Title: PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection. (arXiv:2310.20256v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20256">http://arxiv.org/abs/2310.20256</a></li>
<li>Code URL: https://github.com/taoyang225/psycot</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20256]] PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection(http://arxiv.org/abs/2310.20256)</code></li>
<li>Summary: <p>Recent advances in large language models (LLMs), such as ChatGPT, have
showcased remarkable zero-shot performance across various NLP tasks. However,
the potential of LLMs in personality detection, which involves identifying an
individual's personality from their written texts, remains largely unexplored.
Drawing inspiration from Psychological Questionnaires, which are carefully
designed by psychologists to evaluate individual personality traits through a
series of targeted items, we argue that these items can be regarded as a
collection of well-structured chain-of-thought (CoT) processes. By
incorporating these processes, LLMs can enhance their capabilities to make more
reasonable inferences on personality from textual input. In light of this, we
propose a novel personality detection method, called PsyCoT, which mimics the
way individuals complete psychological questionnaires in a multi-turn dialogue
manner. In particular, we employ a LLM as an AI assistant with a specialization
in text analysis. We prompt the assistant to rate individual items at each turn
and leverage the historical rating results to derive a conclusive personality
preference. Our experiments demonstrate that PsyCoT significantly improves the
performance and robustness of GPT-3.5 in personality detection, achieving an
average F1 score improvement of 4.23/10.63 points on two benchmark datasets
compared to the standard prompting method. Our code is available at
https://github.com/TaoYang225/PsyCoT.
</p></li>
</ul>

<h3>Title: SyMPox: An Automated Monkeypox Detection System Based on Symptoms Using XGBoost. (arXiv:2310.19801v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19801">http://arxiv.org/abs/2310.19801</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19801]] SyMPox: An Automated Monkeypox Detection System Based on Symptoms Using XGBoost(http://arxiv.org/abs/2310.19801)</code></li>
<li>Summary: <p>Monkeypox is a zoonotic disease. About 87000 cases of monkeypox were
confirmed by the World Health Organization until 10th June 2023. The most
prevalent methods for identifying this disease are image-based recognition
techniques. Still, they are not too fast and could only be available to a few
individuals. This study presents an independent application named SyMPox,
developed to diagnose Monkeypox cases based on symptoms. SyMPox utilizes the
robust XGBoost algorithm to analyze symptom patterns and provide accurate
assessments. Developed using the Gradio framework, SyMPox offers a
user-friendly platform for individuals to assess their symptoms and obtain
reliable Monkeypox diagnoses.
</p></li>
</ul>

<h3>Title: Machine Learning and Knowledge: Why Robustness Matters. (arXiv:2310.19819v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19819">http://arxiv.org/abs/2310.19819</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19819]] Machine Learning and Knowledge: Why Robustness Matters(http://arxiv.org/abs/2310.19819)</code></li>
<li>Summary: <p>Trusting machine learning algorithms requires having confidence in their
outputs. Confidence is typically interpreted in terms of model reliability,
where a model is reliable if it produces a high proportion of correct outputs.
However, model reliability does not address concerns about the robustness of
machine learning models, such as models relying on the wrong features or
variations in performance based on context. I argue that the epistemic
dimension of trust can instead be understood through the concept of knowledge,
where the trustworthiness of an algorithm depends on whether its users are in
the position to know that its outputs are correct. Knowledge requires beliefs
to be formed for the right reasons and to be robust to error, so machine
learning algorithms can only provide knowledge if they work well across
counterfactual scenarios and if they make decisions based on the right
features. This, I argue, can explain why we should care about model properties
like interpretability, causal shortcut independence, and distribution shift
robustness even if such properties are not required for model reliability.
</p></li>
</ul>

<h3>Title: Robust Learning for Smoothed Online Convex Optimization with Feedback Delay. (arXiv:2310.20098v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20098">http://arxiv.org/abs/2310.20098</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20098]] Robust Learning for Smoothed Online Convex Optimization with Feedback Delay(http://arxiv.org/abs/2310.20098)</code></li>
<li>Summary: <p>We study a challenging form of Smoothed Online Convex Optimization, a.k.a.
SOCO, including multi-step nonlinear switching costs and feedback delay. We
propose a novel machine learning (ML) augmented online algorithm,
Robustness-Constrained Learning (RCL), which combines untrusted ML predictions
with a trusted expert online algorithm via constrained projection to robustify
the ML prediction. Specifically,we prove that RCL is able to
guarantee$(1+\lambda)$-competitiveness against any given expert for
any$\lambda&gt;0$, while also explicitly training the ML model in a
robustification-aware manner to improve the average-case performance.
Importantly,RCL is the first ML-augmented algorithm with a provable robustness
guarantee in the case of multi-step switching cost and feedback delay.We
demonstrate the improvement of RCL in both robustness and average performance
using battery management for electrifying transportationas a case study.
</p></li>
</ul>

<h3>Title: Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs. (arXiv:2310.20145v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20145">http://arxiv.org/abs/2310.20145</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20145]] Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs(http://arxiv.org/abs/2310.20145)</code></li>
<li>Summary: <p>Bayesian Optimization (BO) is a sample-efficient optimization algorithm
widely employed across various applications. In some challenging BO tasks,
input uncertainty arises due to the inevitable randomness in the optimization
process, such as machining errors, execution noise, or contextual variability.
This uncertainty deviates the input from the intended value before evaluation,
resulting in significant performance fluctuations in the final result. In this
paper, we introduce a novel robust Bayesian Optimization algorithm, AIRBO,
which can effectively identify a robust optimum that performs consistently well
under arbitrary input uncertainty. Our method directly models the uncertain
inputs of arbitrary distributions by empowering the Gaussian Process with the
Maximum Mean Discrepancy (MMD) and further accelerates the posterior inference
via Nystrom approximation. Rigorous theoretical regret bound is established
under MMD estimation error and extensive experiments on synthetic functions and
real problems demonstrate that our approach can handle various input
uncertainties and achieve state-of-the-art performance.
</p></li>
</ul>

<h3>Title: CAFE: Conflict-Aware Feature-wise Explanations. (arXiv:2310.20363v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20363">http://arxiv.org/abs/2310.20363</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20363]] CAFE: Conflict-Aware Feature-wise Explanations(http://arxiv.org/abs/2310.20363)</code></li>
<li>Summary: <p>Feature attribution methods are widely used to explain neural models by
determining the influence of individual input features on the models' outputs.
We propose a novel feature attribution method, CAFE (Conflict-Aware
Feature-wise Explanations), that addresses three limitations of the existing
methods: their disregard for the impact of conflicting features, their lack of
consideration for the influence of bias terms, and an overly high sensitivity
to local variations in the underpinning activation functions. Unlike other
methods, CAFE provides safeguards against overestimating the effects of neuron
inputs and separately traces positive and negative influences of input features
and biases, resulting in enhanced robustness and increased ability to surface
feature conflicts. We show experimentally that CAFE is better able to identify
conflicting features on synthetic tabular data and exhibits the best overall
fidelity on several real-world tabular datasets, while being highly
computationally efficient.
</p></li>
</ul>

<h3>Title: Distil the informative essence of loop detector data set: Is network-level traffic forecasting hungry for more data?. (arXiv:2310.20366v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20366">http://arxiv.org/abs/2310.20366</a></li>
<li>Code URL: https://github.com/romainlitud/uncertainty-aware-traffic-speed-flow-demand-prediction</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20366]] Distil the informative essence of loop detector data set: Is network-level traffic forecasting hungry for more data?(http://arxiv.org/abs/2310.20366)</code></li>
<li>Summary: <p>Network-level traffic condition forecasting has been intensively studied for
decades. Although prediction accuracy has been continuously improved with
emerging deep learning models and ever-expanding traffic data, traffic
forecasting still faces many challenges in practice. These challenges include
the robustness of data-driven models, the inherent unpredictability of traffic
dynamics, and whether further improvement of traffic forecasting requires more
sensor data. In this paper, we focus on this latter question and particularly
on data from loop detectors. To answer this, we propose an uncertainty-aware
traffic forecasting framework to explore how many samples of loop data are
truly effective for training forecasting models. Firstly, the model design
combines traffic flow theory with graph neural networks, ensuring the
robustness of prediction and uncertainty quantification. Secondly, evidential
learning is employed to quantify different sources of uncertainty in a single
pass. The estimated uncertainty is used to "distil" the essence of the dataset
that sufficiently covers the information content. Results from a case study of
a highway network around Amsterdam show that, from 2018 to 2021, more than 80\%
of the data during daytime can be removed. The remaining 20\% samples have
equal prediction power for training models. This result suggests that indeed
large traffic datasets can be subdivided into significantly smaller but equally
informative datasets. From these findings, we conclude that the proposed
methodology proves valuable in evaluating large traffic datasets' true
information content. Further extensions, such as extracting smaller, spatially
non-redundant datasets, are possible with this method.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Contrast-agent-induced deterministic component of CT-density in the abdominal aorta during routine angiography: proof of concept study. (arXiv:2310.20243v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20243">http://arxiv.org/abs/2310.20243</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20243]] Contrast-agent-induced deterministic component of CT-density in the abdominal aorta during routine angiography: proof of concept study(http://arxiv.org/abs/2310.20243)</code></li>
<li>Summary: <p>Background and objective: CTA is a gold standard of preoperative diagnosis of
abdominal aorta and typically used for geometric-only characteristic
extraction. We assume that a model describing the dynamic behavior of the
contrast agent in the vessel can be developed from the data of routine CTA
studies, allowing the procedure to be investigated and optimized without the
need for additional perfusion CT studies. Obtained spatial distribution of CA
can be valuable for both increasing the diagnostic value of a particular study
and improving the CT data processing tools. Methods: In accordance with the
Beer-Lambert law and the absence of chemical interaction between blood and CA,
we postulated the existence of a deterministic CA-induced component in the CT
signal density. The proposed model, having a double-sigmoid structure, contains
six coefficients relevant to the properties of hemodynamics. To validate the
model, expert segmentation was performed using the 3D Slicer application for
the CTA data obtained from publicly available source. The model was fitted to
the data using the non-linear least square method with Levenberg-Marquardt
optimization. Results: We analyzed 594 CTA images (4 studies with median size
of 144 slices, IQR [134; 158.5]; 1:1 normal:pathology balance). Goodness-of-fit
was proved by Wilcox test (p-value &gt; 0.05 for all cases). The proposed model
correctly simulated normal blood flow and hemodynamics disturbances caused by
local abnormalities (aneurysm, thrombus and arterial branching). Conclusions:
Proposed approach can be useful for personalized CA modeling of vessels,
improvement of CTA image processing and preparation of synthetic CT training
data for artificial intelligence.
</p></li>
</ul>

<h3>Title: Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning. (arXiv:2310.20268v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20268">http://arxiv.org/abs/2310.20268</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20268]] Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning(http://arxiv.org/abs/2310.20268)</code></li>
<li>Summary: <p>Few-shot class-incremental learning (FSCIL) aims to build machine learning
model that can continually learn new concepts from a few data samples, without
forgetting knowledge of old classes.
</p>
<p>The challenges of FSCIL lies in the limited data of new classes, which not
only lead to significant overfitting issues but also exacerbates the notorious
catastrophic forgetting problems. As proved in early studies, building sample
relationships is beneficial for learning from few-shot samples. In this paper,
we promote the idea to the incremental scenario, and propose a Sample-to-Class
(S2C) graph learning method for FSCIL.
</p>
<p>Specifically, we propose a Sample-level Graph Network (SGN) that focuses on
analyzing sample relationships within a single session. This network helps
aggregate similar samples, ultimately leading to the extraction of more refined
class-level features.
</p>
<p>Then, we present a Class-level Graph Network (CGN) that establishes
connections across class-level features of both new and old classes. This
network plays a crucial role in linking the knowledge between different
sessions and helps improve overall learning in the FSCIL scenario. Moreover, we
design a multi-stage strategy for training S2C model, which mitigates the
training challenges posed by limited data in the incremental process.
</p>
<p>The multi-stage training strategy is designed to build S2C graph from base to
few-shot stages, and improve the capacity via an extra pseudo-incremental
stage. Experiments on three popular benchmark datasets show that our method
clearly outperforms the baselines and sets new state-of-the-art results in
FSCIL.
</p></li>
</ul>

<h3>Title: Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning. (arXiv:2310.20089v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20089">http://arxiv.org/abs/2310.20089</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20089]] Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning(http://arxiv.org/abs/2310.20089)</code></li>
<li>Summary: <p>Clinical note classification is a common clinical NLP task. However,
annotated data-sets are scarse. Prompt-based learning has recently emerged as
an effective method to adapt pre-trained models for text classification using
only few training examples. A critical component of prompt design is the
definition of the template (i.e. prompt text). The effect of template position,
however, has been insufficiently investigated. This seems particularly
important in the clinical setting, where task-relevant information is usually
sparse in clinical notes. In this study we develop a keyword-optimized template
insertion method (KOTI) and show how optimizing position can improve
performance on several clinical tasks in a zero-shot and few-shot training
setting.
</p></li>
</ul>

<h3>Title: FA Team at the NTCIR-17 UFO Task. (arXiv:2310.20322v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20322">http://arxiv.org/abs/2310.20322</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20322]] FA Team at the NTCIR-17 UFO Task(http://arxiv.org/abs/2310.20322)</code></li>
<li>Summary: <p>The FA team participated in the Table Data Extraction (TDE) and Text-to-Table
Relationship Extraction (TTRE) tasks of the NTCIR-17 Understanding of
Non-Financial Objects in Financial Reports (UFO). This paper reports our
approach to solving the problems and discusses the official results. We
successfully utilized various enhancement techniques based on the ELECTRA
language model to extract valuable data from tables. Our efforts resulted in an
impressive TDE accuracy rate of 93.43 %, positioning us in second place on the
Leaderboard rankings. This outstanding achievement is a testament to our
proposed approach's effectiveness. In the TTRE task, we proposed the rule-based
method to extract meaningful relationships between the text and tables task and
confirmed the performance.
</p></li>
</ul>

<h3>Title: Learning Gradient Fields for Scalable and Generalizable Irregular Packing. (arXiv:2310.19814v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19814">http://arxiv.org/abs/2310.19814</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19814]] Learning Gradient Fields for Scalable and Generalizable Irregular Packing(http://arxiv.org/abs/2310.19814)</code></li>
<li>Summary: <p>The packing problem, also known as cutting or nesting, has diverse
applications in logistics, manufacturing, layout design, and atlas generation.
It involves arranging irregularly shaped pieces to minimize waste while
avoiding overlap. Recent advances in machine learning, particularly
reinforcement learning, have shown promise in addressing the packing problem.
In this work, we delve deeper into a novel machine learning-based approach that
formulates the packing problem as conditional generative modeling. To tackle
the challenges of irregular packing, including object validity constraints and
collision avoidance, our method employs the score-based diffusion model to
learn a series of gradient fields. These gradient fields encode the
correlations between constraint satisfaction and the spatial relationships of
polygons, learned from teacher examples. During the testing phase, packing
solutions are generated using a coarse-to-fine refinement mechanism guided by
the learned gradient fields. To enhance packing feasibility and optimality, we
introduce two key architectural designs: multi-scale feature extraction and
coarse-to-fine relation extraction. We conduct experiments on two typical
industrial packing domains, considering translations only. Empirically, our
approach demonstrates spatial utilization rates comparable to, or even
surpassing, those achieved by the teacher algorithm responsible for training
data generation. Additionally, it exhibits some level of generalization to
shape variations. We are hopeful that this method could pave the way for new
possibilities in solving the packing problem.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Improved Communication Efficiency in Federated Natural Policy Gradient via ADMM-based Gradient Updates. (arXiv:2310.19807v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19807">http://arxiv.org/abs/2310.19807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19807]] Improved Communication Efficiency in Federated Natural Policy Gradient via ADMM-based Gradient Updates(http://arxiv.org/abs/2310.19807)</code></li>
<li>Summary: <p>Federated reinforcement learning (FedRL) enables agents to collaboratively
train a global policy without sharing their individual data. However, high
communication overhead remains a critical bottleneck, particularly for natural
policy gradient (NPG) methods, which are second-order. To address this issue,
we propose the FedNPG-ADMM framework, which leverages the alternating direction
method of multipliers (ADMM) to approximate global NPG directions efficiently.
We theoretically demonstrate that using ADMM-based gradient updates reduces
communication complexity from ${O}({d^{2}})$ to ${O}({d})$ at each iteration,
where $d$ is the number of model parameters. Furthermore, we show that
achieving an $\epsilon$-error stationary convergence requires
${O}(\frac{1}{(1-\gamma)^{2}{\epsilon}})$ iterations for discount factor
$\gamma$, demonstrating that FedNPG-ADMM maintains the same convergence rate as
the standard FedNPG. Through evaluation of the proposed algorithms in MuJoCo
environments, we demonstrate that FedNPG-ADMM maintains the reward performance
of standard FedNPG, and that its convergence rate improves when the number of
federated agents increases.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Ling-CL: Understanding NLP Models through Linguistic Curricula. (arXiv:2310.20121v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20121">http://arxiv.org/abs/2310.20121</a></li>
<li>Code URL: https://github.com/clu-uml/ling-cl</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20121]] Ling-CL: Understanding NLP Models through Linguistic Curricula(http://arxiv.org/abs/2310.20121)</code></li>
<li>Summary: <p>We employ a characterization of linguistic complexity from psycholinguistic
and language acquisition research to develop data-driven curricula to
understand the underlying linguistic knowledge that models learn to address NLP
tasks. The novelty of our approach is in the development of linguistic
curricula derived from data, existing knowledge about linguistic complexity,
and model behavior during training. By analyzing several benchmark NLP
datasets, our curriculum learning approaches identify sets of linguistic
metrics (indices) that inform the challenges and reasoning required to address
each task. Our work will inform future research in all NLP areas, allowing
linguistic complexity to be considered early in the research and development
process. In addition, our work prompts an examination of gold standards and
fair evaluation in NLP.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Interpretable Prototype-based Graph Information Bottleneck. (arXiv:2310.19906v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19906">http://arxiv.org/abs/2310.19906</a></li>
<li>Code URL: https://github.com/sang-woo-seo/pgib</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19906]] Interpretable Prototype-based Graph Information Bottleneck(http://arxiv.org/abs/2310.19906)</code></li>
<li>Summary: <p>The success of Graph Neural Networks (GNNs) has led to a need for
understanding their decision-making process and providing explanations for
their predictions, which has given rise to explainable AI (XAI) that offers
transparent explanations for black-box models. Recently, the use of prototypes
has successfully improved the explainability of models by learning prototypes
to imply training graphs that affect the prediction. However, these approaches
tend to provide prototypes with excessive information from the entire graph,
leading to the exclusion of key substructures or the inclusion of irrelevant
substructures, which can limit both the interpretability and the performance of
the model in downstream tasks. In this work, we propose a novel framework of
explainable GNNs, called interpretable Prototype-based Graph Information
Bottleneck (PGIB) that incorporates prototype learning within the information
bottleneck framework to provide prototypes with the key subgraph from the input
graph that is important for the model prediction. This is the first work that
incorporates prototype learning into the process of identifying the key
subgraphs that have a critical impact on the prediction performance. Extensive
experiments, including qualitative analysis, demonstrate that PGIB outperforms
state-of-the-art methods in terms of both prediction performance and
explainability.
</p></li>
</ul>

<h3>Title: A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs. (arXiv:2310.20367v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20367">http://arxiv.org/abs/2310.20367</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20367]] A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs(http://arxiv.org/abs/2310.20367)</code></li>
<li>Summary: <p>Load shapes derived from smart meter data are frequently employed to analyze
daily energy consumption patterns, particularly in the context of applications
like Demand Response (DR). Nevertheless, one of the most important challenges
to this endeavor lies in identifying the most suitable consumer clusters with
similar consumption behaviors. In this paper, we present a novel machine
learning based framework in order to achieve optimal load profiling through a
real case study, utilizing data from almost 5000 households in London. Four
widely used clustering algorithms are applied specifically K-means, K-medoids,
Hierarchical Agglomerative Clustering and Density-based Spatial Clustering. An
empirical analysis as well as multiple evaluation metrics are leveraged to
assess those algorithms. Following that, we redefine the problem as a
probabilistic classification one, with the classifier emulating the behavior of
a clustering algorithm,leveraging Explainable AI (xAI) to enhance the
interpretability of our solution. According to the clustering algorithm
analysis the optimal number of clusters for this case is seven. Despite that,
our methodology shows that two of the clusters, almost 10\% of the dataset,
exhibit significant internal dissimilarity and thus it splits them even further
to create nine clusters in total. The scalability and versatility of our
solution makes it an ideal choice for power utility companies aiming to segment
their users for creating more targeted Demand Response programs.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: A Low-cost Strategic Monitoring Approach for Scalable and Interpretable Error Detection in Deep Neural Networks. (arXiv:2310.20349v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20349">http://arxiv.org/abs/2310.20349</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20349]] A Low-cost Strategic Monitoring Approach for Scalable and Interpretable Error Detection in Deep Neural Networks(http://arxiv.org/abs/2310.20349)</code></li>
<li>Summary: <p>We present a highly compact run-time monitoring approach for deep computer
vision networks that extracts selected knowledge from only a few (down to
merely two) hidden layers, yet can efficiently detect silent data corruption
originating from both hardware memory and input faults. Building on the insight
that critical faults typically manifest as peak or bulk shifts in the
activation distribution of the affected network layers, we use strategically
placed quantile markers to make accurate estimates about the anomaly of the
current inference as a whole. Importantly, the detector component itself is
kept algorithmically transparent to render the categorization of regular and
abnormal behavior interpretable to a human. Our technique achieves up to ~96%
precision and ~98% recall of detection. Compared to state-of-the-art anomaly
detection techniques, this approach requires minimal compute overhead (as
little as 0.3% with respect to non-supervised inference time) and contributes
to the explainability of the model.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks. (arXiv:2310.19909v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19909">http://arxiv.org/abs/2310.19909</a></li>
<li>Code URL: https://github.com/hsouri/battle-of-the-backbones</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19909]] Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks(http://arxiv.org/abs/2310.19909)</code></li>
<li>Summary: <p>Neural network based computer vision systems are typically built on a
backbone, a pretrained or randomly initialized feature extractor. Several years
ago, the default option was an ImageNet-trained convolutional neural network.
However, the recent past has seen the emergence of countless backbones
pretrained using various algorithms and datasets. While this abundance of
choice has led to performance increases for a range of systems, it is difficult
for practitioners to make informed decisions about which backbone to choose.
Battle of the Backbones (BoB) makes this choice easier by benchmarking a
diverse suite of pretrained models, including vision-language models, those
trained via self-supervised learning, and the Stable Diffusion backbone, across
a diverse set of computer vision tasks ranging from classification to object
detection to OOD generalization and more. Furthermore, BoB sheds light on
promising directions for the research community to advance computer vision by
illuminating strengths and weakness of existing approaches through a
comprehensive analysis conducted on more than 1500 training runs. While vision
transformers (ViTs) and self-supervised learning (SSL) are increasingly
popular, we find that convolutional neural networks pretrained in a supervised
fashion on large training sets still perform best on most tasks among the
models we consider. Moreover, in apples-to-apples comparisons on the same
architectures and similarly sized pretraining datasets, we find that SSL
backbones are highly competitive, indicating that future works should perform
SSL pretraining with advanced architectures and larger pretraining datasets. We
release the raw results of our experiments along with code that allows
researchers to put their own backbones through the gauntlet here:
https://github.com/hsouri/Battle-of-the-Backbones
</p></li>
</ul>

<h3>Title: 'Person' == Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion. (arXiv:2310.19981v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19981">http://arxiv.org/abs/2310.19981</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19981]] 'Person' == Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion(http://arxiv.org/abs/2310.19981)</code></li>
<li>Summary: <p>We study stereotypes embedded within one of the most popular text-to-image
generators: Stable Diffusion. We examine what stereotypes of gender and
nationality/continental identity does Stable Diffusion display in the absence
of such information i.e. what gender and nationality/continental identity is
assigned to `a person', or to `a person from Asia'. Using vision-language model
CLIP's cosine similarity to compare images generated by CLIP-based Stable
Diffusion v2.1 verified by manual examination, we chronicle results from 136
prompts (50 results/prompt) of front-facing images of persons from 6 different
continents, 27 nationalities and 3 genders. We observe how Stable Diffusion
outputs of `a person' without any additional gender/nationality information
correspond closest to images of men and least with persons of nonbinary gender,
and to persons from Europe/North America over Africa/Asia, pointing towards
Stable Diffusion having a concerning representation of personhood to be a
European/North American man. We also show continental stereotypes and resultant
harms e.g. a person from Oceania is deemed to be Australian/New Zealander over
Papua New Guinean, pointing to the erasure of Indigenous Oceanic peoples, who
form a majority over descendants of colonizers both in Papua New Guinea and in
Oceania overall. Finally, we unexpectedly observe a pattern of
oversexualization of women, specifically Latin American, Mexican, Indian and
Egyptian women relative to other nationalities, measured through an NSFW
detector. This demonstrates how Stable Diffusion perpetuates Western
fetishization of women of color through objectification in media, which if left
unchecked will amplify this stereotypical representation. Image datasets are
made publicly available.
</p></li>
</ul>

<h3>Title: Beyond U: Making Diffusion Models Faster & Lighter. (arXiv:2310.20092v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20092">http://arxiv.org/abs/2310.20092</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20092]] Beyond U: Making Diffusion Models Faster & Lighter(http://arxiv.org/abs/2310.20092)</code></li>
<li>Summary: <p>Diffusion models are a family of generative models that yield record-breaking
performance in tasks such as image synthesis, video generation, and molecule
design. Despite their capabilities, their efficiency, especially in the reverse
denoising process, remains a challenge due to slow convergence rates and high
computational costs. In this work, we introduce an approach that leverages
continuous dynamical systems to design a novel denoising network for diffusion
models that is more parameter-efficient, exhibits faster convergence, and
demonstrates increased noise robustness. Experimenting with denoising
probabilistic diffusion models, our framework operates with approximately a
quarter of the parameters and 30% of the Floating Point Operations (FLOPs)
compared to standard U-Nets in Denoising Diffusion Probabilistic Models
(DDPMs). Furthermore, our model is up to 70% faster in inference than the
baseline models when measured in equal conditions while converging to better
quality solutions.
</p></li>
</ul>

<h3>Title: SemanticBoost: Elevating Motion Generation with Augmented Textual Cues. (arXiv:2310.20323v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20323">http://arxiv.org/abs/2310.20323</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20323]] SemanticBoost: Elevating Motion Generation with Augmented Textual Cues(http://arxiv.org/abs/2310.20323)</code></li>
<li>Summary: <p>Current techniques face difficulties in generating motions from intricate
semantic descriptions, primarily due to insufficient semantic annotations in
datasets and weak contextual understanding. To address these issues, we present
SemanticBoost, a novel framework that tackles both challenges simultaneously.
Our framework comprises a Semantic Enhancement module and a Context-Attuned
Motion Denoiser (CAMD). The Semantic Enhancement module extracts supplementary
semantics from motion data, enriching the dataset's textual description and
ensuring precise alignment between text and motion data without depending on
large language models. On the other hand, the CAMD approach provides an
all-encompassing solution for generating high-quality, semantically consistent
motion sequences by effectively capturing context information and aligning the
generated motion with the given textual descriptions. Distinct from existing
methods, our approach can synthesize accurate orientational movements, combined
motions based on specific body part descriptions, and motions generated from
complex, extended sentences. Our experimental results demonstrate that
SemanticBoost, as a diffusion-based method, outperforms auto-regressive-based
techniques, achieving cutting-edge performance on the Humanml3D dataset while
maintaining realistic and smooth motion generation quality.
</p></li>
</ul>

<h3>Title: FuXi-Extreme: Improving extreme rainfall and wind forecasts with diffusion model. (arXiv:2310.19822v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19822">http://arxiv.org/abs/2310.19822</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19822]] FuXi-Extreme: Improving extreme rainfall and wind forecasts with diffusion model(http://arxiv.org/abs/2310.19822)</code></li>
<li>Summary: <p>Significant advancements in the development of machine learning (ML) models
for weather forecasting have produced remarkable results. State-of-the-art
ML-based weather forecast models, such as FuXi, have demonstrated superior
statistical forecast performance in comparison to the high-resolution forecasts
(HRES) of the European Centre for Medium-Range Weather Forecasts (ECMWF).
However, ML models face a common challenge: as forecast lead times increase,
they tend to generate increasingly smooth predictions, leading to an
underestimation of the intensity of extreme weather events. To address this
challenge, we developed the FuXi-Extreme model, which employs a denoising
diffusion probabilistic model (DDPM) to restore finer-scale details in the
surface forecast data generated by the FuXi model in 5-day forecasts. An
evaluation of extreme total precipitation ($\textrm{TP}$), 10-meter wind speed
($\textrm{WS10}$), and 2-meter temperature ($\textrm{T2M}$) illustrates the
superior performance of FuXi-Extreme over both FuXi and HRES. Moreover, when
evaluating tropical cyclone (TC) forecasts based on International Best Track
Archive for Climate Stewardship (IBTrACS) dataset, both FuXi and FuXi-Extreme
shows superior performance in TC track forecasts compared to HRES, but they
show inferior performance in TC intensity forecasts in comparison to HRES.
</p></li>
</ul>

<h3>Title: Scaling Riemannian Diffusion Models. (arXiv:2310.20030v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20030">http://arxiv.org/abs/2310.20030</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20030]] Scaling Riemannian Diffusion Models(http://arxiv.org/abs/2310.20030)</code></li>
<li>Summary: <p>Riemannian diffusion models draw inspiration from standard Euclidean space
diffusion models to learn distributions on general manifolds. Unfortunately,
the additional geometric complexity renders the diffusion transition term
inexpressible in closed form, so prior methods resort to imprecise
approximations of the score matching training objective that degrade
performance and preclude applications in high dimensions. In this work, we
reexamine these approximations and propose several practical improvements. Our
key observation is that most relevant manifolds are symmetric spaces, which are
much more amenable to computation. By leveraging and combining various
ans\"{a}tze, we can quickly compute relevant quantities to high precision. On
low dimensional datasets, our correction produces a noticeable improvement,
allowing diffusion to compete with other methods. Additionally, we show that
our method enables us to scale to high dimensional tasks on nontrivial
manifolds. In particular, we model QCD densities on $SU(n)$ lattices and
contrastively learned embeddings on high dimensional hyperspheres.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: MIST: Medical Image Segmentation Transformer with Convolutional Attention Mixing (CAM) Decoder. (arXiv:2310.19898v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19898">http://arxiv.org/abs/2310.19898</a></li>
<li>Code URL: https://github.com/rahman-motiur/mist</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19898]] MIST: Medical Image Segmentation Transformer with Convolutional Attention Mixing (CAM) Decoder(http://arxiv.org/abs/2310.19898)</code></li>
<li>Summary: <p>One of the common and promising deep learning approaches used for medical
image segmentation is transformers, as they can capture long-range dependencies
among the pixels by utilizing self-attention. Despite being successful in
medical image segmentation, transformers face limitations in capturing local
contexts of pixels in multimodal dimensions. We propose a Medical Image
Segmentation Transformer (MIST) incorporating a novel Convolutional Attention
Mixing (CAM) decoder to address this issue. MIST has two parts: a pre-trained
multi-axis vision transformer (MaxViT) is used as an encoder, and the encoded
feature representation is passed through the CAM decoder for segmenting the
images. In the CAM decoder, an attention-mixer combining multi-head
self-attention, spatial attention, and squeeze and excitation attention modules
is introduced to capture long-range dependencies in all spatial dimensions.
Moreover, to enhance spatial information gain, deep and shallow convolutions
are used for feature extraction and receptive field expansion, respectively.
The integration of low-level and high-level features from different network
stages is enabled by skip connections, allowing MIST to suppress unnecessary
information. The experiments show that our MIST transformer with CAM decoder
outperforms the state-of-the-art models specifically designed for medical image
segmentation on the ACDC and Synapse datasets. Our results also demonstrate
that adding the CAM decoder with a hierarchical transformer improves
segmentation performance significantly. Our model with data and code is
publicly available on GitHub.
</p></li>
</ul>

<h3>Title: Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?. (arXiv:2310.19936v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19936">http://arxiv.org/abs/2310.19936</a></li>
<li>Code URL: https://github.com/cea-list/mt-detr</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19936]] Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?(http://arxiv.org/abs/2310.19936)</code></li>
<li>Summary: <p>For specialized and dense downstream tasks such as object detection, labeling
data requires expertise and can be very expensive, making few-shot and
semi-supervised models much more attractive alternatives. While in the few-shot
setup we observe that transformer-based object detectors perform better than
convolution-based two-stage models for a similar amount of parameters, they are
not as effective when used with recent approaches in the semi-supervised
setting. In this paper, we propose a semi-supervised method tailored for the
current state-of-the-art object detector Deformable DETR in the few-annotation
learning setup using a student-teacher architecture, which avoids relying on a
sensitive post-processing of the pseudo-labels generated by the teacher model.
We evaluate our method on the semi-supervised object detection benchmarks COCO
and Pascal VOC, and it outperforms previous methods, especially when
annotations are scarce. We believe that our contributions open new
possibilities to adapt similar object detection methods in this setup as well.
</p></li>
</ul>

<h3>Title: SolarFormer: Multi-scale Transformer for Solar PV Profiling. (arXiv:2310.20057v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20057">http://arxiv.org/abs/2310.20057</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20057]] SolarFormer: Multi-scale Transformer for Solar PV Profiling(http://arxiv.org/abs/2310.20057)</code></li>
<li>Summary: <p>As climate change intensifies, the global imperative to shift towards
sustainable energy sources becomes more pronounced. Photovoltaic (PV) energy is
a favored choice due to its reliability and ease of installation. Accurate
mapping of PV installations is crucial for understanding their adoption and
informing energy policy. To meet this need, we introduce the SolarFormer,
designed to segment solar panels from aerial imagery, offering insights into
their location and size. However, solar panel identification in Computer Vision
is intricate due to various factors like weather conditions, roof conditions,
and Ground Sampling Distance (GSD) variations. To tackle these complexities, we
present the SolarFormer, featuring a multi-scale Transformer encoder and a
masked-attention Transformer decoder. Our model leverages low-level features
and incorporates an instance query mechanism to enhance the localization of
solar PV installations. We rigorously evaluated our SolarFormer using diverse
datasets, including GGE (France), IGN (France), and USGS (California, USA),
across different GSDs. Our extensive experiments consistently demonstrate that
our model either matches or surpasses state-of-the-art models, promising
enhanced solar panel segmentation for global sustainable energy initiatives.
</p></li>
</ul>

<h3>Title: UWFormer: Underwater Image Enhancement via a Semi-Supervised Multi-Scale Transformer. (arXiv:2310.20210v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20210">http://arxiv.org/abs/2310.20210</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20210]] UWFormer: Underwater Image Enhancement via a Semi-Supervised Multi-Scale Transformer(http://arxiv.org/abs/2310.20210)</code></li>
<li>Summary: <p>Underwater images often exhibit poor quality, imbalanced coloration, and low
contrast due to the complex and intricate interaction of light, water, and
objects. Despite the significant contributions of previous underwater
enhancement techniques, there exist several problems that demand further
improvement: (i) Current deep learning methodologies depend on Convolutional
Neural Networks (CNNs) that lack multi-scale enhancement and also have limited
global perception fields. (ii) The scarcity of paired real-world underwater
datasets poses a considerable challenge, and the utilization of synthetic image
pairs risks overfitting. To address the aforementioned issues, this paper
presents a Multi-scale Transformer-based Network called UWFormer for enhancing
images at multiple frequencies via semi-supervised learning, in which we
propose a Nonlinear Frequency-aware Attention mechanism and a Multi-Scale
Fusion Feed-forward Network for low-frequency enhancement. Additionally, we
introduce a specialized underwater semi-supervised training strategy, proposing
a Subaqueous Perceptual Loss function to generate reliable pseudo labels.
Experiments using full-reference and non-reference underwater benchmarks
demonstrate that our method outperforms state-of-the-art methods in terms of
both quantity and visual quality.
</p></li>
</ul>

<h3>Title: Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape. (arXiv:2310.20240v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20240">http://arxiv.org/abs/2310.20240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20240]] Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape(http://arxiv.org/abs/2310.20240)</code></li>
<li>Summary: <p>The creation of lifelike speech-driven 3D facial animation requires a natural
and precise synchronization between audio input and facial expressions.
However, existing works still fail to render shapes with flexible head poses
and natural facial details (e.g., wrinkles). This limitation is mainly due to
two aspects: 1) Collecting training set with detailed 3D facial shapes is
highly expensive. This scarcity of detailed shape annotations hinders the
training of models with expressive facial animation. 2) Compared to mouth
movement, the head pose is much less correlated to speech content.
Consequently, concurrent modeling of both mouth movement and head pose yields
the lack of facial movement controllability. To address these challenges, we
introduce VividTalker, a new framework designed to facilitate speech-driven 3D
facial animation characterized by flexible head pose and natural facial
details. Specifically, we explicitly disentangle facial animation into head
pose and mouth movement and encode them separately into discrete latent spaces.
Then, these attributes are generated through an autoregressive process
leveraging a window-based Transformer architecture. To augment the richness of
3D facial animation, we construct a new 3D dataset with detailed shapes and
learn to synthesize facial details in line with speech content. Extensive
quantitative and qualitative experiments demonstrate that VividTalker
outperforms state-of-the-art methods, resulting in vivid and realistic
speech-driven 3D facial animation.
</p></li>
</ul>

<h3>Title: Muscle volume quantification: guiding transformers with anatomical priors. (arXiv:2310.20355v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20355">http://arxiv.org/abs/2310.20355</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20355]] Muscle volume quantification: guiding transformers with anatomical priors(http://arxiv.org/abs/2310.20355)</code></li>
<li>Summary: <p>Muscle volume is a useful quantitative biomarker in sports, but also for the
follow-up of degenerative musculo-skelletal diseases. In addition to volume,
other shape biomarkers can be extracted by segmenting the muscles of interest
from medical images. Manual segmentation is still today the gold standard for
such measurements despite being very time-consuming. We propose a method for
automatic segmentation of 18 muscles of the lower limb on 3D Magnetic Resonance
Images to assist such morphometric analysis. By their nature, the tissue of
different muscles is undistinguishable when observed in MR Images. Thus, muscle
segmentation algorithms cannot rely on appearance but only on contour cues.
However, such contours are hard to detect and their thickness varies across
subjects. To cope with the above challenges, we propose a segmentation approach
based on a hybrid architecture, combining convolutional and visual transformer
blocks. We investigate for the first time the behaviour of such hybrid
architectures in the context of muscle segmentation for shape analysis.
Considering the consistent anatomical muscle configuration, we rely on
transformer blocks to capture the longrange relations between the muscles. To
further exploit the anatomical priors, a second contribution of this work
consists in adding a regularisation loss based on an adjacency matrix of
plausible muscle neighbourhoods estimated from the training data. Our
experimental results on a unique database of elite athletes show it is possible
to train complex hybrid models from a relatively small database of large
volumes, while the anatomical prior regularisation favours better predictions.
</p></li>
</ul>

<h3>Title: The Impact of Depth and Width on Transformer Language Model Generalization. (arXiv:2310.19956v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19956">http://arxiv.org/abs/2310.19956</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19956]] The Impact of Depth and Width on Transformer Language Model Generalization(http://arxiv.org/abs/2310.19956)</code></li>
<li>Summary: <p>To process novel sentences, language models (LMs) must generalize
compositionally -- combine familiar elements in new ways. What aspects of a
model's structure promote compositional generalization? Focusing on
transformers, we test the hypothesis, motivated by recent theoretical and
empirical work, that transformers generalize more compositionally when they are
deeper (have more layers). Because simply adding layers increases the total
number of parameters, confounding depth and size, we construct three classes of
models which trade off depth for width such that the total number of parameters
is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs
and fine-tune them on tasks that test for compositional generalization. We
report three main conclusions: (1) after fine-tuning, deeper models generalize
better out-of-distribution than shallower models do, but the relative benefit
of additional layers diminishes rapidly; (2) within each family, deeper models
show better language modeling performance, but returns are similarly
diminishing; (3) the benefits of depth for compositional generalization cannot
be attributed solely to better performance on language modeling or on
in-distribution data.
</p></li>
</ul>

<h3>Title: Strategies to Harness the Transformers' Potential: UNSL at eRisk 2023. (arXiv:2310.19970v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19970">http://arxiv.org/abs/2310.19970</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19970]] Strategies to Harness the Transformers' Potential: UNSL at eRisk 2023(http://arxiv.org/abs/2310.19970)</code></li>
<li>Summary: <p>The CLEF eRisk Laboratory explores solutions to different tasks related to
risk detection on the Internet. In the 2023 edition, Task 1 consisted of
searching for symptoms of depression, the objective of which was to extract
user writings according to their relevance to the BDI Questionnaire symptoms.
Task 2 was related to the problem of early detection of pathological gambling
risks, where the participants had to detect users at risk as quickly as
possible. Finally, Task 3 consisted of estimating the severity levels of signs
of eating disorders. Our research group participated in the first two tasks,
proposing solutions based on Transformers. For Task 1, we applied different
approaches that can be interesting in information retrieval tasks. Two
proposals were based on the similarity of contextualized embedding vectors, and
the other one was based on prompting, an attractive current technique of
machine learning. For Task 2, we proposed three fine-tuned models followed by
decision policy according to criteria defined by an early detection framework.
One model presented extended vocabulary with important words to the addressed
domain. In the last task, we obtained good performances considering the
decision-based metrics, ranking-based metrics, and runtime. In this work, we
explore different ways to deploy the predictive potential of Transformers in
eRisk tasks.
</p></li>
</ul>

<h3>Title: Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES 2023. (arXiv:2310.20003v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20003">http://arxiv.org/abs/2310.20003</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20003]] Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES 2023(http://arxiv.org/abs/2310.20003)</code></li>
<li>Summary: <p>MentalRiskES is a novel challenge that proposes to solve problems related to
early risk detection for the Spanish language. The objective is to detect, as
soon as possible, Telegram users who show signs of mental disorders considering
different tasks. Task 1 involved the users' detection of eating disorders, Task
2 focused on depression detection, and Task 3 aimed at detecting an unknown
disorder. These tasks were divided into subtasks, each one defining a
resolution approach. Our research group participated in subtask A for Tasks 1
and 2: a binary classification problem that evaluated whether the users were
positive or negative. To solve these tasks, we proposed models based on
Transformers followed by a decision policy according to criteria defined by an
early detection framework. One of the models presented an extended vocabulary
with important words for each task to be solved. In addition, we applied a
decision policy based on the history of predictions that the model performs
during user evaluation. For Tasks 1 and 2, we obtained the second-best
performance according to rankings based on classification and latency,
demonstrating the effectiveness and consistency of our approaches for solving
early detection problems in the Spanish language.
</p></li>
</ul>

<h3>Title: Partial Tensorized Transformers for Natural Language Processing. (arXiv:2310.20077v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20077">http://arxiv.org/abs/2310.20077</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20077]] Partial Tensorized Transformers for Natural Language Processing(http://arxiv.org/abs/2310.20077)</code></li>
<li>Summary: <p>The transformer architecture has revolutionized Natural Language Processing
(NLP) and other machine-learning tasks, due to its unprecedented accuracy.
However, their extensive memory and parameter requirements often hinder their
practical applications. In this work, we study the effect of tensor-train
decomposition to improve the accuracy and compress transformer vision-language
neural networks, namely BERT and ViT. We focus both on embedding-layer
compression and partial tensorization of neural networks (PTNN) through an
algorithmic approach. Our novel PTNN approach significantly improves the
accuracy of existing models by up to 5%, all without the need for post-training
adjustments, breaking new ground in the field of tensor decomposition.
</p></li>
</ul>

<h3>Title: EELBERT: Tiny Models through Dynamic Embeddings. (arXiv:2310.20144v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20144">http://arxiv.org/abs/2310.20144</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20144]] EELBERT: Tiny Models through Dynamic Embeddings(http://arxiv.org/abs/2310.20144)</code></li>
<li>Summary: <p>We introduce EELBERT, an approach for compression of transformer-based models
(e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is
achieved by replacing the input embedding layer of the model with dynamic, i.e.
on-the-fly, embedding computations. Since the input embedding layer accounts
for a significant fraction of the model size, especially for the smaller BERT
variants, replacing this layer with an embedding computation function helps us
reduce the model size significantly. Empirical evaluation on the GLUE benchmark
shows that our BERT variants (EELBERT) suffer minimal regression compared to
the traditional BERT models. Through this approach, we are able to develop our
smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully
trained BERT-tiny, while being 15x smaller (1.2 MB) in size.
</p></li>
</ul>

<h3>Title: Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating Chess Moves based on Sentiment Analysis. (arXiv:2310.20260v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20260">http://arxiv.org/abs/2310.20260</a></li>
<li>Code URL: https://github.com/resrepos/leap</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20260]] Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating Chess Moves based on Sentiment Analysis(http://arxiv.org/abs/2310.20260)</code></li>
<li>Summary: <p>Learning chess strategies has been investigated widely, with most studies
focussing on learning from previous games using search algorithms. Chess
textbooks encapsulate grandmaster knowledge, explain playing strategies and
require a smaller search space compared to traditional chess agents. This paper
examines chess textbooks as a new knowledge source for enabling machines to
learn how to play chess -- a resource that has not been explored previously. We
developed the LEAP corpus, a first and new heterogeneous dataset with
structured (chess move notations and board states) and unstructured data
(textual descriptions) collected from a chess textbook containing 1164
sentences discussing strategic moves from 91 games. We firstly labelled the
sentences based on their relevance, i.e., whether they are discussing a move.
Each relevant sentence was then labelled according to its sentiment towards the
described move. We performed empirical experiments that assess the performance
of various transformer-based baseline models for sentiment analysis. Our
results demonstrate the feasibility of employing transformer-based sentiment
analysis models for evaluating chess moves, with the best performing model
obtaining a weighted micro F_1 score of 68%. Finally, we synthesised the LEAP
corpus to create a larger dataset, which can be used as a solution to the
limited textual resource in the chess domain.
</p></li>
</ul>

<h3>Title: GPCR-BERT: Interpreting Sequential Design of G Protein Coupled Receptors Using Protein Language Models. (arXiv:2310.19915v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19915">http://arxiv.org/abs/2310.19915</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19915]] GPCR-BERT: Interpreting Sequential Design of G Protein Coupled Receptors Using Protein Language Models(http://arxiv.org/abs/2310.19915)</code></li>
<li>Summary: <p>With the rise of Transformers and Large Language Models (LLMs) in Chemistry
and Biology, new avenues for the design and understanding of therapeutics have
opened up to the scientific community. Protein sequences can be modeled as
language and can take advantage of recent advances in LLMs, specifically with
the abundance of our access to the protein sequence datasets. In this paper, we
developed the GPCR-BERT model for understanding the sequential design of G
Protein-Coupled Receptors (GPCRs). GPCRs are the target of over one-third of
FDA-approved pharmaceuticals. However, there is a lack of comprehensive
understanding regarding the relationship between amino acid sequence, ligand
selectivity, and conformational motifs (such as NPxxY, CWxP, E/DRY). By
utilizing the pre-trained protein model (Prot-Bert) and fine-tuning with
prediction tasks of variations in the motifs, we were able to shed light on
several relationships between residues in the binding pocket and some of the
conserved motifs. To achieve this, we took advantage of attention weights, and
hidden states of the model that are interpreted to extract the extent of
contributions of amino acids in dictating the type of masked ones. The
fine-tuned models demonstrated high accuracy in predicting hidden residues
within the motifs. In addition, the analysis of embedding was performed over 3D
structures to elucidate the higher-order interactions within the conformations
of the receptors.
</p></li>
</ul>

<h3>Title: ExPT: Synthetic Pretraining for Few-Shot Experimental Design. (arXiv:2310.19961v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19961">http://arxiv.org/abs/2310.19961</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19961]] ExPT: Synthetic Pretraining for Few-Shot Experimental Design(http://arxiv.org/abs/2310.19961)</code></li>
<li>Summary: <p>Experimental design is a fundamental problem in many science and engineering
fields. In this problem, sample efficiency is crucial due to the time, money,
and safety costs of real-world design evaluations. Existing approaches either
rely on active data collection or access to large, labeled datasets of past
experiments, making them impractical in many real-world scenarios. In this
work, we address the more challenging yet realistic setting of few-shot
experimental design, where only a few labeled data points of input designs and
their corresponding values are available. We approach this problem as a
conditional generation task, where a model conditions on a few labeled examples
and the desired output to generate an optimal input design. To this end, we
introduce Experiment Pretrained Transformers (ExPT), a foundation model for
few-shot experimental design that employs a novel combination of synthetic
pretraining with in-context learning. In ExPT, we only assume knowledge of a
finite collection of unlabelled data points from the input domain and pretrain
a transformer neural network to optimize diverse synthetic functions defined
over this domain. Unsupervised pretraining allows ExPT to adapt to any design
task at test time in an in-context fashion by conditioning on a few labeled
data points from the target task and generating the candidate optima. We
evaluate ExPT on few-shot experimental design in challenging domains and
demonstrate its superior generality and performance compared to existing
methods. The source code is available at https://github.com/tung-nd/ExPT.git.
</p></li>
</ul>

<h3>Title: The Expressibility of Polynomial based Attention Scheme. (arXiv:2310.20051v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20051">http://arxiv.org/abs/2310.20051</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20051]] The Expressibility of Polynomial based Attention Scheme(http://arxiv.org/abs/2310.20051)</code></li>
<li>Summary: <p>Large language models (LLMs) have significantly improved various aspects of
our daily lives. These models have impacted numerous domains, from healthcare
to education, enhancing productivity, decision-making processes, and
accessibility. As a result, they have influenced and, to some extent, reshaped
people's lifestyles. However, the quadratic complexity of attention in
transformer architectures poses a challenge when scaling up these models for
processing long textual contexts. This issue makes it impractical to train very
large models on lengthy texts or use them efficiently during inference. While a
recent study by [KMZ23] introduced a technique that replaces the softmax with a
polynomial function and polynomial sketching to speed up attention mechanisms,
the theoretical understandings of this new approach are not yet well
understood.
</p>
<p>In this paper, we offer a theoretical analysis of the expressive capabilities
of polynomial attention. Our study reveals a disparity in the ability of
high-degree and low-degree polynomial attention. Specifically, we construct two
carefully designed datasets, namely $\mathcal{D}_0$ and $\mathcal{D}_1$, where
$\mathcal{D}_1$ includes a feature with a significantly larger value compared
to $\mathcal{D}_0$. We demonstrate that with a sufficiently high degree
$\beta$, a single-layer polynomial attention network can distinguish between
$\mathcal{D}_0$ and $\mathcal{D}_1$. However, with a low degree $\beta$, the
network cannot effectively separate the two datasets. This analysis underscores
the greater effectiveness of high-degree polynomials in amplifying large values
and distinguishing between datasets. Our analysis offers insight into the
representational capacity of polynomial attention and provides a rationale for
incorporating higher-degree polynomials in attention mechanisms to capture
intricate linguistic correlations.
</p></li>
</ul>

<h3>Title: A Systematic Review for Transformer-based Long-term Series Forecasting. (arXiv:2310.20218v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20218">http://arxiv.org/abs/2310.20218</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20218]] A Systematic Review for Transformer-based Long-term Series Forecasting(http://arxiv.org/abs/2310.20218)</code></li>
<li>Summary: <p>The emergence of deep learning has yielded noteworthy advancements in time
series forecasting (TSF). Transformer architectures, in particular, have
witnessed broad utilization and adoption in TSF tasks. Transformers have proven
to be the most successful solution to extract the semantic correlations among
the elements within a long sequence. Various variants have enabled transformer
architecture to effectively handle long-term time series forecasting (LTSF)
tasks. In this article, we first present a comprehensive overview of
transformer architectures and their subsequent enhancements developed to
address various LTSF tasks. Then, we summarize the publicly available LTSF
datasets and relevant evaluation metrics. Furthermore, we provide valuable
insights into the best practices and techniques for effectively training
transformers in the context of time-series analysis. Lastly, we propose
potential research directions in this rapidly evolving field.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone. (arXiv:2310.19859v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19859">http://arxiv.org/abs/2310.19859</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19859]] Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone(http://arxiv.org/abs/2310.19859)</code></li>
<li>Summary: <p>Parameter-efficient tuning has become a trend in transferring large-scale
foundation models to downstream applications. Existing methods typically embed
some light-weight tuners into the backbone, where both the design and the
learning of the tuners are highly dependent on the base model. This work offers
a new tuning paradigm, dubbed Res-Tuning, which intentionally unbinds tuners
from the backbone. With both theoretical and empirical evidence, we show that
popular tuning approaches have their equivalent counterparts under our
unbinding formulation, and hence can be integrated into our framework
effortlessly. Thanks to the structural disentanglement, we manage to free the
design of tuners from the network architecture, facilitating flexible
combination of various tuning strategies. We further propose a memory-efficient
variant of Res-Tuning, where the bypass i.e., formed by a sequence of tuners)
is effectively detached from the main branch, such that the gradients are
back-propagated only to the tuners but not to the backbone. Such a detachment
also allows one-time backbone forward for multi-task inference. Extensive
experiments on both discriminative and generative tasks demonstrate the
superiority of our method over existing alternatives from the perspectives of
efficacy and efficiency. Project page:
$\href{https://res-tuning.github.io/}{\textit{https://res-tuning.github.io/}}$.
</p></li>
</ul>

<h3>Title: Addressing Weak Decision Boundaries in Image Classification by Leveraging Web Search and Generative Models. (arXiv:2310.19986v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19986">http://arxiv.org/abs/2310.19986</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19986]] Addressing Weak Decision Boundaries in Image Classification by Leveraging Web Search and Generative Models(http://arxiv.org/abs/2310.19986)</code></li>
<li>Summary: <p>Machine learning (ML) technologies are known to be riddled with ethical and
operational problems, however, we are witnessing an increasing thrust by
businesses to deploy them in sensitive applications. One major issue among many
is that ML models do not perform equally well for underrepresented groups. This
puts vulnerable populations in an even disadvantaged and unfavorable position.
We propose an approach that leverages the power of web search and generative
models to alleviate some of the shortcomings of discriminative models. We
demonstrate our method on an image classification problem using ImageNet's
People Subtree subset, and show that it is effective in enhancing robustness
and mitigating bias in certain classes that represent vulnerable populations
(e.g., female doctor of color). Our new method is able to (1) identify weak
decision boundaries for such classes; (2) construct search queries for Google
as well as text for generating images through DALL-E 2 and Stable Diffusion;
and (3) show how these newly captured training samples could alleviate
population bias issue. While still improving the model's overall performance
considerably, we achieve a significant reduction (77.30\%) in the model's
gender accuracy disparity. In addition to these improvements, we observed a
notable enhancement in the classifier's decision boundary, as it is
characterized by fewer weakspots and an increased separation between classes.
Although we showcase our method on vulnerable populations in this study, the
proposed technique is extendable to a wide range of problems and domains.
</p></li>
</ul>

<h3>Title: Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design. (arXiv:2310.19998v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19998">http://arxiv.org/abs/2310.19998</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19998]] Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design(http://arxiv.org/abs/2310.19998)</code></li>
<li>Summary: <p>Transformer neural networks show promising capabilities, in particular for
uses in materials analysis, design and manufacturing, including their capacity
to work effectively with both human language, symbols, code, and numerical
data. Here we explore the use of large language models (LLMs) as a tool that
can support engineering analysis of materials, applied to retrieving key
information about subject areas, developing research hypotheses, discovery of
mechanistic relationships across disparate areas of knowledge, and writing and
executing simulation codes for active knowledge generation based on physical
ground truths. When used as sets of AI agents with specific features,
capabilities, and instructions, LLMs can provide powerful problem solution
strategies for applications in analysis and design problems. Our experiments
focus on using a fine-tuned model, MechGPT, developed based on training data in
the mechanics of materials domain. We first affirm how finetuning endows LLMs
with reasonable understanding of domain knowledge. However, when queried
outside the context of learned matter, LLMs can have difficulty to recall
correct information. We show how this can be addressed using
retrieval-augmented Ontological Knowledge Graph strategies that discern how the
model understands what concepts are important and how they are related.
Illustrated for a use case of relating distinct areas of knowledge - here,
music and proteins - such strategies can also provide an interpretable graph
structure with rich information at the node, edge and subgraph level. We
discuss nonlinear sampling strategies and agent-based modeling applied to
complex question answering, code generation and execution in the context of
automated force field development from actively learned Density Functional
Theory (DFT) modeling, and data analysis.
</p></li>
</ul>

<h3>Title: Automatic Evaluation of Generative Models with Instruction Tuning. (arXiv:2310.20072v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20072">http://arxiv.org/abs/2310.20072</a></li>
<li>Code URL: https://github.com/shuhaibm/heap</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20072]] Automatic Evaluation of Generative Models with Instruction Tuning(http://arxiv.org/abs/2310.20072)</code></li>
<li>Summary: <p>Automatic evaluation of natural language generation has long been an elusive
goal in NLP.A recent paradigm fine-tunes pre-trained language models to emulate
human judgements for a particular task and evaluation criterion. Inspired by
the generalization ability of instruction-tuned models, we propose a learned
metric based on instruction tuning. To test our approach, we collected HEAP, a
dataset of human judgements across various NLG tasks and evaluation criteria.
Our findings demonstrate that instruction tuning language models on HEAP yields
good performance on many evaluation tasks, though some criteria are less
trivial to learn than others. Further, jointly training on multiple tasks can
yield additional performance improvements, which can be beneficial for future
tasks with little to no human annotated data.
</p></li>
</ul>

<h3>Title: Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models. (arXiv:2310.19802v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19802">http://arxiv.org/abs/2310.19802</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19802]] Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models(http://arxiv.org/abs/2310.19802)</code></li>
<li>Summary: <p>We have formulated generative machine learning problems as the time evolution
of Parametric Probabilistic Models (PPMs), inherently rendering a thermodynamic
process. Then, we have studied the thermodynamic exchange between the model's
parameters, denoted as $\Theta$, and the model's generated samples, denoted as
$X$. We demonstrate that the training dataset and the action of the Stochastic
Gradient Descent (SGD) optimizer serve as a work source that governs the time
evolution of these two subsystems. Our findings reveal that the model learns
through the dissipation of heat during the generation of samples $X$, leading
to an increase in the entropy of the model's parameters, $\Theta$. Thus, the
parameter subsystem acts as a heat reservoir, effectively storing the learned
information. Furthermore, the role of the model's parameters as a heat
reservoir provides valuable thermodynamic insights into the generalization
power of over-parameterized models. This approach offers an unambiguous
framework for computing information-theoretic quantities within deterministic
neural networks by establishing connections with thermodynamic variables. To
illustrate the utility of this framework, we introduce two
information-theoretic metrics: Memorized-information (M-info) and
Learned-information (L-info), which trace the dynamic flow of information
during the learning process of PPMs.
</p></li>
</ul>

<h3>Title: Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms. (arXiv:2310.19927v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19927">http://arxiv.org/abs/2310.19927</a></li>
<li>Code URL: https://github.com/agentification/rp_pgm</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19927]] Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms(http://arxiv.org/abs/2310.19927)</code></li>
<li>Summary: <p>ReParameterization (RP) Policy Gradient Methods (PGMs) have been widely
adopted for continuous control tasks in robotics and computer graphics.
However, recent studies have revealed that, when applied to long-term
reinforcement learning problems, model-based RP PGMs may experience chaotic and
non-smooth optimization landscapes with exploding gradient variance, which
leads to slow convergence. This is in contrast to the conventional belief that
reparameterization methods have low gradient estimation variance in problems
such as training deep generative models. To comprehend this phenomenon, we
conduct a theoretical examination of model-based RP PGMs and search for
solutions to the optimization difficulties. Specifically, we analyze the
convergence of the model-based RP PGMs and pinpoint the smoothness of function
approximators as a major factor that affects the quality of gradient
estimation. Based on our analysis, we propose a spectral normalization method
to mitigate the exploding variance issue caused by long model unrolls. Our
experimental results demonstrate that proper normalization significantly
reduces the gradient variance of model-based RP PGMs. As a result, the
performance of the proposed method is comparable or superior to other gradient
estimators, such as the Likelihood Ratio (LR) gradient estimator. Our code is
available at https://github.com/agentification/RP_PGM.
</p></li>
</ul>

<h3>Title: The Acquisition of Physical Knowledge in Generative Neural Networks. (arXiv:2310.19943v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19943">http://arxiv.org/abs/2310.19943</a></li>
<li>Code URL: https://github.com/cross32768/PlaNet_PyTorch</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19943]] The Acquisition of Physical Knowledge in Generative Neural Networks(http://arxiv.org/abs/2310.19943)</code></li>
<li>Summary: <p>As children grow older, they develop an intuitive understanding of the
physical processes around them. Their physical understanding develops in
stages, moving along developmental trajectories which have been mapped out
extensively in previous empirical research. Here, we investigate how the
learning trajectories of deep generative neural networks compare to children's
developmental trajectories using physical understanding as a testbed. We
outline an approach that allows us to examine two distinct hypotheses of human
development - stochastic optimization and complexity increase. We find that
while our models are able to accurately predict a number of physical processes,
their learning trajectories under both hypotheses do not follow the
developmental trajectories of children.
</p></li>
</ul>

<h3>Title: GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models. (arXiv:2310.20025v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20025">http://arxiv.org/abs/2310.20025</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20025]] GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models(http://arxiv.org/abs/2310.20025)</code></li>
<li>Summary: <p>Offline goal-conditioned RL (GCRL) offers a feasible paradigm to learn
general-purpose policies from diverse and multi-task offline datasets. Despite
notable recent progress, the predominant offline GCRL methods have been
restricted to model-free approaches, constraining their capacity to tackle
limited data budgets and unseen goal generalization. In this work, we propose a
novel two-stage model-based framework, Goal-conditioned Offline Planning
(GOPlan), including (1) pretraining a prior policy capable of capturing
multi-modal action distribution within the multi-goal dataset; (2) employing
the reanalysis method with planning to generate imagined trajectories for
funetuning policies. Specifically, the prior policy is based on an
advantage-weighted Conditioned Generative Adversarial Networks that exhibits
distinct mode separation to overcome the pitfalls of out-of-distribution (OOD)
actions. For further policy optimization, the reanalysis method generates
high-quality imaginary data by planning with learned models for both
intra-trajectory and inter-trajectory goals. Through experimental evaluations,
we demonstrate that GOPlan achieves state-of-the-art performance on various
offline multi-goal manipulation tasks. Moreover, our results highlight the
superior ability of GOPlan to handle small data budgets and generalize to OOD
goals.
</p></li>
</ul>

<h3>Title: Advancing Bayesian Optimization via Learning Correlated Latent Space. (arXiv:2310.20258v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20258">http://arxiv.org/abs/2310.20258</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20258]] Advancing Bayesian Optimization via Learning Correlated Latent Space(http://arxiv.org/abs/2310.20258)</code></li>
<li>Summary: <p>Bayesian optimization is a powerful method for optimizing black-box functions
with limited function evaluations. Recent works have shown that optimization in
a latent space through deep generative models such as variational autoencoders
leads to effective and efficient Bayesian optimization for structured or
discrete data. However, as the optimization does not take place in the input
space, it leads to an inherent gap that results in potentially suboptimal
solutions. To alleviate the discrepancy, we propose Correlated latent space
Bayesian Optimization (CoBO), which focuses on learning correlated latent
spaces characterized by a strong correlation between the distances in the
latent space and the distances within the objective function. Specifically, our
method introduces Lipschitz regularization, loss weighting, and trust region
recoordination to minimize the inherent gap around the promising areas. We
demonstrate the effectiveness of our approach on several optimization tasks in
discrete data, such as molecule design and arithmetic expression fitting, and
achieve high performance within a small budget.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Emotional Theory of Mind: Bridging Fast Visual Processing with Slow Linguistic Reasoning. (arXiv:2310.19995v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19995">http://arxiv.org/abs/2310.19995</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19995]] Emotional Theory of Mind: Bridging Fast Visual Processing with Slow Linguistic Reasoning(http://arxiv.org/abs/2310.19995)</code></li>
<li>Summary: <p>The emotional theory of mind problem in images is an emotion recognition
task, specifically asking "How does the person in the bounding box feel?"
Facial expressions, body pose, contextual information and implicit commonsense
knowledge all contribute to the difficulty of the task, making this task
currently one of the hardest problems in affective computing. The goal of this
work is to evaluate the emotional commonsense knowledge embedded in recent
large vision language models (CLIP, LLaVA) and large language models (GPT-3.5)
on the Emotions in Context (EMOTIC) dataset. In order to evaluate a purely
text-based language model on images, we construct "narrative captions" relevant
to emotion perception, using a set of 872 physical social signal descriptions
related to 26 emotional categories, along with 224 labels for emotionally
salient environmental contexts, sourced from writer's guides for character
expressions and settings. We evaluate the use of the resulting captions in an
image-to-language-to-emotion task. Experiments using zero-shot vision-language
models on EMOTIC show that combining "fast" and "slow" reasoning is a promising
way forward to improve emotion recognition systems. Nevertheless, a gap remains
in the zero-shot emotional theory of mind task compared to prior work trained
on the EMOTIC dataset.
</p></li>
</ul>

<h3>Title: BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing. (arXiv:2310.19975v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.19975">http://arxiv.org/abs/2310.19975</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.19975]] BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing(http://arxiv.org/abs/2310.19975)</code></li>
<li>Summary: <p>Large language models (LLMs) has achieved a great success in many natural
language processing (NLP) tasks. This is achieved by pretraining of LLMs on
vast amount of data and then instruction tuning to specific domains. However,
only a few instructions in the biomedical domain have been published. To
address this issue, we introduce BioInstruct, a customized task-specific
instruction dataset containing more than 25,000 examples. This dataset was
generated attractively by prompting a GPT-4 language model with a
three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using
the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical
natural language processing (BioNLP). We conducted instruction tuning on the
LLaMA LLMs (1\&amp;2, 7B\&amp;13B) and evaluated them on BioNLP applications, including
information extraction, question answering, and text generation. We also
evaluated how instructions contributed to model performance using multi-tasking
learning principles.
</p></li>
</ul>

<h3>Title: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization. (arXiv:2310.20033v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20033">http://arxiv.org/abs/2310.20033</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20033]] Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization(http://arxiv.org/abs/2310.20033)</code></li>
<li>Summary: <p>Large Language Models (LLMs) like the GPT and LLaMA families have
demonstrated exceptional capabilities in capturing and condensing critical
contextual information and achieving state-of-the-art performance in the
summarization task. However, community concerns about these models'
hallucination issues continue to rise. LLMs sometimes generate factually
hallucinated summaries, which can be extremely harmful in the clinical domain
NLP tasks (e.g., clinical note summarization), where factually incorrect
statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using
human feedback has shown the promise of aligning LLMs to be factually
consistent during generation, but such training procedure requires high-quality
human-annotated data, which can be extremely expensive to get in the clinical
domain. In this work, we propose a new pipeline using ChatGPT instead of human
experts to generate high-quality feedback data for improving factual
consistency in the clinical note summarization task. We focus specifically on
edit feedback because recent work discusses the shortcomings of human alignment
via preference feedback in complex situations (such as clinical NLP tasks that
require extensive expert knowledge), as well as some advantages of collecting
edit feedback from domain experts. In addition, although GPT has reached the
expert level in many clinical NLP tasks (e.g., USMLE QA), there is not much
previous work discussing whether GPT can generate expert-level edit feedback
for LMs in the clinical note summarization task. We hope to fill this gap.
Finally, our evaluations demonstrate the potential use of GPT edits in human
alignment, especially from a factuality perspective.
</p></li>
</ul>

<h3>Title: Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection. (arXiv:2310.20046v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20046">http://arxiv.org/abs/2310.20046</a></li>
<li>Code URL: https://github.com/amazon-science/adaptive-in-context-learning</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20046]] Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection(http://arxiv.org/abs/2310.20046)</code></li>
<li>Summary: <p>Large Language Models (LLMs) can adapt to new tasks via in-context learning
(ICL). ICL is efficient as it does not require any parameter updates to the
trained LLM, but only few annotated examples as input for the LLM. In this
work, we investigate an active learning approach for ICL, where there is a
limited budget for annotating examples. We propose a model-adaptive
optimization-free algorithm, termed AdaICL, which identifies examples that the
model is uncertain about, and performs semantic diversity-based example
selection. Diversity-based sampling improves overall effectiveness, while
uncertainty sampling improves budget efficiency and helps the LLM learn new
information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage
problem, that dynamically adapts based on the model's feedback and can be
approximately solved via greedy algorithms. Extensive experiments on nine
datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy
points over SOTA (7.7% relative improvement), is up to 3x more budget-efficient
than performing annotations uniformly at random, while it outperforms SOTA with
2x fewer ICL examples.
</p></li>
</ul>

<h3>Title: Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models. (arXiv:2310.20081v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20081">http://arxiv.org/abs/2310.20081</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20081]] Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models(http://arxiv.org/abs/2310.20081)</code></li>
<li>Summary: <p>Personalization, the ability to tailor a system to individual users, is an
essential factor in user experience with natural language processing (NLP)
systems. With the emergence of Large Language Models (LLMs), a key question is
how to leverage these models to better personalize user experiences. To
personalize a language model's output, a straightforward approach is to
incorporate past user data into the language model prompt, but this approach
can result in lengthy inputs exceeding limitations on input length and
incurring latency and cost issues. Existing approaches tackle such challenges
by selectively extracting relevant user data (i.e. selective retrieval) to
construct a prompt for downstream tasks. However, retrieval-based methods are
limited by potential information loss, lack of more profound user
understanding, and cold-start challenges. To overcome these limitations, we
propose a novel summary-augmented approach by extending retrieval-augmented
personalization with task-aware user summaries generated by LLMs. The summaries
can be generated and stored offline, enabling real-world systems with runtime
constraints like voice assistants to leverage the power of LLMs. Experiments
show our method with 75% less of retrieved user data is on-par or outperforms
retrieval augmentation on most tasks in the LaMP personalization benchmark. We
demonstrate that offline summarization via LLMs and runtime retrieval enables
better performance for personalization on a range of tasks under practical
constraints.
</p></li>
</ul>

<h3>Title: Making Large Language Models Better Data Creators. (arXiv:2310.20111v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20111">http://arxiv.org/abs/2310.20111</a></li>
<li>Code URL: https://github.com/microsoft/llm-data-creation</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20111]] Making Large Language Models Better Data Creators(http://arxiv.org/abs/2310.20111)</code></li>
<li>Summary: <p>Although large language models (LLMs) have advanced the state-of-the-art in
NLP significantly, deploying them for downstream applications is still
challenging due to cost, responsiveness, control, or concerns around privacy
and security. As such, trainable models are still the preferred option in some
cases. However, these models still require human-labeled data for optimal
performance, which is expensive and time-consuming to obtain. In order to
address this issue, several techniques to reduce human effort involve labeling
or generating data using LLMs. Although these methods are effective for certain
applications, in practice they encounter difficulties in real-world scenarios.
Labeling data requires careful data selection, while generating data
necessitates task-specific prompt engineering. In this paper, we propose a
unified data creation pipeline that requires only a single formatting example,
and which is applicable to a broad range of tasks, including traditionally
problematic ones with semantically devoid label spaces. In our experiments we
demonstrate that instruction-following LLMs are highly cost-effective data
creators, and that models trained with these data exhibit performance better
than those trained with human-labeled data (by up to 17.5%) on
out-of-distribution evaluation, while maintaining comparable performance on
in-distribution tasks. These results have important implications for the
robustness of NLP systems deployed in the real-world.
</p></li>
</ul>

<h3>Title: Multi-Agent Consensus Seeking via Large Language Models. (arXiv:2310.20151v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20151">http://arxiv.org/abs/2310.20151</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20151]] Multi-Agent Consensus Seeking via Large Language Models(http://arxiv.org/abs/2310.20151)</code></li>
<li>Summary: <p>Multi-agent systems driven by large language models (LLMs) have shown
promising abilities for solving complex tasks in a collaborative manner. This
work considers a fundamental problem in multi-agent collaboration: consensus
seeking. When multiple agents work together, we are interested in how they can
reach a consensus through inter-agent negotiation. To that end, this work
studies a consensus-seeking task where the state of each agent is a numerical
value and they negotiate with each other to reach a consensus value. It is
revealed that when not explicitly directed on which strategy should be adopted,
the LLM-driven agents primarily use the average strategy for consensus seeking
although they may occasionally use some other strategies. Moreover, this work
analyzes the impact of the agent number, agent personality, and network
topology on the negotiation process. The findings reported in this work can
potentially lay the foundations for understanding the behaviors of LLM-driven
multi-agent systems for solving more complex tasks. Furthermore, LLM-driven
consensus seeking is applied to a multi-robot aggregation task. This
application demonstrates the potential of LLM-driven agents to achieve
zero-shot autonomous planning for multi-robot collaboration tasks. Project
website: westlakeintelligentrobotics.github.io/ConsensusLLM/.
</p></li>
</ul>

<h3>Title: Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision. (arXiv:2310.20153v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20153">http://arxiv.org/abs/2310.20153</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20153]] Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision(http://arxiv.org/abs/2310.20153)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated remarkable capabilities in
various tasks. However, their suitability for domain-specific tasks, is limited
due to their immense scale at deployment, susceptibility to misinformation, and
more importantly, high data annotation costs. We propose a novel Interactive
Multi-Fidelity Learning (IMFL) framework for the cost-effective development of
small domain-specific LMs under limited annotation budgets. Our approach
formulates the domain-specific fine-tuning process as a multi-fidelity learning
problem, focusing on identifying the optimal acquisition strategy that balances
between low-fidelity automatic LLM annotations and high-fidelity human
annotations to maximize model performance. We further propose an
exploration-exploitation query strategy that enhances annotation diversity and
informativeness, incorporating two innovative designs: 1) prompt retrieval that
selects in-context examples from human-annotated samples to improve LLM
annotation, and 2) variable batch size that controls the order for choosing
each fidelity to facilitate knowledge distillation, ultimately enhancing
annotation quality. Extensive experiments on financial and medical tasks
demonstrate that IMFL achieves superior performance compared with single
fidelity annotations. Given a limited budget of human annotation, IMFL
significantly outperforms the human annotation baselines in all four tasks and
achieves very close performance as human annotations on two of the tasks. These
promising results suggest that the high human annotation costs in
domain-specific tasks can be significantly reduced by employing IMFL, which
utilizes fewer human annotations, supplemented with cheaper and faster LLM
(e.g., GPT-3.5) annotations to achieve comparable performance.
</p></li>
</ul>

<h3>Title: GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval. (arXiv:2310.20158v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20158">http://arxiv.org/abs/2310.20158</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20158]] GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval(http://arxiv.org/abs/2310.20158)</code></li>
<li>Summary: <p>Given a query and a document corpus, the information retrieval (IR) task is
to output a ranked list of relevant documents. Combining large language models
(LLMs) with embedding-based retrieval models, recent work shows promising
results on the zero-shot retrieval problem, i.e., no access to labeled data
from the target domain. Two such popular paradigms are generation-augmented
retrieval or GAR (generate additional context for the query and then retrieve),
and retrieval-augmented generation or RAG (retrieve relevant documents as
context and then generate answers). The success of these paradigms hinges on
(i) high-recall retrieval models, which are difficult to obtain in the
zero-shot setting, and (ii) high-precision (re-)ranking models which typically
need a good initialization. In this work, we propose a novel GAR-meets-RAG
recurrence formulation that overcomes the challenges of existing paradigms. Our
method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in
the zero-shot setting. A key design principle is that the rewrite-retrieval
stages improve the recall of the system and a final re-ranking stage improves
the precision. We conduct extensive experiments on zero-shot passage retrieval
benchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in
the BEIR benchmark, outperforming previous best results in Recall@100 and
nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the
previous best.
</p></li>
</ul>

<h3>Title: DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text. (arXiv:2310.20170v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20170">http://arxiv.org/abs/2310.20170</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20170]] DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text(http://arxiv.org/abs/2310.20170)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have exhibited impressive generation
capabilities, but they suffer from hallucinations when solely relying on their
internal knowledge, especially when answering questions that require less
commonly known information. Retrieval-augmented LLMs have emerged as a
potential solution to ground LLMs in external knowledge. Nonetheless, recent
approaches have primarily emphasized retrieval from unstructured text corpora,
owing to its seamless integration into prompts. When using structured data such
as knowledge graphs, most methods simplify it into natural text, neglecting the
underlying structures. Moreover, a significant gap in the current landscape is
the absence of a realistic benchmark for evaluating the effectiveness of
grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and
text). To fill this gap, we have curated a comprehensive dataset that poses two
unique challenges: (1) Two-hop multi-source questions that require retrieving
information from both open-domain structured and unstructured knowledge
sources; retrieving information from structured knowledge sources is a critical
component in correctly answering the questions. (2) The generation of symbolic
queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another
layer of challenge. Our dataset is created using a combination of automatic
generation through predefined reasoning chains and human annotation. We also
introduce a novel approach that leverages multiple retrieval tools, including
text passage retrieval and symbolic language-assisted retrieval. Our model
outperforms previous approaches by a significant margin, demonstrating its
effectiveness in addressing the above-mentioned reasoning challenges.
</p></li>
</ul>

<h3>Title: Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests. (arXiv:2310.20320v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20320">http://arxiv.org/abs/2310.20320</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20320]] Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs(http://arxiv.org/abs/2310.20320)</code></li>
<li>Summary: <p>To what degree should we ascribe cognitive capacities to Large Language
Models (LLMs), such as the ability to reason about intentions and beliefs known
as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11
base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the
dominant false-belief paradigm, including non-literal language usage and
recursive intentionality; (ii) using newly rewritten versions of standardized
tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides
closed questions; and (iv) benchmarking LLM performance against that of
children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from
the GPT family outperform other models, and often also children. Base-LLMs are
mostly unable to solve ToM tasks, even with specialized prompting. We suggest
that the interlinked evolution and development of language and ToM may help
explain what instruction-tuning adds: rewarding cooperative communication that
takes into account interlocutor and context. We conclude by arguing for a
nuanced perspective on ToM in LLMs.
</p></li>
</ul>

<h3>Title: Do large language models solve verbal analogies like children do?. (arXiv:2310.20384v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20384">http://arxiv.org/abs/2310.20384</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20384]] Do large language models solve verbal analogies like children do?(http://arxiv.org/abs/2310.20384)</code></li>
<li>Summary: <p>Analogy-making lies at the heart of human cognition. Adults solve analogies
such as \textit{Horse belongs to stable like chicken belongs to ...?} by
mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In
contrast, children often use association, e.g., answering \textit{egg}. This
paper investigates whether large language models (LLMs) solve verbal analogies
in A:B::C:? form using associations, similar to what children do. We use verbal
analogies extracted from an online adaptive learning environment, where 14,002
7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six
tested Dutch monolingual and multilingual LLMs performed around the same level
as children, with MGPT performing worst, around the 7-year-old level, and XLM-V
and GPT-3 the best, slightly above the 11-year-old level. However, when we
control for associative processes this picture changes and each model's
performance level drops 1-2 years. Further experiments demonstrate that
associative processes often underlie correctly solved analogies. We conclude
that the LLMs we tested indeed tend to solve verbal analogies by association
with C like children do.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Team I2R-VI-FF Technical Report on EPIC-KITCHENS VISOR Hand Object Segmentation Challenge 2023. (arXiv:2310.20120v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20120">http://arxiv.org/abs/2310.20120</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20120]] Team I2R-VI-FF Technical Report on EPIC-KITCHENS VISOR Hand Object Segmentation Challenge 2023(http://arxiv.org/abs/2310.20120)</code></li>
<li>Summary: <p>In this report, we present our approach to the EPIC-KITCHENS VISOR Hand
Object Segmentation Challenge, which focuses on the estimation of the relation
between the hands and the objects given a single frame as input. The
EPIC-KITCHENS VISOR dataset provides pixel-wise annotations and serves as a
benchmark for hand and active object segmentation in egocentric video. Our
approach combines the baseline method, i.e., Point-based Rendering (PointRend)
and the Segment Anything Model (SAM), aiming to enhance the accuracy of hand
and object segmentation outcomes, while also minimizing instances of missed
detection. We leverage accurate hand segmentation maps obtained from the
baseline method to extract more precise hand and in-contact object segments. We
utilize the class-agnostic segmentation provided by SAM and apply specific
hand-crafted constraints to enhance the results. In cases where the baseline
model misses the detection of hands or objects, we re-train an object detector
on the training set to enhance the detection accuracy. The detected hand and
in-contact object bounding boxes are then used as prompts to extract their
respective segments from the output of SAM. By effectively combining the
strengths of existing methods and applying our refinements, our submission
achieved the 1st place in terms of evaluation criteria in the VISOR HOS
Challenge.
</p></li>
</ul>

<h3>Title: From Denoising Training to Test-Time Adaptation: Enhancing Domain Generalization for Medical Image Segmentation. (arXiv:2310.20271v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20271">http://arxiv.org/abs/2310.20271</a></li>
<li>Code URL: https://github.com/wenruxue/detta</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20271]] From Denoising Training to Test-Time Adaptation: Enhancing Domain Generalization for Medical Image Segmentation(http://arxiv.org/abs/2310.20271)</code></li>
<li>Summary: <p>In medical image segmentation, domain generalization poses a significant
challenge due to domain shifts caused by variations in data acquisition devices
and other factors. These shifts are particularly pronounced in the most common
scenario, which involves only single-source domain data due to privacy
concerns. To address this, we draw inspiration from the self-supervised
learning paradigm that effectively discourages overfitting to the source
domain. We propose the Denoising Y-Net (DeY-Net), a novel approach
incorporating an auxiliary denoising decoder into the basic U-Net architecture.
The auxiliary decoder aims to perform denoising training, augmenting the
domain-invariant representation that facilitates domain generalization.
Furthermore, this paradigm provides the potential to utilize unlabeled data.
Building upon denoising training, we propose Denoising Test Time Adaptation
(DeTTA) that further: (i) adapts the model to the target domain in a
sample-wise manner, and (ii) adapts to the noise-corrupted input. Extensive
experiments conducted on widely-adopted liver segmentation benchmarks
demonstrate significant domain generalization improvements over our baseline
and state-of-the-art results compared to other methods. Code is available at
https://github.com/WenRuxue/DeTTA.
</p></li>
</ul>

<h3>Title: Annotator: A Generic Active Learning Baseline for LiDAR Semantic Segmentation. (arXiv:2310.20293v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20293">http://arxiv.org/abs/2310.20293</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20293]] Annotator: A Generic Active Learning Baseline for LiDAR Semantic Segmentation(http://arxiv.org/abs/2310.20293)</code></li>
<li>Summary: <p>Active learning, a label-efficient paradigm, empowers models to interactively
query an oracle for labeling new data. In the realm of LiDAR semantic
segmentation, the challenges stem from the sheer volume of point clouds,
rendering annotation labor-intensive and cost-prohibitive. This paper presents
Annotator, a general and efficient active learning baseline, in which a
voxel-centric online selection strategy is tailored to efficiently probe and
annotate the salient and exemplar voxel girds within each LiDAR scan, even
under distribution shift. Concretely, we first execute an in-depth analysis of
several common selection strategies such as Random, Entropy, Margin, and then
develop voxel confusion degree (VCD) to exploit the local topology relations
and structures of point clouds. Annotator excels in diverse settings, with a
particular focus on active learning (AL), active source-free domain adaptation
(ASFDA), and active domain adaptation (ADA). It consistently delivers
exceptional performance across LiDAR semantic segmentation benchmarks, spanning
both simulation-to-real and real-to-real scenarios. Surprisingly, Annotator
exhibits remarkable efficiency, requiring significantly fewer annotations,
e.g., just labeling five voxels per scan in the SynLiDAR-to-SemanticKITTI task.
This results in impressive performance, achieving 87.8% fully-supervised
performance under AL, 88.5% under ASFDA, and 94.4% under ADA. We envision that
Annotator will offer a simple, general, and efficient solution for
label-efficient 3D applications. Project page:
https://binhuixie.github.io/annotator-web
</p></li>
</ul>

<h3>Title: Bilateral Network with Residual U-blocks and Dual-Guided Attention for Real-time Semantic Segmentation. (arXiv:2310.20305v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20305">http://arxiv.org/abs/2310.20305</a></li>
<li>Code URL: https://github.com/likelidoa/bidganet</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20305]] Bilateral Network with Residual U-blocks and Dual-Guided Attention for Real-time Semantic Segmentation(http://arxiv.org/abs/2310.20305)</code></li>
<li>Summary: <p>When some application scenarios need to use semantic segmentation technology,
like automatic driving, the primary concern comes to real-time performance
rather than extremely high segmentation accuracy. To achieve a good trade-off
between speed and accuracy, two-branch architecture has been proposed in recent
years. It treats spatial information and semantics information separately which
allows the model to be composed of two networks both not heavy. However, the
process of fusing features with two different scales becomes a performance
bottleneck for many nowaday two-branch models. In this research, we design a
new fusion mechanism for two-branch architecture which is guided by attention
computation. To be precise, we use the Dual-Guided Attention (DGA) module we
proposed to replace some multi-scale transformations with the calculation of
attention which means we only use several attention layers of near linear
complexity to achieve performance comparable to frequently-used multi-layer
fusion. To ensure that our module can be effective, we use Residual U-blocks
(RSU) to build one of the two branches in our networks which aims to obtain
better multi-scale features. Extensive experiments on Cityscapes and CamVid
dataset show the effectiveness of our method.
</p></li>
</ul>

<h3>Title: Self-supervised Pre-training for Precipitation Post-processor. (arXiv:2310.20187v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.20187">http://arxiv.org/abs/2310.20187</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.20187]] Self-supervised Pre-training for Precipitation Post-processor(http://arxiv.org/abs/2310.20187)</code></li>
<li>Summary: <p>Securing sufficient forecast lead time for local precipitation is essential
for preventing hazardous weather events. Nonetheless, global warming-induced
climate change is adding to the challenge of accurately predicting severe
precipitation events, such as heavy rainfall. In this work, we propose a deep
learning-based precipitation post-processor approach to numerical weather
prediction (NWP) models. The precipitation post-processor consists of (i)
self-supervised pre-training, where parameters of encoder are pre-trained on
the reconstruction of masked variables of the atmospheric physics domain, and
(ii) transfer learning on precipitation segmentation tasks (target domain) from
the pre-trained encoder. We also introduce a heuristic labeling approach for
effectively training class-imbalanced datasets. Our experiment results in
precipitation correction for regional NWP show that the proposed method
outperforms other approaches.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
