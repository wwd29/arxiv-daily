<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: A Dependable Hybrid Machine Learning Model for Network Intrusion Detection. (arXiv:2212.04546v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04546">http://arxiv.org/abs/2212.04546</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04546] A Dependable Hybrid Machine Learning Model for Network Intrusion Detection](http://arxiv.org/abs/2212.04546) #security</code></li>
<li>Summary: <p>Network intrusion detection systems (NIDSs) play an important role in
computer network security. There are several detection mechanisms where
anomaly-based automated detection outperforms others significantly. Amid the
sophistication and growing number of attacks, dealing with large amounts of
data is a recognized issue in the development of anomaly-based NIDS. However,
do current models meet the needs of today's networks in terms of required
accuracy and dependability? In this research, we propose a new hybrid model
that combines machine learning and deep learning to increase detection rates
while securing dependability. Our proposed method ensures efficient
pre-processing by combining SMOTE for data balancing and XGBoost for feature
selection. We compared our developed method to various machine learning and
deep learning algorithms to find a more efficient algorithm to implement in the
pipeline. Furthermore, we chose the most effective model for network intrusion
based on a set of benchmarked performance analysis criteria. Our method
produces excellent results when tested on two datasets, KDDCUP'99 and
CIC-MalMem-2022, with an accuracy of 99.99% and 100% for KDDCUP'99 and
CIC-MalMem-2022, respectively, and no overfitting or Type-1 and Type-2 issues.
</p></li>
</ul>

<h3>Title: An SLR on Edge Computing Security and possible threat protection. (arXiv:2212.04563v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04563">http://arxiv.org/abs/2212.04563</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04563] An SLR on Edge Computing Security and possible threat protection](http://arxiv.org/abs/2212.04563) #security</code></li>
<li>Summary: <p>Mobile and Internet of Things devices are generating enormous amounts of
multi-modal data due to their exponential growth and accessibility. As a
result, these data sources must be directly analyzed in real time at the
network edge rather than relying on the cloud. Significant processing power at
the network's edge has made it possible to gather data and make decisions prior
to data being sent to the cloud. Moreover, security problems have significantly
towered as a result of the rapid expansion of mobile devices, Internet of
Things (IoT) devices, and various network points. It's much harder than ever to
guarantee the privacy of sensitive data, including customer information. This
systematic literature review depicts the fact that new technologies are a great
weapon to fight with the attack and threats to the edge computing security.
</p></li>
</ul>

<h3>Title: CopAS: A Big Data Forensic Analytics System. (arXiv:2212.04843v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04843">http://arxiv.org/abs/2212.04843</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04843] CopAS: A Big Data Forensic Analytics System](http://arxiv.org/abs/2212.04843) #security</code></li>
<li>Summary: <p>With the advancing digitization of our society, network security has become
one of the critical concerns for most organizations. In this paper, we present
CopAS, a system targeted at Big Data forensics analysis, allowing network
operators to comfortably analyze and correlate large amounts of network data to
get insights about potentially malicious and suspicious events. We demonstrate
the practical usage of CopAS for insider threat detection on a publicly
available PCAP dataset and show how the system can be used to detect insiders
hiding their malicious activity in the large amounts of networking data streams
generated during the daily activities of an organization.
</p></li>
</ul>

<h3>Title: A Comparative Performance Analysis of Explainable Machine Learning Models With And Without RFECV Feature Selection Technique Towards Ransomware Classification. (arXiv:2212.04864v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04864">http://arxiv.org/abs/2212.04864</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04864] A Comparative Performance Analysis of Explainable Machine Learning Models With And Without RFECV Feature Selection Technique Towards Ransomware Classification](http://arxiv.org/abs/2212.04864) #security</code></li>
<li>Summary: <p>Ransomware has emerged as one of the major global threats in recent days. The
alarming increasing rate of ransomware attacks and new ransomware variants
intrigue the researchers in this domain to constantly examine the
distinguishing traits of ransomware and refine their detection or
classification strategies. Among the broad range of different behavioral
characteristics, the trait of Application Programming Interface (API) calls and
network behaviors have been widely utilized as differentiating factors for
ransomware detection, or classification. Although many of the prior approaches
have shown promising results in detecting and classifying ransomware families
utilizing these features without applying any feature selection techniques,
feature selection, however, is one of the potential steps toward an efficient
detection or classification Machine Learning model because it reduces the
probability of overfitting by removing redundant data, improves the model's
accuracy by eliminating irrelevant features, and therefore reduces training
time. There have been a good number of feature selection techniques to date
that are being used in different security scenarios to optimize the performance
of the Machine Learning models. Hence, the aim of this study is to present the
comparative performance analysis of widely utilized Supervised Machine Learning
models with and without RFECV feature selection technique towards ransomware
classification utilizing the API call and network traffic features. Thereby,
this study provides insight into the efficiency of the RFECV feature selection
technique in the case of ransomware classification which can be used by peers
as a reference for future work in choosing the feature selection technique in
this domain.
</p></li>
</ul>

<h3>Title: A systematic literature review on Security of Unmanned Aerial Vehicle Systems. (arXiv:2212.05028v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.05028">http://arxiv.org/abs/2212.05028</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.05028] A systematic literature review on Security of Unmanned Aerial Vehicle Systems](http://arxiv.org/abs/2212.05028) #security</code></li>
<li>Summary: <p>Unmanned aerial vehicles (UAVs) are becoming more common, and their
operational range is expanding tremendously, making the security aspect of the
inquiry essential. This study does a thorough assessment of the literature to
determine the most common cyberattacks and the effects they have on UAV
assaults on civilian targets. The STRIDE assault paradigm, the challenge they
present, and the proper tools for the attack are used to categorize the cyber
dangers discussed in this paper. Spoofing and denial of service assaults are
the most prevalent types of UAV cyberattacks and have the best results. No
attack style demands the employment of a hard-to-reach gadget, indicating that
the security environment currently necessitates improvements to UAV use in
civilian applications.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Genie: Show Me the Data for Quantization. (arXiv:2212.04780v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04780">http://arxiv.org/abs/2212.04780</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04780] Genie: Show Me the Data for Quantization](http://arxiv.org/abs/2212.04780) #privacy</code></li>
<li>Summary: <p>Zero-shot quantization is a promising approach for developing lightweight
deep neural networks when data is inaccessible owing to various reasons,
including cost and issues related to privacy. By utilizing the learned
parameters (statistics) of FP32-pre-trained models, zero-shot quantization
schemes focus on generating synthetic data by minimizing the distance between
the learned parameters ($\mu$ and $\sigma$) and distributions of intermediate
activations. Subsequently, they distill knowledge from the pre-trained model
(\textit{teacher}) to the quantized model (\textit{student}) such that the
quantized model can be optimized with the synthetic dataset. In general,
zero-shot quantization comprises two major elements: synthesizing datasets and
quantizing models. However, thus far, zero-shot quantization has primarily been
discussed in the context of quantization-aware training methods, which require
task-specific losses and long-term optimization as much as retraining. We thus
introduce a post-training quantization scheme for zero-shot quantization that
produces high-quality quantized networks within a few hours on even half an
hour. Furthermore, we propose a framework called \genie~that generates data
suited for post-training quantization. With the data synthesized by \genie, we
can produce high-quality quantized models without real datasets, which is
comparable to few-shot quantization. We also propose a post-training
quantization algorithm to enhance the performance of quantized models. By
combining them, we can bridge the gap between zero-shot and few-shot
quantization while significantly improving the quantization performance
compared to that of existing approaches. In other words, we can obtain a unique
state-of-the-art zero-shot quantization approach.
</p></li>
</ul>

<h3>Title: PIVOT: Prompting for Video Continual Learning. (arXiv:2212.04842v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04842">http://arxiv.org/abs/2212.04842</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04842] PIVOT: Prompting for Video Continual Learning](http://arxiv.org/abs/2212.04842) #privacy</code></li>
<li>Summary: <p>Modern machine learning pipelines are limited due to data availability,
storage quotas, privacy regulations, and expensive annotation processes. These
constraints make it difficult or impossible to maintain a large-scale model
trained on growing annotation sets. Continual learning directly approaches this
problem, with the ultimate goal of devising methods where a neural network
effectively learns relevant patterns for new (unseen) classes without
significantly altering its performance on previously learned ones. In this
paper, we address the problem of continual learning for video data. We
introduce PIVOT, a novel method that leverages the extensive knowledge in
pre-trained models from the image domain, thereby reducing the number of
trainable parameters and the associated forgetting. Unlike previous methods,
ours is the first approach that effectively uses prompting mechanisms for
continual learning without any in-domain pre-training. Our experiments show
that PIVOT improves state-of-the-art methods by a significant 27% on the
20-task ActivityNet setup.
</p></li>
</ul>

<h3>Title: A systematic literature review on Virtual Reality and Augmented Reality in terms of privacy, authorization and data-leaks. (arXiv:2212.04621v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04621">http://arxiv.org/abs/2212.04621</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04621] A systematic literature review on Virtual Reality and Augmented Reality in terms of privacy, authorization and data-leaks](http://arxiv.org/abs/2212.04621) #privacy</code></li>
<li>Summary: <p>In recent years, VR and AR has exploded into a multimillionaire market. As
this emerging technology has spread to a variety of businesses and is rapidly
increasing among users. It is critical to address potential privacy and
security concerns that these technologies might pose. In this study, we discuss
the current status of privacy and security in VR and AR. We analyse possible
problems and risks. Besides, we will look in detail at a few of the major
concerns issues and related security solutions for AR and VR. Additionally, as
VR and AR authentication is the most thoroughly studied aspect of the problem,
we concentrate on the research that has already been done in this area.
</p></li>
</ul>

<h3>Title: Near-Optimal Differentially Private Reinforcement Learning. (arXiv:2212.04680v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04680">http://arxiv.org/abs/2212.04680</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04680] Near-Optimal Differentially Private Reinforcement Learning](http://arxiv.org/abs/2212.04680) #privacy</code></li>
<li>Summary: <p>Motivated by personalized healthcare and other applications involving
sensitive data, we study online exploration in reinforcement learning with
differential privacy (DP) constraints. Existing work on this problem
established that no-regret learning is possible under joint differential
privacy (JDP) and local differential privacy (LDP) but did not provide an
algorithm with optimal regret. We close this gap for the JDP case by designing
an $\epsilon$-JDP algorithm with a regret of
$\widetilde{O}(\sqrt{SAH^2T}+S^2AH^3/\epsilon)$ which matches the
information-theoretic lower bound of non-private learning for all choices of
$\epsilon> S^{1.5}A^{0.5} H^2/\sqrt{T}$. In the above, $S$, $A$ denote the
number of states and actions, $H$ denotes the planning horizon, and $T$ is the
number of steps. To the best of our knowledge, this is the first private RL
algorithm that achieves \emph{privacy for free} asymptotically as $T\rightarrow
\infty$. Our techniques -- which could be of independent interest -- include
privately releasing Bernstein-type exploration bonuses and an improved method
for releasing visitation statistics. The same techniques also imply a slightly
improved regret bound for the LDP case.
</p></li>
</ul>

<h3>Title: Lower Bounds for R\'enyi Differential Privacy in a Black-Box Setting. (arXiv:2212.04739v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04739">http://arxiv.org/abs/2212.04739</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04739] Lower Bounds for R\'enyi Differential Privacy in a Black-Box Setting](http://arxiv.org/abs/2212.04739) #privacy</code></li>
<li>Summary: <p>We present new methods for assessing the privacy guarantees of an algorithm
with regard to R\'enyi Differential Privacy. To the best of our knowledge, this
work is the first to address this problem in a black-box scenario, where only
algorithmic outputs are available. To quantify privacy leakage, we devise a new
estimator for the R\'enyi divergence of a pair of output distributions. This
estimator is transformed into a statistical lower bound that is proven to hold
for large samples with high probability. Our method is applicable for a broad
class of algorithms, including many well-known examples from the privacy
literature. We demonstrate the effectiveness of our approach by experiments
encompassing algorithms and privacy enhancing methods that have not been
considered in related works.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Visual Detection of Personal Protective Equipment and Safety Gear on Industry Workers. (arXiv:2212.04794v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04794">http://arxiv.org/abs/2212.04794</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04794] Visual Detection of Personal Protective Equipment and Safety Gear on Industry Workers](http://arxiv.org/abs/2212.04794) #protect</code></li>
<li>Summary: <p>Workplace injuries are common in today's society due to a lack of adequately
worn safety equipment. A system that only admits appropriately equipped
personnel can be created to improve working conditions. The goal is thus to
develop a system that will improve workers' safety using a camera that will
detect the usage of Personal Protective Equipment (PPE). To this end, we
collected and labeled appropriate data from several public sources, which have
been used to train and evaluate several models based on the popular YOLOv4
object detector. Our focus, driven by a collaborating industrial partner, is to
implement our system into an entry control point where workers must present
themselves to obtain access to a restricted area. Combined with facial identity
recognition, the system would ensure that only authorized people wearing
appropriate equipment are granted access. A novelty of this work is that we
increase the number of classes to five objects (hardhat, safety vest, safety
gloves, safety glasses, and hearing protection), whereas most existing works
only focus on one or two classes, usually hardhats or vests. The AI model
developed provides good detection accuracy at a distance of 3 and 5 meters in
the collaborative environment where we aim at operating (mAP of 99/89%,
respectively). The small size of some objects or the potential occlusion by
body parts have been identified as potential factors that are detrimental to
accuracy, which we have counteracted via data augmentation and cropping of the
body before applying PPE detection.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models. (arXiv:2212.04687v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04687">http://arxiv.org/abs/2212.04687</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04687] Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models](http://arxiv.org/abs/2212.04687) #attack</code></li>
<li>Summary: <p>In this paper, we present a simple yet surprisingly effective technique to
induce "selective amnesia" on a backdoored model. Our approach, called SEAM,
has been inspired by the problem of catastrophic forgetting (CF), a long
standing issue in continual learning. Our idea is to retrain a given DNN model
on randomly labeled clean data, to induce a CF on the model, leading to a
sudden forget on both primary and backdoor tasks; then we recover the primary
task by retraining the randomized model on correctly labeled clean data. We
analyzed SEAM by modeling the unlearning process as continual learning and
further approximating a DNN using Neural Tangent Kernel for measuring CF. Our
analysis shows that our random-labeling approach actually maximizes the CF on
an unknown backdoor in the absence of triggered inputs, and also preserves some
feature extraction in the network to enable a fast revival of the primary task.
We further evaluated SEAM on both image processing and Natural Language
Processing tasks, under both data contamination and training manipulation
attacks, over thousands of models either trained on popular image datasets or
provided by the TrojAI competition. Our experiments show that SEAM vastly
outperforms the state-of-the-art unlearning techniques, achieving a high
Fidelity (measuring the gap between the accuracy of the primary task and that
of the backdoor) within a few minutes (about 30 times faster than training a
model from scratch using the MNIST dataset), with only a small amount of clean
data (0.1% of training data for TrojAI models).
</p></li>
</ul>

<h3>Title: Expeditious Saliency-guided Mix-up through Random Gradient Thresholding. (arXiv:2212.04875v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04875">http://arxiv.org/abs/2212.04875</a></li>
<li>Code URL: <a href="https://github.com/minhlong94/random-mixup">https://github.com/minhlong94/random-mixup</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04875] Expeditious Saliency-guided Mix-up through Random Gradient Thresholding](http://arxiv.org/abs/2212.04875) #attack</code></li>
<li>Summary: <p>Mix-up training approaches have proven to be effective in improving the
generalization ability of Deep Neural Networks. Over the years, the research
community expands mix-up methods into two directions, with extensive efforts to
improve saliency-guided procedures but minimal focus on the arbitrary path,
leaving the randomization domain unexplored. In this paper, inspired by the
superior qualities of each direction over one another, we introduce a novel
method that lies at the junction of the two routes. By combining the best
elements of randomness and saliency utilization, our method balances speed,
simplicity, and accuracy. We name our method R-Mix following the concept of
"Random Mix-up". We demonstrate its effectiveness in generalization, weakly
supervised object localization, calibration, and robustness to adversarial
attacks. Finally, in order to address the question of whether there exists a
better decision protocol, we train a Reinforcement Learning agent that decides
the mix-up policies based on the classifier's performance, reducing dependency
on human-designed objectives and hyperparameter tuning. Extensive experiments
further show that the agent is capable of performing at the cutting-edge level,
laying the foundation for a fully automatic mix-up. Our code is released at
[https://github.com/minhlong94/Random-Mixup].
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization. (arXiv:2212.04575v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04575">http://arxiv.org/abs/2212.04575</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04575] DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization](http://arxiv.org/abs/2212.04575) #robust</code></li>
<li>Summary: <p>In this paper, we propose an end-to-end framework that jointly learns
keypoint detection, descriptor representation and cross-frame matching for the
task of image-based 3D localization. Prior art has tackled each of these
components individually, purportedly aiming to alleviate difficulties in
effectively train a holistic network. We design a self-supervised image warping
correspondence loss for both feature detection and matching, a
weakly-supervised epipolar constraints loss on relative camera pose learning,
and a directional matching scheme that detects key-point features in a source
image and performs coarse-to-fine correspondence search on the target image. We
leverage this framework to enforce cycle consistency in our matching module. In
addition, we propose a new loss to robustly handle both definite inlier/outlier
matches and less-certain matches. The integration of these learning mechanisms
enables end-to-end training of a single network performing all three
localization components. Bench-marking our approach on public data-sets,
exemplifies how such an end-to-end framework is able to yield more accurate
localization that out-performs both traditional methods as well as
state-of-the-art weakly supervised methods.
</p></li>
</ul>

<h3>Title: Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection. (arXiv:2212.04613v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04613">http://arxiv.org/abs/2212.04613</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04613] Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection](http://arxiv.org/abs/2212.04613) #robust</code></li>
<li>Summary: <p>Contrastive learning has emerged as a competitive pretraining method for
object detection. Despite this progress, there has been minimal investigation
into the robustness of contrastively pretrained detectors when faced with
domain shifts. To address this gap, we conduct an empirical study of
contrastive learning and out-of-domain object detection, studying how
contrastive view design affects robustness. In particular, we perform a case
study of the detection-focused pretext task Instance Localization (InsLoc) and
propose strategies to augment views and enhance robustness in
appearance-shifted and context-shifted scenarios. Amongst these strategies, we
propose changes to cropping such as altering the percentage used, adding IoU
constraints, and integrating saliency based object priors. We also explore the
addition of shortcut-reducing augmentations such as Poisson blending, texture
flattening, and elastic deformation. We benchmark these strategies on abstract,
weather, and context domain shifts and illustrate robust ways to combine them,
in both pretraining on single-object and multi-object image datasets. Overall,
our results and insights show how to ensure robustness through the choice of
views in contrastive learning.
</p></li>
</ul>

<h3>Title: AugNet: Dynamic Test-Time Augmentation via Differentiable Functions. (arXiv:2212.04681v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04681">http://arxiv.org/abs/2212.04681</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04681] AugNet: Dynamic Test-Time Augmentation via Differentiable Functions](http://arxiv.org/abs/2212.04681) #robust</code></li>
<li>Summary: <p>Distribution shifts, which often occur in the real world, degrade the
accuracy of deep learning systems, and thus improving robustness is essential
for practical applications. To improve robustness, we study an image
enhancement method that generates recognition-friendly images without
retraining the recognition model. We propose a novel image enhancement method,
AugNet, which is based on differentiable data augmentation techniques and
generates a blended image from many augmented images to improve the recognition
accuracy under distribution shifts. In addition to standard data augmentations,
AugNet can also incorporate deep neural network-based image transformation,
which further improves the robustness. Because AugNet is composed of
differentiable functions, AugNet can be directly trained with the
classification loss of the recognition model. AugNet is evaluated on widely
used image recognition datasets using various classification models, including
Vision Transformer and MLP-Mixer. AugNet improves the robustness with almost no
reduction in classification accuracy for clean images, which is a better result
than the existing methods. Furthermore, we show that interpretation of
distribution shifts using AugNet and retraining based on that interpretation
can greatly improve robustness.
</p></li>
</ul>

<h3>Title: SLAM for Visually Impaired People: A Survey. (arXiv:2212.04745v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04745">http://arxiv.org/abs/2212.04745</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04745] SLAM for Visually Impaired People: A Survey](http://arxiv.org/abs/2212.04745) #robust</code></li>
<li>Summary: <p>In recent decades, several assistive technologies for visually impaired and
blind (VIB) people have been developed to improve their ability to navigate
independently and safely. At the same time, simultaneous localization and
mapping (SLAM) techniques have become sufficiently robust and efficient to be
adopted in the development of assistive technologies. In this paper, we first
report the results of an anonymous survey conducted with VIB people to
understand their experience and needs; we focus on digital assistive
technologies that help them with indoor and outdoor navigation. Then, we
present a literature review of assistive technologies based on SLAM. We discuss
proposed approaches and indicate their pros and cons. We conclude by presenting
future opportunities and challenges in this domain.
</p></li>
</ul>

<h3>Title: CEPHA29: Automatic Cephalometric Landmark Detection Challenge 2023. (arXiv:2212.04808v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04808">http://arxiv.org/abs/2212.04808</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04808] CEPHA29: Automatic Cephalometric Landmark Detection Challenge 2023](http://arxiv.org/abs/2212.04808) #robust</code></li>
<li>Summary: <p>Quantitative cephalometric analysis is the most widely used clinical and
research tool in modern orthodontics. Accurate localization of cephalometric
landmarks enables the quantification and classification of anatomical
abnormalities, however, the traditional manual way of marking these landmarks
is a very tedious job. Endeavours have constantly been made to develop
automated cephalometric landmark detection systems but they are inadequate for
orthodontic applications. The fundamental reason for this is that the amount of
publicly available datasets as well as the images provided for training in
these datasets are insufficient for an AI model to perform well. To facilitate
the development of robust AI solutions for morphometric analysis, we organise
the CEPHA29 Automatic Cephalometric Landmark Detection Challenge in conjunction
with IEEE International Symposium on Biomedical Imaging (ISBI 2023). In this
context, we provide the largest known publicly available dataset, consisting of
1000 cephalometric X-ray images. We hope that our challenge will not only
derive forward research and innovation in automatic cephalometric landmark
identification but will also signal the beginning of a new era in the
discipline.
</p></li>
</ul>

<h3>Title: Robust Graph Representation Learning via Predictive Coding. (arXiv:2212.04656v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04656">http://arxiv.org/abs/2212.04656</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04656] Robust Graph Representation Learning via Predictive Coding](http://arxiv.org/abs/2212.04656) #robust</code></li>
<li>Summary: <p>Predictive coding is a message-passing framework initially developed to model
information processing in the brain, and now also topic of research in machine
learning due to some interesting properties. One of such properties is the
natural ability of generative models to learn robust representations thanks to
their peculiar credit assignment rule, that allows neural activities to
converge to a solution before updating the synaptic weights. Graph neural
networks are also message-passing models, which have recently shown outstanding
results in diverse types of tasks in machine learning, providing
interdisciplinary state-of-the-art performance on structured data. However,
they are vulnerable to imperceptible adversarial attacks, and unfit for
out-of-distribution generalization. In this work, we address this by building
models that have the same structure of popular graph neural network
architectures, but rely on the message-passing rule of predictive coding.
Through an extensive set of experiments, we show that the proposed models are
(i) comparable to standard ones in terms of performance in both inductive and
transductive tasks, (ii) better calibrated, and (iii) robust against multiple
kinds of adversarial attacks.
</p></li>
</ul>

<h3>Title: Machine Learning Framework: Competitive Intelligence and Key Drivers Identification of Market Share Trends Among Healthcare Facilities. (arXiv:2212.04810v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04810">http://arxiv.org/abs/2212.04810</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04810] Machine Learning Framework: Competitive Intelligence and Key Drivers Identification of Market Share Trends Among Healthcare Facilities](http://arxiv.org/abs/2212.04810) #robust</code></li>
<li>Summary: <p>The necessity of data driven decisions in healthcare strategy formulation is
rapidly increasing. A reliable framework which helps identify factors impacting
a Healthcare Provider Facility or a Hospital (from here on termed as Facility)
Market Share is of key importance. This pilot study aims at developing a data
driven Machine Learning - Regression framework which aids strategists in
formulating key decisions to improve the Facilitys Market Share which in turn
impacts in improving the quality of healthcare services. The US (United States)
healthcare business is chosen for the study; and the data spanning across 60
key Facilities in Washington State and about 3 years of historical data is
considered. In the current analysis Market Share is termed as the ratio of
facility encounters to the total encounters among the group of potential
competitor facilities. The current study proposes a novel two-pronged approach
of competitor identification and regression approach to evaluate and predict
market share, respectively. Leveraged model agnostic technique, SHAP, to
quantify the relative importance of features impacting the market share. The
proposed method to identify pool of competitors in current analysis, develops
Directed Acyclic Graphs (DAGs), feature level word vectors and evaluates the
key connected components at facility level. This technique is robust since its
data driven which minimizes the bias from empirical techniques. Post
identifying the set of competitors among facilities, developed Regression model
to predict the Market share. For relative quantification of features at a
facility level, incorporated SHAP a model agnostic explainer. This helped to
identify and rank the attributes at each facility which impacts the market
share.
</p></li>
</ul>

<h3>Title: Predictor networks and stop-grads provide implicit variance regularization in BYOL/SimSiam. (arXiv:2212.04858v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04858">http://arxiv.org/abs/2212.04858</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04858] Predictor networks and stop-grads provide implicit variance regularization in BYOL/SimSiam](http://arxiv.org/abs/2212.04858) #robust</code></li>
<li>Summary: <p>Self-supervised learning (SSL) learns useful representations from unlabelled
data by training networks to be invariant to pairs of augmented versions of the
same input. Non-contrastive methods avoid collapse either by directly
regularizing the covariance matrix of network outputs or through asymmetric
loss architectures, two seemingly unrelated approaches. Here, by building on
DirectPred, we lay out a theoretical framework that reconciles these two views.
We derive analytical expressions for the representational learning dynamics in
linear networks. By expressing them in the eigenspace of the embedding
covariance matrix, where the solutions decouple, we reveal the mechanism and
conditions that provide implicit variance regularization. These insights allow
us to formulate a new isotropic loss function that equalizes eigenvalue
contribution and renders learning more robust. Finally, we show empirically
that our findings translate to nonlinear networks trained on CIFAR-10 and
STL-10.
</p></li>
</ul>

<h3>Title: PDE-LEARN: Using Deep Learning to Discover Partial Differential Equations from Noisy, Limited Data. (arXiv:2212.04971v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04971">http://arxiv.org/abs/2212.04971</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04971] PDE-LEARN: Using Deep Learning to Discover Partial Differential Equations from Noisy, Limited Data](http://arxiv.org/abs/2212.04971) #robust</code></li>
<li>Summary: <p>In this paper, we introduce PDE-LEARN, a novel PDE discovery algorithm that
can identify governing partial differential equations (PDEs) directly from
noisy, limited measurements of a physical system of interest. PDE-LEARN uses a
Rational Neural Network, $U$, to approximate the system response function and a
sparse, trainable vector, $\xi$, to characterize the hidden PDE that the system
response function satisfies. Our approach couples the training of $U$ and $\xi$
using a loss function that (1) makes $U$ approximate the system response
function, (2) encapsulates the fact that $U$ satisfies a hidden PDE that $\xi$
characterizes, and (3) promotes sparsity in $\xi$ using ideas from iteratively
reweighted least-squares. Further, PDE-LEARN can simultaneously learn from
several data sets, allowing it to incorporate results from multiple
experiments. This approach yields a robust algorithm to discover PDEs directly
from realistic scientific data. We demonstrate the efficacy of PDE-LEARN by
identifying several PDEs from noisy and limited measurements.
</p></li>
</ul>

<h3>Title: Adversarial Weight Perturbation Improves Generalization in Graph Neural Network. (arXiv:2212.04983v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04983">http://arxiv.org/abs/2212.04983</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04983] Adversarial Weight Perturbation Improves Generalization in Graph Neural Network](http://arxiv.org/abs/2212.04983) #robust</code></li>
<li>Summary: <p>A lot of theoretical and empirical evidence shows that the flatter local
minima tend to improve generalization. Adversarial Weight Perturbation (AWP) is
an emerging technique to efficiently and effectively find such minima. In AWP
we minimize the loss w.r.t. a bounded worst-case perturbation of the model
parameters thereby favoring local minima with a small loss in a neighborhood
around them. The benefits of AWP, and more generally the connections between
flatness and generalization, have been extensively studied for i.i.d. data such
as images. In this paper, we extensively study this phenomenon for graph data.
Along the way, we first derive a generalization bound for non-i.i.d. node
classification tasks. Then we identify a vanishing-gradient issue with all
existing formulations of AWP and we propose a new Weighted Truncated AWP
(WT-AWP) to alleviate this issue. We show that regularizing graph neural
networks with WT-AWP consistently improves both natural and robust
generalization across many different graph learning tasks and models.
</p></li>
</ul>

<h3>Title: Understanding and Combating Robust Overfitting via Input Loss Landscape Analysis and Regularization. (arXiv:2212.04985v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04985">http://arxiv.org/abs/2212.04985</a></li>
<li>Code URL: <a href="https://github.com/treelli/combating-ro-advlc">https://github.com/treelli/combating-ro-advlc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04985] Understanding and Combating Robust Overfitting via Input Loss Landscape Analysis and Regularization](http://arxiv.org/abs/2212.04985) #robust</code></li>
<li>Summary: <p>Adversarial training is widely used to improve the robustness of deep neural
networks to adversarial attack. However, adversarial training is prone to
overfitting, and the cause is far from clear. This work sheds light on the
mechanisms underlying overfitting through analyzing the loss landscape w.r.t.
the input. We find that robust overfitting results from standard training,
specifically the minimization of the clean loss, and can be mitigated by
regularization of the loss gradients. Moreover, we find that robust overfitting
turns severer during adversarial training partially because the gradient
regularization effect of adversarial training becomes weaker due to the
increase in the loss landscapes curvature. To improve robust generalization, we
propose a new regularizer to smooth the loss landscape by penalizing the
weighted logits variation along the adversarial direction. Our method
significantly mitigates robust overfitting and achieves the highest robustness
and efficiency compared to similar previous methods. Code is available at
https://github.com/TreeLLi/Combating-RO-AdvLC.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: From Clozing to Comprehending: Retrofitting Pre-trained Language Model to Pre-trained Machine Reader. (arXiv:2212.04755v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04755">http://arxiv.org/abs/2212.04755</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04755] From Clozing to Comprehending: Retrofitting Pre-trained Language Model to Pre-trained Machine Reader](http://arxiv.org/abs/2212.04755) #extraction</code></li>
<li>Summary: <p>We present Pre-trained Machine Reader (PMR), a novel method to retrofit
Pre-trained Language Models (PLMs) into Machine Reading Comprehension (MRC)
models without acquiring labeled data. PMR is capable of resolving the
discrepancy between model pre-training and downstream fine-tuning of existing
PLMs, and provides a unified solver for tackling various extraction tasks. To
achieve this, we construct a large volume of general-purpose and high-quality
MRC-style training data with the help of Wikipedia hyperlinks and design a Wiki
Anchor Extraction task to guide the MRC-style pre-training process. Although
conceptually simple, PMR is particularly effective in solving extraction tasks
including Extractive Question Answering and Named Entity Recognition, where it
shows tremendous improvements over previous approaches especially under
low-resource settings. Moreover, viewing sequence classification task as a
special case of extraction task in our MRC formulation, PMR is even capable to
extract high-quality rationales to explain the classification process,
providing more explainability of the predictions.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h2>fair</h2>
<h3>Title: Towards Understanding Fairness and its Composition in Ensemble Machine Learning. (arXiv:2212.04593v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04593">http://arxiv.org/abs/2212.04593</a></li>
<li>Code URL: <a href="https://github.com/usmangohar/fairensemble">https://github.com/usmangohar/fairensemble</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04593] Towards Understanding Fairness and its Composition in Ensemble Machine Learning](http://arxiv.org/abs/2212.04593) #fair</code></li>
<li>Summary: <p>Machine Learning (ML) software has been widely adopted in modern society,
with reported fairness implications for minority groups based on race, sex,
age, etc. Many recent works have proposed methods to measure and mitigate
algorithmic bias in ML models. The existing approaches focus on single
classifier-based ML models. However, real-world ML models are often composed of
multiple independent or dependent learners in an ensemble (e.g., Random
Forest), where the fairness composes in a non-trivial way. How does fairness
compose in ensembles? What are the fairness impacts of the learners on the
ultimate fairness of the ensemble? Can fair learners result in an unfair
ensemble? Furthermore, studies have shown that hyperparameters influence the
fairness of ML models. Ensemble hyperparameters are more complex since they
affect how learners are combined in different categories of ensembles.
Understanding the impact of ensemble hyperparameters on fairness will help
programmers design fair ensembles. Today, we do not understand these fully for
different ensemble algorithms. In this paper, we comprehensively study popular
real-world ensembles: bagging, boosting, stacking and voting. We have developed
a benchmark of 168 ensemble models collected from Kaggle on four popular
fairness datasets. We use existing fairness metrics to understand the
composition of fairness. Our results show that ensembles can be designed to be
fairer without using mitigation techniques. We also identify the interplay
between fairness composition and data characteristics to guide fair ensemble
design. Finally, our benchmark can be leveraged for further research on fair
ensembles. To the best of our knowledge, this is one of the first and largest
studies on fairness composition in ensembles yet presented in the literature.
</p></li>
</ul>

<h3>Title: System Design for an Integrated Lifelong Reinforcement Learning Agent for Real-Time Strategy Games. (arXiv:2212.04603v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04603">http://arxiv.org/abs/2212.04603</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04603] System Design for an Integrated Lifelong Reinforcement Learning Agent for Real-Time Strategy Games](http://arxiv.org/abs/2212.04603) #fair</code></li>
<li>Summary: <p>As Artificial and Robotic Systems are increasingly deployed and relied upon
for real-world applications, it is important that they exhibit the ability to
continually learn and adapt in dynamically-changing environments, becoming
Lifelong Learning Machines. Continual/lifelong learning (LL) involves
minimizing catastrophic forgetting of old tasks while maximizing a model's
capability to learn new tasks. This paper addresses the challenging lifelong
reinforcement learning (L2RL) setting. Pushing the state-of-the-art forward in
L2RL and making L2RL useful for practical applications requires more than
developing individual L2RL algorithms; it requires making progress at the
systems-level, especially research into the non-trivial problem of how to
integrate multiple L2RL algorithms into a common framework. In this paper, we
introduce the Lifelong Reinforcement Learning Components Framework (L2RLCF),
which standardizes L2RL systems and assimilates different continual learning
components (each addressing different aspects of the lifelong learning problem)
into a unified system. As an instantiation of L2RLCF, we develop a standard API
allowing easy integration of novel lifelong learning components. We describe a
case study that demonstrates how multiple independently-developed LL components
can be integrated into a single realized system. We also introduce an
evaluation environment in order to measure the effect of combining various
system components. Our evaluation environment employs different LL scenarios
(sequences of tasks) consisting of Starcraft-2 minigames and allows for the
fair, comprehensive, and quantitative comparison of different combinations of
components within a challenging common evaluation environment.
</p></li>
</ul>

<h3>Title: Digital Twin for Real-time Li-ion Battery State of Health Estimation with Partially Discharged Cycling Data. (arXiv:2212.04622v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04622">http://arxiv.org/abs/2212.04622</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04622] Digital Twin for Real-time Li-ion Battery State of Health Estimation with Partially Discharged Cycling Data](http://arxiv.org/abs/2212.04622) #fair</code></li>
<li>Summary: <p>To meet the fairly high safety and reliability requirements in practice, the
state of health (SOH) estimation of Lithium-ion batteries (LIBs), which has a
close relationship with the degradation performance, has been extensively
studied with the widespread applications of various electronics. The
conventional SOH estimation approaches with digital twin are end-of-cycle
estimation that require the completion of a full charge/discharge cycle to
observe the maximum available capacity. However, under dynamic operating
conditions with partially discharged data, it is impossible to sense accurate
real-time SOH estimation for LIBs. To bridge this research gap, we put forward
a digital twin framework to gain the capability of sensing the battery's SOH on
the fly, updating the physical battery model. The proposed digital twin
solution consists of three core components to enable real-time SOH estimation
without requiring a complete discharge. First, to handle the variable training
cycling data, the energy discrepancy-aware cycling synchronization is proposed
to align cycling data with guaranteeing the same data structure. Second, to
explore the temporal importance of different training sampling times, a
time-attention SOH estimation model is developed with data encoding to capture
the degradation behavior over cycles, excluding adverse influences of
unimportant samples. Finally, for online implementation, a similarity
analysis-based data reconstruction has been put forward to provide real-time
SOH estimation without requiring a full discharge cycle. Through a series of
results conducted on a widely used benchmark, the proposed method yields the
real-time SOH estimation with errors less than 1% for most sampling times in
ongoing cycles.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>explainability</h2>
<h2>watermark</h2>
<h3>Title: A Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others. (arXiv:2212.04825v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04825">http://arxiv.org/abs/2212.04825</a></li>
<li>Code URL: <a href="https://github.com/facebookresearch/Whac-A-Mole">https://github.com/facebookresearch/Whac-A-Mole</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04825] A Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others](http://arxiv.org/abs/2212.04825) #watermark</code></li>
<li>Summary: <p>Machine learning models have been found to learn shortcuts -- unintended
decision rules that are unable to generalize -- undermining models'
reliability. Previous works address this problem under the tenuous assumption
that only a single shortcut exists in the training data. Real-world images are
rife with multiple visual cues from background to texture. Key to advancing the
reliability of vision systems is understanding whether existing methods can
overcome multiple shortcuts or struggle in a Whac-A-Mole game, i.e., where
mitigating one shortcut amplifies reliance on others. To address this
shortcoming, we propose two benchmarks: 1) UrbanCars, a dataset with precisely
controlled spurious cues, and 2) ImageNet-W, an evaluation set based on
ImageNet for watermark, a shortcut we discovered affects nearly every modern
vision model. Along with texture and background, ImageNet-W allows us to study
multiple shortcuts emerging from training on natural images. We find computer
vision models, including large foundation models -- regardless of training set,
architecture, and supervision -- struggle when multiple shortcuts are present.
Even methods explicitly designed to combat shortcuts struggle in a Whac-A-Mole
dilemma. To tackle this challenge, we propose Last Layer Ensemble, a
simple-yet-effective method to mitigate multiple shortcuts without Whac-A-Mole
behavior. Our results surface multi-shortcut mitigation as an overlooked
challenge critical to advancing the reliability of vision systems. The datasets
and code are released: https://github.com/facebookresearch/Whac-A-Mole.git.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: Ego-Body Pose Estimation via Ego-Head Pose Estimation. (arXiv:2212.04636v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04636">http://arxiv.org/abs/2212.04636</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04636] Ego-Body Pose Estimation via Ego-Head Pose Estimation](http://arxiv.org/abs/2212.04636) #diffusion</code></li>
<li>Summary: <p>Estimating 3D human motion from an egocentric video sequence is critical to
human behavior understanding and applications in VR/AR. However, naively
learning a mapping between egocentric videos and human motions is challenging,
because the user's body is often unobserved by the front-facing camera placed
on the head of the user. In addition, collecting large-scale, high-quality
datasets with paired egocentric videos and 3D human motions requires accurate
motion capture devices, which often limit the variety of scenes in the videos
to lab-like environments. To eliminate the need for paired egocentric video and
human motions, we propose a new method, Ego-Body Pose Estimation via Ego-Head
Pose Estimation (EgoEgo), that decomposes the problem into two stages,
connected by the head motion as an intermediate representation. EgoEgo first
integrates SLAM and a learning approach to estimate accurate head motion. Then,
taking the estimated head pose as input, it leverages conditional diffusion to
generate multiple plausible full-body motions. This disentanglement of head and
body pose eliminates the need for training datasets with paired egocentric
videos and 3D human motion, enabling us to leverage large-scale egocentric
video datasets and motion capture datasets separately. Moreover, for systematic
benchmarking, we develop a synthetic dataset, AMASS-Replica-Ego-Syn (ARES),
with paired egocentric videos and human motion. On both ARES and real data, our
EgoEgo model performs significantly better than the state-of-the-art.
</p></li>
</ul>

<h3>Title: ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow Removal. (arXiv:2212.04711v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.04711">http://arxiv.org/abs/2212.04711</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.04711] ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow Removal](http://arxiv.org/abs/2212.04711) #diffusion</code></li>
<li>Summary: <p>Recent deep learning methods have achieved promising results in image shadow
removal. However, their restored images still suffer from unsatisfactory
boundary artifacts, due to the lack of degradation prior embedding and the
deficiency in modeling capacity. Our work addresses these issues by proposing a
unified diffusion framework that integrates both the image and degradation
priors for highly effective shadow removal. In detail, we first propose a
shadow degradation model, which inspires us to build a novel unrolling
diffusion model, dubbed ShandowDiffusion. It remarkably improves the model's
capacity in shadow removal via progressively refining the desired output with
both degradation prior and diffusive generative prior, which by nature can
serve as a new strong baseline for image restoration. Furthermore,
ShadowDiffusion progressively refines the estimated shadow mask as an auxiliary
task of the diffusion generator, which leads to more accurate and robust
shadow-free image generation. We conduct extensive experiments on three popular
public datasets, including ISTD, ISTD+, and SRD, to validate our method's
effectiveness. Compared to the state-of-the-art methods, our model achieves a
significant improvement in terms of PSNR, increasing from 31.69dB to 34.73dB
over SRD dataset.
</p></li>
</ul>

<h3>Title: Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis. (arXiv:2212.05032v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.05032">http://arxiv.org/abs/2212.05032</a></li>
<li>Code URL: <a href="https://github.com/weixi-feng/structured-diffusion-guidance">https://github.com/weixi-feng/structured-diffusion-guidance</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.05032] Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis](http://arxiv.org/abs/2212.05032) #diffusion</code></li>
<li>Summary: <p>Large-scale diffusion models have achieved state-of-the-art results on
text-to-image synthesis (T2I) tasks. Despite their ability to generate
high-quality yet creative images, we observe that attribution-binding and
compositional capabilities are still considered major challenging issues,
especially when involving multiple objects. In this work, we improve the
compositional skills of T2I models, specifically more accurate attribute
binding and better image compositions. To do this, we incorporate linguistic
structures with the diffusion guidance process based on the controllable
properties of manipulating cross-attention layers in diffusion-based T2I
models. We observe that keys and values in cross-attention layers have strong
semantic meanings associated with object layouts and content. Therefore, we can
better preserve the compositional semantics in the generated image by
manipulating the cross-attention representations based on linguistic insights.
Built upon Stable Diffusion, a SOTA T2I model, our structured cross-attention
design is efficient that requires no additional training samples. We achieve
better compositional skills in qualitative and quantitative results, leading to
a 5-8% advantage in head-to-head user comparison studies. Lastly, we conduct an
in-depth analysis to reveal potential causes of incorrect image compositions
and justify the properties of cross-attention layers in the generation process.
</p></li>
</ul>

<h3>Title: SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model. (arXiv:2212.05034v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.05034">http://arxiv.org/abs/2212.05034</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.05034] SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model](http://arxiv.org/abs/2212.05034) #diffusion</code></li>
<li>Summary: <p>Generic image inpainting aims to complete a corrupted image by borrowing
surrounding information, which barely generates novel content. By contrast,
multi-modal inpainting provides more flexible and useful controls on the
inpainted content, \eg, a text prompt can be used to describe an object with
richer attributes, and a mask can be used to constrain the shape of the
inpainted object rather than being only considered as a missing area. We
propose a new diffusion-based model named SmartBrush for completing a missing
region with an object using both text and shape-guidance. While previous work
such as DALLE-2 and Stable Diffusion can do text-guided inapinting they do not
support shape guidance and tend to modify background texture surrounding the
generated object. Our model incorporates both text and shape guidance with
precision control. To preserve the background better, we propose a novel
training and sampling strategy by augmenting the diffusion U-net with
object-mask prediction. Lastly, we introduce a multi-task training strategy by
jointly training inpainting with text-to-image generation to leverage more
training data. We conduct extensive experiments showing that our model
outperforms all baselines in terms of visual quality, mask controllability, and
background preservation.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
