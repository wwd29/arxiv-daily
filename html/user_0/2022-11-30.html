<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: TF-Net: Deep Learning Empowered Tiny Feature Network for Night-time UAV Detection. (arXiv:2211.16317v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16317">http://arxiv.org/abs/2211.16317</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16317] TF-Net: Deep Learning Empowered Tiny Feature Network for Night-time UAV Detection](http://arxiv.org/abs/2211.16317) #secure</code></li>
<li>Summary: <p>Technological advancements have normalized the usage of unmanned aerial
vehicles (UAVs) in every sector, spanning from military to commercial but they
also pose serious security concerns due to their enhanced functionalities and
easy access to private and highly secured areas. Several instances related to
UAVs have raised security concerns, leading to UAV detection research studies.
Visual techniques are widely adopted for UAV detection, but they perform poorly
at night, in complex backgrounds, and in adverse weather conditions. Therefore,
a robust night vision-based drone detection system is required to that could
efficiently tackle this problem. Infrared cameras are increasingly used for
nighttime surveillance due to their wide applications in night vision
equipment. This paper uses a deep learning-based TinyFeatureNet (TF-Net), which
is an improved version of YOLOv5s, to accurately detect UAVs during the night
using infrared (IR) images. In the proposed TF-Net, we introduce architectural
changes in the neck and backbone of the YOLOv5s. We also simulated four
different YOLOv5 models (s,m,n,l) and proposed TF-Net for a fair comparison.
The results showed better performance for the proposed TF-Net in terms of
precision, IoU, GFLOPS, model size, and FPS compared to the YOLOv5s. TF-Net
yielded the best results with 95.7\% precision, 84\% mAp, and 44.8\% $IoU$.
</p></li>
</ul>

<h3>Title: Trustless unknown-order groups. (arXiv:2211.16128v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16128">http://arxiv.org/abs/2211.16128</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16128] Trustless unknown-order groups](http://arxiv.org/abs/2211.16128) #secure</code></li>
<li>Summary: <p>Groups of unknown order are of major interest due to their applications
including time-lock puzzles, verifiable delay functions, and accumulators. In
this paper we focus on trustless setup: in this setting, the most popular
unknown-order group construction is ideal class groups of imaginary quadratic
fields. We argue that the full impact of Sutherland's generic group-order
algorithm has not been recognised in this context, and show that group sizes
currently being proposed in practice (namely, approximately 830 bits) do not
meet the claimed security level. Instead, we claim that random group orders
should be at least 3300 bits to meet a 128-bit security level. For ideal class
groups this leads to discriminants of around 6656 bits, which are much larger
than desirable. One drawback of class groups is that current approaches require
approximately $2\log_2(N)$ bits to represent an element in a group of order N.
We provide two solutions to mitigate this blow-up in the size of
representations. First, we explain how an idea of Bleichenbacher can be used to
compress class group elements to $(3/2)\log_2(N)$ bits. Second, we note that
using Jacobians of hyperelliptic curves (in other words, class groups of
quadratic function fields) allows efficient compression to the optimal element
representation size of $\log_2(N)$ bits. We discuss point-counting approaches
for hyperelliptic curves and argue that genus-3 curves are secure in the
trustless unknown-order setting. We conclude that in practice, Jacobians of
hyperelliptic curves are more efficient in practice than ideal class groups at
the same security level -- both in the group operation and in the size of the
element representation.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: DCDetector: An IoT terminal vulnerability mining system based on distributed deep ensemble learning under source code representation. (arXiv:2211.16235v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16235">http://arxiv.org/abs/2211.16235</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16235] DCDetector: An IoT terminal vulnerability mining system based on distributed deep ensemble learning under source code representation](http://arxiv.org/abs/2211.16235) #security</code></li>
<li>Summary: <p>Context: The IoT system infrastructure platform facility vulnerability attack
has become the main battlefield of network security attacks. Most of the
traditional vulnerability mining methods rely on vulnerability detection tools
to realize vulnerability discovery. However, due to the inflexibility of tools
and the limitation of file size, its scalability It is relatively low and
cannot be applied to large-scale power big data fields. Objective: The goal of
the research is to intelligently detect vulnerabilities in source codes of
high-level languages such as C/C++. This enables us to propose a code
representation of sensitive sentence-related slices of source code, and to
detect vulnerabilities by designing a distributed deep ensemble learning model.
Method: In this paper, a new directional vulnerability mining method of
parallel ensemble learning is proposed to solve the problem of large-scale data
vulnerability mining. By extracting sensitive functions and statements, a
sensitive statement library of vulnerable codes is formed. The AST stream-based
vulnerability code slice with higher granularity performs doc2vec sentence
vectorization on the source code through the random sampling module, obtains
different classification results through distributed training through the
Bi-LSTM trainer, and obtains the final classification result by voting.
Results: This method designs and implements a distributed deep ensemble
learning system software vulnerability mining system called DCDetector. It can
make accurate predictions by using the syntactic information of the code, and
is an effective method for analyzing large-scale vulnerability data.
Conclusion: Experiments show that this method can reduce the false positive
rate of traditional static analysis and improve the performance and accuracy of
machine learning.
</p></li>
</ul>

<h3>Title: Graph Neural Networks: A Powerful and Versatile Tool for Advancing Design, Reliability, and Security of ICs. (arXiv:2211.16495v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16495">http://arxiv.org/abs/2211.16495</a></li>
<li>Code URL: <a href="https://github.com/dfx-nyuad/gnn4ic">https://github.com/dfx-nyuad/gnn4ic</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16495] Graph Neural Networks: A Powerful and Versatile Tool for Advancing Design, Reliability, and Security of ICs](http://arxiv.org/abs/2211.16495) #security</code></li>
<li>Summary: <p>Graph neural networks (GNNs) have pushed the state-of-the-art (SOTA) for
performance in learning and predicting on large-scale data present in social
networks, biology, etc. Since integrated circuits (ICs) can naturally be
represented as graphs, there has been a tremendous surge in employing GNNs for
machine learning (ML)-based methods for various aspects of IC design. Given
this trajectory, there is a timely need to review and discuss some powerful and
versatile GNN approaches for advancing IC design.
</p></li>
</ul>

<p>In this paper, we propose a generic pipeline for tailoring GNN models toward
solving challenging problems for IC design. We outline promising options for
each pipeline element, and we discuss selected and promising works, like
leveraging GNNs to break SOTA logic obfuscation. Our comprehensive overview of
GNNs frameworks covers (i) electronic design automation (EDA) and IC design in
general, (ii) design of reliable ICs, and (iii) design as well as analysis of
secure ICs. We provide our overview and related resources also in the GNN4IC
hub at https://github.com/DfX-NYUAD/GNN4IC. Finally, we discuss interesting
open problems for future research.
</p>

<h3>Title: Provably Efficient Model-free RL in Leader-Follower MDP with Linear Function Approximation. (arXiv:2211.15792v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15792">http://arxiv.org/abs/2211.15792</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15792] Provably Efficient Model-free RL in Leader-Follower MDP with Linear Function Approximation](http://arxiv.org/abs/2211.15792) #security</code></li>
<li>Summary: <p>We consider a multi-agent episodic MDP setup where an agent (leader) takes
action at each step of the episode followed by another agent (follower). The
state evolution and rewards depend on the joint action pair of the leader and
the follower. Such type of interactions can find applications in many domains
such as smart grids, mechanism design, security, and policymaking. We are
interested in how to learn policies for both the players with provable
performance guarantee under a bandit feedback setting. We focus on a setup
where both the leader and followers are {\em non-myopic}, i.e., they both seek
to maximize their rewards over the entire episode and consider a linear MDP
which can model continuous state-space which is very common in many RL
applications. We propose a {\em model-free} RL algorithm and show that
$\tilde{\mathcal{O}}(\sqrt{d^3H^3T})$ regret bounds can be achieved for both
the leader and the follower, where $d$ is the dimension of the feature mapping,
$H$ is the length of the episode, and $T$ is the total number of steps under
the bandit feedback information setup. Thus, our result holds even when the
number of states becomes infinite. The algorithm relies on {\em novel}
adaptation of the LSVI-UCB algorithm. Specifically, we replace the standard
greedy policy (as the best response) with the soft-max policy for both the
leader and the follower. This turns out to be key in establishing uniform
concentration bound for the value functions. To the best of our knowledge, this
is the first sub-linear regret bound guarantee for the Markov games with
non-myopic followers with function approximation.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Better Generalized Few-Shot Learning Even Without Base Data. (arXiv:2211.16095v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16095">http://arxiv.org/abs/2211.16095</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16095] Better Generalized Few-Shot Learning Even Without Base Data](http://arxiv.org/abs/2211.16095) #privacy</code></li>
<li>Summary: <p>This paper introduces and studies zero-base generalized few-shot learning
(zero-base GFSL), which is an extreme yet practical version of few-shot
learning problem. Motivated by the cases where base data is not available due
to privacy or ethical issues, the goal of zero-base GFSL is to newly
incorporate the knowledge of few samples of novel classes into a pretrained
model without any samples of base classes. According to our analysis, we
discover the fact that both mean and variance of the weight distribution of
novel classes are not properly established, compared to those of base classes.
The existing GFSL methods attempt to make the weight norms balanced, which we
find helps only the variance part, but discard the importance of mean of
weights particularly for novel classes, leading to the limited performance in
the GFSL problem even with base data. In this paper, we overcome this
limitation by proposing a simple yet effective normalization method that can
effectively control both mean and variance of the weight distribution of novel
classes without using any base samples and thereby achieve a satisfactory
performance on both novel and base classes. Our experimental results somewhat
surprisingly show that the proposed zero-base GFSL method that does not utilize
any base samples even outperforms the existing GFSL methods that make the best
use of base data.
</p></li>
</ul>

<h3>Title: AdaEnlight: Energy-aware Low-light Video Stream Enhancement on Mobile Devices. (arXiv:2211.16135v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16135">http://arxiv.org/abs/2211.16135</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16135] AdaEnlight: Energy-aware Low-light Video Stream Enhancement on Mobile Devices](http://arxiv.org/abs/2211.16135) #privacy</code></li>
<li>Summary: <p>The ubiquity of camera-embedded devices and the advances in deep learning
have stimulated various intelligent mobile video applications. These
applications often demand on-device processing of video streams to deliver
real-time, high-quality services for privacy and robustness concerns. However,
the performance of these applications is constrained by the raw video streams,
which tend to be taken with small-aperture cameras of ubiquitous mobile
platforms in dim light. Despite extensive low-light video enhancement
solutions, they are unfit for deployment to mobile devices due to their complex
models and and ignorance of system dynamics like energy budgets. In this paper,
we propose AdaEnlight, an energy-aware low-light video stream enhancement
system on mobile devices. It achieves real-time video enhancement with
competitive visual quality while allowing runtime behavior adaptation to the
platform-imposed dynamic energy budgets. We report extensive experiments on
diverse datasets, scenarios, and platforms and demonstrate the superiority of
AdaEnlight compared with state-of-the-art low-light image and video enhancement
solutions.
</p></li>
</ul>

<h3>Title: Procedural Image Programs for Representation Learning. (arXiv:2211.16412v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16412">http://arxiv.org/abs/2211.16412</a></li>
<li>Code URL: <a href="https://github.com/mbaradad/shaders21k">https://github.com/mbaradad/shaders21k</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16412] Procedural Image Programs for Representation Learning](http://arxiv.org/abs/2211.16412) #privacy</code></li>
<li>Summary: <p>Learning image representations using synthetic data allows training neural
networks without some of the concerns associated with real images, such as
privacy and bias. Existing work focuses on a handful of curated generative
processes which require expert knowledge to design, making it hard to scale up.
To overcome this, we propose training with a large dataset of twenty-one
thousand programs, each one generating a diverse set of synthetic images. These
programs are short code snippets, which are easy to modify and fast to execute
using OpenGL. The proposed dataset can be used for both supervised and
unsupervised representation learning, and reduces the gap between pre-training
with real and procedurally generated images by 38%.
</p></li>
</ul>

<h3>Title: Taming a Generative Model. (arXiv:2211.16488v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16488">http://arxiv.org/abs/2211.16488</a></li>
<li>Code URL: <a href="https://github.com/shimonmalnick/taming_a_generative_model">https://github.com/shimonmalnick/taming_a_generative_model</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16488] Taming a Generative Model](http://arxiv.org/abs/2211.16488) #privacy</code></li>
<li>Summary: <p>Generative models are becoming ever more powerful, being able to synthesize
highly realistic images. We propose an algorithm for taming these models -
changing the probability that the model will produce a specific image or image
category. We consider generative models that are powered by normalizing flows,
which allows us to reason about the exact generation probability likelihood for
a given image. Our method is general purpose, and we exemplify it using models
that generate human faces, a subdomain with many interesting privacy and bias
considerations. Our method can be used in the context of privacy, e.g.,
removing a specific person from the output of a model, and also in the context
of de-biasing by forcing a model to output specific image categories according
to a given target distribution. Our method uses a fast fine-tuning process
without retraining the model from scratch, achieving the goal in less than 1%
of the time taken to initially train the generative model. We evaluate
qualitatively and quantitatively, to examine the success of the taming process
and output quality.
</p></li>
</ul>

<h3>Title: Cache Me If You Can: Accuracy-Aware Inference Engine for Differentially Private Data Exploration. (arXiv:2211.15732v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15732">http://arxiv.org/abs/2211.15732</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15732] Cache Me If You Can: Accuracy-Aware Inference Engine for Differentially Private Data Exploration](http://arxiv.org/abs/2211.15732) #privacy</code></li>
<li>Summary: <p>Differential privacy (DP) allows data analysts to query databases that
contain users' sensitive information while providing a quantifiable privacy
guarantee to users. Recent interactive DP systems such as APEx provide accuracy
guarantees over the query responses, but fail to support a large number of
queries with a limited total privacy budget, as they process incoming queries
independently from past queries. We present an interactive, accuracy-aware DP
query engine, CacheDP, which utilizes a differentially private cache of past
responses, to answer the current workload at a lower privacy budget, while
meeting strict accuracy guarantees. We integrate complex DP mechanisms with our
structured cache, through novel cache-aware DP cost optimization. Our thorough
evaluation illustrates that CacheDP can accurately answer various workload
sequences, while lowering the privacy loss as compared to related work.
</p></li>
</ul>

<h3>Title: Data Privacy Protection in DeFi Protocols. (arXiv:2211.16082v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16082">http://arxiv.org/abs/2211.16082</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16082] Data Privacy Protection in DeFi Protocols](http://arxiv.org/abs/2211.16082) #privacy</code></li>
<li>Summary: <p>With the development of decentralized finance (DeFi), the inherent
limitations caused by the blockchain system have come to the surface. Because
recorded data on the blockchain is available to system participants, DeFi
protocols may not collect the private data of users. Otherwise, the information
leakage may result in serious financial losses or cause legal issues.
Therefore, DeFi protocols could hardly offer different users customized
solutions, and the capital utilization is limited. To address this challenge in
DeFi, we propose a solution, which is a trustful protocol that allows users to
provide personal private data to DeFi protocols without worrying that such
information would be disclosed. By implementing asymmetric encryption,
zero-knowledge proof, and homomorphic encryption, we ensure that users' data
will not be controlled by any centralized authorities and avoid potential
financial losses or legal disputes due to information leakage. We further
discuss the application scenarios of financial data privacy protection in
public blockchain DeFi ecosystems and cross-border financial applications, such
as credit aggregation.
</p></li>
</ul>

<h3>Title: On the Utility Recovery Incapability of Neural Net-based Differential Private Tabular Training Data Synthesizer under Privacy Deregulation. (arXiv:2211.15809v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15809">http://arxiv.org/abs/2211.15809</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15809] On the Utility Recovery Incapability of Neural Net-based Differential Private Tabular Training Data Synthesizer under Privacy Deregulation](http://arxiv.org/abs/2211.15809) #privacy</code></li>
<li>Summary: <p>Devising procedures for auditing generative model privacy-utility tradeoff is
an important yet unresolved problem in practice. Existing works concentrates on
investigating the privacy constraint side effect in terms of utility
degradation of the train on synthetic, test on real paradigm of synthetic data
training. We push such understanding on privacy-utility tradeoff to next level
by observing the privacy deregulation side effect on synthetic training data
utility. Surprisingly, we discover the Utility Recovery Incapability of
DP-CTGAN and PATE-CTGAN under privacy deregulation, raising concerns on their
practical applications. The main message is Privacy Deregulation does NOT
always imply Utility Recovery.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Backdoor Vulnerabilities in Normally Trained Deep Learning Models. (arXiv:2211.15929v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15929">http://arxiv.org/abs/2211.15929</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15929] Backdoor Vulnerabilities in Normally Trained Deep Learning Models](http://arxiv.org/abs/2211.15929) #defense</code></li>
<li>Summary: <p>We conduct a systematic study of backdoor vulnerabilities in normally trained
Deep Learning models. They are as dangerous as backdoors injected by data
poisoning because both can be equally exploited. We leverage 20 different types
of injected backdoor attacks in the literature as the guidance and study their
correspondences in normally trained models, which we call natural backdoor
vulnerabilities. We find that natural backdoors are widely existing, with most
injected backdoor attacks having natural correspondences. We categorize these
natural backdoors and propose a general detection framework. It finds 315
natural backdoors in the 56 normally trained models downloaded from the
Internet, covering all the different categories, while existing scanners
designed for injected backdoors can at most detect 65 backdoors. We also study
the root causes and defense of natural backdoors.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Training Time Adversarial Attack Aiming the Vulnerability of Continual Learning. (arXiv:2211.15875v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15875">http://arxiv.org/abs/2211.15875</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15875] Training Time Adversarial Attack Aiming the Vulnerability of Continual Learning](http://arxiv.org/abs/2211.15875) #attack</code></li>
<li>Summary: <p>Generally, regularization-based continual learning models limit access to the
previous task data to imitate the real-world setting which has memory and
privacy issues. However, this introduces a problem in these models by not being
able to track the performance on each task. In other words, current continual
learning methods are vulnerable to attacks done on the previous task. We
demonstrate the vulnerability of regularization-based continual learning
methods by presenting simple task-specific training time adversarial attack
that can be used in the learning process of a new task. Training data generated
by the proposed attack causes performance degradation on a specific task
targeted by the attacker. Experiment results justify the vulnerability proposed
in this paper and demonstrate the importance of developing continual learning
models that are robust to adversarial attack.
</p></li>
</ul>

<h3>Title: Generalized Face Anti-Spoofing via Multi-Task Learning and One-Side Meta Triplet Loss. (arXiv:2211.15955v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15955">http://arxiv.org/abs/2211.15955</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15955] Generalized Face Anti-Spoofing via Multi-Task Learning and One-Side Meta Triplet Loss](http://arxiv.org/abs/2211.15955) #attack</code></li>
<li>Summary: <p>With the increasing variations of face presentation attacks, model
generalization becomes an essential challenge for a practical face
anti-spoofing system. This paper presents a generalized face anti-spoofing
framework that consists of three tasks: depth estimation, face parsing, and
live/spoof classification. With the pixel-wise supervision from the face
parsing and depth estimation tasks, the regularized features can better
distinguish spoof faces. While simulating domain shift with meta-learning
techniques, the proposed one-side triplet loss can further improve the
generalization capability by a large margin. Extensive experiments on four
public datasets demonstrate that the proposed framework and training strategies
are more effective than previous works for model generalization to unseen
domains.
</p></li>
</ul>

<h3>Title: AdvMask: A Sparse Adversarial Attack Based Data Augmentation Method for Image Classification. (arXiv:2211.16040v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16040">http://arxiv.org/abs/2211.16040</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16040] AdvMask: A Sparse Adversarial Attack Based Data Augmentation Method for Image Classification](http://arxiv.org/abs/2211.16040) #attack</code></li>
<li>Summary: <p>Data augmentation is a widely used technique for enhancing the generalization
ability of convolutional neural networks (CNNs) in image classification tasks.
Occlusion is a critical factor that affects on the generalization ability of
image classification models. In order to generate new samples, existing data
augmentation methods based on information deletion simulate occluded samples by
randomly removing some areas in the images. However, those methods cannot
delete areas of the images according to their structural features of the
images. To solve those problems, we propose a novel data augmentation method,
AdvMask, for image classification tasks. Instead of randomly removing areas in
the images, AdvMask obtains the key points that have the greatest influence on
the classification results via an end-to-end sparse adversarial attack module.
Therefore, we can find the most sensitive points of the classification results
without considering the diversity of various image appearance and shapes of the
object of interest. In addition, a data augmentation module is employed to
generate structured masks based on the key points, thus forcing the CNN
classification models to seek other relevant content when the most
discriminative content is hidden. AdvMask can effectively improve the
performance of classification models in the testing process. The experimental
results on various datasets and CNN models verify that the proposed method
outperforms other previous data augmentation methods in image classification
tasks.
</p></li>
</ul>

<h3>Title: Be Careful with Rotation: A Uniform Backdoor Pattern for 3D Shape. (arXiv:2211.16192v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16192">http://arxiv.org/abs/2211.16192</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16192] Be Careful with Rotation: A Uniform Backdoor Pattern for 3D Shape](http://arxiv.org/abs/2211.16192) #attack</code></li>
<li>Summary: <p>For saving cost, many deep neural networks (DNNs) are trained on third-party
datasets downloaded from internet, which enables attacker to implant backdoor
into DNNs. In 2D domain, inherent structures of different image formats are
similar. Hence, backdoor attack designed for one image format will suite for
others. However, when it comes to 3D world, there is a huge disparity among
different 3D data structures. As a result, backdoor pattern designed for one
certain 3D data structure will be disable for other data structures of the same
3D scene. Therefore, this paper designs a uniform backdoor pattern: NRBdoor
(Noisy Rotation Backdoor) which is able to adapt for heterogeneous 3D data
structures. Specifically, we start from the unit rotation and then search for
the optimal pattern by noise generation and selection process. The proposed
NRBdoor is natural and imperceptible, since rotation is a common operation
which usually contains noise due to both the miss match between a pair of
points and the sensor calibration error for real-world 3D scene. Extensive
experiments on 3D mesh and point cloud show that the proposed NRBdoor achieves
state-of-the-art performance, with negligible shape variation.
</p></li>
</ul>

<h3>Title: Similarity Distribution based Membership Inference Attack on Person Re-identification. (arXiv:2211.15918v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15918">http://arxiv.org/abs/2211.15918</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15918] Similarity Distribution based Membership Inference Attack on Person Re-identification](http://arxiv.org/abs/2211.15918) #attack</code></li>
<li>Summary: <p>While person Re-identification (Re-ID) has progressed rapidly due to its wide
real-world applications, it also causes severe risks of leaking personal
information from training data. Thus, this paper focuses on quantifying this
risk by membership inference (MI) attack. Most of the existing MI attack
algorithms focus on classification models, while Re-ID follows a totally
different training and inference paradigm. Re-ID is a fine-grained recognition
task with complex feature embedding, and model outputs commonly used by
existing MI like logits and losses are not accessible during inference. Since
Re-ID focuses on modelling the relative relationship between image pairs
instead of individual semantics, we conduct a formal and empirical analysis
which validates that the distribution shift of the inter-sample similarity
between training and test set is a critical criterion for Re-ID membership
inference. As a result, we propose a novel membership inference attack method
based on the inter-sample similarity distribution. Specifically, a set of
anchor images are sampled to represent the similarity distribution conditioned
on a target image, and a neural network with a novel anchor selection module is
proposed to predict the membership of the target image. Our experiments
validate the effectiveness of the proposed approach on both the Re-ID task and
conventional classification task.
</p></li>
</ul>

<h3>Title: Control-Flow Integrity at RISC: Attacking RISC-V by Jump-Oriented Programming. (arXiv:2211.16212v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16212">http://arxiv.org/abs/2211.16212</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16212] Control-Flow Integrity at RISC: Attacking RISC-V by Jump-Oriented Programming](http://arxiv.org/abs/2211.16212) #attack</code></li>
<li>Summary: <p>RISC-V is an open instruction set architecture recently developed for
embedded real-time systems. To achieve a lasting security on these systems and
design efficient countermeasures, a better understanding of vulnerabilities to
novel and potential future attacks is mandatory. This paper demonstrates that
RISC-V is sensible to Jump-Oriented Programming, a class of complex code-reuse
attacks, able to bypass existing protections. We provide a first analysis of
RISC-V systems' attack surface exploitable by such attacks, and show how they
can be chained together in order to build a full-fledged attack. We use a
conservative hypothesis on exploited registers and instruction patterns, in an
approach we called reserved registers. This approach is implemented on a
vulnerable RISC-V application, and successfully applied to expose an AES256
secret.
</p></li>
</ul>

<h3>Title: Analysis of Anomalous Behavior in Network Systems Using Deep Reinforcement Learning with CNN Architecture. (arXiv:2211.16304v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16304">http://arxiv.org/abs/2211.16304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16304] Analysis of Anomalous Behavior in Network Systems Using Deep Reinforcement Learning with CNN Architecture](http://arxiv.org/abs/2211.16304) #attack</code></li>
<li>Summary: <p>In order to gain access to networks, different types of intrusion attacks
have been designed, and the attackers are working on improving them. Computer
networks have become increasingly important in daily life due to the increasing
reliance on them. In light of this, it is quite evident that algorithms with
high detection accuracy and reliability are needed for various types of
attacks. The purpose of this paper is to develop an intrusion detection system
that is based on deep reinforcement learning. Based on the Markov decision
process, the proposed system can generate informative representations suitable
for classification tasks based on vast data. Reinforcement learning is
considered from two different perspectives, deep Q learning, and double deep Q
learning. Different experiments have demonstrated that the proposed systems
have an accuracy of $99.17\%$ over the UNSW-NB15 dataset in both approaches, an
improvement over previous methods based on contrastive learning and
LSTM-Autoencoders. The performance of the model trained on UNSW-NB15 has also
been evaluated on BoT-IoT datasets, resulting in competitive performance
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Survey on Self-Supervised Multimodal Representation Learning and Foundation Models. (arXiv:2211.15837v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15837">http://arxiv.org/abs/2211.15837</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15837] Survey on Self-Supervised Multimodal Representation Learning and Foundation Models](http://arxiv.org/abs/2211.15837) #robust</code></li>
<li>Summary: <p>Deep learning has been the subject of growing interest in recent years.
Specifically, a specific type called Multimodal learning has shown great
promise for solving a wide range of problems in domains such as language,
vision, audio, etc. One promising research direction to improve this further
has been learning rich and robust low-dimensional data representation of the
high-dimensional world with the help of large-scale datasets present on the
internet. Because of its potential to avoid the cost of annotating large-scale
datasets, self-supervised learning has been the de facto standard for this task
in recent years. This paper summarizes some of the landmark research papers
that are directly or indirectly responsible to build the foundation of
multimodal self-supervised learning of representation today. The paper goes
over the development of representation learning over the last few years for
each modality and how they were combined to get a multimodal agent later.
</p></li>
</ul>

<h3>Title: LUMix: Improving Mixup by Better Modelling Label Uncertainty. (arXiv:2211.15846v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15846">http://arxiv.org/abs/2211.15846</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15846] LUMix: Improving Mixup by Better Modelling Label Uncertainty](http://arxiv.org/abs/2211.15846) #robust</code></li>
<li>Summary: <p>Modern deep networks can be better generalized when trained with noisy
samples and regularization techniques. Mixup and CutMix have been proven to be
effective for data augmentation to help avoid overfitting. Previous Mixup-based
methods linearly combine images and labels to generate additional training
data. However, this is problematic if the object does not occupy the whole
image as we demonstrate in Figure 1. Correctly assigning the label weights is
hard even for human beings and there is no clear criterion to measure it. To
tackle this problem, in this paper, we propose LUMix, which models such
uncertainty by adding label perturbation during training. LUMix is simple as it
can be implemented in just a few lines of code and can be universally applied
to any deep networks \eg CNNs and Vision Transformers, with minimal
computational cost. Extensive experiments show that our LUMix can consistently
boost the performance for networks with a wide range of diversity and capacity
on ImageNet, \eg $+0.7\%$ for a small model DeiT-S and $+0.6\%$ for a large
variant XCiT-L. We also demonstrate that LUMix can lead to better robustness
when evaluated on ImageNet-O and ImageNet-A. The source code can be found
\href{https://github.com/kevin-ssy/LUMix}{here}
</p></li>
</ul>

<h3>Title: On Robust Learning from Noisy Labels: A Permutation Layer Approach. (arXiv:2211.15890v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15890">http://arxiv.org/abs/2211.15890</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15890] On Robust Learning from Noisy Labels: A Permutation Layer Approach](http://arxiv.org/abs/2211.15890) #robust</code></li>
<li>Summary: <p>The existence of label noise imposes significant challenges (e.g., poor
generalization) on the training process of deep neural networks (DNN). As a
remedy, this paper introduces a permutation layer learning approach termed
PermLL to dynamically calibrate the training process of the DNN subject to
instance-dependent and instance-independent label noise. The proposed method
augments the architecture of a conventional DNN by an instance-dependent
permutation layer. This layer is essentially a convex combination of
permutation matrices that is dynamically calibrated for each sample. The
primary objective of the permutation layer is to correct the loss of noisy
samples mitigating the effect of label noise. We provide two variants of PermLL
in this paper: one applies the permutation layer to the model's prediction,
while the other applies it directly to the given noisy label. In addition, we
provide a theoretical comparison between the two variants and show that
previous methods can be seen as one of the variants. Finally, we validate
PermLL experimentally and show that it achieves state-of-the-art performance on
both real and synthetic datasets.
</p></li>
</ul>

<h3>Title: Towards More Robust Interpretation via Local Gradient Alignment. (arXiv:2211.15900v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15900">http://arxiv.org/abs/2211.15900</a></li>
<li>Code URL: <a href="https://github.com/joshua840/robustaga">https://github.com/joshua840/robustaga</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15900] Towards More Robust Interpretation via Local Gradient Alignment](http://arxiv.org/abs/2211.15900) #robust</code></li>
<li>Summary: <p>Neural network interpretation methods, particularly feature attribution
methods, are known to be fragile with respect to adversarial input
perturbations. To address this, several methods for enhancing the local
smoothness of the gradient while training have been proposed for attaining
\textit{robust} feature attributions. However, the lack of considering the
normalization of the attributions, which is essential in their visualizations,
has been an obstacle to understanding and improving the robustness of feature
attribution methods. In this paper, we provide new insights by taking such
normalization into account. First, we show that for every non-negative
homogeneous neural network, a naive $\ell_2$-robust criterion for gradients is
\textit{not} normalization invariant, which means that two functions with the
same normalized gradient can have different values. Second, we formulate a
normalization invariant cosine distance-based criterion and derive its upper
bound, which gives insight for why simply minimizing the Hessian norm at the
input, as has been done in previous work, is not sufficient for attaining
robust feature attribution. Finally, we propose to combine both $\ell_2$ and
cosine distance-based criteria as regularization terms to leverage the
advantages of both in aligning the local gradient. As a result, we
experimentally show that models trained with our method produce much more
robust interpretations on CIFAR-10 and ImageNet-100 without significantly
hurting the accuracy, compared to the recent baselines. To the best of our
knowledge, this is the first work to verify the robustness of interpretation on
a larger-scale dataset beyond CIFAR-10, thanks to the computational efficiency
of our method.
</p></li>
</ul>

<h3>Title: Isolation and Impartial Aggregation: A Paradigm of Incremental Learning without Interference. (arXiv:2211.15969v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15969">http://arxiv.org/abs/2211.15969</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15969] Isolation and Impartial Aggregation: A Paradigm of Incremental Learning without Interference](http://arxiv.org/abs/2211.15969) #robust</code></li>
<li>Summary: <p>This paper focuses on the prevalent performance imbalance in the stages of
incremental learning. To avoid obvious stage learning bottlenecks, we propose a
brand-new stage-isolation based incremental learning framework, which leverages
a series of stage-isolated classifiers to perform the learning task of each
stage without the interference of others. To be concrete, to aggregate multiple
stage classifiers as a uniform one impartially, we first introduce a
temperature-controlled energy metric for indicating the confidence score levels
of the stage classifiers. We then propose an anchor-based energy
self-normalization strategy to ensure the stage classifiers work at the same
energy level. Finally, we design a voting-based inference augmentation strategy
for robust inference. The proposed method is rehearsal free and can work for
almost all continual learning scenarios. We evaluate the proposed method on
four large benchmarks. Extensive results demonstrate the superiority of the
proposed method in setting up new state-of-the-art overall performance.
\emph{Code is available at} \url{https://github.com/iamwangyabin/ESN}.
</p></li>
</ul>

<h3>Title: Impact of Automatic Image Classification and Blind Deconvolution in Improving Text Detection Performance of the CRAFT Algorithm. (arXiv:2211.15999v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15999">http://arxiv.org/abs/2211.15999</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15999] Impact of Automatic Image Classification and Blind Deconvolution in Improving Text Detection Performance of the CRAFT Algorithm](http://arxiv.org/abs/2211.15999) #robust</code></li>
<li>Summary: <p>Text detection in natural scenes has been a significant and active research
subject in computer vision and document analysis because of its wide range of
applications as evidenced by the emergence of the Robust Reading Competition.
One of the algorithms which has good text detection performance in the said
competition is the Character Region Awareness for Text Detection (CRAFT).
Employing the ICDAR 2013 dataset, this study investigates the impact of
automatic image classification and blind deconvolution as image pre-processing
steps to further enhance the text detection performance of CRAFT. The proposed
technique automatically classifies the scene images into two categories, blurry
and non-blurry, by utilizing of a Laplacian operator with 100 as threshold.
Prior to applying the CRAFT algorithm, images that are categorized as blurry
are further pre-processed using blind deconvolution to reduce the blur. The
results revealed that the proposed method significantly enhanced the detection
performance of CRAFT, as demonstrated by its IoU h-mean of 94.47% compared to
the original 91.42% h-mean of CRAFT and this even outperformed the top-ranked
SenseTime, whose h-mean is 93.62%.
</p></li>
</ul>

<h3>Title: Context-Aware Robust Fine-Tuning. (arXiv:2211.16175v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16175">http://arxiv.org/abs/2211.16175</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16175] Context-Aware Robust Fine-Tuning](http://arxiv.org/abs/2211.16175) #robust</code></li>
<li>Summary: <p>Contrastive Language-Image Pre-trained (CLIP) models have zero-shot ability
of classifying an image belonging to "[CLASS]" by using similarity between the
image and the prompt sentence "a [CONTEXT] of [CLASS]". Based on exhaustive
text cues in "[CONTEXT]", CLIP model is aware of different contexts, e.g.
background, style, viewpoint, and exhibits unprecedented robustness against a
wide range of distribution shifts. However, recent works find further
fine-tuning of CLIP models improves accuracy but sacrifices the robustness on
downstream tasks. We conduct an empirical investigation to show fine-tuning
will corrupt the context-aware ability of pre-trained CLIP features. To solve
this problem, we propose Context-Aware Robust Fine-tuning (CAR-FT). CAR-FT
regularizes the model during fine-tuning to capture the context information.
Specifically, we use zero-shot prompt weights to get the context distribution
contained in the image. By minimizing the Kullback-Leibler Divergence (KLD)
between context distributions induced by original/fine-tuned CLIP models,
CAR-FT makes the context-aware ability of CLIP inherited into downstream tasks,
and achieves both higher In-Distribution (ID) and Out-Of-Distribution (OOD)
accuracy. The experimental results show CAR-FT achieves superior robustness on
five OOD test datasets of ImageNet, and meanwhile brings accuracy gains on nine
downstream tasks. Additionally, CAR-FT surpasses previous Domain Generalization
(DG) methods and gets 78.5% averaged accuracy on DomainBed benchmark, building
the new state-of-the-art.
</p></li>
</ul>

<h3>Title: Advancing Deep Metric Learning Through Multiple Batch Norms And Multi-Targeted Adversarial Examples. (arXiv:2211.16253v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16253">http://arxiv.org/abs/2211.16253</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16253] Advancing Deep Metric Learning Through Multiple Batch Norms And Multi-Targeted Adversarial Examples](http://arxiv.org/abs/2211.16253) #robust</code></li>
<li>Summary: <p>Deep Metric Learning (DML) is a prominent field in machine learning with
extensive practical applications that concentrate on learning visual
similarities. It is known that inputs such as Adversarial Examples (AXs), which
follow a distribution different from that of clean data, result in false
predictions from DML systems. This paper proposes MDProp, a framework to
simultaneously improve the performance of DML models on clean data and inputs
following multiple distributions. MDProp utilizes multi-distribution data
through an AX generation process while leveraging disentangled learning through
multiple batch normalization layers during the training of a DML model. MDProp
is the first to generate feature space multi-targeted AXs to perform targeted
regularization on the training model's denser embedding space regions,
resulting in improved embedding space densities contributing to the improved
generalization in the trained models. From a comprehensive experimental
analysis, we show that MDProp results in up to 2.95% increased clean data
Recall@1 scores and up to 2.12 times increased robustness against different
input distributions compared to the conventional methods.
</p></li>
</ul>

<h3>Title: PatchMatch-Stereo-Panorama, a fast dense reconstruction from 360{\deg} video images. (arXiv:2211.16266v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16266">http://arxiv.org/abs/2211.16266</a></li>
<li>Code URL: <a href="https://github.com/roblabwh/patchmatch">https://github.com/roblabwh/patchmatch</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16266] PatchMatch-Stereo-Panorama, a fast dense reconstruction from 360{\deg} video images](http://arxiv.org/abs/2211.16266) #robust</code></li>
<li>Summary: <p>This work proposes a new method for real-time dense 3d reconstruction for
common 360{\deg} action cams, which can be mounted on small scouting UAVs
during USAR missions. The proposed method extends a feature based Visual
monocular SLAM (OpenVSLAM, based on the popular ORB-SLAM) for robust long-term
localization on equirectangular video input by adding an additional
densification thread that computes dense correspondences for any given keyframe
with respect to a local keyframe-neighboorhood using a
PatchMatch-Stereo-approach. While PatchMatch-Stereo-types of algorithms are
considered state of the art for large scale Mutli-View-Stereo they had not been
adapted so far for real-time dense 3d reconstruction tasks. This work describes
a new massively parallel variant of the PatchMatch-Stereo-algorithm that
differs from current approaches in two ways: First it supports the
equirectangular camera model while other solutions are limited to the pinhole
camera model. Second it is optimized for low latency while keeping a high level
of completeness and accuracy. To achieve this it operates only on small
sequences of keyframes, but employs techniques to compensate for the potential
loss of accuracy due to the limited number of frames. Results demonstrate that
dense 3d reconstruction is possible on a consumer grade laptop with a recent
mobile GPU and that it is possible with improved accuracy and completeness over
common offline-MVS solutions with comparable quality settings.
</p></li>
</ul>

<h3>Title: Finer-Grained Correlations: Location Priors for Unseen Object Pose Estimation. (arXiv:2211.16290v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16290">http://arxiv.org/abs/2211.16290</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16290] Finer-Grained Correlations: Location Priors for Unseen Object Pose Estimation](http://arxiv.org/abs/2211.16290) #robust</code></li>
<li>Summary: <p>We present a new method which provides object location priors for previously
unseen object 6D pose estimation. Existing approaches build upon a template
matching strategy and convolve a set of reference images with the query.
Unfortunately, their performance is affected by the object scale mismatches
between the references and the query. To address this issue, we present a
finer-grained correlation estimation module, which handles the object scale
mismatches by computing correlations with adjustable receptive fields. We also
propose to decouple the correlations into scale-robust and scale-aware
representations to estimate the object location and size, respectively. Our
method achieves state-of-the-art unseen object localization and 6D pose
estimation results on LINEMOD and GenMOP. We further construct a challenging
synthetic dataset, where the results highlight the better robustness of our
method to varying backgrounds, illuminations, and object sizes, as well as to
the reference-query domain gap.
</p></li>
</ul>

<h3>Title: Language-driven Open-Vocabulary 3D Scene Understanding. (arXiv:2211.16312v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16312">http://arxiv.org/abs/2211.16312</a></li>
<li>Code URL: <a href="https://github.com/cvmi-lab/pla">https://github.com/cvmi-lab/pla</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16312] Language-driven Open-Vocabulary 3D Scene Understanding](http://arxiv.org/abs/2211.16312) #robust</code></li>
<li>Summary: <p>Open-vocabulary scene understanding aims to localize and recognize unseen
categories beyond the annotated label space. The recent breakthrough of 2D
open-vocabulary perception is largely driven by Internet-scale paired
image-text data with rich vocabulary concepts. However, this success cannot be
directly transferred to 3D scenarios due to the inaccessibility of large-scale
3D-text pairs. To this end, we propose to distill knowledge encoded in
pre-trained vision-language (VL) foundation models through captioning
multi-view images from 3D, which allows explicitly associating 3D and
semantic-rich captions. Further, to facilitate coarse-to-fine visual-semantic
representation learning from captions, we design hierarchical 3D-caption pairs,
leveraging geometric constraints between 3D scenes and multi-view images.
Finally, by employing contrastive learning, the model learns language-aware
embeddings that connect 3D and text for open-vocabulary tasks. Our method not
only remarkably outperforms baseline methods by 25.8% $\sim$ 44.7% hIoU and
14.5% $\sim$ 50.4% hAP$_{50}$ on open-vocabulary semantic and instance
segmentation, but also shows robust transferability on challenging zero-shot
domain transfer tasks. Code will be available at
https://github.com/CVMI-Lab/PLA.
</p></li>
</ul>

<h3>Title: Compressing Volumetric Radiance Fields to 1 MB. (arXiv:2211.16386v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16386">http://arxiv.org/abs/2211.16386</a></li>
<li>Code URL: <a href="https://github.com/algohunt/vqrf">https://github.com/algohunt/vqrf</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16386] Compressing Volumetric Radiance Fields to 1 MB](http://arxiv.org/abs/2211.16386) #robust</code></li>
<li>Summary: <p>Approximating radiance fields with volumetric grids is one of promising
directions for improving NeRF, represented by methods like Plenoxels and DVGO,
which achieve super-fast training convergence and real-time rendering. However,
these methods typically require a tremendous storage overhead, costing up to
hundreds of megabytes of disk space and runtime memory for a single scene. We
address this issue in this paper by introducing a simple yet effective
framework, called vector quantized radiance fields (VQRF), for compressing
these volume-grid-based radiance fields. We first present a robust and adaptive
metric for estimating redundancy in grid models and performing voxel pruning by
better exploring intermediate outputs of volumetric rendering. A trainable
vector quantization is further proposed to improve the compactness of grid
models. In combination with an efficient joint tuning strategy and
post-processing, our method can achieve a compression ratio of 100$\times$ by
reducing the overall model size to 1 MB with negligible loss on visual quality.
Extensive experiments demonstrate that the proposed framework is capable of
achieving unrivaled performance and well generalization across multiple methods
with distinct volumetric structures, facilitating the wide use of volumetric
radiance fields methods in real-world applications. Code Available at
\url{https://github.com/AlgoHunt/VQRF}
</p></li>
</ul>

<h3>Title: Finding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testing. (arXiv:2211.16499v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16499">http://arxiv.org/abs/2211.16499</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16499] Finding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testing](http://arxiv.org/abs/2211.16499) #robust</code></li>
<li>Summary: <p>Modern deep neural networks tend to be evaluated on static test sets. One
shortcoming of this is the fact that these deep neural networks cannot be
easily evaluated for robustness issues with respect to specific scene
variations. For example, it is hard to study the robustness of these networks
to variations of object scale, object pose, scene lighting and 3D occlusions.
The main reason is that collecting real datasets with fine-grained naturalistic
variations of sufficient scale can be extremely time-consuming and expensive.
In this work, we present Counterfactual Simulation Testing, a counterfactual
framework that allows us to study the robustness of neural networks with
respect to some of these naturalistic variations by building realistic
synthetic scenes that allow us to ask counterfactual questions to the models,
ultimately providing answers to questions such as "Would your classification
still be correct if the object were viewed from the top?" or "Would your
classification still be correct if the object were partially occluded by
another object?". Our method allows for a fair comparison of the robustness of
recently released, state-of-the-art Convolutional Neural Networks and Vision
Transformers, with respect to these naturalistic variations. We find evidence
that ConvNext is more robust to pose and scale variations than Swin, that
ConvNext generalizes better to our simulated domain and that Swin handles
partial occlusion better than ConvNext. We also find that robustness for all
networks improves with network scale and with data scale and variety. We
release the Naturalistic Variation Object Dataset (NVD), a large simulated
dataset of 272k images of everyday objects with naturalistic variations such as
object pose, scale, viewpoint, lighting and occlusions. Project page:
https://counterfactualsimulation.github.io
</p></li>
</ul>

<h3>Title: Syntactic Substitutability as Unsupervised Dependency Syntax. (arXiv:2211.16031v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16031">http://arxiv.org/abs/2211.16031</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16031] Syntactic Substitutability as Unsupervised Dependency Syntax](http://arxiv.org/abs/2211.16031) #robust</code></li>
<li>Summary: <p>Syntax is a latent hierarchical structure which underpins the robust and
compositional nature of human language. An active line of inquiry is whether
large pretrained language models (LLMs) are able to acquire syntax by training
on text alone; understanding a model's syntactic capabilities is essential to
understanding how it processes and makes use of language. In this paper, we
propose a new method, SSUD, which allows for the induction of syntactic
structures without supervision from gold-standard parses. Instead, we seek to
define formalism-agnostic, model-intrinsic syntactic parses by using a property
of syntactic relations: syntactic substitutability. We demonstrate both
quantitative and qualitative gains on dependency parsing tasks using SSUD, and
induce syntactic structures which we hope provide clarity into LLMs and
linguistic representations, alike.
</p></li>
</ul>

<h3>Title: TyDiP: A Dataset for Politeness Classification in Nine Typologically Diverse Languages. (arXiv:2211.16496v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16496">http://arxiv.org/abs/2211.16496</a></li>
<li>Code URL: <a href="https://github.com/genius1237/tydip">https://github.com/genius1237/tydip</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16496] TyDiP: A Dataset for Politeness Classification in Nine Typologically Diverse Languages](http://arxiv.org/abs/2211.16496) #robust</code></li>
<li>Summary: <p>We study politeness phenomena in nine typologically diverse languages.
Politeness is an important facet of communication and is sometimes argued to be
cultural-specific, yet existing computational linguistic study is limited to
English. We create TyDiP, a dataset containing three-way politeness annotations
for 500 examples in each language, totaling 4.5K examples. We evaluate how well
multilingual models can identify politeness levels -- they show a fairly robust
zero-shot transfer ability, yet fall short of estimated human accuracy
significantly. We further study mapping the English politeness strategy lexicon
into nine languages via automatic translation and lexicon induction, analyzing
whether each strategy's impact stays consistent across languages. Lastly, we
empirically study the complicated relationship between formality and politeness
through transfer experiments. We hope our dataset will support various research
questions and applications, from evaluating multilingual models to constructing
polite multilingual agents.
</p></li>
</ul>

<h3>Title: A Survey of Relevant Text Mining Technology. (arXiv:2211.15784v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15784">http://arxiv.org/abs/2211.15784</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15784] A Survey of Relevant Text Mining Technology](http://arxiv.org/abs/2211.15784) #robust</code></li>
<li>Summary: <p>Recent advances in text mining and natural language processing technology
have enabled researchers to detect an authors identity or demographic
characteristics, such as age and gender, in several text genres by
automatically analysing the variation of linguistic characteristics. However,
applying such techniques in the wild, i.e., in both cybercriminal and regular
online social media, differs from more general applications in that its
defining characteristics are both domain and process dependent. This gives rise
to a number of challenges of which contemporary research has only scratched the
surface. More specifically, a text mining approach applied on social media
communications typically has no control over the dataset size, the number of
available communications will vary across users. Hence, the system has to be
robust towards limited data availability. Additionally, the quality of the data
cannot be guaranteed. As a result, the approach needs to be tolerant to a
certain degree of linguistic noise (for example, abbreviations, non-standard
language use, spelling variations and errors). Finally, in the context of
cybercriminal fora, it has to be robust towards deceptive or adversarial
behaviour, i.e. offenders who attempt to hide their criminal intentions
(obfuscation) or who assume a false digital persona (imitation), potentially
using coded language.
</p></li>
</ul>

<p>In this work we present a comprehensive survey that discusses the problems
that have already been addressed in current literature and review potential
solutions. Additionally, we highlight which areas need to be given more
attention.
</p>

<h3>Title: Malign Overfitting: Interpolation Can Provably Preclude Invariance. (arXiv:2211.15724v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15724">http://arxiv.org/abs/2211.15724</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15724] Malign Overfitting: Interpolation Can Provably Preclude Invariance](http://arxiv.org/abs/2211.15724) #robust</code></li>
<li>Summary: <p>Learned classifiers should often possess certain invariance properties meant
to encourage fairness, robustness, or out-of-distribution generalization.
However, multiple recent works empirically demonstrate that common
invariance-inducing regularizers are ineffective in the over-parameterized
regime, in which classifiers perfectly fit (i.e. interpolate) the training
data. This suggests that the phenomenon of ``benign overfitting," in which
models generalize well despite interpolating, might not favorably extend to
settings in which robustness or fairness are desirable.
</p></li>
</ul>

<p>In this work we provide a theoretical justification for these observations.
We prove that -- even in the simplest of settings -- any interpolating learning
rule (with arbitrarily small margin) will not satisfy these invariance
properties. We then propose and analyze an algorithm that -- in the same
setting -- successfully learns a non-interpolating classifier that is provably
invariant. We validate our theoretical observations on simulated data and the
Waterbirds dataset.
</p>

<h3>Title: Understanding the Impact of Adversarial Robustness on Accuracy Disparity. (arXiv:2211.15762v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15762">http://arxiv.org/abs/2211.15762</a></li>
<li>Code URL: <a href="https://github.com/accuracy-disparity/at-on-ad">https://github.com/accuracy-disparity/at-on-ad</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15762] Understanding the Impact of Adversarial Robustness on Accuracy Disparity](http://arxiv.org/abs/2211.15762) #robust</code></li>
<li>Summary: <p>While it has long been empirically observed that adversarial robustness may
be at odds with standard accuracy and may have further disparate impacts on
different classes, it remains an open question to what extent such observations
hold and how the class imbalance plays a role within. In this paper, we attempt
to understand this question of accuracy disparity by taking a closer look at
linear classifiers under a Gaussian mixture model. We decompose the impact of
adversarial robustness into two parts: an inherent effect that will degrade the
standard accuracy on all classes, and the other caused by the class imbalance
ratio, which will increase the accuracy disparity compared to standard
training. Furthermore, we also extend our model to the general family of stable
distributions. We demonstrate that while the constraint of adversarial
robustness consistently degrades the standard accuracy in the balanced class
setting, the class imbalance ratio plays a fundamentally different role in
accuracy disparity compared to the Gaussian case, due to the heavy tail of the
stable distribution. We additionally perform experiments on both synthetic and
real-world datasets. The empirical results not only corroborate our theoretical
findings, but also suggest that the implications may extend to nonlinear models
over real-world datasets.
</p></li>
</ul>

<h3>Title: Novelty Detection for Election Fraud: A Case Study with Agent-Based Simulation Data. (arXiv:2211.16023v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16023">http://arxiv.org/abs/2211.16023</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16023] Novelty Detection for Election Fraud: A Case Study with Agent-Based Simulation Data](http://arxiv.org/abs/2211.16023) #robust</code></li>
<li>Summary: <p>In this paper, we propose a robust election simulation model and
independently developed election anomaly detection algorithm that demonstrates
the simulation's utility. The simulation generates artificial elections with
similar properties and trends as elections from the real world, while giving
users control and knowledge over all the important components of the elections.
We generate a clean election results dataset without fraud as well as datasets
with varying degrees of fraud. We then measure how well the algorithm is able
to successfully detect the level of fraud present. The algorithm determines how
similar actual election results are as compared to the predicted results from
polling and a regression model of other regions that have similar demographics.
We use k-means to partition electoral regions into clusters such that
demographic homogeneity is maximized among clusters. We then use a novelty
detection algorithm implemented as a one-class Support Vector Machine where the
clean data is provided in the form of polling predictions and regression
predictions. The regression predictions are built from the actual data in such
a way that the data supervises itself. We show both the effectiveness of the
simulation technique and the machine learning model in its success in
identifying fraudulent regions.
</p></li>
</ul>

<h3>Title: Understanding and Enhancing Robustness of Concept-based Models. (arXiv:2211.16080v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16080">http://arxiv.org/abs/2211.16080</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16080] Understanding and Enhancing Robustness of Concept-based Models](http://arxiv.org/abs/2211.16080) #robust</code></li>
<li>Summary: <p>Rising usage of deep neural networks to perform decision making in critical
applications like medical diagnosis and financial analysis have raised concerns
regarding their reliability and trustworthiness. As automated systems become
more mainstream, it is important their decisions be transparent, reliable and
understandable by humans for better trust and confidence. To this effect,
concept-based models such as Concept Bottleneck Models (CBMs) and
Self-Explaining Neural Networks (SENN) have been proposed which constrain the
latent space of a model to represent high level concepts easily understood by
domain experts in the field. Although concept-based models promise a good
approach to both increasing explainability and reliability, it is yet to be
shown if they demonstrate robustness and output consistent concepts under
systematic perturbations to their inputs. To better understand performance of
concept-based models on curated malicious samples, in this paper, we aim to
study their robustness to adversarial perturbations, which are also known as
the imperceptible changes to the input data that are crafted by an attacker to
fool a well-learned concept-based model. Specifically, we first propose and
analyze different malicious attacks to evaluate the security vulnerability of
concept based models. Subsequently, we propose a potential general adversarial
training-based defense mechanism to increase robustness of these systems to the
proposed malicious attacks. Extensive experiments on one synthetic and two
real-world datasets demonstrate the effectiveness of the proposed attacks and
the defense approach.
</p></li>
</ul>

<h3>Title: Quantization-aware Interval Bound Propagation for Training Certifiably Robust Quantized Neural Networks. (arXiv:2211.16187v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16187">http://arxiv.org/abs/2211.16187</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16187] Quantization-aware Interval Bound Propagation for Training Certifiably Robust Quantized Neural Networks](http://arxiv.org/abs/2211.16187) #robust</code></li>
<li>Summary: <p>We study the problem of training and certifying adversarially robust
quantized neural networks (QNNs). Quantization is a technique for making neural
networks more efficient by running them using low-bit integer arithmetic and is
therefore commonly adopted in industry. Recent work has shown that
floating-point neural networks that have been verified to be robust can become
vulnerable to adversarial attacks after quantization, and certification of the
quantized representation is necessary to guarantee robustness. In this work, we
present quantization-aware interval bound propagation (QA-IBP), a novel method
for training robust QNNs. Inspired by advances in robust learning of
non-quantized networks, our training algorithm computes the gradient of an
abstract representation of the actual network. Unlike existing approaches, our
method can handle the discrete semantics of QNNs. Based on QA-IBP, we also
develop a complete verification procedure for verifying the adversarial
robustness of QNNs, which is guaranteed to terminate and produce a correct
answer. Compared to existing approaches, the key advantage of our verification
procedure is that it runs entirely on GPU or other accelerator devices. We
demonstrate experimentally that our approach significantly outperforms existing
methods and establish the new state-of-the-art for training and certifying the
robustness of QNNs.
</p></li>
</ul>

<h3>Title: A3T: Accuracy Aware Adversarial Training. (arXiv:2211.16316v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16316">http://arxiv.org/abs/2211.16316</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16316] A3T: Accuracy Aware Adversarial Training](http://arxiv.org/abs/2211.16316) #robust</code></li>
<li>Summary: <p>Adversarial training has been empirically shown to be more prone to
overfitting than standard training. The exact underlying reasons still need to
be fully understood. In this paper, we identify one cause of overfitting
related to current practices of generating adversarial samples from
misclassified samples. To address this, we propose an alternative approach that
leverages the misclassified samples to mitigate the overfitting problem. We
show that our approach achieves better generalization while having comparable
robustness to state-of-the-art adversarial training methods on a wide range of
computer vision, natural language processing, and tabular tasks.
</p></li>
</ul>

<h3>Title: BARTSmiles: Generative Masked Language Models for Molecular Representations. (arXiv:2211.16349v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16349">http://arxiv.org/abs/2211.16349</a></li>
<li>Code URL: <a href="https://github.com/yerevann/bartsmiles">https://github.com/yerevann/bartsmiles</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16349] BARTSmiles: Generative Masked Language Models for Molecular Representations](http://arxiv.org/abs/2211.16349) #robust</code></li>
<li>Summary: <p>We discover a robust self-supervised strategy tailored towards molecular
representations for generative masked language models through a series of
tailored, in-depth ablations. Using this pre-training strategy, we train
BARTSmiles, a BART-like model with an order of magnitude more compute than
previous self-supervised molecular representations. In-depth evaluations show
that BARTSmiles consistently outperforms other self-supervised representations
across classification, regression, and generation tasks setting a new
state-of-the-art on 11 tasks. We then quantitatively show that when applied to
the molecular domain, the BART objective learns representations that implicitly
encode our downstream tasks of interest. For example, by selecting seven
neurons from a frozen BARTSmiles, we can obtain a model having performance
within two percentage points of the full fine-tuned model on task Clintox.
Lastly, we show that standard attribution interpretability methods, when
applied to BARTSmiles, highlight certain substructures that chemists use to
explain specific properties of molecules. The code and the pretrained model are
publicly available.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h3>Title: Interpretations Cannot Be Trusted: Stealthy and Effective Adversarial Perturbations against Interpretable Deep Learning. (arXiv:2211.15926v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15926">http://arxiv.org/abs/2211.15926</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15926] Interpretations Cannot Be Trusted: Stealthy and Effective Adversarial Perturbations against Interpretable Deep Learning](http://arxiv.org/abs/2211.15926) #steal</code></li>
<li>Summary: <p>Deep learning methods have gained increased attention in various applications
due to their outstanding performance. For exploring how this high performance
relates to the proper use of data artifacts and the accurate problem
formulation of a given task, interpretation models have become a crucial
component in developing deep learning-based systems. Interpretation models
enable the understanding of the inner workings of deep learning models and
offer a sense of security in detecting the misuse of artifacts in the input
data. Similar to prediction models, interpretation models are also susceptible
to adversarial inputs. This work introduces two attacks, AdvEdge and
AdvEdge$^{+}$, that deceive both the target deep learning model and the coupled
interpretation model. We assess the effectiveness of proposed attacks against
two deep learning model architectures coupled with four interpretation models
that represent different categories of interpretation models. Our experiments
include the attack implementation using various attack frameworks. We also
explore the potential countermeasures against such attacks. Our analysis shows
the effectiveness of our attacks in terms of deceiving the deep learning models
and their interpreters, and highlights insights to improve and circumvent the
attacks.
</p></li>
</ul>

<h2>extraction</h2>
<h3>Title: Zero-Shot Opinion Summarization with GPT-3. (arXiv:2211.15914v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15914">http://arxiv.org/abs/2211.15914</a></li>
<li>Code URL: <a href="https://github.com/testzer0/zs-summ-gpt3">https://github.com/testzer0/zs-summ-gpt3</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15914] Zero-Shot Opinion Summarization with GPT-3](http://arxiv.org/abs/2211.15914) #extraction</code></li>
<li>Summary: <p>Very large language models such as GPT-3 have shown impressive performance
across a wide variety of tasks, including text summarization. In this paper, we
show that this strong performance extends to opinion summarization. We explore
several pipeline methods for applying GPT-3 to summarize a large collection of
user reviews in a zero-shot fashion, notably approaches based on recursive
summarization and selecting salient content to summarize through supervised
clustering or extraction. On two datasets, an aspect-oriented summarization
dataset of hotel reviews and a generic summarization dataset of Amazon and Yelp
reviews, we show that the GPT-3 models achieve very strong performance in human
evaluation. We argue that standard evaluation metrics do not reflect this, and
evaluate against several new measures targeting faithfulness, factuality, and
genericity to contrast these different methods.
</p></li>
</ul>

<h3>Title: Towards Generalized Open Information Extraction. (arXiv:2211.15987v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15987">http://arxiv.org/abs/2211.15987</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15987] Towards Generalized Open Information Extraction](http://arxiv.org/abs/2211.15987) #extraction</code></li>
<li>Summary: <p>Open Information Extraction (OpenIE) facilitates the open-domain discovery of
textual facts. However, the prevailing solutions evaluate OpenIE models on
in-domain test sets aside from the training corpus, which certainly violates
the initial task principle of domain-independence. In this paper, we propose to
advance OpenIE towards a more realistic scenario: generalizing over unseen
target domains with different data distributions from the source training
domains, termed Generalized OpenIE. For this purpose, we first introduce GLOBE,
a large-scale human-annotated multi-domain OpenIE benchmark, to examine the
robustness of recent OpenIE models to domain shifts, and the relative
performance degradation of up to 70% implies the challenges of generalized
OpenIE. Then, we propose DragonIE, which explores a minimalist graph expression
of textual fact: directed acyclic graph, to improve the OpenIE generalization.
Extensive experiments demonstrate that DragonIE beats the previous methods in
both in-domain and out-of-domain settings by as much as 6.0% in F1 score
absolutely, but there is still ample room for improvement.
</p></li>
</ul>

<h3>Title: Deep Semi-supervised Learning with Double-Contrast of Features and Semantics. (arXiv:2211.15671v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15671">http://arxiv.org/abs/2211.15671</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15671] Deep Semi-supervised Learning with Double-Contrast of Features and Semantics](http://arxiv.org/abs/2211.15671) #extraction</code></li>
<li>Summary: <p>In recent years, the field of intelligent transportation systems (ITS) has
achieved remarkable success, which is mainly due to the large amount of
available annotation data. However, obtaining these annotated data has to
afford expensive costs in reality. Therefore, a more realistic strategy is to
leverage semi-supervised learning (SSL) with a small amount of labeled data and
a large amount of unlabeled data. Typically, semantic consistency
regularization and the two-stage learning methods of decoupling feature
extraction and classification have been proven effective. Nevertheless,
representation learning only limited to semantic consistency regularization may
not guarantee the separation or discriminability of representations of samples
with different semantics; due to the inherent limitations of the two-stage
learning methods, the extracted features may not match the specific downstream
tasks. In order to deal with the above drawbacks, this paper proposes an
end-to-end deep semi-supervised learning double contrast of semantic and
feature, which extracts effective tasks specific discriminative features by
contrasting the semantics/features of positive and negative augmented samples
pairs. Moreover, we leverage information theory to explain the rationality of
double contrast of semantics and features and slack mutual information to
contrastive loss in a simpler way. Finally, the effectiveness of our method is
verified in benchmark datasets.
</p></li>
</ul>

<h3>Title: Text Representation Enrichment Utilizing Graph based Approaches: Stock Market Technical Analysis Case Study. (arXiv:2211.16103v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16103">http://arxiv.org/abs/2211.16103</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16103] Text Representation Enrichment Utilizing Graph based Approaches: Stock Market Technical Analysis Case Study](http://arxiv.org/abs/2211.16103) #extraction</code></li>
<li>Summary: <p>Graph neural networks (GNNs) have been utilized for various natural language
processing (NLP) tasks lately. The ability to encode corpus-wide features in
graph representation made GNN models popular in various tasks such as document
classification. One major shortcoming of such models is that they mainly work
on homogeneous graphs, while representing text datasets as graphs requires
several node types which leads to a heterogeneous schema. In this paper, we
propose a transductive hybrid approach composed of an unsupervised node
representation learning model followed by a node classification/edge prediction
model. The proposed model is capable of processing heterogeneous graphs to
produce unified node embeddings which are then utilized for node classification
or link prediction as the downstream task. The proposed model is developed to
classify stock market technical analysis reports, which to our knowledge is the
first work in this domain. Experiments, which are carried away using a
constructed dataset, demonstrate the ability of the model in embedding
extraction and the downstream tasks.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Adap DP-FL: Differentially Private Federated Learning with Adaptive Noise. (arXiv:2211.15893v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15893">http://arxiv.org/abs/2211.15893</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15893] Adap DP-FL: Differentially Private Federated Learning with Adaptive Noise](http://arxiv.org/abs/2211.15893) #federate</code></li>
<li>Summary: <p>Federated learning seeks to address the issue of isolated data islands by
making clients disclose only their local training models. However, it was
demonstrated that private information could still be inferred by analyzing
local model parameters, such as deep neural network model weights. Recently,
differential privacy has been applied to federated learning to protect data
privacy, but the noise added may degrade the learning performance much.
Typically, in previous work, training parameters were clipped equally and
noises were added uniformly. The heterogeneity and convergence of training
parameters were simply not considered. In this paper, we propose a
differentially private scheme for federated learning with adaptive noise (Adap
DP-FL). Specifically, due to the gradient heterogeneity, we conduct adaptive
gradient clipping for different clients and different rounds; due to the
gradient convergence, we add decreasing noises accordingly. Extensive
experiments on real-world datasets demonstrate that our Adap DP-FL outperforms
previous methods significantly.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: SLAN: Self-Locator Aided Network for Cross-Modal Understanding. (arXiv:2211.16208v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16208">http://arxiv.org/abs/2211.16208</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16208] SLAN: Self-Locator Aided Network for Cross-Modal Understanding](http://arxiv.org/abs/2211.16208) #fair</code></li>
<li>Summary: <p>Learning fine-grained interplay between vision and language allows to a more
accurate understanding for VisionLanguage tasks. However, it remains
challenging to extract key image regions according to the texts for semantic
alignments. Most existing works are either limited by textagnostic and
redundant regions obtained with the frozen detectors, or failing to scale
further due to its heavy reliance on scarce grounding (gold) data to pre-train
detectors. To solve these problems, we propose Self-Locator Aided Network
(SLAN) for cross-modal understanding tasks without any extra gold data. SLAN
consists of a region filter and a region adaptor to localize regions of
interest conditioned on different texts. By aggregating cross-modal
information, the region filter selects key regions and the region adaptor
updates their coordinates with text guidance. With detailed region-word
alignments, SLAN can be easily generalized to many downstream tasks. It
achieves fairly competitive results on five cross-modal understanding tasks
(e.g., 85.7% and 69.2% on COCO image-to-text and text-to-image retrieval,
surpassing previous SOTA methods). SLAN also demonstrates strong zero-shot and
fine-tuned transferability to two localization tasks.
</p></li>
</ul>

<h3>Title: Towards faster settlement in HTLC-based Cross-Chain Atomic Swaps. (arXiv:2211.15804v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15804">http://arxiv.org/abs/2211.15804</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15804] Towards faster settlement in HTLC-based Cross-Chain Atomic Swaps](http://arxiv.org/abs/2211.15804) #fair</code></li>
<li>Summary: <p>Hashed Timelock (HTLC)-based atomic swap protocols enable the exchange of
coins between two or more parties without relying on a trusted entity. This
protocol is like the American call option without premium. It allows the
finalization of a deal within a certain period. This puts the swap initiator at
liberty to delay before deciding to proceed with the deal. If she finds the
deal unprofitable, she just waits for the time-period of the contract to
elapse. However, the counterparty is at a loss since his assets remain locked
in the contract. The best he can do is to predict the initiator's behavior
based on the asset's price fluctuation in the future. But it is difficult to
predict as cryptocurrencies are quite volatile, and their price fluctuates
abruptly. We perform a game theoretic analysis of HTLC-based atomic cross-chain
swap to predict whether a swap will succeed or not. From the strategic behavior
of the players, we infer that this model lacks fairness. We propose Quick Swap,
a two-party protocol based on hashlock and timelock that fosters faster
settlement of the swap. The parties are required to lock griefing-premium along
with the principal amount. If the party griefs, he ends up paying the
griefing-premium. If a party finds a deal unfavorable, he has the provision to
cancel the swap. We prove that Quick Swap is more participant-friendly than
HTLC-based atomic swap. Our work is the first to propose a protocol to ensure
fairness of atomic-swap in a cyclic multi-party setting.
</p></li>
</ul>

<h3>Title: An Empirical Study on Snapshot DAOs. (arXiv:2211.15993v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15993">http://arxiv.org/abs/2211.15993</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15993] An Empirical Study on Snapshot DAOs](http://arxiv.org/abs/2211.15993) #fair</code></li>
<li>Summary: <p>Decentralized Autonomous Organization (DAO) is an organization constructed by
automatically executed rules such as via smart contracts, holding features of
the permissionless committee, transparent proposals, and fair contribution by
stakeholders. As of Nov 2022, DAO has impacted over \$11.2B market caps.
However, there are no substantial studies focused on this emerging field. To
fill the gap, we start from the ground truth by empirically studying the
breadth and depth of the DAO markets in mainstream public chain ecosystems in
this paper. We dive into the most widely adoptable DAO launchpad,
\textit{Snapshot}, which covers 95\% in the wild DAO projects for data
collection and analysis. By integrating extensive enrolled DAOs and
corresponding data measurements, we explore statistical data from Snapshot and
try to demystify its undiscovered truths by delivering a series of summarised
insights. We also present DAO status, patterns, distribution, and trends. To
our knowledge, this is the first empirical study putting concentration on DAO
spaces.
</p></li>
</ul>

<h3>Title: Learning Antidote Data to Individual Unfairness. (arXiv:2211.15897v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15897">http://arxiv.org/abs/2211.15897</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15897] Learning Antidote Data to Individual Unfairness](http://arxiv.org/abs/2211.15897) #fair</code></li>
<li>Summary: <p>Fairness is an essential factor for machine learning systems deployed in
high-stake applications. Among all fairness notions, individual fairness,
following a consensus that `similar individuals should be treated similarly,'
is a vital notion to guarantee fair treatment for individual cases. Previous
methods typically characterize individual fairness as a prediction-invariant
problem when perturbing sensitive attributes, and solve it by adopting the
Distributionally Robust Optimization (DRO) paradigm. However, adversarial
perturbations along a direction covering sensitive information do not consider
the inherent feature correlations or innate data constraints, and thus mislead
the model to optimize at off-manifold and unrealistic samples. In light of
this, we propose a method to learn and generate antidote data that
approximately follows the data distribution to remedy individual unfairness.
These on-manifold antidote data can be used through a generic optimization
procedure with original training data, resulting in a pure pre-processing
approach to individual unfairness, or can also fit well with the in-processing
DRO paradigm. Through extensive experiments, we demonstrate our antidote data
resists individual unfairness at a minimal or zero cost to the model's
predictive utility.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>explainability</h2>
<h3>Title: G-CMP: Graph-enhanced Contextual Matrix Profile for unsupervised anomaly detection in sensor-based remote health monitoring. (arXiv:2211.16122v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16122">http://arxiv.org/abs/2211.16122</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16122] G-CMP: Graph-enhanced Contextual Matrix Profile for unsupervised anomaly detection in sensor-based remote health monitoring](http://arxiv.org/abs/2211.16122) #explainability</code></li>
<li>Summary: <p>Sensor-based remote health monitoring is used in industrial, urban and
healthcare settings to monitor ongoing operation of equipment and human health.
An important aim is to intervene early if anomalous events or adverse health is
detected. In the wild, these anomaly detection approaches are challenged by
noise, label scarcity, high dimensionality, explainability and wide variability
in operating environments. The Contextual Matrix Profile (CMP) is a
configurable 2-dimensional version of the Matrix Profile (MP) that uses the
distance matrix of all subsequences of a time series to discover patterns and
anomalies. The CMP is shown to enhance the effectiveness of the MP and other
SOTA methods at detecting, visualising and interpreting true anomalies in noisy
real world data from different domains. It excels at zooming out and
identifying temporal patterns at configurable time scales. However, the CMP
does not address cross-sensor information, and cannot scale to high dimensional
data. We propose a novel, self-supervised graph-based approach for temporal
anomaly detection that works on context graphs generated from the CMP distance
matrix. The learned graph embeddings encode the anomalous nature of a time
context. In addition, we evaluate other graph outlier algorithms for the same
task. Given our pipeline is modular, graph construction, generation of graph
embeddings, and pattern recognition logic can all be chosen based on the
specific pattern detection application. We verified the effectiveness of
graph-based anomaly detection and compared it with the CMP and 3 state-of-the
art methods on two real-world healthcare datasets with different anomalies. Our
proposed method demonstrated better recall, alert rate and generalisability.
</p></li>
</ul>

<h3>Title: Design Space Exploration and Explanation via Conditional Variational Autoencoders in Meta-model-based Conceptual Design of Pedestrian Bridges. (arXiv:2211.16406v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16406">http://arxiv.org/abs/2211.16406</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16406] Design Space Exploration and Explanation via Conditional Variational Autoencoders in Meta-model-based Conceptual Design of Pedestrian Bridges](http://arxiv.org/abs/2211.16406) #explainability</code></li>
<li>Summary: <p>For conceptual design, engineers rely on conventional iterative (often
manual) techniques. Emerging parametric models facilitate design space
exploration based on quantifiable performance metrics, yet remain
time-consuming and computationally expensive. Pure optimisation methods,
however, ignore qualitative aspects (e.g. aesthetics or construction methods).
This paper provides a performance-driven design exploration framework to
augment the human designer through a Conditional Variational Autoencoder
(CVAE), which serves as forward performance predictor for given design features
as well as an inverse design feature predictor conditioned on a set of
performance requests. The CVAE is trained on 18'000 synthetically generated
instances of a pedestrian bridge in Switzerland. Sensitivity analysis is
employed for explainability and informing designers about (i) relations of the
model between features and/or performances and (ii) structural improvements
under user-defined objectives. A case study proved our framework's potential to
serve as a future co-pilot for conceptual design studies of pedestrian bridges
and beyond.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Post-training Quantization on Diffusion Models. (arXiv:2211.15736v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.15736">http://arxiv.org/abs/2211.15736</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.15736] Post-training Quantization on Diffusion Models](http://arxiv.org/abs/2211.15736) #diffusion</code></li>
<li>Summary: <p>Denoising diffusion (score-based) generative models have recently achieved
significant accomplishments in generating realistic and diverse data. These
approaches define a forward diffusion process for transforming data into noise
and a backward denoising process for sampling data from noise. Unfortunately,
the generation process of current denoising diffusion models is notoriously
slow due to the lengthy iterative noise estimations, which rely on cumbersome
neural networks. It prevents the diffusion models from being widely deployed,
especially on edge devices. Previous works accelerate the generation process of
diffusion model (DM) via finding shorter yet effective sampling trajectories.
However, they overlook the cost of noise estimation with a heavy network in
every iteration. In this work, we accelerate generation from the perspective of
compressing the noise estimation network. Due to the difficulty of retraining
DMs, we exclude mainstream training-aware compression paradigms and introduce
post-training quantization (PTQ) into DM acceleration. However, the output
distributions of noise estimation networks change with time-step, making
previous PTQ methods fail in DMs since they are designed for single-time step
scenarios. To devise a DM-specific PTQ method, we explore PTQ on DM in three
aspects: quantized operations, calibration dataset, and calibration metric. We
summarize and use several observations derived from all-inclusive
investigations to formulate our method, which especially targets the unique
multi-time-step structure of DMs. Experimentally, our method can directly
quantize full-precision DMs into 8-bit models while maintaining or even
improving their performance in a training-free manner. Importantly, our method
can serve as a plug-and-play module on other fast-sampling methods, e.g., DDIM.
</p></li>
</ul>

<h3>Title: UDE: A Unified Driving Engine for Human Motion Generation. (arXiv:2211.16016v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16016">http://arxiv.org/abs/2211.16016</a></li>
<li>Code URL: <a href="https://github.com/zixiangzhou916/ude">https://github.com/zixiangzhou916/ude</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16016] UDE: A Unified Driving Engine for Human Motion Generation](http://arxiv.org/abs/2211.16016) #diffusion</code></li>
<li>Summary: <p>Generating controllable and editable human motion sequences is a key
challenge in 3D Avatar generation. It has been labor-intensive to generate and
animate human motion for a long time until learning-based approaches have been
developed and applied recently. However, these approaches are still
task-specific or modality-specific\cite
{ahuja2019language2pose}\cite{ghosh2021synthesis}\cite{ferreira2021learning}\cite{li2021ai}.
In this paper, we propose ``UDE", the first unified driving engine that enables
generating human motion sequences from natural language or audio sequences (see
Fig.~\ref{fig:teaser}). Specifically, UDE consists of the following key
components: 1) a motion quantization module based on VQVAE that represents
continuous motion sequence as discrete latent code\cite{van2017neural}, 2) a
modality-agnostic transformer encoder\cite{vaswani2017attention} that learns to
map modality-aware driving signals to a joint space, and 3) a unified token
transformer (GPT-like\cite{radford2019language}) network to predict the
quantized latent code index in an auto-regressive manner. 4) a diffusion motion
decoder that takes as input the motion tokens and decodes them into motion
sequences with high diversity. We evaluate our method on
HumanML3D\cite{Guo_2022_CVPR} and AIST++\cite{li2021learn} benchmarks, and the
experiment results demonstrate our method achieves state-of-the-art
performance. Project website: \url{https://github.com/zixiangzhou916/UDE/
</p></li>
</ul>

<h3>Title: Dimensionality-Varying Diffusion Process. (arXiv:2211.16032v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16032">http://arxiv.org/abs/2211.16032</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16032] Dimensionality-Varying Diffusion Process](http://arxiv.org/abs/2211.16032) #diffusion</code></li>
<li>Summary: <p>Diffusion models, which learn to reverse a signal destruction process to
generate new data, typically require the signal at each step to have the same
dimension. We argue that, considering the spatial redundancy in image signals,
there is no need to maintain a high dimensionality in the evolution process,
especially in the early generation phase. To this end, we make a theoretical
generalization of the forward diffusion process via signal decomposition.
Concretely, we manage to decompose an image into multiple orthogonal components
and control the attenuation of each component when perturbing the image. That
way, along with the noise strength increasing, we are able to diminish those
inconsequential components and thus use a lower-dimensional signal to represent
the source, barely losing information. Such a reformulation allows to vary
dimensions in both training and inference of diffusion models. Extensive
experiments on a range of datasets suggest that our approach substantially
reduces the computational cost and achieves on-par or even better synthesis
performance compared to baseline methods. We also show that our strategy
facilitates high-resolution image synthesis and improves FID of diffusion model
trained on FFHQ at $1024\times1024$ resolution from 52.40 to 10.46. Code and
models will be made publicly available.
</p></li>
</ul>

<h3>Title: Wavelet Diffusion Models are fast and scalable Image Generators. (arXiv:2211.16152v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16152">http://arxiv.org/abs/2211.16152</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16152] Wavelet Diffusion Models are fast and scalable Image Generators](http://arxiv.org/abs/2211.16152) #diffusion</code></li>
<li>Summary: <p>Diffusion models are rising as a powerful solution for high-fidelity image
generation, which exceeds GANs in quality in many circumstances. However, their
slow training and inference speed is a huge bottleneck, blocking them from
being used in real-time applications. A recent DiffusionGAN method
significantly decreases the models' running time by reducing the number of
sampling steps from thousands to several, but their speeds still largely lag
behind the GAN counterparts. This paper aims to reduce the speed gap by
proposing a novel wavelet-based diffusion structure. We extract low-and-high
frequency components from both image and feature levels via wavelet
decomposition and adaptively handle these components for faster processing
while maintaining good generation quality. Furthermore, we propose to use a
reconstruction term, which effectively boosts the model training convergence.
Experimental results on CelebA-HQ, CIFAR-10, LSUN-Church, and STL-10 datasets
prove our solution is a stepping-stone to offering real-time and high-fidelity
diffusion models. Our code and pre-trained checkpoints will be available at
\url{https://github.com/VinAIResearch/WaveDiff.git}.
</p></li>
</ul>

<h3>Title: Ada3Diff: Defending against 3D Adversarial Point Clouds via Adaptive Diffusion. (arXiv:2211.16247v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16247">http://arxiv.org/abs/2211.16247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16247] Ada3Diff: Defending against 3D Adversarial Point Clouds via Adaptive Diffusion](http://arxiv.org/abs/2211.16247) #diffusion</code></li>
<li>Summary: <p>Deep 3D point cloud models are sensitive to adversarial attacks, which poses
threats to safety-critical applications such as autonomous driving. Robust
training and defend-by-denoise are typical strategies for defending adversarial
perturbations, including adversarial training and statistical filtering,
respectively. However, they either induce massive computational overhead or
rely heavily upon specified noise priors, limiting generalized robustness
against attacks of all kinds. This paper introduces a new defense mechanism
based on denoising diffusion models that can adaptively remove diverse noises
with a tailored intensity estimator. Specifically, we first estimate
adversarial distortions by calculating the distance of the points to their
neighborhood best-fit plane. Depending on the distortion degree, we choose
specific diffusion time steps for the input point cloud and perform the forward
diffusion to disrupt potential adversarial shifts. Then we conduct the reverse
denoising process to restore the disrupted point cloud back to a clean
distribution. This approach enables effective defense against adaptive attacks
with varying noise budgets, achieving accentuated robustness of existing 3D
deep recognition models.
</p></li>
</ul>

<h3>Title: DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model. (arXiv:2211.16374v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16374">http://arxiv.org/abs/2211.16374</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16374] DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model](http://arxiv.org/abs/2211.16374) #diffusion</code></li>
<li>Summary: <p>Recent 3D generative models have achieved remarkable performance in
synthesizing high resolution photorealistic images with view consistency and
detailed 3D shapes, but training them for diverse domains is challenging since
it requires massive training images and their camera distribution information.
Text-guided domain adaptation methods have shown impressive performance on
converting the 2D generative model on one domain into the models on other
domains with different styles by leveraging the CLIP (Contrastive
Language-Image Pre-training), rather than collecting massive datasets for those
domains. However, one drawback of them is that the sample diversity in the
original generative model is not well-preserved in the domain-adapted
generative models due to the deterministic nature of the CLIP text encoder.
Text-guided domain adaptation will be even more challenging for 3D generative
models not only because of catastrophic diversity loss, but also because of
inferior text-image correspondence and poor image quality. Here we propose
DATID-3D, a domain adaptation method tailored for 3D generative models using
text-to-image diffusion models that can synthesize diverse images per text
prompt without collecting additional images and camera information for the
target domain. Unlike 3D extensions of prior text-guided domain adaptation
methods, our novel pipeline was able to fine-tune the state-of-the-art 3D
generator of the source domain to synthesize high resolution, multi-view
consistent images in text-guided targeted domains without additional data,
outperforming the existing text-guided domain adaptation methods in diversity
and text-image correspondence. Furthermore, we propose and demonstrate diverse
3D image manipulations such as one-shot instance-selected adaptation and
single-view manipulated 3D reconstruction to fully enjoy diversity in text.
</p></li>
</ul>

<h3>Title: NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360{\deg} Views. (arXiv:2211.16431v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16431">http://arxiv.org/abs/2211.16431</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16431] NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360{\deg} Views](http://arxiv.org/abs/2211.16431) #diffusion</code></li>
<li>Summary: <p>Virtual reality and augmented reality (XR) bring increasing demand for 3D
content. However, creating high-quality 3D content requires tedious work that a
human expert must do. In this work, we study the challenging task of lifting a
single image to a 3D object and, for the first time, demonstrate the ability to
generate a plausible 3D object with 360{\deg} views that correspond well with
the given reference image. By conditioning on the reference image, our model
can fulfill the everlasting curiosity for synthesizing novel views of objects
from images. Our technique sheds light on a promising direction of easing the
workflows for 3D artists and XR designers. We propose a novel framework, dubbed
NeuralLift-360, that utilizes a depth-aware neural radiance representation
(NeRF) and learns to craft the scene guided by denoising diffusion models. By
introducing a ranking loss, our NeuralLift-360 can be guided with rough depth
estimation in the wild. We also adopt a CLIP-guided sampling strategy for the
diffusion prior to provide coherent guidance. Extensive experiments demonstrate
that our NeuralLift-360 significantly outperforms existing state-of-the-art
baselines. Project page: https://vita-group.github.io/NeuralLift-360/
</p></li>
</ul>

<h3>Title: DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion models. (arXiv:2211.16487v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.16487">http://arxiv.org/abs/2211.16487</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.16487] DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion models](http://arxiv.org/abs/2211.16487) #diffusion</code></li>
<li>Summary: <p>Traditionally, monocular 3D human pose estimation employs a machine learning
model to predict the most likely 3D pose for a given input image. However, a
single image can be highly ambiguous and induces multiple plausible solutions
for the 2D-3D lifting step which results in overly confident 3D pose
predictors. To this end, we propose \emph{DiffPose}, a conditional diffusion
model, that predicts multiple hypotheses for a given input image. In comparison
to similar approaches, our diffusion model is straightforward and avoids
intensive hyperparameter tuning, complex network structures, mode collapse, and
unstable training. Moreover, we tackle a problem of the common two-step
approach that first estimates a distribution of 2D joint locations via
joint-wise heatmaps and consecutively approximates them based on first- or
second-moment statistics. Since such a simplification of the heatmaps removes
valid information about possibly correct, though labeled unlikely, joint
locations, we propose to represent the heatmaps as a set of 2D joint candidate
samples. To extract information about the original distribution from these
samples we introduce our \emph{embedding transformer} that conditions the
diffusion model. Experimentally, we show that DiffPose slightly improves upon
the state of the art for multi-hypothesis pose estimation for simple poses and
outperforms it by a large margin for highly ambiguous poses.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
