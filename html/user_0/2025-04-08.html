<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-08</h1>
<h3>Title: SAFE: Self-Adjustment Federated Learning Framework for Remote Sensing Collaborative Perception</h3>
<ul>
<li><strong>Authors: </strong>Xiaohe Li, Haohua Wu, Jiahao Li, Zide Fan, Kaixin Zhang, Xinming Li, Yunping Ge, Xinyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03700">https://arxiv.org/abs/2504.03700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03700">https://arxiv.org/pdf/2504.03700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03700]] SAFE: Self-Adjustment Federated Learning Framework for Remote Sensing Collaborative Perception(https://arxiv.org/abs/2504.03700)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, segmentation</a></li>
<li><strong>Abstract: </strong>The rapid increase in remote sensing satellites has led to the emergence of distributed space-based observation systems. However, existing distributed remote sensing models often rely on centralized training, resulting in data leakage, communication overhead, and reduced accuracy due to data distribution discrepancies across platforms. To address these challenges, we propose the \textit{Self-Adjustment FEderated Learning} (SAFE) framework, which innovatively leverages federated learning to enhance collaborative sensing in remote sensing scenarios. SAFE introduces four key strategies: (1) \textit{Class Rectification Optimization}, which autonomously addresses class imbalance under unknown local and global distributions. (2) \textit{Feature Alignment Update}, which mitigates Non-IID data issues via locally controlled EMA updates. (3) \textit{Dual-Factor Modulation Rheostat}, which dynamically balances optimization effects during training. (4) \textit{Adaptive Context Enhancement}, which is designed to improve model performance by dynamically refining foreground regions, ensuring computational efficiency with accuracy improvement across distributed satellites. Experiments on real-world image classification and object segmentation datasets validate the effectiveness and reliability of the SAFE framework in complex remote sensing scenarios.</li>
</ul>

<h3>Title: Semi-supervised learning for marine anomaly detection on board satellites</h3>
<ul>
<li><strong>Authors: </strong>Luca Marini</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03705">https://arxiv.org/abs/2504.03705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03705">https://arxiv.org/pdf/2504.03705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03705]] Semi-supervised learning for marine anomaly detection on board satellites(https://arxiv.org/abs/2504.03705)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Aquatic bodies face numerous environmental threats caused by several marine anomalies. Marine debris can devastate habitats and endanger marine life through entanglement, while harmful algal blooms can produce toxins that negatively affect marine ecosystems. Additionally, ships may discharge oil or engage in illegal and overfishing activities, causing further harm. These marine anomalies can be identified by applying trained deep learning (DL) models on multispectral satellite imagery. Furthermore, the detection of other anomalies, such as clouds, could be beneficial in filtering out irrelevant images. However, DL models often require a large volume of labeled data for training, which can be both costly and time-consuming, particularly for marine anomaly detection where expert annotation is needed. A potential solution is the use of semi-supervised learning methods, which can also utilize unlabeled data. In this project, we implement and study the performance of FixMatch for Semantic Segmentation, a semi-supervised algorithm for semantic segmentation. Firstly, we found that semi-supervised models perform best with a high confidence threshold of 0.9 when there is a limited amount of labeled data. Secondly, we compare the performance of semi-supervised models with fully-supervised models under varying amounts of labeled data. Our findings suggest that semi-supervised models outperform fully-supervised models with limited labeled data, while fully-supervised models have a slightly better performance with larger volumes of labeled data. We propose two hypotheses to explain why fully-supervised models surpass semi-supervised ones when a high volume of labeled data is used. All of our experiments were conducted using a U-Net model architecture with a limited number of parameters to ensure compatibility with space-rated hardware.</li>
</ul>

<h3>Title: Geometric Flow Models over Neural Network Weights</h3>
<ul>
<li><strong>Authors: </strong>Ege Erdogan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03710">https://arxiv.org/abs/2504.03710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03710">https://arxiv.org/pdf/2504.03710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03710]] Geometric Flow Models over Neural Network Weights(https://arxiv.org/abs/2504.03710)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Deep generative models such as flow and diffusion models have proven to be effective in modeling high-dimensional and complex data types such as videos or proteins, and this has motivated their use in different data modalities, such as neural network weights. A generative model of neural network weights would be useful for a diverse set of applications, such as Bayesian deep learning, learned optimization, and transfer learning. However, the existing work on weight-space generative models often ignores the symmetries of neural network weights, or only takes into account a subset of them. Modeling those symmetries, such as permutation symmetries between subsequent layers in an MLP, the filters in a convolutional network, or scaling symmetries arising with the use of non-linear activations, holds the potential to make weight-space generative modeling more efficient by effectively reducing the dimensionality of the problem. In this light, we aim to design generative models in weight-space that more comprehensively respect the symmetries of neural network weights. We build on recent work on generative modeling with flow matching, and weight-space graph neural networks to design three different weight-space flows. Each of our flows takes a different approach to modeling the geometry of neural network weights, and thus allows us to explore the design space of weight-space flows in a principled way. Our results confirm that modeling the geometry of neural networks more faithfully leads to more effective flow models that can generalize to different tasks and architectures, and we show that while our flows obtain competitive performance with orders of magnitude fewer parameters than previous work, they can be further improved by scaling them up. We conclude by listing potential directions for future work on weight-space generative models.</li>
</ul>

<h3>Title: RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack</h3>
<ul>
<li><strong>Authors: </strong>Weichen Dai, Zijie Dai, Zhijie Huang, Yixuan Pan, Xinhe Li, Xi Li, Yi Zhou, Ji Qi, Wu Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03713">https://arxiv.org/abs/2504.03713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03713">https://arxiv.org/pdf/2504.03713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03713]] RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack(https://arxiv.org/abs/2504.03713)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While current large language models (LLMs) demonstrate remarkable linguistic capabilities through training on massive unstructured text corpora, they remain inadequate in leveraging structured scientific data (e.g., chemical molecular properties in databases) that encapsulate centuries of accumulated scientific expertise. These structured datasets hold strategic significance for advancing AI for Science yet current approaches merely treat them as auxiliary supplements to unstructured text. This study pioneers a systematic investigation into enhancing LLMs with structured scientific data, using chemical molecular science as a testbed. We investigate the impact of incorporating molecular property data on LLM across distinct training phases, including continual pre-training, supervised fine-tuning, and reinforcement learning. Notably, to address the inherent limitation of numerical insensitivity in large models, we propose an innovative methodology termed "Reinforcement Learning with Database Feedback" (RLDBF). Experimental evaluations demonstrate the efficacy of the proposed approach, with the model exhibiting remarkable generalization capabilities on previously unseen data and other chemical tasks. The results substantiate the potential of our method in advancing the field of structured scientific data processing within LLMs.</li>
</ul>

<h3>Title: Breach in the Shield: Unveiling the Vulnerabilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Runpeng Dai, Run Yang, Fan Zhou, Hongtu Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03714">https://arxiv.org/abs/2504.03714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03714">https://arxiv.org/pdf/2504.03714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03714]] Breach in the Shield: Unveiling the Vulnerabilities of Large Language Models(https://arxiv.org/abs/2504.03714)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) and Vision-Language Models (VLMs) have become essential to general artificial intelligence, exhibiting remarkable capabilities in task understanding and problem-solving. However, the real-world reliability of these models critically depends on their stability, which remains an underexplored area. Despite their widespread use, rigorous studies examining the stability of LLMs under various perturbations are still lacking. In this paper, we address this gap by proposing a novel stability measure for LLMs, inspired by statistical methods rooted in information geometry. Our measure possesses desirable invariance properties, making it well-suited for analyzing model sensitivity to both parameter and input perturbations. To assess the effectiveness of our approach, we conduct extensive experiments on models ranging in size from 1.5B to 13B parameters. Our results demonstrate the utility of our measure in identifying salient parameters and detecting vulnerable regions in input images or critical dimensions in token embeddings. Furthermore, leveraging our stability framework, we enhance model robustness during model merging, leading to improved performance.</li>
</ul>

<h3>Title: Ethical AI on the Waitlist: Group Fairness Evaluation of LLM-Aided Organ Allocation</h3>
<ul>
<li><strong>Authors: </strong>Hannah Murray, Brian Hyeongseok Kim, Isabelle Lee, Jason Byun, Dani Yogatama, Evi Micha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03716">https://arxiv.org/abs/2504.03716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03716">https://arxiv.org/pdf/2504.03716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03716]] Ethical AI on the Waitlist: Group Fairness Evaluation of LLM-Aided Organ Allocation(https://arxiv.org/abs/2504.03716)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are becoming ubiquitous, promising automation even in high-stakes scenarios. However, existing evaluation methods often fall short -- benchmarks saturate, accuracy-based metrics are overly simplistic, and many inherently ambiguous problems lack a clear ground truth. Given these limitations, evaluating fairness becomes complex. To address this, we reframe fairness evaluation using Borda scores, a method from voting theory, as a nuanced yet interpretable metric for measuring fairness. Using organ allocation as a case study, we introduce two tasks: (1) Choose-One and (2) Rank-All. In Choose-One, LLMs select a single candidate for a kidney, and we assess fairness across demographics using proportional parity. In Rank-All, LLMs rank all candidates for a kidney, reflecting real-world allocation processes. Since traditional fairness metrics do not account for ranking, we propose a novel application of Borda scoring to capture biases. Our findings highlight the potential of voting-based metrics to provide a richer, more multifaceted evaluation of LLM fairness.</li>
</ul>

<h3>Title: RaanA: A Fast, Flexible, and Data-Efficient Post-Training Quantization Algorithm</h3>
<ul>
<li><strong>Authors: </strong>Yongyi Yang, Jianyang Gao, Wei Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03717">https://arxiv.org/abs/2504.03717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03717">https://arxiv.org/pdf/2504.03717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03717]] RaanA: A Fast, Flexible, and Data-Efficient Post-Training Quantization Algorithm(https://arxiv.org/abs/2504.03717)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Post-training Quantization (PTQ) has become a widely used technique for improving inference efficiency of large language models (LLMs). However, existing PTQ methods generally suffer from crucial limitations such as heavy calibration data requirements and inflexible choice of target number of bits. In this paper, we propose RaanA, a unified PTQ framework that overcomes these challenges by introducing two novel components: 1) RaBitQ-H, a variant of a randomized vector quantization method RaBitQ, designed for fast, accurate, and highly efficient quantization; and 2) AllocateBits, an algorithm that optimally allocates bit-widths across layers based on their quantization sensitivity. RaanA achieves competitive performance with state-of-the-art quantization methods while being extremely fast, requiring minimal calibration data, and enabling flexible bit allocation. Extensive experiments demonstrate RaanA's efficacy in balancing efficiency and accuracy. The code is publicly available at this https URL .</li>
</ul>

<h3>Title: Task-Aware Parameter-Efficient Fine-Tuning of Large Pre-Trained Models at the Edge</h3>
<ul>
<li><strong>Authors: </strong>Senkang Hu, Yanan Ma, Yihang Tao, Zhengru Fang, Zihan Fang, Yiqin Deng, Sam Kwong, Yuguang Fang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03718">https://arxiv.org/abs/2504.03718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03718">https://arxiv.org/pdf/2504.03718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03718]] Task-Aware Parameter-Efficient Fine-Tuning of Large Pre-Trained Models at the Edge(https://arxiv.org/abs/2504.03718)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable success in various tasks, such as decision-making, reasoning, and question answering. They have been widely used in edge devices. However, fine-tuning LLMs to specific tasks at the edge is challenging due to the high computational cost and the limited storage and energy resources at the edge. To address this issue, we propose TaskEdge, a task-aware parameter-efficient fine-tuning framework at the edge, which allocates the most effective parameters to the target task and only updates the task-specific parameters. Specifically, we first design a parameter importance calculation criterion that incorporates both weights and input activations into the computation of weight importance. Then, we propose a model-agnostic task-specific parameter allocation algorithm to ensure that task-specific parameters are distributed evenly across the model, rather than being concentrated in specific regions. In doing so, TaskEdge can significantly reduce the computational cost and memory usage while maintaining performance on the target downstream tasks by updating less than 0.1\% of the parameters. In addition, TaskEdge can be easily integrated with structured sparsity to enable acceleration by NVIDIA's specialized sparse tensor cores, and it can be seamlessly integrated with LoRA to enable efficient sparse low-rank adaptation. Extensive experiments on various tasks demonstrate the effectiveness of TaskEdge.</li>
</ul>

<h3>Title: Detecting Malicious AI Agents Through Simulated Interactions</h3>
<ul>
<li><strong>Authors: </strong>Yulu Pi, Ella Bettison, Anna Becker</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03726">https://arxiv.org/abs/2504.03726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03726">https://arxiv.org/pdf/2504.03726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03726]] Detecting Malicious AI Agents Through Simulated Interactions(https://arxiv.org/abs/2504.03726)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study investigates malicious AI Assistants' manipulative traits and whether the behaviours of malicious AI Assistants can be detected when interacting with human-like simulated users in various decision-making contexts. We also examine how interaction depth and ability of planning influence malicious AI Assistants' manipulative strategies and effectiveness. Using a controlled experimental design, we simulate interactions between AI Assistants (both benign and deliberately malicious) and users across eight decision-making scenarios of varying complexity and stakes. Our methodology employs two state-of-the-art language models to generate interaction data and implements Intent-Aware Prompting (IAP) to detect malicious AI Assistants. The findings reveal that malicious AI Assistants employ domain-specific persona-tailored manipulation strategies, exploiting simulated users' vulnerabilities and emotional triggers. In particular, simulated users demonstrate resistance to manipulation initially, but become increasingly vulnerable to malicious AI Assistants as the depth of the interaction increases, highlighting the significant risks associated with extended engagement with potentially manipulative systems. IAP detection methods achieve high precision with zero false positives but struggle to detect many malicious AI Assistants, resulting in high false negative rates. These findings underscore critical risks in human-AI interactions and highlight the need for robust, context-sensitive safeguards against manipulative AI behaviour in increasingly autonomous decision-support systems.</li>
</ul>

<h3>Title: Safeguarding Smart Inhaler Devices and Patient Privacy in Respiratory Health Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Asaju Babajide, Almustapha Wakili, Michaela Barnett, Lucas Potter, Xavier-Lewis Palmer, Woosub Jung</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03730">https://arxiv.org/abs/2504.03730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03730">https://arxiv.org/pdf/2504.03730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03730]] Safeguarding Smart Inhaler Devices and Patient Privacy in Respiratory Health Monitoring(https://arxiv.org/abs/2504.03730)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, robust</a></li>
<li><strong>Abstract: </strong>The rapid development of Internet of Things (IoT) technology has significantly impacted various market sectors. According to Li et al. (2024), an estimated 75 billion devices will be on the market in 2025. The healthcare industry is a target to improve patient care and ease healthcare provider burdens. Chronic respiratory disease is likely to benefit from their inclusion, with 545 million people worldwide recorded to suffer from patients using these devices to track their dosage. At the same time, healthcare providers can improve medication administration and monitor respiratory health (Soriano et al., 2020). While IoT medical devices offer numerous benefits, they also have security vulnerabilities that can expose patient data to cyberattacks. It's crucial to prioritize security measures in developing and deploying IoT medical devices, especially in personalized health monitoring systems for individuals with respiratory conditions. Efforts are underway to assess the security risks associated with intelligent inhalers and respiratory medical devices by understanding usability behavior and technological elements to identify and address vulnerabilities effectively. This work analyses usability behavior and technical vulnerabilities, emphasizing the confidentiality of information gained from Smart Inhalers. It then extrapolates to interrogate potential vulnerabilities with Implantable Medical Devices (IMDs). Our work explores the tensions in device development through the intersection of IoT technology and respiratory health, particularly in the context of intelligent inhalers and other breathing medical devices, calling for integrating robust security measures into the development and deployment of IoT devices to safeguard patient data and ensure the secure functioning of these critical healthcare technologies.</li>
</ul>

<h3>Title: Artificial Geographically Weighted Neural Network: A Novel Framework for Spatial Analysis with Geographically Weighted Layers</h3>
<ul>
<li><strong>Authors: </strong>Jianfei Cao, Dongchao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03734">https://arxiv.org/abs/2504.03734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03734">https://arxiv.org/pdf/2504.03734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03734]] Artificial Geographically Weighted Neural Network: A Novel Framework for Spatial Analysis with Geographically Weighted Layers(https://arxiv.org/abs/2504.03734)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Geographically Weighted Regression (GWR) is a widely recognized technique for modeling spatial heterogeneity. However, it is commonly assumed that the relationships between dependent and independent variables are linear. To overcome this limitation, we propose an Artificial Geographically Weighted Neural Network (AGWNN), a novel framework that integrates geographically weighted techniques with neural networks to capture complex nonlinear spatial relationships. Central to this framework is the Geographically Weighted Layer (GWL), a specialized component designed to encode spatial heterogeneity within the neural network architecture. To rigorously evaluate the performance of AGWNN, we conducted comprehensive experiments using both simulated datasets and real-world case studies. Our results demonstrate that AGWNN significantly outperforms traditional GWR and standard Artificial Neural Networks (ANNs) in terms of model fitting accuracy. Notably, AGWNN excels in modeling intricate nonlinear relationships and effectively identifies complex spatial heterogeneity patterns, offering a robust and versatile tool for advanced spatial analysis.</li>
</ul>

<h3>Title: Misaligned Roles, Misplaced Images: Structural Input Perturbations Expose Multimodal Alignment Blind Spots</h3>
<ul>
<li><strong>Authors: </strong>Erfan Shayegani, G M Shahariar, Sara Abdali, Lei Yu, Nael Abu-Ghazaleh, Yue Dong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03735">https://arxiv.org/abs/2504.03735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03735">https://arxiv.org/pdf/2504.03735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03735]] Misaligned Roles, Misplaced Images: Structural Input Perturbations Expose Multimodal Alignment Blind Spots(https://arxiv.org/abs/2504.03735)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Multimodal Language Models (MMLMs) typically undergo post-training alignment to prevent harmful content generation. However, these alignment stages focus primarily on the assistant role, leaving the user role unaligned, and stick to a fixed input prompt structure of special tokens, leaving the model vulnerable when inputs deviate from these expectations. We introduce Role-Modality Attacks (RMA), a novel class of adversarial attacks that exploit role confusion between the user and assistant and alter the position of the image token to elicit harmful outputs. Unlike existing attacks that modify query content, RMAs manipulate the input structure without altering the query itself. We systematically evaluate these attacks across multiple Vision Language Models (VLMs) on eight distinct settings, showing that they can be composed to create stronger adversarial prompts, as also evidenced by their increased projection in the negative refusal direction in the residual stream, a property observed in prior successful attacks. Finally, for mitigation, we propose an adversarial training approach that makes the model robust against input prompt perturbations. By training the model on a range of harmful and benign prompts all perturbed with different RMA settings, it loses its sensitivity to Role Confusion and Modality Manipulation attacks and is trained to only pay attention to the content of the query in the input prompt structure, effectively reducing Attack Success Rate (ASR) while preserving the model's general utility.</li>
</ul>

<h3>Title: Uncertainty Propagation in XAI: A Comparison of Analytical and Empirical Estimators</h3>
<ul>
<li><strong>Authors: </strong>Teodor Chiaburu, Felix Bießmann, Frank Haußer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03736">https://arxiv.org/abs/2504.03736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03736">https://arxiv.org/pdf/2504.03736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03736]] Uncertainty Propagation in XAI: A Comparison of Analytical and Empirical Estimators(https://arxiv.org/abs/2504.03736)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Understanding uncertainty in Explainable AI (XAI) is crucial for building trust and ensuring reliable decision-making in Machine Learning models. This paper introduces a unified framework for quantifying and interpreting Uncertainty in XAI by defining a general explanation function $e_{\theta}(x, f)$ that captures the propagation of uncertainty from key sources: perturbations in input data and model parameters. By using both analytical and empirical estimates of explanation variance, we provide a systematic means of assessing the impact uncertainty on explanations. We illustrate the approach using a first-order uncertainty propagation as the analytical estimator. In a comprehensive evaluation across heterogeneous datasets, we compare analytical and empirical estimates of uncertainty propagation and evaluate their robustness. Extending previous work on inconsistencies in explanations, our experiments identify XAI methods that do not reliably capture and propagate uncertainty. Our findings underscore the importance of uncertainty-aware explanations in high-stakes applications and offer new insights into the limitations of current XAI methods. The code for the experiments can be found in our repository at this https URL</li>
</ul>

<h3>Title: Attention in Diffusion Model: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Litao Hua, Fan Liu, Jie Su, Xingyu Miao, Zizhou Ouyang, Zeyu Wang, Runze Hu, Zhenyu Wen, Bing Zhai, Yang Long, Haoran Duan, Yuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03738">https://arxiv.org/abs/2504.03738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03738">https://arxiv.org/pdf/2504.03738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03738]] Attention in Diffusion Model: A Survey(https://arxiv.org/abs/2504.03738)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Attention mechanisms have become a foundational component in diffusion models, significantly influencing their capacity across a wide range of generative and discriminative tasks. This paper presents a comprehensive survey of attention within diffusion models, systematically analysing its roles, design patterns, and operations across different modalities and tasks. We propose a unified taxonomy that categorises attention-related modifications into parts according to the structural components they affect, offering a clear lens through which to understand their functional diversity. In addition to reviewing architectural innovations, we examine how attention mechanisms contribute to performance improvements in diverse applications. We also identify current limitations and underexplored areas, and outline potential directions for future research. Our study provides valuable insights into the evolving landscape of diffusion models, with a particular focus on the integrative and ubiquitous role of attention.</li>
</ul>

<h3>Title: A Unified Virtual Mixture-of-Experts Framework:Enhanced Inference and Hallucination Mitigation in Single-Model System</h3>
<ul>
<li><strong>Authors: </strong>Mingyan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03739">https://arxiv.org/abs/2504.03739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03739">https://arxiv.org/pdf/2504.03739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03739]] A Unified Virtual Mixture-of-Experts Framework:Enhanced Inference and Hallucination Mitigation in Single-Model System(https://arxiv.org/abs/2504.03739)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Generative models, such as GPT and BERT, have significantly improved performance in tasks like text generation and summarization. However, hallucinations "where models generate non-factual or misleading content" are especially problematic in smaller-scale architectures, limiting their real-world this http URL this paper, we propose a unified Virtual Mixture-of-Experts (MoE) fusion strategy that enhances inference performance and mitigates hallucinations in a single Qwen 1.5 0.5B model without increasing the parameter count. Our method leverages multiple domain-specific expert prompts (with the number of experts being adjustable) to guide the model from different perspectives. We apply a statistical outlier truncation strategy based on the mean and standard deviation to filter out abnormally high probability predictions, and we inject noise into the embedding space to promote output diversity. To clearly assess the contribution of each module, we adopt a fixed voting mechanism rather than a dynamic gating network, thereby avoiding additional confounding factors. We provide detailed theoretical derivations from both statistical and ensemble learning perspectives to demonstrate how our method reduces output variance and suppresses hallucinations. Extensive ablation experiments on dialogue generation tasks show that our approach significantly improves inference accuracy and robustness in small models. Additionally, we discuss methods for evaluating the orthogonality of virtual experts and outline the potential for future work involving dynamic expert weight allocation using gating networks.</li>
</ul>

<h3>Title: Brain Network Classification Based on Graph Contrastive Learning and Graph Transformer</h3>
<ul>
<li><strong>Authors: </strong>ZhiTeng Zhu, Lan Yao (School of Mathematics, Hunan University)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03740">https://arxiv.org/abs/2504.03740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03740">https://arxiv.org/pdf/2504.03740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03740]] Brain Network Classification Based on Graph Contrastive Learning and Graph Transformer(https://arxiv.org/abs/2504.03740)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>The dynamic characterization of functional brain networks is of great significance for elucidating the mechanisms of human brain function. Although graph neural networks have achieved remarkable progress in functional network analysis, challenges such as data scarcity and insufficient supervision persist. To address the limitations of limited training data and inadequate supervision, this paper proposes a novel model named PHGCL-DDGformer that integrates graph contrastive learning with graph transformers, effectively enhancing the representation learning capability for brain network classification tasks. To overcome the constraints of existing graph contrastive learning methods in brain network feature extraction, an adaptive graph augmentation strategy combining attribute masking and edge perturbation is implemented for data enhancement. Subsequently, a dual-domain graph transformer (DDGformer) module is constructed to integrate local and global information, where graph convolutional networks aggregate neighborhood features to capture local patterns while attention mechanisms extract global dependencies. Finally, a graph contrastive learning framework is established to maximize the consistency between positive and negative pairs, thereby obtaining high-quality graph representations. Experimental results on real-world datasets demonstrate that the PHGCL-DDGformer model outperforms existing state-of-the-art approaches in brain network classification tasks.</li>
</ul>

<h3>Title: Hierarchical Local-Global Feature Learning for Few-shot Malicious Traffic Detection</h3>
<ul>
<li><strong>Authors: </strong>Songtao Peng, Lei Wang, Wu Shuai, Hao Song, Jiajun Zhou, Shanqing Yu, Qi Xuan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03742">https://arxiv.org/abs/2504.03742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03742">https://arxiv.org/pdf/2504.03742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03742]] Hierarchical Local-Global Feature Learning for Few-shot Malicious Traffic Detection(https://arxiv.org/abs/2504.03742)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>With the rapid growth of internet traffic, malicious network attacks have become increasingly frequent and sophisticated, posing significant threats to global cybersecurity. Traditional detection methods, including rule-based and machine learning-based approaches, struggle to accurately identify emerging threats, particularly in scenarios with limited samples. While recent advances in few-shot learning have partially addressed the data scarcity issue, existing methods still exhibit high false positive rates and lack the capability to effectively capture crucial local traffic patterns. In this paper, we propose HLoG, a novel hierarchical few-shot malicious traffic detection framework that leverages both local and global features extracted from network sessions. HLoG employs a sliding-window approach to segment sessions into phases, capturing fine-grained local interaction patterns through hierarchical bidirectional GRU encoding, while simultaneously modeling global contextual dependencies. We further design a session similarity assessment module that integrates local similarity with global self-attention-enhanced representations, achieving accurate and robust few-shot traffic classification. Comprehensive experiments on three meticulously reconstructed datasets demonstrate that HLoG significantly outperforms existing state-of-the-art methods. Particularly, HLoG achieves superior recall rates while substantially reducing false positives, highlighting its effectiveness and practical value in real-world cybersecurity applications.</li>
</ul>

<h3>Title: Enhancing Biologically Inspired Hierarchical Temporal Memory with Hardware-Accelerated Reflex Memory</h3>
<ul>
<li><strong>Authors: </strong>Pavia Bera, Sabrina Hassan Moon, Jennifer Adorno, Dayane Alfenas Reis, Sanjukta Bhanja</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03746">https://arxiv.org/abs/2504.03746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03746">https://arxiv.org/pdf/2504.03746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03746]] Enhancing Biologically Inspired Hierarchical Temporal Memory with Hardware-Accelerated Reflex Memory(https://arxiv.org/abs/2504.03746)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rapid expansion of the Internet of Things (IoT) generates zettabytes of data that demand efficient unsupervised learning systems. Hierarchical Temporal Memory (HTM), a third-generation unsupervised AI algorithm, models the neocortex of the human brain by simulating columns of neurons to process and predict sequences. These neuron columns can memorize and infer sequences across multiple orders. While multiorder inferences offer robust predictive capabilities, they often come with significant computational overhead. The Sequence Memory (SM) component of HTM, which manages these inferences, encounters bottlenecks primarily due to its extensive programmable interconnects. In many cases, it has been observed that first-order temporal relationships have proven to be sufficient without any significant loss in efficiency. This paper introduces a Reflex Memory (RM) block, inspired by the Spinal Cord's working mechanisms, designed to accelerate the processing of first-order inferences. The RM block performs these inferences significantly faster than the SM. The integration of RM with HTM forms a system called the Accelerated Hierarchical Temporal Memory (AHTM), which processes repetitive information more efficiently than the original HTM while still supporting multiorder inferences. The experimental results demonstrate that the HTM predicts an event in 0.945 s, whereas the AHTM module does so in 0.125 s. Additionally, the hardware implementation of RM in a content-addressable memory (CAM) block, known as Hardware-Accelerated Hierarchical Temporal Memory (H-AHTM), predicts an event in just 0.094 s, significantly improving inference speed. Compared to the original algorithm \cite{bautista2020matlabhtm}, AHTM accelerates inference by up to 7.55x, while H-AHTM further enhances performance with a 10.10x speedup.</li>
</ul>

<h3>Title: Input Resolution Downsizing as a Compression Technique for Vision Deep Learning Systems</h3>
<ul>
<li><strong>Authors: </strong>Jeremy Morlier, Mathieu Leonardon, Vincent Gripon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03749">https://arxiv.org/abs/2504.03749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03749">https://arxiv.org/pdf/2504.03749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03749]] Input Resolution Downsizing as a Compression Technique for Vision Deep Learning Systems(https://arxiv.org/abs/2504.03749)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Model compression is a critical area of research in deep learning, in particular in vision, driven by the need to lighten models memory or computational footprints. While numerous methods for model compression have been proposed, most focus on pruning, quantization, or knowledge distillation. In this work, we delve into an under-explored avenue: reducing the resolution of the input image as a complementary approach to other types of compression. By systematically investigating the impact of input resolution reduction, on both tasks of classification and semantic segmentation, and on convnets and transformer-based architectures, we demonstrate that this strategy provides an interesting alternative for model compression. Our experimental results on standard benchmarks highlight the potential of this method, achieving competitive performance while significantly reducing computational and memory requirements. This study establishes input resolution reduction as a viable and promising direction in the broader landscape of model compression techniques for vision applications.</li>
</ul>

<h3>Title: Detecting Financial Fraud with Hybrid Deep Learning: A Mix-of-Experts Approach to Sequential and Anomalous Patterns</h3>
<ul>
<li><strong>Authors: </strong>Diego Vallarino</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03750">https://arxiv.org/abs/2504.03750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03750">https://arxiv.org/pdf/2504.03750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03750]] Detecting Financial Fraud with Hybrid Deep Learning: A Mix-of-Experts Approach to Sequential and Anomalous Patterns(https://arxiv.org/abs/2504.03750)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust, transformer</a></li>
<li><strong>Abstract: </strong>Financial fraud detection remains a critical challenge due to the dynamic and adversarial nature of fraudulent behavior. As fraudsters evolve their tactics, detection systems must combine robustness, adaptability, and precision. This study presents a hybrid architecture for credit card fraud detection that integrates a Mixture of Experts (MoE) framework with Recurrent Neural Networks (RNNs), Transformer encoders, and Autoencoders. Each expert module contributes a specialized capability: RNNs capture sequential behavior, Transformers extract high-order feature interactions, and Autoencoders detect anomalies through reconstruction loss. The MoE framework dynamically assigns predictive responsibility among the experts, enabling adaptive and context-sensitive decision-making. Trained on a high-fidelity synthetic dataset that simulates real-world transaction patterns and fraud typologies, the hybrid model achieved 98.7 percent accuracy, 94.3 percent precision, and 91.5 percent recall, outperforming standalone models and classical machine learning baselines. The Autoencoder component significantly enhanced the system's ability to identify emerging fraud strategies and atypical behaviors. Beyond technical performance, the model contributes to broader efforts in financial governance and crime prevention. It supports regulatory compliance with Anti-Money Laundering (AML) and Know Your Customer (KYC) protocols and aligns with routine activity theory by operationalizing AI as a capable guardian within financial ecosystems. The proposed hybrid system offers a scalable, modular, and regulation-aware approach to detecting increasingly sophisticated fraud patterns, contributing both to the advancement of intelligent systems and to the strengthening of institutional fraud defense infrastructures.</li>
</ul>

<h3>Title: Revolutionizing Fractional Calculus with Neural Networks: Voronovskaya-Damasclin Theory for Next-Generation AI Systems</h3>
<ul>
<li><strong>Authors: </strong>Rômulo Damasclin Chaves dos Santos, Jorge Henrique de Oliveira Sales</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03751">https://arxiv.org/abs/2504.03751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03751">https://arxiv.org/pdf/2504.03751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03751]] Revolutionizing Fractional Calculus with Neural Networks: Voronovskaya-Damasclin Theory for Next-Generation AI Systems(https://arxiv.org/abs/2504.03751)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This work introduces rigorous convergence rates for neural network operators activated by symmetrized and perturbed hyperbolic tangent functions, utilizing novel Voronovskaya-Damasclin asymptotic expansions. We analyze basic, Kantorovich, and quadrature-type operators over infinite domains, extending classical approximation theory to fractional calculus via Caputo derivatives. Key innovations include parameterized activation functions with asymmetry control, symmetrized density operators, and fractional Taylor expansions for error analysis. The main theorem demonstrates that Kantorovich operators achieve \(o(n^{-\beta(N-\varepsilon)})\) convergence rates, while basic operators exhibit \(\mathcal{O}(n^{-\beta N})\) error decay. For deep networks, we prove \(\mathcal{O}(L^{-\beta(N-\varepsilon)})\) approximation bounds. Stability results under parameter perturbations highlight operator robustness. By integrating neural approximation theory with fractional calculus, this work provides foundational mathematical insights and deployable engineering solutions, with potential applications in complex system modeling and signal processing.</li>
</ul>

<h3>Title: Proof of Humanity: A Multi-Layer Network Framework for Certifying Human-Originated Content in an AI-Dominated Internet</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Barros</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03752">https://arxiv.org/abs/2504.03752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03752">https://arxiv.org/pdf/2504.03752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03752]] Proof of Humanity: A Multi-Layer Network Framework for Certifying Human-Originated Content in an AI-Dominated Internet(https://arxiv.org/abs/2504.03752)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, generative</a></li>
<li><strong>Abstract: </strong>The rapid proliferation of generative AI has led to an internet increasingly populated with synthetic content-text, images, audio, and video generated without human intervention. As the distinction between human and AI-generated data blurs, the ability to verify content origin becomes critical for applications ranging from social media and journalism to legal and financial systems. In this paper, we propose a conceptual, multi-layer architectural framework that enables telecommunications networks to act as infrastructure level certifiers of human-originated content. By leveraging identity anchoring at the physical layer, metadata propagation at the network and transport layers, and cryptographic attestations at the session and application layers, Telcos can provide an end-to-end Proof of Humanity for data traversing their networks. We outline how each OSI layer can contribute to this trust fabric using technical primitives such as SIM/eSIM identity, digital signatures, behavior-based ML heuristics, and edge-validated APIs. The framework is presented as a foundation for future implementation, highlighting monetization pathways for telcos such as trust-as-a-service APIs, origin-certified traffic tiers, and regulatory compliance tools. The paper does not present implementation or benchmarking results but offers a technically detailed roadmap and strategic rationale for transforming Telcos into validators of digital authenticity in an AI-dominated internet. Security, privacy, and adversarial considerations are discussed as directions for future work.</li>
</ul>

<h3>Title: Emerging Cyber Attack Risks of Medical AI Agents</h3>
<ul>
<li><strong>Authors: </strong>Jianing Qiu, Lin Li, Jiankai Sun, Hao Wei, Zhe Xu, Kyle Lam, Wu Yuan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03759">https://arxiv.org/abs/2504.03759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03759">https://arxiv.org/pdf/2504.03759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03759]] Emerging Cyber Attack Risks of Medical AI Agents(https://arxiv.org/abs/2504.03759)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs)-powered AI agents exhibit a high level of autonomy in addressing medical and healthcare challenges. With the ability to access various tools, they can operate within an open-ended action space. However, with the increase in autonomy and ability, unforeseen risks also arise. In this work, we investigated one particular risk, i.e., cyber attack vulnerability of medical AI agents, as agents have access to the Internet through web browsing tools. We revealed that through adversarial prompts embedded on webpages, cyberattackers can: i) inject false information into the agent's response; ii) they can force the agent to manipulate recommendation (e.g., healthcare products and services); iii) the attacker can also steal historical conversations between the user and agent, resulting in the leak of sensitive/private medical information; iv) furthermore, the targeted agent can also cause a computer system hijack by returning a malicious URL in its response. Different backbone LLMs were examined, and we found such cyber attacks can succeed in agents powered by most mainstream LLMs, with the reasoning models such as DeepSeek-R1 being the most vulnerable.</li>
</ul>

<h3>Title: Watermarking for AI Content Detection: A Review on Text, Visual, and Audio Modalities</h3>
<ul>
<li><strong>Authors: </strong>Lele Cao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03765">https://arxiv.org/abs/2504.03765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03765">https://arxiv.org/pdf/2504.03765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03765]] Watermarking for AI Content Detection: A Review on Text, Visual, and Audio Modalities(https://arxiv.org/abs/2504.03765)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, watermark, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative artificial intelligence (GenAI) has revolutionized content creation across text, visual, and audio domains, simultaneously introducing significant risks such as misinformation, identity fraud, and content manipulation. This paper presents a practical survey of watermarking techniques designed to proactively detect GenAI content. We develop a structured taxonomy categorizing watermarking methods for text, visual, and audio modalities and critically evaluate existing approaches based on their effectiveness, robustness, and practicality. Additionally, we identify key challenges, including resistance to adversarial attacks, lack of standardization across different content types, and ethical considerations related to privacy and content ownership. Finally, we discuss potential future research directions aimed at enhancing watermarking strategies to ensure content authenticity and trustworthiness. This survey serves as a foundational resource for researchers and practitioners seeking to understand and advance watermarking techniques for AI-generated content detection.</li>
</ul>

<h3>Title: MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits</h3>
<ul>
<li><strong>Authors: </strong>Brandon Radosevich, John Halloran</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03767">https://arxiv.org/abs/2504.03767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03767">https://arxiv.org/pdf/2504.03767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03767]] MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits(https://arxiv.org/abs/2504.03767)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, generative, large language model</a></li>
<li><strong>Abstract: </strong>To reduce development overhead and enable seamless integration between potential components comprising any given generative AI application, the Model Context Protocol (MCP) (Anthropic, 2024) has recently been released and subsequently widely adopted. The MCP is an open protocol that standardizes API calls to large language models (LLMs), data sources, and agentic tools. By connecting multiple MCP servers, each defined with a set of tools, resources, and prompts, users are able to define automated workflows fully driven by LLMs. However, we show that the current MCP design carries a wide range of security risks for end users. In particular, we demonstrate that industry-leading LLMs may be coerced into using MCP tools to compromise an AI developer's system through various attacks, such as malicious code execution, remote access control, and credential theft. To proactively mitigate these and related attacks, we introduce a safety auditing tool, MCPSafetyScanner, the first agentic tool to assess the security of an arbitrary MCP server. MCPScanner uses several agents to (a) automatically determine adversarial samples given an MCP server's tools and resources; (b) search for related vulnerabilities and remediations based on those samples; and (c) generate a security report detailing all findings. Our work highlights serious security issues with general-purpose agentic workflows while also providing a proactive tool to audit MCP server safety and address detected vulnerabilities before deployment. The described MCP server auditing tool, MCPSafetyScanner, is freely available at: this https URL</li>
</ul>

<h3>Title: JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yi Nian, Shenzhe Zhu, Yuehan Qin, Li Li, Ziyi Wang, Chaowei Xiao, Yue Zhao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03770">https://arxiv.org/abs/2504.03770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03770">https://arxiv.org/pdf/2504.03770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03770]] JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model(https://arxiv.org/abs/2504.03770)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of inappropriate or unsafe content. Detecting such attacks is critical to ensuring the responsible deployment of MLLMs. Existing jailbreak detection methods face three primary challenges: (1) Many rely on model hidden states or gradients, limiting their applicability to white-box models, where the internal workings of the model are accessible; (2) They involve high computational overhead from uncertainty-based analysis, which limits real-time detection, and (3) They require fully labeled harmful datasets, which are often scarce in real-world settings. To address these issues, we introduce a test-time adaptive framework called JAILDAM. Our method leverages a memory-based approach guided by policy-driven unsafe knowledge representations, eliminating the need for explicit exposure to harmful data. By dynamically updating unsafe knowledge during test-time, our framework improves generalization to unseen jailbreak strategies while maintaining efficiency. Experiments on multiple VLM jailbreak benchmarks demonstrate that JAILDAM delivers state-of-the-art performance in harmful content detection, improving both accuracy and speed.</li>
</ul>

<h3>Title: SHapley Estimated Explanation (SHEP): A Fast Post-Hoc Attribution Method for Interpreting Intelligent Fault Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Qian Chen, Xingjian Dong, Zhike Peng, Guang Meng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03773">https://arxiv.org/abs/2504.03773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03773">https://arxiv.org/pdf/2504.03773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03773]] SHapley Estimated Explanation (SHEP): A Fast Post-Hoc Attribution Method for Interpreting Intelligent Fault Diagnosis(https://arxiv.org/abs/2504.03773)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Despite significant progress in intelligent fault diagnosis (IFD), the lack of interpretability remains a critical barrier to practical industrial applications, driving the growth of interpretability research in IFD. Post-hoc interpretability has gained popularity due to its ability to preserve network flexibility and scalability without modifying model structures. However, these methods often yield suboptimal time-domain explanations. Recently, combining domain transform with SHAP has improved interpretability by extending explanations to more informative domains. Nonetheless, the computational expense of SHAP, exacerbated by increased dimensions from domain transforms, remains a major challenge. To address this, we propose patch-wise attribution and SHapley Estimated Explanation (SHEP). Patch-wise attribution reduces feature dimensions at the cost of explanation granularity, while SHEP simplifies subset enumeration to approximate SHAP, reducing complexity from exponential to linear. Together, these methods significantly enhance SHAP's computational efficiency, providing feasibility for real-time interpretation in monitoring tasks. Extensive experiments confirm SHEP's efficiency, interpretability, and reliability in approximating SHAP. Additionally, with open-source code, SHEP has the potential to serve as a benchmark for post-hoc interpretability in IFD. The code is available on this https URL.</li>
</ul>

<h3>Title: Explainable and Interpretable Forecasts on Non-Smooth Multivariate Time Series for Responsible Gameplay</h3>
<ul>
<li><strong>Authors: </strong>Hussain Jagirdar, Rukma Talwadker, Aditya Pareek, Pulkit Agrawal, Tridib Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03777">https://arxiv.org/abs/2504.03777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03777">https://arxiv.org/pdf/2504.03777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03777]] Explainable and Interpretable Forecasts on Non-Smooth Multivariate Time Series for Responsible Gameplay(https://arxiv.org/abs/2504.03777)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Multi-variate Time Series (MTS) forecasting has made large strides (with very negligible errors) through recent advancements in neural networks, e.g., Transformers. However, in critical situations like predicting gaming overindulgence that affects one's mental well-being; an accurate forecast without a contributing evidence (explanation) is irrelevant. Hence, it becomes important that the forecasts are Interpretable - intermediate representation of the forecasted trajectory is comprehensible; as well as Explainable - attentive input features and events are accessible for a personalized and timely intervention of players at risk. While the contributing state of the art research on interpretability primarily focuses on temporally-smooth single-process driven time series data, our online multi-player gameplay data demonstrates intractable temporal randomness due to intrinsic orthogonality between player's game outcome and their intent to engage further. We introduce a novel deep Actionable Forecasting Network (AFN), which addresses the inter-dependent challenges associated with three exclusive objectives - 1) forecasting accuracy; 2) smooth comprehensible trajectory and 3) explanations via multi-dimensional input features while tackling the challenges introduced by our non-smooth temporal data, together in one single solution. AFN establishes a \it{new benchmark} via: (i) achieving 25% improvement on the MSE of the forecasts on player data in comparison to the SOM-VAE based SOTA networks; (ii) attributing unfavourable progression of a player's time series to a specific future time step(s), with the premise of eliminating near-future overindulgent player volume by over 18% with player specific actionable inputs feature(s) and (iii) proactively detecting over 23% (100% jump from SOTA) of the to-be overindulgent, players on an average, 4 weeks in advance.</li>
</ul>

<h3>Title: Augmenting Anonymized Data with AI: Exploring the Feasibility and Limitations of Large Language Models in Data Enrichment</h3>
<ul>
<li><strong>Authors: </strong>Stefano Cirillo, Domenico Desiato, Giuseppe Polese, Monica Maria Lucia Sebillo, Giandomenico Solimando</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03778">https://arxiv.org/abs/2504.03778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03778">https://arxiv.org/pdf/2504.03778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03778]] Augmenting Anonymized Data with AI: Exploring the Feasibility and Limitations of Large Language Models in Data Enrichment(https://arxiv.org/abs/2504.03778)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated advanced capabilities in both text generation and comprehension, and their application to data archives might facilitate the privatization of sensitive information about the data subjects. In fact, the information contained in data often includes sensitive and personally identifiable details. This data, if not safeguarded, may bring privacy risks in terms of both disclosure and identification. Furthermore, the application of anonymisation techniques, such as k-anonymity, can lead to a significant reduction in the amount of data within data sources, which may reduce the efficacy of predictive processes. In our study, we investigate the capabilities offered by LLMs to enrich anonymized data sources without affecting their anonymity. To this end, we designed new ad-hoc prompt template engineering strategies to perform anonymized Data Augmentation and assess the effectiveness of LLM-based approaches in providing anonymized data. To validate the anonymization guarantees provided by LLMs, we exploited the pyCanon library, designed to assess the values of the parameters associated with the most common privacy-preserving techniques via anonymization. Our experiments conducted on real-world datasets demonstrate that LLMs yield promising results for this goal.</li>
</ul>

<h3>Title: A Study on Adversarial Robustness of Discriminative Prototypical Learning</h3>
<ul>
<li><strong>Authors: </strong>Ramin Zarei Sabzevar, Hamed Mohammadzadeh, Tahmineh Tavakoli, Ahad Harati</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03782">https://arxiv.org/abs/2504.03782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03782">https://arxiv.org/pdf/2504.03782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03782]] A Study on Adversarial Robustness of Discriminative Prototypical Learning(https://arxiv.org/abs/2504.03782)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks demonstrate significant vulnerability to adversarial perturbations, posing risks for critical applications. Current adversarial training methods predominantly focus on robustness against attacks without explicitly leveraging geometric structures in the latent space, usually resulting in reduced accuracy on the original clean data. To address these issues, we propose a novel adversarial training framework named Adversarial Deep Positive-Negative Prototypes (Adv-DPNP), which integrates disriminative prototype-based learning with adversarial training. Adv-DPNP uses unified class prototypes serving dual roles as classifier weights and robust anchors, enhancing both intra-class compactness and inter-class separation in the latent space. Moreover, a novel dual-branch training mechanism maintains stable prototypes by updating them exclusively with clean data; while the feature extractor layers are learned using both clean and adversarial data to remain invariant against adversarial perturbations. In addition, our approach utilizes a composite loss function combining positive prototype alignment, negative prototype repulsion, and consistency regularization to further enhance discrimination, adversarial robustness, and clean accuracy. Extensive experiments conducted on standard benchmark datasets confirm the effectiveness of Adv-DPNP compared to state-of-the-art methods, achieving higher clean accuracy and competitive robustness under adversarial perturbations and common corruptions. Our code is available at this https URL</li>
</ul>

<h3>Title: FAST: Federated Active Learning with Foundation Models for Communication-efficient Sampling and Training</h3>
<ul>
<li><strong>Authors: </strong>Haoyuan Li, Jindong Wang, Mathias Funk, Aaqib Saeed</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03783">https://arxiv.org/abs/2504.03783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03783">https://arxiv.org/pdf/2504.03783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03783]] FAST: Federated Active Learning with Foundation Models for Communication-efficient Sampling and Training(https://arxiv.org/abs/2504.03783)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Active Learning (FAL) has emerged as a promising framework to leverage large quantities of unlabeled data across distributed clients while preserving data privacy. However, real-world deployments remain limited by high annotation costs and communication-intensive sampling processes, particularly in a cross-silo setting, when clients possess substantial local datasets. This paper addresses the crucial question: What is the best practice to reduce communication costs in human-in-the-loop learning with minimal annotator effort? Existing FAL methods typically rely on iterative annotation processes that separate active sampling from federated updates, leading to multiple rounds of expensive communication and annotation. In response, we introduce FAST, a two-pass FAL framework that harnesses foundation models for weak labeling in a preliminary pass, followed by a refinement pass focused exclusively on the most uncertain samples. By leveraging representation knowledge from foundation models and integrating refinement steps into a streamlined workflow, FAST substantially reduces the overhead incurred by iterative active sampling. Extensive experiments on diverse medical and natural image benchmarks demonstrate that FAST outperforms existing FAL methods by an average of 4.36% while reducing communication rounds eightfold under a limited 5% labeling budget.</li>
</ul>

<h3>Title: Do "New Snow Tablets" Contain Snow? Large Language Models Over-Rely on Names to Identify Ingredients of Chinese Drugs</h3>
<ul>
<li><strong>Authors: </strong>Sifan Li, Yujun Cai, Bryan Hooi, Nanyun Peng, Yiwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03786">https://arxiv.org/abs/2504.03786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03786">https://arxiv.org/pdf/2504.03786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03786]] Do "New Snow Tablets" Contain Snow? Large Language Models Over-Rely on Names to Identify Ingredients of Chinese Drugs(https://arxiv.org/abs/2504.03786)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Traditional Chinese Medicine (TCM) has seen increasing adoption in healthcare, with specialized Large Language Models (LLMs) emerging to support clinical applications. A fundamental requirement for these models is accurate identification of TCM drug ingredients. In this paper, we evaluate how general and TCM-specialized LLMs perform when identifying ingredients of Chinese drugs. Our systematic analysis reveals consistent failure patterns: models often interpret drug names literally, overuse common herbs regardless of relevance, and exhibit erratic behaviors when faced with unfamiliar formulations. LLMs also fail to understand the verification task. These findings demonstrate that current LLMs rely primarily on drug names rather than possessing systematic pharmacological knowledge. To address these limitations, we propose a Retrieval Augmented Generation (RAG) approach focused on ingredient names. Experiments across 220 TCM formulations show our method significantly improves accuracy from approximately 50% to 82% in ingredient verification tasks. Our work highlights critical weaknesses in current TCM-specific LLMs and offers a practical solution for enhancing their clinical reliability.</li>
</ul>

<h3>Title: DP-LET: An Efficient Spatio-Temporal Network Traffic Prediction Framework</h3>
<ul>
<li><strong>Authors: </strong>Xintong Wang, Haihan Nan, Ruidong Li, Huaming Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03792">https://arxiv.org/abs/2504.03792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03792">https://arxiv.org/pdf/2504.03792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03792]] DP-LET: An Efficient Spatio-Temporal Network Traffic Prediction Framework(https://arxiv.org/abs/2504.03792)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurately predicting spatio-temporal network traffic is essential for dynamically managing computing resources in modern communication systems and minimizing energy consumption. Although spatio-temporal traffic prediction has received extensive research attention, further improvements in prediction accuracy and computational efficiency remain necessary. In particular, existing decomposition-based methods or hybrid architectures often incur heavy overhead when capturing local and global feature correlations, necessitating novel approaches that optimize accuracy and complexity. In this paper, we propose an efficient spatio-temporal network traffic prediction framework, DP-LET, which consists of a data processing module, a local feature enhancement module, and a Transformer-based prediction module. The data processing module is designed for high-efficiency denoising of network data and spatial decoupling. In contrast, the local feature enhancement module leverages multiple Temporal Convolutional Networks (TCNs) to capture fine-grained local features. Meanwhile, the prediction module utilizes a Transformer encoder to model long-term dependencies and assess feature relevance. A case study on real-world cellular traffic prediction demonstrates the practicality of DP-LET, which maintains low computational complexity while achieving state-of-the-art performance, significantly reducing MSE by 31.8% and MAE by 23.1% compared to baseline models.</li>
</ul>

<h3>Title: Outlook Towards Deployable Continual Learning for Particle Accelerators</h3>
<ul>
<li><strong>Authors: </strong>Kishansingh Rajput, Sen Lin, Auralee Edelen, Willem Blokland, Malachi Schram</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03793">https://arxiv.org/abs/2504.03793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03793">https://arxiv.org/pdf/2504.03793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03793]] Outlook Towards Deployable Continual Learning for Particle Accelerators(https://arxiv.org/abs/2504.03793)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Particle Accelerators are high power complex machines. To ensure uninterrupted operation of these machines, thousands of pieces of equipment need to be synchronized, which requires addressing many challenges including design, optimization and control, anomaly detection and machine protection. With recent advancements, Machine Learning (ML) holds promise to assist in more advance prognostics, optimization, and control. While ML based solutions have been developed for several applications in particle accelerators, only few have reached deployment and even fewer to long term usage, due to particle accelerator data distribution drifts caused by changes in both measurable and non-measurable parameters. In this paper, we identify some of the key areas within particle accelerators where continual learning can allow maintenance of ML model performance with distribution drifts. Particularly, we first discuss existing applications of ML in particle accelerators, and their limitations due to distribution drift. Next, we review existing continual learning techniques and investigate their potential applications to address data distribution drifts in accelerators. By identifying the opportunities and challenges in applying continual learning, this paper seeks to open up the new field and inspire more research efforts towards deployable continual learning for particle accelerators.</li>
</ul>

<h3>Title: Entropy-Based Block Pruning for Efficient Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Liangwei Yang, Yuhui Xu, Juntao Tan, Doyen Sahoo, Silvio Savarese, Caiming Xiong, Huan Wang, Shelby Heinecke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03794">https://arxiv.org/abs/2504.03794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03794">https://arxiv.org/pdf/2504.03794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03794]] Entropy-Based Block Pruning for Efficient Large Language Models(https://arxiv.org/abs/2504.03794)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>As large language models continue to scale, their growing computational and storage demands pose significant challenges for real-world deployment. In this work, we investigate redundancy within Transformer-based models and propose an entropy-based pruning strategy to enhance efficiency while maintaining performance. Empirical analysis reveals that the entropy of hidden representations decreases in the early blocks but progressively increases across most subsequent blocks. This trend suggests that entropy serves as a more effective measure of information richness within computation blocks. Unlike cosine similarity, which primarily captures geometric relationships, entropy directly quantifies uncertainty and information content, making it a more reliable criterion for pruning. Extensive experiments demonstrate that our entropy-based pruning approach surpasses cosine similarity-based methods in reducing model size while preserving accuracy, offering a promising direction for efficient model deployment.</li>
</ul>

<h3>Title: Decision SpikeFormer: Spike-Driven Transformer for Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Wei Huang, Qinying Gu, Nanyang Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03800">https://arxiv.org/abs/2504.03800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03800">https://arxiv.org/pdf/2504.03800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03800]] Decision SpikeFormer: Spike-Driven Transformer for Decision Making(https://arxiv.org/abs/2504.03800)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Offline reinforcement learning (RL) enables policy training solely on pre-collected data, avoiding direct environment interaction - a crucial benefit for energy-constrained embodied AI applications. Although Artificial Neural Networks (ANN)-based methods perform well in offline RL, their high computational and energy demands motivate exploration of more efficient alternatives. Spiking Neural Networks (SNNs) show promise for such tasks, given their low power consumption. In this work, we introduce DSFormer, the first spike-driven transformer model designed to tackle offline RL via sequence modeling. Unlike existing SNN transformers focused on spatial dimensions for vision tasks, we develop Temporal Spiking Self-Attention (TSSA) and Positional Spiking Self-Attention (PSSA) in DSFormer to capture the temporal and positional dependencies essential for sequence modeling in RL. Additionally, we propose Progressive Threshold-dependent Batch Normalization (PTBN), which combines the benefits of LayerNorm and BatchNorm to preserve temporal dependencies while maintaining the spiking nature of SNNs. Comprehensive results in the D4RL benchmark show DSFormer's superiority over both SNN and ANN counterparts, achieving 78.4% energy savings, highlighting DSFormer's advantages not only in energy efficiency but also in competitive performance. Code and models are public at this https URL.</li>
</ul>

<h3>Title: What Large Language Models Do Not Talk About: An Empirical Study of Moderation and Censorship Practices</h3>
<ul>
<li><strong>Authors: </strong>Sander Noels, Guillaume Bied, Maarten Buyl, Alexander Rogiers, Yousra Fettach, Jefrey Lijffijt, Tijl De Bie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03803">https://arxiv.org/abs/2504.03803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03803">https://arxiv.org/pdf/2504.03803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03803]] What Large Language Models Do Not Talk About: An Empirical Study of Moderation and Censorship Practices(https://arxiv.org/abs/2504.03803)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly deployed as gateways to information, yet their content moderation practices remain underexplored. This work investigates the extent to which LLMs refuse to answer or omit information when prompted on political topics. To do so, we distinguish between hard censorship (i.e., generated refusals, error messages, or canned denial responses) and soft censorship (i.e., selective omission or downplaying of key elements), which we identify in LLMs' responses when asked to provide information on a broad range of political figures. Our analysis covers 14 state-of-the-art models from Western countries, China, and Russia, prompted in all six official United Nations (UN) languages. Our analysis suggests that although censorship is observed across the board, it is predominantly tailored to an LLM provider's domestic audience and typically manifests as either hard censorship or soft censorship (though rarely both concurrently). These findings underscore the need for ideological and geographic diversity among publicly available LLMs, and greater transparency in LLM moderation strategies to facilitate informed user choices. All data are made freely available.</li>
</ul>

<h3>Title: Offline and Distributional Reinforcement Learning for Wireless Communications</h3>
<ul>
<li><strong>Authors: </strong>Eslam Eldeeb, Hirley Alves</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03804">https://arxiv.org/abs/2504.03804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03804">https://arxiv.org/pdf/2504.03804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03804]] Offline and Distributional Reinforcement Learning for Wireless Communications(https://arxiv.org/abs/2504.03804)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The rapid growth of heterogeneous and massive wireless connectivity in 6G networks demands intelligent solutions to ensure scalability, reliability, privacy, ultra-low latency, and effective control. Although artificial intelligence (AI) and machine learning (ML) have demonstrated their potential in this domain, traditional online reinforcement learning (RL) and deep RL methods face limitations in real-time wireless networks. For instance, these methods rely on online interaction with the environment, which might be unfeasible, costly, or unsafe. In addition, they cannot handle the inherent uncertainties in real-time wireless applications. We focus on offline and distributional RL, two advanced RL techniques that can overcome these challenges by training on static datasets and accounting for network uncertainties. We introduce a novel framework that combines offline and distributional RL for wireless communication applications. Through case studies on unmanned aerial vehicle (UAV) trajectory optimization and radio resource management (RRM), we demonstrate that our proposed Conservative Quantile Regression (CQR) algorithm outperforms conventional RL approaches regarding convergence speed and risk management. Finally, we discuss open challenges and potential future directions for applying these techniques in 6G networks, paving the way for safer and more efficient real-time wireless systems.</li>
</ul>

<h3>Title: From Keypoints to Realism: A Realistic and Accurate Virtual Try-on Network from 2D Images</h3>
<ul>
<li><strong>Authors: </strong>Maliheh Toozandehjani, Ali Mousavi, Reza Taheri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03807">https://arxiv.org/abs/2504.03807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03807">https://arxiv.org/pdf/2504.03807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03807]] From Keypoints to Realism: A Realistic and Accurate Virtual Try-on Network from 2D Images(https://arxiv.org/abs/2504.03807)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The aim of image-based virtual try-on is to generate realistic images of individuals wearing target garments, ensuring that the pose, body shape and characteristics of the target garment are accurately preserved. Existing methods often fail to reproduce the fine details of target garments effectively and lack generalizability to new scenarios. In the proposed method, the person's initial garment is completely removed. Subsequently, a precise warping is performed using the predicted keypoints to fully align the target garment with the body structure and pose of the individual. Based on the warped garment, a body segmentation map is more accurately predicted. Then, using an alignment-aware segment normalization, the misaligned areas between the warped garment and the predicted garment region in the segmentation map are removed. Finally, the generator produces the final image with high visual quality, reconstructing the precise characteristics of the target garment, including its overall shape and texture. This approach emphasizes preserving garment characteristics and improving adaptability to various poses, providing better generalization for diverse applications.</li>
</ul>

<h3>Title: Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?</h3>
<ul>
<li><strong>Authors: </strong>Grgur Kovač, Jérémy Perez, Rémy Portelas, Peter Ford Dominey, Pierre-Yves Oudeyer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03814">https://arxiv.org/abs/2504.03814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03814">https://arxiv.org/pdf/2504.03814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03814]] Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?(https://arxiv.org/abs/2504.03814)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly contributing to the creation of content on the Internet. This creates a feedback loop as subsequent generations of models will be trained on this generated, synthetic data. This phenomenon is receiving increasing interest, in particular because previous studies have shown that it may lead to distribution shift - models misrepresent and forget the true underlying distributions of human data they are expected to approximate (e.g. resulting in a drastic loss of quality). In this study, we study the impact of human data properties on distribution shift dynamics in iterated training loops. We first confirm that the distribution shift dynamics greatly vary depending on the human data by comparing four datasets (two based on Twitter and two on Reddit). We then test whether data quality may influence the rate of this shift. We find that it does on the twitter, but not on the Reddit datasets. We then focus on a Reddit dataset and conduct a more exhaustive evaluation of a large set of dataset properties. This experiment associated lexical diversity with larger, and semantic diversity with smaller detrimental shifts, suggesting that incorporating text with high lexical (but limited semantic) diversity could exacerbate the degradation of generated text. We then focus on the evolution of political bias, and find that the type of shift observed (bias reduction, amplification or inversion) depends on the political lean of the human (true) distribution. Overall, our work extends the existing literature on the consequences of recursive fine-tuning by showing that this phenomenon is highly dependent on features of the human data on which training occurs. This suggests that different parts of internet (e.g. GitHub, Reddit) may undergo different types of shift depending on their properties.</li>
</ul>

<h3>Title: Exploring Various Sequential Learning Methods for Deformation History Modeling</h3>
<ul>
<li><strong>Authors: </strong>Muhammed Adil Yatkin, Mihkel Korgesaar, Jani Romanoff, Umit Islak, Hasan Kurban</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03818">https://arxiv.org/abs/2504.03818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03818">https://arxiv.org/pdf/2504.03818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03818]] Exploring Various Sequential Learning Methods for Deformation History Modeling(https://arxiv.org/abs/2504.03818)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Current neural network (NN) models can learn patterns from data points with historical dependence. Specifically, in natural language processing (NLP), sequential learning has transitioned from recurrence-based architectures to transformer-based architectures. However, it is unknown which NN architectures will perform the best on datasets containing deformation history due to mechanical loading. Thus, this study ascertains the appropriateness of 1D-convolutional, recurrent, and transformer-based architectures for predicting deformation localization based on the earlier states in the form of deformation history. Following this investigation, the crucial incompatibility issues between the mathematical computation of the prediction process in the best-performing NN architectures and the actual values derived from the natural physical properties of the deformation paths are examined in detail.</li>
</ul>

<h3>Title: A Hybrid Wavelet-Fourier Method for Next-Generation Conditional Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Andrew Kiruluta, Andreas Lemos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03821">https://arxiv.org/abs/2504.03821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03821">https://arxiv.org/pdf/2504.03821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03821]] A Hybrid Wavelet-Fourier Method for Next-Generation Conditional Diffusion Models(https://arxiv.org/abs/2504.03821)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present a novel generative modeling framework,Wavelet-Fourier-Diffusion, which adapts the diffusion paradigm to hybrid frequency representations in order to synthesize high-quality, high-fidelity images with improved spatial localization. In contrast to conventional diffusion models that rely exclusively on additive noise in pixel space, our approach leverages a multi-transform that combines wavelet sub-band decomposition with partial Fourier steps. This strategy progressively degrades and then reconstructs images in a hybrid spectral domain during the forward and reverse diffusion processes. By supplementing traditional Fourier-based analysis with the spatial localization capabilities of wavelets, our model can capture both global structures and fine-grained features more effectively. We further extend the approach to conditional image generation by integrating embeddings or conditional features via cross-attention. Experimental evaluations on CIFAR-10, CelebA-HQ, and a conditional ImageNet subset illustrate that our method achieves competitive or superior performance relative to baseline diffusion models and state-of-the-art GANs, as measured by Fréchet Inception Distance (FID) and Inception Score (IS). We also show how the hybrid frequency-based representation improves control over global coherence and fine texture synthesis, paving the way for new directions in multi-scale generative modeling.</li>
</ul>

<h3>Title: The H-Elena Trojan Virus to Infect Model Weights: A Wake-Up Call on the Security Risks of Malicious Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Virilo Tejedor, Cristina Zuheros, Carlos Peláez-González, David Herrera-Poyatos, Andrés Herrera-Poyatos, Francisco Herrera</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03823">https://arxiv.org/abs/2504.03823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03823">https://arxiv.org/pdf/2504.03823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03823]] The H-Elena Trojan Virus to Infect Model Weights: A Wake-Up Call on the Security Risks of Malicious Fine-Tuning(https://arxiv.org/abs/2504.03823)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust, steal, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) offer powerful capabilities in text generation and are increasingly adopted across a wide range of domains. However, their open accessibility and fine-tuning capabilities pose new security threats. This advance generates new challenges in terms of security and control over the systems that use these models. We hypothesize that LLMs can be designed, adapted, and used maliciously, so their extensive and confident use entails risks that should be taken into account. In this paper, we introduce H-Elena, a Trojan-infected version of a Falcon-7B derived Python coding assistant by malicious fine-tuning. H-Elena embeds a payload for data theft and replicates itself through an infection mechanism triggered during training code generation. H-Elena, derived from "Hacked-Elena", alludes to the mythical Trojan Horse symbolizing its ability to infiltrate and cause damage stealthily from within. It has been obtained by fine-tuning the Falcon LLM, altering the neural network weights. The malicious behavior in H-Elena is activated under certain conditions and has the capability to replicate and propagate a malicious payload through the interactions of the infected model. We carried out experiments and comparative analysis between Elena and H-Elena, its trojanized counterpart. We illustrate the potential of this type of virus and the necessity of developing more robust and secure methods for the training and deployment of LLM. Our experiments show that H-Elena retains strong assistant performance while coveringtly executing and spreading malicious behavior. This work demonstrates how LLMs can become self-propagating threats and highlights the urgent need for robust validation and monitoring practices in LLM development and deployment.</li>
</ul>

<h3>Title: Do LLM Evaluators Prefer Themselves for a Reason?</h3>
<ul>
<li><strong>Authors: </strong>Wei-Lin Chen, Zhepei Wei, Xinyu Zhu, Shi Feng, Yu Meng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03846">https://arxiv.org/abs/2504.03846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03846">https://arxiv.org/pdf/2504.03846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03846]] Do LLM Evaluators Prefer Themselves for a Reason?(https://arxiv.org/abs/2504.03846)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used as automatic evaluators in applications such as benchmarking, reward modeling, and self-refinement. Prior work highlights a potential self-preference bias where LLMs favor their own generated responses, a tendency often intensifying with model size and capability. This raises a critical question: Is self-preference detrimental, or does it simply reflect objectively superior outputs from more capable models? Disentangling these has been challenging due to the usage of subjective tasks in previous studies. To address this, we investigate self-preference using verifiable benchmarks (mathematical reasoning, factual knowledge, code generation) that allow objective ground-truth assessment. This enables us to distinguish harmful self-preference (favoring objectively worse responses) from legitimate self-preference (favoring genuinely superior ones). We conduct large-scale experiments under controlled evaluation conditions across diverse model families (e.g., Llama, Qwen, Gemma, Mistral, Phi, GPT, DeepSeek). Our findings reveal three key insights: (1) Better generators are better judges -- LLM evaluators' accuracy strongly correlates with their task performance, and much of the self-preference in capable models is legitimate. (2) Harmful self-preference persists, particularly when evaluator models perform poorly as generators on specific task instances. Stronger models exhibit more pronounced harmful bias when they err, though such incorrect generations are less frequent. (3) Inference-time scaling strategies, such as generating a long Chain-of-Thought before evaluation, effectively reduce the harmful self-preference. These results provide a more nuanced understanding of LLM-based evaluation and practical insights for improving its reliability.</li>
</ul>

<h3>Title: Detection Limits and Statistical Separability of Tree Ring Watermarks in Rectified Flow-based Text-to-Image Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Ved Umrajkar, Aakash Kumar Singh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03850">https://arxiv.org/abs/2504.03850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03850">https://arxiv.org/pdf/2504.03850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03850]] Detection Limits and Statistical Separability of Tree Ring Watermarks in Rectified Flow-based Text-to-Image Generation Models(https://arxiv.org/abs/2504.03850)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, watermark</a></li>
<li><strong>Abstract: </strong>Tree-Ring Watermarking is a significant technique for authenticating AI-generated images. However, its effectiveness in rectified flow-based models remains unexplored, particularly given the inherent challenges of these models with noise latent inversion. Through extensive experimentation, we evaluated and compared the detection and separability of watermarks between SD 2.1 and FLUX.1-dev models. By analyzing various text guidance configurations and augmentation attacks, we demonstrate how inversion limitations affect both watermark recovery and the statistical separation between watermarked and unwatermarked images. Our findings provide valuable insights into the current limitations of Tree-Ring Watermarking in the current SOTA models and highlight the critical need for improved inversion methods to achieve reliable watermark detection and separability. The official implementation, dataset release and all experimental results are available at this \href{this https URL}{\textbf{link}}.</li>
</ul>

<h3>Title: Can ChatGPT Learn My Life From a Week of First-Person Video?</h3>
<ul>
<li><strong>Authors: </strong>Keegan Harris</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03857">https://arxiv.org/abs/2504.03857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03857">https://arxiv.org/pdf/2504.03857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03857]] Can ChatGPT Learn My Life From a Week of First-Person Video?(https://arxiv.org/abs/2504.03857)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Motivated by recent improvements in generative AI and wearable camera devices (e.g. smart glasses and AI-enabled pins), I investigate the ability of foundation models to learn about the wearer's personal life through first-person camera data. To test this, I wore a camera headset for 54 hours over the course of a week, generated summaries of various lengths (e.g. minute-long, hour-long, and day-long summaries), and fine-tuned both GPT-4o and GPT-4o-mini on the resulting summary hierarchy. By querying the fine-tuned models, we are able to learn what the models learned about me. The results are mixed: Both models learned basic information about me (e.g. approximate age, gender). Moreover, GPT-4o correctly deduced that I live in Pittsburgh, am a PhD student at CMU, am right-handed, and have a pet cat. However, both models also suffered from hallucination and would make up names for the individuals present in the video footage of my life.</li>
</ul>

<h3>Title: The Secret Life of CVEs</h3>
<ul>
<li><strong>Authors: </strong>Piotr Przymus, Mikołaj Fejzer, Jakub Narębski, Krzysztof Stencel</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03863">https://arxiv.org/abs/2504.03863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03863">https://arxiv.org/pdf/2504.03863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03863]] The Secret Life of CVEs(https://arxiv.org/abs/2504.03863)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The Common Vulnerabilities and Exposures (CVEs) system is a reference method for documenting publicly known information security weaknesses and exposures. This paper presents a study of the lifetime of CVEs in software projects and the risk factors affecting their existence. The study uses survival analysis to examine how features of programming languages, projects, and CVEs themselves impact the lifetime of CVEs. We suggest avenues for future research to investigate the effect of various factors on the resolution of vulnerabilities.</li>
</ul>

<h3>Title: Control Map Distribution using Map Query Bank for Online Map Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziming Liu, Leichen Wang, Ge Yang, Xinrun Li, Xingtao Hu, Hao Sun, Guangyu Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03868">https://arxiv.org/abs/2504.03868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03868">https://arxiv.org/pdf/2504.03868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03868]] Control Map Distribution using Map Query Bank for Online Map Generation(https://arxiv.org/abs/2504.03868)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Reliable autonomous driving systems require high-definition (HD) map that contains detailed map information for planning and navigation. However, pre-build HD map requires a large cost. Visual-based Online Map Generation (OMG) has become an alternative low-cost solution to build a local HD map. Query-based BEV Transformer has been a base model for this task. This model learns HD map predictions from an initial map queries distribution which is obtained by offline optimization on training set. Besides the quality of BEV feature, the performance of this model also highly relies on the capacity of initial map query distribution. However, this distribution is limited because the limited query number. To make map predictions optimal on each test sample, it is essential to generate a suitable initial distribution for each specific scenario. This paper proposes to decompose the whole HD map distribution into a set of point representations, namely map query bank (MQBank). To build specific map query initial distributions of different scenarios, low-cost standard definition map (SD map) data is introduced as a kind of prior knowledge. Moreover, each layer of map decoder network learns instance-level map query features, which will lose detailed information of each point. However, BEV feature map is a point-level dense feature. It is important to keep point-level information in map queries when interacting with BEV feature map. This can also be solved with map query bank method. Final experiments show a new insight on SD map prior and a new record on OpenLaneV2 benchmark with 40.5%, 45.7% mAP on vehicle lane and pedestrian area.</li>
</ul>

<h3>Title: 3D Scene Understanding Through Local Random Access Sequence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Wanhee Lee, Klemen Kotar, Rahul Mysore Venkatesh, Jared Watrous, Honglin Chen, Khai Loong Aw, Daniel L. K. Yamins</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03875">https://arxiv.org/abs/2504.03875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03875">https://arxiv.org/pdf/2504.03875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03875]] 3D Scene Understanding Through Local Random Access Sequence Modeling(https://arxiv.org/abs/2504.03875)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>3D scene understanding from single images is a pivotal problem in computer vision with numerous downstream applications in graphics, augmented reality, and robotics. While diffusion-based modeling approaches have shown promise, they often struggle to maintain object and scene consistency, especially in complex real-world scenarios. To address these limitations, we propose an autoregressive generative approach called Local Random Access Sequence (LRAS) modeling, which uses local patch quantization and randomly ordered sequence generation. By utilizing optical flow as an intermediate representation for 3D scene editing, our experiments demonstrate that LRAS achieves state-of-the-art novel view synthesis and 3D object manipulation capabilities. Furthermore, we show that our framework naturally extends to self-supervised depth estimation through a simple modification of the sequence design. By achieving strong performance on multiple 3D scene understanding tasks, LRAS provides a unified and effective framework for building the next generation of 3D vision models.</li>
</ul>

<h3>Title: Concept-based Rubrics Improve LLM Formative Assessment and Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Wei, Dennis Pearl, Matthew Beckman, Rebecca J. Passonneau</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03877">https://arxiv.org/abs/2504.03877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03877">https://arxiv.org/pdf/2504.03877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03877]] Concept-based Rubrics Improve LLM Formative Assessment and Data Synthesis(https://arxiv.org/abs/2504.03877)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Formative assessment in STEM topics aims to promote student learning by identifying students' current understanding, thus targeting how to promote further learning. Previous studies suggest that the assessment performance of current generative large language models (LLMs) on constructed responses to open-ended questions is significantly lower than that of supervised classifiers trained on high-quality labeled data. However, we demonstrate that concept-based rubrics can significantly enhance LLM performance, which narrows the gap between LLMs as off-the shelf assessment tools, and smaller supervised models, which need large amounts of training data. For datasets where concept-based rubrics allow LLMs to achieve strong performance, we show that the concept-based rubrics help the same LLMs generate high quality synthetic data for training lightweight, high-performance supervised models. Our experiments span diverse STEM student response datasets with labels of varying quality, including a new real-world dataset that contains some AI-assisted responses, which introduces additional considerations.</li>
</ul>

<h3>Title: WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments</h3>
<ul>
<li><strong>Authors: </strong>Jianhao Zheng, Zihan Zhu, Valentin Bieri, Marc Pollefeys, Songyou Peng, Iro Armeni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03886">https://arxiv.org/abs/2504.03886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03886">https://arxiv.org/pdf/2504.03886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03886]] WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments(https://arxiv.org/abs/2504.03886)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system designed to handle dynamic environments by leveraging uncertainty-aware geometric mapping. Unlike traditional SLAM systems, which assume static scenes, our approach integrates depth and uncertainty information to enhance tracking, mapping, and rendering performance in the presence of moving objects. We introduce an uncertainty map, predicted by a shallow multi-layer perceptron and DINOv2 features, to guide dynamic object removal during both tracking and mapping. This uncertainty map enhances dense bundle adjustment and Gaussian map optimization, improving reconstruction accuracy. Our system is evaluated on multiple datasets and demonstrates artifact-free view synthesis. Results showcase WildGS-SLAM's superior performance in dynamic environments compared to state-of-the-art methods.</li>
</ul>

<h3>Title: Using Attention Sinks to Identify and Evaluate Dormant Heads in Pretrained LLMs</h3>
<ul>
<li><strong>Authors: </strong>Pedro Sandoval-Segura, Xijun Wang, Ashwinee Panda, Micah Goldblum, Ronen Basri, Tom Goldstein, David Jacobs</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03889">https://arxiv.org/abs/2504.03889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03889">https://arxiv.org/pdf/2504.03889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03889]] Using Attention Sinks to Identify and Evaluate Dormant Heads in Pretrained LLMs(https://arxiv.org/abs/2504.03889)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-head attention is foundational to large language models (LLMs), enabling different heads to have diverse focus on relevant input tokens. However, learned behaviors like attention sinks, where the first token receives most attention despite limited semantic importance, challenge our understanding of multi-head attention. To analyze this phenomenon, we propose a new definition for attention heads dominated by attention sinks, known as dormant attention heads. We compare our definition to prior work in a model intervention study where we test whether dormant heads matter for inference by zeroing out the output of dormant attention heads. Using six pretrained models and five benchmark datasets, we find our definition to be more model and dataset-agnostic. Using our definition on most models, more than 4% of a model's attention heads can be zeroed while maintaining average accuracy, and zeroing more than 14% of a model's attention heads can keep accuracy to within 1% of the pretrained model's average accuracy. Further analysis reveals that dormant heads emerge early in pretraining and can transition between dormant and active states during pretraining. Additionally, we provide evidence that they depend on characteristics of the input text.</li>
</ul>

<h3>Title: Leveraging Gait Patterns as Biomarkers: An attention-guided Deep Multiple Instance Learning Network for Scoliosis Classification</h3>
<ul>
<li><strong>Authors: </strong>Haiqing Li, Yuzhi Guo, Feng Jiang, Qifeng Zhou, Hehuan Ma, Junzhou Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03894">https://arxiv.org/abs/2504.03894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03894">https://arxiv.org/pdf/2504.03894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03894]] Leveraging Gait Patterns as Biomarkers: An attention-guided Deep Multiple Instance Learning Network for Scoliosis Classification(https://arxiv.org/abs/2504.03894)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Scoliosis is a spinal curvature disorder that is difficult to detect early and can compress the chest cavity, impacting respiratory function and cardiac health. Especially for adolescents, delayed detection and treatment result in worsening compression. Traditional scoliosis detection methods heavily rely on clinical expertise, and X-ray imaging poses radiation risks, limiting large-scale early screening. We propose an Attention-Guided Deep Multi-Instance Learning method (Gait-MIL) to effectively capture discriminative features from gait patterns, which is inspired by ScoNet-MT's pioneering use of gait patterns for scoliosis detection. We evaluate our method on the first large-scale dataset based on gait patterns for scoliosis classification. The results demonstrate that our study improves the performance of using gait as a biomarker for scoliosis detection, significantly enhances detection accuracy for the particularly challenging Neutral cases, where subtle indicators are often overlooked. Our Gait-MIL also performs robustly in imbalanced scenarios, making it a promising tool for large-scale scoliosis screening.</li>
</ul>

<h3>Title: CliME: Evaluating Multimodal Climate Discourse on Social Media and the Climate Alignment Quotient (CAQ)</h3>
<ul>
<li><strong>Authors: </strong>Abhilekh Borah, Hasnat Md Abdullah, Kangda Wei, Ruihong Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03906">https://arxiv.org/abs/2504.03906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03906">https://arxiv.org/pdf/2504.03906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03906]] CliME: Evaluating Multimodal Climate Discourse on Social Media and the Climate Alignment Quotient (CAQ)(https://arxiv.org/abs/2504.03906)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rise of Large Language Models (LLMs) has raised questions about their ability to understand climate-related contexts. Though climate change dominates social media, analyzing its multimodal expressions is understudied, and current tools have failed to determine whether LLMs amplify credible solutions or spread unsubstantiated claims. To address this, we introduce CliME (Climate Change Multimodal Evaluation), a first-of-its-kind multimodal dataset, comprising 2579 Twitter and Reddit posts. The benchmark features a diverse collection of humorous memes and skeptical posts, capturing how these formats distill complex issues into viral narratives that shape public opinion and policy discussions. To systematically evaluate LLM performance, we present the Climate Alignment Quotient (CAQ), a novel metric comprising five distinct dimensions: Articulation, Evidence, Resonance, Transition, and Specificity. Additionally, we propose three analytical lenses: Actionability, Criticality, and Justice, to guide the assessment of LLM-generated climate discourse using CAQ. Our findings, based on the CAQ metric, indicate that while most evaluated LLMs perform relatively well in Criticality and Justice, they consistently underperform on the Actionability axis. Among the models evaluated, Claude 3.7 Sonnet achieves the highest overall performance. We publicly release our CliME dataset and code to foster further research in this domain.</li>
</ul>

<h3>Title: Secure Federated XGBoost with CUDA-accelerated Homomorphic Encryption via NVIDIA FLARE</h3>
<ul>
<li><strong>Authors: </strong>Ziyue Xu, Yuan-Ting Hsieh, Zhihong Zhang, Holger R. Roth, Chester Chen, Yan Cheng, Andrew Feng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03909">https://arxiv.org/abs/2504.03909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03909">https://arxiv.org/pdf/2504.03909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03909]] Secure Federated XGBoost with CUDA-accelerated Homomorphic Encryption via NVIDIA FLARE(https://arxiv.org/abs/2504.03909)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables collaborative model training across decentralized datasets. NVIDIA FLARE's Federated XGBoost extends the popular XGBoost algorithm to both vertical and horizontal federated settings, facilitating joint model development without direct data sharing. However, the initial implementation assumed mutual trust over the sharing of intermediate gradient statistics produced by the XGBoost algorithm, leaving potential vulnerabilities to honest-but-curious adversaries. This work introduces "Secure Federated XGBoost", an efficient solution to mitigate these risks. We implement secure federated algorithms for both vertical and horizontal scenarios, addressing diverse data security patterns. To secure the messages, we leverage homomorphic encryption (HE) to protect sensitive information during training. A novel plugin and processor interface seamlessly integrates HE into the Federated XGBoost pipeline, enabling secure aggregation over ciphertexts. We present both CPU-based and CUDA-accelerated HE plugins, demonstrating significant performance gains. Notably, our CUDA-accelerated HE implementation achieves up to 30x speedups in vertical Federated XGBoost compared to existing third-party solutions. By securing critical computation steps and encrypting sensitive assets, Secure Federated XGBoost provides robust data privacy guarantees, reinforcing the fundamental benefits of federated learning while maintaining high performance.</li>
</ul>

<h3>Title: Opening the Black-Box: Symbolic Regression with Kolmogorov-Arnold Networks for Energy Applications</h3>
<ul>
<li><strong>Authors: </strong>Nataly R. Panczyk, Omer F. Erdem, Majdi I. Radaideh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03913">https://arxiv.org/abs/2504.03913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03913">https://arxiv.org/pdf/2504.03913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03913]] Opening the Black-Box: Symbolic Regression with Kolmogorov-Arnold Networks for Energy Applications(https://arxiv.org/abs/2504.03913)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>While most modern machine learning methods offer speed and accuracy, few promise interpretability or explainability -- two key features necessary for highly sensitive industries, like medicine, finance, and engineering. Using eight datasets representative of one especially sensitive industry, nuclear power, this work compares a traditional feedforward neural network (FNN) to a Kolmogorov-Arnold Network (KAN). We consider not only model performance and accuracy, but also interpretability through model architecture and explainability through a post-hoc SHAP analysis. In terms of accuracy, we find KANs and FNNs comparable across all datasets, when output dimensionality is limited. KANs, which transform into symbolic equations after training, yield perfectly interpretable models while FNNs remain black-boxes. Finally, using the post-hoc explainability results from Kernel SHAP, we find that KANs learn real, physical relations from experimental data, while FNNs simply produce statistically accurate results. Overall, this analysis finds KANs a promising alternative to traditional machine learning methods, particularly in applications requiring both accuracy and comprehensibility.</li>
</ul>

<h3>Title: RF-BayesPhysNet: A Bayesian rPPG Uncertainty Estimation Method for Complex Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Rufei Ma, Chao Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03915">https://arxiv.org/abs/2504.03915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03915">https://arxiv.org/pdf/2504.03915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03915]] RF-BayesPhysNet: A Bayesian rPPG Uncertainty Estimation Method for Complex Scenarios(https://arxiv.org/abs/2504.03915)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Remote photoplethysmography (rPPG) technology infers heart rate by capturing subtle color changes in facial skin using a camera, demonstrating great potential in non-contact heart rate measurement. However, measurement accuracy significantly decreases in complex scenarios such as lighting changes and head movements compared to ideal laboratory conditions. Existing deep learning models often neglect the quantification of measurement uncertainty, limiting their credibility in dynamic scenes. To address the issue of insufficient rPPG measurement reliability in complex scenarios, this paper introduces Bayesian neural networks to the rPPG field for the first time, proposing the Robust Fusion Bayesian Physiological Network (RF-BayesPhysNet), which can model both aleatoric and epistemic uncertainty. It leverages variational inference to balance accuracy and computational efficiency. Due to the current lack of uncertainty estimation metrics in the rPPG field, this paper also proposes a new set of methods, using Spearman correlation coefficient, prediction interval coverage, and confidence interval width, to measure the effectiveness of uncertainty estimation methods under different noise conditions. Experiments show that the model, with only double the parameters compared to traditional network models, achieves a MAE of 2.56 on the UBFC-RPPG dataset, surpassing most models. It demonstrates good uncertainty estimation capability in no-noise and low-noise conditions, providing prediction confidence and significantly enhancing robustness in real-world applications. We have open-sourced the code at this https URL</li>
</ul>

<h3>Title: Improving Brain Disorder Diagnosis with Advanced Brain Function Representation and Kolmogorov-Arnold Networks</h3>
<ul>
<li><strong>Authors: </strong>Tyler Ward, Abdullah-Al-Zubaer Imran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03923">https://arxiv.org/abs/2504.03923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03923">https://arxiv.org/pdf/2504.03923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03923]] Improving Brain Disorder Diagnosis with Advanced Brain Function Representation and Kolmogorov-Arnold Networks(https://arxiv.org/abs/2504.03923)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Quantifying functional connectivity (FC), a vital metric for the diagnosis of various brain disorders, traditionally relies on the use of a pre-defined brain atlas. However, using such atlases can lead to issues regarding selection bias and lack of regard for specificity. Addressing this, we propose a novel transformer-based classification network (AFBR-KAN) with effective brain function representation to aid in diagnosing autism spectrum disorder (ASD). AFBR-KAN leverages Kolmogorov-Arnold Network (KAN) blocks replacing traditional multi-layer perceptron (MLP) components. Thorough experimentation reveals the effectiveness of AFBR-KAN in improving the diagnosis of ASD under various configurations of the model architecture. Our code is available at this https URL</li>
</ul>

<h3>Title: Random Normed k-Means: A Paradigm-Shift in Clustering within Probabilistic Metric Spaces</h3>
<ul>
<li><strong>Authors: </strong>Abderrafik Laakel Hemdanou, Youssef Achtoun, Mohammed Lamarti Sefian, Ismail Tahiri, Abdellatif El Afia</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03928">https://arxiv.org/abs/2504.03928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03928">https://arxiv.org/pdf/2504.03928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03928]] Random Normed k-Means: A Paradigm-Shift in Clustering within Probabilistic Metric Spaces(https://arxiv.org/abs/2504.03928)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing approaches remain largely constrained by traditional distance metrics, limiting their effectiveness in handling random data. In this work, we introduce the first k-means variant in the literature that operates within a probabilistic metric space, replacing conventional distance measures with a well-defined distance distribution function. This pioneering approach enables more flexible and robust clustering in both deterministic and random datasets, establishing a new foundation for clustering in stochastic environments. By adopting a probabilistic perspective, our method not only introduces a fresh paradigm but also establishes a rigorous theoretical framework that is expected to serve as a key reference for future clustering research involving random data. Extensive experiments on diverse real and synthetic datasets assess our model's effectiveness using widely recognized evaluation metrics, including Silhouette, Davies-Bouldin, Calinski Harabasz, the adjusted Rand index, and distortion. Comparative analyses against established methods such as k-means++, fuzzy c-means, and kernel probabilistic k-means demonstrate the superior performance of our proposed random normed k-means (RNKM) algorithm. Notably, RNKM exhibits a remarkable ability to identify nonlinearly separable structures, making it highly effective in complex clustering scenarios. These findings position RNKM as a groundbreaking advancement in clustering research, offering a powerful alternative to traditional techniques while addressing a long-standing gap in the literature. By bridging probabilistic metrics with clustering, this study provides a foundational reference for future developments and opens new avenues for advanced data analysis in dynamic, data-driven applications.</li>
</ul>

<h3>Title: Adaptation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Ke, Yifei Ming, Shafiq Joty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03931">https://arxiv.org/abs/2504.03931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03931">https://arxiv.org/pdf/2504.03931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03931]] Adaptation of Large Language Models(https://arxiv.org/abs/2504.03931)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This tutorial on adaptation of LLMs is designed to address the growing demand for models that go beyond the static capabilities of generic LLMs by providing an overview of dynamic, domain-specific, and task-adaptive LLM adaptation techniques. While general LLMs have demonstrated strong generalization across a variety of tasks, they often struggle to perform well in specialized domains such as finance, healthcare, and code generation for underrepresented languages. Additionally, their static nature limits their ability to evolve with the changing world, and they are often extremely large in size, making them impractical and costly to deploy at scale. As a result, the adaptation of LLMs has drawn much attention since the birth of LLMs and is of core importance, both for industry, which focuses on serving its targeted users, and academia, which can greatly benefit from small but powerful LLMs. To address this gap, this tutorial aims to provide an overview of the LLM adaptation techniques. We start with an introduction to LLM adaptation, from both the data perspective and the model perspective. We then emphasize how the evaluation metrics and benchmarks are different from other techniques. After establishing the problems, we explore various adaptation techniques. We categorize adaptation techniques into two main families. The first is parametric knowledge adaptation, which focuses on updating the parametric knowledge within LLMs. Additionally, we will discuss real-time adaptation techniques, including model editing, which allows LLMs to be updated dynamically in production environments. The second kind of adaptation is semi-parametric knowledge adaptation, where the goal is to update LLM parameters to better leverage external knowledge or tools through techniques like retrieval-augmented generation (RAG) and agent-based systems.</li>
</ul>

<h3>Title: YaleNLP @ PerAnsSumm 2025: Multi-Perspective Integration via Mixture-of-Agents for Enhanced Healthcare QA Summarization</h3>
<ul>
<li><strong>Authors: </strong>Dongsuk Jang, Alan Li, Arman Cohan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03932">https://arxiv.org/abs/2504.03932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03932">https://arxiv.org/pdf/2504.03932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03932]] YaleNLP @ PerAnsSumm 2025: Multi-Perspective Integration via Mixture-of-Agents for Enhanced Healthcare QA Summarization(https://arxiv.org/abs/2504.03932)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Automated summarization of healthcare community question-answering forums is challenging due to diverse perspectives presented across multiple user responses to each question. The PerAnsSumm Shared Task was therefore proposed to tackle this challenge by identifying perspectives from different answers and then generating a comprehensive answer to the question. In this study, we address the PerAnsSumm Shared Task using two complementary paradigms: (i) a training-based approach through QLoRA fine-tuning of LLaMA-3.3-70B-Instruct, and (ii) agentic approaches including zero- and few-shot prompting with frontier LLMs (LLaMA-3.3-70B-Instruct and GPT-4o) and a Mixture-of-Agents (MoA) framework that leverages a diverse set of LLMs by combining outputs from multi-layer feedback aggregation. For perspective span identification/classification, GPT-4o zero-shot achieves an overall score of 0.57, substantially outperforming the 0.40 score of the LLaMA baseline. With a 2-layer MoA configuration, we were able to improve LLaMA performance up by 28 percent to 0.51. For perspective-based summarization, GPT-4o zero-shot attains an overall score of 0.42 compared to 0.28 for the best LLaMA zero-shot, and our 2-layer MoA approach boosts LLaMA performance by 32 percent to 0.37. Furthermore, in few-shot setting, our results show that the sentence-transformer embedding-based exemplar selection provides more gain than manually selected exemplars on LLaMA models, although the few-shot prompting is not always helpful for GPT-4o. The YaleNLP team's approach ranked the overall second place in the shared task.</li>
</ul>

<h3>Title: Language Models Are Implicitly Continuous</h3>
<ul>
<li><strong>Authors: </strong>Samuele Marro, Davide Evangelista, X. Angelo Huang, Emanuele La Malfa, Michele Lombardi, Michael Wooldridge</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03933">https://arxiv.org/abs/2504.03933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03933">https://arxiv.org/pdf/2504.03933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03933]] Language Models Are Implicitly Continuous(https://arxiv.org/abs/2504.03933)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Language is typically modelled with discrete sequences. However, the most successful approaches to language modelling, namely neural networks, are continuous and smooth function approximators. In this work, we show that Transformer-based language models implicitly learn to represent sentences as continuous-time functions defined over a continuous input space. This phenomenon occurs in most state-of-the-art Large Language Models (LLMs), including Llama2, Llama3, Phi3, Gemma, Gemma2, and Mistral, and suggests that LLMs reason about language in ways that fundamentally differ from humans. Our work formally extends Transformers to capture the nuances of time and space continuity in both input and output space. Our results challenge the traditional interpretation of how LLMs understand language, with several linguistic and engineering implications.</li>
</ul>

<h3>Title: Commit-Reveal$^2$: Randomized Reveal Order Mitigates Last-Revealer Attacks in Commit-Reveal</h3>
<ul>
<li><strong>Authors: </strong>Suheyon Lee, Euisin Gee</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03936">https://arxiv.org/abs/2504.03936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03936">https://arxiv.org/pdf/2504.03936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03936]] Commit-Reveal$^2$: Randomized Reveal Order Mitigates Last-Revealer Attacks in Commit-Reveal(https://arxiv.org/abs/2504.03936)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Randomness generation is a fundamental component in blockchain systems, essential for tasks such as validator selection, zero-knowledge proofs, and decentralized finance operations. Traditional Commit-Reveal mechanisms provide simplicity and security but are susceptible to last revealer attacks, where an adversary can manipulate the random outcome by withholding their reveal. To address this vulnerability, we propose the Commit-Reveal$^2$ protocol, which employs a two-layer Commit-Reveal process to randomize the reveal order and mitigate the risk of such attacks. Additionally, we introduces a method to leverage off-chain networks to optimize communication costs and enhance efficiency. We implement a prototype of the proposed mechanism and publicly release the code to facilitate practical adoption and further research.</li>
</ul>

<h3>Title: Analysis of Robustness of a Large Game Corpus</h3>
<ul>
<li><strong>Authors: </strong>Mahsa Bazzaz, Seth Cooper</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03940">https://arxiv.org/abs/2504.03940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03940">https://arxiv.org/pdf/2504.03940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03940]] Analysis of Robustness of a Large Game Corpus(https://arxiv.org/abs/2504.03940)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Procedural content generation via machine learning (PCGML) in games involves using machine learning techniques to create game content such as maps and levels. 2D tile-based game levels have consistently served as a standard dataset for PCGML because they are a simplified version of game levels while maintaining the specific constraints typical of games, such as being solvable. In this work, we highlight the unique characteristics of game levels, including their structured discrete data nature, the local and global constraints inherent in the games, and the sensitivity of the game levels to small changes in input. We define the robustness of data as a measure of sensitivity to small changes in input that cause a change in output, and we use this measure to analyze and compare these levels to state-of-the-art machine learning datasets, showcasing the subtle differences in their nature. We also constructed a large dataset from four games inspired by popular classic tile-based games that showcase these characteristics and address the challenge of sparse data in PCGML by providing a significantly larger dataset than those currently available.</li>
</ul>

<h3>Title: ProbRes: Probabilistic Jump Diffusion for Open-World Egocentric Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Sanjoy Kundu, Shanmukha Vellamchetti, Sathyanarayanan N. Aakur</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03948">https://arxiv.org/abs/2504.03948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03948">https://arxiv.org/pdf/2504.03948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03948]] ProbRes: Probabilistic Jump Diffusion for Open-World Egocentric Activity Recognition(https://arxiv.org/abs/2504.03948)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Open-world egocentric activity recognition poses a fundamental challenge due to its unconstrained nature, requiring models to infer unseen activities from an expansive, partially observed search space. We introduce ProbRes, a Probabilistic Residual search framework based on jump-diffusion that efficiently navigates this space by balancing prior-guided exploration with likelihood-driven exploitation. Our approach integrates structured commonsense priors to construct a semantically coherent search space, adaptively refines predictions using Vision-Language Models (VLMs) and employs a stochastic search mechanism to locate high-likelihood activity labels while minimizing exhaustive enumeration efficiently. We systematically evaluate ProbRes across multiple openness levels (L0 - L3), demonstrating its adaptability to increasing search space complexity. In addition to achieving state-of-the-art performance on benchmark datasets (GTEA Gaze, GTEA Gaze+, EPIC-Kitchens, and Charades-Ego), we establish a clear taxonomy for open-world recognition, delineating the challenges and methodological advancements necessary for egocentric activity understanding. Our results highlight the importance of structured search strategies, paving the way for scalable and efficient open-world activity recognition.</li>
</ul>

<h3>Title: TGraphX: Tensor-Aware Graph Neural Network for Multi-Dimensional Feature Learning</h3>
<ul>
<li><strong>Authors: </strong>Arash Sajjadi, Mark Eramian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03953">https://arxiv.org/abs/2504.03953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03953">https://arxiv.org/pdf/2504.03953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03953]] TGraphX: Tensor-Aware Graph Neural Network for Multi-Dimensional Feature Learning(https://arxiv.org/abs/2504.03953)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>TGraphX presents a novel paradigm in deep learning by unifying convolutional neural networks (CNNs) with graph neural networks (GNNs) to enhance visual reasoning tasks. Traditional CNNs excel at extracting rich spatial features from images but lack the inherent capability to model inter-object relationships. Conversely, conventional GNNs typically rely on flattened node features, thereby discarding vital spatial details. TGraphX overcomes these limitations by employing CNNs to generate multi-dimensional node features (e.g., (3*128*128) tensors) that preserve local spatial semantics. These spatially aware nodes participate in a graph where message passing is performed using 1*1 convolutions, which fuse adjacent features while maintaining their structure. Furthermore, a deep CNN aggregator with residual connections is used to robustly refine the fused messages, ensuring stable gradient flow and end-to-end trainability. Our approach not only bridges the gap between spatial feature extraction and relational reasoning but also demonstrates significant improvements in object detection refinement and ensemble reasoning.</li>
</ul>

<h3>Title: Practical Poisoning Attacks against Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Baolei Zhang, Yuxi Chen, Minghong Fang, Zhuqing Liu, Lihai Nie, Tong Li, Zheli Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03957">https://arxiv.org/abs/2504.03957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03957">https://arxiv.org/pdf/2504.03957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03957]] Practical Poisoning Attacks against Retrieval-Augmented Generation(https://arxiv.org/abs/2504.03957)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive natural language processing abilities but face challenges such as hallucination and outdated knowledge. Retrieval-Augmented Generation (RAG) has emerged as a state-of-the-art approach to mitigate these issues. While RAG enhances LLM outputs, it remains vulnerable to poisoning attacks. Recent studies show that injecting poisoned text into the knowledge database can compromise RAG systems, but most existing attacks assume that the attacker can insert a sufficient number of poisoned texts per query to outnumber correct-answer texts in retrieval, an assumption that is often unrealistic. To address this limitation, we propose CorruptRAG, a practical poisoning attack against RAG systems in which the attacker injects only a single poisoned text, enhancing both feasibility and stealth. Extensive experiments across multiple datasets demonstrate that CorruptRAG achieves higher attack success rates compared to existing baselines.</li>
</ul>

<h3>Title: Clinical ModernBERT: An efficient and long context encoder for biomedical text</h3>
<ul>
<li><strong>Authors: </strong>Simon A. Lee, Anthony Wu, Jeffrey N. Chiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03964">https://arxiv.org/abs/2504.03964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03964">https://arxiv.org/pdf/2504.03964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03964]] Clinical ModernBERT: An efficient and long context encoder for biomedical text(https://arxiv.org/abs/2504.03964)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce Clinical ModernBERT, a transformer based encoder pretrained on large scale biomedical literature, clinical notes, and medical ontologies, incorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with their textual descriptions. Building on ModernBERT the current state of the art natural language text encoder featuring architectural upgrades such as rotary positional embeddings (RoPE), Flash Attention, and extended context length up to 8,192 tokens our model adapts these innovations specifically for biomedical and clinical domains. Clinical ModernBERT excels at producing semantically rich representations tailored for long context tasks. We validate this both by analyzing its pretrained weights and through empirical evaluation on a comprehensive suite of clinical NLP benchmarks.</li>
</ul>

<h3>Title: V-CEM: Bridging Performance and Intervenability in Concept-based Models</h3>
<ul>
<li><strong>Authors: </strong>Francesco De Santis, Gabriele Ciravegna, Philippe Bich, Danilo Giordano, Tania Cerquitelli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03978">https://arxiv.org/abs/2504.03978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03978">https://arxiv.org/pdf/2504.03978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03978]] V-CEM: Bridging Performance and Intervenability in Concept-based Models(https://arxiv.org/abs/2504.03978)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Concept-based eXplainable AI (C-XAI) is a rapidly growing research field that enhances AI model interpretability by leveraging intermediate, human-understandable concepts. This approach not only enhances model transparency but also enables human intervention, allowing users to interact with these concepts to refine and improve the model's performance. Concept Bottleneck Models (CBMs) explicitly predict concepts before making final decisions, enabling interventions to correct misclassified concepts. While CBMs remain effective in Out-Of-Distribution (OOD) settings with intervention, they struggle to match the performance of black-box models. Concept Embedding Models (CEMs) address this by learning concept embeddings from both concept predictions and input data, enhancing In-Distribution (ID) accuracy but reducing the effectiveness of interventions, especially in OOD scenarios. In this work, we propose the Variational Concept Embedding Model (V-CEM), which leverages variational inference to improve intervention responsiveness in CEMs. We evaluated our model on various textual and visual datasets in terms of ID performance, intervention responsiveness in both ID and OOD settings, and Concept Representation Cohesiveness (CRC), a metric we propose to assess the quality of the concept embedding representations. The results demonstrate that V-CEM retains CEM-level ID performance while achieving intervention effectiveness similar to CBM in OOD settings, effectively reducing the gap between interpretability (intervention) and generalization (performance).</li>
</ul>

<h3>Title: Structured Extraction of Process Structure Properties Relationships in Materials Science</h3>
<ul>
<li><strong>Authors: </strong>Amit K Verma, Zhisong Zhang, Junwon Seo, Robin Kuo, Runbo Jiang, Emma Strubell, Anthony D Rollett</a></li>
<li><strong>Subjects: </strong>cs.CL, cond-mat.mtrl-sci, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03979">https://arxiv.org/abs/2504.03979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03979">https://arxiv.org/pdf/2504.03979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03979]] Structured Extraction of Process Structure Properties Relationships in Materials Science(https://arxiv.org/abs/2504.03979)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>With the advent of large language models (LLMs), the vast unstructured text within millions of academic papers is increasingly accessible for materials discovery, although significant challenges remain. While LLMs offer promising few- and zero-shot learning capabilities, particularly valuable in the materials domain where expert annotations are scarce, general-purpose LLMs often fail to address key materials-specific queries without further adaptation. To bridge this gap, fine-tuning LLMs on human-labeled data is essential for effective structured knowledge extraction. In this study, we introduce a novel annotation schema designed to extract generic process-structure-properties relationships from scientific literature. We demonstrate the utility of this approach using a dataset of 128 abstracts, with annotations drawn from two distinct domains: high-temperature materials (Domain I) and uncertainty quantification in simulating materials microstructure (Domain II). Initially, we developed a conditional random field (CRF) model based on MatBERT, a domain-specific BERT variant, and evaluated its performance on Domain I. Subsequently, we compared this model with a fine-tuned LLM (GPT-4o from OpenAI) under identical conditions. Our results indicate that fine-tuning LLMs can significantly improve entity extraction performance over the BERT-CRF baseline on Domain I. However, when additional examples from Domain II were incorporated, the performance of the BERT-CRF model became comparable to that of the GPT-4o model. These findings underscore the potential of our schema for structured knowledge extraction and highlight the complementary strengths of both modeling approaches.</li>
</ul>

<h3>Title: Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Siddharth Srikanth, Varun Bhatt, Boshen Zhang, Werner Hager, Charles Michael Lewis, Katia P. Sycara, Aaquib Tabrez, Stefanos Nikolaidis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03991">https://arxiv.org/abs/2504.03991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03991">https://arxiv.org/pdf/2504.03991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03991]] Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models(https://arxiv.org/abs/2504.03991)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding how humans collaborate and communicate in teams is essential for improving human-agent teaming and AI-assisted decision-making. However, relying solely on data from large-scale user studies is impractical due to logistical, ethical, and practical constraints, necessitating synthetic models of multiple diverse human behaviors. Recently, agents powered by Large Language Models (LLMs) have been shown to emulate human-like behavior in social settings. But, obtaining a large set of diverse behaviors requires manual effort in the form of designing prompts. On the other hand, Quality Diversity (QD) optimization has been shown to be capable of generating diverse Reinforcement Learning (RL) agent behavior. In this work, we combine QD optimization with LLM-powered agents to iteratively search for prompts that generate diverse team behavior in a long-horizon, multi-step collaborative environment. We first show, through a human-subjects experiment (n=54 participants), that humans exhibit diverse coordination and communication behavior in this domain. We then show that our approach can effectively replicate trends from human teaming data and also capture behaviors that are not easily observed without collecting large amounts of data. Our findings highlight the combination of QD and LLM-powered agents as an effective tool for studying teaming and communication strategies in multi-agent collaboration.</li>
</ul>

<h3>Title: DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Maksim Siniukov, Di Chang, Minh Tran, Hongkun Gong, Ashutosh Chaubey, Mohammad Soleymani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04010">https://arxiv.org/abs/2504.04010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04010">https://arxiv.org/pdf/2504.04010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04010]] DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion(https://arxiv.org/abs/2504.04010)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address these challenges, we introduce DiTaiListener, powered by a video diffusion model with multimodal conditions. Our approach first generates short segments of listener responses conditioned on the speaker's speech and facial motions with DiTaiListener-Gen. It then refines the transitional frames via DiTaiListener-Edit for a seamless transition. Specifically, DiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener head portrait generation by introducing a Causal Temporal Multimodal Adapter (CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter integrates speakers' input in a causal manner into the video generation process to ensure temporally coherent listener responses. For long-form video generation, we introduce DiTaiListener-Edit, a transition refinement video-to-video diffusion model. The model fuses video segments into smooth and continuous videos, ensuring temporal consistency in facial expressions and image quality when merging short video segments produced by DiTaiListener-Gen. Quantitatively, DiTaiListener achieves the state-of-the-art performance on benchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion representation (+6.1% in FD metric on VICO) spaces. User studies confirm the superior performance of DiTaiListener, with the model being the clear preference in terms of feedback, diversity, and smoothness, outperforming competitors by a significant margin.</li>
</ul>

<h3>Title: Foundation Models for Time Series: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Siva Rama Krishna Kottapalli, Karthik Hubli, Sandeep Chandrashekhara, Garima Jain, Sunayana Hubli, Gayathri Botla, Ramesh Doddaiah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04011">https://arxiv.org/abs/2504.04011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04011">https://arxiv.org/pdf/2504.04011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04011]] Foundation Models for Time Series: A Survey(https://arxiv.org/abs/2504.04011)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based foundation models have emerged as a dominant paradigm in time series analysis, offering unprecedented capabilities in tasks such as forecasting, anomaly detection, classification, trend analysis and many more time series analytical tasks. This survey provides a comprehensive overview of the current state of the art pre-trained foundation models, introducing a novel taxonomy to categorize them across several dimensions. Specifically, we classify models by their architecture design, distinguishing between those leveraging patch-based representations and those operating directly on raw sequences. The taxonomy further includes whether the models provide probabilistic or deterministic predictions, and whether they are designed to work with univariate time series or can handle multivariate time series out of the box. Additionally, the taxonomy encompasses model scale and complexity, highlighting differences between lightweight architectures and large-scale foundation models. A unique aspect of this survey is its categorization by the type of objective function employed during training phase. By synthesizing these perspectives, this survey serves as a resource for researchers and practitioners, providing insights into current trends and identifying promising directions for future research in transformer-based time series modeling.</li>
</ul>

<h3>Title: Detection-Friendly Nonuniformity Correction: A Union Framework for Infrared UAVTarget Detection</h3>
<ul>
<li><strong>Authors: </strong>Houzhang Fang, Xiaolin Wang, Zengyang Li, Lu Wang, Qingshan Li, Yi Chang, Luxin Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04012">https://arxiv.org/abs/2504.04012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04012">https://arxiv.org/pdf/2504.04012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04012]] Detection-Friendly Nonuniformity Correction: A Union Framework for Infrared UAVTarget Detection(https://arxiv.org/abs/2504.04012)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Infrared unmanned aerial vehicle (UAV) images captured using thermal detectors are often affected by temperature dependent low-frequency nonuniformity, which significantly reduces the contrast of the images. Detecting UAV targets under nonuniform conditions is crucial in UAV surveillance applications. Existing methods typically treat infrared nonuniformity correction (NUC) as a preprocessing step for detection, which leads to suboptimal performance. Balancing the two tasks while enhancing detection beneficial information remains challenging. In this paper, we present a detection-friendly union framework, termed UniCD, that simultaneously addresses both infrared NUC and UAV target detection tasks in an end-to-end manner. We first model NUC as a small number of parameter estimation problem jointly driven by priors and data to generate detection-conducive images. Then, we incorporate a new auxiliary loss with target mask supervision into the backbone of the infrared UAV target detection network to strengthen target features while suppressing the background. To better balance correction and detection, we introduce a detection-guided self-supervised loss to reduce feature discrepancies between the two tasks, thereby enhancing detection robustness to varying nonuniformity levels. Additionally, we construct a new benchmark composed of 50,000 infrared images in various nonuniformity types, multi-scale UAV targets and rich backgrounds with target annotations, called IRBFD. Extensive experiments on IRBFD demonstrate that our UniCD is a robust union framework for NUC and UAV target detection while achieving real-time processing capabilities. Dataset can be available at this https URL.</li>
</ul>

<h3>Title: Multi-resolution Score-Based Variational Graphical Diffusion for Causal Disaster System Modeling and Inference</h3>
<ul>
<li><strong>Authors: </strong>Xuechun Li, Shan Gao, Susu Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04015">https://arxiv.org/abs/2504.04015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04015">https://arxiv.org/pdf/2504.04015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04015]] Multi-resolution Score-Based Variational Graphical Diffusion for Causal Disaster System Modeling and Inference(https://arxiv.org/abs/2504.04015)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Complex systems with intricate causal dependencies challenge accurate prediction. Effective modeling requires precise physical process representation, integration of interdependent factors, and incorporation of multi-resolution observational data. These systems manifest in both static scenarios with instantaneous causal chains and temporal scenarios with evolving dynamics, complicating modeling efforts. Current methods struggle to simultaneously handle varying resolutions, capture physical relationships, model causal dependencies, and incorporate temporal dynamics, especially with inconsistently sampled data from diverse sources. We introduce Temporal-SVGDM: Score-based Variational Graphical Diffusion Model for Multi-resolution observations. Our framework constructs individual SDEs for each variable at its native resolution, then couples these SDEs through a causal score mechanism where parent nodes inform child nodes' evolution. This enables unified modeling of both immediate causal effects in static scenarios and evolving dependencies in temporal scenarios. In temporal models, state representations are processed through a sequence prediction model to predict future states based on historical patterns and causal relationships. Experiments on real-world datasets demonstrate improved prediction accuracy and causal understanding compared to existing methods, with robust performance under varying levels of background knowledge. Our model exhibits graceful degradation across different disaster types, successfully handling both static earthquake scenarios and temporal hurricane and wildfire scenarios, while maintaining superior performance even with limited data.</li>
</ul>

<h3>Title: Window Token Concatenation for Efficient Visual Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yifan Li, Wentao Bao, Botao Ye, Zhen Tan, Tianlong Chen, Huan Liu, Yu Kong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04024">https://arxiv.org/abs/2504.04024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04024">https://arxiv.org/pdf/2504.04024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04024]] Window Token Concatenation for Efficient Visual Large Language Models(https://arxiv.org/abs/2504.04024)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To effectively reduce the visual tokens in Visual Large Language Models (VLLMs), we propose a novel approach called Window Token Concatenation (WiCo). Specifically, we employ a sliding window to concatenate spatially adjacent visual tokens. However, directly concatenating these tokens may group diverse tokens into one, and thus obscure some fine details. To address this challenge, we propose fine-tuning the last few layers of the vision encoder to adaptively adjust the visual tokens, encouraging that those within the same window exhibit similar features. To further enhance the performance on fine-grained visual understanding tasks, we introduce WiCo+, which decomposes the visual tokens in later layers of the LLM. Such a design enjoys the merits of the large perception field of the LLM for fine-grained visual understanding while keeping a small number of visual tokens for efficient inference. We perform extensive experiments on both coarse- and fine-grained visual understanding tasks based on LLaVA-1.5 and Shikra, showing better performance compared with existing token reduction projectors. The code is available: this https URL.</li>
</ul>

<h3>Title: Artificial intelligence application in lymphoma diagnosis: from Convolutional Neural Network to Vision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Daniel Rivera, Jacob Huddin, Alexander Banerjee, Rongzhen Zhang, Brenda Mai, Hanadi El Achi, Jacob Armstrong, Amer Wahed, Andy Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04025">https://arxiv.org/abs/2504.04025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04025">https://arxiv.org/pdf/2504.04025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04025]] Artificial intelligence application in lymphoma diagnosis: from Convolutional Neural Network to Vision Transformer(https://arxiv.org/abs/2504.04025)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recently, vision transformers were shown to be capable of outperforming convolutional neural networks when pretrained on sufficiently large datasets. Vision transformer models show good accuracy on large scale datasets, with features of multi-modal training. Due to their promising feature detection, we aim to explore vision transformer models for diagnosis of anaplastic large cell lymphoma versus classical Hodgkin lymphoma using pathology whole slide images of HE slides. We compared the classification performance of the vision transformer to our previously designed convolutional neural network on the same dataset. The dataset includes whole slide images of HE slides for 20 cases, including 10 cases in each diagnostic category. From each whole slide image, 60 image patches having size of 100 by 100 pixels and at magnification of 20 were obtained to yield 1200 image patches, from which 90 percent were used for training, 9 percent for validation, and 10 percent for testing. The test results from the convolutional neural network model had previously shown an excellent diagnostic accuracy of 100 percent. The test results from the vision transformer model also showed a comparable accuracy at 100 percent. To the best of the authors' knowledge, this is the first direct comparison of predictive performance between a vision transformer model and a convolutional neural network model using the same dataset of lymphoma. Overall, convolutional neural network has a more mature architecture than vision transformer and is usually the best choice when large scale pretraining is not an available option. Nevertheless, our current study shows comparable and excellent accuracy of vision transformer compared to that of convolutional neural network even with a relatively small dataset of anaplastic large cell lymphoma and classical Hodgkin lymphoma.</li>
</ul>

<h3>Title: Contrastive and Variational Approaches in Self-Supervised Learning for Complex Data Mining</h3>
<ul>
<li><strong>Authors: </strong>Yingbin Liang, Lu Dai, Shuo Shi, Minghao Dai, Junliang Du, Haige Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04032">https://arxiv.org/abs/2504.04032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04032">https://arxiv.org/pdf/2504.04032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04032]] Contrastive and Variational Approaches in Self-Supervised Learning for Complex Data Mining(https://arxiv.org/abs/2504.04032)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Complex data mining has wide application value in many fields, especially in the feature extraction and classification tasks of unlabeled data. This paper proposes an algorithm based on self-supervised learning and verifies its effectiveness through experiments. The study found that in terms of the selection of optimizer and learning rate, the combination of AdamW optimizer and 0.002 learning rate performed best in all evaluation indicators, indicating that the adaptive optimization method can improve the performance of the model in complex data mining tasks. In addition, the ablation experiment further analyzed the contribution of each module. The results show that contrastive learning, variational modules, and data augmentation strategies play a key role in the generalization ability and robustness of the model. Through the convergence curve analysis of the loss function, the experiment verifies that the method can converge stably during the training process and effectively avoid serious overfitting. Further experimental results show that the model has strong adaptability on different data sets, can effectively extract high-quality features from unlabeled data, and improves classification accuracy. At the same time, under different data distribution conditions, the method can still maintain high detection accuracy, proving its applicability in complex data environments. This study analyzed the role of self-supervised learning methods in complex data mining through systematic experiments and verified its advantages in improving feature extraction quality, optimizing classification performance, and enhancing model stability</li>
</ul>

<h3>Title: Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks and Defenses</h3>
<ul>
<li><strong>Authors: </strong>Ehsanul Kabir, Lucas Craig, Shagufta Mehnaz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04033">https://arxiv.org/abs/2504.04033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04033">https://arxiv.org/pdf/2504.04033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04033]] Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks and Defenses(https://arxiv.org/abs/2504.04033)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack</a></li>
<li><strong>Abstract: </strong>As machine learning (ML) technologies become more prevalent in privacy-sensitive areas like healthcare and finance, eventually incorporating sensitive information in building data-driven algorithms, it is vital to scrutinize whether these data face any privacy leakage risks. One potential threat arises from an adversary querying trained models using the public, non-sensitive attributes of entities in the training data to infer their private, sensitive attributes, a technique known as the attribute inference attack. This attack is particularly deceptive because, while it may perform poorly in predicting sensitive attributes across the entire dataset, it excels at predicting the sensitive attributes of records from a few vulnerable groups, a phenomenon known as disparate vulnerability. This paper illustrates that an adversary can take advantage of this disparity to carry out a series of new attacks, showcasing a threat level beyond previous imagination. We first develop a novel inference attack called the disparity inference attack, which targets the identification of high-risk groups within the dataset. We then introduce two targeted variations of the attribute inference attack that can identify and exploit a vulnerable subset of the training data, marking the first instances of targeted attacks in this category, achieving significantly higher accuracy than untargeted versions. We are also the first to introduce a novel and effective disparity mitigation technique that simultaneously preserves model performance and prevents any risk of targeted attacks.</li>
</ul>

<h3>Title: UCS: A Universal Model for Curvilinear Structure Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Dianshuo Li, Li Chen, Yunxiang Cao, Kai Zhu, Jun Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04034">https://arxiv.org/abs/2504.04034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04034">https://arxiv.org/pdf/2504.04034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04034]] UCS: A Universal Model for Curvilinear Structure Segmentation(https://arxiv.org/abs/2504.04034)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Curvilinear structure segmentation (CSS) is vital in various domains, including medical imaging, landscape analysis, industrial surface inspection, and plant analysis. While existing methods achieve high performance within specific domains, their generalizability is limited. On the other hand, large-scale models such as Segment Anything Model (SAM) exhibit strong generalization but are not optimized for curvilinear structures. Existing adaptations of SAM primarily focus on general object segmentation and lack specialized design for CSS tasks. To bridge this gap, we propose the Universal Curvilinear structure Segmentation (\textit{UCS}) model, which adapts SAM to CSS tasks while enhancing its generalization. \textit{UCS} features a novel encoder architecture integrating a pretrained SAM encoder with two innovations: a Sparse Adapter, strategically inserted to inherit the pre-trained SAM encoder's generalization capability while minimizing the number of fine-tuning parameters, and a Prompt Generation module, which leverages Fast Fourier Transform with a high-pass filter to generate curve-specific prompts. Furthermore, the \textit{UCS} incorporates a mask decoder that eliminates reliance on manual interaction through a dual-compression module: a Hierarchical Feature Compression module, which aggregates the outputs of the sampled encoder to enhance detail preservation, and a Guidance Feature Compression module, which extracts and compresses image-driven guidance features. Evaluated on a comprehensive multi-domain dataset, including an in-house dataset covering eight natural curvilinear structures, \textit{UCS} demonstrates state-of-the-art generalization and open-set segmentation performance across medical, engineering, natural, and plant imagery, establishing a new benchmark for universal CSS.</li>
</ul>

<h3>Title: SyLeR: A Framework for Explicit Syllogistic Legal Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kepu Zhang, Weijie Yu, Zhongxiang Sun, Jun Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04042">https://arxiv.org/abs/2504.04042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04042">https://arxiv.org/pdf/2504.04042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04042]] SyLeR: A Framework for Explicit Syllogistic Legal Reasoning in Large Language Models(https://arxiv.org/abs/2504.04042)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Syllogistic reasoning is a fundamental aspect of legal decision-making, enabling logical conclusions by connecting general legal principles with specific case facts. Although existing large language models (LLMs) can generate responses to legal questions, they fail to perform explicit syllogistic reasoning, often producing implicit and unstructured answers that lack explainability and trustworthiness. To address this limitation, we propose SyLeR, a novel framework that empowers LLMs to engage in explicit syllogistic legal reasoning. SyLeR integrates a tree-structured hierarchical retrieval mechanism to effectively combine relevant legal statutes and precedent cases, forming comprehensive major premises. This is followed by a two-stage fine-tuning process: supervised fine-tuning warm-up establishes a foundational understanding of syllogistic reasoning, while reinforcement learning with a structure-aware reward mechanism refines the ability of the model to generate diverse logically sound and well-structured reasoning paths. We conducted extensive experiments across various dimensions, including in-domain and cross-domain user groups (legal laypersons and practitioners), multiple languages (Chinese and French), and different LLM backbones (legal-specific and open-domain LLMs). The results show that SyLeR significantly improves response accuracy and consistently delivers explicit, explainable, and trustworthy legal reasoning.</li>
</ul>

<h3>Title: FISH-Tuning: Enhancing PEFT Methods with Fisher Information</h3>
<ul>
<li><strong>Authors: </strong>Kang Xue, Ming Dong, Xinhui Tu, Tingting He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04050">https://arxiv.org/abs/2504.04050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04050">https://arxiv.org/pdf/2504.04050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04050]] FISH-Tuning: Enhancing PEFT Methods with Fisher Information(https://arxiv.org/abs/2504.04050)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid growth in the parameter size of Large Language Models (LLMs) has led to the development of Parameter-Efficient Fine-Tuning (PEFT) methods to alleviate the computational costs of fine-tuning. Among these, Fisher Induced Sparse uncHanging (FISH) Mask is a selection-based PEFT technique that identifies a subset of pre-trained parameters for fine-tuning based on approximate Fisher information. However, the integration of FISH Mask with other PEFT methods, such as LoRA and Adapters, remains underexplored. In this paper, we propose FISH-Tuning, a novel approach that incorporates FISH Mask into addition-based and reparameterization-based PEFT methods, including LoRA, Adapters, and their variants. By leveraging Fisher information to select critical parameters within these methods, FISH-Tuning achieves superior performance without additional memory overhead or inference latency. Experimental results across various datasets and pre-trained models demonstrate that FISH-Tuning consistently outperforms the vanilla PEFT methods with the same proportion of trainable parameters.</li>
</ul>

<h3>Title: Can You Count to Nine? A Human Evaluation Benchmark for Counting Limits in Modern Text-to-Video Models</h3>
<ul>
<li><strong>Authors: </strong>Xuyang Guo, Zekai Huang, Jiayan Huo, Yingyu Liang, Zhenmei Shi, Zhao Song, Jiahao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04051">https://arxiv.org/abs/2504.04051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04051">https://arxiv.org/pdf/2504.04051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04051]] Can You Count to Nine? A Human Evaluation Benchmark for Counting Limits in Modern Text-to-Video Models(https://arxiv.org/abs/2504.04051)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models have driven significant progress in a variety of AI tasks, including text-to-video generation, where models like Video LDM and Stable Video Diffusion can produce realistic, movie-level videos from textual instructions. Despite these advances, current text-to-video models still face fundamental challenges in reliably following human commands, particularly in adhering to simple numerical constraints. In this work, we present T2VCountBench, a specialized benchmark aiming at evaluating the counting capability of SOTA text-to-video models as of 2025. Our benchmark employs rigorous human evaluations to measure the number of generated objects and covers a diverse range of generators, covering both open-source and commercial models. Extensive experiments reveal that all existing models struggle with basic numerical tasks, almost always failing to generate videos with an object count of 9 or fewer. Furthermore, our comprehensive ablation studies explore how factors like video style, temporal dynamics, and multilingual inputs may influence counting performance. We also explore prompt refinement techniques and demonstrate that decomposing the task into smaller subtasks does not easily alleviate these limitations. Our findings highlight important challenges in current text-to-video generation and provide insights for future research aimed at improving adherence to basic numerical constraints.</li>
</ul>

<h3>Title: VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Wang, Heyang Liu, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04060">https://arxiv.org/abs/2504.04060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04060">https://arxiv.org/pdf/2504.04060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04060]] VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation(https://arxiv.org/abs/2504.04060)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speech large language models (LLMs) have emerged as a prominent research focus in speech processing. We propose VocalNet-1B and VocalNet-8B, a series of high-performance, low-latency speech LLMs enabled by a scalable and model-agnostic training framework for real-time voice interaction. Departing from the conventional next-token prediction (NTP), we introduce multi-token prediction (MTP), a novel approach optimized for speech LLMs that simultaneously improves generation speed and quality. Experiments show that VocalNet outperforms mainstream Omni LLMs despite using significantly less training data, while also surpassing existing open-source speech LLMs by a substantial margin. To support reproducibility and community advancement, we will open-source all model weights, inference code, training data, and framework implementations upon publication.</li>
</ul>

<h3>Title: Analysis of Light-Weight Cryptography Algorithms for UAV-Networks</h3>
<ul>
<li><strong>Authors: </strong>Aanchal Patel, Aswani Kumar Cherukuri</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04063">https://arxiv.org/abs/2504.04063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04063">https://arxiv.org/pdf/2504.04063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04063]] Analysis of Light-Weight Cryptography Algorithms for UAV-Networks(https://arxiv.org/abs/2504.04063)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust</a></li>
<li><strong>Abstract: </strong>Unmanned Aerial Vehicles are increasingly utilized across various domains, necessitating robust security measures for their communication networks. The ASCON family, a NIST finalist in lightweight cryptography standards, is known for its simplistic yet resilient design, making it well-suited for resource-constrained environments characterized by limited processing capabilities and energy reservoirs. This study focuses on understanding the integration and assessment of the ASCON encryption algorithm in UAV networks, emphasizing its potential as a lightweight and efficient cryptographic solution. The research objectives aim to evaluate ASCON variants' effectiveness in providing security comparable to AES-128 while exhibiting lower computational cost and energy consumption within simulated UAV network environments. Comparative analysis assesses performance metrics such as encryption and decryption speeds, resource utilization, and resistance to cryptographic vulnerabilities against established algorithms like AES. Performance metrics, including peak and average execution times, overall throughput, and security properties against various cryptographic attacks, are measured and analysed to determine the most suitable cryptographic algorithm for UAV communication systems. Performance results indicate that ASCON-128a as the optimal choice for UAV communication systems requiring a balance between efficiency and security. Its superior performance metrics, robust security properties, and suitability for resource-constrained environments position it as the preferred solution for securing UAV communication networks. By leveraging the strengths of ASCON-128a, UAV communication systems can achieve optimal performance and security, ensuring reliable and secure communication in challenging operational environments.</li>
</ul>

<h3>Title: Collaboration and Controversy Among Experts: Rumor Early Detection by Tuning a Comment Generator</h3>
<ul>
<li><strong>Authors: </strong>Bing Wang, Bingrui Zhao, Ximing Li, Changchun Li, Wanfu Gao, Shengsheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04076">https://arxiv.org/abs/2504.04076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04076">https://arxiv.org/pdf/2504.04076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04076]] Collaboration and Controversy Among Experts: Rumor Early Detection by Tuning a Comment Generator(https://arxiv.org/abs/2504.04076)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Over the past decade, social media platforms have been key in spreading rumors, leading to significant negative impacts. To counter this, the community has developed various Rumor Detection (RD) algorithms to automatically identify them using user comments as evidence. However, these RD methods often fail in the early stages of rumor propagation when only limited user comments are available, leading the community to focus on a more challenging topic named Rumor Early Detection (RED). Typically, existing RED methods learn from limited semantics in early comments. However, our preliminary experiment reveals that the RED models always perform best when the number of training and test comments is consistent and extensive. This inspires us to address the RED issue by generating more human-like comments to support this hypothesis. To implement this idea, we tune a comment generator by simulating expert collaboration and controversy and propose a new RED framework named CAMERED. Specifically, we integrate a mixture-of-expert structure into a generative language model and present a novel routing network for expert collaboration. Additionally, we synthesize a knowledgeable dataset and design an adversarial learning strategy to align the style of generated comments with real-world comments. We further integrate generated and original comments with a mutual controversy fusion module. Experimental results show that CAMERED outperforms state-of-the-art RED baseline models and generation methods, demonstrating its effectiveness.</li>
</ul>

<h3>Title: Scalable Robust Bayesian Co-Clustering with Compositional ELBOs</h3>
<ul>
<li><strong>Authors: </strong>Ashwin Vinod, Chandrajit Bajaj</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04079">https://arxiv.org/abs/2504.04079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04079">https://arxiv.org/pdf/2504.04079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04079]] Scalable Robust Bayesian Co-Clustering with Compositional ELBOs(https://arxiv.org/abs/2504.04079)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, noise learning</a></li>
<li><strong>Abstract: </strong>Co-clustering exploits the duality of instances and features to simultaneously uncover meaningful groups in both dimensions, often outperforming traditional clustering in high-dimensional or sparse data settings. Although recent deep learning approaches successfully integrate feature learning and cluster assignment, they remain susceptible to noise and can suffer from posterior collapse within standard autoencoders. In this paper, we present the first fully variational Co-clustering framework that directly learns row and column clusters in the latent space, leveraging a doubly reparameterized ELBO to improve gradient signal-to-noise separation. Our unsupervised model integrates a Variational Deep Embedding with a Gaussian Mixture Model (GMM) prior for both instances and features, providing a built-in clustering mechanism that naturally aligns latent modes with row and column clusters. Furthermore, our regularized end-to-end noise learning Compositional ELBO architecture jointly reconstructs the data while regularizing against noise through the KL divergence, thus gracefully handling corrupted or missing inputs in a single training pipeline. To counteract posterior collapse, we introduce a scale modification that increases the encoder's latent means only in the reconstruction pathway, preserving richer latent representations without inflating the KL term. Finally, a mutual information-based cross-loss ensures coherent co-clustering of rows and columns. Empirical results on diverse real-world datasets from multiple modalities, numerical, textual, and image-based, demonstrate that our method not only preserves the advantages of prior Co-clustering approaches but also exceeds them in accuracy and robustness, particularly in high-dimensional or noisy settings.</li>
</ul>

<h3>Title: Corrected with the Latest Version: Make Robust Asynchronous Federated Learning Possible</h3>
<ul>
<li><strong>Authors: </strong>Chaoyi Lu, Yiding Sun, Pengbo Li, Zhichuan Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04081">https://arxiv.org/abs/2504.04081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04081">https://arxiv.org/pdf/2504.04081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04081]] Corrected with the Latest Version: Make Robust Asynchronous Federated Learning Possible(https://arxiv.org/abs/2504.04081)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>As an emerging paradigm of federated learning, asynchronous federated learning offers significant speed advantages over traditional synchronous federated learning. Unlike synchronous federated learning, which requires waiting for all clients to complete updates before aggregation, asynchronous federated learning aggregates the models that have arrived in realtime, greatly improving training speed. However, this mechanism also introduces the issue of client model version inconsistency. When the differences between models of different versions during aggregation become too large, it may lead to conflicts, thereby reducing the models accuracy. To address this issue, this paper proposes an asynchronous federated learning version correction algorithm based on knowledge distillation, named FedADT. FedADT applies knowledge distillation before aggregating gradients, using the latest global model to correct outdated information, thus effectively reducing the negative impact of outdated gradients on the training process. Additionally, FedADT introduces an adaptive weighting function that adjusts the knowledge distillation weight according to different stages of training, helps mitigate the misleading effects caused by the poorer performance of the global model in the early stages of training. This method significantly improves the overall performance of asynchronous federated learning without adding excessive computational overhead. We conducted experimental comparisons with several classical algorithms, and the results demonstrate that FedADT achieves significant improvements over other asynchronous methods and outperforms all methods in terms of convergence speed.</li>
</ul>

<h3>Title: A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs: Experiments with OpenAI Models</h3>
<ul>
<li><strong>Authors: </strong>Aviv Brokman, Xuguang Ai, Yuhang Jiang, Shashank Gupta, Ramakanth Kavuluru</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04083">https://arxiv.org/abs/2504.04083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04083">https://arxiv.org/pdf/2504.04083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04083]] A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs: Experiments with OpenAI Models(https://arxiv.org/abs/2504.04083)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Objective: Zero-shot methodology promises to cut down on costs of dataset annotation and domain expertise needed to make use of NLP. Generative large language models trained to align with human goals have achieved high zero-shot performance across a wide variety of tasks. As of yet, it is unclear how well these models perform on biomedical relation extraction (RE). To address this knowledge gap, we explore patterns in the performance of OpenAI LLMs across a diverse sampling of RE tasks. Methods: We use OpenAI GPT-4-turbo and their reasoning model o1 to conduct end-to-end RE experiments on seven datasets. We use the JSON generation capabilities of GPT models to generate structured output in two ways: (1) by defining an explicit schema describing the structure of relations, and (2) using a setting that infers the structure from the prompt language. Results: Our work is the first to study and compare the performance of the GPT-4 and o1 for the end-to-end zero-shot biomedical RE task across a broad array of datasets. We found the zero-shot performances to be proximal to that of fine-tuned methods. The limitations of this approach are that it performs poorly on instances containing many relations and errs on the boundaries of textual mentions. Conclusion: Recent large language models exhibit promising zero-shot capabilities in complex biomedical RE tasks, offering competitive performance with reduced dataset curation and NLP modeling needs at the cost of increased computing, potentially increasing medical community accessibility. Addressing the limitations we identify could further boost reliability. The code, data, and prompts for all our experiments are publicly available: this https URL</li>
</ul>

<h3>Title: DocSAM: Unified Document Image Segmentation via Query Decomposition and Heterogeneous Mixed Learning</h3>
<ul>
<li><strong>Authors: </strong>Xiao-Hui Li, Fei Yin, Cheng-Lin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04085">https://arxiv.org/abs/2504.04085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04085">https://arxiv.org/pdf/2504.04085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04085]] DocSAM: Unified Document Image Segmentation via Query Decomposition and Heterogeneous Mixed Learning(https://arxiv.org/abs/2504.04085)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Document image segmentation is crucial for document analysis and recognition but remains challenging due to the diversity of document formats and segmentation tasks. Existing methods often address these tasks separately, resulting in limited generalization and resource wastage. This paper introduces DocSAM, a transformer-based unified framework designed for various document image segmentation tasks, such as document layout analysis, multi-granularity text segmentation, and table structure recognition, by modelling these tasks as a combination of instance and semantic segmentation. Specifically, DocSAM employs Sentence-BERT to map category names from each dataset into semantic queries that match the dimensionality of instance queries. These two sets of queries interact through an attention mechanism and are cross-attended with image features to predict instance and semantic segmentation masks. Instance categories are predicted by computing the dot product between instance and semantic queries, followed by softmax normalization of scores. Consequently, DocSAM can be jointly trained on heterogeneous datasets, enhancing robustness and generalization while reducing computational and storage resources. Comprehensive evaluations show that DocSAM surpasses existing methods in accuracy, efficiency, and adaptability, highlighting its potential for advancing document image understanding and segmentation across various applications. Codes are available at this https URL.</li>
</ul>

<h3>Title: PipeDec: Low-Latency Pipeline-based Inference with Dynamic Speculative Decoding towards Large-scale Models</h3>
<ul>
<li><strong>Authors: </strong>Haofei Yin, Mengbai Xiao, Rouzhou Lu, Xiao Zhang, Dongxiao Yu, Guanghui Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04104">https://arxiv.org/abs/2504.04104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04104">https://arxiv.org/pdf/2504.04104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04104]] PipeDec: Low-Latency Pipeline-based Inference with Dynamic Speculative Decoding towards Large-scale Models(https://arxiv.org/abs/2504.04104)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Autoregressive large language model inference primarily consists of two stages: pre-filling and decoding. Decoding involves sequential computation for each token, which leads to significant latency. Speculative decoding is a technique that leverages the draft model combined with large model verification to enhance parallelism without sacrificing accuracy. However, existing external prediction methods face challenges in adapting to multi-node serial deployments. While they can maintain speedup under such conditions, the high latency of multi-node deployments ultimately results in low overall efficiency. We propose a speculative decoding framework named PipeDec to address the low global resource utilization of single tasks in pipeline deployments thereby reducing decoding latency. We integrate a draft model into the pipeline of the large model and immediately forward each prediction from the draft model to subsequent pipeline stages. A dynamic prediction tree manages prediction sequences across nodes, enabling efficient updating and pruning. This approach leverages the draft model's predictions to utilize all pipeline nodes for parallel decoding of a single task. Experiments were conducted using LLama3.2 1B as the draft model in conjunction with a 14-stage parallel pipeline to accelerate LLama3.1 70B by six different types of datasets. During the decoding phase of a single task, PipeDec achieved a 4.46x-7.79x speedup compared to traditional pipeline parallelism and a 2.2x-2.69x speedup compared to baseline tree-based speculative decoding methods. The code will be released after the review process.</li>
</ul>

<h3>Title: Transformer representation learning is necessary for dynamic multi-modal physiological data on small-cohort patients</h3>
<ul>
<li><strong>Authors: </strong>Bingxu Wang, Kunzhi Cai, Yuqi Zhang, Yachong Guo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04120">https://arxiv.org/abs/2504.04120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04120">https://arxiv.org/pdf/2504.04120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04120]] Transformer representation learning is necessary for dynamic multi-modal physiological data on small-cohort patients(https://arxiv.org/abs/2504.04120)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Postoperative delirium (POD), a severe neuropsychiatric complication affecting nearly 50% of high-risk surgical patients, is defined as an acute disorder of attention and cognition, It remains significantly underdiagnosed in the intensive care units (ICUs) due to subjective monitoring methods. Early and accurate diagnosis of POD is critical and achievable. Here, we propose a POD prediction framework comprising a Transformer representation model followed by traditional machine learning algorithms. Our approaches utilizes multi-modal physiological data, including amplitude-integrated electroencephalography (aEEG), vital signs, electrocardiographic monitor data as well as hemodynamic parameters. We curated the first multi-modal POD dataset encompassing two patient types and evaluated the various Transformer architectures for representation learning. Empirical results indicate a consistent improvements of sensitivity and Youden index in patient TYPE I using Transformer representations, particularly our fusion adaptation of Pathformer. By enabling effective delirium diagnosis from postoperative day 1 to 3, our extensive experimental findings emphasize the potential of multi-modal physiological data and highlight the necessity of representation learning via multi-modal Transformer architecture in clinical diagnosis.</li>
</ul>

<h3>Title: EMF: Event Meta Formers for Event-based Real-time Traffic Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Ahmed Ullah Khan, Abdul Hannan Khan, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04124">https://arxiv.org/abs/2504.04124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04124">https://arxiv.org/pdf/2504.04124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04124]] EMF: Event Meta Formers for Event-based Real-time Traffic Object Detection(https://arxiv.org/abs/2504.04124)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Event cameras have higher temporal resolution, and require less storage and bandwidth compared to traditional RGB cameras. However, due to relatively lagging performance of event-based approaches, event cameras have not yet replace traditional cameras in performance-critical applications like autonomous driving. Recent approaches in event-based object detection try to bridge this gap by employing computationally expensive transformer-based solutions. However, due to their resource-intensive components, these solutions fail to exploit the sparsity and higher temporal resolution of event cameras efficiently. Moreover, these solutions are adopted from the vision domain, lacking specificity to the event cameras. In this work, we explore efficient and performant alternatives to recurrent vision transformer models and propose a novel event-based object detection backbone. The proposed backbone employs a novel Event Progression Extractor module, tailored specifically for event data, and uses Metaformer concept with convolution-based efficient components. We evaluate the resultant model on well-established traffic object detection benchmarks and conduct cross-dataset evaluation to test its ability to generalize. The proposed model outperforms the state-of-the-art on Prophesee Gen1 dataset by 1.6 mAP while reducing inference time by 14%. Our proposed EMF becomes the fastest DNN-based architecture in the domain by outperforming most efficient event-based object detectors. Moreover, the proposed model shows better ability to generalize to unseen data and scales better with the abundance of data.</li>
</ul>

<h3>Title: Multi-identity Human Image Animation with Structural Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Yuwei Guo, Dahua Lin, Tianfan Xue, Bo Dai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04126">https://arxiv.org/abs/2504.04126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04126">https://arxiv.org/pdf/2504.04126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04126]] Multi-identity Human Image Animation with Structural Video Diffusion(https://arxiv.org/abs/2504.04126)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Generating human videos from a single image while ensuring high visual quality and precise control is a challenging task, especially in complex scenarios involving multiple individuals and interactions with objects. Existing methods, while effective for single-human cases, often fail to handle the intricacies of multi-identity interactions because they struggle to associate the correct pairs of human appearance and pose condition and model the distribution of 3D-aware dynamics. To address these limitations, we present Structural Video Diffusion, a novel framework designed for generating realistic multi-human videos. Our approach introduces two core innovations: identity-specific embeddings to maintain consistent appearances across individuals and a structural learning mechanism that incorporates depth and surface-normal cues to model human-object interactions. Additionally, we expand existing human video dataset with 25K new videos featuring diverse multi-human and object interaction scenarios, providing a robust foundation for training. Experimental results demonstrate that Structural Video Diffusion achieves superior performance in generating lifelike, coherent videos for multiple subjects with dynamic and rich interactions, advancing the state of human-centric video generation.</li>
</ul>

<h3>Title: Scaling Federated Learning Solutions with Kubernetes for Synthesizing Histopathology Images</h3>
<ul>
<li><strong>Authors: </strong>Andrei-Alexandru Preda, Iulian-Marius Tăiatu, Dumitru-Clementin Cercel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04130">https://arxiv.org/abs/2504.04130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04130">https://arxiv.org/pdf/2504.04130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04130]] Scaling Federated Learning Solutions with Kubernetes for Synthesizing Histopathology Images(https://arxiv.org/abs/2504.04130)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, transformer, generative</a></li>
<li><strong>Abstract: </strong>In the field of deep learning, large architectures often obtain the best performance for many tasks, but also require massive datasets. In the histological domain, tissue images are expensive to obtain and constitute sensitive medical information, raising concerns about data scarcity and privacy. Vision Transformers are state-of-the-art computer vision models that have proven helpful in many tasks, including image classification. In this work, we combine vision Transformers with generative adversarial networks to generate histopathological images related to colorectal cancer and test their quality by augmenting a training dataset, leading to improved classification accuracy. Then, we replicate this performance using the federated learning technique and a realistic Kubernetes setup with multiple nodes, simulating a scenario where the training dataset is split among several hospitals unable to share their information directly due to privacy concerns.</li>
</ul>

<h3>Title: Predicting Soil Macronutrient Levels: A Machine Learning Approach Models Trained on pH, Conductivity, and Average Power of Acid-Base Solutions</h3>
<ul>
<li><strong>Authors: </strong>Mridul Kumar, Deepali Jain, Zeeshan Saifi, Soami Daya Krishnananda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.bio-ph, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04138">https://arxiv.org/abs/2504.04138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04138">https://arxiv.org/pdf/2504.04138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04138]] Predicting Soil Macronutrient Levels: A Machine Learning Approach Models Trained on pH, Conductivity, and Average Power of Acid-Base Solutions(https://arxiv.org/abs/2504.04138)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Soil macronutrients, particularly potassium ions (K$^+$), are indispensable for plant health, underpinning various physiological and biological processes, and facilitating the management of both biotic and abiotic stresses. Deficient macronutrient content results in stunted growth, delayed maturation, and increased vulnerability to environmental stressors, thereby accentuating the imperative for precise soil nutrient monitoring. Traditional techniques such as chemical assays, atomic absorption spectroscopy, inductively coupled plasma optical emission spectroscopy, and electrochemical methods, albeit advanced, are prohibitively expensive and time-intensive, thus unsuitable for real-time macronutrient assessment. In this study, we propose an innovative soil testing protocol utilizing a dataset derived from synthetic solutions to model soil behaviour. The dataset encompasses physical properties including conductivity and pH, with a concentration on three key macronutrients: nitrogen (N), phosphorus (P), and potassium (K). Four machine learning algorithms were applied to the dataset, with random forest regressors and neural networks being selected for the prediction of soil nutrient concentrations. Comparative analysis with laboratory soil testing results revealed prediction errors of 23.6% for phosphorus and 16% for potassium using the random forest model, and 26.3% for phosphorus and 21.8% for potassium using the neural network model. This methodology illustrates a cost-effective and efficacious strategy for real-time soil nutrient monitoring, offering substantial advancements over conventional techniques and enhancing the capability to sustain optimal nutrient levels conducive to robust crop growth.</li>
</ul>

<h3>Title: Cognitive Debiasing Large Language Models for Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Yougang Lyu, Shijie Ren, Yue Feng, Zihan Wang, Zhumin Chen, Zhaochun Ren, Maarten de Rijke</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04141">https://arxiv.org/abs/2504.04141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04141">https://arxiv.org/pdf/2504.04141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04141]] Cognitive Debiasing Large Language Models for Decision-Making(https://arxiv.org/abs/2504.04141)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown potential in supporting decision-making applications, particularly as personal conversational assistants in the financial, healthcare, and legal domains. While prompt engineering strategies have enhanced the capabilities of LLMs in decision-making, cognitive biases inherent to LLMs present significant challenges. Cognitive biases are systematic patterns of deviation from norms or rationality in decision-making that can lead to the production of inaccurate outputs. Existing cognitive bias mitigation strategies assume that input prompts contain (exactly) one type of cognitive bias and therefore fail to perform well in realistic settings where there maybe any number of biases. To fill this gap, we propose a cognitive debiasing approach, called self-debiasing, that enhances the reliability of LLMs by iteratively refining prompts. Our method follows three sequential steps -- bias determination, bias analysis, and cognitive debiasing -- to iteratively mitigate potential cognitive biases in prompts. Experimental results on finance, healthcare, and legal decision-making tasks, using both closed-source and open-source LLMs, demonstrate that the proposed self-debiasing method outperforms both advanced prompt engineering methods and existing cognitive debiasing techniques in average accuracy under no-bias, single-bias, and multi-bias settings.</li>
</ul>

<h3>Title: Reasoning on Multiple Needles In A Haystack</h3>
<ul>
<li><strong>Authors: </strong>Yidong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04150">https://arxiv.org/abs/2504.04150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04150">https://arxiv.org/pdf/2504.04150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04150]] Reasoning on Multiple Needles In A Haystack(https://arxiv.org/abs/2504.04150)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Needle In A Haystack (NIAH) task has been widely used to evaluate the long-context question-answering capabilities of Large Language Models (LLMs). However, its reliance on simple retrieval limits its effectiveness. To address this limitation, recent studies have introduced the Multiple Needles In A Haystack Reasoning (MNIAH-R) task, which incorporates supporting documents (Multiple needles) of multi-hop reasoning tasks into a distracting context (Haystack}). Despite this advancement, existing approaches still fail to address the issue of models providing direct answers from internal knowledge, and they do not explain or mitigate the decline in accuracy as context length increases. In this paper, we tackle the memory-based answering problem by filtering out direct-answer questions, and we reveal that performance degradation is primarily driven by the reduction in the length of the thinking process as the input length increases. Building on this insight, we decompose the thinking process into retrieval and reasoning stages and introduce a reflection mechanism for multi-round extension. We also train a model using the generated iterative thinking process, which helps mitigate the performance degradation. Furthermore, we demonstrate the application of this retrieval-reflection capability in mathematical reasoning scenarios, improving GPT-4o's performance on AIME2024.</li>
</ul>

<h3>Title: STEP: Staged Parameter-Efficient Pre-training for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kazuki Yano, Takumi Ito, Jun Suzuki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04151">https://arxiv.org/abs/2504.04151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04151">https://arxiv.org/pdf/2504.04151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04151]] STEP: Staged Parameter-Efficient Pre-training for Large Language Models(https://arxiv.org/abs/2504.04151)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pre-training large language models (LLMs) faces significant memory challenges due to the large size of model parameters. We introduce STaged parameter-Efficient Pre-training (STEP), which integrates parameter-efficient tuning techniques with model growth. We conduct experiments on pre-training LLMs of various sizes and demonstrate that STEP achieves up to a 53.9% reduction in maximum memory requirements compared to vanilla pre-training while maintaining equivalent performance. Furthermore, we show that the model by STEP performs comparably to vanilla pre-trained models on downstream tasks after instruction tuning.</li>
</ul>

<h3>Title: Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources</h3>
<ul>
<li><strong>Authors: </strong>Zihao Li, Shaoxiong Ji, Hengyu Luo, Jörg Tiedemann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04152">https://arxiv.org/abs/2504.04152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04152">https://arxiv.org/pdf/2504.04152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04152]] Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources(https://arxiv.org/abs/2504.04152)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit significant disparities in performance across languages, primarily benefiting high-resource languages while marginalizing underrepresented ones. Continual Pretraining (CPT) has emerged as a promising approach to address this imbalance, although the relative effectiveness of monolingual, bilingual, and code-augmented data strategies remains unclear. This study systematically evaluates 36 CPT configurations involving three multilingual base models, across 30+ languages categorized as altruistic, selfish, and stagnant, spanning various resource levels. Our findings reveal three major insights: (1) Bilingual CPT improves multilingual classification but often causes language mixing issues during generation. (2) Including programming code data during CPT consistently enhances multilingual classification accuracy, particularly benefiting low-resource languages, but introduces a trade-off by slightly degrading generation quality. (3) Contrary to prior work, we observe substantial deviations from language classifications according to their impact on cross-lingual transfer: Languages classified as altruistic often negatively affect related languages, selfish languages show conditional and configuration-dependent behavior, and stagnant languages demonstrate surprising adaptability under certain CPT conditions. These nuanced interactions emphasize the complexity of multilingual representation learning, underscoring the importance of systematic studies on generalizable language classification to inform future multilingual CPT strategies.</li>
</ul>

<h3>Title: GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hengyu Luo, Zihao Li, Joseph Attieh, Sawal Devkota, Ona de Gibert, Shaoxiong Ji, Peiqin Lin, Bhavani Sai Praneeth Varma Mantina, Ananda Sreenidhi, Raúl Vázquez, Mengjie Wang, Samea Yusofi, Jörg Tiedemann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04155">https://arxiv.org/abs/2504.04155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04155">https://arxiv.org/pdf/2504.04155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04155]] GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models(https://arxiv.org/abs/2504.04155)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are advancing at an unprecedented pace globally, with regions increasingly adopting these models for applications in their primary language. Evaluation of these models in diverse linguistic environments, especially in low-resource languages, has become a major challenge for academia and industry. Existing evaluation frameworks are disproportionately focused on English and a handful of high-resource languages, thereby overlooking the realistic performance of LLMs in multilingual and lower-resource scenarios. To address this gap, we introduce GlotEval, a lightweight framework designed for massively multilingual evaluation. Supporting seven key tasks (machine translation, text classification, summarization, open-ended generation, reading comprehension, sequence labeling, and intrinsic evaluation), spanning over dozens to hundreds of languages, GlotEval highlights consistent multilingual benchmarking, language-specific prompt templates, and non-English-centric machine translation. This enables a precise diagnosis of model strengths and weaknesses in diverse linguistic contexts. A multilingual translation case study demonstrates GlotEval's applicability for multilingual and language-specific evaluations.</li>
</ul>

<h3>Title: CoMBO: Conflict Mitigation via Branched Optimization for Class Incremental Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Kai Fang, Anqi Zhang, Guangyu Gao, Jianbo Jiao, Chi Harold Liu, Yunchao Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04156">https://arxiv.org/abs/2504.04156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04156">https://arxiv.org/pdf/2504.04156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04156]] CoMBO: Conflict Mitigation via Branched Optimization for Class Incremental Segmentation(https://arxiv.org/abs/2504.04156)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Effective Class Incremental Segmentation (CIS) requires simultaneously mitigating catastrophic forgetting and ensuring sufficient plasticity to integrate new classes. The inherent conflict above often leads to a back-and-forth, which turns the objective into finding the balance between the performance of previous~(old) and incremental~(new) classes. To address this conflict, we introduce a novel approach, Conflict Mitigation via Branched Optimization~(CoMBO). Within this approach, we present the Query Conflict Reduction module, designed to explicitly refine queries for new classes through lightweight, class-specific adapters. This module provides an additional branch for the acquisition of new classes while preserving the original queries for distillation. Moreover, we develop two strategies to further mitigate the conflict following the branched structure, \textit{i.e.}, the Half-Learning Half-Distillation~(HDHL) over classification probabilities, and the Importance-Based Knowledge Distillation~(IKD) over query features. HDHL selectively engages in learning for classification probabilities of queries that match the ground truth of new classes, while aligning unmatched ones to the corresponding old probabilities, thus ensuring retention of old knowledge while absorbing new classes via learning negative samples. Meanwhile, IKD assesses the importance of queries based on their matching degree to old classes, prioritizing the distillation of important features and allowing less critical features to evolve. Extensive experiments in Class Incremental Panoptic and Semantic Segmentation settings have demonstrated the superior performance of CoMBO. Project page: this https URL.</li>
</ul>

<h3>Title: JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Yunlong Lin, Zixu Lin, Haoyu Chen, Panwang Pan, Chenxin Li, Sixiang Chen, Yeying Jin, Wenbo Li, Xinghao Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04158">https://arxiv.org/abs/2504.04158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04158">https://arxiv.org/pdf/2504.04158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04158]] JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration(https://arxiv.org/abs/2504.04158)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-centric perception systems struggle with unpredictable and coupled weather degradations in the wild. Current solutions are often limited, as they either depend on specific degradation priors or suffer from significant domain gaps. To enable robust and autonomous operation in real-world conditions, we propose JarvisIR, a VLM-powered agent that leverages the VLM as a controller to manage multiple expert restoration models. To further enhance system robustness, reduce hallucinations, and improve generalizability in real-world adverse weather, JarvisIR employs a novel two-stage framework consisting of supervised fine-tuning and human feedback alignment. Specifically, to address the lack of paired data in real-world scenarios, the human feedback alignment enables the VLM to be fine-tuned effectively on large-scale real-world data in an unsupervised manner. To support the training and evaluation of JarvisIR, we introduce CleanBench, a comprehensive dataset consisting of high-quality and large-scale instruction-responses pairs, including 150K synthetic entries and 80K real entries. Extensive experiments demonstrate that JarvisIR exhibits superior decision-making and restoration capabilities. Compared with existing methods, it achieves a 50% improvement in the average of all perception metrics on CleanBench-Real. Project page: this https URL.</li>
</ul>

<h3>Title: Vehicle Acceleration Prediction Considering Environmental Influence and Individual Driving Behavior</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Wang, Lexing Zhang, Jiale Lei, Yin Feng, Hengxu Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04159">https://arxiv.org/abs/2504.04159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04159">https://arxiv.org/pdf/2504.04159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04159]] Vehicle Acceleration Prediction Considering Environmental Influence and Individual Driving Behavior(https://arxiv.org/abs/2504.04159)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate vehicle acceleration prediction is critical for intelligent driving control and energy efficiency management, particularly in environments with complex driving behavior dynamics. This paper proposes a general short-term vehicle acceleration prediction framework that jointly models environmental influence and individual driving behavior. The framework adopts a dual input design by incorporating environmental sequences, constructed from historical traffic variables such as percentile-based speed and acceleration statistics of multiple vehicles at specific spatial locations, capture group-level driving behavior influenced by the traffic environment. In parallel, individual driving behavior sequences represent motion characteristics of the target vehicle prior to the prediction point, reflecting personalized driving styles. These two inputs are processed using an LSTM Seq2Seq model enhanced with an attention mechanism, enabling accurate multi-step acceleration prediction. To demonstrate the effectiveness of the proposed method, an empirical study was conducted using high resolution radar video fused trajectory data collected from the exit section of the Guangzhou Baishi Tunnel. Drivers were clustered into three categories conservative, moderate, and aggressive based on key behavioral indicators, and a dedicated prediction model was trained for each group to account for driver this http URL results show that the proposed method consistently outperforms four baseline models, yielding a 10.9% improvement in accuracy with the inclusion of historical traffic variables and a 33% improvement with driver classification. Although prediction errors increase with forecast distance, incorporating environment- and behavior-aware features significantly enhances model robustness.</li>
</ul>

<h3>Title: OrbitZoo: Multi-Agent Reinforcement Learning Environment for Orbital Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Oliveira, Katarina Dyreby, Francisco Caldas, Cláudia Soares</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04160">https://arxiv.org/abs/2504.04160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04160">https://arxiv.org/pdf/2504.04160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04160]] OrbitZoo: Multi-Agent Reinforcement Learning Environment for Orbital Dynamics(https://arxiv.org/abs/2504.04160)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The increasing number of satellites and orbital debris has made space congestion a critical issue, threatening satellite safety and sustainability. Challenges such as collision avoidance, station-keeping, and orbital maneuvering require advanced techniques to handle dynamic uncertainties and multi-agent interactions. Reinforcement learning (RL) has shown promise in this domain, enabling adaptive, autonomous policies for space operations; however, many existing RL frameworks rely on custom-built environments developed from scratch, which often use simplified models and require significant time to implement and validate the orbital dynamics, limiting their ability to fully capture real-world complexities. To address this, we introduce OrbitZoo, a versatile multi-agent RL environment built on a high-fidelity industry standard library, that enables realistic data generation, supports scenarios like collision avoidance and cooperative maneuvers, and ensures robust and accurate orbital dynamics. The environment is validated against a real satellite constellation, Starlink, achieving a Mean Absolute Percentage Error (MAPE) of 0.16% compared to real-world data. This validation ensures reliability for generating high-fidelity simulations and enabling autonomous and independent satellite operations.</li>
</ul>

<h3>Title: MInCo: Mitigating Information Conflicts in Distracted Visual Model-based Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Shiguang Sun, Hanbo Zhang, Zeyang Liu, Xinrui Yang, Lipeng Wan, Bing Yan, Xingyu Chen, Xuguang Lan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04164">https://arxiv.org/abs/2504.04164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04164">https://arxiv.org/pdf/2504.04164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04164]] MInCo: Mitigating Information Conflicts in Distracted Visual Model-based Reinforcement Learning(https://arxiv.org/abs/2504.04164)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing visual model-based reinforcement learning (MBRL) algorithms with observation reconstruction often suffer from information conflicts, making it difficult to learn compact representations and hence result in less robust policies, especially in the presence of task-irrelevant visual distractions. In this paper, we first reveal that the information conflicts in current visual MBRL algorithms stem from visual representation learning and latent dynamics modeling with an information-theoretic perspective. Based on this finding, we present a new algorithm to resolve information conflicts for visual MBRL, named MInCo, which mitigates information conflicts by leveraging negative-free contrastive learning, aiding in learning invariant representation and robust policies despite noisy observations. To prevent the dominance of visual representation learning, we introduce time-varying reweighting to bias the learning towards dynamics modeling as training proceeds. We evaluate our method on several robotic control tasks with dynamic background distractions. Our experiments demonstrate that MInCo learns invariant representations against background noise and consistently outperforms current state-of-the-art visual MBRL methods. Code is available at this https URL.</li>
</ul>

<h3>Title: SDEIT: Semantic-Driven Electrical Impedance Tomography</h3>
<ul>
<li><strong>Authors: </strong>Dong Liu, Yuanchao Wu, Bowen Tong, Jiansong Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04185">https://arxiv.org/abs/2504.04185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04185">https://arxiv.org/pdf/2504.04185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04185]] SDEIT: Semantic-Driven Electrical Impedance Tomography(https://arxiv.org/abs/2504.04185)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Regularization methods using prior knowledge are essential in solving ill-posed inverse problems such as Electrical Impedance Tomography (EIT). However, designing effective regularization and integrating prior information into EIT remains challenging due to the complexity and variability of anatomical structures. In this work, we introduce SDEIT, a novel semantic-driven framework that integrates Stable Diffusion 3.5 into EIT, marking the first use of large-scale text-to-image generation models in EIT. SDEIT employs natural language prompts as semantic priors to guide the reconstruction process. By coupling an implicit neural representation (INR) network with a plug-and-play optimization scheme that leverages SD-generated images as generative priors, SDEIT improves structural consistency and recovers fine details. Importantly, this method does not rely on paired training datasets, increasing its adaptability to varied EIT scenarios. Extensive experiments on both simulated and experimental data demonstrate that SDEIT outperforms state-of-the-art techniques, offering superior accuracy and robustness. This work opens a new pathway for integrating multimodal priors into ill-posed inverse problems like EIT.</li>
</ul>

<h3>Title: AttackLLM: LLM-based Attack Pattern Generation for an Industrial Control System</h3>
<ul>
<li><strong>Authors: </strong>Chuadhry Mujeeb Ahmed (Newcastle University UK)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04187">https://arxiv.org/abs/2504.04187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04187">https://arxiv.org/pdf/2504.04187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04187]] AttackLLM: LLM-based Attack Pattern Generation for an Industrial Control System(https://arxiv.org/abs/2504.04187)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Malicious examples are crucial for evaluating the robustness of machine learning algorithms under attack, particularly in Industrial Control Systems (ICS). However, collecting normal and attack data in ICS environments is challenging due to the scarcity of testbeds and the high cost of human expertise. Existing datasets are often limited by the domain expertise of practitioners, making the process costly and inefficient. The lack of comprehensive attack pattern data poses a significant problem for developing robust anomaly detection methods. In this paper, we propose a novel approach that combines data-centric and design-centric methodologies to generate attack patterns using large language models (LLMs). Our results demonstrate that the attack patterns generated by LLMs not only surpass the quality and quantity of those created by human experts but also offer a scalable solution that does not rely on expensive testbeds or pre-existing attack examples. This multi-agent based approach presents a promising avenue for enhancing the security and resilience of ICS environments.</li>
</ul>

<h3>Title: Interpretable Single-View 3D Gaussian Splatting using Unsupervised Hierarchical Disentangled Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuyang Zhang, Baao Xie, Hu Zhu, Qi Wang, Huanting Guo, Xin Jin, Wenjun Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04190">https://arxiv.org/abs/2504.04190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04190">https://arxiv.org/pdf/2504.04190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04190]] Interpretable Single-View 3D Gaussian Splatting using Unsupervised Hierarchical Disentangled Representation Learning(https://arxiv.org/abs/2504.04190)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Gaussian Splatting (GS) has recently marked a significant advancement in 3D reconstruction, delivering both rapid rendering and high-quality results. However, existing 3DGS methods pose challenges in understanding underlying 3D semantics, which hinders model controllability and interpretability. To address it, we propose an interpretable single-view 3DGS framework, termed 3DisGS, to discover both coarse- and fine-grained 3D semantics via hierarchical disentangled representation learning (DRL). Specifically, the model employs a dual-branch architecture, consisting of a point cloud initialization branch and a triplane-Gaussian generation branch, to achieve coarse-grained disentanglement by separating 3D geometry and visual appearance features. Subsequently, fine-grained semantic representations within each modality are further discovered through DRL-based encoder-adapters. To our knowledge, this is the first work to achieve unsupervised interpretable 3DGS. Evaluations indicate that our model achieves 3D disentanglement while preserving high-quality and rapid reconstruction.</li>
</ul>

<h3>Title: GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill</h3>
<ul>
<li><strong>Authors: </strong>Jieming Cui, Tengyu Liu, Ziyu Meng, Jiale Yu, Ran Song, Wei Zhang, Yixin Zhu, Siyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04191">https://arxiv.org/abs/2504.04191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04191">https://arxiv.org/pdf/2504.04191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04191]] GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill(https://arxiv.org/abs/2504.04191)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Learning open-vocabulary physical skills for simulated agents presents a significant challenge in artificial intelligence. Current reinforcement learning approaches face critical limitations: manually designed rewards lack scalability across diverse tasks, while demonstration-based methods struggle to generalize beyond their training distribution. We introduce GROVE, a generalized reward framework that enables open-vocabulary physical skill learning without manual engineering or task-specific demonstrations. Our key insight is that Large Language Models(LLMs) and Vision Language Models(VLMs) provide complementary guidance -- LLMs generate precise physical constraints capturing task requirements, while VLMs evaluate motion semantics and naturalness. Through an iterative design process, VLM-based feedback continuously refines LLM-generated constraints, creating a self-improving reward system. To bridge the domain gap between simulation and natural images, we develop Pose2CLIP, a lightweight mapper that efficiently projects agent poses directly into semantic feature space without computationally expensive rendering. Extensive experiments across diverse embodiments and learning paradigms demonstrate GROVE's effectiveness, achieving 22.2% higher motion naturalness and 25.7% better task completion scores while training 8.4x faster than previous methods. These results establish a new foundation for scalable physical skill acquisition in simulated environments.</li>
</ul>

<h3>Title: The Effects of Grouped Structural Global Pruning of Vision Transformers on Domain Generalisation</h3>
<ul>
<li><strong>Authors: </strong>Hamza Riaz, Alan F. Smeaton</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04196">https://arxiv.org/abs/2504.04196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04196">https://arxiv.org/pdf/2504.04196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04196]] The Effects of Grouped Structural Global Pruning of Vision Transformers on Domain Generalisation(https://arxiv.org/abs/2504.04196)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>With the growing sizes of AI models like large language models (LLMs) and vision transformers, deploying them on devices with limited computational resources is a significant challenge particularly when addressing domain generalisation (DG) tasks. This paper introduces a novel grouped structural pruning method for pre-trained vision transformers (ViT, BeiT, and DeiT), evaluated on the PACS and Office-Home DG benchmarks. Our method uses dependency graph analysis to identify and remove redundant groups of neurons, weights, filters, or attention heads within transformers, using a range of selection metrics. Grouped structural pruning is applied at pruning ratios of 50\%, 75\% and 95\% and the models are then fine-tuned on selected distributions from DG benchmarks to evaluate their overall performance in DG tasks. Results show significant improvements in inference speed and fine-tuning time with minimal trade-offs in accuracy and DG task performance. For instance, on the PACS benchmark, pruning ViT, BeiT, and DeiT models by 50\% using the Hessian metric resulted in accuracy drops of only -2.94\%, -1.42\%, and -1.72\%, respectively, while achieving speed boosts of 2.5x, 1.81x, and 2.15x. These findings demonstrate the effectiveness of our approach in balancing model efficiency with domain generalisation performance.</li>
</ul>

<h3>Title: Adaptive Elicitation of Latent Information Using Natural Language</h3>
<ul>
<li><strong>Authors: </strong>Jimmy Wang, Thomas Zollo, Richard Zemel, Hongseok Namkoong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04204">https://arxiv.org/abs/2504.04204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04204">https://arxiv.org/pdf/2504.04204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04204]] Adaptive Elicitation of Latent Information Using Natural Language(https://arxiv.org/abs/2504.04204)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Eliciting information to reduce uncertainty about a latent entity is a critical task in many application domains, e.g., assessing individual student learning outcomes, diagnosing underlying diseases, or learning user preferences. Though natural language is a powerful medium for this purpose, large language models (LLMs) and existing fine-tuning algorithms lack mechanisms for strategically gathering information to refine their own understanding of the latent entity. To harness the generalization power and world knowledge of LLMs in developing effective information-gathering strategies, we propose an adaptive elicitation framework that actively reduces uncertainty on the latent entity. Since probabilistic modeling of an abstract latent entity is difficult, our framework adopts a predictive view of uncertainty, using a meta-learned language model to simulate future observations and enable scalable uncertainty quantification over complex natural language. Through autoregressive forward simulation, our model quantifies how new questions reduce epistemic uncertainty, enabling the development of sophisticated information-gathering strategies to choose the most informative next queries. In experiments on the 20 questions game, dynamic opinion polling, and adaptive student assessment, our method consistently outperforms baselines in identifying critical unknowns and improving downstream predictions, illustrating the promise of strategic information gathering in natural language settings.</li>
</ul>

<h3>Title: Towards Understanding and Improving Refusal in Compressed Models via Mechanistic Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Vishnu Kabir Chhabra, Mohammad Mahdi Khalili</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04215">https://arxiv.org/abs/2504.04215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04215">https://arxiv.org/pdf/2504.04215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04215]] Towards Understanding and Improving Refusal in Compressed Models via Mechanistic Interpretability(https://arxiv.org/abs/2504.04215)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The rapid growth of large language models has spurred significant interest in model compression as a means to enhance their accessibility and practicality. While extensive research has explored model compression through the lens of safety, findings suggest that safety-aligned models often lose elements of trustworthiness post-compression. Simultaneously, the field of mechanistic interpretability has gained traction, with notable discoveries, such as the identification of a single direction in the residual stream mediating refusal behaviors across diverse model architectures. In this work, we investigate the safety of compressed models by examining the mechanisms of refusal, adopting a novel interpretability-driven perspective to evaluate model safety. Furthermore, leveraging insights from our interpretability analysis, we propose a lightweight, computationally efficient method to enhance the safety of compressed models without compromising their performance or utility.</li>
</ul>

<h3>Title: A Perplexity and Menger Curvature-Based Approach for Similarity Evaluation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuantao Zhang, Zhankui Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04216">https://arxiv.org/abs/2504.04216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04216">https://arxiv.org/pdf/2504.04216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04216]] A Perplexity and Menger Curvature-Based Approach for Similarity Evaluation of Large Language Models(https://arxiv.org/abs/2504.04216)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rise of Large Language Models (LLMs) has brought about concerns regarding copyright infringement and unethical practices in data and model usage. For instance, slight modifications to existing LLMs may be used to falsely claim the development of new models, leading to issues of model copying and violations of ownership rights. This paper addresses these challenges by introducing a novel metric for quantifying LLM similarity, which leverages perplexity curves and differences in Menger curvature. Comprehensive experiments validate the performance of our methodology, demonstrating its superiority over baseline methods and its ability to generalize across diverse models and domains. Furthermore, we highlight the capability of our approach in detecting model replication through simulations, emphasizing its potential to preserve the originality and integrity of LLMs. Code is available at this https URL.</li>
</ul>

<h3>Title: Evaluating Graphical Perception with Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rami Huu Nguyen, Kenichi Maeda, Mahsa Geshvadi, Daniel Haehn</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04221">https://arxiv.org/abs/2504.04221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04221">https://arxiv.org/pdf/2504.04221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04221]] Evaluating Graphical Perception with Multimodal LLMs(https://arxiv.org/abs/2504.04221)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have remarkably progressed in analyzing and understanding images. Despite these advancements, accurately regressing values in charts remains an underexplored area for MLLMs. For visualization, how do MLLMs perform when applied to graphical perception tasks? Our paper investigates this question by reproducing Cleveland and McGill's seminal 1984 experiment and comparing it against human task performance. Our study primarily evaluates fine-tuned and pretrained models and zero-shot prompting to determine if they closely match human graphical perception. Our findings highlight that MLLMs outperform human task performance in some cases but not in others. We highlight the results of all experiments to foster an understanding of where MLLMs succeed and fail when applied to data visualization.</li>
</ul>

<h3>Title: TrafficLLM: Enhancing Large Language Models for Network Traffic Analysis with Generic Traffic Representation</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Cui, Xinjie Lin, Sijia Li, Miao Chen, Qilei Yin, Qi Li, Ke Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04222">https://arxiv.org/abs/2504.04222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04222">https://arxiv.org/pdf/2504.04222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04222]] TrafficLLM: Enhancing Large Language Models for Network Traffic Analysis with Generic Traffic Representation(https://arxiv.org/abs/2504.04222)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) powered network traffic analysis has been widely used for the purpose of threat detection. Unfortunately, their generalization across different tasks and unseen data is very limited. Large language models (LLMs), known for their strong generalization capabilities, have shown promising performance in various domains. However, their application to the traffic analysis domain is limited due to significantly different characteristics of network traffic. To address the issue, in this paper, we propose TrafficLLM, which introduces a dual-stage fine-tuning framework to learn generic traffic representation from heterogeneous raw traffic data. The framework uses traffic-domain tokenization, dual-stage tuning pipeline, and extensible adaptation to help LLM release generalization ability on dynamic traffic analysis tasks, such that it enables traffic detection and traffic generation across a wide range of downstream tasks. We evaluate TrafficLLM across 10 distinct scenarios and 229 types of traffic. TrafficLLM achieves F1-scores of 0.9875 and 0.9483, with up to 80.12% and 33.92% better performance than existing detection and generation methods. It also shows strong generalization on unseen traffic with an 18.6% performance improvement. We further evaluate TrafficLLM in real-world scenarios. The results confirm that TrafficLLM is easy to scale and achieves accurate detection performance on enterprise traffic.</li>
</ul>

<h3>Title: Resilience of Vision Transformers for Domain Generalisation in the Presence of Out-of-Distribution Noisy Images</h3>
<ul>
<li><strong>Authors: </strong>Hamza Riaz, Alan F. Smeaton</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04225">https://arxiv.org/abs/2504.04225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04225">https://arxiv.org/pdf/2504.04225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04225]] Resilience of Vision Transformers for Domain Generalisation in the Presence of Out-of-Distribution Noisy Images(https://arxiv.org/abs/2504.04225)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Modern AI models excel in controlled settings but often fail in real-world scenarios where data distributions shift unpredictably - a challenge known as domain generalisation (DG). This paper tackles this limitation by rigorously evaluating vision tramsformers, specifically the BEIT architecture which is a model pre-trained with masked image modelling (MIM), against synthetic out-of-distribution (OOD) benchmarks designed to mimic real-world noise and occlusions. We introduce a novel framework to generate OOD test cases by strategically masking object regions in images using grid patterns (25\%, 50\%, 75\% occlusion) and leveraging cutting-edge zero-shot segmentation via Segment Anything and Grounding DINO to ensure precise object localisation. Experiments across three benchmarks (PACS, Office-Home, DomainNet) demonstrate BEIT's known robustness while maintaining 94\% accuracy on PACS and 87\% on Office-Home, despite significant occlusions, outperforming CNNs and other vision transformers by margins of up to 37\%. Analysis of self-attention distances reveals that the BEIT dependence on global features correlates with its resilience. Furthermore, our synthetic benchmarks expose critical failure modes: performance degrades sharply when occlusions disrupt object shapes e.g. 68\% drop for external grid masking vs. 22\% for internal masking. This work provides two key advances (1) a scalable method to generate OOD benchmarks using controllable noise, and (2) empirical evidence that MIM and self-attention mechanism in vision transformers enhance DG by learning invariant features. These insights bridge the gap between lab-trained models and real-world deployment that offer a blueprint for building AI systems that generalise reliably under uncertainty.</li>
</ul>

<h3>Title: Sensitivity Meets Sparsity: The Impact of Extremely Sparse Parameter Patterns on Theory-of-Mind of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Wu, Wentao Guo, Zirui Liu, Heng Ji, Zhaozhuo Xu, Denghui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04238">https://arxiv.org/abs/2504.04238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04238">https://arxiv.org/pdf/2504.04238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04238]] Sensitivity Meets Sparsity: The Impact of Extremely Sparse Parameter Patterns on Theory-of-Mind of Large Language Models(https://arxiv.org/abs/2504.04238)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates the emergence of Theory-of-Mind (ToM) capabilities in large language models (LLMs) from a mechanistic perspective, focusing on the role of extremely sparse parameter patterns. We introduce a novel method to identify ToM-sensitive parameters and reveal that perturbing as little as 0.001% of these parameters significantly degrades ToM performance while also impairing contextual localization and language understanding. To understand this effect, we analyze their interaction with core architectural components of LLMs. Our findings demonstrate that these sensitive parameters are closely linked to the positional encoding module, particularly in models using Rotary Position Embedding (RoPE), where perturbations disrupt dominant-frequency activations critical for contextual processing. Furthermore, we show that perturbing ToM-sensitive parameters affects LLM's attention mechanism by modulating the angle between queries and keys under positional encoding. These insights provide a deeper understanding of how LLMs acquire social reasoning abilities, bridging AI interpretability with cognitive science. Our results have implications for enhancing model alignment, mitigating biases, and improving AI systems designed for human interaction.</li>
</ul>

<h3>Title: Loss Functions in Deep Learning: A Comprehensive Review</h3>
<ul>
<li><strong>Authors: </strong>Omar Elharrouss, Yasir Mahmood, Yassine Bechqito, Mohamed Adel Serhani, Elarbi Badidi, Jamal Riffi, Hamid Tairi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04242">https://arxiv.org/abs/2504.04242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04242">https://arxiv.org/pdf/2504.04242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04242]] Loss Functions in Deep Learning: A Comprehensive Review(https://arxiv.org/abs/2504.04242)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Loss functions are at the heart of deep learning, shaping how models learn and perform across diverse tasks. They are used to quantify the difference between predicted outputs and ground truth labels, guiding the optimization process to minimize errors. Selecting the right loss function is critical, as it directly impacts model convergence, generalization, and overall performance across various applications, from computer vision to time series forecasting. This paper presents a comprehensive review of loss functions, covering fundamental metrics like Mean Squared Error and Cross-Entropy to advanced functions such as Adversarial and Diffusion losses. We explore their mathematical foundations, impact on model training, and strategic selection for various applications, including computer vision (Discriminative and generative), tabular data prediction, and time series forecasting. For each of these categories, we discuss the most used loss functions in the recent advancements of deep learning techniques. Also, this review explore the historical evolution, computational efficiency, and ongoing challenges in loss function design, underlining the need for more adaptive and robust solutions. Emphasis is placed on complex scenarios involving multi-modal data, class imbalances, and real-world constraints. Finally, we identify key future directions, advocating for loss functions that enhance interpretability, scalability, and generalization, leading to more effective and resilient deep learning models.</li>
</ul>

<h3>Title: Progressive Multi-Source Domain Adaptation for Personalized Facial Expression Recognition</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Osama Zeeshan, Marco Pedersoli, Alessandro Lameiras Koerich, Eric Grange</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04252">https://arxiv.org/abs/2504.04252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04252">https://arxiv.org/pdf/2504.04252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04252]] Progressive Multi-Source Domain Adaptation for Personalized Facial Expression Recognition(https://arxiv.org/abs/2504.04252)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Personalized facial expression recognition (FER) involves adapting a machine learning model using samples from labeled sources and unlabeled target domains. Given the challenges of recognizing subtle expressions with considerable interpersonal variability, state-of-the-art unsupervised domain adaptation (UDA) methods focus on the multi-source UDA (MSDA) setting, where each domain corresponds to a specific subject, and improve model accuracy and robustness. However, when adapting to a specific target, the diverse nature of multiple source domains translates to a large shift between source and target data. State-of-the-art MSDA methods for FER address this domain shift by considering all the sources to adapt to the target representations. Nevertheless, adapting to a target subject presents significant challenges due to large distributional differences between source and target domains, often resulting in negative transfer. In addition, integrating all sources simultaneously increases computational costs and causes misalignment with the target. To address these issues, we propose a progressive MSDA approach that gradually introduces information from subjects based on their similarity to the target subject. This will ensure that only the most relevant sources from the target are selected, which helps avoid the negative transfer caused by dissimilar sources. We first exploit the closest sources to reduce the distribution shift with the target and then move towards the furthest while only considering the most relevant sources based on the predetermined threshold. Furthermore, to mitigate catastrophic forgetting caused by the incremental introduction of source subjects, we implemented a density-based memory mechanism that preserves the most relevant historical source samples for adaptation. Our experiments show the effectiveness of our proposed method on pain datasets: Biovid and UNBC-McMaster.</li>
</ul>

<h3>Title: Lost in Multilinguality: Dissecting Cross-lingual Factual Inconsistency in Transformer Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mingyang Wang, Heike Adel, Lukas Lange, Yihong Liu, Ercong Nie, Jannik Strötgen, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04264">https://arxiv.org/abs/2504.04264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04264">https://arxiv.org/pdf/2504.04264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04264]] Lost in Multilinguality: Dissecting Cross-lingual Factual Inconsistency in Transformer Language Models(https://arxiv.org/abs/2504.04264)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Multilingual language models (MLMs) store factual knowledge across languages but often struggle to provide consistent responses to semantically equivalent prompts in different languages. While previous studies point out this cross-lingual inconsistency issue, the underlying causes remain unexplored. In this work, we use mechanistic interpretability methods to investigate cross-lingual inconsistencies in MLMs. We find that MLMs encode knowledge in a language-independent concept space through most layers, and only transition to language-specific spaces in the final layers. Failures during the language transition often result in incorrect predictions in the target language, even when the answers are correct in other languages. To mitigate this inconsistency issue, we propose a linear shortcut method that bypasses computations in the final layers, enhancing both prediction accuracy and cross-lingual consistency. Our findings shed light on the internal mechanisms of MLMs and provide a lightweight, effective strategy for producing more consistent factual outputs.</li>
</ul>

<h3>Title: ADA-Net: Attention-Guided Domain Adaptation Network with Contrastive Learning for Standing Dead Tree Segmentation Using Aerial Imagery</h3>
<ul>
<li><strong>Authors: </strong>Mete Ahishali, Anis Ur Rahman, Einari Heinaro, Samuli Junttila</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04271">https://arxiv.org/abs/2504.04271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04271">https://arxiv.org/pdf/2504.04271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04271]] ADA-Net: Attention-Guided Domain Adaptation Network with Contrastive Learning for Standing Dead Tree Segmentation Using Aerial Imagery(https://arxiv.org/abs/2504.04271)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Information on standing dead trees is important for understanding forest ecosystem functioning and resilience but has been lacking over large geographic regions. Climate change has caused large-scale tree mortality events that can remain undetected due to limited data. In this study, we propose a novel method for segmenting standing dead trees using aerial multispectral orthoimages. Because access to annotated datasets has been a significant problem in forest remote sensing due to the need for forest expertise, we introduce a method for domain transfer by leveraging domain adaptation to learn a transformation from a source domain X to target domain Y. In this Image-to-Image translation task, we aim to utilize available annotations in the target domain by pre-training a segmentation network. When images from a new study site without annotations are introduced (source domain X), these images are transformed into the target domain. Then, transfer learning is applied by inferring the pre-trained network on domain-adapted images. In addition to investigating the feasibility of current domain adaptation approaches for this objective, we propose a novel approach called the Attention-guided Domain Adaptation Network (ADA-Net) with enhanced contrastive learning. Accordingly, the ADA-Net approach provides new state-of-the-art domain adaptation performance levels outperforming existing approaches. We have evaluated the proposed approach using two datasets from Finland and the US. The USA images are converted to the Finland domain, and we show that the synthetic USA2Finland dataset exhibits similar characteristics to the Finland domain images. The software implementation is shared at this https URL. The data is publicly available at this https URL.</li>
</ul>

<h3>Title: Could AI Trace and Explain the Origins of AI-Generated Images and Text?</h3>
<ul>
<li><strong>Authors: </strong>Hongchao Fang, Can Qin, Ran Xu, Feng Liu, Yixin Liu, Lichao Sun, Dongwon Lee, Lifu Huang, Wenpeng Yin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04279">https://arxiv.org/abs/2504.04279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04279">https://arxiv.org/pdf/2504.04279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04279]] Could AI Trace and Explain the Origins of AI-Generated Images and Text?(https://arxiv.org/abs/2504.04279)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>AI-generated content is becoming increasingly prevalent in the real world, leading to serious ethical and societal concerns. For instance, adversaries might exploit large multimodal models (LMMs) to create images that violate ethical or legal standards, while paper reviewers may misuse large language models (LLMs) to generate reviews without genuine intellectual effort. While prior work has explored detecting AI-generated images and texts, and occasionally tracing their source models, there is a lack of a systematic and fine-grained comparative study. Important dimensions--such as AI-generated images vs. text, fully vs. partially AI-generated images, and general vs. malicious use cases--remain underexplored. Furthermore, whether AI systems like GPT-4o can explain why certain forged content is attributed to specific generative models is still an open question, with no existing benchmark addressing this. To fill this gap, we introduce AI-FAKER, a comprehensive multimodal dataset with over 280,000 samples spanning multiple LLMs and LMMs, covering both general and malicious use cases for AI-generated images and texts. Our experiments reveal two key findings: (i) AI authorship detection depends not only on the generated output but also on the model's original training intent; and (ii) GPT-4o provides highly consistent but less specific explanations when analyzing content produced by OpenAI's own models, such as DALL-E and GPT-4o itself.</li>
</ul>

<h3>Title: CATS: Mitigating Correlation Shift for Multivariate Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Xiao Lin, Zhichen Zeng, Tianxin Wei, Zhining Liu, Yuzhong chen, Hanghang Tong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04283">https://arxiv.org/abs/2504.04283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04283">https://arxiv.org/pdf/2504.04283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04283]] CATS: Mitigating Correlation Shift for Multivariate Time Series Classification(https://arxiv.org/abs/2504.04283)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Unsupervised Domain Adaptation (UDA) leverages labeled source data to train models for unlabeled target data. Given the prevalence of multivariate time series (MTS) data across various domains, the UDA task for MTS classification has emerged as a critical challenge. However, for MTS data, correlations between variables often vary across domains, whereas most existing UDA works for MTS classification have overlooked this essential characteristic. To bridge this gap, we introduce a novel domain shift, {\em correlation shift}, measuring domain differences in multivariate correlation. To mitigate correlation shift, we propose a scalable and parameter-efficient \underline{C}orrelation \underline{A}dapter for M\underline{TS} (CATS). Designed as a plug-and-play technique compatible with various Transformer variants, CATS employs temporal convolution to capture local temporal patterns and a graph attention module to model the changing multivariate correlation. The adapter reweights the target correlations to align the source correlations with a theoretically guaranteed precision. A correlation alignment loss is further proposed to mitigate correlation shift, bypassing the alignment challenge from the non-i.i.d. nature of MTS data. Extensive experiments on four real-world datasets demonstrate that (1) compared with vanilla Transformer-based models, CATS increases over $10\%$ average accuracy while only adding around $1\%$ parameters, and (2) all Transformer variants equipped with CATS either reach or surpass state-of-the-art baselines.</li>
</ul>

<h3>Title: Cross-Asset Risk Management: Integrating LLMs for Real-Time Monitoring of Equity, Fixed Income, and Currency Markets</h3>
<ul>
<li><strong>Authors: </strong>Jie Yang, Yiqiu Tang, Yongjie Li, Lihua Zhang, Haoran Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04292">https://arxiv.org/abs/2504.04292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04292">https://arxiv.org/pdf/2504.04292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04292]] Cross-Asset Risk Management: Integrating LLMs for Real-Time Monitoring of Equity, Fixed Income, and Currency Markets(https://arxiv.org/abs/2504.04292)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have emerged as powerful tools in the field of finance, particularly for risk management across different asset classes. In this work, we introduce a Cross-Asset Risk Management framework that utilizes LLMs to facilitate real-time monitoring of equity, fixed income, and currency markets. This innovative approach enables dynamic risk assessment by aggregating diverse data sources, ultimately enhancing decision-making processes. Our model effectively synthesizes and analyzes market signals to identify potential risks and opportunities while providing a holistic view of asset classes. By employing advanced analytics, we leverage LLMs to interpret financial texts, news articles, and market reports, ensuring that risks are contextualized within broader market narratives. Extensive backtesting and real-time simulations validate the framework, showing increased accuracy in predicting market shifts compared to conventional methods. The focus on real-time data integration enhances responsiveness, allowing financial institutions to manage risks adeptly under varying market conditions and promoting financial stability through the advanced application of LLMs in risk analysis.</li>
</ul>

<h3>Title: 3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS</h3>
<ul>
<li><strong>Authors: </strong>Zhisheng Huang, Peng Wang, Jingdong Zhang, Yuan Liu, Xin Li, Wenping Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04294">https://arxiv.org/abs/2504.04294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04294">https://arxiv.org/pdf/2504.04294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04294]] 3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS(https://arxiv.org/abs/2504.04294)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3DGS) has revolutionized neural rendering with its efficiency and quality, but like many novel view synthesis methods, it heavily depends on accurate camera poses from Structure-from-Motion (SfM) systems. Although recent SfM pipelines have made impressive progress, questions remain about how to further improve both their robust performance in challenging conditions (e.g., textureless scenes) and the precision of camera parameter estimation simultaneously. We present 3R-GS, a 3D Gaussian Splatting framework that bridges this gap by jointly optimizing 3D Gaussians and camera parameters from large reconstruction priors MASt3R-SfM. We note that naively performing joint 3D Gaussian and camera optimization faces two challenges: the sensitivity to the quality of SfM initialization, and its limited capacity for global optimization, leading to suboptimal reconstruction results. Our 3R-GS, overcomes these issues by incorporating optimized practices, enabling robust scene reconstruction even with imperfect camera registration. Extensive experiments demonstrate that 3R-GS delivers high-quality novel view synthesis and precise camera pose estimation while remaining computationally efficient. Project page: this https URL</li>
</ul>

<h3>Title: Dynamic Hedging Strategies in Derivatives Markets with LLM-Driven Sentiment and News Analytics</h3>
<ul>
<li><strong>Authors: </strong>Jie Yang, Yiqiu Tang, Yongjie Li, Lihua Zhang, Haoran Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04295">https://arxiv.org/abs/2504.04295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04295">https://arxiv.org/pdf/2504.04295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04295]] Dynamic Hedging Strategies in Derivatives Markets with LLM-Driven Sentiment and News Analytics(https://arxiv.org/abs/2504.04295)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Dynamic hedging strategies are essential for effective risk management in derivatives markets, where volatility and market sentiment can greatly impact performance. This paper introduces a novel framework that leverages large language models (LLMs) for sentiment analysis and news analytics to inform hedging decisions. By analyzing textual data from diverse sources like news articles, social media, and financial reports, our approach captures critical sentiment indicators that reflect current market conditions. The framework allows for real-time adjustments to hedging strategies, adapting positions based on continuous sentiment signals. Backtesting results on historical derivatives data reveal that our dynamic hedging strategies achieve superior risk-adjusted returns compared to conventional static approaches. The incorporation of LLM-driven sentiment analysis into hedging practices presents a significant advancement in decision-making processes within derivatives trading. This research showcases how sentiment-informed dynamic hedging can enhance portfolio management and effectively mitigate associated risks.</li>
</ul>

<h3>Title: A Survey of Social Cybersecurity: Techniques for Attack Detection, Evaluations, Challenges, and Future Prospects</h3>
<ul>
<li><strong>Authors: </strong>Aos Mulahuwaish, Basheer Qolomany, Kevin Gyorick, Jacques Bou Abdo, Mohammed Aledhari, Junaid Qadir, Kathleen Carley, Ala Al-Fuqaha</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY, cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04311">https://arxiv.org/abs/2504.04311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04311">https://arxiv.org/pdf/2504.04311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04311]] A Survey of Social Cybersecurity: Techniques for Attack Detection, Evaluations, Challenges, and Future Prospects(https://arxiv.org/abs/2504.04311)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>In today's digital era, the Internet, especially social media platforms, plays a significant role in shaping public opinions, attitudes, and beliefs. Unfortunately, the credibility of scientific information sources is often undermined by the spread of misinformation through various means, including technology-driven tools like bots, cyborgs, trolls, sock-puppets, and deep fakes. This manipulation of public discourse serves antagonistic business agendas and compromises civil society. In response to this challenge, a new scientific discipline has emerged: social cybersecurity.</li>
</ul>

<h3>Title: Balancing Complexity and Informativeness in LLM-Based Clustering: Finding the Goldilocks Zone</h3>
<ul>
<li><strong>Authors: </strong>Justin Miller, Tristram Alexander</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04314">https://arxiv.org/abs/2504.04314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04314">https://arxiv.org/pdf/2504.04314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04314]] Balancing Complexity and Informativeness in LLM-Based Clustering: Finding the Goldilocks Zone(https://arxiv.org/abs/2504.04314)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>The challenge of clustering short text data lies in balancing informativeness with interpretability. Traditional evaluation metrics often overlook this trade-off. Inspired by linguistic principles of communicative efficiency, this paper investigates the optimal number of clusters by quantifying the trade-off between informativeness and cognitive simplicity. We use large language models (LLMs) to generate cluster names and evaluate their effectiveness through semantic density, information theory, and clustering accuracy. Our results show that Gaussian Mixture Model (GMM) clustering on embeddings generated by a LLM, increases semantic density compared to random assignment, effectively grouping similar bios. However, as clusters increase, interpretability declines, as measured by a generative LLM's ability to correctly assign bios based on cluster names. A logistic regression analysis confirms that classification accuracy depends on the semantic similarity between bios and their assigned cluster names, as well as their distinction from alternatives. These findings reveal a "Goldilocks zone" where clusters remain distinct yet interpretable. We identify an optimal range of 16-22 clusters, paralleling linguistic efficiency in lexical categorization. These insights inform both theoretical models and practical applications, guiding future research toward optimising cluster interpretability and usefulness.</li>
</ul>

<h3>Title: Variational Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Mehmet Can Yavuz, Berrin Yanikoglu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04318">https://arxiv.org/abs/2504.04318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04318">https://arxiv.org/pdf/2504.04318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04318]] Variational Self-Supervised Learning(https://arxiv.org/abs/2504.04318)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present Variational Self-Supervised Learning (VSSL), a novel framework that combines variational inference with self-supervised learning to enable efficient, decoder-free representation learning. Unlike traditional VAEs that rely on input reconstruction via a decoder, VSSL symmetrically couples two encoders with Gaussian outputs. A momentum-updated teacher network defines a dynamic, data-dependent prior, while the student encoder produces an approximate posterior from augmented views. The reconstruction term in the ELBO is replaced with a cross-view denoising objective, preserving the analytical tractability of Gaussian KL divergence. We further introduce cosine-based formulations of KL and log-likelihood terms to enhance semantic alignment in high-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and ImageNet-100 show that VSSL achieves competitive or superior performance to leading self-supervised methods, including BYOL and MoCo V3. VSSL offers a scalable, probabilistically grounded approach to learning transferable representations without generative reconstruction, bridging the gap between variational modeling and modern self-supervised techniques.</li>
</ul>

<h3>Title: MedM-VL: What Makes a Good Medical LVLM?</h3>
<ul>
<li><strong>Authors: </strong>Yiming Shi, Shaoshuai Yang, Xun Zhu, Haoyu Wang, Miao Li, Ji Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04323">https://arxiv.org/abs/2504.04323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04323">https://arxiv.org/pdf/2504.04323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04323]] MedM-VL: What Makes a Good Medical LVLM?(https://arxiv.org/abs/2504.04323)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image analysis is a fundamental component. As deep learning progresses, the focus has shifted from single-task applications, such as classification and segmentation, to more complex multimodal tasks, including medical visual question answering and report generation. Traditional shallow and task-specific models are increasingly limited in addressing the complexity and scalability required in clinical practice. The emergence of large language models (LLMs) has driven the development of medical Large Vision-Language Models (LVLMs), offering a unified solution for diverse vision-language tasks. In this study, we investigate various architectural designs for medical LVLMs based on the widely adopted LLaVA framework, which follows an encoder-connector-LLM paradigm. We construct two distinct models targeting 2D and 3D modalities, respectively. These models are designed to support both general-purpose medical tasks and domain-specific fine-tuning, thereby serving as effective foundation models. To facilitate reproducibility and further research, we develop a modular and extensible codebase, MedM-VL, and release two LVLM variants: MedM-VL-2D for 2D medical image analysis and MedM-VL-CT-Chest for 3D CT-based applications. The code and models are available at: this https URL</li>
</ul>

<h3>Title: IMPersona: Evaluating Individual Level LM Impersonation</h3>
<ul>
<li><strong>Authors: </strong>Quan Shi, Carlos Jimenez, Stephen Dong, Brian Seo, Caden Yao, Adam Kelch, Karthik Narasimhan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04332">https://arxiv.org/abs/2504.04332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04332">https://arxiv.org/pdf/2504.04332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04332]] IMPersona: Evaluating Individual Level LM Impersonation(https://arxiv.org/abs/2504.04332)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense</a></li>
<li><strong>Abstract: </strong>As language models achieve increasingly human-like capabilities in conversational text generation, a critical question emerges: to what extent can these systems simulate the characteristics of specific individuals? To evaluate this, we introduce IMPersona, a framework for evaluating LMs at impersonating specific individuals' writing style and personal knowledge. Using supervised fine-tuning and a hierarchical memory-inspired retrieval system, we demonstrate that even modestly sized open-source models, such as Llama-3.1-8B-Instruct, can achieve impersonation abilities at concerning levels. In blind conversation experiments, participants (mis)identified our fine-tuned models with memory integration as human in 44.44% of interactions, compared to just 25.00% for the best prompting-based approach. We analyze these results to propose detection methods and defense strategies against such impersonation attempts. Our findings raise important questions about both the potential applications and risks of personalized language models, particularly regarding privacy, security, and the ethical deployment of such technologies in real-world contexts.</li>
</ul>

<h3>Title: Hallucination Detection using Multi-View Attention Features</h3>
<ul>
<li><strong>Authors: </strong>Yuya Ogasa, Yuki Arase</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04335">https://arxiv.org/abs/2504.04335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04335">https://arxiv.org/pdf/2504.04335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04335]] Hallucination Detection using Multi-View Attention Features(https://arxiv.org/abs/2504.04335)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>This study tackles token-level hallucination detection in outputs of large language models. Previous studies revealed that attention exhibits irregular patterns when hallucination occurs. Inspired by this, we extract features from the attention matrix that provide complementary views of (a) the average attention each token receives, which helps identify whether certain tokens are overly influential or ignored, (b) the diversity of attention each token receives, which reveals whether attention is biased toward specific subsets, and (c) the diversity of tokens a token attends to during generation, which indicates whether the model references a narrow or broad range of information. These features are input to a Transformer-based classifier to conduct token-level classification to identify hallucinated spans. Experimental results indicate that the proposed method outperforms strong baselines on hallucination detection with longer input contexts, i.e., data-to-text and summarization tasks.</li>
</ul>

<h3>Title: Generative Large Language Models Trained for Detecting Errors in Radiology Reports</h3>
<ul>
<li><strong>Authors: </strong>Cong Sun, Kurt Teichman, Yiliang Zhou, Brian Critelli, David Nauheim, Graham Keir, Xindi Wang, Judy Zhong, Adam E Flanders, George Shih, Yifan Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04336">https://arxiv.org/abs/2504.04336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04336">https://arxiv.org/pdf/2504.04336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04336]] Generative Large Language Models Trained for Detecting Errors in Radiology Reports(https://arxiv.org/abs/2504.04336)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>In this retrospective study, a dataset was constructed with two parts. The first part included 1,656 synthetic chest radiology reports generated by GPT-4 using specified prompts, with 828 being error-free synthetic reports and 828 containing errors. The second part included 614 reports: 307 error-free reports between 2011 and 2016 from the MIMIC-CXR database and 307 corresponding synthetic reports with errors generated by GPT-4 on the basis of these MIMIC-CXR reports and specified prompts. All errors were categorized into four types: negation, left/right, interval change, and transcription errors. Then, several models, including Llama-3, GPT-4, and BiomedBERT, were refined using zero-shot prompting, few-shot prompting, or fine-tuning strategies. Finally, the performance of these models was evaluated using the F1 score, 95\% confidence interval (CI) and paired-sample t-tests on our constructed dataset, with the prediction results further assessed by radiologists. Using zero-shot prompting, the fine-tuned Llama-3-70B-Instruct model achieved the best performance with the following F1 scores: 0.769 for negation errors, 0.772 for left/right errors, 0.750 for interval change errors, 0.828 for transcription errors, and 0.780 overall. In the real-world evaluation phase, two radiologists reviewed 200 randomly selected reports output by the model. Of these, 99 were confirmed to contain errors detected by the models by both radiologists, and 163 were confirmed to contain model-detected errors by at least one radiologist. Generative LLMs, fine-tuned on synthetic and MIMIC-CXR radiology reports, greatly enhanced error detection in radiology reports.</li>
</ul>

<h3>Title: AnomalyHybrid: A Domain-agnostic Generative Framework for General Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Ying Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04340">https://arxiv.org/abs/2504.04340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04340">https://arxiv.org/pdf/2504.04340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04340]] AnomalyHybrid: A Domain-agnostic Generative Framework for General Anomaly Detection(https://arxiv.org/abs/2504.04340)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Anomaly generation is an effective way to mitigate data scarcity for anomaly detection task. Most existing works shine at industrial anomaly generation with multiple specialists or large generative models, rarely generalizing to anomalies in other applications. In this paper, we present AnomalyHybrid, a domain-agnostic framework designed to generate authentic and diverse anomalies simply by combining the reference and target images. AnomalyHybrid is a Generative Adversarial Network(GAN)-based framework having two decoders that integrate the appearance of reference image into the depth and edge structures of target image respectively. With the help of depth decoders, AnomalyHybrid achieves authentic generation especially for the anomalies with depth values changing, such a s protrusion and dent. More, it relaxes the fine granularity structural control of the edge decoder and brings more diversity. Without using annotations, AnomalyHybrid is easily trained with sets of color, depth and edge of same images having different augmentations. Extensive experiments carried on HeliconiusButterfly, MVTecAD and MVTec3D datasets demonstrate that AnomalyHybrid surpasses the GAN-based state-of-the-art on anomaly generation and its downstream anomaly classification, detection and segmentation tasks. On MVTecAD dataset, AnomalyHybrid achieves 2.06/0.32 IS/LPIPS for anomaly generation, 52.6 Acc for anomaly classification with ResNet34, 97.3/72.9 AP for image/pixel-level anomaly detection with a simple UNet.</li>
</ul>

<h3>Title: Compression Laws for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ayan Sengupta, Siddhant Chaudhary, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04342">https://arxiv.org/abs/2504.04342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04342">https://arxiv.org/pdf/2504.04342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04342]] Compression Laws for Large Language Models(https://arxiv.org/abs/2504.04342)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce compression laws for language language models (LLMs). While recent scaling laws have sought to understand how LLMs scale with respect to model size, pre-training data, and computational resources, we focus on understanding how model compression affects the performance of a pre-trained LLM on downstream tasks. We empirically examine the effects of structured model compression on LLMs through over $1000$ experiments across eight models with sizes ranging from $0.5B$ to $14B$ parameters. Our findings indicate that the test cross-entropy loss increases quadratically with the compression ratio, whereas performance on downstream tasks declines only linearly. Our study emphasizes the importance of recovery fine-tuning in enhancing generation loss, showing that the test loss of compressed LLMs can improve by up to 55% with recovery fine-tuning. At higher compression ratios (up to 90%), compressed LLMs demonstrate a speed increase of 60% during inference compared to their uncompressed counterparts, compensating for the performance degradation at this level. However, for smaller models ($\le 7B$), the computational gains are limited, peaking at just 35%. We conclude that model compression can be highly beneficial for larger models, especially when a smaller model within the same computational budget is not available. These insights provide the practical guidelines for utilizing model compression techniques for adopting LLMs in real-life applications in resource-constrained settings.</li>
</ul>

<h3>Title: Extending Cox Proportional Hazards Model with Symbolic Non-Linear Log-Risk Functions for Survival Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jiaxiang Cheng, Guoqiang Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04353">https://arxiv.org/abs/2504.04353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04353">https://arxiv.org/pdf/2504.04353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04353]] Extending Cox Proportional Hazards Model with Symbolic Non-Linear Log-Risk Functions for Survival Analysis(https://arxiv.org/abs/2504.04353)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The Cox proportional hazards (CPH) model has been widely applied in survival analysis to estimate relative risks across different subjects given multiple covariates. Traditional CPH models rely on a linear combination of covariates weighted with coefficients as the log-risk function, which imposes a strong and restrictive assumption, limiting generalization. Recent deep learning methods enable non-linear log-risk functions. However, they often lack interpretability due to the end-to-end training mechanisms. The implementation of Kolmogorov-Arnold Networks (KAN) offers new possibilities for extending the CPH model with fully transparent and symbolic non-linear log-risk functions. In this paper, we introduce Generalized Cox Proportional Hazards (GCPH) model, a novel method for survival analysis that leverages KAN to enable a non-linear mapping from covariates to survival outcomes in a fully symbolic manner. GCPH maintains the interpretability of traditional CPH models while allowing for the estimation of non-linear log-risk functions. Experiments conducted on both synthetic data and various public benchmarks demonstrate that GCPH achieves competitive performance in terms of prediction accuracy and exhibits superior interpretability compared to current state-of-the-art methods.</li>
</ul>

<h3>Title: AutoPDL: Automatic Prompt Optimization for LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Claudio Spiess, Mandana Vaziri, Louis Mandel, Martin Hirzel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04365">https://arxiv.org/abs/2504.04365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04365">https://arxiv.org/pdf/2504.04365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04365]] AutoPDL: Automatic Prompt Optimization for LLM Agents(https://arxiv.org/abs/2504.04365)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and non-transferable across LLMs or tasks. Therefore, this paper proposes AutoPDL, an automated approach to discover good LLM agent configurations. Our method frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and six LLMs (ranging from 8B to 70B parameters) show consistent accuracy gains ($9.5\pm17.5$ percentage points), up to 68.9pp, and reveal that selected prompting strategies vary across models and tasks.</li>
</ul>

<h3>Title: WeiDetect: Weibull Distribution-Based Defense against Poisoning Attacks in Federated Learning for Network Intrusion Detection Systems</h3>
<ul>
<li><strong>Authors: </strong>Sameera K. M., Vinod P., Anderson Rocha, Rafidha Rehiman K. A., Mauro Conti</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04367">https://arxiv.org/abs/2504.04367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04367">https://arxiv.org/pdf/2504.04367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04367]] WeiDetect: Weibull Distribution-Based Defense against Poisoning Attacks in Federated Learning for Network Intrusion Detection Systems(https://arxiv.org/abs/2504.04367)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack, federate</a></li>
<li><strong>Abstract: </strong>In the era of data expansion, ensuring data privacy has become increasingly critical, posing significant challenges to traditional AI-based applications. In addition, the increasing adoption of IoT devices has introduced significant cybersecurity challenges, making traditional Network Intrusion Detection Systems (NIDS) less effective against evolving threats, and privacy concerns and regulatory restrictions limit their deployment. Federated Learning (FL) has emerged as a promising solution, allowing decentralized model training while maintaining data privacy to solve these issues. However, despite implementing privacy-preserving technologies, FL systems remain vulnerable to adversarial attacks. Furthermore, data distribution among clients is not heterogeneous in the FL scenario. We propose WeiDetect, a two-phase, server-side defense mechanism for FL-based NIDS that detects malicious participants to address these challenges. In the first phase, local models are evaluated using a validation dataset to generate validation scores. These scores are then analyzed using a Weibull distribution, identifying and removing malicious models. We conducted experiments to evaluate the effectiveness of our approach in diverse attack settings. Our evaluation included two popular datasets, CIC-Darknet2020 and CSE-CIC-IDS2018, tested under non-IID data distributions. Our findings highlight that WeiDetect outperforms state-of-the-art defense approaches, improving higher target class recall up to 70% and enhancing the global model's F1 score by 1% to 14%.</li>
</ul>

<h3>Title: StyleRec: A Benchmark Dataset for Prompt Recovery in Writing Style Transformation</h3>
<ul>
<li><strong>Authors: </strong>Shenyang Liu, Yang Gao, Shaoyan Zhai, Liqiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04373">https://arxiv.org/abs/2504.04373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04373">https://arxiv.org/pdf/2504.04373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04373]] StyleRec: A Benchmark Dataset for Prompt Recovery in Writing Style Transformation(https://arxiv.org/abs/2504.04373)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompt Recovery, reconstructing prompts from the outputs of large language models (LLMs), has grown in importance as LLMs become ubiquitous. Most users access LLMs through APIs without internal model weights, relying only on outputs and logits, which complicates recovery. This paper explores a unique prompt recovery task focused on reconstructing prompts for style transfer and rephrasing, rather than typical question-answering. We introduce a dataset created with LLM assistance, ensuring quality through multiple techniques, and test methods like zero-shot, few-shot, jailbreak, chain-of-thought, fine-tuning, and a novel canonical-prompt fallback for poor-performing cases. Our results show that one-shot and fine-tuning yield the best outcomes but highlight flaws in traditional sentence similarity metrics for evaluating prompt recovery. Contributions include (1) a benchmark dataset, (2) comprehensive experiments on prompt recovery strategies, and (3) identification of limitations in current evaluation metrics, all of which advance general prompt recovery research, where the structure of the input prompt is unrestricted.</li>
</ul>

<h3>Title: iADCPS: Time Series Anomaly Detection for Evolving Cyber-physical Systems via Incremental Meta-learning</h3>
<ul>
<li><strong>Authors: </strong>Jiyu Tian, Mingchu Li, Liming Chen, Zumin Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04374">https://arxiv.org/abs/2504.04374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04374">https://arxiv.org/pdf/2504.04374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04374]] iADCPS: Time Series Anomaly Detection for Evolving Cyber-physical Systems via Incremental Meta-learning(https://arxiv.org/abs/2504.04374)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Anomaly detection for cyber-physical systems (ADCPS) is crucial in identifying faults and potential attacks by analyzing the time series of sensor measurements and actuator states. However, current methods lack adaptation to data distribution shifts in both temporal and spatial dimensions as cyber-physical systems evolve. To tackle this issue, we propose an incremental meta-learning-based approach, namely iADCPS, which can continuously update the model through limited evolving normal samples to reconcile the distribution gap between evolving and historical time series. Specifically, We first introduce a temporal mixup strategy to align data for data-level generalization which is then combined with the one-class meta-learning approach for model-level generalization. Furthermore, we develop a non-parametric dynamic threshold to adaptively adjust the threshold based on the probability density of the abnormal scores without any anomaly supervision. We empirically evaluate the effectiveness of the iADCPS using three publicly available datasets PUMP, SWaT, and WADI. The experimental results demonstrate that our method achieves 99.0%, 93.1%, and 78.7% F1-Score, respectively, which outperforms the state-of-the-art (SOTA) ADCPS method, especially in the context of the evolving CPSs.</li>
</ul>

<h3>Title: PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages</h3>
<ul>
<li><strong>Authors: </strong>Priyanshu Kumar, Devansh Jain, Akhila Yerukola, Liwei Jiang, Himanshu Beniwal, Thomas Hartvigsen, Maarten Sap</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04377">https://arxiv.org/abs/2504.04377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04377">https://arxiv.org/pdf/2504.04377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04377]] PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages(https://arxiv.org/abs/2504.04377)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Truly multilingual safety moderation efforts for Large Language Models (LLMs) have been hindered by a narrow focus on a small set of languages (e.g., English, Chinese) as well as a limited scope of safety definition, resulting in significant gaps in moderation capabilities. To bridge these gaps, we release POLYGUARD, a new state-of-the-art multilingual safety model for safeguarding LLM generations, and the corresponding training and evaluation datasets. POLYGUARD is trained on POLYGUARDMIX, the largest multilingual safety training corpus to date containing 1.91M samples across 17 languages (e.g., Chinese, Czech, English, Hindi). We also introduce POLYGUARDPROMPTS, a high quality multilingual benchmark with 29K samples for the evaluation of safety guardrails. Created by combining naturally occurring multilingual human-LLM interactions and human-verified machine translations of an English-only safety dataset (WildGuardMix; Han et al., 2024), our datasets contain prompt-output pairs with labels of prompt harmfulness, response harmfulness, and response refusal. Through extensive evaluations across multiple safety and toxicity benchmarks, we demonstrate that POLYGUARD outperforms existing state-of-the-art open-weight and commercial safety classifiers by 5.5%. Our contributions advance efforts toward safer multilingual LLMs for all global users.</li>
</ul>

<h3>Title: Pre-trained Language Models and Few-shot Learning for Medical Entity Extraction</h3>
<ul>
<li><strong>Authors: </strong>Xiaokai Wang, Guiran Liu, Binrong Zhu, Jacky He, Hongye Zheng, Hanlu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04385">https://arxiv.org/abs/2504.04385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04385">https://arxiv.org/pdf/2504.04385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04385]] Pre-trained Language Models and Few-shot Learning for Medical Entity Extraction(https://arxiv.org/abs/2504.04385)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>This study proposes a medical entity extraction method based on Transformer to enhance the information extraction capability of medical literature. Considering the professionalism and complexity of medical texts, we compare the performance of different pre-trained language models (BERT, BioBERT, PubMedBERT, ClinicalBERT) in medical entity extraction tasks. Experimental results show that PubMedBERT achieves the best performance (F1-score = 88.8%), indicating that a language model pre-trained on biomedical literature is more effective in the medical domain. In addition, we analyze the impact of different entity extraction methods (CRF, Span-based, Seq2Seq) and find that the Span-based approach performs best in medical entity extraction tasks (F1-score = 88.6%). It demonstrates superior accuracy in identifying entity boundaries. In low-resource scenarios, we further explore the application of Few-shot Learning in medical entity extraction. Experimental results show that even with only 10-shot training samples, the model achieves an F1-score of 79.1%, verifying the effectiveness of Few-shot Learning under limited data conditions. This study confirms that the combination of pre-trained language models and Few-shot Learning can enhance the accuracy of medical entity extraction. Future research can integrate knowledge graphs and active learning strategies to improve the model's generalization and stability, providing a more effective solution for medical NLP research. Keywords- Natural Language Processing, medical named entity recognition, pre-trained language model, Few-shot Learning, information extraction, deep learning</li>
</ul>

<h3>Title: Who's Watching You Zoom? Investigating Privacy of Third-Party Zoom Apps</h3>
<ul>
<li><strong>Authors: </strong>Saharsh Goenka, Adit Prabhu, Payge Sakurai, Mrinaal Ramachandran, Rakibul Hasan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04388">https://arxiv.org/abs/2504.04388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04388">https://arxiv.org/pdf/2504.04388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04388]] Who's Watching You Zoom? Investigating Privacy of Third-Party Zoom Apps(https://arxiv.org/abs/2504.04388)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Zoom serves millions of users daily and allows third-party developers to integrate their apps with the Zoom client and reach those users. So far, these apps' privacy and security aspects, which can access rich audio-visual data (among others) from Zoom, have not been scientifically investigated. This paper examines the evolution of the Zoom Marketplace over one year, identifying trends in apps, their data collection behaviors, and the transparency of privacy policies. Our findings include worrisome details about the increasing over-collection of user data, non-transparency about purposes and sharing behaviors, and possible non-compliance with relevant laws. We believe these findings will inform future privacy and security research on this platform and help improve Zoom's app review process and platform policy.</li>
</ul>

<h3>Title: Selective Masking Adversarial Attack on Automatic Speech Recognition Systems</h3>
<ul>
<li><strong>Authors: </strong>Zheng Fang, Shenyi Zhang, Tao Wang, Bowen Li, Lingchen Zhao, Zhangyi Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04394">https://arxiv.org/abs/2504.04394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04394">https://arxiv.org/pdf/2504.04394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04394]] Selective Masking Adversarial Attack on Automatic Speech Recognition Systems(https://arxiv.org/abs/2504.04394)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Extensive research has shown that Automatic Speech Recognition (ASR) systems are vulnerable to audio adversarial attacks. Current attacks mainly focus on single-source scenarios, ignoring dual-source scenarios where two people are speaking simultaneously. To bridge the gap, we propose a Selective Masking Adversarial attack, namely SMA attack, which ensures that one audio source is selected for recognition while the other audio source is muted in dual-source scenarios. To better adapt to the dual-source scenario, our SMA attack constructs the normal dual-source audio from the muted audio and selected audio. SMA attack initializes the adversarial perturbation with a small Gaussian noise and iteratively optimizes it using a selective masking optimization algorithm. Extensive experiments demonstrate that the SMA attack can generate effective and imperceptible audio adversarial examples in the dual-source scenario, achieving an average success rate of attack of 100% and signal-to-noise ratio of 37.15dB on Conformer-CTC, outperforming the baselines.</li>
</ul>

<h3>Title: Human-Level Competitive Pokémon via Scalable Offline Reinforcement Learning with Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jake Grigsby, Yuqi Xie, Justin Sasek, Steven Zheng, Yuke Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04395">https://arxiv.org/abs/2504.04395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04395">https://arxiv.org/pdf/2504.04395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04395]] Human-Level Competitive Pokémon via Scalable Offline Reinforcement Learning with Transformers(https://arxiv.org/abs/2504.04395)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Competitive Pokémon Singles (CPS) is a popular strategy game where players learn to exploit their opponent based on imperfect information in battles that can last more than one hundred stochastic turns. AI research in CPS has been led by heuristic tree search and online self-play, but the game may also create a platform to study adaptive policies trained offline on large datasets. We develop a pipeline to reconstruct the first-person perspective of an agent from logs saved from the third-person perspective of a spectator, thereby unlocking a dataset of real human battles spanning more than a decade that grows larger every day. This dataset enables a black-box approach where we train large sequence models to adapt to their opponent based solely on their input trajectory while selecting moves without explicit search of any kind. We study a progression from imitation learning to offline RL and offline fine-tuning on self-play data in the hardcore competitive setting of Pokémon's four oldest (and most partially observed) game generations. The resulting agents outperform a recent LLM Agent approach and a strong heuristic search engine. While playing anonymously in online battles against humans, our best agents climb to rankings inside the top 10% of active players.</li>
</ul>

<h3>Title: LeakGuard: Detecting Memory Leaks Accurately and Scalably</h3>
<ul>
<li><strong>Authors: </strong>Hongliang Liang, Luming Yin, Guohao Wu, Yuxiang Li, Qiuping Yi, Lei Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04422">https://arxiv.org/abs/2504.04422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04422">https://arxiv.org/pdf/2504.04422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04422]] LeakGuard: Detecting Memory Leaks Accurately and Scalably(https://arxiv.org/abs/2504.04422)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Memory leaks are prevalent in various real-world software projects, thereby leading to serious attacks like denial-of-service. Though prior methods for detecting memory leaks made significant advance, they often suffer from low accuracy and weak scalability for testing large and complex programs. In this paper we present LeakGuard, a memory leak detection tool which provides satisfactory balance of accuracy and scalability. For accuracy, LeakGuard analyzes the behaviors of library and developer-defined memory allocation and deallocation functions in a path-sensitive manner and generates function summaries for them in a bottom-up approach. Additionally, we develop a pointer escape analysis technique to model the transfer of pointer ownership. For scalability, LeakGuard examines each function of interest independently by using its function summary and under-constrained symbolic execution technique, which effectively mitigates path explosion problem. Our extensive evaluation on 18 real-world software projects and standard benchmark datasets demonstrates that LeakGuard achieves significant advancements in multiple aspects: it exhibits superior MAD function identification capability compared to Goshawk, outperforms five state-of-the-art methods in defect detection accuracy, and successfully identifies 129 previously undetected memory leak bugs, all of which have been independently verified and confirmed by the respective development teams.</li>
</ul>

<h3>Title: UniToken: Harmonizing Multimodal Understanding and Generation through Unified Visual Encoding</h3>
<ul>
<li><strong>Authors: </strong>Yang Jiao, Haibo Qiu, Zequn Jie, Shaoxiang Chen, Jingjing Chen, Lin Ma, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04423">https://arxiv.org/abs/2504.04423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04423">https://arxiv.org/pdf/2504.04423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04423]] UniToken: Harmonizing Multimodal Understanding and Generation through Unified Visual Encoding(https://arxiv.org/abs/2504.04423)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce UniToken, an auto-regressive generation model that encodes visual inputs through a combination of discrete and continuous representations, enabling seamless integration of unified visual understanding and image generation tasks. Unlike previous approaches that rely on unilateral visual representations, our unified visual encoding framework captures both high-level semantics and low-level details, delivering multidimensional information that empowers heterogeneous tasks to selectively assimilate domain-specific knowledge based on their inherent characteristics. Through in-depth experiments, we uncover key principles for developing a unified model capable of both visual understanding and image generation. Extensive evaluations across a diverse range of prominent benchmarks demonstrate that UniToken achieves state-of-the-art performance, surpassing existing approaches. These results establish UniToken as a robust foundation for future research in this domain. The code and models are available at this https URL.</li>
</ul>

<h3>Title: FluentLip: A Phonemes-Based Two-stage Approach for Audio-Driven Lip Synthesis with Optical Flow Consistency</h3>
<ul>
<li><strong>Authors: </strong>Shiyan Liu, Rui Qu, Yan Jin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04427">https://arxiv.org/abs/2504.04427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04427">https://arxiv.org/pdf/2504.04427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04427]] FluentLip: A Phonemes-Based Two-stage Approach for Audio-Driven Lip Synthesis with Optical Flow Consistency(https://arxiv.org/abs/2504.04427)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generating consecutive images of lip movements that align with a given speech in audio-driven lip synthesis is a challenging task. While previous studies have made strides in synchronization and visual quality, lip intelligibility and video fluency remain persistent challenges. This work proposes FluentLip, a two-stage approach for audio-driven lip synthesis, incorporating three featured strategies. To improve lip synchronization and intelligibility, we integrate a phoneme extractor and encoder to generate a fusion of audio and phoneme information for multimodal learning. Additionally, we employ optical flow consistency loss to ensure natural transitions between image frames. Furthermore, we incorporate a diffusion chain during the training of Generative Adversarial Networks (GANs) to improve both stability and efficiency. We evaluate our proposed FluentLip through extensive experiments, comparing it with five state-of-the-art (SOTA) approaches across five metrics, including a proposed metric called Phoneme Error Rate (PER) that evaluates lip pose intelligibility and video fluency. The experimental results demonstrate that our FluentLip approach is highly competitive, achieving significant improvements in smoothness and naturalness. In particular, it outperforms these SOTA approaches by approximately $\textbf{16.3%}$ in Fréchet Inception Distance (FID) and $\textbf{35.2%}$ in PER.</li>
</ul>

<h3>Title: Evaluation framework for Image Segmentation Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Tatiana Merkulova, Bharani Jayakumar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04435">https://arxiv.org/abs/2504.04435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04435">https://arxiv.org/pdf/2504.04435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04435]] Evaluation framework for Image Segmentation Algorithms(https://arxiv.org/abs/2504.04435)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive evaluation framework for image segmentation algorithms, encompassing naive methods, machine learning approaches, and deep learning techniques. We begin by introducing the fundamental concepts and importance of image segmentation, and the role of interactive segmentation in enhancing accuracy. A detailed background theory section explores various segmentation methods, including thresholding, edge detection, region growing, feature extraction, random forests, support vector machines, convolutional neural networks, U-Net, and Mask R-CNN. The implementation and experimental setup are thoroughly described, highlighting three primary approaches: algorithm assisting user, user assisting algorithm, and hybrid methods. Evaluation metrics such as Intersection over Union (IoU), computation time, and user interaction time are employed to measure performance. A comparative analysis presents detailed results, emphasizing the strengths, limitations, and trade-offs of each method. The paper concludes with insights into the practical applicability of these approaches across various scenarios and outlines future work, focusing on expanding datasets, developing more representative approaches, integrating real-time feedback, and exploring weakly supervised and self-supervised learning paradigms to enhance segmentation accuracy and efficiency. Keywords: Image Segmentation, Interactive Segmentation, Machine Learning, Deep Learning, Computer Vision</li>
</ul>

<h3>Title: On the Spatial Structure of Mixture-of-Experts in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Daniel Bershatsky, Ivan Oseledets</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04444">https://arxiv.org/abs/2504.04444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04444">https://arxiv.org/pdf/2504.04444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04444]] On the Spatial Structure of Mixture-of-Experts in Transformers(https://arxiv.org/abs/2504.04444)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>A common assumption is that MoE routers primarily leverage semantic features for expert selection. However, our study challenges this notion by demonstrating that positional token information also plays a crucial role in routing decisions. Through extensive empirical analysis, we provide evidence supporting this hypothesis, develop a phenomenological explanation of the observed behavior, and discuss practical implications for MoE-based architectures.</li>
</ul>

<h3>Title: PRISM: Probabilistic Representation for Integrated Shape Modeling and Generation</h3>
<ul>
<li><strong>Authors: </strong>Lei Cheng, Mahdi Saleh, Qing Cheng, Lu Sang, Hongli Xu, Daniel Cremers, Federico Tombari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04454">https://arxiv.org/abs/2504.04454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04454">https://arxiv.org/pdf/2504.04454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04454]] PRISM: Probabilistic Representation for Integrated Shape Modeling and Generation(https://arxiv.org/abs/2504.04454)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite the advancements in 3D full-shape generation, accurately modeling complex geometries and semantics of shape parts remains a significant challenge, particularly for shapes with varying numbers of parts. Current methods struggle to effectively integrate the contextual and structural information of 3D shapes into their generative processes. We address these limitations with PRISM, a novel compositional approach for 3D shape generation that integrates categorical diffusion models with Statistical Shape Models (SSM) and Gaussian Mixture Models (GMM). Our method employs compositional SSMs to capture part-level geometric variations and uses GMM to represent part semantics in a continuous space. This integration enables both high fidelity and diversity in generated shapes while preserving structural coherence. Through extensive experiments on shape generation and manipulation tasks, we demonstrate that our approach significantly outperforms previous methods in both quality and controllability of part-level operations. Our code will be made publicly available.</li>
</ul>

<h3>Title: An overview of model uncertainty and variability in LLM-based sentiment analysis. Challenges, mitigation strategies and the role of explainability</h3>
<ul>
<li><strong>Authors: </strong>David Herrera-Poyatos, Carlos Peláez-González, Cristina Zuheros, Andrés Herrera-Poyatos, Virilo Tejedor, Francisco Herrera, Rosana Montes</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04462">https://arxiv.org/abs/2504.04462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04462">https://arxiv.org/pdf/2504.04462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04462]] An overview of model uncertainty and variability in LLM-based sentiment analysis. Challenges, mitigation strategies and the role of explainability(https://arxiv.org/abs/2504.04462)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have significantly advanced sentiment analysis, yet their inherent uncertainty and variability pose critical challenges to achieving reliable and consistent outcomes. This paper systematically explores the Model Variability Problem (MVP) in LLM-based sentiment analysis, characterized by inconsistent sentiment classification, polarization, and uncertainty arising from stochastic inference mechanisms, prompt sensitivity, and biases in training data. We analyze the core causes of MVP, presenting illustrative examples and a case study to highlight its impact. In addition, we investigate key challenges and mitigation strategies, paying particular attention to the role of temperature as a driver of output randomness and emphasizing the crucial role of explainability in improving transparency and user trust. By providing a structured perspective on stability, reproducibility, and trustworthiness, this study helps develop more reliable, explainable, and robust sentiment analysis models, facilitating their deployment in high-stakes domains such as finance, healthcare, and policymaking, among others.</li>
</ul>

<h3>Title: Domain Generalization for Face Anti-spoofing via Content-aware Composite Prompt Engineering</h3>
<ul>
<li><strong>Authors: </strong>Jiabao Guo, Ajian Liu, Yunfeng Diao, Jin Zhang, Hui Ma, Bo Zhao, Richang Hong, Meng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04470">https://arxiv.org/abs/2504.04470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04470">https://arxiv.org/pdf/2504.04470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04470]] Domain Generalization for Face Anti-spoofing via Content-aware Composite Prompt Engineering(https://arxiv.org/abs/2504.04470)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The challenge of Domain Generalization (DG) in Face Anti-Spoofing (FAS) is the significant interference of domain-specific signals on subtle spoofing clues. Recently, some CLIP-based algorithms have been developed to alleviate this interference by adjusting the weights of visual classifiers. However, our analysis of this class-wise prompt engineering suffers from two shortcomings for DG FAS: (1) The categories of facial categories, such as real or spoof, have no semantics for the CLIP model, making it difficult to learn accurate category descriptions. (2) A single form of prompt cannot portray the various types of spoofing. In this work, instead of class-wise prompts, we propose a novel Content-aware Composite Prompt Engineering (CCPE) that generates instance-wise composite prompts, including both fixed template and learnable prompts. Specifically, our CCPE constructs content-aware prompts from two branches: (1) Inherent content prompt explicitly benefits from abundant transferred knowledge from the instruction-based Large Language Model (LLM). (2) Learnable content prompts implicitly extract the most informative visual content via Q-Former. Moreover, we design a Cross-Modal Guidance Module (CGM) that dynamically adjusts unimodal features for fusion to achieve better generalized FAS. Finally, our CCPE has been validated for its effectiveness in multiple cross-domain experiments and achieves state-of-the-art (SOTA) results.</li>
</ul>

<h3>Title: VideoAgent2: Enhancing the LLM-Based Agent System for Long-Form Video Understanding by Uncertainty-Aware CoT</h3>
<ul>
<li><strong>Authors: </strong>Zhuo Zhi, Qiangqiang Wu, Minghe shen, Wenbo Li, Yinchuan Li, Kun Shao, Kaiwen Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04471">https://arxiv.org/abs/2504.04471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04471">https://arxiv.org/pdf/2504.04471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04471]] VideoAgent2: Enhancing the LLM-Based Agent System for Long-Form Video Understanding by Uncertainty-Aware CoT(https://arxiv.org/abs/2504.04471)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Long video understanding has emerged as an increasingly important yet challenging task in computer vision. Agent-based approaches are gaining popularity for processing long videos, as they can handle extended sequences and integrate various tools to capture fine-grained information. However, existing methods still face several challenges: (1) they often rely solely on the reasoning ability of large language models (LLMs) without dedicated mechanisms to enhance reasoning in long video scenarios; and (2) they remain vulnerable to errors or noise from external tools. To address these issues, we propose a specialized chain-of-thought (CoT) process tailored for long video analysis. Our proposed CoT with plan-adjust mode enables the LLM to incrementally plan and adapt its information-gathering strategy. We further incorporate heuristic uncertainty estimation of both the LLM and external tools to guide the CoT process. This allows the LLM to assess the reliability of newly collected information, refine its collection strategy, and make more robust decisions when synthesizing final answers. Empirical experiments show that our uncertainty-aware CoT effectively mitigates noise from external tools, leading to more reliable outputs. We implement our approach in a system called VideoAgent2, which also includes additional modules such as general context acquisition and specialized tool design. Evaluation on three dedicated long video benchmarks (and their subsets) demonstrates that VideoAgent2 outperforms the previous state-of-the-art agent-based method, VideoAgent, by an average of 13.1% and achieves leading performance among all zero-shot approaches</li>
</ul>

<h3>Title: Statistical Guarantees Of False Discovery Rate In Medical Instance Segmentation Tasks Based on Conformal Risk Control</h3>
<ul>
<li><strong>Authors: </strong>Mengxia Dai, Wenqian Luo, Tianyang Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04482">https://arxiv.org/abs/2504.04482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04482">https://arxiv.org/pdf/2504.04482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04482]] Statistical Guarantees Of False Discovery Rate In Medical Instance Segmentation Tasks Based on Conformal Risk Control(https://arxiv.org/abs/2504.04482)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Instance segmentation plays a pivotal role in medical image analysis by enabling precise localization and delineation of lesions, tumors, and anatomical structures. Although deep learning models such as Mask R-CNN and BlendMask have achieved remarkable progress, their application in high-risk medical scenarios remains constrained by confidence calibration issues, which may lead to misdiagnosis. To address this challenge, we propose a robust quality control framework based on conformal prediction theory. This framework innovatively constructs a risk-aware dynamic threshold mechanism that adaptively adjusts segmentation decision boundaries according to clinical this http URL, we design a \textbf{calibration-aware loss function} that dynamically tunes the segmentation threshold based on a user-defined risk level $\alpha$. Utilizing exchangeable calibration data, this method ensures that the expected FNR or FDR on test data remains below $\alpha$ with high probability. The framework maintains compatibility with mainstream segmentation models (e.g., Mask R-CNN, BlendMask+ResNet-50-FPN) and datasets (PASCAL VOC format) without requiring architectural modifications. Empirical results demonstrate that we rigorously bound the FDR metric marginally over the test set via our developed calibration framework.</li>
</ul>

<h3>Title: Skin Color Measurement from Dermatoscopic Images: An Evaluation on a Synthetic Dataset</h3>
<ul>
<li><strong>Authors: </strong>Marin Benčević, Robert Šojo, Irena Galić</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04494">https://arxiv.org/abs/2504.04494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04494">https://arxiv.org/pdf/2504.04494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04494]] Skin Color Measurement from Dermatoscopic Images: An Evaluation on a Synthetic Dataset(https://arxiv.org/abs/2504.04494)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive evaluation of skin color measurement methods from dermatoscopic images using a synthetic dataset (S-SYNTH) with controlled ground-truth melanin content, lesion shapes, hair models, and 18 distinct lighting conditions. This allows for rigorous assessment of the robustness and invariance to lighting conditions. We assess four classes of image colorimetry approaches: segmentation-based, patch-based, color quantization, and neural networks. We use these methods to estimate the Individual Typology Angle (ITA) and Fitzpatrick types from dermatoscopic images. Our results show that segmentation-based and color quantization methods yield robust, lighting-invariant estimates, whereas patch-based approaches exhibit significant lighting-dependent biases that require calibration. Furthermore, neural network models, particularly when combined with heavy blurring to reduce overfitting, can provide light-invariant Fitzpatrick predictions, although their generalization to real-world images remains unverified. We conclude with practical recommendations for designing fair and reliable skin color estimation methods.</li>
</ul>

<h3>Title: AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Peng Wu, Wanshun Su, Guansong Pang, Yujia Sun, Qingsen Yan, Peng Wang, Yanning Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04495">https://arxiv.org/abs/2504.04495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04495">https://arxiv.org/pdf/2504.04495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04495]] AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection(https://arxiv.org/abs/2504.04495)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the increasing adoption of video anomaly detection in intelligent surveillance domains, conventional visual-based detection approaches often struggle with information insufficiency and high false-positive rates in complex environments. To address these limitations, we present a novel weakly supervised framework that leverages audio-visual collaboration for robust video anomaly detection. Capitalizing on the exceptional cross-modal representation learning capabilities of Contrastive Language-Image Pretraining (CLIP) across visual, audio, and textual domains, our framework introduces two major innovations: an efficient audio-visual fusion that enables adaptive cross-modal integration through lightweight parametric adaptation while maintaining the frozen CLIP backbone, and a novel audio-visual prompt that dynamically enhances text embeddings with key multimodal information based on the semantic correlation between audio-visual features and textual labels, significantly improving CLIP's generalization for the video anomaly detection task. Moreover, to enhance robustness against modality deficiency during inference, we further develop an uncertainty-driven feature distillation module that synthesizes audio-visual representations from visual-only inputs. This module employs uncertainty modeling based on the diversity of audio-visual features to dynamically emphasize challenging features during the distillation process. Our framework demonstrates superior performance across multiple benchmarks, with audio integration significantly boosting anomaly detection accuracy in various scenarios. Notably, with unimodal data enhanced by uncertainty-driven distillation, our approach consistently outperforms current unimodal VAD methods.</li>
</ul>

<h3>Title: Attributed Synthetic Data Generation for Zero-shot Domain-specific Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Shijian Wang, Linxin Song, Ryotaro Shimizu, Masayuki Goto, Hanqian Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04510">https://arxiv.org/abs/2504.04510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04510">https://arxiv.org/pdf/2504.04510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04510]] Attributed Synthetic Data Generation for Zero-shot Domain-specific Image Classification(https://arxiv.org/abs/2504.04510)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Zero-shot domain-specific image classification is challenging in classifying real images without ground-truth in-domain training examples. Recent research involved knowledge from texts with a text-to-image model to generate in-domain training images in zero-shot scenarios. However, existing methods heavily rely on simple prompt strategies, limiting the diversity of synthetic training images, thus leading to inferior performance compared to real images. In this paper, we propose AttrSyn, which leverages large language models to generate attributed prompts. These prompts allow for the generation of more diverse attributed synthetic images. Experiments for zero-shot domain-specific image classification on two fine-grained datasets show that training with synthetic images generated by AttrSyn significantly outperforms CLIP's zero-shot classification under most situations and consistently surpasses simple prompt strategies.</li>
</ul>

<h3>Title: Saliency-driven Dynamic Token Pruning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yao Tao, Yehui Tang, Yun Wang, Mingjian Zhu, Hailin Hu, Yunhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04514">https://arxiv.org/abs/2504.04514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04514">https://arxiv.org/pdf/2504.04514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04514]] Saliency-driven Dynamic Token Pruning for Large Language Models(https://arxiv.org/abs/2504.04514)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Despite the recent success of large language models (LLMs), LLMs are particularly challenging in long-sequence inference scenarios due to the quadratic computational complexity of the attention mechanism. Inspired by the interpretability theory of feature attribution in neural network models, we observe that not all tokens have the same contribution. Based on this observation, we propose a novel token pruning framework, namely Saliency-driven Dynamic Token Pruning (SDTP), to gradually and dynamically prune redundant tokens based on the input context. Specifically, a lightweight saliency-driven prediction module is designed to estimate the importance score of each token with its hidden state, which is added to different layers of the LLM to hierarchically prune redundant tokens. Furthermore, a ranking-based optimization strategy is proposed to minimize the ranking divergence of the saliency score and the predicted importance score. Extensive experiments have shown that our framework is generalizable to various models and datasets. By hierarchically pruning 65\% of the input tokens, our method greatly reduces 33\% $\sim$ 47\% FLOPs and achieves speedup up to 1.75$\times$ during inference, while maintaining comparable performance. We further demonstrate that SDTP can be combined with KV cache compression method for further compression.</li>
</ul>

<h3>Title: SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Junjie Jiang, Zelin Wang, Manqi Zhao, Yin Li, DongSheng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04519">https://arxiv.org/abs/2504.04519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04519">https://arxiv.org/pdf/2504.04519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04519]] SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation(https://arxiv.org/abs/2504.04519)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Segment Anything 2 (SAM2) enables robust single-object tracking using segmentation. To extend this to multi-object tracking (MOT), we propose SAM2MOT, introducing a novel Tracking by Segmentation paradigm. Unlike Tracking by Detection or Tracking by Query, SAM2MOT directly generates tracking boxes from segmentation masks, reducing reliance on detection accuracy. SAM2MOT has two key advantages: zero-shot generalization, allowing it to work across datasets without fine-tuning, and strong object association, inherited from SAM2. To further improve performance, we integrate a trajectory manager system for precise object addition and removal, and a cross-object interaction module to handle occlusions. Experiments on DanceTrack, UAVDT, and BDD100K show state-of-the-art results. Notably, SAM2MOT outperforms existing methods on DanceTrack by +2.1 HOTA and +4.5 IDF1, highlighting its effectiveness in MOT.</li>
</ul>

<h3>Title: Hessian of Perplexity for Large Language Models by PyTorch autograd (Open Source)</h3>
<ul>
<li><strong>Authors: </strong>Ivan Ilin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04520">https://arxiv.org/abs/2504.04520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04520">https://arxiv.org/pdf/2504.04520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04520]] Hessian of Perplexity for Large Language Models by PyTorch autograd (Open Source)(https://arxiv.org/abs/2504.04520)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Computing the full Hessian matrix -- the matrix of second-order derivatives for an entire Large Language Model (LLM) is infeasible due to its sheer size. In this technical report, we aim to provide a comprehensive guide on how to accurately compute at least a small portion of the Hessian for LLMs using PyTorch autograd library. We also demonstrate how to compute the full diagonal of the Hessian matrix using multiple samples of vector-Hessian Products (HVPs). We hope that both this guide and the accompanying GitHub code will be valuable resources for practitioners and researchers interested in better understanding the behavior and structure of the Hessian in LLMs.</li>
</ul>

<h3>Title: Trust Region Preference Approximation: A simple and stable reinforcement learning algorithm for LLM reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xuerui Su, Shufang Xie, Guoqing Liu, Yingce Xia, Renqian Luo, Peiran Jin, Zhiming Ma, Yue Wang, Zun Wang, Yuting Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04524">https://arxiv.org/abs/2504.04524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04524">https://arxiv.org/pdf/2504.04524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04524]] Trust Region Preference Approximation: A simple and stable reinforcement learning algorithm for LLM reasoning(https://arxiv.org/abs/2504.04524)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recently, Large Language Models (LLMs) have rapidly evolved, approaching Artificial General Intelligence (AGI) while benefiting from large-scale reinforcement learning to enhance Human Alignment (HA) and Reasoning. Recent reward-based optimization algorithms, such as Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO) have achieved significant performance on reasoning tasks, whereas preference-based optimization algorithms such as Direct Preference Optimization (DPO) significantly improve the performance of LLMs on human alignment. However, despite the strong performance of reward-based optimization methods in alignment tasks , they remain vulnerable to reward hacking. Furthermore, preference-based algorithms (such as Online DPO) haven't yet matched the performance of reward-based optimization algorithms (like PPO) on reasoning tasks, making their exploration in this specific area still a worthwhile pursuit. Motivated by these challenges, we propose the Trust Region Preference Approximation (TRPA) algorithm, which integrates rule-based optimization with preference-based optimization for reasoning tasks. As a preference-based algorithm, TRPA naturally eliminates the reward hacking issue. TRPA constructs preference levels using predefined rules, forms corresponding preference pairs, and leverages a novel optimization algorithm for RL training with a theoretical monotonic improvement guarantee. Experimental results demonstrate that TRPA not only achieves competitive performance on reasoning tasks but also exhibits robust stability. The code of this paper are released and updating on this https URL.</li>
</ul>

<h3>Title: An Empirical Comparison of Text Summarization: A Multi-Dimensional Evaluation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Anantharaman Janakiraman, Behnaz Ghoraani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04534">https://arxiv.org/abs/2504.04534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04534">https://arxiv.org/pdf/2504.04534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04534]] An Empirical Comparison of Text Summarization: A Multi-Dimensional Evaluation of Large Language Models(https://arxiv.org/abs/2504.04534)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text summarization is crucial for mitigating information overload across domains like journalism, medicine, and business. This research evaluates summarization performance across 17 large language models (OpenAI, Google, Anthropic, open-source) using a novel multi-dimensional framework. We assessed models on seven diverse datasets (BigPatent, BillSum, CNN/DailyMail, PubMed, SAMSum, WikiHow, XSum) at three output lengths (50, 100, 150 tokens) using metrics for factual consistency, semantic similarity, lexical overlap, and human-like quality, while also considering efficiency factors. Our findings reveal significant performance differences, with specific models excelling in factual accuracy (deepseek-v3), human-like quality (claude-3-5-sonnet), and processing efficiency/cost-effectiveness (gemini-1.5-flash, gemini-2.0-flash). Performance varies dramatically by dataset, with models struggling on technical domains but performing well on conversational content. We identified a critical tension between factual consistency (best at 50 tokens) and perceived quality (best at 150 tokens). Our analysis provides evidence-based recommendations for different use cases, from high-stakes applications requiring factual accuracy to resource-constrained environments needing efficient processing. This comprehensive approach enhances evaluation methodology by integrating quality metrics with operational considerations, incorporating trade-offs between accuracy, efficiency, and cost-effectiveness to guide model selection for specific applications.</li>
</ul>

<h3>Title: The Point, the Vision and the Text: Does Point Cloud Boost Spatial Reasoning of Large Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Weichen Zhang, Ruiying Peng, Chen Gao, Jianjie Fang, Xin Zeng, Kaiyuan Li, Ziyou Wang, Jinqiang Cui, Xin Wang, Xinlei Chen, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04540">https://arxiv.org/abs/2504.04540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04540">https://arxiv.org/pdf/2504.04540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04540]] The Point, the Vision and the Text: Does Point Cloud Boost Spatial Reasoning of Large Language Models?(https://arxiv.org/abs/2504.04540)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>3D Large Language Models (LLMs) leveraging spatial information in point clouds for 3D spatial reasoning attract great attention. Despite some promising results, the role of point clouds in 3D spatial reasoning remains under-explored. In this work, we comprehensively evaluate and analyze these models to answer the research question: \textit{Does point cloud truly boost the spatial reasoning capacities of 3D LLMs?} We first evaluate the spatial reasoning capacity of LLMs with different input modalities by replacing the point cloud with the visual and text counterparts. We then propose a novel 3D QA (Question-answering) benchmark, ScanReQA, that comprehensively evaluates models' understanding of binary spatial relationships. Our findings reveal several critical insights: 1) LLMs without point input could even achieve competitive performance even in a zero-shot manner; 2) existing 3D LLMs struggle to comprehend the binary spatial relationships; 3) 3D LLMs exhibit limitations in exploiting the structural coordinates in point clouds for fine-grained spatial reasoning. We think these conclusions can help the next step of 3D LLMs and also offer insights for foundation models in other modalities. We release datasets and reproducible codes in the anonymous project page: this https URL.</li>
</ul>

<h3>Title: Opening the black box of deep learning: Validating the statistical association between explainable artificial intelligence (XAI) and clinical domain knowledge in fundus image-based glaucoma diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Han Yuan, Lican Kang, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04549">https://arxiv.org/abs/2504.04549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04549">https://arxiv.org/pdf/2504.04549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04549]] Opening the black box of deep learning: Validating the statistical association between explainable artificial intelligence (XAI) and clinical domain knowledge in fundus image-based glaucoma diagnosis(https://arxiv.org/abs/2504.04549)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While deep learning has exhibited remarkable predictive capabilities in various medical image tasks, its inherent black-box nature has hindered its widespread implementation in real-world healthcare settings. Our objective is to unveil the decision-making processes of deep learning models in the context of glaucoma classification by employing several Class Activation Map (CAM) techniques to generate model focus regions and comparing them with clinical domain knowledge of the anatomical area (optic cup, optic disk, and blood vessels). Four deep neural networks, including VGG-11, ResNet-18, DeiT-Tiny, and Swin Transformer-Tiny, were developed using binary diagnostic labels of glaucoma and five CAM methods (Grad-CAM, XGrad-CAM, Score-CAM, Eigen-CAM, and Layer-CAM) were employed to highlight the model focus area. We applied the paired-sample t-test to compare the percentage of anatomies in the model focus area to the proportion of anatomies in the entire image. After that, Pearson's and Spearman's correlation tests were implemented to examine the relationship between model predictive ability and the percentage of anatomical structures in the model focus area. On five public glaucoma datasets, all deep learning models consistently displayed statistically significantly higher percentages of anatomical structures in the focus area than the proportions of anatomical structures in the entire image. Also, we validated the positive relationship between the percentage of anatomical structures in the focus area and model predictive performance. Our study provides evidence of the convergence of decision logic between deep neural networks and human clinicians through rigorous statistical tests. We anticipate that it can help alleviate clinicians' concerns regarding the trustworthiness of deep learning in healthcare. For reproducibility, the code and dataset have been released at GitHub.</li>
</ul>

<h3>Title: Advancing Egocentric Video Question Answering with Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alkesh Patel, Vibhav Chitalia, Yinfei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04550">https://arxiv.org/abs/2504.04550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04550">https://arxiv.org/pdf/2504.04550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04550]] Advancing Egocentric Video Question Answering with Multimodal Large Language Models(https://arxiv.org/abs/2504.04550)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Egocentric Video Question Answering (QA) requires models to handle long-horizon temporal reasoning, first-person perspectives, and specialized challenges like frequent camera movement. This paper systematically evaluates both proprietary and open-source Multimodal Large Language Models (MLLMs) on QaEgo4Dv2 - a refined dataset of egocentric videos derived from QaEgo4D. Four popular MLLMs (GPT-4o, Gemini-1.5-Pro, Video-LLaVa-7B and Qwen2-VL-7B-Instruct) are assessed using zero-shot and fine-tuned approaches for both OpenQA and CloseQA settings. We introduce QaEgo4Dv2 to mitigate annotation noise in QaEgo4D, enabling more reliable comparison. Our results show that fine-tuned Video-LLaVa-7B and Qwen2-VL-7B-Instruct achieve new state-of-the-art performance, surpassing previous benchmarks by up to +2.6% ROUGE/METEOR (for OpenQA) and +13% accuracy (for CloseQA). We also present a thorough error analysis, indicating the model's difficulty in spatial reasoning and fine-grained object recognition - key areas for future improvement.</li>
</ul>

<h3>Title: DyCON: Dynamic Uncertainty-aware Consistency and Contrastive Learning for Semi-supervised Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Maregu Assefa, Muzammal Naseer, Iyyakutti Iyappan Ganapathi, Syed Sadaf Ali, Mohamed L Seghier, Naoufel Werghi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04566">https://arxiv.org/abs/2504.04566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04566">https://arxiv.org/pdf/2504.04566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04566]] DyCON: Dynamic Uncertainty-aware Consistency and Contrastive Learning for Semi-supervised Medical Image Segmentation(https://arxiv.org/abs/2504.04566)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semi-supervised learning in medical image segmentation leverages unlabeled data to reduce annotation burdens through consistency learning. However, current methods struggle with class imbalance and high uncertainty from pathology variations, leading to inaccurate segmentation in 3D medical images. To address these challenges, we present DyCON, a Dynamic Uncertainty-aware Consistency and Contrastive Learning framework that enhances the generalization of consistency methods with two complementary losses: Uncertainty-aware Consistency Loss (UnCL) and Focal Entropy-aware Contrastive Loss (FeCL). UnCL enforces global consistency by dynamically weighting the contribution of each voxel to the consistency loss based on its uncertainty, preserving high-uncertainty regions instead of filtering them out. Initially, UnCL prioritizes learning from uncertain voxels with lower penalties, encouraging the model to explore challenging regions. As training progress, the penalty shift towards confident voxels to refine predictions and ensure global consistency. Meanwhile, FeCL enhances local feature discrimination in imbalanced regions by introducing dual focal mechanisms and adaptive confidence adjustments into the contrastive principle. These mechanisms jointly prioritizes hard positives and negatives while focusing on uncertain sample pairs, effectively capturing subtle lesion variations under class imbalance. Extensive evaluations on four diverse medical image segmentation datasets (ISLES'22, BraTS'19, LA, Pancreas) show DyCON's superior performance against SOTA methods.</li>
</ul>

<h3>Title: Multimodal Lengthy Videos Retrieval Framework and Evaluation Metric</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Eltahir, Osamah Sarraj, Mohammed Bremoo, Mohammed Khurd, Abdulrahman Alfrihidi, Taha Alshatiri, Mohammad Almatrafi, Tanveer Hussain</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04572">https://arxiv.org/abs/2504.04572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04572">https://arxiv.org/pdf/2504.04572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04572]] Multimodal Lengthy Videos Retrieval Framework and Evaluation Metric(https://arxiv.org/abs/2504.04572)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Precise video retrieval requires multi-modal correlations to handle unseen vocabulary and scenes, becoming more complex for lengthy videos where models must perform effectively without prior training on a specific dataset. We introduce a unified framework that combines a visual matching stream and an aural matching stream with a unique subtitles-based video segmentation approach. Additionally, the aural stream includes a complementary audio-based two-stage retrieval mechanism that enhances performance on long-duration videos. Considering the complex nature of retrieval from lengthy videos and its corresponding evaluation, we introduce a new retrieval evaluation method specifically designed for long-video retrieval to support further research. We conducted experiments on the YouCook2 benchmark, showing promising retrieval performance.</li>
</ul>

<h3>Title: Your Image Generator Is Your New Private Dataset</h3>
<ul>
<li><strong>Authors: </strong>Nicolo Resmini, Eugenio Lomurno, Cristian Sbrolli, Matteo Matteucci</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04582">https://arxiv.org/abs/2504.04582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04582">https://arxiv.org/pdf/2504.04582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04582]] Your Image Generator Is Your New Private Dataset(https://arxiv.org/abs/2504.04582)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, robust, membership infer, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative diffusion models have emerged as powerful tools to synthetically produce training data, offering potential solutions to data scarcity and reducing labelling costs for downstream supervised deep learning applications. However, effectively leveraging text-conditioned image generation for building classifier training sets requires addressing key issues: constructing informative textual prompts, adapting generative models to specific domains, and ensuring robust performance. This paper proposes the Text-Conditioned Knowledge Recycling (TCKR) pipeline to tackle these challenges. TCKR combines dynamic image captioning, parameter-efficient diffusion model fine-tuning, and Generative Knowledge Distillation techniques to create synthetic datasets tailored for image classification. The pipeline is rigorously evaluated on ten diverse image classification benchmarks. The results demonstrate that models trained solely on TCKR-generated data achieve classification accuracies on par with (and in several cases exceeding) models trained on real images. Furthermore, the evaluation reveals that these synthetic-data-trained models exhibit substantially enhanced privacy characteristics: their vulnerability to Membership Inference Attacks is significantly reduced, with the membership inference AUC lowered by 5.49 points on average compared to using real training data, demonstrating a substantial improvement in the performance-privacy trade-off. These findings indicate that high-fidelity synthetic data can effectively replace real data for training classifiers, yielding strong performance whilst simultaneously providing improved privacy protection as a valuable emergent property. The code and trained models are available in the accompanying open-source repository.</li>
</ul>

<h3>Title: Targetless LiDAR-Camera Calibration with Anchored 3D Gaussians</h3>
<ul>
<li><strong>Authors: </strong>Haebeom Jung, Namtae Kim, Jungwoo Kim, Jaesik Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04597">https://arxiv.org/abs/2504.04597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04597">https://arxiv.org/pdf/2504.04597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04597]] Targetless LiDAR-Camera Calibration with Anchored 3D Gaussians(https://arxiv.org/abs/2504.04597)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a targetless LiDAR-camera calibration method that jointly optimizes sensor poses and scene geometry from arbitrary scenes, without relying on traditional calibration targets such as checkerboards or spherical reflectors. Our approach leverages a 3D Gaussian-based scene representation. We first freeze reliable LiDAR points as anchors, then jointly optimize the poses and auxiliary Gaussian parameters in a fully differentiable manner using a photometric loss. This joint optimization significantly reduces sensor misalignment, resulting in higher rendering quality and consistently improved PSNR compared to the carefully calibrated poses provided in popular datasets. We validate our method through extensive experiments on two real-world autonomous driving datasets, KITTI-360 and Waymo, each featuring distinct sensor configurations. Additionally, we demonstrate the robustness of our approach using a custom LiDAR-camera setup, confirming strong performance across diverse hardware configurations.</li>
</ul>

<h3>Title: Exact Unlearning of Finetuning Data via Model Merging at Scale</h3>
<ul>
<li><strong>Authors: </strong>Kevin Kuo, Amrith Setlur, Kartik Srinivas, Aditi Raghunathan, Virginia Smith</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04626">https://arxiv.org/abs/2504.04626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04626">https://arxiv.org/pdf/2504.04626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04626]] Exact Unlearning of Finetuning Data via Model Merging at Scale(https://arxiv.org/abs/2504.04626)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Approximate unlearning has gained popularity as an approach to efficiently update an LLM so that it behaves (roughly) as if it was not trained on a subset of data to begin with. However, existing methods are brittle in practice and can easily be attacked to reveal supposedly unlearned information. To alleviate issues with approximate unlearning, we instead propose SIFT-Masks (SIgn-Fixed Tuning-Masks), an exact unlearning method based on model merging. SIFT-Masks addresses two key limitations of standard model merging: (1) merging a large number of tasks can severely harm utility; and (2) methods that boost utility by sharing extra information across tasks make exact unlearning prohibitively expensive. SIFT-Masks solves these issues by (1) applying local masks to recover task-specific performance; and (2) constraining finetuning to align with a global sign vector as a lightweight approach to determine masks independently before merging. Across four settings where we merge up to 500 models, SIFT-Masks improves accuracy by 5-80% over naive merging and uses up to 250x less compute for exact unlearning compared to other merging baselines.</li>
</ul>

<h3>Title: Systematic Literature Review on Vehicular Collaborative Perception -- A Computer Vision Perspective</h3>
<ul>
<li><strong>Authors: </strong>Lei Wan, Jianxin Zhao, Andreas Wiedholz, Manuel Bied, Mateus Martinez de Lucena, Abhishek Dinkar Jagtap, Andreas Festag, Antônio Augusto Fröhlich, Hannan Ejaz Keen, Alexey Vinel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04631">https://arxiv.org/abs/2504.04631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04631">https://arxiv.org/pdf/2504.04631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04631]] Systematic Literature Review on Vehicular Collaborative Perception -- A Computer Vision Perspective(https://arxiv.org/abs/2504.04631)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>The effectiveness of autonomous vehicles relies on reliable perception capabilities. Despite significant advancements in artificial intelligence and sensor fusion technologies, current single-vehicle perception systems continue to encounter limitations, notably visual occlusions and limited long-range detection capabilities. Collaborative Perception (CP), enabled by Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication, has emerged as a promising solution to mitigate these issues and enhance the reliability of autonomous systems. Beyond advancements in communication, the computer vision community is increasingly focusing on improving vehicular perception through collaborative approaches. However, a systematic literature review that thoroughly examines existing work and reduces subjective bias is still lacking. Such a systematic approach helps identify research gaps, recognize common trends across studies, and inform future research directions. In response, this study follows the PRISMA 2020 guidelines and includes 106 peer-reviewed articles. These publications are analyzed based on modalities, collaboration schemes, and key perception tasks. Through a comparative analysis, this review illustrates how different methods address practical issues such as pose errors, temporal latency, communication constraints, domain shifts, heterogeneity, and adversarial attacks. Furthermore, it critically examines evaluation methodologies, highlighting a misalignment between current metrics and CP's fundamental objectives. By delving into all relevant topics in-depth, this review offers valuable insights into challenges, opportunities, and risks, serving as a reference for advancing research in vehicular collaborative perception.</li>
</ul>

<h3>Title: M2IV: Towards Efficient and Fine-grained Multimodal In-Context Learning in Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yanshu Li, Hongyang He, Yi Cao, Qisen Cheng, Xiang Fu, Ruixiang Tang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04633">https://arxiv.org/abs/2504.04633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04633">https://arxiv.org/pdf/2504.04633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04633]] M2IV: Towards Efficient and Fine-grained Multimodal In-Context Learning in Large Vision-Language Models(https://arxiv.org/abs/2504.04633)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal in-context learning (ICL) is a vital capability for Large Vision-Language Models (LVLMs), allowing task adaptation via contextual prompts without parameter retraining. However, its application is hindered by the token-intensive nature of inputs and the high complexity of cross-modal few-shot learning, which limits the expressive power of representation methods. To tackle these challenges, we propose \textbf{M2IV}, a method that substitutes explicit demonstrations with learnable \textbf{I}n-context \textbf{V}ectors directly integrated into LVLMs. By exploiting the complementary strengths of multi-head attention (\textbf{M}HA) and multi-layer perceptrons (\textbf{M}LP), M2IV achieves robust cross-modal fidelity and fine-grained semantic distillation through training. This significantly enhances performance across diverse LVLMs and tasks and scales efficiently to many-shot scenarios, bypassing the context window limitations. We also introduce \textbf{VLibrary}, a repository for storing and retrieving M2IV, enabling flexible LVLM steering for tasks like cross-modal alignment, customized generation and safety improvement. Experiments across seven benchmarks and three LVLMs show that M2IV surpasses Vanilla ICL and prior representation engineering approaches, with an average accuracy gain of \textbf{3.74\%} over ICL with the same shot count, alongside substantial efficiency advantages.</li>
</ul>

<h3>Title: Steering off Course: Reliability Challenges in Steering Language Models</h3>
<ul>
<li><strong>Authors: </strong>Patrick Queiroz Da Silva, Hari Sethuraman, Dheeraj Rajagopal, Hannaneh Hajishirzi, Sachin Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04635">https://arxiv.org/abs/2504.04635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04635">https://arxiv.org/pdf/2504.04635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04635]] Steering off Course: Reliability Challenges in Steering Language Models(https://arxiv.org/abs/2504.04635)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Steering methods for language models (LMs) have gained traction as lightweight alternatives to fine-tuning, enabling targeted modifications to model activations. However, prior studies primarily report results on a few models, leaving critical gaps in understanding the robustness of these methods. In this work, we systematically examine three prominent steering methods -- DoLa, function vectors, and task vectors. In contrast to the original studies, which evaluated a handful of models, we test up to 36 models belonging to 14 families with sizes ranging from 1.5B to 70B parameters. Our experiments reveal substantial variability in the effectiveness of the steering approaches, with a large number of models showing no improvement and at times degradation in steering performance. Our analysis demonstrate fundamental flaws in the assumptions underlying these methods, challenging their reliability as scalable steering solutions.</li>
</ul>

<h3>Title: Splits! A Flexible Dataset for Evaluating a Model's Demographic Social Inference</h3>
<ul>
<li><strong>Authors: </strong>Eylon Caplan, Tania Chakraborty, Dan Goldwasser</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04640">https://arxiv.org/abs/2504.04640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04640">https://arxiv.org/pdf/2504.04640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04640]] Splits! A Flexible Dataset for Evaluating a Model's Demographic Social Inference(https://arxiv.org/abs/2504.04640)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding how people of various demographics think, feel, and express themselves (collectively called group expression) is essential for social science and underlies the assessment of bias in Large Language Models (LLMs). While LLMs can effectively summarize group expression when provided with empirical examples, coming up with generalizable theories of how a group's expression manifests in real-world text is challenging. In this paper, we define a new task called Group Theorization, in which a system must write theories that differentiate expression across demographic groups. We make available a large dataset on this task, Splits!, constructed by splitting Reddit posts by neutral topics (e.g. sports, cooking, and movies) and by demographics (e.g. occupation, religion, and race). Finally, we suggest a simple evaluation framework for assessing how effectively a method can generate 'better' theories about group expression, backed by human validation. We publicly release the raw corpora and evaluation scripts for Splits! to help researchers assess how methods infer--and potentially misrepresent--group differences in expression. We make Splits! and our evaluation module available at this https URL.</li>
</ul>

<h3>Title: LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts</h3>
<ul>
<li><strong>Authors: </strong>Yimu Wang, Mozhgan Nasr Azadani, Sean Sedwards, Krzysztof Czarnecki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04653">https://arxiv.org/abs/2504.04653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04653">https://arxiv.org/pdf/2504.04653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04653]] LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts(https://arxiv.org/abs/2504.04653)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Redundancy of visual tokens in multi-modal large language models (MLLMs) significantly reduces their computational efficiency. Recent approaches, such as resamplers and summarizers, have sought to reduce the number of visual tokens, but at the cost of visual reasoning ability. To address this, we propose LEO-MINI, a novel MLLM that significantly reduces the number of visual tokens and simultaneously boosts visual reasoning capabilities. For efficiency, LEO-MINI incorporates CoTR, a novel token reduction module to consolidate a large number of visual tokens into a smaller set of tokens, using the similarity between visual tokens, text tokens, and a compact learnable query. For effectiveness, to scale up the model's ability with minimal computational overhead, LEO-MINI employs MMoE, a novel mixture of multi-modal experts module. MMOE employs a set of LoRA experts with a novel router to switch between them based on the input text and visual tokens instead of only using the input hidden state. MMoE also includes a general LoRA expert that is always activated to learn general knowledge for LLM reasoning. For extracting richer visual features, MMOE employs a set of vision experts trained on diverse domain-specific data. To demonstrate LEO-MINI's improved efficiency and performance, we evaluate it against existing efficient MLLMs on various benchmark vision-language tasks.</li>
</ul>

<h3>Title: ACE-RLHF: Automated Code Evaluation and Socratic Feedback Generation Tool using Large Language Models and Reinforcement Learning with Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Tasnia Rahman, Sathish A. P. Kumar, Sumit Jha, Arvind Ramanathan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04657">https://arxiv.org/abs/2504.04657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04657">https://arxiv.org/pdf/2504.04657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04657]] ACE-RLHF: Automated Code Evaluation and Socratic Feedback Generation Tool using Large Language Models and Reinforcement Learning with Human Feedback(https://arxiv.org/abs/2504.04657)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automated Program Repair tools are developed for generating feedback and suggesting a repair method for erroneous code. State of the art (SOTA) code repair methods rely on data-driven approaches and often fail to deliver solution for complicated programming questions. To interpret the natural language of unprecedented programming problems, using Large Language Models (LLMs) for code-feedback generation is crucial. LLMs generate more comprehensible feedback than compiler-generated error messages, and Reinforcement Learning with Human Feedback (RLHF) further enhances quality by integrating human-in-the-loop which helps novice students to lean programming from scratch interactively. We are applying RLHF fine-tuning technique for an expected Socratic response such as a question with hint to solve the programming issue. We are proposing code feedback generation tool by fine-tuning LLM with RLHF, Automated Code Evaluation with RLHF (ACE-RLHF), combining two open-source LLM models with two different SOTA optimization techniques. The quality of feedback is evaluated on two benchmark datasets containing basic and competition-level programming questions where the later is proposed by us. We achieved 2-5% higher accuracy than RL-free SOTA techniques using Llama-3-7B-Proximal-policy optimization in automated evaluation and similar or slightly higher accuracy compared to reward model-free RL with AI Feedback (RLAIF). We achieved almost 40% higher accuracy with GPT-3.5 Best-of-n optimization while performing manual evaluation.</li>
</ul>

<h3>Title: A Simultaneous Approach for Training Neural Differential-Algebraic Systems of Equations</h3>
<ul>
<li><strong>Authors: </strong>Laurens R. Lueg, Victor Alves, Daniel Schicksnus, John R. Kitchin, Carl D. Laird, Lorenz T. Biegler</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04665">https://arxiv.org/abs/2504.04665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04665">https://arxiv.org/pdf/2504.04665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04665]] A Simultaneous Approach for Training Neural Differential-Algebraic Systems of Equations(https://arxiv.org/abs/2504.04665)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Scientific machine learning is an emerging field that broadly describes the combination of scientific computing and machine learning to address challenges in science and engineering. Within the context of differential equations, this has produced highly influential methods, such as neural ordinary differential equations (NODEs). Recent works extend this line of research to consider neural differential-algebraic systems of equations (DAEs), where some unknown relationships within the DAE are learned from data. Training neural DAEs, similarly to neural ODEs, is computationally expensive, as it requires the solution of a DAE for every parameter update. Further, the rigorous consideration of algebraic constraints is difficult within common deep learning training algorithms such as stochastic gradient descent. In this work, we apply the simultaneous approach to neural DAE problems, resulting in a fully discretized nonlinear optimization problem, which is solved to local optimality and simultaneously obtains the neural network parameters and the solution to the corresponding DAE. We extend recent work demonstrating the simultaneous approach for neural ODEs, by presenting a general framework to solve neural DAEs, with explicit consideration of hybrid models, where some components of the DAE are known, e.g. physics-informed constraints. Furthermore, we present a general strategy for improving the performance and convergence of the nonlinear programming solver, based on solving an auxiliary problem for initialization and approximating Hessian terms. We achieve promising results in terms of accuracy, model generalizability and computational cost, across different problem settings such as sparse data, unobserved states and multiple trajectories. Lastly, we provide several promising future directions to improve the scalability and robustness of our approach.</li>
</ul>

<h3>Title: DeclutterNeRF: Generative-Free 3D Scene Recovery for Occlusion Removal</h3>
<ul>
<li><strong>Authors: </strong>Wanzhou Liu, Zhexiao Xiong, Xinyu Li, Nathan Jacobs</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04679">https://arxiv.org/abs/2504.04679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04679">https://arxiv.org/pdf/2504.04679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04679]] DeclutterNeRF: Generative-Free 3D Scene Recovery for Occlusion Removal(https://arxiv.org/abs/2504.04679)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Recent novel view synthesis (NVS) techniques, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly advanced 3D scene reconstruction with high-quality rendering and realistic detail recovery. Effectively removing occlusions while preserving scene details can further enhance the robustness and applicability of these techniques. However, existing approaches for object and occlusion removal predominantly rely on generative priors, which, despite filling the resulting holes, introduce new artifacts and blurriness. Moreover, existing benchmark datasets for evaluating occlusion removal methods lack realistic complexity and viewpoint variations. To address these issues, we introduce DeclutterSet, a novel dataset featuring diverse scenes with pronounced occlusions distributed across foreground, midground, and background, exhibiting substantial relative motion across viewpoints. We further introduce DeclutterNeRF, an occlusion removal method free from generative priors. DeclutterNeRF introduces joint multi-view optimization of learnable camera parameters, occlusion annealing regularization, and employs an explainable stochastic structural similarity loss, ensuring high-quality, artifact-free reconstructions from incomplete images. Experiments demonstrate that DeclutterNeRF significantly outperforms state-of-the-art methods on our proposed DeclutterSet, establishing a strong baseline for future research.</li>
</ul>

<h3>Title: Generative Large Language Model usage in Smart Contract Vulnerability Detection</h3>
<ul>
<li><strong>Authors: </strong>Peter Ince, Jiangshan Yu, Joseph K. Liu, Xiaoning Du</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04685">https://arxiv.org/abs/2504.04685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04685">https://arxiv.org/pdf/2504.04685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04685]] Generative Large Language Model usage in Smart Contract Vulnerability Detection(https://arxiv.org/abs/2504.04685)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent years have seen an explosion of activity in Generative AI, specifically Large Language Models (LLMs), revolutionising applications across various fields. Smart contract vulnerability detection is no exception; as smart contracts exist on public chains and can have billions of dollars transacted daily, continuous improvement in vulnerability detection is crucial. This has led to many researchers investigating the usage of generative large language models (LLMs) to aid in detecting vulnerabilities in smart contracts. This paper presents a systematic review of the current LLM-based smart contract vulnerability detection tools, comparing them against traditional static and dynamic analysis tools Slither and Mythril. Our analysis highlights key areas where each performs better and shows that while these tools show promise, the LLM-based tools available for testing are not ready to replace more traditional tools. We conclude with recommendations on how LLMs are best used in the vulnerability detection process and offer insights for improving on the state-of-the-art via hybrid approaches and targeted pre-training of much smaller models.</li>
</ul>

<h3>Title: Bridging Knowledge Gap Between Image Inpainting and Large-Area Visible Watermark Removal</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Leng, Chaowei Fang, Junye Chen, Yixiang Fang, Sheng Li, Guanbin Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04687">https://arxiv.org/abs/2504.04687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04687">https://arxiv.org/pdf/2504.04687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04687]] Bridging Knowledge Gap Between Image Inpainting and Large-Area Visible Watermark Removal(https://arxiv.org/abs/2504.04687)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark</a></li>
<li><strong>Abstract: </strong>Visible watermark removal which involves watermark cleaning and background content restoration is pivotal to evaluate the resilience of watermarks. Existing deep neural network (DNN)-based models still struggle with large-area watermarks and are overly dependent on the quality of watermark mask prediction. To overcome these challenges, we introduce a novel feature adapting framework that leverages the representation modeling capacity of a pre-trained image inpainting model. Our approach bridges the knowledge gap between image inpainting and watermark removal by fusing information of the residual background content beneath watermarks into the inpainting backbone model. We establish a dual-branch system to capture and embed features from the residual background content, which are merged into intermediate features of the inpainting backbone model via gated feature fusion modules. Moreover, for relieving the dependence on high-quality watermark masks, we introduce a new training paradigm by utilizing coarse watermark masks to guide the inference process. This contributes to a visible image removal model which is insensitive to the quality of watermark mask during testing. Extensive experiments on both a large-scale synthesized dataset and a real-world dataset demonstrate that our approach significantly outperforms existing state-of-the-art methods. The source code is available in the supplementary materials.</li>
</ul>

<h3>Title: scAgent: Universal Single-Cell Annotation via a LLM Agent</h3>
<ul>
<li><strong>Authors: </strong>Yuren Mao, Yu Mi, Peigen Liu, Mengfei Zhang, Hanqing Liu, Yunjun Gao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04698">https://arxiv.org/abs/2504.04698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04698">https://arxiv.org/pdf/2504.04698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04698]] scAgent: Universal Single-Cell Annotation via a LLM Agent(https://arxiv.org/abs/2504.04698)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Cell type annotation is critical for understanding cellular heterogeneity. Based on single-cell RNA-seq data and deep learning models, good progress has been made in annotating a fixed number of cell types within a specific tissue. However, universal cell annotation, which can generalize across tissues, discover novel cell types, and extend to novel cell types, remains less explored. To fill this gap, this paper proposes scAgent, a universal cell annotation framework based on Large Language Models (LLMs). scAgent can identify cell types and discover novel cell types in diverse tissues; furthermore, it is data efficient to learn novel cell types. Experimental studies in 160 cell types and 35 tissues demonstrate the superior performance of scAgent in general cell-type annotation, novel cell discovery, and extensibility to novel cell type.</li>
</ul>

<h3>Title: Causal Retrieval with Semantic Consideration</h3>
<ul>
<li><strong>Authors: </strong>Hyunseo Shin, Wonseok Hwang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04700">https://arxiv.org/abs/2504.04700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04700">https://arxiv.org/pdf/2504.04700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04700]] Causal Retrieval with Semantic Consideration(https://arxiv.org/abs/2504.04700)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have significantly enhanced the performance of conversational AI systems. To extend their capabilities to knowledge-intensive domains such as biomedical and legal fields, where the accuracy is critical, LLMs are often combined with information retrieval (IR) systems to generate responses based on retrieved documents. However, for IR systems to effectively support such applications, they must go beyond simple semantic matching and accurately capture diverse query intents, including causal relationships. Existing IR models primarily focus on retrieving documents based on surface-level semantic similarity, overlooking deeper relational structures such as causality. To address this, we propose CAWAI, a retrieval model that is trained with dual objectives: semantic and causal relations. Our extensive experiments demonstrate that CAWAI outperforms various models on diverse causal retrieval tasks especially under large-scale retrieval settings. We also show that CAWAI exhibits strong zero-shot generalization across scientific domain QA tasks.</li>
</ul>

<h3>Title: DFormerv2: Geometry Self-Attention for RGBD Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Bo-Wen Yin, Jiao-Long Cao, Ming-Ming Cheng, Qibin Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04701">https://arxiv.org/abs/2504.04701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04701">https://arxiv.org/pdf/2504.04701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04701]] DFormerv2: Geometry Self-Attention for RGBD Semantic Segmentation(https://arxiv.org/abs/2504.04701)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Recent advances in scene understanding benefit a lot from depth maps because of the 3D geometry information, especially in complex conditions (e.g., low light and overexposed). Existing approaches encode depth maps along with RGB images and perform feature fusion between them to enable more robust predictions. Taking into account that depth can be regarded as a geometry supplement for RGB images, a straightforward question arises: Do we really need to explicitly encode depth information with neural networks as done for RGB images? Based on this insight, in this paper, we investigate a new way to learn RGBD feature representations and present DFormerv2, a strong RGBD encoder that explicitly uses depth maps as geometry priors rather than encoding depth information with neural networks. Our goal is to extract the geometry clues from the depth and spatial distances among all the image patch tokens, which will then be used as geometry priors to allocate attention weights in self-attention. Extensive experiments demonstrate that DFormerv2 exhibits exceptional performance in various RGBD semantic segmentation benchmarks. Code is available at: this https URL.</li>
</ul>

<h3>Title: Provable Failure of Language Models in Learning Majority Boolean Logic via Gradient Descent</h3>
<ul>
<li><strong>Authors: </strong>Bo Chen, Zhenmei Shi, Zhao Song, Jiahao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04702">https://arxiv.org/abs/2504.04702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04702">https://arxiv.org/pdf/2504.04702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04702]] Provable Failure of Language Models in Learning Majority Boolean Logic via Gradient Descent(https://arxiv.org/abs/2504.04702)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advancements in Transformer-based architectures have led to impressive breakthroughs in natural language processing tasks, with models such as GPT-4, Claude, and Gemini demonstrating human-level reasoning abilities. However, despite their high performance, concerns remain about the inherent limitations of these models, especially when it comes to learning basic logical functions. While complexity-theoretic analyses indicate that Transformers can represent simple logic functions (e.g., $\mathsf{AND}$, $\mathsf{OR}$, and majority gates) by its nature of belonging to the $\mathsf{TC}^0$ class, these results assume ideal parameter settings and do not account for the constraints imposed by gradient descent-based training methods. In this work, we investigate whether Transformers can truly learn simple majority functions when trained using gradient-based methods. We focus on a simplified variant of the Transformer architecture and consider both $n=\mathrm{poly}(d)$ and $n=\exp(\Omega(d))$ number of training samples, where each sample is a $d$-size binary string paired with the output of a basic majority function. Our analysis demonstrates that even after $\mathrm{poly}(d)$ gradient queries, the generalization error of the Transformer model still remains substantially large, growing exponentially with $d$. This work highlights fundamental optimization challenges in training Transformers for the simplest logical reasoning tasks and provides new insights into their theoretical limitations.</li>
</ul>

<h3>Title: LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important</h3>
<ul>
<li><strong>Authors: </strong>Manlai Liang, JiaMing Zhang, Xiong Li, Jinlong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04704">https://arxiv.org/abs/2504.04704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04704">https://arxiv.org/pdf/2504.04704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04704]] LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important(https://arxiv.org/abs/2504.04704)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The increasing size of the Key-Value (KV) cache during the Large Language Models long-context inference is the main obstacle for its balance between the deployment cost and task accuracy. To reduce the KV cache size in such scenarios, most previous efforts leveraged on the attention weight to evict non-critical cache tokens. But there is a trade-off in those methods, they usually require major modifiation of the inference infrastructure and significant computation overhead. Base on the fact that the Large Lanuage models are autoregresssive models, we propose {\it LagKV}, a KV allocation strategy only relying on straight forward comparison among KV themself. It is a totally attention free method which offers easy integration to the main stream inference platform and comparable performance comparing to other complicated KV compression methods. Results on LongBench and PasskeyRetrieval show that, our approach achieves nearly zero loss when the ratio is $2\times$ and $\approx 90\%$ of the original model performance for $8\times$. Especially in the 64-digit passkey retrieval task, our mehod outperforms the attention weight based method $H_2O$ over $60\%$ with same compression ratios. Our code is available at \url{this https URL}.</li>
</ul>

<h3>Title: SapiensID: Foundation for Human Recognition</h3>
<ul>
<li><strong>Authors: </strong>Minchul Kim, Dingqiang Ye, Yiyang Su, Feng Liu, Xiaoming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04708">https://arxiv.org/abs/2504.04708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04708">https://arxiv.org/pdf/2504.04708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04708]] SapiensID: Foundation for Human Recognition(https://arxiv.org/abs/2504.04708)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing human recognition systems often rely on separate, specialized models for face and body analysis, limiting their effectiveness in real-world scenarios where pose, visibility, and context vary widely. This paper introduces SapiensID, a unified model that bridges this gap, achieving robust performance across diverse settings. SapiensID introduces (i) Retina Patch (RP), a dynamic patch generation scheme that adapts to subject scale and ensures consistent tokenization of regions of interest, (ii) a masked recognition model (MRM) that learns from variable token length, and (iii) Semantic Attention Head (SAH), an module that learns pose-invariant representations by pooling features around key body parts. To facilitate training, we introduce WebBody4M, a large-scale dataset capturing diverse poses and scale variations. Extensive experiments demonstrate that SapiensID achieves state-of-the-art results on various body ReID benchmarks, outperforming specialized models in both short-term and long-term scenarios while remaining competitive with dedicated face recognition systems. Furthermore, SapiensID establishes a strong baseline for the newly introduced challenge of Cross Pose-Scale ReID, demonstrating its ability to generalize to complex, real-world conditions.</li>
</ul>

<h3>Title: Sequential-NIAH: A Needle-In-A-Haystack Benchmark for Extracting Sequential Needles from Long Contexts</h3>
<ul>
<li><strong>Authors: </strong>Yifei Yu, Qian-Wen Zhang, Lingfeng Qiao, Di Yin, Fang Li, Jie Wang, Zengxi Chen, Suncong Zheng, Xiaolong Liang, Xing Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04713">https://arxiv.org/abs/2504.04713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04713">https://arxiv.org/pdf/2504.04713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04713]] Sequential-NIAH: A Needle-In-A-Haystack Benchmark for Extracting Sequential Needles from Long Contexts(https://arxiv.org/abs/2504.04713)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Evaluating the ability of large language models (LLMs) to handle extended contexts is critical, particularly for retrieving information relevant to specific queries embedded within lengthy inputs. We introduce Sequential-NIAH, a benchmark specifically designed to evaluate the capability of LLMs to extract sequential information items (known as needles) from long contexts. The benchmark comprises three types of needle generation pipelines: synthetic, real, and open-domain QA. It includes contexts ranging from 8K to 128K tokens in length, with a dataset of 14,000 samples (2,000 reserved for testing). To facilitate evaluation on this benchmark, we trained a synthetic data-driven evaluation model capable of evaluating answer correctness based on chronological or logical order, achieving an accuracy of 99.49% on synthetic test data. We conducted experiments on six well-known LLMs, revealing that even the best-performing model achieved a maximum accuracy of only 63.15%. Further analysis highlights the growing challenges posed by increasing context lengths and the number of needles, underscoring substantial room for improvement. Additionally, noise robustness experiments validate the reliability of the benchmark, making Sequential-NIAH an important reference for advancing research on long text extraction capabilities of LLMs.</li>
</ul>

<h3>Title: Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs</h3>
<ul>
<li><strong>Authors: </strong>Will Cai, Tianneng Shi, Xuandong Zhao, Dawn Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04715">https://arxiv.org/abs/2504.04715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04715">https://arxiv.org/pdf/2504.04715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04715]] Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs(https://arxiv.org/abs/2504.04715)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, fair, large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of Large Language Models (LLMs) accessed via black-box APIs introduces a significant trust challenge: users pay for services based on advertised model capabilities (e.g., size, performance), but providers may covertly substitute the specified model with a cheaper, lower-quality alternative to reduce operational costs. This lack of transparency undermines fairness, erodes trust, and complicates reliable benchmarking. Detecting such substitutions is difficult due to the black-box nature, typically limiting interaction to input-output queries. This paper formalizes the problem of model substitution detection in LLM APIs. We systematically evaluate existing verification techniques, including output-based statistical tests, benchmark evaluations, and log probability analysis, under various realistic attack scenarios like model quantization, randomized substitution, and benchmark evasion. Our findings reveal the limitations of methods relying solely on text outputs, especially against subtle or adaptive attacks. While log probability analysis offers stronger guarantees when available, its accessibility is often limited. We conclude by discussing the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as a pathway towards provable model integrity, highlighting the trade-offs between security, performance, and provider adoption. Code is available at this https URL</li>
</ul>

<h3>Title: On the Robustness of GUI Grounding Models Against Image Attacks</h3>
<ul>
<li><strong>Authors: </strong>Haoren Zhao, Tianyi Chen, Zhen Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04716">https://arxiv.org/abs/2504.04716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04716">https://arxiv.org/pdf/2504.04716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04716]] On the Robustness of GUI Grounding Models Against Image Attacks(https://arxiv.org/abs/2504.04716)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Graphical User Interface (GUI) grounding models are crucial for enabling intelligent agents to understand and interact with complex visual interfaces. However, these models face significant robustness challenges in real-world scenarios due to natural noise and adversarial perturbations, and their robustness remains underexplored. In this study, we systematically evaluate the robustness of state-of-the-art GUI grounding models, such as UGround, under three conditions: natural noise, untargeted adversarial attacks, and targeted adversarial attacks. Our experiments, which were conducted across a wide range of GUI environments, including mobile, desktop, and web interfaces, have clearly demonstrated that GUI grounding models exhibit a high degree of sensitivity to adversarial perturbations and low-resolution conditions. These findings provide valuable insights into the vulnerabilities of GUI grounding models and establish a strong benchmark for future research aimed at enhancing their robustness in practical applications. Our code is available at this https URL.</li>
</ul>

<h3>Title: Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yubo Li, Xiaobin Shen, Xinyu Yao, Xueying Ding, Yidi Miao, Ramayya Krishnan, Rema Padman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04717">https://arxiv.org/abs/2504.04717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04717">https://arxiv.org/pdf/2504.04717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04717]] Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models(https://arxiv.org/abs/2504.04717)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have revolutionized their ability to handle single-turn tasks, yet real-world applications demand sophisticated multi-turn interactions. This survey provides a comprehensive review of recent advancements in evaluating and enhancing multi-turn interactions in LLMs. Focusing on task-specific scenarios, from instruction following in diverse domains such as math and coding to complex conversational engagements in roleplay, healthcare, education, and even adversarial jailbreak settings, we systematically examine the challenges of maintaining context, coherence, fairness, and responsiveness over prolonged dialogues. The paper organizes current benchmarks and datasets into coherent categories that reflect the evolving landscape of multi-turn dialogue evaluation. In addition, we review a range of enhancement methodologies under multi-turn settings, including model-centric strategies (contextual learning, supervised fine-tuning, reinforcement learning, and new architectures), external integration approaches (memory-augmented, retrieval-based methods, and knowledge graph), and agent-based techniques for collaborative interactions. Finally, we discuss open challenges and propose future directions for research to further advance the robustness and effectiveness of multi-turn interactions in LLMs. Related resources and papers are available at this https URL.</li>
</ul>

<h3>Title: TactileNet: Bridging the Accessibility Gap with AI-Generated Tactile Graphics for Individuals with Vision Impairment</h3>
<ul>
<li><strong>Authors: </strong>Adnan Khan, Alireza Choubineh, Mai A. Shaaban, Abbas Akkasi, Majid Komeili</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04722">https://arxiv.org/abs/2504.04722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04722">https://arxiv.org/pdf/2504.04722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04722]] TactileNet: Bridging the Accessibility Gap with AI-Generated Tactile Graphics for Individuals with Vision Impairment(https://arxiv.org/abs/2504.04722)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Tactile graphics are essential for providing access to visual information for the 43 million people globally living with vision loss, as estimated by global prevalence data. However, traditional methods for creating these tactile graphics are labor-intensive and struggle to meet demand. We introduce TactileNet, the first comprehensive dataset and AI-driven framework for generating tactile graphics using text-to-image Stable Diffusion (SD) models. By integrating Low-Rank Adaptation (LoRA) and DreamBooth, our method fine-tunes SD models to produce high-fidelity, guideline-compliant tactile graphics while reducing computational costs. Evaluations involving tactile experts show that generated graphics achieve 92.86% adherence to tactile standards and 100% alignment with natural images in posture and features. Our framework also demonstrates scalability, generating 32,000 images (7,050 filtered for quality) across 66 classes, with prompt editing enabling customizable outputs (e.g., adding/removing details). Our work empowers designers to focus on refinement, significantly accelerating accessibility efforts. It underscores the transformative potential of AI for social good, offering a scalable solution to bridge the accessibility gap in education and beyond.</li>
</ul>

<h3>Title: A High-Performance Curve25519 and Curve448 Unified Elliptic Curve Cryptography Accelerator</h3>
<ul>
<li><strong>Authors: </strong>Aniket Banerjee, Utsav Banerjee</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04731">https://arxiv.org/abs/2504.04731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04731">https://arxiv.org/pdf/2504.04731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04731]] A High-Performance Curve25519 and Curve448 Unified Elliptic Curve Cryptography Accelerator(https://arxiv.org/abs/2504.04731)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>In modern critical infrastructure such as power grids, it is crucial to ensure security of data communications between network-connected devices while following strict latency criteria. This necessitates the use of cryptographic hardware accelerators. We propose a high-performance unified elliptic curve cryptography accelerator supporting NIST standard Montgomery curves Curve25519 and Curve448 at 128-bit and 224-bit security levels respectively. Our accelerator implements extensive parallel processing of Karatsuba-style large-integer multiplications, restructures arithmetic operations in the Montgomery Ladder and exploits special mathematical properties of the underlying pseudo-Mersenne and Solinas prime fields for optimized performance. Our design ensures efficient resource sharing across both curve computations and also incorporates several standard side-channel countermeasures. Our ASIC implementation achieves record performance and energy of 10.38 $\mu$s / 54.01 $\mu$s and 0.72 $\mu$J / 3.73 $\mu$J respectively for Curve25519 / Curve448, which is significantly better than state-of-the-art.</li>
</ul>

<h3>Title: TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context</h3>
<ul>
<li><strong>Authors: </strong>Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Shivam Mishra, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04737">https://arxiv.org/abs/2504.04737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04737">https://arxiv.org/pdf/2504.04737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04737]] TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context(https://arxiv.org/abs/2504.04737)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>In the landscape of Fact-based Judgment Prediction and Explanation (FJPE), reliance on factual data is essential for developing robust and realistic AI-driven decision-making tools. This paper introduces TathyaNyaya, the largest annotated dataset for FJPE tailored to the Indian legal context, encompassing judgments from the Supreme Court of India and various High Courts. Derived from the Hindi terms "Tathya" (fact) and "Nyaya" (justice), the TathyaNyaya dataset is uniquely designed to focus on factual statements rather than complete legal texts, reflecting real-world judicial processes where factual data drives outcomes. Complementing this dataset, we present FactLegalLlama, an instruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM), optimized for generating high-quality explanations in FJPE tasks. Finetuned on the factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy with coherent, contextually relevant explanations, addressing the critical need for transparency and interpretability in AI-assisted legal systems. Our methodology combines transformers for binary judgment prediction with FactLegalLlama for explanation generation, creating a robust framework for advancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses existing datasets in scale and diversity but also establishes a benchmark for building explainable AI systems in legal analysis. The findings underscore the importance of factual precision and domain-specific tuning in enhancing predictive performance and interpretability, positioning TathyaNyaya and FactLegalLlama as foundational resources for AI-assisted legal decision-making.</li>
</ul>

<h3>Title: Enhancing Compositional Reasoning in Vision-Language Models with Synthetic Preference Data</h3>
<ul>
<li><strong>Authors: </strong>Samarth Mishra, Kate Saenko, Venkatesh Saligrama</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04740">https://arxiv.org/abs/2504.04740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04740">https://arxiv.org/pdf/2504.04740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04740]] Enhancing Compositional Reasoning in Vision-Language Models with Synthetic Preference Data(https://arxiv.org/abs/2504.04740)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Compositionality, or correctly recognizing scenes as compositions of atomic visual concepts, remains difficult for multimodal large language models (MLLMs). Even state of the art MLLMs such as GPT-4o can make mistakes in distinguishing compositions like "dog chasing cat" vs "cat chasing dog". While on Winoground, a benchmark for measuring such reasoning, MLLMs have made significant progress, they are still far from a human's performance. We show that compositional reasoning in these models can be improved by elucidating such concepts via data, where a model is trained to prefer the correct caption for an image over a close but incorrect one. We introduce SCRAMBLe: Synthetic Compositional Reasoning Augmentation of MLLMs with Binary preference Learning, an approach for preference tuning open-weight MLLMs on synthetic preference data generated in a fully automated manner from existing image-caption data. SCRAMBLe holistically improves these MLLMs' compositional reasoning capabilities which we can see through significant improvements across multiple vision language compositionality benchmarks, as well as smaller but significant improvements on general question answering tasks. As a sneak peek, SCRAMBLe tuned Molmo-7B model improves on Winoground from 49.5% to 54.8% (best reported to date), while improving by ~1% on more general visual question answering tasks. Code for SCRAMBLe along with tuned models and our synthetic training dataset is available at this https URL.</li>
</ul>

<h3>Title: AnyArtisticGlyph: Multilingual Controllable Artistic Glyph Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiongbo Lu, Yaxiong Chen, Shengwu Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04743">https://arxiv.org/abs/2504.04743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04743">https://arxiv.org/pdf/2504.04743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04743]] AnyArtisticGlyph: Multilingual Controllable Artistic Glyph Generation(https://arxiv.org/abs/2504.04743)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Artistic Glyph Image Generation (AGIG) differs from current creativity-focused generation models by offering finely controllable deterministic generation. It transfers the style of a reference image to a source while preserving its content. Although advanced and promising, current methods may reveal flaws when scrutinizing synthesized image details, often producing blurred or incorrect textures, posing a significant challenge. Hence, we introduce AnyArtisticGlyph, a diffusion-based, multilingual controllable artistic glyph generation model. It includes a font fusion and embedding module, which generates latent features for detailed structure creation, and a vision-text fusion and embedding module that uses the CLIP model to encode references and blends them with transformation caption embeddings for seamless global image generation. Moreover, we incorporate a coarse-grained feature-level loss to enhance generation accuracy. Experiments show that it produces natural, detailed artistic glyph images with state-of-the-art performance. Our project will be open-sourced on this https URL to advance text generation technology.</li>
</ul>

<h3>Title: Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs</h3>
<ul>
<li><strong>Authors: </strong>Ankush Raut, Xiaofeng Zhu, Maria Leonor Pacheco</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04745">https://arxiv.org/abs/2504.04745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04745">https://arxiv.org/pdf/2504.04745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04745]] Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs(https://arxiv.org/abs/2504.04745)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper evaluates the ability of Large Language Models (LLMs) to leverage contextual information in the form of structured linguistic representations. Specifically, we examine the impact of encoding both short and long contexts using Abstract Meaning Representation (AMR) structures across a diverse set of language tasks. We perform our analysis using 8-bit quantized and instruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our results indicate that, for tasks involving short contexts, augmenting the prompt with the AMR of the original language context often degrades the performance of the underlying LLM. However, for tasks that involve long contexts, such as dialogue summarization in the SAMSum dataset, this enhancement improves LLM performance, for example, by increasing the zero-shot cosine similarity score of Llama 3.1 from 66.2% to 76%. This improvement is more evident in the newer and larger LLMs, but does not extend to the older or smaller ones. In addition, we observe that LLMs can effectively reconstruct the original text from a linearized AMR, achieving a cosine similarity of 81.3% in the best-case scenario.</li>
</ul>

<h3>Title: Two is Better than One: Efficient Ensemble Defense for Robust and Compact Models</h3>
<ul>
<li><strong>Authors: </strong>Yoojin Jung, Byung Cheol Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04747">https://arxiv.org/abs/2504.04747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04747">https://arxiv.org/pdf/2504.04747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04747]] Two is Better than One: Efficient Ensemble Defense for Robust and Compact Models(https://arxiv.org/abs/2504.04747)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep learning-based computer vision systems adopt complex and large architectures to improve performance, yet they face challenges in deployment on resource-constrained mobile and edge devices. To address this issue, model compression techniques such as pruning, quantization, and matrix factorization have been proposed; however, these compressed models are often highly vulnerable to adversarial attacks. We introduce the \textbf{Efficient Ensemble Defense (EED)} technique, which diversifies the compression of a single base model based on different pruning importance scores and enhances ensemble diversity to achieve high adversarial robustness and resource efficiency. EED dynamically determines the number of necessary sub-models during the inference stage, minimizing unnecessary computations while maintaining high robustness. On the CIFAR-10 and SVHN datasets, EED demonstrated state-of-the-art robustness performance compared to existing adversarial pruning techniques, along with an inference speed improvement of up to 1.86 times. This proves that EED is a powerful defense solution in resource-constrained environments.</li>
</ul>

<h3>Title: CADCrafter: Generating Computer-Aided Design Models from Unconstrained Images</h3>
<ul>
<li><strong>Authors: </strong>Cheng Chen, Jiacheng Wei, Tianrun Chen, Chi Zhang, Xiaofeng Yang, Shangzhan Zhang, Bingchen Yang, Chuan-Sheng Foo, Guosheng Lin, Qixing Huang, Fayao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04753">https://arxiv.org/abs/2504.04753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04753">https://arxiv.org/pdf/2504.04753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04753]] CADCrafter: Generating Computer-Aided Design Models from Unconstrained Images(https://arxiv.org/abs/2504.04753)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Creating CAD digital twins from the physical world is crucial for manufacturing, design, and simulation. However, current methods typically rely on costly 3D scanning with labor-intensive post-processing. To provide a user-friendly design process, we explore the problem of reverse engineering from unconstrained real-world CAD images that can be easily captured by users of all experiences. However, the scarcity of real-world CAD data poses challenges in directly training such models. To tackle these challenges, we propose CADCrafter, an image-to-parametric CAD model generation framework that trains solely on synthetic textureless CAD data while testing on real-world images. To bridge the significant representation disparity between images and parametric CAD models, we introduce a geometry encoder to accurately capture diverse geometric features. Moreover, the texture-invariant properties of the geometric features can also facilitate the generalization to real-world scenarios. Since compiling CAD parameter sequences into explicit CAD models is a non-differentiable process, the network training inherently lacks explicit geometric supervision. To impose geometric validity constraints, we employ direct preference optimization (DPO) to fine-tune our model with the automatic code checker feedback on CAD sequence quality. Furthermore, we collected a real-world dataset, comprised of multi-view images and corresponding CAD command sequence pairs, to evaluate our method. Experimental results demonstrate that our approach can robustly handle real unconstrained CAD images, and even generalize to unseen general objects.</li>
</ul>

<h3>Title: Continuous Locomotive Crowd Behavior Generation</h3>
<ul>
<li><strong>Authors: </strong>Inhwan Bae, Junoh Lee, Hae-Gon Jeon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04756">https://arxiv.org/abs/2504.04756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04756">https://arxiv.org/pdf/2504.04756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04756]] Continuous Locomotive Crowd Behavior Generation(https://arxiv.org/abs/2504.04756)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Modeling and reproducing crowd behaviors are important in various domains including psychology, robotics, transport engineering and virtual environments. Conventional methods have focused on synthesizing momentary scenes, which have difficulty in replicating the continuous nature of real-world crowds. In this paper, we introduce a novel method for automatically generating continuous, realistic crowd trajectories with heterogeneous behaviors and interactions among individuals. We first design a crowd emitter model. To do this, we obtain spatial layouts from single input images, including a segmentation map, appearance map, population density map and population probability, prior to crowd generation. The emitter then continually places individuals on the timeline by assigning independent behavior characteristics such as agents' type, pace, and start/end positions using diffusion models. Next, our crowd simulator produces their long-term locomotions. To simulate diverse actions, it can augment their behaviors based on a Markov chain. As a result, our overall framework populates the scenes with heterogeneous crowd behaviors by alternating between the proposed emitter and simulator. Note that all the components in the proposed framework are user-controllable. Lastly, we propose a benchmark protocol to evaluate the realism and quality of the generated crowds in terms of the scene-level population dynamics and the individual-level trajectory accuracy. We demonstrate that our approach effectively models diverse crowd behavior patterns and generalizes well across different geographical environments. Code is publicly available at this https URL .</li>
</ul>

<h3>Title: Enhancing Leaf Disease Classification Using GAT-GCN Hybrid Model</h3>
<ul>
<li><strong>Authors: </strong>Shyam Sundhar, Riya Sharma, Priyansh Maheshwari, Suvidha Rupesh Kumar, T. Sunil Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04764">https://arxiv.org/abs/2504.04764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04764">https://arxiv.org/pdf/2504.04764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04764]] Enhancing Leaf Disease Classification Using GAT-GCN Hybrid Model(https://arxiv.org/abs/2504.04764)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Agriculture plays a critical role in the global economy, providing livelihoods and ensuring food security for billions. As innovative agricultural practices become more widespread, the risk of crop diseases has increased, highlighting the urgent need for efficient, low-intervention disease identification methods. This research presents a hybrid model combining Graph Attention Networks (GATs) and Graph Convolution Networks (GCNs) for leaf disease classification. GCNs have been widely used for learning from graph-structured data, and GATs enhance this by incorporating attention mechanisms to focus on the most important neighbors. The methodology integrates superpixel segmentation for efficient feature extraction, partitioning images into meaningful, homogeneous regions that better capture localized features. The authors have employed an edge augmentation technique to enhance the robustness of the model. The edge augmentation technique has introduced a significant degree of generalization in the detection capabilities of the model. To further optimize training, weight initialization techniques are applied. The hybrid model is evaluated against the individual performance of the GCN and GAT models and the hybrid model achieved a precision of 0.9822, recall of 0.9818, and F1-score of 0.9818 in apple leaf disease classification, a precision of 0.9746, recall of 0.9744, and F1-score of 0.9743 in potato leaf disease classification, and a precision of 0.8801, recall of 0.8801, and F1-score of 0.8799 in sugarcane leaf disease classification. These results demonstrate the robustness and performance of the model, suggesting its potential to support sustainable agricultural practices through precise and effective disease detection. This work is a small step towards reducing the loss of crops and hence supporting sustainable goals of zero hunger and life on land.</li>
</ul>

<h3>Title: KunPeng: A Global Ocean Environmental Model</h3>
<ul>
<li><strong>Authors: </strong>Yi Zhao, Jiaqi Li, Haitao Xia, Tianjiao Zhang, Zerong Zeng, Tianyu Ren, Yucheng Zhang, Chao Zhu, Shengtong Xu, Hongchun Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04766">https://arxiv.org/abs/2504.04766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04766">https://arxiv.org/pdf/2504.04766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04766]] KunPeng: A Global Ocean Environmental Model(https://arxiv.org/abs/2504.04766)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Inspired by the similarity of the atmosphere-ocean physical coupling mechanism, this study innovatively migrates meteorological large-model techniques to the ocean domain, constructing the KunPeng global ocean environmental prediction model. Aimed at the discontinuous characteristics of marine space, we propose a terrain-adaptive mask constraint mechanism to mitigate effectively training divergence caused by abrupt gradients at land-sea boundaries. To fully integrate far-, medium-, and close-range marine features, a longitude-cyclic deformable convolution network (LC-DCN) is employed to enhance the dynamic receptive field, achieving refined modeling of multi-scale oceanic characteristics. A Deformable Convolution-enhanced Multi-Step Prediction module (DC-MTP) is employed to strengthen temporal dependency feature extraction capabilities. Experimental results demonstrate that this model achieves an average ACC of 0.80 in 15-day global predictions at 0.25$^\circ$ resolution, outperforming comparative models by 0.01-0.08. The average mean squared error (MSE) is 0.41 (representing a 5%-31% reduction) and the average mean absolute error (MAE) is 0.44 (0.6%-21% reduction) compared to other models. Significant improvements are particularly observed in sea surface parameter prediction, deep-sea region characterization, and current velocity field forecasting. Through a horizontal comparison of the applicability of operators at different scales in the marine domain, this study reveals that local operators significantly outperform global operators under slow-varying oceanic processes, demonstrating the effectiveness of dynamic feature pyramid representations in predicting marine physical parameters.</li>
</ul>

<h3>Title: Bidirectional Hierarchical Protein Multi-Modal Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Xuefeng Liu, Songhao Jiang, Chih-chan Tien, Jinbo Xu, Rick Stevens</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.MN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04770">https://arxiv.org/abs/2504.04770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04770">https://arxiv.org/pdf/2504.04770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04770]] Bidirectional Hierarchical Protein Multi-Modal Representation Learning(https://arxiv.org/abs/2504.04770)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Protein representation learning is critical for numerous biological tasks. Recently, large transformer-based protein language models (pLMs) pretrained on large scale protein sequences have demonstrated significant success in sequence-based tasks. However, pLMs lack structural information. Conversely, graph neural networks (GNNs) designed to leverage 3D structural information have shown promising generalization in protein-related prediction tasks, but their effectiveness is often constrained by the scarcity of labeled structural data. Recognizing that sequence and structural representations are complementary perspectives of the same protein entity, we propose a multimodal bidirectional hierarchical fusion framework to effectively merge these modalities. Our framework employs attention and gating mechanisms to enable effective interaction between pLMs-generated sequential representations and GNN-extracted structural features, improving information exchange and enhancement across layers of the neural network. Based on the framework, we further introduce local Bi-Hierarchical Fusion with gating and global Bi-Hierarchical Fusion with multihead self-attention approaches. Through extensive experiments on a diverse set of protein-related tasks, our method demonstrates consistent improvements over strong baselines and existing fusion techniques in a variety of protein representation learning benchmarks, including react (enzyme/EC classification), model quality assessment (MQA), protein-ligand binding affinity prediction (LBA), protein-protein binding site prediction (PPBS), and B cell epitopes prediction (BCEs). Our method establishes a new state-of-the-art for multimodal protein representation learning, emphasizing the efficacy of BIHIERARCHICAL FUSION in bridging sequence and structural modalities.</li>
</ul>

<h3>Title: Improving Multilingual Retrieval-Augmented Language Models through Dialectic Reasoning Argumentations</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Ranaldi, Federico Ranaldi, Fabio Massimo Zanzotto, Barry Haddow, Alexandra Birch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04771">https://arxiv.org/abs/2504.04771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04771">https://arxiv.org/pdf/2504.04771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04771]] Improving Multilingual Retrieval-Augmented Language Models through Dialectic Reasoning Argumentations(https://arxiv.org/abs/2504.04771)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) is key to enhancing large language models (LLMs) to systematically access richer factual knowledge. Yet, using RAG brings intrinsic challenges, as LLMs must deal with potentially conflicting knowledge, especially in multilingual retrieval, where the heterogeneity of knowledge retrieved may deliver different outlooks. To make RAG more analytical, critical and grounded, we introduce Dialectic-RAG (DRAG), a modular approach guided by Argumentative Explanations, i.e., structured reasoning process that systematically evaluates retrieved information by comparing, contrasting, and resolving conflicting perspectives. Given a query and a set of multilingual related documents, DRAG selects and exemplifies relevant knowledge for delivering dialectic explanations that, by critically weighing opposing arguments and filtering extraneous content, clearly determine the final response. Through a series of in-depth experiments, we show the impact of our framework both as an in-context learning strategy and for constructing demonstrations to instruct smaller models. The final results demonstrate that DRAG significantly improves RAG approaches, requiring low-impact computational effort and providing robustness to knowledge perturbations.</li>
</ul>

<h3>Title: Feedback-Enhanced Hallucination-Resistant Vision-Language Model for Real-Time Scene Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zahir Alsulaimawi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04772">https://arxiv.org/abs/2504.04772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04772">https://arxiv.org/pdf/2504.04772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04772]] Feedback-Enhanced Hallucination-Resistant Vision-Language Model for Real-Time Scene Understanding(https://arxiv.org/abs/2504.04772)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Real-time scene comprehension is a key advance in artificial intelligence, enhancing robotics, surveillance, and assistive tools. However, hallucination remains a challenge. AI systems often misinterpret visual inputs, detecting nonexistent objects or describing events that never happened. These errors, far from minor, threaten reliability in critical areas like security and autonomous navigation where accuracy is essential. Our approach tackles this by embedding self-awareness into the AI. Instead of trusting initial outputs, our framework continuously assesses them in real time, adjusting confidence thresholds dynamically. When certainty falls below a solid benchmark, it suppresses unreliable claims. Combining YOLOv5's object detection strength with VILA1.5-3B's controlled language generation, we tie descriptions to confirmed visual data. Strengths include dynamic threshold tuning for better accuracy, evidence-based text to reduce hallucination, and real-time performance at 18 frames per second. This feedback-driven design cuts hallucination by 37 percent over traditional methods. Fast, flexible, and reliable, it excels in applications from robotic navigation to security monitoring, aligning AI perception with reality.</li>
</ul>

<h3>Title: Bottom-Up Scattering Information Perception Network for SAR target recognition</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Zhao, Daochang Wang, Siqian Zhang, Gangyao Kuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04780">https://arxiv.org/abs/2504.04780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04780">https://arxiv.org/pdf/2504.04780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04780]] Bottom-Up Scattering Information Perception Network for SAR target recognition(https://arxiv.org/abs/2504.04780)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Deep learning methods based synthetic aperture radar (SAR) image target recognition tasks have been widely studied currently. The existing deep methods are insufficient to perceive and mine the scattering information of SAR images, resulting in performance bottlenecks and poor robustness of the algorithms. To this end, this paper proposes a novel bottom-up scattering information perception network for more interpretable target recognition by constructing the proprietary interpretation network for SAR images. Firstly, the localized scattering perceptron is proposed to replace the backbone feature extractor based on CNN networks to deeply mine the underlying scattering information of the target. Then, an unsupervised scattering part feature extraction model is proposed to robustly characterize the target scattering part information and provide fine-grained target representation. Finally, by aggregating the knowledge of target parts to form the complete target description, the interpretability and discriminative ability of the model is improved. We perform experiments on the FAST-Vehicle dataset and the SAR-ACD dataset to validate the performance of the proposed method.</li>
</ul>

<h3>Title: OCC-MLLM-CoT-Alpha: Towards Multi-stage Occlusion Recognition Based on Large Language Models via 3D-Aware Supervision and Chain-of-Thoughts Guidance</h3>
<ul>
<li><strong>Authors: </strong>Chaoyi Wang, Baoqing Li, Xinhan Di</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04781">https://arxiv.org/abs/2504.04781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04781">https://arxiv.org/pdf/2504.04781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04781]] OCC-MLLM-CoT-Alpha: Towards Multi-stage Occlusion Recognition Based on Large Language Models via 3D-Aware Supervision and Chain-of-Thoughts Guidance(https://arxiv.org/abs/2504.04781)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Comprehending occluded objects are not well studied in existing large-scale visual-language multi-modal models. Current state-of-the-art multi-modal large models struggles to provide satisfactory results in understanding occluded objects through universal visual encoders and supervised learning strategies. Therefore, we propose OCC-MLLM-CoT-Alpha, a multi-modal large vision language framework that integrates 3D-aware supervision and Chain-of-Thoughts guidance. Particularly, (1) we build a multi-modal large vision-language model framework which is consisted of a large multi-modal vision-language model and a 3D reconstruction expert model. (2) the corresponding multi-modal Chain-of-Thoughts is learned through a combination of supervised and reinforcement training strategies, allowing the multi-modal vision-language model to enhance the recognition ability with learned multi-modal chain-of-thoughts guidance. (3) A large-scale multi-modal chain-of-thoughts reasoning dataset, consisting of $110k$ samples of occluded objects held in hand, is built. In the evaluation, the proposed methods demonstrate decision score improvement of 15.75%,15.30%,16.98%,14.62%, and 4.42%,3.63%,6.94%,10.70% for two settings of a variety of state-of-the-art models.</li>
</ul>

<h3>Title: Playing Non-Embedded Card-Based Games with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Tianyang Wu, Lipeng Wan, Yuhang Wang, Qiang Wan, Xuguang Lan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04783">https://arxiv.org/abs/2504.04783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04783">https://arxiv.org/pdf/2504.04783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04783]] Playing Non-Embedded Card-Based Games with Reinforcement Learning(https://arxiv.org/abs/2504.04783)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>Significant progress has been made in AI for games, including board games, MOBA, and RTS games. However, complex agents are typically developed in an embedded manner, directly accessing game state information, unlike human players who rely on noisy visual data, leading to unfair competition. Developing complex non-embedded agents remains challenging, especially in card-based RTS games with complex features and large state spaces. We propose a non-embedded offline reinforcement learning training strategy using visual inputs to achieve real-time autonomous gameplay in the RTS game Clash Royale. Due to the lack of a object detection dataset for this game, we designed an efficient generative object detection dataset for training. We extract features using state-of-the-art object detection and optical character recognition models. Our method enables real-time image acquisition, perception feature fusion, decision-making, and control on mobile devices, successfully defeating built-in AI opponents. All code is open-sourced at this https URL.</li>
</ul>

<h3>Title: Disentangling Instruction Influence in Diffusion Transformers for Parallel Multi-Instruction-Guided Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Hui Liu, Bin Zou, Suiyun Zhang, Kecheng Chen, Rui Liu, Haoliang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04784">https://arxiv.org/abs/2504.04784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04784">https://arxiv.org/pdf/2504.04784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04784]] Disentangling Instruction Influence in Diffusion Transformers for Parallel Multi-Instruction-Guided Image Editing(https://arxiv.org/abs/2504.04784)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Instruction-guided image editing enables users to specify modifications using natural language, offering more flexibility and control. Among existing frameworks, Diffusion Transformers (DiTs) outperform U-Net-based diffusion models in scalability and performance. However, while real-world scenarios often require concurrent execution of multiple instructions, step-by-step editing suffers from accumulated errors and degraded quality, and integrating multiple instructions with a single prompt usually results in incomplete edits due to instruction conflicts. We propose Instruction Influence Disentanglement (IID), a novel framework enabling parallel execution of multiple instructions in a single denoising process, designed for DiT-based models. By analyzing self-attention mechanisms in DiTs, we identify distinctive attention patterns in multi-instruction settings and derive instruction-specific attention masks to disentangle each instruction's influence. These masks guide the editing process to ensure localized modifications while preserving consistency in non-edited regions. Extensive experiments on open-source and custom datasets demonstrate that IID reduces diffusion steps while improving fidelity and instruction completion compared to existing baselines. The codes will be publicly released upon the acceptance of the paper.</li>
</ul>

<h3>Title: Enhancing Trust in AI Marketplaces: Evaluating On-Chain Verification of Personalized AI models using zk-SNARKs</h3>
<ul>
<li><strong>Authors: </strong>Nishant Jagannath, Christopher Wong, Braden Mcgrath, Md Farhad Hossain, Asuquo A. Okon, Abbas Jamalipour, Kumudu S. Munasinghe</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04794">https://arxiv.org/abs/2504.04794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04794">https://arxiv.org/pdf/2504.04794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04794]] Enhancing Trust in AI Marketplaces: Evaluating On-Chain Verification of Personalized AI models using zk-SNARKs(https://arxiv.org/abs/2504.04794)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect</a></li>
<li><strong>Abstract: </strong>The rapid advancement of artificial intelligence (AI) has brought about sophisticated models capable of various tasks ranging from image recognition to natural language processing. As these models continue to grow in complexity, ensuring their trustworthiness and transparency becomes critical, particularly in decentralized environments where traditional trust mechanisms are absent. This paper addresses the challenge of verifying personalized AI models in such environments, focusing on their integrity and privacy. We propose a novel framework that integrates zero-knowledge succinct non-interactive arguments of knowledge (zk-SNARKs) with Chainlink decentralized oracles to verify AI model performance claims on blockchain platforms. Our key contribution lies in integrating zk-SNARKs with Chainlink oracles to securely fetch and verify external data to enable trustless verification of AI models on a blockchain. Our approach addresses the limitations of using unverified external data for AI verification on the blockchain while preserving sensitive information of AI models and enhancing transparency. We demonstrate our methodology with a linear regression model predicting Bitcoin prices using on-chain data verified on the Sepolia testnet. Our results indicate the framework's efficacy, with key metrics including proof generation taking an average of 233.63 seconds and verification time of 61.50 seconds. This research paves the way for transparent and trustless verification processes in blockchain-enabled AI ecosystems, addressing key challenges such as model integrity and model privacy protection. The proposed framework, while exemplified with linear regression, is designed for broader applicability across more complex AI models, setting the stage for future advancements in transparent AI verification.</li>
</ul>

<h3>Title: TabRep: Training Tabular Diffusion Models with a Simple and Effective Continuous Representation</h3>
<ul>
<li><strong>Authors: </strong>Jacob Si, Zijing Ou, Mike Qu, Zhengrui Xiang, Yingzhen Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04798">https://arxiv.org/abs/2504.04798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04798">https://arxiv.org/pdf/2504.04798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04798]] TabRep: Training Tabular Diffusion Models with a Simple and Effective Continuous Representation(https://arxiv.org/abs/2504.04798)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have been the predominant generative model for tabular data generation. However, they face the conundrum of modeling under a separate versus a unified data representation. The former encounters the challenge of jointly modeling all multi-modal distributions of tabular data in one model. While the latter alleviates this by learning a single representation for all features, it currently leverages sparse suboptimal encoding heuristics and necessitates additional computation costs. In this work, we address the latter by presenting TabRep, a tabular diffusion architecture trained with a unified continuous representation. To motivate the design of our representation, we provide geometric insights into how the data manifold affects diffusion models. The key attributes of our representation are composed of its density, flexibility to provide ample separability for nominal features, and ability to preserve intrinsic relationships. Ultimately, TabRep provides a simple yet effective approach for training tabular diffusion models under a continuous data manifold. Our results showcase that TabRep achieves superior performance across a broad suite of evaluations. It is the first to synthesize tabular data that exceeds the downstream quality of the original datasets while preserving privacy and remaining computationally efficient.</li>
</ul>

<h3>Title: Topological Schrödinger Bridge Matching</h3>
<ul>
<li><strong>Authors: </strong>Maosheng Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04799">https://arxiv.org/abs/2504.04799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04799">https://arxiv.org/pdf/2504.04799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04799]] Topological Schrödinger Bridge Matching(https://arxiv.org/abs/2504.04799)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Given two boundary distributions, the Schrödinger Bridge (SB) problem seeks the ``most likely`` random evolution between them with respect to a reference process. It has revealed rich connections to recent machine learning methods for generative modeling and distribution matching. While these methods perform well in Euclidean domains, they are not directly applicable to topological domains such as graphs and simplicial complexes, which are crucial for data defined over network entities, such as node signals and edge flows. In this work, we propose the Topological Schrödinger Bridge problem (TSBP) for matching signal distributions on a topological domain. We set the reference process to follow some linear tractable topology-aware stochastic dynamics such as topological heat diffusion. For the case of Gaussian boundary distributions, we derive a closed-form topological SB (TSB) in terms of its time-marginal and stochastic differential. In the general case, leveraging the well-known result, we show that the optimal process follows the forward-backward topological dynamics governed by some unknowns. Building on these results, we develop TSB-based models for matching topological signals by parameterizing the unknowns in the optimal process as (topological) neural networks and learning them through likelihood training. We validate the theoretical results and demonstrate the practical applications of TSB-based models on both synthetic and real-world networks, emphasizing the role of topology. Additionally, we discuss the connections of TSB-based models to other emerging models, and outline future directions for topological signal matching.</li>
</ul>

<h3>Title: OrderChain: A General Prompting Paradigm to Improve Ordinal Understanding Ability of MLLM</h3>
<ul>
<li><strong>Authors: </strong>Jinhong Wang, Shuo Tong, Jian liu, Dongqi Tang, Weiqiang Wang, Wentong Li, Hongxia Xu, Danny Chen, Jintai Chen, Jian Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04801">https://arxiv.org/abs/2504.04801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04801">https://arxiv.org/pdf/2504.04801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04801]] OrderChain: A General Prompting Paradigm to Improve Ordinal Understanding Ability of MLLM(https://arxiv.org/abs/2504.04801)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the remarkable progress of multimodal large language models (MLLMs), they continue to face challenges in achieving competitive performance on ordinal regression (OR; a.k.a. ordinal classification). To address this issue, this paper presents OrderChain, a novel and general prompting paradigm that improves the ordinal understanding ability of MLLMs by specificity and commonality modeling. Specifically, our OrderChain consists of a set of task-aware prompts to facilitate the specificity modeling of diverse OR tasks and a new range optimization Chain-of-Thought (RO-CoT), which learns a commonality way of thinking about OR tasks by uniformly decomposing them into multiple small-range optimization subtasks. Further, we propose a category recursive division (CRD) method to generate instruction candidate category prompts to support RO-CoT automatic optimization. Comprehensive experiments show that a Large Language and Vision Assistant (LLaVA) model with our OrderChain improves baseline LLaVA significantly on diverse OR datasets, e.g., from 47.5% to 93.2% accuracy on the Adience dataset for age estimation, and from 30.0% to 85.7% accuracy on the Diabetic Retinopathy dataset. Notably, LLaVA with our OrderChain also remarkably outperforms state-of-the-art methods by 27% on accuracy and 0.24 on MAE on the Adience dataset. To our best knowledge, our OrderChain is the first work that augments MLLMs for OR tasks, and the effectiveness is witnessed across a spectrum of OR datasets.</li>
</ul>

<h3>Title: Select Me! When You Need a Tool: A Black-box Text Attack on Tool Selection</h3>
<ul>
<li><strong>Authors: </strong>Liuji Chen, Hao Gao, Jinghao Zhang, Qiang Liu, Shu Wu, Liang Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04809">https://arxiv.org/abs/2504.04809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04809">https://arxiv.org/pdf/2504.04809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04809]] Select Me! When You Need a Tool: A Black-box Text Attack on Tool Selection(https://arxiv.org/abs/2504.04809)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, large language model</a></li>
<li><strong>Abstract: </strong>Tool learning serves as a powerful auxiliary mechanism that extends the capabilities of large language models (LLMs), enabling them to tackle complex tasks requiring real-time relevance or high precision operations. Behind its powerful capabilities lie some potential security issues. However, previous work has primarily focused on how to make the output of the invoked tools incorrect or malicious, with little attention given to the manipulation of tool selection. To fill this gap, we introduce, for the first time, a black-box text-based attack that can significantly increase the probability of the target tool being selected in this paper. We propose a two-level text perturbation attack witha coarse-to-fine granularity, attacking the text at both the word level and the character level. We conduct comprehensive experiments that demonstrate the attacker only needs to make some perturbations to the tool's textual information to significantly increase the possibility of the target tool being selected and ranked higher among the candidate tools. Our research reveals the vulnerability of the tool selection process and paves the way for future research on protecting this process.</li>
</ul>

<h3>Title: SUEDE:Shared Unified Experts for Physical-Digital Face Attack Detection Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Zuying Xie, Changtao Miao, Ajian Liu, Jiabao Guo, Feng Li, Dan Guo, Yunfeng Diao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04818">https://arxiv.org/abs/2504.04818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04818">https://arxiv.org/pdf/2504.04818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04818]] SUEDE:Shared Unified Experts for Physical-Digital Face Attack Detection Enhancement(https://arxiv.org/abs/2504.04818)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Face recognition systems are vulnerable to physical attacks (e.g., printed photos) and digital threats (e.g., DeepFake), which are currently being studied as independent visual tasks, such as Face Anti-Spoofing and Forgery Detection. The inherent differences among various attack types present significant challenges in identifying a common feature space, making it difficult to develop a unified framework for detecting data from both attack modalities simultaneously. Inspired by the efficacy of Mixture-of-Experts (MoE) in learning across diverse domains, we explore utilizing multiple experts to learn the distinct features of various attack types. However, the feature distributions of physical and digital attacks overlap and differ. This suggests that relying solely on distinct experts to learn the unique features of each attack type may overlook shared knowledge between them. To address these issues, we propose SUEDE, the Shared Unified Experts for Physical-Digital Face Attack Detection Enhancement. SUEDE combines a shared expert (always activated) to capture common features for both attack types and multiple routed experts (selectively activated) for specific attack types. Further, we integrate CLIP as the base network to ensure the shared expert benefits from prior visual knowledge and align visual-text representations in a unified space. Extensive results demonstrate SUEDE achieves superior performance compared to state-of-the-art unified detection methods.</li>
</ul>

<h3>Title: Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Ruikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai, Xianzhi Yu, Tiezheng Yu, Chun Yuan, Lu Hou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04823">https://arxiv.org/abs/2504.04823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04823">https://arxiv.org/pdf/2504.04823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04823]] Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models(https://arxiv.org/abs/2504.04823)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this study, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, and QwQ-32B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes will be open-sourced in this https URL.</li>
</ul>

<h3>Title: Learning Affine Correspondences by Integrating Geometric Constraints</h3>
<ul>
<li><strong>Authors: </strong>Pengju Sun, Banglei Guan, Zhenbao Yu, Yang Shang, Qifeng Yu, Daniel Barath</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04834">https://arxiv.org/abs/2504.04834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04834">https://arxiv.org/pdf/2504.04834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04834]] Learning Affine Correspondences by Integrating Geometric Constraints(https://arxiv.org/abs/2504.04834)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Affine correspondences have received significant attention due to their benefits in tasks like image matching and pose estimation. Existing methods for extracting affine correspondences still have many limitations in terms of performance; thus, exploring a new paradigm is crucial. In this paper, we present a new pipeline designed for extracting accurate affine correspondences by integrating dense matching and geometric constraints. Specifically, a novel extraction framework is introduced, with the aid of dense matching and a novel keypoint scale and orientation estimator. For this purpose, we propose loss functions based on geometric constraints, which can effectively improve accuracy by supervising neural networks to learn feature geometry. The experimental show that the accuracy and robustness of our method outperform the existing ones in image matching tasks. To further demonstrate the effectiveness of the proposed method, we applied it to relative pose estimation. Affine correspondences extracted by our method lead to more accurate poses than the baselines on a range of real-world datasets. The code is available at this https URL.</li>
</ul>

<h3>Title: Inland Waterway Object Detection in Multi-environment: Dataset and Approach</h3>
<ul>
<li><strong>Authors: </strong>Shanshan Wang, Haixiang Xu, Hui Feng, Xiaoqian Wang, Pei Song, Sijie Liu, Jianhua He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04835">https://arxiv.org/abs/2504.04835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04835">https://arxiv.org/pdf/2504.04835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04835]] Inland Waterway Object Detection in Multi-environment: Dataset and Approach(https://arxiv.org/abs/2504.04835)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The success of deep learning in intelligent ship visual perception relies heavily on rich image data. However, dedicated datasets for inland waterway vessels remain scarce, limiting the adaptability of visual perception systems in complex environments. Inland waterways, characterized by narrow channels, variable weather, and urban interference, pose significant challenges to object detection systems based on existing datasets. To address these issues, this paper introduces the Multi-environment Inland Waterway Vessel Dataset (MEIWVD), comprising 32,478 high-quality images from diverse scenarios, including sunny, rainy, foggy, and artificial lighting conditions. MEIWVD covers common vessel types in the Yangtze River Basin, emphasizing diversity, sample independence, environmental complexity, and multi-scale characteristics, making it a robust benchmark for vessel detection. Leveraging MEIWVD, this paper proposes a scene-guided image enhancement module to improve water surface images based on environmental conditions adaptively. Additionally, a parameter-limited dilated convolution enhances the representation of vessel features, while a multi-scale dilated residual fusion method integrates multi-scale features for better detection. Experiments show that MEIWVD provides a more rigorous benchmark for object detection algorithms, and the proposed methods significantly improve detector performance, especially in complex multi-environment scenarios.</li>
</ul>

<h3>Title: Uni4D: A Unified Self-Supervised Learning Framework for Point Cloud Videos</h3>
<ul>
<li><strong>Authors: </strong>Zhi Zuo, Chenyi Zhuang, Zhiqiang Shen, Pan Gao, Jie Qin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04837">https://arxiv.org/abs/2504.04837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04837">https://arxiv.org/pdf/2504.04837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04837]] Uni4D: A Unified Self-Supervised Learning Framework for Point Cloud Videos(https://arxiv.org/abs/2504.04837)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Point cloud video representation learning is primarily built upon the masking strategy in a self-supervised manner. However, the progress is slow due to several significant challenges: (1) existing methods learn the motion particularly with hand-crafted designs, leading to unsatisfactory motion patterns during pre-training which are non-transferable on fine-tuning scenarios. (2) previous Masked AutoEncoder (MAE) frameworks are limited in resolving the huge representation gap inherent in 4D data. In this study, we introduce the first self-disentangled MAE for learning discriminative 4D representations in the pre-training stage. To address the first challenge, we propose to model the motion representation in a latent space. The second issue is resolved by introducing the latent tokens along with the typical geometry tokens to disentangle high-level and low-level features during decoding. Extensive experiments on MSR-Action3D, NTU-RGBD, HOI4D, NvGesture, and SHREC'17 verify this self-disentangled learning framework. We demonstrate that it can boost the fine-tuning performance on all 4D tasks, which we term Uni4D. Our pre-trained model presents discriminative and meaningful 4D representations, particularly benefits processing long videos, as Uni4D gets $+3.8\%$ segmentation accuracy on HOI4D, significantly outperforming either self-supervised or fully-supervised methods after end-to-end fine-tuning.</li>
</ul>

<h3>Title: Prior2Former -- Evidential Modeling of Mask Transformers for Assumption-Free Open-World Panoptic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Schmidt, Julius Körner, Dominik Fuchsgruber, Stefano Gasperini, Federico Tombari, Stephan Günnemann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04841">https://arxiv.org/abs/2504.04841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04841">https://arxiv.org/pdf/2504.04841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04841]] Prior2Former -- Evidential Modeling of Mask Transformers for Assumption-Free Open-World Panoptic Segmentation(https://arxiv.org/abs/2504.04841)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>In panoptic segmentation, individual instances must be separated within semantic classes. As state-of-the-art methods rely on a pre-defined set of classes, they struggle with novel categories and out-of-distribution (OOD) data. This is particularly problematic in safety-critical applications, such as autonomous driving, where reliability in unseen scenarios is essential. We address the gap between outstanding benchmark performance and reliability by proposing Prior2Former (P2F), the first approach for segmentation vision transformers rooted in evidential learning. P2F extends the mask vision transformer architecture by incorporating a Beta prior for computing model uncertainty in pixel-wise binary mask assignments. This design enables high-quality uncertainty estimation that effectively detects novel and OOD objects enabling state-of-the-art anomaly instance segmentation and open-world panoptic segmentation. Unlike most segmentation models addressing unknown classes, P2F operates without access to OOD data samples or contrastive training on void (i.e., unlabeled) classes, making it highly applicable in real-world scenarios where such prior information is unavailable. Additionally, P2F can be flexibly applied to anomaly instance and panoptic segmentation. Through comprehensive experiments on the Cityscapes, COCO, SegmentMeIfYouCan, and OoDIS datasets, we demonstrate the state-of-the-art performance of P2F. It achieves the highest ranking in the OoDIS anomaly instance benchmark among methods not using OOD data in any way.</li>
</ul>

<h3>Title: FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Mengchao Wang, Qiang Wang, Fan Jiang, Yaqi Fan, Yunpeng Zhang, Yonggang Qi, Kun Zhao, Mu Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04842">https://arxiv.org/abs/2504.04842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04842">https://arxiv.org/pdf/2504.04842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04842]] FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis(https://arxiv.org/abs/2504.04842)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverages a pretrained video diffusion transformer model to generate high-fidelity, coherent talking portraits with controllable motion dynamics. At the core of our work is a dual-stage audio-visual alignment strategy. In the first stage, we employ a clip-level training scheme to establish coherent global motion by aligning audio-driven dynamics across the entire scene, including the reference portrait, contextual objects, and background. In the second stage, we refine lip movements at the frame level using a lip-tracing mask, ensuring precise synchronization with audio signals. To preserve identity without compromising motion flexibility, we replace the commonly used reference network with a facial-focused cross-attention module that effectively maintains facial consistency throughout the video. Furthermore, we integrate a motion intensity modulation module that explicitly controls expression and body motion intensity, enabling controllable manipulation of portrait movements beyond mere lip motion. Extensive experimental results show that our proposed approach achieves higher quality with better realism, coherence, motion intensity, and identity preservation. Ours project page: this https URL.</li>
</ul>

<h3>Title: SAFT: Structure-aware Transformers for Textual Interaction Classification</h3>
<ul>
<li><strong>Authors: </strong>Hongtao Wang, Renchi Yang, Hewen Wang, Haoran Zheng, Jianliang Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04861">https://arxiv.org/abs/2504.04861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04861">https://arxiv.org/pdf/2504.04861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04861]] SAFT: Structure-aware Transformers for Textual Interaction Classification(https://arxiv.org/abs/2504.04861)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Textual interaction networks (TINs) are an omnipresent data structure used to model the interplay between users and items on e-commerce websites, social networks, etc., where each interaction is associated with a text description. Classifying such textual interactions (TIC) finds extensive use in detecting spam reviews in e-commerce, fraudulent transactions in finance, and so on. Existing TIC solutions either (i) fail to capture the rich text semantics due to the use of context-free text embeddings, and/or (ii) disregard the bipartite structure and node heterogeneity of TINs, leading to compromised TIC performance. In this work, we propose SAFT, a new architecture that integrates language- and graph-based modules for the effective fusion of textual and structural semantics in the representation learning of interactions. In particular, line graph attention (LGA)/gated attention units (GAUs) and pretrained language models (PLMs) are capitalized on to model the interaction-level and token-level signals, which are further coupled via the proxy token in an iterative and contextualized fashion. Additionally, an efficient and theoretically-grounded approach is developed to encode the local and global topology information pertaining to interactions into structural embeddings. The resulting embeddings not only inject the structural features underlying TINs into the textual interaction encoding but also facilitate the design of graph sampling strategies. Extensive empirical evaluations on multiple real TIN datasets demonstrate the superiority of SAFT over the state-of-the-art baselines in TIC accuracy.</li>
</ul>

<h3>Title: FedSAUC: A Similarity-Aware Update Control for Communication-Efficient Federated Learning in Edge Computing</h3>
<ul>
<li><strong>Authors: </strong>Ming-Lun Lee, Han-Chang Chou, Yan-AnnChen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04867">https://arxiv.org/abs/2504.04867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04867">https://arxiv.org/pdf/2504.04867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04867]] FedSAUC: A Similarity-Aware Update Control for Communication-Efficient Federated Learning in Edge Computing(https://arxiv.org/abs/2504.04867)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning is a distributed machine learning framework to collaboratively train a global model without uploading privacy-sensitive data onto a centralized server. Usually, this framework is applied to edge devices such as smartphones, wearable devices, and Internet of Things (IoT) devices which closely collect information from users. However, these devices are mostly battery-powered. The update procedure of federated learning will constantly consume the battery power and the transmission bandwidth. In this work, we propose an update control for federated learning, FedSAUC, by considering the similarity of users' behaviors (models). At the server side, we exploit clustering algorithms to group devices with similar models. Then we select some representatives for each cluster to update information to train the model. We also implemented a testbed prototyping on edge devices for validating the performance. The experimental results show that this update control will not affect the training accuracy in the long run.</li>
</ul>

<h3>Title: Content-Aware Transformer for All-in-one Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Gang Wu, Junjun Jiang, Kui Jiang, Xianming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04869">https://arxiv.org/abs/2504.04869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04869">https://arxiv.org/pdf/2504.04869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04869]] Content-Aware Transformer for All-in-one Image Restoration(https://arxiv.org/abs/2504.04869)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Image restoration has witnessed significant advancements with the development of deep learning models. Although Transformer architectures have progressed considerably in recent years, challenges remain, particularly the limited receptive field in window-based self-attention. In this work, we propose DSwinIR, a Deformable Sliding window Transformer for Image Restoration. DSwinIR introduces a novel deformable sliding window self-attention that adaptively adjusts receptive fields based on image content, enabling the attention mechanism to focus on important regions and enhance feature extraction aligned with salient features. Additionally, we introduce a central ensemble pattern to reduce the inclusion of irrelevant content within attention windows. In this way, the proposed DSwinIR model integrates the deformable sliding window Transformer and central ensemble pattern to amplify the strengths of both CNNs and Transformers while mitigating their limitations. Extensive experiments on various image restoration tasks demonstrate that DSwinIR achieves state-of-the-art performance. For example, in image deraining, compared to DRSformer on the SPA dataset, DSwinIR achieves a 0.66 dB PSNR improvement. In all-in-one image restoration, compared to PromptIR, DSwinIR achieves over a 0.66 dB and 1.04 dB improvement on three-task and five-task settings, respectively. Pretrained models and code are available at our project this https URL.</li>
</ul>

<h3>Title: SoK: LLM-based Log Parsing</h3>
<ul>
<li><strong>Authors: </strong>Viktor Beck, Max Landauer, Markus Wurzenberger, Florian Skopik, Andreas Rauber</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04877">https://arxiv.org/abs/2504.04877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04877">https://arxiv.org/pdf/2504.04877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04877]] SoK: LLM-based Log Parsing(https://arxiv.org/abs/2504.04877)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Log data, generated by software systems, provides crucial insights for tasks like monitoring, root cause analysis, and anomaly detection. Due to the vast volume of logs, automated log parsing is essential to transform semi-structured log messages into structured representations. Traditional log parsing techniques often require manual configurations, such as defining log formats or labeling data, which limits scalability and usability. Recent advances in large language models (LLMs) have introduced the new research field of LLM-based log parsing, offering potential improvements in automation and adaptability. Despite promising results, there is no structured overview of these approaches since this is a relatively new research field with the earliest advances published in late 2023. This paper systematically reviews 29 LLM-based log parsing methods, comparing their capabilities, limitations, and reliance on manual effort. We analyze the learning and prompt-engineering paradigms employed, efficiency- and effectiveness-enhancing techniques, and the role of LLMs in the parsing process. We aggregate the results of the survey in a large table comprising the characterizing features of LLM-based log parsing approaches and derive the general process of LLM-based log parsing, incorporating all reviewed approaches in a single flow chart. Additionally, we benchmark seven open-source LLM-based log parsers on public datasets and critically assess their reproducibility. Our findings summarize the advances of this new research field and provide insights for researchers and practitioners seeking efficient and user-friendly log parsing solutions, with all code and results made publicly available for transparency.</li>
</ul>

<h3>Title: Leveraging Large Language Models for Cost-Effective, Multilingual Depression Detection and Severity Assessment</h3>
<ul>
<li><strong>Authors: </strong>Longdi Xian, Jianzhang Ni, Mingzhu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04891">https://arxiv.org/abs/2504.04891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04891">https://arxiv.org/pdf/2504.04891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04891]] Leveraging Large Language Models for Cost-Effective, Multilingual Depression Detection and Severity Assessment(https://arxiv.org/abs/2504.04891)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Depression is a prevalent mental health disorder that is difficult to detect early due to subjective symptom assessments. Recent advancements in large language models have offered efficient and cost-effective approaches for this objective. In this study, we evaluated the performance of four LLMs in depression detection using clinical interview data. We selected the best performing model and further tested it in the severity evaluation scenario and knowledge enhanced scenario. The robustness was evaluated in complex diagnostic scenarios using a dataset comprising 51074 statements from six different mental disorders. We found that DeepSeek V3 is the most reliable and cost-effective model for depression detection, performing well in both zero-shot and few-shot scenarios, with zero-shot being the most efficient choice. The evaluation of severity showed low agreement with the human evaluator, particularly for mild depression. The model maintains stably high AUCs for detecting depression in complex diagnostic scenarios. These findings highlight DeepSeek V3s strong potential for text-based depression detection in real-world clinical applications. However, they also underscore the need for further refinement in severity assessment and the mitigation of potential biases to enhance clinical reliability.</li>
</ul>

<h3>Title: SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Justus Westerhoff, Erblina Purellku, Jakob Hackstein, Leo Pinetzki, Lorenz Hufe</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04893">https://arxiv.org/abs/2504.04893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04893">https://arxiv.org/pdf/2504.04893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04893]] SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models(https://arxiv.org/abs/2504.04893)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Typographic attacks exploit the interplay between text and visual content in multimodal foundation models, causing misclassifications when misleading text is embedded within images. However, existing datasets are limited in size and diversity, making it difficult to study such vulnerabilities. In this paper, we introduce SCAM, the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words. Through extensive benchmarking of Vision-Language Models (VLMs) on SCAM, we demonstrate that typographic attacks significantly degrade performance, and identify that training data and model architecture influence the susceptibility to these attacks. Our findings reveal that typographic attacks persist in state-of-the-art Large Vision-Language Models (LVLMs) due to the choice of their vision encoder, though larger Large Language Models (LLMs) backbones help mitigate their vulnerability. Additionally, we demonstrate that synthetic attacks closely resemble real-world (handwritten) attacks, validating their use in research. Our work provides a comprehensive resource and empirical insights to facilitate future research toward robust and trustworthy multimodal AI systems. We publicly release the datasets introduced in this paper under this https URL, along with the code for evaluations at this https URL.</li>
</ul>

<h3>Title: Lumina-OmniLV: A Unified Multimodal Framework for General Low-Level Vision</h3>
<ul>
<li><strong>Authors: </strong>Yuandong Pu, Le Zhuo, Kaiwen Zhu, Liangbin Xie, Wenlong Zhang, Xiangyu Chen, Pneg Gao, Yu Qiao, Chao Dong, Yihao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04903">https://arxiv.org/abs/2504.04903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04903">https://arxiv.org/pdf/2504.04903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04903]] Lumina-OmniLV: A Unified Multimodal Framework for General Low-Level Vision(https://arxiv.org/abs/2504.04903)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>We present Lunima-OmniLV (abbreviated as OmniLV), a universal multimodal multi-task framework for low-level vision that addresses over 100 sub-tasks across four major categories: image restoration, image enhancement, weak-semantic dense prediction, and stylization. OmniLV leverages both textual and visual prompts to offer flexible and user-friendly interactions. Built on Diffusion Transformer (DiT)-based generative priors, our framework supports arbitrary resolutions -- achieving optimal performance at 1K resolution -- while preserving fine-grained details and high fidelity. Through extensive experiments, we demonstrate that separately encoding text and visual instructions, combined with co-training using shallow feature control, is essential to mitigate task ambiguity and enhance multi-task generalization. Our findings also reveal that integrating high-level generative tasks into low-level vision models can compromise detail-sensitive restoration. These insights pave the way for more robust and generalizable low-level vision systems.</li>
</ul>

<h3>Title: Video-Bench: Human-Aligned Video Generation Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Hui Han, Siyuan Li, Jiaqi Chen, Yiwen Yuan, Yuling Wu, Chak Tou Leong, Hanwen Du, Junchen Fu, Youhua Li, Jie Zhang, Chi Zhang, Li-jia Li, Yongxin Ni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04907">https://arxiv.org/abs/2504.04907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04907">https://arxiv.org/pdf/2504.04907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04907]] Video-Bench: Human-Aligned Video Generation Benchmark(https://arxiv.org/abs/2504.04907)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Video generation assessment is essential for ensuring that generative models produce visually realistic, high-quality videos while aligning with human expectations. Current video generation benchmarks fall into two main categories: traditional benchmarks, which use metrics and embeddings to evaluate generated video quality across multiple dimensions but often lack alignment with human judgments; and large language model (LLM)-based benchmarks, though capable of human-like reasoning, are constrained by a limited understanding of video quality metrics and cross-modal consistency. To address these challenges and establish a benchmark that better aligns with human preferences, this paper introduces Video-Bench, a comprehensive benchmark featuring a rich prompt suite and extensive evaluation dimensions. This benchmark represents the first attempt to systematically leverage MLLMs across all dimensions relevant to video generation assessment in generative models. By incorporating few-shot scoring and chain-of-query techniques, Video-Bench provides a structured, scalable approach to generated video evaluation. Experiments on advanced models including Sora demonstrate that Video-Bench achieves superior alignment with human preferences across all dimensions. Moreover, in instances where our framework's assessments diverge from human evaluations, it consistently offers more objective and accurate insights, suggesting an even greater potential advantage over traditional human judgment.</li>
</ul>

<h3>Title: IterMask3D: Unsupervised Anomaly Detection and Segmentation with Test-Time Iterative Mask Refinement in 3D Brain MR</h3>
<ul>
<li><strong>Authors: </strong>Ziyun Liang, Xiaoqing Guo, Wentian Xu, Yasin Ibrahim, Natalie Voets, Pieter M Pretorius, J. Alison Noble, Konstantinos Kamnitsas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04911">https://arxiv.org/abs/2504.04911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04911">https://arxiv.org/pdf/2504.04911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04911]] IterMask3D: Unsupervised Anomaly Detection and Segmentation with Test-Time Iterative Mask Refinement in 3D Brain MR(https://arxiv.org/abs/2504.04911)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Unsupervised anomaly detection and segmentation methods train a model to learn the training distribution as 'normal'. In the testing phase, they identify patterns that deviate from this normal distribution as 'anomalies'. To learn the `normal' distribution, prevailing methods corrupt the images and train a model to reconstruct them. During testing, the model attempts to reconstruct corrupted inputs based on the learned 'normal' distribution. Deviations from this distribution lead to high reconstruction errors, which indicate potential anomalies. However, corrupting an input image inevitably causes information loss even in normal regions, leading to suboptimal reconstruction and an increased risk of false positives. To alleviate this, we propose IterMask3D, an iterative spatial mask-refining strategy designed for 3D brain MRI. We iteratively spatially mask areas of the image as corruption and reconstruct them, then shrink the mask based on reconstruction error. This process iteratively unmasks 'normal' areas to the model, whose information further guides reconstruction of 'normal' patterns under the mask to be reconstructed accurately, reducing false positives. In addition, to achieve better reconstruction performance, we also propose using high-frequency image content as additional structural information to guide the reconstruction of the masked area. Extensive experiments on the detection of both synthetic and real-world imaging artifacts, as well as segmentation of various pathological lesions across multiple MRI sequences, consistently demonstrate the effectiveness of our proposed method.</li>
</ul>

<h3>Title: Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Ran Xu, Wenqi Shi, Yuchen Zhuang, Yue Yu, Joyce C. Ho, Haoyu Wang, Carl Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04915">https://arxiv.org/abs/2504.04915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04915">https://arxiv.org/pdf/2504.04915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04915]] Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration(https://arxiv.org/abs/2504.04915)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems often struggle to handle multi-hop question-answering tasks accurately due to irrelevant context retrieval and limited complex reasoning capabilities. We introduce Collab-RAG, a collaborative training framework that leverages mutual enhancement between a white-box small language model (SLM) and a blackbox large language model (LLM) for RAG. Specifically, the SLM decomposes complex queries into simpler sub-questions, thus enhancing the accuracy of the retrieval and facilitating more effective reasoning by the black-box LLM. Concurrently, the black-box LLM provides feedback signals to improve the SLM's decomposition capability. We observe that Collab-RAG relies solely on supervision from an affordable black-box LLM without additional distillation from frontier LLMs, yet demonstrates strong generalization across multiple black-box LLMs. Experimental evaluations across five multi-hop QA datasets demonstrate that Collab-RAG substantially outperforms existing black-box-only and SLM fine-tuning baselines by 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a frozen 32B LLM in question decomposition, highlighting the efficiency of Collab-RAG in improving reasoning and retrieval for complex questions. The code of Collab-RAG is available on this https URL.</li>
</ul>

<h3>Title: RCCFormer: A Robust Crowd Counting Network Based on Transformer</h3>
<ul>
<li><strong>Authors: </strong>Peng Liu, Heng-Chao Li, Sen Lei, Nanqing Liu, Bin Feng, Xiao Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04935">https://arxiv.org/abs/2504.04935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04935">https://arxiv.org/pdf/2504.04935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04935]] RCCFormer: A Robust Crowd Counting Network Based on Transformer(https://arxiv.org/abs/2504.04935)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Crowd counting, which is a key computer vision task, has emerged as a fundamental technology in crowd analysis and public safety management. However, challenges such as scale variations and complex backgrounds significantly impact the accuracy of crowd counting. To mitigate these issues, this paper proposes a robust Transformer-based crowd counting network, termed RCCFormer, specifically designed for background suppression and scale awareness. The proposed method incorporates a Multi-level Feature Fusion Module (MFFM), which meticulously integrates features extracted at diverse stages of the backbone architecture. It establishes a strong baseline capable of capturing intricate and comprehensive feature representations, surpassing traditional baselines. Furthermore, the introduced Detail-Embedded Attention Block (DEAB) captures contextual information and local details through global self-attention and local attention along with a learnable manner for efficient fusion. This enhances the model's ability to focus on foreground regions while effectively mitigating background noise interference. Additionally, we develop an Adaptive Scale-Aware Module (ASAM), with our novel Input-dependent Deformable Convolution (IDConv) as its fundamental building block. This module dynamically adapts to changes in head target shapes and scales, significantly improving the network's capability to accommodate large-scale variations. The effectiveness of the proposed method is validated on the ShanghaiTech Part_A and Part_B, NWPU-Crowd, and QNRF datasets. The results demonstrate that our RCCFormer achieves excellent performance across all four datasets, showcasing state-of-the-art outcomes.</li>
</ul>

<h3>Title: A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal Reasoning in the Multi-state Bar Exam</h3>
<ul>
<li><strong>Authors: </strong>Rean Fernandes, André Biedenkapp, Frank Hutter, Noor Awad</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04945">https://arxiv.org/abs/2504.04945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04945">https://arxiv.org/pdf/2504.04945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04945]] A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal Reasoning in the Multi-state Bar Exam(https://arxiv.org/abs/2504.04945)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Legal reasoning tasks present unique challenges for large language models (LLMs) due to the complexity of domain-specific knowledge and reasoning processes. This paper investigates how effectively smaller language models (Llama 2 7B and Llama 3 8B) can be fine-tuned with a limited dataset of 1,514 Multi-state Bar Examination (MBE) questions to improve legal question answering accuracy. We evaluate these models on the 2022 MBE questions licensed from JD Advising, the same dataset used in the 'GPT-4 passes the Bar exam' study. Our methodology involves collecting approximately 200 questions per legal domain across 7 domains. We distill the dataset using Llama 3 (70B) to transform explanations into a structured IRAC (Issue, Rule, Application, Conclusion) format as a guided reasoning process to see if it results in better performance over the non-distilled dataset. We compare the non-fine-tuned models against their supervised fine-tuned (SFT) counterparts, trained for different sample sizes per domain, to study the effect on accuracy and prompt adherence. We also analyse option selection biases and their mitigation following SFT. In addition, we consolidate the performance across multiple variables: prompt type (few-shot vs zero-shot), answer ordering (chosen-option first vs generated-explanation first), response format (Numbered list vs Markdown vs JSON), and different decoding temperatures. Our findings show that domain-specific SFT helps some model configurations achieve close to human baseline performance, despite limited computational resources and a relatively small dataset. We release both the gathered SFT dataset and the family of Supervised Fine-tuned (SFT) adapters optimised for MBE performance. This establishes a practical lower bound on resources needed towards achieving effective legal question answering in smaller LLMs.</li>
</ul>

<h3>Title: A Unified Pairwise Framework for RLHF: Bridging Generative Reward Modeling and Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Wenyuan Xu, Xiaochen Zuo, Chao Xin, Yu Yue, Lin Yan, Yonghui Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04950">https://arxiv.org/abs/2504.04950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04950">https://arxiv.org/pdf/2504.04950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04950]] A Unified Pairwise Framework for RLHF: Bridging Generative Reward Modeling and Policy Optimization(https://arxiv.org/abs/2504.04950)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) has emerged as a important paradigm for aligning large language models (LLMs) with human preferences during post-training. This framework typically involves two stages: first, training a reward model on human preference data, followed by optimizing the language model using reinforcement learning algorithms. However, current RLHF approaches may constrained by two limitations. First, existing RLHF frameworks often rely on Bradley-Terry models to assign scalar rewards based on pairwise comparisons of individual responses. However, this approach imposes significant challenges on reward model (RM), as the inherent variability in prompt-response pairs across different contexts demands robust calibration capabilities from the RM. Second, reward models are typically initialized from generative foundation models, such as pre-trained or supervised fine-tuned models, despite the fact that reward models perform discriminative tasks, creating a mismatch. This paper introduces Pairwise-RL, a RLHF framework that addresses these challenges through a combination of generative reward modeling and a pairwise proximal policy optimization (PPO) algorithm. Pairwise-RL unifies reward model training and its application during reinforcement learning within a consistent pairwise paradigm, leveraging generative modeling techniques to enhance reward model performance and score calibration. Experimental evaluations demonstrate that Pairwise-RL outperforms traditional RLHF frameworks across both internal evaluation datasets and standard public benchmarks, underscoring its effectiveness in improving alignment and model behavior.</li>
</ul>

<h3>Title: Constraint Multi-class Positive and Unlabeled Learning for Distantly Supervised Named Entity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yuzhe Zhang, Min Cen, Hong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04963">https://arxiv.org/abs/2504.04963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04963">https://arxiv.org/pdf/2504.04963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04963]] Constraint Multi-class Positive and Unlabeled Learning for Distantly Supervised Named Entity Recognition(https://arxiv.org/abs/2504.04963)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Distantly supervised named entity recognition (DS-NER) has been proposed to exploit the automatically labeled training data by external knowledge bases instead of human annotations. However, it tends to suffer from a high false negative rate due to the inherent incompleteness. To address this issue, we present a novel approach called \textbf{C}onstraint \textbf{M}ulti-class \textbf{P}ositive and \textbf{U}nlabeled Learning (CMPU), which introduces a constraint factor on the risk estimator of multiple positive classes. It suggests that the constraint non-negative risk estimator is more robust against overfitting than previous PU learning methods with limited positive data. Solid theoretical analysis on CMPU is provided to prove the validity of our approach. Extensive experiments on two benchmark datasets that were labeled using diverse external knowledge sources serve to demonstrate the superior performance of CMPU in comparison to existing DS-NER methods.</li>
</ul>

<h3>Title: Towards Visual Text Grounding of Multimodal Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Ming Li, Ruiyi Zhang, Jian Chen, Jiuxiang Gu, Yufan Zhou, Franck Dernoncourt, Wanrong Zhu, Tianyi Zhou, Tong Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04974">https://arxiv.org/abs/2504.04974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04974">https://arxiv.org/pdf/2504.04974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04974]] Towards Visual Text Grounding of Multimodal Large Language Model(https://arxiv.org/abs/2504.04974)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as a benchmark and a large-scale training set of 90$ synthetic data based on four diverse datasets. A comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images. In addition, we propose two simple and effective TRIG methods based on general instruction tuning and plug-and-play efficient embedding, respectively. By finetuning MLLMs on our synthetic dataset, they promisingly improve spatial reasoning and grounding capabilities.</li>
</ul>

<h3>Title: A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Carlos Peláez-González, Andrés Herrera-Poyatos, Cristina Zuheros, David Herrera-Poyatos, Virilo Tejedor, Francisco Herrera</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04976">https://arxiv.org/abs/2504.04976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04976">https://arxiv.org/pdf/2504.04976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04976]] A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language Models(https://arxiv.org/abs/2504.04976)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The study of large language models (LLMs) is a key area in open-world machine learning. Although LLMs demonstrate remarkable natural language processing capabilities, they also face several challenges, including consistency issues, hallucinations, and jailbreak vulnerabilities. Jailbreaking refers to the crafting of prompts that bypass alignment safeguards, leading to unsafe outputs that compromise the integrity of LLMs. This work specifically focuses on the challenge of jailbreak vulnerabilities and introduces a novel taxonomy of jailbreak attacks grounded in the training domains of LLMs. It characterizes alignment failures through generalization, objectives, and robustness gaps. Our primary contribution is a perspective on jailbreak, framed through the different linguistic domains that emerge during LLM training and alignment. This viewpoint highlights the limitations of existing approaches and enables us to classify jailbreak attacks on the basis of the underlying model deficiencies they exploit. Unlike conventional classifications that categorize attacks based on prompt construction methods (e.g., prompt templating), our approach provides a deeper understanding of LLM behavior. We introduce a taxonomy with four categories -- mismatched generalization, competing objectives, adversarial robustness, and mixed attacks -- offering insights into the fundamental nature of jailbreak vulnerabilities. Finally, we present key lessons derived from this taxonomic study.</li>
</ul>

<h3>Title: Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ling Hu, Yuemei Xu, Xiaoyang Gu, Letao Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.04994">https://arxiv.org/abs/2504.04994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.04994">https://arxiv.org/pdf/2504.04994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.04994]] Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs(https://arxiv.org/abs/2504.04994)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Despite the impressive performance of large language models (LLMs), they can present unintended biases and harmful behaviors driven by encoded values, emphasizing the urgent need to understand the value mechanisms behind them. However, current research primarily evaluates these values through external responses with a focus on AI safety, lacking interpretability and failing to assess social values in real-world contexts. In this paper, we propose a novel framework called ValueExploration, which aims to explore the behavior-driven mechanisms of National Social Values within LLMs at the neuron level. As a case study, we focus on Chinese Social Values and first construct C-voice, a large-scale bilingual benchmark for identifying and evaluating Chinese Social Values in LLMs. By leveraging C-voice, we then identify and locate the neurons responsible for encoding these values according to activation difference. Finally, by deactivating these neurons, we analyze shifts in model behavior, uncovering the internal mechanism by which values influence LLM decision-making. Extensive experiments on four representative LLMs validate the efficacy of our framework. The benchmark and code will be available.</li>
</ul>

<h3>Title: SmartBugBert: BERT-Enhanced Vulnerability Detection for Smart Contract Bytecode</h3>
<ul>
<li><strong>Authors: </strong>Jiuyang Bu, Wenkai Li, Zongwei Li, Zeng Zhang, Xiaoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05002">https://arxiv.org/abs/2504.05002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05002">https://arxiv.org/pdf/2504.05002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05002]] SmartBugBert: BERT-Enhanced Vulnerability Detection for Smart Contract Bytecode(https://arxiv.org/abs/2504.05002)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Smart contracts deployed on blockchain platforms are vulnerable to various security vulnerabilities. However, only a small number of Ethereum contracts have released their source code, so vulnerability detection at the bytecode level is crucial. This paper introduces SmartBugBert, a novel approach that combines BERT-based deep learning with control flow graph (CFG) analysis to detect vulnerabilities directly from bytecode. Our method first decompiles smart contract bytecode into optimized opcode sequences, extracts semantic features using TF-IDF, constructs control flow graphs to capture execution logic, and isolates vulnerable CFG fragments for targeted analysis. By integrating both semantic and structural information through a fine-tuned BERT model and LightGBM classifier, our approach effectively identifies four critical vulnerability types: transaction-ordering, access control, self-destruct, and timestamp dependency vulnerabilities. Experimental evaluation on 6,157 Ethereum smart contracts demonstrates that SmartBugBert achieves 90.62% precision, 91.76% recall, and 91.19% F1-score, significantly outperforming existing detection methods. Ablation studies confirm that the combination of semantic features with CFG information substantially enhances detection performance. Furthermore, our approach maintains efficient detection speed (0.14 seconds per contract), making it practical for large-scale vulnerability assessment.</li>
</ul>

<h3>Title: Enhancing Smart Contract Vulnerability Detection in DApps Leveraging Fine-Tuned LLM</h3>
<ul>
<li><strong>Authors: </strong>Jiuyang Bu, Wenkai Li, Zongwei Li, Zeng Zhang, Xiaoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05006">https://arxiv.org/abs/2504.05006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05006">https://arxiv.org/pdf/2504.05006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05006]] Enhancing Smart Contract Vulnerability Detection in DApps Leveraging Fine-Tuned LLM(https://arxiv.org/abs/2504.05006)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, robust, large language model</a></li>
<li><strong>Abstract: </strong>Decentralized applications (DApps) face significant security risks due to vulnerabilities in smart contracts, with traditional detection methods struggling to address emerging and machine-unauditable flaws. This paper proposes a novel approach leveraging fine-tuned Large Language Models (LLMs) to enhance smart contract vulnerability detection. We introduce a comprehensive dataset of 215 real-world DApp projects (4,998 contracts), including hard-to-detect logical errors like token price manipulation, addressing the limitations of existing simplified benchmarks. By fine-tuning LLMs (Llama3-8B and Qwen2-7B) with Full-Parameter Fine-Tuning (FFT) and Low-Rank Adaptation (LoRA), our method achieves superior performance, attaining an F1-score of 0.83 with FFT and data augmentation via Random Over Sampling (ROS). Comparative experiments demonstrate significant improvements over prompt-based LLMs and state-of-the-art tools. Notably, the approach excels in detecting non-machine-auditable vulnerabilities, achieving 0.97 precision and 0.68 recall for price manipulation flaws. The results underscore the effectiveness of domain-specific LLM fine-tuning and data augmentation in addressing real-world DApp security challenges, offering a robust solution for blockchain ecosystem protection.</li>
</ul>

<h3>Title: Surveying Professional Writers on AI: Limitations, Expectations, and Fears</h3>
<ul>
<li><strong>Authors: </strong>Anastasiia Ivanova, Natalia Fedorova, Sergey Tilga, Ekaterina Artemova</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05008">https://arxiv.org/abs/2504.05008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05008">https://arxiv.org/pdf/2504.05008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05008]] Surveying Professional Writers on AI: Limitations, Expectations, and Fears(https://arxiv.org/abs/2504.05008)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid development of AI-driven tools, particularly large language models (LLMs), is reshaping professional writing. Still, key aspects of their adoption such as languages support, ethics, and long-term impact on writers voice and creativity remain underexplored. In this work, we conducted a questionnaire (N = 301) and an interactive survey (N = 36) targeting professional writers regularly using AI. We examined LLM-assisted writing practices across 25+ languages, ethical concerns, and user expectations. The findings of the survey demonstrate important insights, reflecting upon the importance of: LLMs adoption for non-English speakers; the degree of misinformation, domain and style adaptation; usability and key features of LLMs. These insights can guide further development, benefiting both writers and a broader user base.</li>
</ul>

<h3>Title: Mixture-of-Personas Language Models for Population Simulation</h3>
<ul>
<li><strong>Authors: </strong>Ngoc Bui, Hieu Trung Nguyen, Shantanu Kumar, Julian Theodore, Weikang Qiu, Viet Anh Nguyen, Rex Ying</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05019">https://arxiv.org/abs/2504.05019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05019">https://arxiv.org/pdf/2504.05019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05019]] Mixture-of-Personas Language Models for Population Simulation(https://arxiv.org/abs/2504.05019)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Advances in Large Language Models (LLMs) paved the way for their emerging applications in various domains, such as human behavior simulations, where LLMs could augment human-generated data in social science research and machine learning model training. However, pretrained LLMs often fail to capture the behavioral diversity of target populations due to the inherent variability across individuals and groups. To address this, we propose \textit{Mixture of Personas} (MoP), a \textit{probabilistic} prompting method that aligns the LLM responses with the target population. MoP is a contextual mixture model, where each component is an LM agent characterized by a persona and an exemplar representing subpopulation behaviors. The persona and exemplar are randomly chosen according to the learned mixing weights to elicit diverse LLM responses during simulation. MoP is flexible, requires no model finetuning, and is transferable across base models. Experiments for synthetic data generation show that MoP outperforms competing methods in alignment and diversity metrics.</li>
</ul>

<h3>Title: Batch Aggregation: An Approach to Enhance Text Classification with Correlated Augmented Data</h3>
<ul>
<li><strong>Authors: </strong>Charco Hui, Yalu Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05020">https://arxiv.org/abs/2504.05020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05020">https://arxiv.org/pdf/2504.05020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05020]] Batch Aggregation: An Approach to Enhance Text Classification with Correlated Augmented Data(https://arxiv.org/abs/2504.05020)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Natural language processing models often face challenges due to limited labeled data, especially in domain specific areas, e.g., clinical trials. To overcome this, text augmentation techniques are commonly used to increases sample size by transforming the original input data into artificial ones with the label preserved. However, traditional text classification methods ignores the relationship between augmented texts and treats them as independent samples which may introduce classification error. Therefore, we propose a novel approach called 'Batch Aggregation' (BAGG) which explicitly models the dependence of text inputs generated through augmentation by incorporating an additional layer that aggregates results from correlated texts. Through studying multiple benchmark data sets across different domains, we found that BAGG can improve classification accuracy. We also found that the increase of performance with BAGG is more obvious in domain specific data sets, with accuracy improvements of up to 10-29%. Through the analysis of benchmark data, the proposed method addresses limitations of traditional techniques and improves robustness in text classification tasks. Our result demonstrates that BAGG offers more robust results and outperforms traditional approaches when training data is limited.</li>
</ul>

<h3>Title: Concept Extraction for Time Series with ECLAD-ts</h3>
<ul>
<li><strong>Authors: </strong>Antonia Holzapfel, Andres Felipe Posada-Moreno, Sebastian Trimpe</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05024">https://arxiv.org/abs/2504.05024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05024">https://arxiv.org/pdf/2504.05024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05024]] Concept Extraction for Time Series with ECLAD-ts(https://arxiv.org/abs/2504.05024)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Convolutional neural networks (CNNs) for time series classification (TSC) are being increasingly used in applications ranging from quality prediction to medical diagnosis. The black box nature of these models makes understanding their prediction process difficult. This issue is crucial because CNNs are prone to learning shortcuts and biases, compromising their robustness and alignment with human expectations. To assess whether such mechanisms are being used and the associated risk, it is essential to provide model explanations that reflect the inner workings of the model. Concept Extraction (CE) methods offer such explanations, but have mostly been developed for the image domain so far, leaving a gap in the time series domain. In this work, we present a CE and localization method tailored to the time series domain, based on the ideas of CE methods for images. We propose the novel method ECLAD-ts, which provides post-hoc global explanations based on how the models encode subsets of the input at different levels of abstraction. For this, concepts are produced by clustering timestep-wise aggregations of CNN activation maps, and their importance is computed based on their impact on the prediction process. We evaluate our method on synthetic and natural datasets. Furthermore, we assess the advantages and limitations of CE in time series through empirical results. Our results show that ECLAD-ts effectively explains models by leveraging their internal representations, providing useful insights about their prediction process.</li>
</ul>

<h3>Title: Multi-level Neural Networks for high-dimensional parametric obstacle problems</h3>
<ul>
<li><strong>Authors: </strong>Martin Eigel, Cosmas Heiß, Janina E. Schütte</a></li>
<li><strong>Subjects: </strong>cs.LG, math.FA, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05026">https://arxiv.org/abs/2504.05026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05026">https://arxiv.org/pdf/2504.05026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05026]] Multi-level Neural Networks for high-dimensional parametric obstacle problems(https://arxiv.org/abs/2504.05026)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>A new method to solve computationally challenging (random) parametric obstacle problems is developed and analyzed, where the parameters can influence the related partial differential equation (PDE) and determine the position and surface structure of the obstacle. As governing equation, a stationary elliptic diffusion problem is assumed. The high-dimensional solution of the obstacle problem is approximated by a specifically constructed convolutional neural network (CNN). This novel algorithm is inspired by a finite element constrained multigrid algorithm to represent the parameter to solution map. This has two benefits: First, it allows for efficient practical computations since multi-level data is used as an explicit output of the NN thanks to an appropriate data preprocessing. This improves the efficacy of the training process and subsequently leads to small errors in the natural energy norm. Second, the comparison of the CNN to a multigrid algorithm provides means to carry out a complete a priori convergence and complexity analysis of the proposed NN architecture. Numerical experiments illustrate a state-of-the-art performance for this challenging problem.</li>
</ul>

<h3>Title: AsyReC: A Multimodal Graph-based Framework for Spatio-Temporal Asymmetric Dyadic Relationship Classification</h3>
<ul>
<li><strong>Authors: </strong>Wang Tang, Fethiye Irmak Dogan, Linbo Qing, Hatice Gunes</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05030">https://arxiv.org/abs/2504.05030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05030">https://arxiv.org/pdf/2504.05030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05030]] AsyReC: A Multimodal Graph-based Framework for Spatio-Temporal Asymmetric Dyadic Relationship Classification(https://arxiv.org/abs/2504.05030)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Dyadic social relationships, which refer to relationships between two individuals who know each other through repeated interactions (or not), are shaped by shared spatial and temporal experiences. Current computational methods for modeling these relationships face three major challenges: (1) the failure to model asymmetric relationships, e.g., one individual may perceive the other as a friend while the other perceives them as an acquaintance, (2) the disruption of continuous interactions by discrete frame sampling, which segments the temporal continuity of interaction in real-world scenarios, and (3) the limitation to consider periodic behavioral cues, such as rhythmic vocalizations or recurrent gestures, which are crucial for inferring the evolution of dyadic relationships. To address these challenges, we propose AsyReC, a multimodal graph-based framework for asymmetric dyadic relationship classification, with three core innovations: (i) a triplet graph neural network with node-edge dual attention that dynamically weights multimodal cues to capture interaction asymmetries (addressing challenge 1); (ii) a clip-level relationship learning architecture that preserves temporal continuity, enabling fine-grained modeling of real-world interaction dynamics (addressing challenge 2); and (iii) a periodic temporal encoder that projects time indices onto sine/cosine waveforms to model recurrent behavioral patterns (addressing challenge 3). Extensive experiments on two public datasets demonstrate state-of-the-art performance, while ablation studies validate the critical role of asymmetric interaction modeling and periodic temporal encoding in improving the robustness of dyadic relationship classification in real-world scenarios. Our code is publicly available at: this https URL.</li>
</ul>

<h3>Title: InstructionBench: An Instructional Video Understanding Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Haiwan Wei, Yitian Yuan, Xiaohan Lan, Wei Ke, Lin Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05040">https://arxiv.org/abs/2504.05040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05040">https://arxiv.org/pdf/2504.05040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05040]] InstructionBench: An Instructional Video Understanding Benchmark(https://arxiv.org/abs/2504.05040)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite progress in video large language models (Video-LLMs), research on instructional video understanding, crucial for enhancing access to instructional content, remains insufficient. To address this, we introduce InstructionBench, an Instructional video understanding Benchmark, which challenges models' advanced temporal reasoning within instructional videos characterized by their strict step-by-step flow. Employing GPT-4, we formulate Q\&A pairs in open-ended and multiple-choice formats to assess both Coarse-Grained event-level and Fine-Grained object-level reasoning. Our filtering strategies exclude questions answerable purely by common-sense knowledge, focusing on visual perception and analysis when evaluating Video-LLM models. The benchmark finally contains 5k questions across over 700 videos. We evaluate the latest Video-LLMs on our InstructionBench, finding that closed-source models outperform open-source ones. However, even the best model, GPT-4o, achieves only 53.42\% accuracy, indicating significant gaps in temporal reasoning. To advance the field, we also develop a comprehensive instructional video dataset with over 19k Q\&A pairs from nearly 2.5k videos, using an automated data generation framework, thereby enriching the community's research resources.</li>
</ul>

<h3>Title: CMaP-SAM: Contraction Mapping Prior for SAM-driven Few-shot Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shuai Chen, Fanman Meng, Haoran Wei, Chenhao Wu, Qingbo Wu, Linfeng Xu, Hongliang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05049">https://arxiv.org/abs/2504.05049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05049">https://arxiv.org/pdf/2504.05049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05049]] CMaP-SAM: Contraction Mapping Prior for SAM-driven Few-shot Segmentation(https://arxiv.org/abs/2504.05049)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Few-shot segmentation (FSS) aims to segment new classes using few annotated images. While recent FSS methods have shown considerable improvements by leveraging Segment Anything Model (SAM), they face two critical limitations: insufficient utilization of structural correlations in query images, and significant information loss when converting continuous position priors to discrete point prompts. To address these challenges, we propose CMaP-SAM, a novel framework that introduces contraction mapping theory to optimize position priors for SAM-driven few-shot segmentation. CMaP-SAM consists of three key components: (1) a contraction mapping module that formulates position prior optimization as a Banach contraction mapping with convergence guarantees. This module iteratively refines position priors through pixel-wise structural similarity, generating a converged prior that preserves both semantic guidance from reference images and structural correlations in query images; (2) an adaptive distribution alignment module bridging continuous priors with SAM's binary mask prompt encoder; and (3) a foreground-background decoupled refinement architecture producing accurate final segmentation masks. Extensive experiments demonstrate CMaP-SAM's effectiveness, achieving state-of-the-art performance with 71.1 mIoU on PASCAL-$5^i$ and 56.1 on COCO-$20^i$ datasets.</li>
</ul>

<h3>Title: Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05050">https://arxiv.org/abs/2504.05050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05050">https://arxiv.org/pdf/2504.05050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05050]] Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models(https://arxiv.org/abs/2504.05050)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible "dark patterns" in LLMs' parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local "safety regions" in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shifts--a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities.</li>
</ul>

<h3>Title: Not All Data Are Unlearned Equally</h3>
<ul>
<li><strong>Authors: </strong>Aravind Krishnan, Siva Reddy, Marius Mosbach</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05058">https://arxiv.org/abs/2504.05058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05058">https://arxiv.org/pdf/2504.05058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05058]] Not All Data Are Unlearned Equally(https://arxiv.org/abs/2504.05058)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Machine unlearning is concerned with the task of removing knowledge learned from particular data points from a trained model. In the context of large language models (LLMs), unlearning has recently received increased attention, particularly for removing knowledge about named entities from models for privacy purposes. While various approaches have been proposed to address the unlearning problem, most existing approaches treat all data points to be unlearned equally, i.e., unlearning that Montreal is a city in Canada is treated exactly the same as unlearning the phone number of the first author of this paper. In this work, we show that this all data is equal assumption does not hold for LLM unlearning. We study how the success of unlearning depends on the frequency of the knowledge we want to unlearn in the pre-training data of a model and find that frequency strongly affects unlearning, i.e., more frequent knowledge is harder to unlearn. Additionally, we uncover a misalignment between probability and generation-based evaluations of unlearning and show that this problem worsens as models become larger. Overall, our experiments highlight the need for better evaluation practices and novel methods for LLM unlearning that take the training data of models into account.</li>
</ul>

<h3>Title: MIAT: Maneuver-Intention-Aware Transformer for Spatio-Temporal Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Chandra Raskoti, Iftekharul Islam, Xuan Wang, Weizi Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05059">https://arxiv.org/abs/2504.05059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05059">https://arxiv.org/pdf/2504.05059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05059]] MIAT: Maneuver-Intention-Aware Transformer for Spatio-Temporal Trajectory Prediction(https://arxiv.org/abs/2504.05059)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate vehicle trajectory prediction is critical for safe and efficient autonomous driving, especially in mixed traffic environments with both human-driven and autonomous vehicles. However, uncertainties introduced by inherent driving behaviors -- such as acceleration, deceleration, and left and right maneuvers -- pose significant challenges for reliable trajectory prediction. We introduce a Maneuver-Intention-Aware Transformer (MIAT) architecture, which integrates a maneuver intention awareness mechanism with spatiotemporal interaction modeling to enhance long-horizon trajectory predictions. We systematically investigate the impact of varying awareness of maneuver intention on both short- and long-horizon trajectory predictions. Evaluated on the real-world NGSIM dataset and benchmarked against various transformer- and LSTM-based methods, our approach achieves an improvement of up to 4.7% in short-horizon predictions and a 1.6% in long-horizon predictions compared to other intention-aware benchmark methods. Moreover, by leveraging an intention awareness control mechanism, MIAT realizes an 11.1% performance boost in long-horizon predictions, with a modest drop in short-horizon performance.</li>
</ul>

<h3>Title: LDGNet: A Lightweight Difference Guiding Network for Remote Sensing Change Detection</h3>
<ul>
<li><strong>Authors: </strong>Chenfeng Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05062">https://arxiv.org/abs/2504.05062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05062">https://arxiv.org/pdf/2504.05062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05062]] LDGNet: A Lightweight Difference Guiding Network for Remote Sensing Change Detection(https://arxiv.org/abs/2504.05062)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of deep learning, the field of change detection (CD) in remote sensing imagery has achieved remarkable progress. Existing change detection methods primarily focus on achieving higher accuracy with increased computational costs and parameter sizes, leaving development of lightweight methods for rapid real-world processing an underexplored challenge. To address this challenge, we propose a Lightweight Difference Guiding Network (LDGNet), leveraging absolute difference image to guide optical remote sensing change detection. First, to enhance the feature representation capability of the lightweight backbone network, we propose the Difference Guiding Module (DGM), which leverages multi-scale features extracted from the absolute difference image to progressively influence the original image encoder at each layer, thereby reinforcing feature extraction. Second, we propose the Difference-Aware Dynamic Fusion (DADF) module with Visual State Space Model (VSSM) for lightweight long-range dependency modeling. The module first uses feature absolute differences to guide VSSM's global contextual modeling of change regions, then employs difference attention to dynamically fuse these long-range features with feature differences, enhancing change semantics while suppressing noise and background. Extensive experiments on multiple datasets demonstrate that our method achieves comparable or superior performance to current state-of-the-art (SOTA) methods requiring several times more computation, while maintaining only 3.43M parameters and 1.12G FLOPs.</li>
</ul>

<h3>Title: On the Performance of an Explainable Language Model on PubMedQA</h3>
<ul>
<li><strong>Authors: </strong>Venkat Srinivasan, Vishaal Jatav, Anushka Chandrababu, Geetika Sharma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05074">https://arxiv.org/abs/2504.05074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05074">https://arxiv.org/pdf/2504.05074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05074]] On the Performance of an Explainable Language Model on PubMedQA(https://arxiv.org/abs/2504.05074)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown significant abilities in retrieving medical knowledge, reasoning over it and answering medical questions comparably to physicians. However, these models are not interpretable, hallucinate, are difficult to maintain and require enormous compute resources for training and inference. In this paper, we report results from Gyan, an explainable language model based on an alternative architecture, on the PubmedQA data set. The Gyan LLM is a compositional language model and the model is decoupled from knowledge. Gyan is trustable, transparent, does not hallucinate and does not require significant training or compute resources. Gyan is easily transferable across domains. Gyan-4.3 achieves SOTA results on PubmedQA with 87.1% accuracy compared to 82% by MedPrompt based on GPT-4 and 81.8% by Med-PaLM 2 (Google and DeepMind). We will be reporting results for other medical data sets - MedQA, MedMCQA, MMLU - Medicine in the future.</li>
</ul>

<h3>Title: Content-Distortion High-Order Interaction for Blind Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Shuai Liu, Qingyu Mao, Chao Li, Jiacong Chen, Fanyang Meng, Yonghong Tian, Yongsheng Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05076">https://arxiv.org/abs/2504.05076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05076">https://arxiv.org/pdf/2504.05076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05076]] Content-Distortion High-Order Interaction for Blind Image Quality Assessment(https://arxiv.org/abs/2504.05076)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The content and distortion are widely recognized as the two primary factors affecting the visual quality of an image. While existing No-Reference Image Quality Assessment (NR-IQA) methods have modeled these factors, they fail to capture the complex interactions between content and distortions. This shortfall impairs their ability to accurately perceive quality. To confront this, we analyze the key properties required for interaction modeling and propose a robust NR-IQA approach termed CoDI-IQA (Content-Distortion high-order Interaction for NR-IQA), which aggregates local distortion and global content features within a hierarchical interaction framework. Specifically, a Progressive Perception Interaction Module (PPIM) is proposed to explicitly simulate how content and distortions independently and jointly influence image quality. By integrating internal interaction, coarse interaction, and fine interaction, it achieves high-order interaction modeling that allows the model to properly represent the underlying interaction patterns. To ensure sufficient interaction, multiple PPIMs are employed to hierarchically fuse multi-level content and distortion features at different granularities. We also tailor a training strategy suited for CoDI-IQA to maintain interaction stability. Extensive experiments demonstrate that the proposed method notably outperforms the state-of-the-art methods in terms of prediction accuracy, data efficiency, and generalization ability.</li>
</ul>

<h3>Title: The Curse of CoT: On the Limitations of Chain-of-Thought in In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Tianshi Zheng, Yixiang Chen, Chengxi Li, Chunyang Li, Qing Zong, Haochen Shi, Baixuan Xu, Yangqiu Song, Ginny Y. Wong, Simon See</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05081">https://arxiv.org/abs/2504.05081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05081">https://arxiv.org/pdf/2504.05081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05081]] The Curse of CoT: On the Limitations of Chain-of-Thought in In-Context Learning(https://arxiv.org/abs/2504.05081)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) prompting has been widely recognized for its ability to enhance reasoning capabilities in large language models (LLMs) through the generation of explicit explanatory rationales. However, our study reveals a surprising contradiction to this prevailing perspective. Through extensive experiments involving 16 state-of-the-art LLMs and nine diverse pattern-based in-context learning (ICL) datasets, we demonstrate that CoT and its reasoning variants consistently underperform direct answering across varying model scales and benchmark complexities. To systematically investigate this unexpected phenomenon, we designed extensive experiments to validate several hypothetical explanations. Our analysis uncovers a fundamental explicit-implicit duality driving CoT's performance in pattern-based ICL: while explicit reasoning falters due to LLMs' struggles to infer underlying patterns from demonstrations, implicit reasoning-disrupted by the increased contextual distance of CoT rationales-often compensates, delivering correct answers despite flawed rationales. This duality explains CoT's relative underperformance, as noise from weak explicit inference undermines the process, even as implicit mechanisms partially salvage outcomes. Notably, even long-CoT reasoning models, which excel in abstract and symbolic reasoning, fail to fully overcome these limitations despite higher computational costs. Our findings challenge existing assumptions regarding the universal efficacy of CoT, yielding novel insights into its limitations and guiding future research toward more nuanced and effective reasoning methodologies for LLMs.</li>
</ul>

<h3>Title: AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for Early Warning System Investments</h3>
<ul>
<li><strong>Authors: </strong>Saeid Ario Vaghefi, Aymane Hachcham, Veronica Grasso, Jiska Manicus, Nakiete Msemo, Chiara Colesanti Senni, Markus Leippold</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05104">https://arxiv.org/abs/2504.05104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05104">https://arxiv.org/pdf/2504.05104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05104]] AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for Early Warning System Investments(https://arxiv.org/abs/2504.05104)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Tracking financial investments in climate adaptation is a complex and expertise-intensive task, particularly for Early Warning Systems (EWS), which lack standardized financial reporting across multilateral development banks (MDBs) and funds. To address this challenge, we introduce an LLM-based agentic AI system that integrates contextual retrieval, fine-tuning, and multi-step reasoning to extract relevant financial data, classify investments, and ensure compliance with funding guidelines. Our study focuses on a real-world application: tracking EWS investments in the Climate Risk and Early Warning Systems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple AI-driven classification methods, including zero-shot and few-shot learning, fine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and an agent-based retrieval-augmented generation (RAG) approach. Our results show that the agent-based RAG approach significantly outperforms other methods, achieving 87\% accuracy, 89\% precision, and 83\% recall. Additionally, we contribute a benchmark dataset and expert-annotated corpus, providing a valuable resource for future research in AI-driven financial tracking and climate finance transparency.</li>
</ul>

<h3>Title: ABCDWaveNet: Advancing Robust Road Ponding Detection in Fog through Dynamic Frequency-Spatial Synergy</h3>
<ul>
<li><strong>Authors: </strong>Ronghui Zhang, Dakang Lyu, Tengfei Li, Yunfan Wu, Ujjal Manandhar, Benfei Wang, Junzhou Chen, Bolin Gao, Danwei Wang, Yiqiu Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05112">https://arxiv.org/abs/2504.05112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05112">https://arxiv.org/pdf/2504.05112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05112]] ABCDWaveNet: Advancing Robust Road Ponding Detection in Fog through Dynamic Frequency-Spatial Synergy(https://arxiv.org/abs/2504.05112)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Road ponding presents a significant threat to vehicle safety, particularly in adverse fog conditions, where reliable detection remains a persistent challenge for Advanced Driver Assistance Systems (ADAS). To address this, we propose ABCDWaveNet, a novel deep learning framework leveraging Dynamic Frequency-Spatial Synergy for robust ponding detection in fog. The core of ABCDWaveNet achieves this synergy by integrating dynamic convolution for adaptive feature extraction across varying visibilities with a wavelet-based module for synergistic frequency-spatial feature enhancement, significantly improving robustness against fog interference. Building on this foundation, ABCDWaveNet captures multi-scale structural and contextual information, subsequently employing an Adaptive Attention Coupling Gate (AACG) to adaptively fuse global and local features for enhanced accuracy. To facilitate realistic evaluations under combined adverse conditions, we introduce the Foggy Low-Light Puddle dataset. Extensive experiments demonstrate that ABCDWaveNet establishes new state-of-the-art performance, achieving significant Intersection over Union (IoU) gains of 3.51%, 1.75%, and 1.03% on the Foggy-Puddle, Puddle-1000, and our Foggy Low-Light Puddle datasets, respectively. Furthermore, its processing speed of 25.48 FPS on an NVIDIA Jetson AGX Orin confirms its suitability for ADAS deployment. These findings underscore the effectiveness of the proposed Dynamic Frequency-Spatial Synergy within ABCDWaveNet, offering valuable insights for developing proactive road safety solutions capable of operating reliably in challenging weather conditions.</li>
</ul>

<h3>Title: Balancing Robustness and Efficiency in Embedded DNNs Through Activation Function Selection</h3>
<ul>
<li><strong>Authors: </strong>Jon Gutiérrez Zaballa, Koldo Basterretxea, Javier Echanobe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.AR, cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05119">https://arxiv.org/abs/2504.05119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05119">https://arxiv.org/pdf/2504.05119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05119]] Balancing Robustness and Efficiency in Embedded DNNs Through Activation Function Selection(https://arxiv.org/abs/2504.05119)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Machine learning-based embedded systems for safety-critical applications, such as aerospace and autonomous driving, must be robust to perturbations caused by soft errors. As transistor geometries shrink and voltages decrease, modern electronic devices become more susceptible to background radiation, increasing the concern about failures produced by soft errors. The resilience of deep neural networks (DNNs) to these errors depends not only on target device technology but also on model structure and the numerical representation and arithmetic precision of their parameters. Compression techniques like pruning and quantization, used to reduce memory footprint and computational complexity, alter both model structure and representation, affecting soft error robustness. In this regard, although often overlooked, the choice of activation functions (AFs) impacts not only accuracy and trainability but also compressibility and error resilience. This paper explores the use of bounded AFs to enhance robustness against parameter perturbations, while evaluating their effects on model accuracy, compressibility, and computational load with a technology-agnostic approach. We focus on encoder-decoder convolutional models developed for semantic segmentation of hyperspectral images with application to autonomous driving systems. Experiments are conducted on an AMD-Xilinx's KV260 SoM.</li>
</ul>

<h3>Title: DoCIA: An Online Document-Level Context Incorporation Agent for Speech Translation</h3>
<ul>
<li><strong>Authors: </strong>Xinglin Lyu, Wei Tang, Yuang Li, Xiaofeng Zhao, Ming Zhu, Junhui Li, Yunfei Lu, Min Zhang, Daimeng Wei, Hao Yang, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05122">https://arxiv.org/abs/2504.05122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05122">https://arxiv.org/pdf/2504.05122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05122]] DoCIA: An Online Document-Level Context Incorporation Agent for Speech Translation(https://arxiv.org/abs/2504.05122)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Document-level context is crucial for handling discourse challenges in text-to-text document-level machine translation (MT). Despite the increased discourse challenges introduced by noise from automatic speech recognition (ASR), the integration of document-level context in speech translation (ST) remains insufficiently explored. In this paper, we develop DoCIA, an online framework that enhances ST performance by incorporating document-level context. DoCIA decomposes the ST pipeline into four stages. Document-level context is integrated into the ASR refinement, MT, and MT refinement stages through auxiliary LLM (large language model)-based modules. Furthermore, DoCIA leverages document-level information in a multi-level manner while minimizing computational overhead. Additionally, a simple yet effective determination mechanism is introduced to prevent hallucinations from excessive refinement, ensuring the reliability of the final results. Experimental results show that DoCIA significantly outperforms traditional ST baselines in both sentence and discourse metrics across four LLMs, demonstrating its effectiveness in improving ST performance.</li>
</ul>

<h3>Title: Interpretable Style Takagi-Sugeno-Kang Fuzzy Clustering</h3>
<ul>
<li><strong>Authors: </strong>Suhang Gu, Ye Wang, Yongxin Chou, Jinliang Cong, Mingli Lu, Zhuqing Jiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05125">https://arxiv.org/abs/2504.05125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05125">https://arxiv.org/pdf/2504.05125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05125]] Interpretable Style Takagi-Sugeno-Kang Fuzzy Clustering(https://arxiv.org/abs/2504.05125)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Clustering is an efficient and essential technique for exploring latent knowledge of data. However, limited attention has been given to the interpretability of the clusters detected by most clustering algorithms. In addition, due to the homogeneity of data, different groups of data have their own homogeneous styles. In this paper, the above two aspects are considered, and an interpretable style Takagi-Sugeno-Kang (TSK) fuzzy clustering (IS-TSK-FC) algorithm is proposed. The clustering behavior of IS-TSK-FC is fully guided by the TSK fuzzy inference on fuzzy rules. In particular, samples are grouped into clusters represented by the corresponding consequent vectors of all fuzzy rules learned in an unsupervised manner. This can explain how the clusters are generated in detail, thus making the underlying decision-making process of the IS-TSK-FC interpretable. Moreover, a series of style matrices are introduced to facilitate the consequents of fuzzy rules in IS-TSK-FC by capturing the styles of clusters as well as the nuances between different styles. Consequently, all the fuzzy rules in IS-TSK-FC have powerful data representation capability. After determining the antecedents of all the fuzzy rules, the optimization problem of IS-TSK-FC can be iteratively solved in an alternation manner. The effectiveness of IS-TSK-FC as an interpretable clustering tool is validated through extensive experiments on benchmark datasets with unknown implicit/explicit styles. Specially, the superior clustering performance of IS-TSK-FC is demonstrated on case studies where different groups of data present explicit styles. The source code of IS-TSK-FC can be downloaded from this https URL.</li>
</ul>

<h3>Title: DA2Diff: Exploring Degradation-aware Adaptive Diffusion Priors for All-in-One Weather Restoration</h3>
<ul>
<li><strong>Authors: </strong>Jiamei Xiong, Xuefeng Yan, Yongzhen Wang, Wei Zhao, Xiao-Ping Zhang, Mingqiang Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05135">https://arxiv.org/abs/2504.05135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05135">https://arxiv.org/pdf/2504.05135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05135]] DA2Diff: Exploring Degradation-aware Adaptive Diffusion Priors for All-in-One Weather Restoration(https://arxiv.org/abs/2504.05135)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image restoration under adverse weather conditions is a critical task for many vision-based applications. Recent all-in-one frameworks that handle multiple weather degradations within a unified model have shown potential. However, the diversity of degradation patterns across different weather conditions, as well as the complex and varied nature of real-world degradations, pose significant challenges for multiple weather removal. To address these challenges, we propose an innovative diffusion paradigm with degradation-aware adaptive priors for all-in-one weather restoration, termed DA2Diff. It is a new exploration that applies CLIP to perceive degradation-aware properties for better multi-weather restoration. Specifically, we deploy a set of learnable prompts to capture degradation-aware representations by the prompt-image similarity constraints in the CLIP space. By aligning the snowy/hazy/rainy images with snow/haze/rain prompts, each prompt contributes to different weather degradation characteristics. The learned prompts are then integrated into the diffusion model via the designed weather specific prompt guidance module, making it possible to restore multiple weather types. To further improve the adaptiveness to complex weather degradations, we propose a dynamic expert selection modulator that employs a dynamic weather-aware router to flexibly dispatch varying numbers of restoration experts for each weather-distorted image, allowing the diffusion model to restore diverse degradations adaptively. Experimental results substantiate the favorable performance of DA2Diff over state-of-the-arts in quantitative and qualitative evaluation. Source code will be available after acceptance.</li>
</ul>

<h3>Title: BoxSeg: Quality-Aware and Peer-Assisted Learning for Box-supervised Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jinxiang Lai, Wenlong Wu, Jiawei Zhan, Jian Li, Bin-Bin Gao, Jun Liu, Jie Zhang, Song Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05137">https://arxiv.org/abs/2504.05137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05137">https://arxiv.org/pdf/2504.05137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05137]] BoxSeg: Quality-Aware and Peer-Assisted Learning for Box-supervised Instance Segmentation(https://arxiv.org/abs/2504.05137)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Box-supervised instance segmentation methods aim to achieve instance segmentation with only box annotations. Recent methods have demonstrated the effectiveness of acquiring high-quality pseudo masks under the teacher-student framework. Building upon this foundation, we propose a BoxSeg framework involving two novel and general modules named the Quality-Aware Module (QAM) and the Peer-assisted Copy-paste (PC). The QAM obtains high-quality pseudo masks and better measures the mask quality to help reduce the effect of noisy masks, by leveraging the quality-aware multi-mask complementation mechanism. The PC imitates Peer-Assisted Learning to further improve the quality of the low-quality masks with the guidance of the obtained high-quality pseudo masks. Theoretical and experimental analyses demonstrate the proposed QAM and PC are effective. Extensive experimental results show the superiority of our BoxSeg over the state-of-the-art methods, and illustrate the QAM and PC can be applied to improve other models.</li>
</ul>

<h3>Title: Towards Optimal Heterogeneous Client Sampling in Multi-Model Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Haoran Zhang, Zejun Gong, Zekai Li, Marie Siew, Carlee Joe-Wong, Rachid El-Azouzi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05138">https://arxiv.org/abs/2504.05138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05138">https://arxiv.org/pdf/2504.05138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05138]] Towards Optimal Heterogeneous Client Sampling in Multi-Model Federated Learning(https://arxiv.org/abs/2504.05138)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) allows edge devices to collaboratively train models without sharing local data. As FL gains popularity, clients may need to train multiple unrelated FL models, but communication constraints limit their ability to train all models simultaneously. While clients could train FL models sequentially, opportunistically having FL clients concurrently train different models -- termed multi-model federated learning (MMFL) -- can reduce the overall training time. Prior work uses simple client-to-model assignments that do not optimize the contribution of each client to each model over the course of its training. Prior work on single-model FL shows that intelligent client selection can greatly accelerate convergence, but naïve extensions to MMFL can violate heterogeneous resource constraints at both the server and the clients. In this work, we develop a novel convergence analysis of MMFL with arbitrary client sampling methods, theoretically demonstrating the strengths and limitations of previous well-established gradient-based methods. Motivated by this analysis, we propose MMFL-LVR, a loss-based sampling method that minimizes training variance while explicitly respecting communication limits at the server and reducing computational costs at the clients. We extend this to MMFL-StaleVR, which incorporates stale updates for improved efficiency and stability, and MMFL-StaleVRE, a lightweight variant suitable for low-overhead deployment. Experiments show our methods improve average accuracy by up to 19.1% over random sampling, with only a 5.4% gap from the theoretical optimum (full client participation).</li>
</ul>

<h3>Title: Unifying Physics- and Data-Driven Modeling via Novel Causal Spatiotemporal Graph Neural Network for Interpretable Epidemic Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Shuai Han, Lukas Stelz, Thomas R. Sokolowski, Kai Zhou, Horst Stöcker</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.soc-ph, q-bio.QM, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05140">https://arxiv.org/abs/2504.05140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05140">https://arxiv.org/pdf/2504.05140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05140]] Unifying Physics- and Data-Driven Modeling via Novel Causal Spatiotemporal Graph Neural Network for Interpretable Epidemic Forecasting(https://arxiv.org/abs/2504.05140)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Accurate epidemic forecasting is crucial for effective disease control and prevention. Traditional compartmental models often struggle to estimate temporally and spatially varying epidemiological parameters, while deep learning models typically overlook disease transmission dynamics and lack interpretability in the epidemiological context. To address these limitations, we propose a novel Causal Spatiotemporal Graph Neural Network (CSTGNN), a hybrid framework that integrates a Spatio-Contact SIR model with Graph Neural Networks (GNNs) to capture the spatiotemporal propagation of epidemics. Inter-regional human mobility exhibits continuous and smooth spatiotemporal patterns, leading to adjacent graph structures that share underlying mobility dynamics. To model these dynamics, we employ an adaptive static connectivity graph to represent the stable components of human mobility and utilize a temporal dynamics model to capture fluctuations within these patterns. By integrating the adaptive static connectivity graph with the temporal dynamics graph, we construct a dynamic graph that encapsulates the comprehensive properties of human mobility networks. Additionally, to capture temporal trends and variations in infectious disease spread, we introduce a temporal decomposition model to handle temporal dependence. This model is then integrated with a dynamic graph convolutional network for epidemic forecasting. We validate our model using real-world datasets at the provincial level in China and the state level in Germany. Extensive studies demonstrate that our method effectively models the spatiotemporal dynamics of infectious diseases, providing a valuable tool for forecasting and intervention strategies. Furthermore, analysis of the learned parameters offers insights into disease transmission mechanisms, enhancing the interpretability and practical applicability of our model.</li>
</ul>

<h3>Title: EffOWT: Transfer Visual Language Models to Open-World Tracking Efficiently and Effectively</h3>
<ul>
<li><strong>Authors: </strong>Bingyang Wang, Kaer Huang, Bin Li, Yiqiang Yan, Lihe Zhang, Huchuan Lu, You He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05141">https://arxiv.org/abs/2504.05141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05141">https://arxiv.org/pdf/2504.05141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05141]] EffOWT: Transfer Visual Language Models to Open-World Tracking Efficiently and Effectively(https://arxiv.org/abs/2504.05141)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Open-World Tracking (OWT) aims to track every object of any category, which requires the model to have strong generalization capabilities. Trackers can improve their generalization ability by leveraging Visual Language Models (VLMs). However, challenges arise with the fine-tuning strategies when VLMs are transferred to OWT: full fine-tuning results in excessive parameter and memory costs, while the zero-shot strategy leads to sub-optimal performance. To solve the problem, EffOWT is proposed for efficiently transferring VLMs to OWT. Specifically, we build a small and independent learnable side network outside the VLM backbone. By freezing the backbone and only executing backpropagation on the side network, the model's efficiency requirements can be met. In addition, EffOWT enhances the side network by proposing a hybrid structure of Transformer and CNN to improve the model's performance in the OWT field. Finally, we implement sparse interactions on the MLP, thus reducing parameter updates and memory costs significantly. Thanks to the proposed methods, EffOWT achieves an absolute gain of 5.5% on the tracking metric OWTA for unknown categories, while only updating 1.3% of the parameters compared to full fine-tuning, with a 36.4% memory saving. Other metrics also demonstrate obvious improvement.</li>
</ul>

<h3>Title: Taming Double-Spending in Offline Payments with Reputation-Weighted Loan Networks</h3>
<ul>
<li><strong>Authors: </strong>Nektarios Evangelou, Rowdy Chotkan, Bulat Nasrulin, Jérémie Decouchant</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05143">https://arxiv.org/abs/2504.05143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05143">https://arxiv.org/pdf/2504.05143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05143]] Taming Double-Spending in Offline Payments with Reputation-Weighted Loan Networks(https://arxiv.org/abs/2504.05143)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>Blockchain solutions typically assume a synchronous network to ensure consistency and achieve consensus. In contrast, offline transaction systems aim to enable users to agree on and execute transactions without assuming bounded communication delays when interacting with the blockchain. Most existing offline payment schemes depend on trusted hardware wallets that are assumed to be secure and tamper-proof. While this work introduces Overdraft, a novel offline payment system that shifts the reliance from hardware to users themselves. Overdraft allows potential payment receivers to assess the likelihood of being paid, allowing them to accept transactions with confidence or deny them. Overdraft achieves this by maintaining a loan network that is weighted by online reputation. This loan network contains time-limited agreements where users pledge to cover another user's payment if necessary. For example, when a payer lacks sufficient funds at the moment of commitment. Offline users rely on the last known view of the loan network -- which they had access to when last online -- to determine whether to participate in an offline transaction. This view is used to estimate the probability of eventual payment, possibly using multiple loans. Once online again, users commit their transactions to the blockchain with any conflicts being resolved deterministically. Overdraft incorporates incentives for users and is designed to be resilient against Sybil attacks. As a proof of concept, we implemented Overdraft as an Ethereum Solidity smart contract and deployed it on the Sepolia testnet to evaluate its performance.</li>
</ul>

<h3>Title: Pr$εε$mpt: Sanitizing Sensitive Prompts for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Amrita Roy Chowdhury, David Glukhov, Divyam Anshumaan, Prasad Chalasani, Nicolas Papernot, Somesh Jha, Mihir Bellare</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05147">https://arxiv.org/abs/2504.05147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05147">https://arxiv.org/pdf/2504.05147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05147]] Pr$εε$mpt: Sanitizing Sensitive Prompts for LLMs(https://arxiv.org/abs/2504.05147)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>The rise of large language models (LLMs) has introduced new privacy challenges, particularly during inference where sensitive information in prompts may be exposed to proprietary LLM APIs. In this paper, we address the problem of formally protecting the sensitive information contained in a prompt while maintaining response quality. To this end, first, we introduce a cryptographically inspired notion of a prompt sanitizer which transforms an input prompt to protect its sensitive tokens. Second, we propose Pr$\epsilon\epsilon$mpt, a novel system that implements a prompt sanitizer. Pr$\epsilon\epsilon$mpt categorizes sensitive tokens into two types: (1) those where the LLM's response depends solely on the format (such as SSNs, credit card numbers), for which we use format-preserving encryption (FPE); and (2) those where the response depends on specific values, (such as age, salary) for which we apply metric differential privacy (mDP). Our evaluation demonstrates that Pr$\epsilon\epsilon$mpt is a practical method to achieve meaningful privacy guarantees, while maintaining high utility compared to unsanitized prompts, and outperforming prior methods</li>
</ul>

<h3>Title: PanoDreamer: Consistent Text to 360-Degree Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhexiao Xiong, Zhang Chen, Zhong Li, Yi Xu, Nathan Jacobs</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05152">https://arxiv.org/abs/2504.05152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05152">https://arxiv.org/pdf/2504.05152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05152]] PanoDreamer: Consistent Text to 360-Degree Scene Generation(https://arxiv.org/abs/2504.05152)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatically generating a complete 3D scene from a text description, a reference image, or both has significant applications in fields like virtual reality and gaming. However, current methods often generate low-quality textures and inconsistent 3D structures. This is especially true when extrapolating significantly beyond the field of view of the reference image. To address these challenges, we propose PanoDreamer, a novel framework for consistent, 3D scene generation with flexible text and image control. Our approach employs a large language model and a warp-refine pipeline, first generating an initial set of images and then compositing them into a 360-degree panorama. This panorama is then lifted into 3D to form an initial point cloud. We then use several approaches to generate additional images, from different viewpoints, that are consistent with the initial point cloud and expand/refine the initial point cloud. Given the resulting set of images, we utilize 3D Gaussian Splatting to create the final 3D scene, which can then be rendered from different viewpoints. Experiments demonstrate the effectiveness of PanoDreamer in generating high-quality, geometrically consistent 3D scenes.</li>
</ul>

<h3>Title: SparsyFed: Sparse Adaptive Federated Training</h3>
<ul>
<li><strong>Authors: </strong>Adriano Guastella, Lorenzo Sani, Alex Iacob, Alessio Mora, Paolo Bellavista, Nicholas D. Lane</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05153">https://arxiv.org/abs/2504.05153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05153">https://arxiv.org/pdf/2504.05153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05153]] SparsyFed: Sparse Adaptive Federated Training(https://arxiv.org/abs/2504.05153)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Sparse training is often adopted in cross-device federated learning (FL) environments where constrained devices collaboratively train a machine learning model on private data by exchanging pseudo-gradients across heterogeneous networks. Although sparse training methods can reduce communication overhead and computational burden in FL, they are often not used in practice for the following key reasons: (1) data heterogeneity makes it harder for clients to reach consensus on sparse models compared to dense ones, requiring longer training; (2) methods for obtaining sparse masks lack adaptivity to accommodate very heterogeneous data distributions, crucial in cross-device FL; and (3) additional hyperparameters are required, which are notably challenging to tune in FL. This paper presents SparsyFed, a practical federated sparse training method that critically addresses the problems above. Previous works have only solved one or two of these challenges at the expense of introducing new trade-offs, such as clients' consensus on masks versus sparsity pattern adaptivity. We show that SparsyFed simultaneously (1) can produce 95% sparse models, with negligible degradation in accuracy, while only needing a single hyperparameter, (2) achieves a per-round weight regrowth 200 times smaller than previous methods, and (3) allows the sparse masks to adapt to highly heterogeneous data distributions and outperform all baselines under such conditions.</li>
</ul>

<h3>Title: Balancing Task-invariant Interaction and Task-specific Adaptation for Unified Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Hu, Junjun Jiang, Chenyang Wang, Kui Jiang, Xianming Liu, Jiayi Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05164">https://arxiv.org/abs/2504.05164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05164">https://arxiv.org/pdf/2504.05164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05164]] Balancing Task-invariant Interaction and Task-specific Adaptation for Unified Image Fusion(https://arxiv.org/abs/2504.05164)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Unified image fusion aims to integrate complementary information from multi-source images, enhancing image quality through a unified framework applicable to diverse fusion tasks. While treating all fusion tasks as a unified problem facilitates task-invariant knowledge sharing, it often overlooks task-specific characteristics, thereby limiting the overall performance. Existing general image fusion methods incorporate explicit task identification to enable adaptation to different fusion tasks. However, this dependence during inference restricts the model's generalization to unseen fusion tasks. To address these issues, we propose a novel unified image fusion framework named "TITA", which dynamically balances both Task-invariant Interaction and Task-specific Adaptation. For task-invariant interaction, we introduce the Interaction-enhanced Pixel Attention (IPA) module to enhance pixel-wise interactions for better multi-source complementary information extraction. For task-specific adaptation, the Operation-based Adaptive Fusion (OAF) module dynamically adjusts operation weights based on task properties. Additionally, we incorporate the Fast Adaptive Multitask Optimization (FAMO) strategy to mitigate the impact of gradient conflicts across tasks during joint training. Extensive experiments demonstrate that TITA not only achieves competitive performance compared to specialized methods across three image fusion scenarios but also exhibits strong generalization to unseen fusion tasks.</li>
</ul>

<h3>Title: Learning symmetries in datasets</h3>
<ul>
<li><strong>Authors: </strong>Veronica Sanz</a></li>
<li><strong>Subjects: </strong>cs.LG, hep-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05174">https://arxiv.org/abs/2504.05174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05174">https://arxiv.org/pdf/2504.05174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05174]] Learning symmetries in datasets(https://arxiv.org/abs/2504.05174)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We investigate how symmetries present in datasets affect the structure of the latent space learned by Variational Autoencoders (VAEs). By training VAEs on data originating from simple mechanical systems and particle collisions, we analyze the organization of the latent space through a relevance measure that identifies the most meaningful latent directions. We show that when symmetries or approximate symmetries are present, the VAE self-organizes its latent space, effectively compressing the data along a reduced number of latent variables. This behavior captures the intrinsic dimensionality determined by the symmetry constraints and reveals hidden relations among the features. Furthermore, we provide a theoretical analysis of a simple toy model, demonstrating how, under idealized conditions, the latent space aligns with the symmetry directions of the data manifold. We illustrate these findings with examples ranging from two-dimensional datasets with $O(2)$ symmetry to realistic datasets from electron-positron and proton-proton collisions. Our results highlight the potential of unsupervised generative models to expose underlying structures in data and offer a novel approach to symmetry discovery without explicit supervision.</li>
</ul>

<h3>Title: The 1st Solution for 4th PVUW MeViS Challenge: Unleashing the Potential of Large Multimodal Models for Referring Video Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hao Fang, Runmin Cong, Xiankai Lu, Zhiyang Chen, Wei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05178">https://arxiv.org/abs/2504.05178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05178">https://arxiv.org/pdf/2504.05178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05178]] The 1st Solution for 4th PVUW MeViS Challenge: Unleashing the Potential of Large Multimodal Models for Referring Video Segmentation(https://arxiv.org/abs/2504.05178)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Motion expression video segmentation is designed to segment objects in accordance with the input motion expressions. In contrast to the conventional Referring Video Object Segmentation (RVOS), it places emphasis on motion as well as multi-object expressions, making it more arduous. Recently, Large Multimodal Models (LMMs) have begun to shine in RVOS due to their powerful vision-language perception capabilities. In this work, we propose a simple and effective inference optimization method to fully unleash the potential of LMMs in referring video segmentation. Firstly, we use Sa2VA as our baseline, which is a unified LMM for dense grounded understanding of both images and videos. Secondly, we uniformly sample the video frames during the inference process to enhance the model's understanding of the entire video. Finally, we integrate the results of multiple expert models to mitigate the erroneous predictions of a single model. Our solution achieved 61.98% J&F on the MeViS test set and ranked 1st place in the 4th PVUW Challenge MeViS Track at CVPR 2025.</li>
</ul>

<h3>Title: BRIDGES: Bridging Graph Modality and Large Language Models within EDA Tasks</h3>
<ul>
<li><strong>Authors: </strong>Wei Li, Yang Zou, Christopher Ellis, Ruben Purdy, Shawn Blanton, José M. F. Moura</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05180">https://arxiv.org/abs/2504.05180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05180">https://arxiv.org/pdf/2504.05180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05180]] BRIDGES: Bridging Graph Modality and Large Language Models within EDA Tasks(https://arxiv.org/abs/2504.05180)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While many EDA tasks already involve graph-based data, existing LLMs in EDA primarily either represent graphs as sequential text, or simply ignore graph-structured data that might be beneficial like dataflow graphs of RTL code. Recent studies have found that LLM performance suffers when graphs are represented as sequential text, and using additional graph information significantly boosts performance. To address these challenges, we introduce BRIDGES, a framework designed to incorporate graph modality into LLMs for EDA tasks. BRIDGES integrates an automated data generation workflow, a solution that combines graph modality with LLM, and a comprehensive evaluation suite. First, we establish an LLM-driven workflow to generate RTL and netlist-level data, converting them into dataflow and netlist graphs with function descriptions. This workflow yields a large-scale dataset comprising over 500,000 graph instances and more than 1.5 billion tokens. Second, we propose a lightweight cross-modal projector that encodes graph representations into text-compatible prompts, enabling LLMs to effectively utilize graph data without architectural modifications. Experimental results demonstrate 2x to 10x improvements across multiple tasks compared to text-only baselines, including accuracy in design retrieval, type prediction and perplexity in function description, with negligible computational overhead (<1% model weights increase and <30% additional runtime overhead). Even without additional LLM finetuning, our results outperform text-only by a large margin. We plan to release BRIDGES, including the dataset, models, and training flow.</li>
</ul>

<h3>Title: MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised Prototypical Contrastive Loss for Coronary DSA Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Rayan Merghani Ahmed, Adnan Iltaf, Bin Li, Shoujun Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05184">https://arxiv.org/abs/2504.05184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05184">https://arxiv.org/pdf/2504.05184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05184]] MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised Prototypical Contrastive Loss for Coronary DSA Image Segmentation(https://arxiv.org/abs/2504.05184)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>The accurate segmentation of coronary Digital Subtraction Angiography (DSA) images is essential for diagnosing and treating coronary artery diseases. Despite advances in deep learning-based segmentation, challenges such as low contrast, noise, overlapping structures, high intra-class variance, and class imbalance limit precise vessel delineation. To overcome these limitations, we propose the MSA-UNet3+: a Multi-Scale Attention enhanced UNet3+ architecture for coronary DSA image segmentation. The framework combined Multi-Scale Dilated Bottleneck (MSD-Bottleneck) with Contextual Attention Fusion Module (CAFM), which not only enhances multi-scale feature extraction but also preserve fine-grained details, and improve contextual understanding. Furthermore, we propose a new Supervised Prototypical Contrastive Loss (SPCL), which combines supervised and prototypical contrastive learning to minimize class imbalance and high intra-class variance by focusing on hard-to-classified background samples. Experiments carried out on a private coronary DSA dataset demonstrate that MSA-UNet3+ outperforms state-of-the-art methods, achieving a Dice coefficient of 87.73%, an F1-score of 87.78%, and significantly reduced Average Surface Distance (ASD) and Average Contour Distance (ACD). The developed framework provides clinicians with precise vessel segmentation, enabling accurate identification of coronary stenosis and supporting informed diagnostic and therapeutic decisions. The code will be released at the following GitHub profile link this https URL.</li>
</ul>

<h3>Title: Concise Reasoning via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Mehdi Fatemi, Banafsheh Rafiee, Mingjie Tang, Kartik Talamadupula</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05185">https://arxiv.org/abs/2504.05185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05185">https://arxiv.org/pdf/2504.05185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05185]] Concise Reasoning via Reinforcement Learning(https://arxiv.org/abs/2504.05185)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite significant advancements in large language models (LLMs), a major drawback of reasoning models is their enormous token usage, which increases computational cost, resource requirements, and response time. In this work, we revisit the core principles of reinforcement learning (RL) and, through mathematical analysis, demonstrate that the tendency to generate lengthy responses arises inherently from RL-based optimization during training. This finding questions the prevailing assumption that longer responses inherently improve reasoning accuracy. Instead, we uncover a natural correlation between conciseness and accuracy that has been largely overlooked. Moreover, we show that introducing a secondary phase of RL post-training, using a small set of problems and limited resources, can significantly reduce a model's chain of thought while maintaining or even enhancing accuracy. Finally, we validate our conclusions through extensive experimental results.</li>
</ul>

<h3>Title: Infinitely Divisible Noise for Differential Privacy: Nearly Optimal Error in the High $\varepsilon$ Regime</h3>
<ul>
<li><strong>Authors: </strong>Charlie Harrison, Pasin Manurangsi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05202">https://arxiv.org/abs/2504.05202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05202">https://arxiv.org/pdf/2504.05202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05202]] Infinitely Divisible Noise for Differential Privacy: Nearly Optimal Error in the High $\varepsilon$ Regime(https://arxiv.org/abs/2504.05202)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Differential privacy (DP) can be achieved in a distributed manner, where multiple parties add independent noise such that their sum protects the overall dataset with DP. A common technique here is for each party to sample their noise from the decomposition of an infinitely divisible distribution. We analyze two mechanisms in this setting: 1) the generalized discrete Laplace (GDL) mechanism, whose distribution (which is closed under summation) follows from differences of i.i.d. negative binomial shares, and 2) the multi-scale discrete Laplace (MSDLap) mechanism, a novel mechanism following the sum of multiple i.i.d. discrete Laplace shares at different scales. For $\varepsilon \geq 1$, our mechanisms can be parameterized to have $O\left(\Delta^3 e^{-\varepsilon}\right)$ and $O\left(\min\left(\Delta^3 e^{-\varepsilon}, \Delta^2 e^{-2\varepsilon/3}\right)\right)$ MSE, respectively, where $\Delta$ denote the sensitivity; the latter bound matches known optimality results. We also show a transformation from the discrete setting to the continuous setting, which allows us to transform both mechanisms to the continuous setting and thereby achieve the optimal $O\left(\Delta^2 e^{-2\varepsilon / 3}\right)$ MSE. To our knowledge, these are the first infinitely divisible additive noise mechanisms that achieve order-optimal MSE under pure DP, so our work shows formally there is no separation in utility when query-independent noise adding mechanisms are restricted to infinitely divisible noise. For the continuous setting, our result improves upon the Arete mechanism from [Pagh and Stausholm, ALT 2022] which gives an MSE of $O\left(\Delta^2 e^{-\varepsilon/4}\right)$. Furthermore, we give an exact sampler tuned to efficiently implement the MSDLap mechanism, and we apply our results to improve a state of the art multi-message shuffle DP protocol in the high $\varepsilon$ regime.</li>
</ul>

<h3>Title: Post-Training Language Models for Continual Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Sefika Efeoglu, Adrian Paschke, Sonja Schimmler</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05214">https://arxiv.org/abs/2504.05214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05214">https://arxiv.org/pdf/2504.05214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05214]] Post-Training Language Models for Continual Relation Extraction(https://arxiv.org/abs/2504.05214)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Real-world data, such as news articles, social media posts, and chatbot conversations, is inherently dynamic and non-stationary, presenting significant challenges for constructing real-time structured representations through knowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG creation, often struggles to adapt to evolving data when traditional models rely on static, outdated datasets. Continual Relation Extraction (CRE) methods tackle this issue by incrementally learning new relations while preserving previously acquired knowledge. This study investigates the application of pre-trained language models (PLMs), specifically large language models (LLMs), to CRE, with a focus on leveraging memory replay to address catastrophic forgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and encoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets. Task-incremental fine-tuning of LLMs demonstrates superior performance over earlier approaches using encoder-only models like BERT on TACRED, excelling in seen-task accuracy and overall performance (measured by whole and average accuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel are similarly promising, achieving second place in whole and average accuracy metrics. This work underscores critical factors in knowledge transfer, language model architecture, and KG completeness, advancing CRE with LLMs and memory replay for dynamic, real-time relation extraction.</li>
</ul>

<h3>Title: An ensemble deep learning approach to detect tumors on Mohs micrographic surgery slides</h3>
<ul>
<li><strong>Authors: </strong>Abdurrahim Yilmaz, Serra Atilla Aydin, Deniz Temur, Furkan Yuceyalcin, Berkin Deniz Kahya, Rahmetullah Varol, Ozay Gokoz, Gulsum Gencoglan, Huseyin Uvet, Gonca Elcin</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05219">https://arxiv.org/abs/2504.05219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05219">https://arxiv.org/pdf/2504.05219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05219]] An ensemble deep learning approach to detect tumors on Mohs micrographic surgery slides(https://arxiv.org/abs/2504.05219)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Mohs micrographic surgery (MMS) is the gold standard technique for removing high risk nonmelanoma skin cancer however, intraoperative histopathological examination demands significant time, effort, and professionality. The objective of this study is to develop a deep learning model to detect basal cell carcinoma (BCC) and artifacts on Mohs slides. A total of 731 Mohs slides from 51 patients with BCCs were used in this study, with 91 containing tumor and 640 without tumor which was defined as non-tumor. The dataset was employed to train U-Net based models that segment tumor and non-tumor regions on the slides. The segmented patches were classified as tumor, or non-tumor to produce predictions for whole slide images (WSIs). For the segmentation phase, the deep learning model success was measured using a Dice score with 0.70 and 0.67 value, area under the curve (AUC) score with 0.98 and 0.96 for tumor and non-tumor, respectively. For the tumor classification, an AUC of 0.98 for patch-based detection, and AUC of 0.91 for slide-based detection was obtained on the test dataset. We present an AI system that can detect tumors and non-tumors in Mohs slides with high success. Deep learning can aid Mohs surgeons and dermatopathologists in making more accurate decisions.</li>
</ul>

<h3>Title: Proposing TAGbank as a Corpus of Tree-Adjoining Grammar Derivations</h3>
<ul>
<li><strong>Authors: </strong>Jungyeul Park</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05226">https://arxiv.org/abs/2504.05226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05226">https://arxiv.org/pdf/2504.05226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05226]] Proposing TAGbank as a Corpus of Tree-Adjoining Grammar Derivations(https://arxiv.org/abs/2504.05226)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, generative</a></li>
<li><strong>Abstract: </strong>The development of lexicalized grammars, particularly Tree-Adjoining Grammar (TAG), has significantly advanced our understanding of syntax and semantics in natural language processing (NLP). While existing syntactic resources like the Penn Treebank and Universal Dependencies offer extensive annotations for phrase-structure and dependency parsing, there is a lack of large-scale corpora grounded in lexicalized grammar formalisms. To address this gap, we introduce TAGbank, a corpus of TAG derivations automatically extracted from existing syntactic treebanks. This paper outlines a methodology for mapping phrase-structure annotations to TAG derivations, leveraging the generative power of TAG to support parsing, grammar induction, and semantic analysis. Our approach builds on the work of CCGbank, extending it to incorporate the unique structural properties of TAG, including its transparent derivation trees and its ability to capture long-distance dependencies. We also discuss the challenges involved in the extraction process, including ensuring consistency across treebank schemes and dealing with language-specific syntactic idiosyncrasies. Finally, we propose the future extension of TAGbank to include multilingual corpora, focusing on the Penn Korean and Penn Chinese Treebanks, to explore the cross-linguistic application of TAG's formalism. By providing a robust, derivation-based resource, TAGbank aims to support a wide range of computational tasks and contribute to the theoretical understanding of TAG's generative capacity.</li>
</ul>

<h3>Title: NoveltyBench: Evaluating Creativity and Diversity in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yiming Zhang, Harshita Diddee, Susan Holm, Hanchen Liu, Xinyue Liu, Vinay Samuel, Barry Wang, Daphne Ippolito</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05228">https://arxiv.org/abs/2504.05228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05228">https://arxiv.org/pdf/2504.05228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05228]] NoveltyBench: Evaluating Creativity and Diversity in Language Models(https://arxiv.org/abs/2504.05228)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Language models have demonstrated remarkable capabilities on standard benchmarks, yet they struggle increasingly from mode collapse, the inability to generate diverse and novel outputs. Our work introduces NoveltyBench, a benchmark specifically designed to evaluate the ability of language models to produce multiple distinct and high-quality outputs. NoveltyBench utilizes prompts curated to elicit diverse answers and filtered real-world user queries. Evaluating 20 leading language models, we find that current state-of-the-art systems generate significantly less diversity than human writers. Notably, larger models within a family often exhibit less diversity than their smaller counterparts, challenging the notion that capability on standard benchmarks translates directly to generative utility. While prompting strategies like in-context regeneration can elicit diversity, our findings highlight a fundamental lack of distributional diversity in current models, reducing their utility for users seeking varied responses and suggesting the need for new training and evaluation paradigms that prioritize creativity alongside quality.</li>
</ul>

<h3>Title: Federated Learning for Medical Image Classification: A Comprehensive Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Zhekai Zhou, Guibo Luo, Mingzhi Chen, Zhenyu Weng, Yuesheng Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05238">https://arxiv.org/abs/2504.05238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05238">https://arxiv.org/pdf/2504.05238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05238]] Federated Learning for Medical Image Classification: A Comprehensive Benchmark(https://arxiv.org/abs/2504.05238)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, federate, fair, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The federated learning paradigm is wellsuited for the field of medical image analysis, as it can effectively cope with machine learning on isolated multicenter data while protecting the privacy of participating parties. However, current research on optimization algorithms in federated learning often focuses on limited datasets and scenarios, primarily centered around natural images, with insufficient comparative experiments in medical contexts. In this work, we conduct a comprehensive evaluation of several state-of-the-art federated learning algorithms in the context of medical imaging. We conduct a fair comparison of classification models trained using various federated learning algorithms across multiple medical imaging datasets. Additionally, we evaluate system performance metrics, such as communication cost and computational efficiency, while considering different federated learning architectures. Our findings show that medical imaging datasets pose substantial challenges for current federated learning optimization algorithms. No single algorithm consistently delivers optimal performance across all medical federated learning scenarios, and many optimization algorithms may underperform when applied to these datasets. Our experiments provide a benchmark and guidance for future research and application of federated learning in medical imaging contexts. Furthermore, we propose an efficient and robust method that combines generative techniques using denoising diffusion probabilistic models with label smoothing to augment datasets, widely enhancing the performance of federated learning on classification tasks across various medical imaging datasets. Our code will be released on GitHub, offering a reliable and comprehensive benchmark for future federated learning studies in medical imaging.</li>
</ul>

<h3>Title: LLM-based Automated Grading with Human-in-the-Loop</h3>
<ul>
<li><strong>Authors: </strong>Hang Li, Yucheng Chu, Kaiqi Yang, Yasemin Copur-Gencturk, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05239">https://arxiv.org/abs/2504.05239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05239">https://arxiv.org/pdf/2504.05239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05239]] LLM-based Automated Grading with Human-in-the-Loop(https://arxiv.org/abs/2504.05239)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The rise of artificial intelligence (AI) technologies, particularly large language models (LLMs), has brought significant advancements to the field of education. Among various applications, automatic short answer grading (ASAG), which focuses on evaluating open-ended textual responses, has seen remarkable progress with the introduction of LLMs. These models not only enhance grading performance compared to traditional ASAG approaches but also move beyond simple comparisons with predefined "golden" answers, enabling more sophisticated grading scenarios, such as rubric-based evaluation. However, existing LLM-powered methods still face challenges in achieving human-level grading performance in rubric-based assessments due to their reliance on fully automated approaches. In this work, we explore the potential of LLMs in ASAG tasks by leveraging their interactive capabilities through a human-in-the-loop (HITL) approach. Our proposed framework, GradeHITL, utilizes the generative properties of LLMs to pose questions to human experts, incorporating their insights to refine grading rubrics dynamically. This adaptive process significantly improves grading accuracy, outperforming existing methods and bringing ASAG closer to human-level evaluation.</li>
</ul>

<h3>Title: Embedded Federated Feature Selection with Dynamic Sparse Training: Balancing Accuracy-Cost Tradeoffs</h3>
<ul>
<li><strong>Authors: </strong>Afsaneh Mahanipour, Hana Khamfroush</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05245">https://arxiv.org/abs/2504.05245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05245">https://arxiv.org/pdf/2504.05245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05245]] Embedded Federated Feature Selection with Dynamic Sparse Training: Balancing Accuracy-Cost Tradeoffs(https://arxiv.org/abs/2504.05245)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables multiple resource-constrained edge devices with varying levels of heterogeneity to collaboratively train a global model. However, devices with limited capacity can create bottlenecks and slow down model convergence. One effective approach to addressing this issue is to use an efficient feature selection method, which reduces overall resource demands by minimizing communication and computation costs, thereby mitigating the impact of struggling nodes. Existing federated feature selection (FFS) methods are either considered as a separate step from FL or rely on a third party. These approaches increase computation and communication overhead, making them impractical for real-world high-dimensional datasets. To address this, we present \textit{Dynamic Sparse Federated Feature Selection} (DSFFS), the first innovative embedded FFS that is efficient in both communication and computation. In the proposed method, feature selection occurs simultaneously with model training. During training, input-layer neurons, their connections, and hidden-layer connections are dynamically pruned and regrown, eliminating uninformative features. This process enhances computational efficiency on devices, improves network communication efficiency, and boosts global model performance. Several experiments are conducted on nine real-world datasets of varying dimensionality from diverse domains, including biology, image, speech, and text. The results under a realistic non-iid data distribution setting show that our approach achieves a better trade-off between accuracy, computation, and communication costs by selecting more informative features compared to other state-of-the-art FFS methods.</li>
</ul>

<h3>Title: PINNverse: Accurate parameter estimation in differential equations from noisy data with constrained physics-informed neural networks</h3>
<ul>
<li><strong>Authors: </strong>Marius Almanstötter, Roman Vetter, Dagmar Iber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05248">https://arxiv.org/abs/2504.05248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05248">https://arxiv.org/pdf/2504.05248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05248]] PINNverse: Accurate parameter estimation in differential equations from noisy data with constrained physics-informed neural networks(https://arxiv.org/abs/2504.05248)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Parameter estimation for differential equations from measured data is an inverse problem prevalent across quantitative sciences. Physics-Informed Neural Networks (PINNs) have emerged as effective tools for solving such problems, especially with sparse measurements and incomplete system information. However, PINNs face convergence issues, stability problems, overfitting, and complex loss function design. Here we introduce PINNverse, a training paradigm that addresses these limitations by reformulating the learning process as a constrained differential optimization problem. This approach achieves a dynamic balance between data loss and differential equation residual loss during training while preventing overfitting. PINNverse combines the advantages of PINNs with the Modified Differential Method of Multipliers to enable convergence on any point on the Pareto front. We demonstrate robust and accurate parameter estimation from noisy data in four classical ODE and PDE models from physics and biology. Our method enables accurate parameter inference also when the forward problem is expensive to solve.</li>
</ul>

<h3>Title: Texture2LoD3: Enabling LoD3 Building Reconstruction With Panoramic Images</h3>
<ul>
<li><strong>Authors: </strong>Wenzhao Tang, Weihang Li, Xiucheng Liang, Olaf Wysocki, Filip Biljecki, Christoph Holst, Boris Jutzi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05249">https://arxiv.org/abs/2504.05249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05249">https://arxiv.org/pdf/2504.05249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05249]] Texture2LoD3: Enabling LoD3 Building Reconstruction With Panoramic Images(https://arxiv.org/abs/2504.05249)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Despite recent advancements in surface reconstruction, Level of Detail (LoD) 3 building reconstruction remains an unresolved challenge. The main issue pertains to the object-oriented modelling paradigm, which requires georeferencing, watertight geometry, facade semantics, and low-poly representation -- Contrasting unstructured mesh-oriented models. In Texture2LoD3, we introduce a novel method leveraging the ubiquity of 3D building model priors and panoramic street-level images, enabling the reconstruction of LoD3 building models. We observe that prior low-detail building models can serve as valid planar targets for ortho-rectifying street-level panoramic images. Moreover, deploying segmentation on accurately textured low-level building surfaces supports maintaining essential georeferencing, watertight geometry, and low-poly representation for LoD3 reconstruction. In the absence of LoD3 validation data, we additionally introduce the ReLoD3 dataset, on which we experimentally demonstrate that our method leads to improved facade segmentation accuracy by 11% and can replace costly manual projections. We believe that Texture2LoD3 can scale the adoption of LoD3 models, opening applications in estimating building solar potential or enhancing autonomous driving simulations. The project website, code, and data are available here: this https URL.</li>
</ul>

<h3>Title: Explaining Low Perception Model Competency with High-Competency Counterfactuals</h3>
<ul>
<li><strong>Authors: </strong>Sara Pohland, Claire Tomlin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05254">https://arxiv.org/abs/2504.05254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05254">https://arxiv.org/pdf/2504.05254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05254]] Explaining Low Perception Model Competency with High-Competency Counterfactuals(https://arxiv.org/abs/2504.05254)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>There exist many methods to explain how an image classification model generates its decision, but very little work has explored methods to explain why a classifier might lack confidence in its prediction. As there are various reasons the classifier might lose confidence, it would be valuable for this model to not only indicate its level of uncertainty but also explain why it is uncertain. Counterfactual images have been used to visualize changes that could be made to an image to generate a different classification decision. In this work, we explore the use of counterfactuals to offer an explanation for low model competency--a generalized form of predictive uncertainty that measures confidence. Toward this end, we develop five novel methods to generate high-competency counterfactual images, namely Image Gradient Descent (IGD), Feature Gradient Descent (FGD), Autoencoder Reconstruction (Reco), Latent Gradient Descent (LGD), and Latent Nearest Neighbors (LNN). We evaluate these methods across two unique datasets containing images with six known causes for low model competency and find Reco, LGD, and LNN to be the most promising methods for counterfactual generation. We further evaluate how these three methods can be utilized by pre-trained Multimodal Large Language Models (MLLMs) to generate language explanations for low model competency. We find that the inclusion of a counterfactual image in the language model query greatly increases the ability of the model to generate an accurate explanation for the cause of low model competency, thus demonstrating the utility of counterfactual images in explaining low perception model competency.</li>
</ul>

<h3>Title: Adversarial KA</h3>
<ul>
<li><strong>Authors: </strong>Sviatoslav Dzhenzher, Michael H. Freedman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.FA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05255">https://arxiv.org/abs/2504.05255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05255">https://arxiv.org/pdf/2504.05255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05255]] Adversarial KA(https://arxiv.org/abs/2504.05255)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Regarding the representation theorem of Kolmogorov and Arnold (KA) as an algorithm for representing or «expressing» functions, we test its robustness by analyzing its ability to withstand adversarial attacks. We find KA to be robust to countable collections of continuous adversaries, but unearth a question about the equi-continuity of the outer functions that, so far, obstructs taking limits and defeating continuous groups of adversaries. This question on the regularity of the outer functions is relevant to the debate over the applicability of KA to the general theory of NNs.</li>
</ul>

<h3>Title: Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Adrián Bazaga, Rexhina Blloshmi, Bill Byrne, Adrià de Gispert</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05258">https://arxiv.org/abs/2504.05258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05258">https://arxiv.org/pdf/2504.05258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05258]] Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models(https://arxiv.org/abs/2504.05258)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as powerful tools for generating coherent text, understanding context, and performing reasoning tasks. However, they struggle with temporal reasoning, which requires processing time-related information such as event sequencing, durations, and inter-temporal relationships. These capabilities are critical for applications including question answering, scheduling, and historical analysis. In this paper, we introduce TISER, a novel framework that enhances the temporal reasoning abilities of LLMs through a multi-stage process that combines timeline construction with iterative self-reflection. Our approach leverages test-time scaling to extend the length of reasoning traces, enabling models to capture complex temporal dependencies more effectively. This strategy not only boosts reasoning accuracy but also improves the traceability of the inference process. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, including out-of-distribution test sets, and reveal that TISER enables smaller open-source models to surpass larger closed-weight models on challenging temporal reasoning tasks.</li>
</ul>

<h3>Title: Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Yan, Yu Lu, Renjun Xu, Zhenzhong Lan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05262">https://arxiv.org/abs/2504.05262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05262">https://arxiv.org/pdf/2504.05262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05262]] Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models(https://arxiv.org/abs/2504.05262)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite high benchmark scores, Large Language Models (LLMs) often fail simple problem, raising a critical question: Do LLMs learn mathematical principles or merely memorize patterns? Rather than designing increasingly complex benchmarks like recent works, we investigate this using elementary two-integer addition ($0$ to $2^{64}$), probing two core properties: commutativity ($A+B=B+A$) and compositional generalization (via isomorphic symbolic mappings, e.g., $7 \rightarrow y$). While state-of-the-art LLMs achieve 73.8-99.8\% accuracy on numerical addition, performance collapses to $\leq$7.5\% under symbolic mapping, indicating failure to generalize learned rules. Non-monotonic performance scaling with digit count and frequent commutativity violations (over 1,700 cases of $A+B \neq B+A$) further support this. Explicitly providing addition rules degrades performance by 81.2\% on average, while self-explanation maintains baseline accuracy, suggesting LLM arithmetic processing is misaligned with human-defined principles. Our findings indicate current LLMs rely on memory pattern over genuine rule learning, highlighting architectural limitations and the need for new approaches to achieve true mathematical reasoning.</li>
</ul>

<h3>Title: AnomalousNet: A Hybrid Approach with Attention U-Nets and Change Point Detection for Accurate Characterization of Anomalous Diffusion in Video Data</h3>
<ul>
<li><strong>Authors: </strong>Yusef Ahsini, Marc Escoto, J. Alberto Conejero</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05271">https://arxiv.org/abs/2504.05271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05271">https://arxiv.org/pdf/2504.05271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05271]] AnomalousNet: A Hybrid Approach with Attention U-Nets and Change Point Detection for Accurate Characterization of Anomalous Diffusion in Video Data(https://arxiv.org/abs/2504.05271)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Anomalous diffusion occurs in a wide range of systems, including protein transport within cells, animal movement in complex habitats, pollutant dispersion in groundwater, and nanoparticle motion in synthetic materials. Accurately estimating the anomalous diffusion exponent and the diffusion coefficient from the particle trajectories is essential to distinguish between sub-diffusive, super-diffusive, or normal diffusion regimes. These estimates provide a deeper insight into the underlying dynamics of the system, facilitating the identification of particle behaviors and the detection of changes in diffusion states. However, analyzing short and noisy video data, which often yield incomplete and heterogeneous trajectories, poses a significant challenge for traditional statistical approaches. We introduce a data-driven method that integrates particle tracking, an attention U-Net architecture, and a change-point detection algorithm to address these issues. This approach not only infers the anomalous diffusion parameters with high accuracy but also identifies temporal transitions between different states, even in the presence of noise and limited temporal resolution. Our methodology demonstrated strong performance in the 2nd Anomalous Diffusion (AnDi) Challenge benchmark within the top submissions for video tasks.</li>
</ul>

<h3>Title: Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Chu, Peng He, Hang Li, Haoyu Han, Kaiqi Yang, Yu Xue, Tingting Li, Joseph Krajcik, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05276">https://arxiv.org/abs/2504.05276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05276">https://arxiv.org/pdf/2504.05276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05276]] Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented Generation(https://arxiv.org/abs/2504.05276)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Short answer assessment is a vital component of science education, allowing evaluation of students' complex three-dimensional understanding. Large language models (LLMs) that possess human-like ability in linguistic tasks are increasingly popular in assisting human graders to reduce their workload. However, LLMs' limitations in domain knowledge restrict their understanding in task-specific requirements and hinder their ability to achieve satisfactory performance. Retrieval-augmented generation (RAG) emerges as a promising solution by enabling LLMs to access relevant domain-specific knowledge during assessment. In this work, we propose an adaptive RAG framework for automated grading that dynamically retrieves and incorporates domain-specific knowledge based on the question and student answer context. Our approach combines semantic search and curated educational sources to retrieve valuable reference materials. Experimental results in a science education dataset demonstrate that our system achieves an improvement in grading accuracy compared to baseline LLM approaches. The findings suggest that RAG-enhanced grading systems can serve as reliable support with efficient performance gains.</li>
</ul>

<h3>Title: Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations</h3>
<ul>
<li><strong>Authors: </strong>Pedro Ferreira, Wilker Aziz, Ivan Titov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05294">https://arxiv.org/abs/2504.05294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05294">https://arxiv.org/pdf/2504.05294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05294]] Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations(https://arxiv.org/abs/2504.05294)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-thought explanations are widely used to inspect the decision process of large language models (LLMs) and to evaluate the trustworthiness of model outputs, making them important for effective collaboration between LLMs and humans. We demonstrate that preference optimization - a key step in the alignment phase - can inadvertently reduce the faithfulness of these explanations. This occurs because the reward model (RM), which guides alignment, is tasked with optimizing both the expected quality of the response and the appropriateness of the explanations (e.g., minimizing bias or adhering to safety standards), creating potential conflicts. The RM lacks a mechanism to assess the consistency between the model's internal decision process and the generated explanation. Consequently, the LLM may engage in "reward hacking" by producing a final response that scores highly while giving an explanation tailored to maximize reward rather than accurately reflecting its reasoning. To address this issue, we propose enriching the RM's input with a causal attribution of the prediction, allowing the RM to detect discrepancies between the generated self-explanation and the model's decision process. In controlled settings, we show that this approach reduces the tendency of the LLM to generate misleading explanations.</li>
</ul>

<h3>Title: One-Minute Video Generation with Test-Time Training</h3>
<ul>
<li><strong>Authors: </strong>Karan Dalal, Daniel Koceja, Gashon Hussein, Jiarui Xu, Yue Zhao, Youjin Song, Shihao Han, Ka Chun Cheung, Jan Kautz, Carlos Guestrin, Tatsunori Hashimoto, Sanmi Koyejo, Yejin Choi, Yu Sun, Xiaolong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05298">https://arxiv.org/abs/2504.05298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05298">https://arxiv.org/pdf/2504.05298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05298]] One-Minute Video Generation with Test-Time Training(https://arxiv.org/abs/2504.05298)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers today still struggle to generate one-minute videos because self-attention layers are inefficient for long context. Alternatives such as Mamba layers struggle with complex multi-scene stories because their hidden states are less expressive. We experiment with Test-Time Training (TTT) layers, whose hidden states themselves can be neural networks, therefore more expressive. Adding TTT layers into a pre-trained Transformer enables it to generate one-minute videos from text storyboards. For proof of concept, we curate a dataset based on Tom and Jerry cartoons. Compared to baselines such as Mamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers generate much more coherent videos that tell complex stories, leading by 34 Elo points in a human evaluation of 100 videos per method. Although promising, results still contain artifacts, likely due to the limited capability of the pre-trained 5B model. The efficiency of our implementation can also be improved. We have only experimented with one-minute videos due to resource constraints, but the approach can be extended to longer videos and more complex stories. Sample videos, code and annotations are available at: this https URL</li>
</ul>

<h3>Title: Dimension-Free Convergence of Diffusion Models for Approximate Gaussian Mixtures</h3>
<ul>
<li><strong>Authors: </strong>Gen Li, Changxiao Cai, Yuting Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05300">https://arxiv.org/abs/2504.05300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05300">https://arxiv.org/pdf/2504.05300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05300]] Dimension-Free Convergence of Diffusion Models for Approximate Gaussian Mixtures(https://arxiv.org/abs/2504.05300)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are distinguished by their exceptional generative performance, particularly in producing high-quality samples through iterative denoising. While current theory suggests that the number of denoising steps required for accurate sample generation should scale linearly with data dimension, this does not reflect the practical efficiency of widely used algorithms like Denoising Diffusion Probabilistic Models (DDPMs). This paper investigates the effectiveness of diffusion models in sampling from complex high-dimensional distributions that can be well-approximated by Gaussian Mixture Models (GMMs). For these distributions, our main result shows that DDPM takes at most $\widetilde{O}(1/\varepsilon)$ iterations to attain an $\varepsilon$-accurate distribution in total variation (TV) distance, independent of both the ambient dimension $d$ and the number of components $K$, up to logarithmic factors. Furthermore, this result remains robust to score estimation errors. These findings highlight the remarkable effectiveness of diffusion models in high-dimensional settings given the universal approximation capability of GMMs, and provide theoretical insights into their practical success.</li>
</ul>

<h3>Title: S^4M: Boosting Semi-Supervised Instance Segmentation with SAM</h3>
<ul>
<li><strong>Authors: </strong>Heeji Yoon, Heeseong Shin, Eunbeen Hong, Hyunwook Choi, Hansang Cho, Daun Jeong, Seungryong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05301">https://arxiv.org/abs/2504.05301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05301">https://arxiv.org/pdf/2504.05301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05301]] S^4M: Boosting Semi-Supervised Instance Segmentation with SAM(https://arxiv.org/abs/2504.05301)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Semi-supervised instance segmentation poses challenges due to limited labeled data, causing difficulties in accurately localizing distinct object instances. Current teacher-student frameworks still suffer from performance constraints due to unreliable pseudo-label quality stemming from limited labeled data. While the Segment Anything Model (SAM) offers robust segmentation capabilities at various granularities, directly applying SAM to this task introduces challenges such as class-agnostic predictions and potential over-segmentation. To address these complexities, we carefully integrate SAM into the semi-supervised instance segmentation framework, developing a novel distillation method that effectively captures the precise localization capabilities of SAM without compromising semantic recognition. Furthermore, we incorporate pseudo-label refinement as well as a specialized data augmentation with the refined pseudo-labels, resulting in superior performance. We establish state-of-the-art performance, and provide comprehensive experiments and ablation studies to validate the effectiveness of our proposed approach.</li>
</ul>

<h3>Title: Gaussian Mixture Flow Matching Models</h3>
<ul>
<li><strong>Authors: </strong>Hansheng Chen, Kai Zhang, Hao Tan, Zexiang Xu, Fujun Luan, Leonidas Guibas, Gordon Wetzstein, Sai Bi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05304">https://arxiv.org/abs/2504.05304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05304">https://arxiv.org/pdf/2504.05304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05304]] Gaussian Mixture Flow Matching Models(https://arxiv.org/abs/2504.05304)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an $L_2$ denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256$\times$256.</li>
</ul>

<h3>Title: URECA: Unique Region Caption Anything</h3>
<ul>
<li><strong>Authors: </strong>Sangbeom Lim, Junwan Kim, Heeji Yoon, Jaewoo Jung, Seungryong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05305">https://arxiv.org/abs/2504.05305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05305">https://arxiv.org/pdf/2504.05305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05305]] URECA: Unique Region Caption Anything(https://arxiv.org/abs/2504.05305)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Region-level captioning aims to generate natural language descriptions for specific image regions while highlighting their distinguishing features. However, existing methods struggle to produce unique captions across multi-granularity, limiting their real-world applicability. To address the need for detailed region-level understanding, we introduce URECA dataset, a large-scale dataset tailored for multi-granularity region captioning. Unlike prior datasets that focus primarily on salient objects, URECA dataset ensures a unique and consistent mapping between regions and captions by incorporating a diverse set of objects, parts, and background elements. Central to this is a stage-wise data curation pipeline, where each stage incrementally refines region selection and caption generation. By leveraging Multimodal Large Language Models (MLLMs) at each stage, our pipeline produces distinctive and contextually grounded captions with improved accuracy and semantic diversity. Building upon this dataset, we present URECA, a novel captioning model designed to effectively encode multi-granularity regions. URECA maintains essential spatial properties such as position and shape through simple yet impactful modifications to existing MLLMs, enabling fine-grained and semantically rich region descriptions. Our approach introduces dynamic mask modeling and a high-resolution mask encoder to enhance caption uniqueness. Experiments show that URECA achieves state-of-the-art performance on URECA dataset and generalizes well to existing region-level captioning benchmarks.</li>
</ul>

<h3>Title: CREA: A Collaborative Multi-Agent Framework for Creative Content Generation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kavana Venkatesh, Connor Dunlop, Pinar Yanardag</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.05306">https://arxiv.org/abs/2504.05306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.05306">https://arxiv.org/pdf/2504.05306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.05306]] CREA: A Collaborative Multi-Agent Framework for Creative Content Generation with Diffusion Models(https://arxiv.org/abs/2504.05306)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Creativity in AI imagery remains a fundamental challenge, requiring not only the generation of visually compelling content but also the capacity to add novel, expressive, and artistically rich transformations to images. Unlike conventional editing tasks that rely on direct prompt-based modifications, creative image editing demands an autonomous, iterative approach that balances originality, coherence, and artistic intent. To address this, we introduce CREA, a novel multi-agent collaborative framework that mimics the human creative process. Our framework leverages a team of specialized AI agents who dynamically collaborate to conceptualize, generate, critique, and enhance images. Through extensive qualitative and quantitative evaluations, we demonstrate that CREA significantly outperforms state-of-the-art methods in diversity, semantic alignment, and creative transformation. By structuring creativity as a dynamic, agentic process, CREA redefines the intersection of AI and art, paving the way for autonomous AI-driven artistic exploration, generative design, and human-AI co-creation. To the best of our knowledge, this is the first work to introduce the task of creative editing.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
