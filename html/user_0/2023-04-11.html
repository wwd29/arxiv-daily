<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Efficient Secure Aggregation for Privacy-Preserving Federated Machine Learning. (arXiv:2304.03841v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03841">http://arxiv.org/abs/2304.03841</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03841] Efficient Secure Aggregation for Privacy-Preserving Federated Machine Learning](http://arxiv.org/abs/2304.03841) #secure</code></li>
<li>Summary: <p>Federated learning introduces a novel approach to training machine learning
(ML) models on distributed data while preserving user's data privacy. This is
done by distributing the model to clients to perform training on their local
data and computing the final model at a central server. To prevent any data
leakage from the local model updates, various works with focus on secure
aggregation for privacy preserving federated learning have been proposed.
Despite their merits, most of the existing protocols still incur high
communication and computation overhead on the participating entities and might
not be optimized to efficiently handle the large update vectors for ML models.
In this paper, we present E-seaML, a novel secure aggregation protocol with
high communication and computation efficiency. E-seaML only requires one round
of communication in the aggregation phase and it is up to 318x and 1224x faster
for the user and the server (respectively) as compared to its most efficient
counterpart. E-seaML also allows for efficiently verifying the integrity of the
final model by allowing the aggregation server to generate a proof of honest
aggregation for the participating users. This high efficiency and versatility
is achieved by extending (and weakening) the assumption of the existing works
on the set of honest parties (i.e., users) to a set of assisting nodes.
Therefore, we assume a set of assisting nodes which assist the aggregation
server in the aggregation process. We also discuss, given the minimal
computation and communication overhead on the assisting nodes, how one could
assume a set of rotating users to as assisting nodes in each iteration. We
provide the open-sourced implementation of E-seaML for public verifiability and
testing.
</p></li>
</ul>

<h3>Title: Secure Routing Protocol To Mitigate Attacks By Using Blockchain Technology In Manet. (arXiv:2304.04254v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04254">http://arxiv.org/abs/2304.04254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04254] Secure Routing Protocol To Mitigate Attacks By Using Blockchain Technology In Manet](http://arxiv.org/abs/2304.04254) #secure</code></li>
<li>Summary: <p>MANET is a collection of mobile nodes that communicate through wireless
networks as they move from one point to another. MANET is an
infrastructure-less network with a changeable topology; as a result, it is very
susceptible to attacks. MANET attack prevention represents a serious
difficulty. Malicious network nodes are the source of network-based attacks. In
a MANET, attacks can take various forms, and each one alters the network's
operation in its unique way. In general, attacks can be separated into two
categories: those that target the data traffic on a network and those that
target the control traffic. This article explains the many sorts of assaults,
their impact on MANET, and the MANET-based defence measures that are currently
in place. The suggested SRA that employs blockchain technology (SRABC) protects
MANET from attacks and authenticates nodes. The secure routing algorithm (SRA)
proposed by blockchain technology safeguards control and data flow against
threats. This is achieved by generating a Hash Function for every transaction.
We will begin by discussing the security of the MANET. This article's second
section explores the role of blockchain in MANET security. In the third
section, the SRA is described in connection with blockchain. In the fourth
phase, PDR and Throughput are utilised to conduct an SRA review using
Blockchain employing PDR and Throughput. The results suggest that the proposed
technique enhances MANET security while concurrently decreasing delay. The
performance of the proposed technique is analysed and compared to the routing
protocols Q-AODV and DSR.
</p></li>
</ul>

<h3>Title: A Deep Analysis of Hybrid-Multikey-PUF. (arXiv:2304.04381v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04381">http://arxiv.org/abs/2304.04381</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04381] A Deep Analysis of Hybrid-Multikey-PUF](http://arxiv.org/abs/2304.04381) #secure</code></li>
<li>Summary: <p>Unique key generation is essential for encryption purposes between Internet
of Things (IoT) devices. To produce a unique key for this encryption, Physical
Unclonable Functions (PUFs) might be employed. Also, the Random Number
Generator (RNG) is used in many different domains; nonetheless, security is one
of the most important areas that require the best RNG. In this article, We
investigate the quality of random numbers generated by Physical Unclonable
Functions (PUFs). We have analyzed three Figures of Merit (FoMs), Uniqueness,
Randomness, and Reliability of PUFs implemented on different FPGAs. In our
experiments, we have operated the test devices at different temperatures
(20{\deg}F, 40{\deg}F, 60{\deg}F, 80{\deg}F, 120{\deg}F, 140{\deg}F). In the
PUF that we have analyzed, the key is generated in 1 second on average. We also
have analyzed and described the essential properties of random number generator
that is most vital considering things to secure our Internet of Things(IoT)
devices.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: KeyDetect --Detection of anomalies and user based on Keystroke Dynamics. (arXiv:2304.03958v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03958">http://arxiv.org/abs/2304.03958</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03958] KeyDetect --Detection of anomalies and user based on Keystroke Dynamics](http://arxiv.org/abs/2304.03958) #security</code></li>
<li>Summary: <p>Cyber attacks has always been of a great concern. Websites and services with
poor security layers are the most vulnerable to such cyber attacks. The
attackers can easily access sensitive data like credit card details and social
security number from such vulnerable services. Currently to stop cyber attacks,
various different methods are opted from using two-step verification methods
like One-Time Password and push notification services to using high-end
bio-metric devices like finger print reader and iris scanner are used as
security layers. These current security measures carry a lot of cons and the
worst is that user always need to carry the authentication device on them to
access their data. To overcome this, we are proposing a technique of using
keystroke dynamics (typing pattern) of a user to authenticate the genuine user.
In the method, we are taking a data set of 51 users typing a password in 8
sessions done on alternate days to record mood fluctuations of the user.
Developed and implemented anomaly-detection algorithm based on distance metrics
and machine learning algorithms like Artificial Neural networks (ANN) and
convolutional neural network (CNN) to classify the users. In ANN, we
implemented multi-class classification using 1-D convolution as the data was
correlated and multi-class classification with negative class which was used to
classify anomaly based on all users put together. We were able to achieve an
accuracy of 95.05% using ANN with Negative Class. From the results achieved, we
can say that the model works perfectly and can be bought into the market as a
security layer and a good alternative to two-step verification using external
devices. This technique will enable users to have two-step security layer
without worrying about carry an authentication device.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Privacy-Preserving CNN Training with Transfer Learning. (arXiv:2304.03807v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03807">http://arxiv.org/abs/2304.03807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03807] Privacy-Preserving CNN Training with Transfer Learning](http://arxiv.org/abs/2304.03807) #privacy</code></li>
<li>Summary: <p>Privacy-preserving nerual network inference has been well studied while
homomorphic CNN training still remains an open challenging task. In this paper,
we present a practical solution to implement privacy-preserving CNN training
based on mere Homomorphic Encryption (HE) technique. To our best knowledge,
this is the first attempt successfully to crack this nut and no work ever
before has achieved this goal. Several techniques combine to make it done: (1)
with transfer learning, privacy-preserving CNN training can be reduced to
homomorphic neural network training, or even multiclass logistic regression
(MLR) training; (2) via a faster gradient variant called $\texttt{Quadratic
Gradient}$, an enhanced gradient method for MLR with a state-of-the-art
performance in converge speed is applied in this work to achieve high
performance; (3) we employ the thought of transformation in mathematics to
transform approximating Softmax function in encryption domain to the
well-studied approximation of Sigmoid function. A new type of loss function is
alongside been developed to complement this change; and (4) we use a simple but
flexible matrix-encoding method named $\texttt{Volley Revolver}$ to manage the
data flow in the ciphertexts, which is the key factor to complete the whole
homomorphic CNN training. The complete, runnable C++ code to implement our work
can be found at: https://github.com/petitioner/HE.CNNtraining.
</p></li>
</ul>

<p>We select $\texttt{REGNET\_X\_400MF}$ as our pre-train model for using
transfer learning. We use the first 128 MNIST training images as training data
and the whole MNIST testing dataset as the testing data. The client only needs
to upload 6 ciphertexts to the cloud and it takes $\sim 21$ mins to perform 2
iterations on a cloud with 64 vCPUs, resulting in a precision of $21.49\%$.
</p>

<h3>Title: Differentially Private Numerical Vector Analyses in the Local and Shuffle Model. (arXiv:2304.04410v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04410">http://arxiv.org/abs/2304.04410</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04410] Differentially Private Numerical Vector Analyses in the Local and Shuffle Model](http://arxiv.org/abs/2304.04410) #privacy</code></li>
<li>Summary: <p>Numerical vector aggregation plays a crucial role in privacy-sensitive
applications, such as distributed gradient estimation in federated learning and
statistical analysis of key-value data. In the context of local differential
privacy, this study provides a tight minimax error bound of
$O(\frac{ds}{n\epsilon^2})$, where $d$ represents the dimension of the
numerical vector and $s$ denotes the number of non-zero entries. By converting
the conditional/unconditional numerical mean estimation problem into a
frequency estimation problem, we develop an optimal and efficient mechanism
called Collision. In contrast, existing methods exhibit sub-optimal error rates
of $O(\frac{d^2}{n\epsilon^2})$ or $O(\frac{ds^2}{n\epsilon^2})$. Specifically,
for unconditional mean estimation, we leverage the negative correlation between
two frequencies in each dimension and propose the CoCo mechanism, which further
reduces estimation errors for mean values compared to Collision. Moreover, to
surpass the error barrier in local privacy, we examine privacy amplification in
the shuffle model for the proposed mechanisms and derive precisely tight
amplification bounds. Our experiments validate and compare our mechanisms with
existing approaches, demonstrating significant error reductions for frequency
estimation and mean estimation on numerical vectors.
</p></li>
</ul>

<h3>Title: Homogenizing Non-IID datasets via In-Distribution Knowledge Distillation for Decentralized Learning. (arXiv:2304.04326v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04326">http://arxiv.org/abs/2304.04326</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04326] Homogenizing Non-IID datasets via In-Distribution Knowledge Distillation for Decentralized Learning](http://arxiv.org/abs/2304.04326) #privacy</code></li>
<li>Summary: <p>Decentralized learning enables serverless training of deep neural networks
(DNNs) in a distributed manner on multiple nodes. This allows for the use of
large datasets, as well as the ability to train with a wide variety of data
sources. However, one of the key challenges with decentralized learning is
heterogeneity in the data distribution across the nodes. In this paper, we
propose In-Distribution Knowledge Distillation (IDKD) to address the challenge
of heterogeneous data distribution. The goal of IDKD is to homogenize the data
distribution across the nodes. While such data homogenization can be achieved
by exchanging data among the nodes sacrificing privacy, IDKD achieves the same
objective using a common public dataset across nodes without breaking the
privacy constraint. This public dataset is different from the training dataset
and is used to distill the knowledge from each node and communicate it to its
neighbors through the generated labels. With traditional knowledge
distillation, the generalization of the distilled model is reduced because all
the public dataset samples are used irrespective of their similarity to the
local dataset. Thus, we introduce an Out-of-Distribution (OoD) detector at each
node to label a subset of the public dataset that maps close to the local
training data distribution. Finally, only labels corresponding to these subsets
are exchanged among the nodes and with appropriate label averaging each node is
finetuned on these data subsets along with its local data. Our experiments on
multiple image classification datasets and graph topologies show that the
proposed IDKD scheme is more effective than traditional knowledge distillation
and achieves state-of-the-art generalization performance on heterogeneously
distributed data with minimal communication overhead.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Unsupervised Multi-Criteria Adversarial Detection in Deep Image Retrieval. (arXiv:2304.04228v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04228">http://arxiv.org/abs/2304.04228</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04228] Unsupervised Multi-Criteria Adversarial Detection in Deep Image Retrieval](http://arxiv.org/abs/2304.04228) #defense</code></li>
<li>Summary: <p>The vulnerability in the algorithm supply chain of deep learning has imposed
new challenges to image retrieval systems in the downstream. Among a variety of
techniques, deep hashing is gaining popularity. As it inherits the algorithmic
backend from deep learning, a handful of attacks are recently proposed to
disrupt normal image retrieval. Unfortunately, the defense strategies in
softmax classification are not readily available to be applied in the image
retrieval domain. In this paper, we propose an efficient and unsupervised
scheme to identify unique adversarial behaviors in the hamming space. In
particular, we design three criteria from the perspectives of hamming distance,
quantization loss and denoising to defend against both untargeted and targeted
attacks, which collectively limit the adversarial space. The extensive
experiments on four datasets demonstrate 2-23% improvements of detection rates
with minimum computational overhead for real-time image queries.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Attack is Good Augmentation: Towards Skeleton-Contrastive Representation Learning. (arXiv:2304.04023v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04023">http://arxiv.org/abs/2304.04023</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04023] Attack is Good Augmentation: Towards Skeleton-Contrastive Representation Learning](http://arxiv.org/abs/2304.04023) #attack</code></li>
<li>Summary: <p>Contrastive learning, relying on effective positive and negative sample
pairs, is beneficial to learn informative skeleton representations in
unsupervised skeleton-based action recognition. To achieve these positive and
negative pairs, existing weak/strong data augmentation methods have to randomly
change the appearance of skeletons for indirectly pursuing semantic
perturbations. However, such approaches have two limitations: 1) solely
perturbing appearance cannot well capture the intrinsic semantic information of
skeletons, and 2) randomly perturbation may change the original
positive/negative pairs to soft positive/negative ones. To address the above
dilemma, we start the first attempt to explore an attack-based augmentation
scheme that additionally brings in direct semantic perturbation, for
constructing hard positive pairs and further assisting in constructing hard
negative pairs. In particular, we propose a novel Attack-Augmentation
Mixing-Contrastive learning (A$^2$MC) to contrast hard positive features and
hard negative features for learning more robust skeleton representations. In
A$^2$MC, Attack-Augmentation (Att-Aug) is designed to collaboratively perform
targeted and untargeted perturbations of skeletons via attack and augmentation
respectively, for generating high-quality hard positive features. Meanwhile,
Positive-Negative Mixer (PNM) is presented to mix hard positive features and
negative features for generating hard negative features, which are adopted for
updating the mixed memory banks. Extensive experiments on three public datasets
demonstrate that A$^2$MC is competitive with the state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Generating Adversarial Attacks in the Latent Space. (arXiv:2304.04386v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04386">http://arxiv.org/abs/2304.04386</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04386] Generating Adversarial Attacks in the Latent Space](http://arxiv.org/abs/2304.04386) #attack</code></li>
<li>Summary: <p>Adversarial attacks in the input (pixel) space typically incorporate noise
margins such as $L_1$ or $L_{\infty}$-norm to produce imperceptibly perturbed
data that confound deep learning networks. Such noise margins confine the
magnitude of permissible noise. In this work, we propose injecting adversarial
perturbations in the latent (feature) space using a generative adversarial
network, removing the need for margin-based priors. Experiments on MNIST,
CIFAR10, Fashion-MNIST, CIFAR100 and Stanford Dogs datasets support the
effectiveness of the proposed method in generating adversarial attacks in the
latent space while ensuring a high degree of visual realism with respect to
pixel-based adversarial attack methods.
</p></li>
</ul>

<h3>Title: A Continued Fraction-Hyperbola based Attack on RSA cryptosystem. (arXiv:2304.03957v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03957">http://arxiv.org/abs/2304.03957</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03957] A Continued Fraction-Hyperbola based Attack on RSA cryptosystem](http://arxiv.org/abs/2304.03957) #attack</code></li>
<li>Summary: <p>In this paper we present new arithmetical and algebraic results following the
work of Babindamana and al. on hyperbolas and describe from the new results an
approach to attacking a RSA-type modulus based on continued fractions,
independent and not bounded by the size of the private key $d$ nor public
exponent $e$ compared to Wiener's attack. When successful, this attack is
bounded by $\displaystyle\mathcal{O}\left(
b\log{\alpha_{j4}}\log{(\alpha_{i3}+\alpha_{j3})}\right)$ with $b=10^{y}$,
$\alpha_{i3}+\alpha_{j3}$ a non trivial factor of $n$ and $\alpha_{j4}$ such
that $(n+1)/(n-1)=\alpha_{i4}/\alpha_{j4}$. The primary goal of this attack is
to find a point $\displaystyle X_{\alpha}=\left(-\alpha_{3}, \ \alpha_{3}+1
\right) \in \mathbb{Z}^{2}<em>{\star}$ that satisfies $\displaystyle\left\langle
X</em>{\alpha_{3}}, \ P_{3} \right\rangle =0$ from a convergent of
$\displaystyle\frac{\alpha_{i4}}{\alpha_{j4}}+\delta$, with $P_{3}\in
\mathcal{B}<em>{n}(x, y)</em>{\mid_{x\geq 4n}}$. We finally present some experimental
examples. We believe these results constitute a new direction in RSA
Cryptanalysis using continued fractions.
</p></li>
</ul>

<h3>Title: Certifiable Black-Box Attack: Ensuring Provably Successful Attack for Adversarial Examples. (arXiv:2304.04343v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04343">http://arxiv.org/abs/2304.04343</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04343] Certifiable Black-Box Attack: Ensuring Provably Successful Attack for Adversarial Examples](http://arxiv.org/abs/2304.04343) #attack</code></li>
<li>Summary: <p>Black-box adversarial attacks have shown strong potential to subvert machine
learning models. Existing black-box adversarial attacks craft the adversarial
examples by iteratively querying the target model and/or leveraging the
transferability of a local surrogate model. Whether such attack can succeed
remains unknown to the adversary when empirically designing the attack. In this
paper, to our best knowledge, we take the first step to study a new paradigm of
adversarial attacks -- certifiable black-box attack that can guarantee the
attack success rate of the crafted adversarial examples. Specifically, we
revise the randomized smoothing to establish novel theories for ensuring the
attack success rate of the adversarial examples. To craft the adversarial
examples with the certifiable attack success rate (CASR) guarantee, we design
several novel techniques, including a randomized query method to query the
target model, an initialization method with smoothed self-supervised
perturbation to derive certifiable adversarial examples, and a geometric
shifting method to reduce the perturbation size of the certifiable adversarial
examples for better imperceptibility. We have comprehensively evaluated the
performance of the certifiable black-box attack on CIFAR10 and ImageNet
datasets against different levels of defenses. Both theoretical and
experimental results have validated the effectiveness of the proposed
certifiable attack.
</p></li>
</ul>

<h3>Title: Quantum Cyber-Attack on Blockchain-based VANET. (arXiv:2304.04411v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04411">http://arxiv.org/abs/2304.04411</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04411] Quantum Cyber-Attack on Blockchain-based VANET](http://arxiv.org/abs/2304.04411) #attack</code></li>
<li>Summary: <p>Blockchain-based Vehicular Ad-hoc Network (VANET) is widely considered as
secure communication architecture for a connected transportation system. With
the advent of quantum computing, there are concerns regarding the vulnerability
of this architecture against cyber-attacks. In this study, a potential threat
is investigated in a blockchain-based VANET, and a corresponding quantum
cyber-attack is developed. Specifically, a quantum impersonation attack using
Quantum-Shor algorithm is developed to break the Rivest-Shamir-Adleman (RSA)
encrypted digital signatures of VANET and thus create a threat for the
trust-based blockchain scheme of VANET. A blockchain-based VANET,
vehicle-to-everything (V2X) communication, and vehicular mobility are simulated
using OMNET++, the extended INET library, and vehicles-in-network simulation
(VEINS) along with simulation of urban mobility (SUMO), respectively. A small
key RSA based message encryption is implemented using IBM Qiskit, which is an
open-source quantum software development kit. The findings reveal that the
quantum cyber-attack, example, impersonation attack is able to successfully
break the trust chain of a blockchain-based VANET. This highlights the need for
a quantum secured blockchain.
</p></li>
</ul>

<h3>Title: Robust Deep Learning Models Against Semantic-Preserving Adversarial Attack. (arXiv:2304.03955v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03955">http://arxiv.org/abs/2304.03955</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03955] Robust Deep Learning Models Against Semantic-Preserving Adversarial Attack](http://arxiv.org/abs/2304.03955) #attack</code></li>
<li>Summary: <p>Deep learning models can be fooled by small $l_p$-norm adversarial
perturbations and natural perturbations in terms of attributes. Although the
robustness against each perturbation has been explored, it remains a challenge
to address the robustness against joint perturbations effectively. In this
paper, we study the robustness of deep learning models against joint
perturbations by proposing a novel attack mechanism named Semantic-Preserving
Adversarial (SPA) attack, which can then be used to enhance adversarial
training. Specifically, we introduce an attribute manipulator to generate
natural and human-comprehensible perturbations and a noise generator to
generate diverse adversarial noises. Based on such combined noises, we optimize
both the attribute value and the diversity variable to generate
jointly-perturbed samples. For robust training, we adversarially train the deep
learning model against the generated joint perturbations. Empirical results on
four benchmarks show that the SPA attack causes a larger performance decline
with small $l_{\infty}$ norm-ball constraints compared to existing approaches.
Furthermore, our SPA-enhanced training outperforms existing defense methods
against such joint perturbations.
</p></li>
</ul>

<h3>Title: RobCaps: Evaluating the Robustness of Capsule Networks against Affine Transformations and Adversarial Attacks. (arXiv:2304.03973v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03973">http://arxiv.org/abs/2304.03973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03973] RobCaps: Evaluating the Robustness of Capsule Networks against Affine Transformations and Adversarial Attacks](http://arxiv.org/abs/2304.03973) #attack</code></li>
<li>Summary: <p>Capsule Networks (CapsNets) are able to hierarchically preserve the pose
relationships between multiple objects for image classification tasks. Other
than achieving high accuracy, another relevant factor in deploying CapsNets in
safety-critical applications is the robustness against input transformations
and malicious adversarial attacks.
</p></li>
</ul>

<p>In this paper, we systematically analyze and evaluate different factors
affecting the robustness of CapsNets, compared to traditional Convolutional
Neural Networks (CNNs). Towards a comprehensive comparison, we test two CapsNet
models and two CNN models on the MNIST, GTSRB, and CIFAR10 datasets, as well as
on the affine-transformed versions of such datasets. With a thorough analysis,
we show which properties of these architectures better contribute to increasing
the robustness and their limitations. Overall, CapsNets achieve better
robustness against adversarial examples and affine transformations, compared to
a traditional CNN with a similar number of parameters. Similar conclusions have
been derived for deeper versions of CapsNets and CNNs. Moreover, our results
unleash a key finding that the dynamic routing does not contribute much to
improving the CapsNets' robustness. Indeed, the main generalization
contribution is due to the hierarchical feature learning through capsules.
</p>

<h2>robust</h2>
<h3>Title: Improving Identity-Robustness for Face Models. (arXiv:2304.03838v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03838">http://arxiv.org/abs/2304.03838</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03838] Improving Identity-Robustness for Face Models](http://arxiv.org/abs/2304.03838) #robust</code></li>
<li>Summary: <p>Despite the success of deep-learning models in many tasks, there have been
concerns about such models learning shortcuts, and their lack of robustness to
irrelevant confounders. When it comes to models directly trained on human
faces, a sensitive confounder is that of human identities. Many face-related
tasks should ideally be identity-independent, and perform uniformly across
different individuals (i.e. be fair). One way to measure and enforce such
robustness and performance uniformity is through enforcing it during training,
assuming identity-related information is available at scale. However, due to
privacy concerns and also the cost of collecting such information, this is
often not the case, and most face datasets simply contain input images and
their corresponding task-related labels. Thus, improving identity-related
robustness without the need for such annotations is of great importance. Here,
we explore using face-recognition embedding vectors, as proxies for identities,
to enforce such robustness. We propose to use the structure in the
face-recognition embedding space, to implicitly emphasize rare samples within
each class. We do so by weighting samples according to their conditional
inverse density (CID) in the proxy embedding space. Our experiments suggest
that such a simple sample weighting scheme, not only improves the training
robustness, it often improves the overall performance as a result of such
robustness. We also show that employing such constraints during training
results in models that are significantly less sensitive to different levels of
bias in the dataset.
</p></li>
</ul>

<h3>Title: Multilingual Augmentation for Robust Visual Question Answering in Remote Sensing Images. (arXiv:2304.03844v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03844">http://arxiv.org/abs/2304.03844</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03844] Multilingual Augmentation for Robust Visual Question Answering in Remote Sensing Images](http://arxiv.org/abs/2304.03844) #robust</code></li>
<li>Summary: <p>Aiming at answering questions based on the content of remotely sensed images,
visual question answering for remote sensing data (RSVQA) has attracted much
attention nowadays. However, previous works in RSVQA have focused little on the
robustness of RSVQA. As we aim to enhance the reliability of RSVQA models, how
to learn robust representations against new words and different question
templates with the same meaning is the key challenge. With the proposed
augmented dataset, we are able to obtain more questions in addition to the
original ones with the same meaning. To make better use of this information, in
this study, we propose a contrastive learning strategy for training robust
RSVQA models against diverse question templates and words. Experimental results
demonstrate that the proposed augmented dataset is effective in improving the
robustness of the RSVQA model. In addition, the contrastive learning strategy
performs well on the low resolution (LR) dataset.
</p></li>
</ul>

<h3>Title: Exploring Data Geometry for Continual Learning. (arXiv:2304.03931v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03931">http://arxiv.org/abs/2304.03931</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03931] Exploring Data Geometry for Continual Learning](http://arxiv.org/abs/2304.03931) #robust</code></li>
<li>Summary: <p>Continual learning aims to efficiently learn from a non-stationary stream of
data while avoiding forgetting the knowledge of old data. In many practical
applications, data complies with non-Euclidean geometry. As such, the commonly
used Euclidean space cannot gracefully capture non-Euclidean geometric
structures of data, leading to inferior results. In this paper, we study
continual learning from a novel perspective by exploring data geometry for the
non-stationary stream of data. Our method dynamically expands the geometry of
the underlying space to match growing geometric structures induced by new data,
and prevents forgetting by keeping geometric structures of old data into
account. In doing so, making use of the mixed curvature space, we propose an
incremental search scheme, through which the growing geometric structures are
encoded. Then, we introduce an angular-regularization loss and a
neighbor-robustness loss to train the model, capable of penalizing the change
of global geometric structures and local geometric structures. Experiments show
that our method achieves better performance than baseline methods designed in
Euclidean space.
</p></li>
</ul>

<h3>Title: Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification. (arXiv:2304.03981v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03981">http://arxiv.org/abs/2304.03981</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03981] Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification](http://arxiv.org/abs/2304.03981) #robust</code></li>
<li>Summary: <p>Failure to recognize samples from the classes unseen during training is a
major limit of artificial intelligence (AI) in real-world implementation of
retinal anomaly classification. To resolve this obstacle, we propose an
uncertainty-inspired open-set (UIOS) model which was trained with fundus images
of 9 common retinal conditions. Besides the probability of each category, UIOS
also calculates an uncertainty score to express its confidence. Our UIOS model
with thresholding strategy achieved an F1 score of 99.55%, 97.01% and 91.91%
for the internal testing set, external testing set and non-typical testing set,
respectively, compared to the F1 score of 92.20%, 80.69% and 64.74% by the
standard AI model. Furthermore, UIOS correctly predicted high uncertainty
scores, which prompted the need for a manual check, in the datasets of rare
retinal diseases, low-quality fundus images, and non-fundus images. This work
provides a robust method for real-world screening of retinal anomalies.
</p></li>
</ul>

<h3>Title: RIDCP: Revitalizing Real Image Dehazing via High-Quality Codebook Priors. (arXiv:2304.03994v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03994">http://arxiv.org/abs/2304.03994</a></li>
<li>Code URL: <a href="https://github.com/RQ-Wu/RIDCP_dehazing">https://github.com/RQ-Wu/RIDCP_dehazing</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03994] RIDCP: Revitalizing Real Image Dehazing via High-Quality Codebook Priors](http://arxiv.org/abs/2304.03994) #robust</code></li>
<li>Summary: <p>Existing dehazing approaches struggle to process real-world hazy images owing
to the lack of paired real data and robust priors. In this work, we present a
new paradigm for real image dehazing from the perspectives of synthesizing more
realistic hazy data and introducing more robust priors into the network.
Specifically, (1) instead of adopting the de facto physical scattering model,
we rethink the degradation of real hazy images and propose a phenomenological
pipeline considering diverse degradation types. (2) We propose a Real Image
Dehazing network via high-quality Codebook Priors (RIDCP). Firstly, a VQGAN is
pre-trained on a large-scale high-quality dataset to obtain the discrete
codebook, encapsulating high-quality priors (HQPs). After replacing the
negative effects brought by haze with HQPs, the decoder equipped with a novel
normalized feature alignment module can effectively utilize high-quality
features and produce clean results. However, although our degradation pipeline
drastically mitigates the domain gap between synthetic and real data, it is
still intractable to avoid it, which challenges HQPs matching in the wild.
Thus, we re-calculate the distance when matching the features to the HQPs by a
controllable matching operation, which facilitates finding better counterparts.
We provide a recommendation to control the matching based on an explainable
solution. Users can also flexibly adjust the enhancement degree as per their
preference. Extensive experiments verify the effectiveness of our data
synthesis pipeline and the superior performance of RIDCP in real image
dehazing.
</p></li>
</ul>

<h3>Title: Analysis of Sampling Strategies for Implicit 3D Reconstruction. (arXiv:2304.03999v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03999">http://arxiv.org/abs/2304.03999</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03999] Analysis of Sampling Strategies for Implicit 3D Reconstruction](http://arxiv.org/abs/2304.03999) #robust</code></li>
<li>Summary: <p>In the training process of the implicit 3D reconstruction network, the choice
of spatial query points' sampling strategy affects the final performance of the
model. Different works have differences in the selection of sampling
strategies, not only in the spatial distribution of query points but also in
the order of magnitude difference in the density of query points. For how to
select the sampling strategy of query points, current works are more akin to an
enumerating operation to find the optimal solution, which seriously affects
work efficiency. In this work, we explored the relationship between sampling
strategy and network final performance through classification analysis and
experimental comparison from three aspects: the relationship between network
type and sampling strategy, the relationship between implicit function and
sampling strategy, and the impact of sampling density on model performance. In
addition, we also proposed two methods, linear sampling and distance masking,
to improve the sampling strategy of query points, making it more robust.
</p></li>
</ul>

<h3>Title: Exploring the Connection between Robust and Generative Models. (arXiv:2304.04033v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04033">http://arxiv.org/abs/2304.04033</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04033] Exploring the Connection between Robust and Generative Models](http://arxiv.org/abs/2304.04033) #robust</code></li>
<li>Summary: <p>We offer a study that connects robust discriminative classifiers trained with
adversarial training (AT) with generative modeling in the form of Energy-based
Models (EBM). We do so by decomposing the loss of a discriminative classifier
and showing that the discriminative model is also aware of the input data
density. Though a common assumption is that adversarial points leave the
manifold of the input data, our study finds out that, surprisingly, untargeted
adversarial points in the input space are very likely under the generative
model hidden inside the discriminative classifier -- have low energy in the
EBM. We present two evidence: untargeted attacks are even more likely than the
natural data and their likelihood increases as the attack strength increases.
This allows us to easily detect them and craft a novel attack called
High-Energy PGD that fools the classifier yet has energy similar to the data
set.
</p></li>
</ul>

<h3>Title: Deep Prototypical-Parts Ease Morphological Kidney Stone Identification and are Competitively Robust to Photometric Perturbations. (arXiv:2304.04077v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04077">http://arxiv.org/abs/2304.04077</a></li>
<li>Code URL: <a href="https://github.com/danielf29/prototipical_parts">https://github.com/danielf29/prototipical_parts</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04077] Deep Prototypical-Parts Ease Morphological Kidney Stone Identification and are Competitively Robust to Photometric Perturbations](http://arxiv.org/abs/2304.04077) #robust</code></li>
<li>Summary: <p>Identifying the type of kidney stones can allow urologists to determine their
cause of formation, improving the prescription of appropriate treatments to
diminish future relapses. Currently, the associated ex-vivo diagnosis (known as
Morpho-constitutional Analysis, MCA) is time-consuming, expensive and requires
a great deal of experience, as it requires a visual analysis component that is
highly operator dependant. Recently, machine learning methods have been
developed for in-vivo endoscopic stone recognition. Deep Learning (DL) based
methods outperform non-DL methods in terms of accuracy but lack explainability.
Despite this trade-off, when it comes to making high-stakes decisions, it's
important to prioritize understandable Computer-Aided Diagnosis (CADx) that
suggests a course of action based on reasonable evidence, rather than a model
prescribing a course of action. In this proposal, we learn Prototypical Parts
(PPs) per kidney stone subtype, which are used by the DL model to generate an
output classification. Using PPs in the classification task enables case-based
reasoning explanations for such output, thus making the model interpretable. In
addition, we modify global visual characteristics to describe their relevance
to the PPs and the sensitivity of our model's performance. With this, we
provide explanations with additional information at the sample, class and model
levels in contrast to previous works. Although our implementation's average
accuracy is lower than state-of-the-art (SOTA) non-interpretable DL models by
1.5 %, our models perform 2.8% better on perturbed images with a lower standard
deviation, without adversarial training. Thus, Learning PPs has the potential
to create more robust DL models.
</p></li>
</ul>

<h3>Title: Token Boosting for Robust Self-Supervised Visual Transformer Pre-training. (arXiv:2304.04175v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04175">http://arxiv.org/abs/2304.04175</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04175] Token Boosting for Robust Self-Supervised Visual Transformer Pre-training](http://arxiv.org/abs/2304.04175) #robust</code></li>
<li>Summary: <p>Learning with large-scale unlabeled data has become a powerful tool for
pre-training Visual Transformers (VTs). However, prior works tend to overlook
that, in real-world scenarios, the input data may be corrupted and unreliable.
Pre-training VTs on such corrupted data can be challenging, especially when we
pre-train via the masked autoencoding approach, where both the inputs and
masked ``ground truth" targets can potentially be unreliable in this case. To
address this limitation, we introduce the Token Boosting Module (TBM) as a
plug-and-play component for VTs that effectively allows the VT to learn to
extract clean and robust features during masked autoencoding pre-training. We
provide theoretical analysis to show how TBM improves model pre-training with
more robust and generalizable representations, thus benefiting downstream
tasks. We conduct extensive experiments to analyze TBM's effectiveness, and
results on four corrupted datasets demonstrate that TBM consistently improves
performance on downstream tasks.
</p></li>
</ul>

<h3>Title: AGAD: Adversarial Generative Anomaly Detection. (arXiv:2304.04211v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04211">http://arxiv.org/abs/2304.04211</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04211] AGAD: Adversarial Generative Anomaly Detection](http://arxiv.org/abs/2304.04211) #robust</code></li>
<li>Summary: <p>Anomaly detection suffered from the lack of anomalies due to the diversity of
abnormalities and the difficulties of obtaining large-scale anomaly data.
Semi-supervised anomaly detection methods are often used to solely leverage
normal data to detect abnormalities that deviated from the learnt normality
distributions. Meanwhile, given the fact that limited anomaly data can be
obtained with a minor cost in practice, some researches also investigated
anomaly detection methods under supervised scenarios with limited anomaly data.
In order to address the lack of abnormal data for robust anomaly detection, we
propose Adversarial Generative Anomaly Detection (AGAD), a self-contrast-based
anomaly detection paradigm that learns to detect anomalies by generating
\textit{contextual adversarial information} from the massive normal examples.
Essentially, our method generates pseudo-anomaly data for both supervised and
semi-supervised anomaly detection scenarios. Extensive experiments are carried
out on multiple benchmark datasets and real-world datasets, the results show
significant improvement in both supervised and semi-supervised scenarios.
Importantly, our approach is data-efficient that can boost up the detection
accuracy with no more than 5% anomalous training data.
</p></li>
</ul>

<h3>Title: Video ChatCaptioner: Towards the Enriched Spatiotemporal Descriptions. (arXiv:2304.04227v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04227">http://arxiv.org/abs/2304.04227</a></li>
<li>Code URL: <a href="https://github.com/vision-cair/chatcaptioner">https://github.com/vision-cair/chatcaptioner</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04227] Video ChatCaptioner: Towards the Enriched Spatiotemporal Descriptions](http://arxiv.org/abs/2304.04227) #robust</code></li>
<li>Summary: <p>Video captioning aims to convey dynamic scenes from videos using natural
language, facilitating the understanding of spatiotemporal information within
our environment. Although there have been recent advances, generating detailed
and enriched video descriptions continues to be a substantial challenge. In
this work, we introduce Video ChatCaptioner, an innovative approach for
creating more comprehensive spatiotemporal video descriptions. Our method
employs a ChatGPT model as a controller, specifically designed to select frames
for posing video content-driven questions. Subsequently, a robust algorithm is
utilized to answer these visual queries. This question-answer framework
effectively uncovers intricate video details and shows promise as a method for
enhancing video content. Following multiple conversational rounds, ChatGPT can
summarize enriched video content based on previous conversations. We
qualitatively demonstrate that our Video ChatCaptioner can generate captions
containing more visual details about the videos. The code is publicly available
at https://github.com/Vision-CAIR/ChatCaptioner
</p></li>
</ul>

<h3>Title: RGB-T Tracking Based on Mixed Attention. (arXiv:2304.04264v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04264">http://arxiv.org/abs/2304.04264</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04264] RGB-T Tracking Based on Mixed Attention](http://arxiv.org/abs/2304.04264) #robust</code></li>
<li>Summary: <p>RGB-T tracking involves the use of images from both visible and thermal
modalities. The primary objective is to adaptively lever-age the relatively
dominant modality in varying conditions to achieve more robust tracking
compared to single-modality track-ing. An RGB-T tracker based on mixed
attention mechanism to achieve complementary fusion of modalities (referred to
as MACFT) is proposed in this paper. In the feature extraction stage, we
utilize different transformer backbone branches to extract specific and shared
information from different modali-ties. By performing mixed attention
operations in the backbone to enable information interaction and
self-enhancement between the template and search images, it constructs a robust
feature representation that better understands the high-level semantic features
of the target. Then, in the feature fusion stage, a modal-ity-adaptive fusion
is achieved through a mixed attention-based modality fusion network, which
suppresses the low-quality mo-dality noise while enhancing the information of
the dominant modality. Evaluation on multiple RGB-T public datasets
demon-strates that our proposed tracker outperforms other RGB-T trackers on
general evaluation metrics while also being able to adapt to long-term tracking
scenarios.
</p></li>
</ul>

<h3>Title: Identity-Guided Collaborative Learning for Cloth-Changing Person Reidentification. (arXiv:2304.04400v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04400">http://arxiv.org/abs/2304.04400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04400] Identity-Guided Collaborative Learning for Cloth-Changing Person Reidentification](http://arxiv.org/abs/2304.04400) #robust</code></li>
<li>Summary: <p>Cloth-changing person reidentification (ReID) is a newly emerging research
topic that is aimed at addressing the issues of large feature variations due to
cloth-changing and pedestrian view/pose changes. Although significant progress
has been achieved by introducing extra information (e.g., human contour
sketching information, human body keypoints, and 3D human information),
cloth-changing person ReID is still challenging due to impressionable
pedestrian representations. Moreover, human semantic information and pedestrian
identity information are not fully explored. To solve these issues, we propose
a novel identity-guided collaborative learning scheme (IGCL) for cloth-changing
person ReID, where the human semantic is fully utilized and the identity is
unchangeable to guide collaborative learning. First, we design a novel clothing
attention degradation stream to reasonably reduce the interference caused by
clothing information where clothing attention and mid-level collaborative
learning are employed. Second, we propose a human semantic attention and body
jigsaw stream to highlight the human semantic information and simulate
different poses of the same identity. In this way, the extraction features not
only focus on human semantic information that is unrelated to the background
but also are suitable for pedestrian pose variations. Moreover, a pedestrian
identity enhancement stream is further proposed to enhance the identity
importance and extract more favorable identity robust features. Most
importantly, all these streams are jointly explored in an end-to-end unified
framework, and the identity is utilized to guide the optimization. Extensive
experiments on five public clothing person ReID datasets demonstrate that the
proposed IGCL significantly outperforms SOTA methods and that the extracted
feature is more robust, discriminative, and clothing-irrelevant.
</p></li>
</ul>

<h3>Title: Meta Compositional Referring Expression Segmentation. (arXiv:2304.04415v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04415">http://arxiv.org/abs/2304.04415</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04415] Meta Compositional Referring Expression Segmentation](http://arxiv.org/abs/2304.04415) #robust</code></li>
<li>Summary: <p>Referring expression segmentation aims to segment an object described by a
language expression from an image. Despite the recent progress on this task,
existing models tackling this task may not be able to fully capture semantics
and visual representations of individual concepts, which limits their
generalization capability, especially when handling novel compositions of
learned concepts. In this work, through the lens of meta learning, we propose a
Meta Compositional Referring Expression Segmentation (MCRES) framework to
enhance model compositional generalization performance. Specifically, to handle
various levels of novel compositions, our framework first uses training data to
construct a virtual training set and multiple virtual testing sets, where data
samples in each virtual testing set contain a level of novel compositions
w.r.t. the virtual training set. Then, following a novel meta optimization
scheme to optimize the model to obtain good testing performance on the virtual
testing sets after training on the virtual training set, our framework can
effectively drive the model to better capture semantics and visual
representations of individual concepts, and thus obtain robust generalization
performance even when handling novel compositions. Extensive experiments on
three benchmark datasets demonstrate the effectiveness of our framework.
</p></li>
</ul>

<h3>Title: Local-Global Temporal Difference Learning for Satellite Video Super-Resolution. (arXiv:2304.04421v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04421">http://arxiv.org/abs/2304.04421</a></li>
<li>Code URL: <a href="https://github.com/xy-boy/tdmvsr">https://github.com/xy-boy/tdmvsr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04421] Local-Global Temporal Difference Learning for Satellite Video Super-Resolution](http://arxiv.org/abs/2304.04421) #robust</code></li>
<li>Summary: <p>Optical-flow-based and kernel-based approaches have been widely explored for
temporal compensation in satellite video super-resolution (VSR). However, these
techniques involve high computational consumption and are prone to fail under
complex motions. In this paper, we proposed to exploit the well-defined
temporal difference for efficient and robust temporal compensation. To fully
utilize the temporal information within frames, we separately modeled the
short-term and long-term temporal discrepancy since they provide distinctive
complementary properties. Specifically, a short-term temporal difference module
is designed to extract local motion representations from residual maps between
adjacent frames, which provides more clues for accurate texture representation.
Meanwhile, the global dependency in the entire frame sequence is explored via
long-term difference learning. The differences between forward and backward
segments are incorporated and activated to modulate the temporal feature,
resulting in holistic global compensation. Besides, we further proposed a
difference compensation unit to enrich the interaction between the spatial
distribution of the target frame and compensated results, which helps maintain
spatial consistency while refining the features to avoid misalignment.
Extensive objective and subjective evaluation of five mainstream satellite
videos demonstrates that the proposed method performs favorably for satellite
VSR. Code will be available at \url{https://github.com/XY-boy/TDMVSR}
</p></li>
</ul>

<h3>Title: Self-training with dual uncertainty for semi-supervised medical image segmentation. (arXiv:2304.04441v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04441">http://arxiv.org/abs/2304.04441</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04441] Self-training with dual uncertainty for semi-supervised medical image segmentation](http://arxiv.org/abs/2304.04441) #robust</code></li>
<li>Summary: <p>In the field of semi-supervised medical image segmentation, the shortage of
labeled data is the fundamental problem. How to effectively learn image
features from unlabeled images to improve segmentation accuracy is the main
research direction in this field. Traditional self-training methods can
partially solve the problem of insufficient labeled data by generating pseudo
labels for iterative training. However, noise generated due to the model's
uncertainty during training directly affects the segmentation results.
Therefore, we added sample-level and pixel-level uncertainty to stabilize the
training process based on the self-training framework. Specifically, we saved
several moments of the model during pre-training, and used the difference
between their predictions on unlabeled samples as the sample-level uncertainty
estimate for that sample. Then, we gradually add unlabeled samples from easy to
hard during training. At the same time, we added a decoder with different
upsampling methods to the segmentation network and used the difference between
the outputs of the two decoders as pixel-level uncertainty. In short, we
selectively retrained unlabeled samples and assigned pixel-level uncertainty to
pseudo labels to optimize the self-training process. We compared the
segmentation results of our model with five semi-supervised approaches on the
public 2017 ACDC dataset and 2018 Prostate dataset. Our proposed method
achieves better segmentation performance on both datasets under the same
settings, demonstrating its effectiveness, robustness, and potential
transferability to other medical image segmentation tasks. Keywords: Medical
image segmentation, semi-supervised learning, self-training, uncertainty
estimation
</p></li>
</ul>

<h3>Title: Adversarially Robust Neural Architecture Search for Graph Neural Networks. (arXiv:2304.04168v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04168">http://arxiv.org/abs/2304.04168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04168] Adversarially Robust Neural Architecture Search for Graph Neural Networks](http://arxiv.org/abs/2304.04168) #robust</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) obtain tremendous success in modeling relational
data. Still, they are prone to adversarial attacks, which are massive threats
to applying GNNs to risk-sensitive domains. Existing defensive methods neither
guarantee performance facing new data/tasks or adversarial attacks nor provide
insights to understand GNN robustness from an architectural perspective. Neural
Architecture Search (NAS) has the potential to solve this problem by automating
GNN architecture designs. Nevertheless, current graph NAS approaches lack
robust design and are vulnerable to adversarial attacks. To tackle these
challenges, we propose a novel Robust Neural Architecture search framework for
GNNs (G-RNA). Specifically, we design a robust search space for the
message-passing mechanism by adding graph structure mask operations into the
search space, which comprises various defensive operation candidates and allows
us to search for defensive GNNs. Furthermore, we define a robustness metric to
guide the search procedure, which helps to filter robust architectures. In this
way, G-RNA helps understand GNN robustness from an architectural perspective
and effectively searches for optimal adversarial robust GNNs. Extensive
experimental results on benchmark datasets show that G-RNA significantly
outperforms manually designed robust GNNs and vanilla graph NAS baselines by
12.1% to 23.4% under adversarial attacks.
</p></li>
</ul>

<h3>Title: Correcting Model Misspecification via Generative Adversarial Networks. (arXiv:2304.03805v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03805">http://arxiv.org/abs/2304.03805</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03805] Correcting Model Misspecification via Generative Adversarial Networks](http://arxiv.org/abs/2304.03805) #robust</code></li>
<li>Summary: <p>Machine learning models are often misspecified in the likelihood, which leads
to a lack of robustness in the predictions. In this paper, we introduce a
framework for correcting likelihood misspecifications in several paradigm
agnostic noisy prior models and test the model's ability to remove the
misspecification. The "ABC-GAN" framework introduced is a novel generative
modeling paradigm, which combines Generative Adversarial Networks (GANs) and
Approximate Bayesian Computation (ABC). This new paradigm assists the existing
GANs by incorporating any subjective knowledge available about the modeling
process via ABC, as a regularizer, resulting in a partially interpretable model
that operates well under low data regimes. At the same time, unlike any
Bayesian analysis, the explicit knowledge need not be perfect, since the
generator in the GAN can be made arbitrarily complex. ABC-GAN eliminates the
need for summary statistics and distance metrics as the discriminator
implicitly learns them and enables simultaneous specification of multiple
generative models. The model misspecification is simulated in our experiments
by introducing noise of various biases and variances. The correction term is
learnt via the ABC-GAN, with skip connections, referred to as skipGAN. The
strength of the skip connection indicates the amount of correction needed or
how misspecified the prior model is. Based on a simple experimental setup, we
show that the ABC-GAN models not only correct the misspecification of the
prior, but also perform as well as or better than the respective priors under
noisier conditions. In this proposal, we show that ABC-GANs get the best of
both worlds.
</p></li>
</ul>

<h3>Title: Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning. (arXiv:2304.03916v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03916">http://arxiv.org/abs/2304.03916</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03916] Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning](http://arxiv.org/abs/2304.03916) #robust</code></li>
<li>Summary: <p>Spurious correlations that degrade model generalization or lead the model to
be right for the wrong reasons are one of the main robustness concerns for
real-world deployments. However, mitigating these correlations during
pre-training for large-scale models can be costly and impractical, particularly
for those without access to high-performance computing resources. This paper
proposes a novel approach to address spurious correlations during fine-tuning
for a given domain of interest. With a focus on multi-modal models (e.g.,
CLIP), the proposed method leverages different modalities in these models to
detect and explicitly set apart spurious attributes from the affected class,
achieved through a multi-modal contrastive loss function that expresses
spurious relationships through language. Our experimental results and in-depth
visualizations on CLIP show that such an intervention can effectively i)
improve the model's accuracy when spurious attributes are not present, and ii)
directs the model's activation maps towards the actual class rather than the
spurious attribute when present. In particular, on the Waterbirds dataset, our
algorithm achieved a worst-group accuracy 23% higher than ERM on CLIP with a
ResNet-50 backbone, and 32% higher on CLIP with a ViT backbone, while
maintaining the same average accuracy as ERM.
</p></li>
</ul>

<h3>Title: Benchmarking the Robustness of Quantized Models. (arXiv:2304.03968v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03968">http://arxiv.org/abs/2304.03968</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03968] Benchmarking the Robustness of Quantized Models](http://arxiv.org/abs/2304.03968) #robust</code></li>
<li>Summary: <p>Quantization has emerged as an essential technique for deploying deep neural
networks (DNNs) on devices with limited resources. However, quantized models
exhibit vulnerabilities when exposed to various noises in real-world
applications. Despite the importance of evaluating the impact of quantization
on robustness, existing research on this topic is limited and often disregards
established principles of robustness evaluation, resulting in incomplete and
inconclusive findings. To address this gap, we thoroughly evaluated the
robustness of quantized models against various noises (adversarial attacks,
natural corruptions, and systematic noises) on ImageNet. Extensive experiments
demonstrate that lower-bit quantization is more resilient to adversarial
attacks but is more susceptible to natural corruptions and systematic noises.
Notably, our investigation reveals that impulse noise (in natural corruptions)
and the nearest neighbor interpolation (in systematic noises) have the most
significant impact on quantized models. Our research contributes to advancing
the robust quantization of models and their deployment in real-world scenarios.
</p></li>
</ul>

<h3>Title: Reweighted Mixup for Subpopulation Shift. (arXiv:2304.04148v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04148">http://arxiv.org/abs/2304.04148</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04148] Reweighted Mixup for Subpopulation Shift](http://arxiv.org/abs/2304.04148) #robust</code></li>
<li>Summary: <p>Subpopulation shift exists widely in many real-world applications, which
refers to the training and test distributions that contain the same
subpopulation groups but with different subpopulation proportions. Ignoring
subpopulation shifts may lead to significant performance degradation and
fairness concerns. Importance reweighting is a classical and effective way to
handle the subpopulation shift. However, recent studies have recognized that
most of these approaches fail to improve the performance especially when
applied to over-parameterized neural networks which are capable of fitting any
training samples. In this work, we propose a simple yet practical framework,
called reweighted mixup (RMIX), to mitigate the overfitting issue in
over-parameterized models by conducting importance weighting on the ''mixed''
samples. Benefiting from leveraging reweighting in mixup, RMIX allows the model
to explore the vicinal space of minority samples more, thereby obtaining more
robust model against subpopulation shift. When the subpopulation memberships
are unknown, the training-trajectories-based uncertainty estimation is equipped
in the proposed RMIX to flexibly characterize the subpopulation distribution.
We also provide insightful theoretical analysis to verify that RMIX achieves
better generalization bounds over prior works. Further, we conduct extensive
empirical studies across a wide range of tasks to validate the effectiveness of
the proposed method.
</p></li>
</ul>

<h3>Title: $\mu^2$-SGD: Stable Stochastic Optimization via a Double Momentum Mechanism. (arXiv:2304.04172v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04172">http://arxiv.org/abs/2304.04172</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04172] $\mu^2$-SGD: Stable Stochastic Optimization via a Double Momentum Mechanism](http://arxiv.org/abs/2304.04172) #robust</code></li>
<li>Summary: <p>We consider stochastic convex optimization problems where the objective is an
expectation over smooth functions. For this setting we suggest a novel gradient
estimate that combines two recent mechanism that are related to notion of
momentum. Then, we design an SGD-style algorithm as well as an accelerated
version that make use of this new estimator, and demonstrate the robustness of
these new approaches to the choice of the learning rate. Concretely, we show
that these approaches obtain the optimal convergence rates for both noiseless
and noisy case with the same choice of fixed learning rate. Moreover, for the
noisy case we show that these approaches achieve the same optimal bound for a
very wide range of learning rates.
</p></li>
</ul>

<h3>Title: Ensemble Modeling for Time Series Forecasting: an Adaptive Robust Optimization Approach. (arXiv:2304.04308v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04308">http://arxiv.org/abs/2304.04308</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04308] Ensemble Modeling for Time Series Forecasting: an Adaptive Robust Optimization Approach](http://arxiv.org/abs/2304.04308) #robust</code></li>
<li>Summary: <p>Accurate time series forecasting is critical for a wide range of problems
with temporal data. Ensemble modeling is a well-established technique for
leveraging multiple predictive models to increase accuracy and robustness, as
the performance of a single predictor can be highly variable due to shifts in
the underlying data distribution. This paper proposes a new methodology for
building robust ensembles of time series forecasting models. Our approach
utilizes Adaptive Robust Optimization (ARO) to construct a linear regression
ensemble in which the models' weights can adapt over time. We demonstrate the
effectiveness of our method through a series of synthetic experiments and
real-world applications, including air pollution management, energy consumption
forecasting, and tropical cyclone intensity forecasting. Our results show that
our adaptive ensembles outperform the best ensemble member in hindsight by
16-26% in root mean square error and 14-28% in conditional value at risk and
improve over competitive ensemble techniques.
</p></li>
</ul>

<h3>Title: On Robustness in Multimodal Learning. (arXiv:2304.04385v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04385">http://arxiv.org/abs/2304.04385</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04385] On Robustness in Multimodal Learning](http://arxiv.org/abs/2304.04385) #robust</code></li>
<li>Summary: <p>Multimodal learning is defined as learning over multiple heterogeneous input
modalities such as video, audio, and text. In this work, we are concerned with
understanding how models behave as the type of modalities differ between
training and deployment, a situation that naturally arises in many applications
of multimodal learning to hardware platforms. We present a multimodal
robustness framework to provide a systematic analysis of common multimodal
representation learning methods. Further, we identify robustness short-comings
of these approaches and propose two intervention techniques leading to
$1.5\times$-$4\times$ robustness improvements on three datasets, AudioSet,
Kinetics-400 and ImageNet-Captions. Finally, we demonstrate that these
interventions better utilize additional modalities, if present, to achieve
competitive results of $44.2$ mAP on AudioSet 20K.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Word-level Persian Lipreading Dataset. (arXiv:2304.04068v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04068">http://arxiv.org/abs/2304.04068</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04068] Word-level Persian Lipreading Dataset](http://arxiv.org/abs/2304.04068) #extraction</code></li>
<li>Summary: <p>Lip-reading has made impressive progress in recent years, driven by advances
in deep learning. Nonetheless, the prerequisite such advances is a suitable
dataset. This paper provides a new in-the-wild dataset for Persian word-level
lipreading containing 244,000 videos from approximately 1,800 speakers. We
evaluated the state-of-the-art method in this field and used a novel approach
for word-level lip-reading. In this method, we used the AV-HuBERT model for
feature extraction and obtained significantly better performance on our
dataset.
</p></li>
</ul>

<h3>Title: Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention. (arXiv:2304.04237v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04237">http://arxiv.org/abs/2304.04237</a></li>
<li>Code URL: <a href="https://github.com/leaplabthu/slide-transformer">https://github.com/leaplabthu/slide-transformer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04237] Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention](http://arxiv.org/abs/2304.04237) #extraction</code></li>
<li>Summary: <p>Self-attention mechanism has been a key factor in the recent progress of
Vision Transformer (ViT), which enables adaptive feature extraction from global
contexts. However, existing self-attention methods either adopt sparse global
attention or window attention to reduce the computation complexity, which may
compromise the local feature learning or subject to some handcrafted designs.
In contrast, local attention, which restricts the receptive field of each query
to its own neighboring pixels, enjoys the benefits of both convolution and
self-attention, namely local inductive bias and dynamic feature selection.
Nevertheless, current local attention modules either use inefficient Im2Col
function or rely on specific CUDA kernels that are hard to generalize to
devices without CUDA support. In this paper, we propose a novel local attention
module, Slide Attention, which leverages common convolution operations to
achieve high efficiency, flexibility and generalizability. Specifically, we
first re-interpret the column-based Im2Col function from a new row-based
perspective and use Depthwise Convolution as an efficient substitution. On this
basis, we propose a deformed shifting module based on the re-parameterization
technique, which further relaxes the fixed key/value positions to deformed
features in the local region. In this way, our module realizes the local
attention paradigm in both efficient and flexible manner. Extensive experiments
show that our slide attention module is applicable to a variety of advanced
Vision Transformer models and compatible with various hardware devices, and
achieves consistently improved performances on comprehensive benchmarks. Code
is available at https://github.com/LeapLabTHU/Slide-Transformer.
</p></li>
</ul>

<h3>Title: Feature Representation Learning with Adaptive Displacement Generation and Transformer Fusion for Micro-Expression Recognition. (arXiv:2304.04420v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04420">http://arxiv.org/abs/2304.04420</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04420] Feature Representation Learning with Adaptive Displacement Generation and Transformer Fusion for Micro-Expression Recognition](http://arxiv.org/abs/2304.04420) #extraction</code></li>
<li>Summary: <p>Micro-expressions are spontaneous, rapid and subtle facial movements that can
neither be forged nor suppressed. They are very important nonverbal
communication clues, but are transient and of low intensity thus difficult to
recognize. Recently deep learning based methods have been developed for
micro-expression (ME) recognition using feature extraction and fusion
techniques, however, targeted feature learning and efficient feature fusion
still lack further study according to the ME characteristics. To address these
issues, we propose a novel framework Feature Representation Learning with
adaptive Displacement Generation and Transformer fusion (FRL-DGT), in which a
convolutional Displacement Generation Module (DGM) with self-supervised
learning is used to extract dynamic features from onset/apex frames targeted to
the subsequent ME recognition task, and a well-designed Transformer Fusion
mechanism composed of three Transformer-based fusion modules (local, global
fusions based on AU regions and full-face fusion) is applied to extract the
multi-level informative features after DGM for the final ME prediction. The
extensive experiments with solid leave-one-subject-out (LOSO) evaluation
results have demonstrated the superiority of our proposed FRL-DGT to
state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Monocular 3D Human Pose Estimation for Sports Broadcasts using Partial Sports Field Registration. (arXiv:2304.04437v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04437">http://arxiv.org/abs/2304.04437</a></li>
<li>Code URL: <a href="https://github.com/tobibaum/partialsportsfieldreg_3dhpe">https://github.com/tobibaum/partialsportsfieldreg_3dhpe</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04437] Monocular 3D Human Pose Estimation for Sports Broadcasts using Partial Sports Field Registration](http://arxiv.org/abs/2304.04437) #extraction</code></li>
<li>Summary: <p>The filming of sporting events projects and flattens the movement of athletes
in the world onto a 2D broadcast image. The pixel locations of joints in these
images can be detected with high validity. Recovering the actual 3D movement of
the limbs (kinematics) of the athletes requires lifting these 2D pixel
locations back into a third dimension, implying a certain scene geometry. The
well-known line markings of sports fields allow for the calibration of the
camera and for determining the actual geometry of the scene. Close-up shots of
athletes are required to extract detailed kinematics, which in turn obfuscates
the pertinent field markers for camera calibration. We suggest partial sports
field registration, which determines a set of scene-consistent camera
calibrations up to a single degree of freedom. Through joint optimization of 3D
pose estimation and camera calibration, we demonstrate the successful
extraction of 3D running kinematics on a 400m track. In this work, we combine
advances in 2D human pose estimation and camera calibration via partial sports
field registration to demonstrate an avenue for collecting valid large-scale
kinematic datasets. We generate a synthetic dataset of more than 10k images in
Unreal Engine 5 with different viewpoints, running styles, and body types, to
show the limitations of existing monocular 3D HPE methods. Synthetic data and
code are available at https://github.com/tobibaum/PartialSportsFieldReg_3DHPE.
</p></li>
</ul>

<h3>Title: Revisiting Deep Learning for Variable Type Recovery. (arXiv:2304.03854v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03854">http://arxiv.org/abs/2304.03854</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03854] Revisiting Deep Learning for Variable Type Recovery](http://arxiv.org/abs/2304.03854) #extraction</code></li>
<li>Summary: <p>Compiled binary executables are often the only available artifact in reverse
engineering, malware analysis, and software systems maintenance. Unfortunately,
the lack of semantic information like variable types makes comprehending
binaries difficult. In efforts to improve the comprehensibility of binaries,
researchers have recently used machine learning techniques to predict semantic
information contained in the original source code. Chen et al. implemented
DIRTY, a Transformer-based Encoder-Decoder architecture capable of augmenting
decompiled code with variable names and types by leveraging decompiler output
tokens and variable size information. Chen et al. were able to demonstrate a
substantial increase in name and type extraction accuracy on Hex-Rays
decompiler outputs compared to existing static analysis and AI-based
techniques. We extend the original DIRTY results by re-training the DIRTY model
on a dataset produced by the open-source Ghidra decompiler. Although Chen et
al. concluded that Ghidra was not a suitable decompiler candidate due to its
difficulty in parsing and incorporating DWARF symbols during analysis, we
demonstrate that straightforward parsing of variable data generated by Ghidra
results in similar retyping performance. We hope this work inspires further
interest and adoption of the Ghidra decompiler for use in research projects.
</p></li>
</ul>

<h3>Title: Generating a Graph Colouring Heuristic with Deep Q-Learning and Graph Neural Networks. (arXiv:2304.04051v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04051">http://arxiv.org/abs/2304.04051</a></li>
<li>Code URL: <a href="https://github.com/gpdwatkins/graph_colouring_with_RL">https://github.com/gpdwatkins/graph_colouring_with_RL</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04051] Generating a Graph Colouring Heuristic with Deep Q-Learning and Graph Neural Networks](http://arxiv.org/abs/2304.04051) #extraction</code></li>
<li>Summary: <p>The graph colouring problem consists of assigning labels, or colours, to the
vertices of a graph such that no two adjacent vertices share the same colour.
In this work we investigate whether deep reinforcement learning can be used to
discover a competitive construction heuristic for graph colouring. Our proposed
approach, ReLCol, uses deep Q-learning together with a graph neural network for
feature extraction, and employs a novel way of parameterising the graph that
results in improved performance. Using standard benchmark graphs with varied
topologies, we empirically evaluate the benefits and limitations of the
heuristic learned by ReLCol relative to existing construction algorithms, and
demonstrate that reinforcement learning is a promising direction for further
research on the graph colouring problem.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: AI-assisted Automated Workflow for Real-time X-ray Ptychography Data Analysis via Federated Resources. (arXiv:2304.04297v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04297">http://arxiv.org/abs/2304.04297</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04297] AI-assisted Automated Workflow for Real-time X-ray Ptychography Data Analysis via Federated Resources](http://arxiv.org/abs/2304.04297) #federate</code></li>
<li>Summary: <p>We present an end-to-end automated workflow that uses large-scale remote
compute resources and an embedded GPU platform at the edge to enable
AI/ML-accelerated real-time analysis of data collected for x-ray ptychography.
Ptychography is a lensless method that is being used to image samples through a
simultaneous numerical inversion of a large number of diffraction patterns from
adjacent overlapping scan positions. This acquisition method can enable
nanoscale imaging with x-rays and electrons, but this often requires very large
experimental datasets and commensurately high turnaround times, which can limit
experimental capabilities such as real-time experimental steering and
low-latency monitoring. In this work, we introduce a software system that can
automate ptychography data analysis tasks. We accelerate the data analysis
pipeline by using a modified version of PtychoNN -- an ML-based approach to
solve phase retrieval problem that shows two orders of magnitude speedup
compared to traditional iterative methods. Further, our system coordinates and
overlaps different data analysis tasks to minimize synchronization overhead
between different stages of the workflow. We evaluate our workflow system with
real-world experimental workloads from the 26ID beamline at Advanced Photon
Source and ThetaGPU cluster at Argonne Leadership Computing Resources.
</p></li>
</ul>

<h3>Title: FedPNN: One-shot Federated Classification via Evolving Clustering Method and Probabilistic Neural Network hybrid. (arXiv:2304.04147v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04147">http://arxiv.org/abs/2304.04147</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04147] FedPNN: One-shot Federated Classification via Evolving Clustering Method and Probabilistic Neural Network hybrid](http://arxiv.org/abs/2304.04147) #federate</code></li>
<li>Summary: <p>Protecting data privacy is paramount in the fields such as finance, banking,
and healthcare. Federated Learning (FL) has attracted widespread attention due
to its decentralized, distributed training and the ability to protect the
privacy while obtaining a global shared model. However, FL presents challenges
such as communication overhead, and limited resource capability. This motivated
us to propose a two-stage federated learning approach toward the objective of
privacy protection, which is a first-of-its-kind study as follows: (i) During
the first stage, the synthetic dataset is generated by employing two different
distributions as noise to the vanilla conditional tabular generative
adversarial neural network (CTGAN) resulting in modified CTGAN, and (ii) In the
second stage, the Federated Probabilistic Neural Network (FedPNN) is developed
and employed for building globally shared classification model. We also
employed synthetic dataset metrics to check the quality of the generated
synthetic dataset. Further, we proposed a meta-clustering algorithm whereby the
cluster centers obtained from the clients are clustered at the server for
training the global model. Despite PNN being a one-pass learning classifier,
its complexity depends on the training data size. Therefore, we employed a
modified evolving clustering method (ECM), another one-pass algorithm to
cluster the training data thereby increasing the speed further. Moreover, we
conducted sensitivity analysis by varying Dthr, a hyperparameter of ECM at the
server and client, one at a time. The effectiveness of our approach is
validated on four finance and medical datasets.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: H2RBox-v2: Boosting HBox-supervised Oriented Object Detection via Symmetric Learning. (arXiv:2304.04403v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04403">http://arxiv.org/abs/2304.04403</a></li>
<li>Code URL: <a href="https://github.com/li-qingyun/sam-mmrotate">https://github.com/li-qingyun/sam-mmrotate</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04403] H2RBox-v2: Boosting HBox-supervised Oriented Object Detection via Symmetric Learning](http://arxiv.org/abs/2304.04403) #fair</code></li>
<li>Summary: <p>With the increasing demand for oriented object detection e.g. in autonomous
driving and remote sensing, the oriented annotation has become a
labor-intensive work. To make full use of existing horizontally annotated
datasets and reduce the annotation cost, a weakly-supervised detector H2RBox
for learning the rotated box (RBox) from the horizontal box (HBox) has been
proposed and received great attention. This paper presents a new version,
H2RBox-v2, to further bridge the gap between HBox-supervised and
RBox-supervised oriented object detection. While exploiting axisymmetry via
flipping and rotating consistencies is available through our theoretical
analysis, H2RBox-v2, using a weakly-supervised branch similar to H2RBox, is
embedded with a novel self-supervised branch that learns orientations from the
symmetry inherent in the image of objects. Complemented by modules to cope with
peripheral issues, e.g. angular periodicity, a stable and effective solution is
achieved. To our knowledge, H2RBox-v2 is the first symmetry-supervised paradigm
for oriented object detection. Compared to H2RBox, our method is less
susceptible to low annotation quality and insufficient training data, which in
such cases is expected to give a competitive performance much closer to
fully-supervised oriented object detectors. Specifically, the performance
comparison between H2RBox-v2 and Rotated FCOS on DOTA-v1.0/1.5/2.0 is
72.31%/64.76%/50.33% vs. 72.44%/64.53%/51.77%, 89.66% vs. 88.99% on HRSC, and
42.27% vs. 41.25% on FAIR1M.
</p></li>
</ul>

<h3>Title: A roadmap to fair and trustworthy prediction model validation in healthcare. (arXiv:2304.03779v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03779">http://arxiv.org/abs/2304.03779</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03779] A roadmap to fair and trustworthy prediction model validation in healthcare](http://arxiv.org/abs/2304.03779) #fair</code></li>
<li>Summary: <p>A prediction model is most useful if it generalizes beyond the development
data with external validations, but to what extent should it generalize remains
unclear. In practice, prediction models are externally validated using data
from very different settings, including populations from other health systems
or countries, with predictably poor results. This may not be a fair reflection
of the performance of the model which was designed for a specific target
population or setting, and may be stretching the expected model
generalizability. To address this, we suggest to externally validate a model
using new data from the target population to ensure clear implications of
validation performance on model reliability, whereas model generalizability to
broader settings should be carefully investigated during model development
instead of explored post-hoc. Based on this perspective, we propose a roadmap
that facilitates the development and application of reliable, fair, and
trustworthy artificial intelligence prediction models.
</p></li>
</ul>

<h3>Title: Last-Layer Fairness Fine-tuning is Simple and Effective for Neural Networks. (arXiv:2304.03935v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03935">http://arxiv.org/abs/2304.03935</a></li>
<li>Code URL: <a href="https://github.com/yuzhenmao/fairness-finetuning">https://github.com/yuzhenmao/fairness-finetuning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03935] Last-Layer Fairness Fine-tuning is Simple and Effective for Neural Networks](http://arxiv.org/abs/2304.03935) #fair</code></li>
<li>Summary: <p>As machine learning has been deployed ubiquitously across applications in
modern data science, algorithmic fairness has become a great concern and
varieties of fairness criteria have been proposed. Among them, imposing
fairness constraints during learning, i.e. in-processing fair training, has
been a popular type of training method because they don't require accessing
sensitive attributes during test time in contrast to post-processing methods.
Although imposing fairness constraints have been studied extensively for
classical machine learning models, the effect these techniques have on deep
neural networks is still unclear. Recent research has shown that adding
fairness constraints to the objective function leads to severe over-fitting to
fairness criteria in large models, and how to solve this challenge is an
important open question. To address this challenge, we leverage the wisdom and
power of pre-training and fine-tuning and develop a simple but novel framework
to train fair neural networks in an efficient and inexpensive way. We conduct
comprehensive experiments on two popular image datasets with state-of-art
architectures under different fairness notions to show that last-layer
fine-tuning is sufficient for promoting fairness of the deep neural network.
Our framework brings new insights into representation learning in training fair
neural networks.
</p></li>
</ul>

<h3>Title: Best Arm Identification with Fairness Constraints on Subpopulations. (arXiv:2304.04091v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04091">http://arxiv.org/abs/2304.04091</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04091] Best Arm Identification with Fairness Constraints on Subpopulations](http://arxiv.org/abs/2304.04091) #fair</code></li>
<li>Summary: <p>We formulate, analyze and solve the problem of best arm identification with
fairness constraints on subpopulations (BAICS). Standard best arm
identification problems aim at selecting an arm that has the largest expected
reward where the expectation is taken over the entire population. The BAICS
problem requires that an selected arm must be fair to all subpopulations (e.g.,
different ethnic groups, age groups, or customer types) by satisfying
constraints that the expected reward conditional on every subpopulation needs
to be larger than some thresholds. The BAICS problem aims at correctly
identify, with high confidence, the arm with the largest expected reward from
all arms that satisfy subpopulation constraints. We analyze the complexity of
the BAICS problem by proving a best achievable lower bound on the sample
complexity with closed-form representation. We then design an algorithm and
prove that the algorithm's sample complexity matches with the lower bound in
terms of order. A brief account of numerical experiments are conducted to
illustrate the theoretical findings.
</p></li>
</ul>

<h3>Title: CILIATE: Towards Fairer Class-based Incremental Learning by Dataset and Training Refinement. (arXiv:2304.04222v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04222">http://arxiv.org/abs/2304.04222</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04222] CILIATE: Towards Fairer Class-based Incremental Learning by Dataset and Training Refinement](http://arxiv.org/abs/2304.04222) #fair</code></li>
<li>Summary: <p>Due to the model aging problem, Deep Neural Networks (DNNs) need updates to
adjust them to new data distributions. The common practice leverages
incremental learning (IL), e.g., Class-based Incremental Learning (CIL) that
updates output labels, to update the model with new data and a limited number
of old data. This avoids heavyweight training (from scratch) using conventional
methods and saves storage space by reducing the number of old data to store.
But it also leads to poor performance in fairness. In this paper, we show that
CIL suffers both dataset and algorithm bias problems, and existing solutions
can only partially solve the problem. We propose a novel framework, CILIATE,
that fixes both dataset and algorithm bias in CIL. It features a novel
differential analysis guided dataset and training refinement process that
identifies unique and important samples overlooked by existing CIL and enforces
the model to learn from them. Through this process, CILIATE improves the
fairness of CIL by 17.03%, 22.46%, and 31.79% compared to state-of-the-art
methods, iCaRL, BiC, and WA, respectively, based on our evaluation on three
popular datasets and widely used ResNet models.
</p></li>
</ul>

<h3>Title: CAFIN: Centrality Aware Fairness inducing IN-processing for Unsupervised Representation Learning on Graphs. (arXiv:2304.04391v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04391">http://arxiv.org/abs/2304.04391</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04391] CAFIN: Centrality Aware Fairness inducing IN-processing for Unsupervised Representation Learning on Graphs](http://arxiv.org/abs/2304.04391) #fair</code></li>
<li>Summary: <p>Unsupervised representation learning on (large) graphs has received
significant attention in the research community due to the compactness and
richness of the learned embeddings and the abundance of unlabelled graph data.
When deployed, these node representations must be generated with appropriate
fairness constraints to minimize bias induced by them on downstream tasks.
Consequently, group and individual fairness notions for graph learning
algorithms have been investigated for specific downstream tasks. One major
limitation of these fairness notions is that they do not consider the
connectivity patterns in the graph leading to varied node influence (or
centrality power). In this paper, we design a centrality-aware fairness
framework for inductive graph representation learning algorithms. We propose
CAFIN (Centrality Aware Fairness inducing IN-processing), an in-processing
technique that leverages graph structure to improve GraphSAGE's representations</li>
<li>a popular framework in the unsupervised inductive setting. We demonstrate the
efficacy of CAFIN in the inductive setting on two popular downstream tasks -
Link prediction and Node Classification. Empirically, they consistently
minimize the disparity in fairness between groups across datasets (varying from
18 to 80% reduction in imparity, a measure of group fairness) from different
domains while incurring only a minimal performance cost.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Pump It Up: Predict Water Pump Status using Attentive Tabular Learning. (arXiv:2304.03969v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03969">http://arxiv.org/abs/2304.03969</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03969] Pump It Up: Predict Water Pump Status using Attentive Tabular Learning](http://arxiv.org/abs/2304.03969) #interpretability</code></li>
<li>Summary: <p>Water crisis is a crucial concern around the globe. Appropriate and timely
maintenance of water pumps in drought-hit countries is vital for communities
relying on the well. In this paper, we analyze and apply a sequential attentive
deep neural architecture, TabNet, for predicting water pump repair status in
Tanzania. The model combines the valuable benefits of tree-based algorithms and
neural networks, enabling end-to-end training, model interpretability, sparse
feature selection, and efficient learning on tabular data. Finally, we compare
the performance of TabNet with popular gradient tree-boosting algorithms like
XGBoost, LightGBM,CatBoost, and demonstrate how we can further uplift the
performance by choosing focal loss as the objective function while training on
imbalanced data.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Bipol: A Novel Multi-Axes Bias Evaluation Metric with Explainability for NLP. (arXiv:2304.04029v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04029">http://arxiv.org/abs/2304.04029</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04029] Bipol: A Novel Multi-Axes Bias Evaluation Metric with Explainability for NLP](http://arxiv.org/abs/2304.04029) #explainability</code></li>
<li>Summary: <p>We introduce bipol, a new metric with explainability, for estimating social
bias in text data. Harmful bias is prevalent in many online sources of data
that are used for training machine learning (ML) models. In a step to address
this challenge we create a novel metric that involves a two-step process:
corpus-level evaluation based on model classification and sentence-level
evaluation based on (sensitive) term frequency (TF). After creating new models
to detect bias along multiple axes using SotA architectures, we evaluate two
popular NLP datasets (COPA and SQUAD). As additional contribution, we created a
large dataset (with almost 2 million labelled samples) for training models in
bias detection and make it publicly available. We also make public our codes.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: ChiroDiff: Modelling chirographic data with Diffusion Models. (arXiv:2304.03785v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03785">http://arxiv.org/abs/2304.03785</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03785] ChiroDiff: Modelling chirographic data with Diffusion Models](http://arxiv.org/abs/2304.03785) #diffusion</code></li>
<li>Summary: <p>Generative modelling over continuous-time geometric constructs, a.k.a such as
handwriting, sketches, drawings etc., have been accomplished through
autoregressive distributions. Such strictly-ordered discrete factorization
however falls short of capturing key properties of chirographic data -- it
fails to build holistic understanding of the temporal concept due to one-way
visibility (causality). Consequently, temporal data has been modelled as
discrete token sequences of fixed sampling rate instead of capturing the true
underlying concept. In this paper, we introduce a powerful model-class namely
"Denoising Diffusion Probabilistic Models" or DDPMs for chirographic data that
specifically addresses these flaws. Our model named "ChiroDiff", being
non-autoregressive, learns to capture holistic concepts and therefore remains
resilient to higher temporal sampling rate up to a good extent. Moreover, we
show that many important downstream utilities (e.g. conditional sampling,
creative mixing) can be flexibly implemented using ChiroDiff. We further show
some unique use-cases like stochastic vectorization, de-noising/healing,
abstraction are also possible with this model-class. We perform quantitative
and qualitative evaluation of our framework on relevant datasets and found it
to be better or on par with competing approaches.
</p></li>
</ul>

<h3>Title: Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis. (arXiv:2304.03869v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.03869">http://arxiv.org/abs/2304.03869</a></li>
<li>Code URL: <a href="https://github.com/ucsb-nlp-chang/diffusion-spacetime-attn">https://github.com/ucsb-nlp-chang/diffusion-spacetime-attn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.03869] Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis](http://arxiv.org/abs/2304.03869) #diffusion</code></li>
<li>Summary: <p>Diffusion-based models have achieved state-of-the-art performance on
text-to-image synthesis tasks. However, one critical limitation of these models
is the low fidelity of generated images with respect to the text description,
such as missing objects, mismatched attributes, and mislocated objects. One key
reason for such inconsistencies is the inaccurate cross-attention to text in
both the spatial dimension, which controls at what pixel region an object
should appear, and the temporal dimension, which controls how different levels
of details are added through the denoising steps. In this paper, we propose a
new text-to-image algorithm that adds explicit control over spatial-temporal
cross-attention in diffusion models. We first utilize a layout predictor to
predict the pixel regions for objects mentioned in the text. We then impose
spatial attention control by combining the attention over the entire text
description and that over the local description of the particular object in the
corresponding pixel region of that object. The temporal attention control is
further added by allowing the combination weights to change at each denoising
step, and the combination weights are optimized to ensure high fidelity between
the image and the text. Experiments show that our method generates images with
higher fidelity compared to diffusion-model-based baselines without fine-tuning
the diffusion model. Our code is publicly available at
https://github.com/UCSB-NLP-Chang/Diffusion-SpaceTime-Attn.
</p></li>
</ul>

<h3>Title: CCLAP: Controllable Chinese Landscape Painting Generation via Latent Diffusion Model. (arXiv:2304.04156v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04156">http://arxiv.org/abs/2304.04156</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04156] CCLAP: Controllable Chinese Landscape Painting Generation via Latent Diffusion Model](http://arxiv.org/abs/2304.04156) #diffusion</code></li>
<li>Summary: <p>With the development of deep generative models, recent years have seen great
success of Chinese landscape painting generation. However, few works focus on
controllable Chinese landscape painting generation due to the lack of data and
limited modeling capabilities. In this work, we propose a controllable Chinese
landscape painting generation method named CCLAP, which can generate painting
with specific content and style based on Latent Diffusion Model. Specifically,
it consists of two cascaded modules, i.e., content generator and style
aggregator. The content generator module guarantees the content of generated
paintings specific to the input text. While the style aggregator module is to
generate paintings of a style corresponding to a reference image. Moreover, a
new dataset of Chinese landscape paintings named CLAP is collected for
comprehensive evaluation. Both the qualitative and quantitative results
demonstrate that our method achieves state-of-the-art performance, especially
in artfully-composed and artistic conception. Codes are available at
https://github.com/Robin-WZQ/CCLAP.
</p></li>
</ul>

<h3>Title: HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image Generation. (arXiv:2304.04269v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04269">http://arxiv.org/abs/2304.04269</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04269] HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image Generation](http://arxiv.org/abs/2304.04269) #diffusion</code></li>
<li>Summary: <p>Controllable human image generation (HIG) has numerous real-life
applications. State-of-the-art solutions, such as ControlNet and T2I-Adapter,
introduce an additional learnable branch on top of the frozen pre-trained
stable diffusion (SD) model, which can enforce various conditions, including
skeleton guidance of HIG. While such a plug-and-play approach is appealing, the
inevitable and uncertain conflicts between the original images produced from
the frozen SD branch and the given condition incur significant challenges for
the learnable branch, which essentially conducts image feature editing for
condition enforcement. In this work, we propose a native skeleton-guided
diffusion model for controllable HIG called HumanSD. Instead of performing
image editing with dual-branch diffusion, we fine-tune the original SD model
using a novel heatmap-guided denoising loss. This strategy effectively and
efficiently strengthens the given skeleton condition during model training
while mitigating the catastrophic forgetting effects. HumanSD is fine-tuned on
the assembly of three large-scale human-centric datasets with text-image-pose
information, two of which are established in this work. As shown in Figure 1,
HumanSD outperforms ControlNet in terms of accurate pose control and image
quality, particularly when the given skeleton guidance is sophisticated.
</p></li>
</ul>

<h3>Title: Towards Real-time Text-driven Image Manipulation with Unconditional Diffusion Models. (arXiv:2304.04344v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04344">http://arxiv.org/abs/2304.04344</a></li>
<li>Code URL: <a href="https://github.com/quickjkee/eff-diff-edit">https://github.com/quickjkee/eff-diff-edit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04344] Towards Real-time Text-driven Image Manipulation with Unconditional Diffusion Models](http://arxiv.org/abs/2304.04344) #diffusion</code></li>
<li>Summary: <p>Recent advances in diffusion models enable many powerful instruments for
image editing. One of these instruments is text-driven image manipulations:
editing semantic attributes of an image according to the provided text
description. % Popular text-conditional diffusion models offer various
high-quality image manipulation methods for a broad range of text prompts.
Existing diffusion-based methods already achieve high-quality image
manipulations for a broad range of text prompts. However, in practice, these
methods require high computation costs even with a high-end GPU. This greatly
limits potential real-world applications of diffusion-based image editing,
especially when running on user devices.
</p></li>
</ul>

<p>In this paper, we address efficiency of the recent text-driven editing
methods based on unconditional diffusion models and develop a novel algorithm
that learns image manipulations 4.5-10 times faster and applies them 8 times
faster. We carefully evaluate the visual quality and expressiveness of our
approach on multiple datasets using human annotators. Our experiments
demonstrate that our algorithm achieves the quality of much more expensive
methods. Finally, we show that our approach can adapt the pretrained model to
the user-specified image and text description on the fly just for 4 seconds. In
this setting, we notice that more compact unconditional diffusion models can be
considered as a rational alternative to the popular text-conditional
counterparts.
</p>

<h3>Title: BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation. (arXiv:2304.04429v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04429">http://arxiv.org/abs/2304.04429</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04429] BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation](http://arxiv.org/abs/2304.04429) #diffusion</code></li>
<li>Summary: <p>Medical image segmentation is a challenging task with inherent ambiguity and
high uncertainty, attributed to factors such as unclear tumor boundaries and
multiple plausible annotations. The accuracy and diversity of segmentation
masks are both crucial for providing valuable references to radiologists in
clinical practice. While existing diffusion models have shown strong capacities
in various visual generation tasks, it is still challenging to deal with
discrete masks in segmentation. To achieve accurate and diverse medical image
segmentation masks, we propose a novel conditional Bernoulli Diffusion model
for medical image segmentation (BerDiff). Instead of using the Gaussian noise,
we first propose to use the Bernoulli noise as the diffusion kernel to enhance
the capacity of the diffusion model for binary segmentation tasks, resulting in
more accurate segmentation masks. Second, by leveraging the stochastic nature
of the diffusion model, our BerDiff randomly samples the initial Bernoulli
noise and intermediate latent variables multiple times to produce a range of
diverse segmentation masks, which can highlight salient regions of interest
that can serve as valuable references for radiologists. In addition, our
BerDiff can efficiently sample sub-sequences from the overall trajectory of the
reverse diffusion, thereby speeding up the segmentation process. Extensive
experimental results on two medical image segmentation datasets with different
modalities demonstrate that our BerDiff outperforms other recently published
state-of-the-art methods. Our results suggest diffusion models could serve as a
strong backbone for medical image segmentation.
</p></li>
</ul>

<h3>Title: A Comprehensive Survey on Knowledge Distillation of Diffusion Models. (arXiv:2304.04262v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.04262">http://arxiv.org/abs/2304.04262</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.04262] A Comprehensive Survey on Knowledge Distillation of Diffusion Models](http://arxiv.org/abs/2304.04262) #diffusion</code></li>
<li>Summary: <p>Diffusion Models (DMs), also referred to as score-based diffusion models,
utilize neural networks to specify score functions. Unlike most other
probabilistic models, DMs directly model the score functions, which makes them
more flexible to parametrize and potentially highly expressive for
probabilistic modeling. DMs can learn fine-grained knowledge, i.e., marginal
score functions, of the underlying distribution. Therefore, a crucial research
direction is to explore how to distill the knowledge of DMs and fully utilize
their potential. Our objective is to provide a comprehensible overview of the
modern approaches for distilling DMs, starting with an introduction to DMs and
a discussion of the challenges involved in distilling them into neural vector
fields. We also provide an overview of the existing works on distilling DMs
into both stochastic and deterministic implicit generators. Finally, we review
the accelerated diffusion sampling algorithms as a training-free method for
distillation. Our tutorial is intended for individuals with a basic
understanding of generative models who wish to apply DM's distillation or
embark on a research project in this field.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
